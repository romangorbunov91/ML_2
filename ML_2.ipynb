{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "453d2819",
   "metadata": {},
   "source": [
    "# Градиентный спуск\n",
    "## Задача поиска оптимальных коэффициентов полиномиальной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3d6c553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from livelossplot import PlotLosses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a770ef8",
   "metadata": {},
   "source": [
    "Формируем синтетический датасет на основе полиномиальной функции с добавлением случайного шума:\n",
    "\n",
    "${f(x) = \\sum_{k=0}^{K-1}{w_k \\cdot x^k} + \\epsilon}$,\n",
    "\n",
    "где ${w}$ - массив весов размера ${K}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "674758b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def f_trend(x):\n",
    "    return x**3 - 3*x**2 + 2*x - 5\n",
    "\n",
    "x_min = 0\n",
    "x_max = 3\n",
    "N_points = 2000\n",
    "# Случайный набор x-ов.\n",
    "X_train = np.random.uniform(low=x_min, high=x_max, size=(N_points,))\n",
    "# Отклики с добавлением шума.\n",
    "y_train = f_trend(X_train) + np.random.normal(0,0.2,N_points)\n",
    "\n",
    "plt.scatter(X_train, y_train)\n",
    "plt.plot(np.sort(X_train), f_trend(np.sort(X_train)), color='red')\n",
    "plt.legend(['train dataset', 'trend line'])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05952c8",
   "metadata": {},
   "source": [
    "Определим полиномиальную функцию в общем виде:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee933d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_poly(X, w_coeff):\n",
    "    Y = np.zeros_like(X)\n",
    "    for n, x in enumerate(X):\n",
    "        for idx, w in enumerate(w_coeff.tolist()[0]):\n",
    "            Y[n] = Y[n] + w * x**idx\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a4f4da",
   "metadata": {},
   "source": [
    "Функция ошибки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "858c2928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(X, y, w_coeff):\n",
    "    return np.sum((y - f_poly(X, w_coeff))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "551da401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit(X_train, y_train, learning_rate, tolerance, beta, batch_ratio, liveplot):\n",
    "    if liveplot:\n",
    "        liveloss = PlotLosses()\n",
    "    batch_size = int(batch_ratio * len(X_train))\n",
    "    \n",
    "    # Коэффициенты (веса) инициилизируются нулями.\n",
    "    w_coeff = np.zeros((1,len(learning_rate)))\n",
    "    # Буфер коэффициентов.\n",
    "    w_coeff_buff = w_coeff.copy()\n",
    "    \n",
    "    # Градиенты инициализируются ненулевыми значениями.\n",
    "    w_grad = np.ones_like(w_coeff) * 1e-3\n",
    "    \n",
    "    # Аккумулятор для Momentum.\n",
    "    G = np.zeros_like(w_coeff)\n",
    "\n",
    "    losses = []\n",
    "    # Инициализируем loss > tolerance.\n",
    "    loss = 2 * tolerance\n",
    "    i = 0\n",
    "    while loss > tolerance:\n",
    "        i += 1\n",
    "        for k, w in enumerate(w_coeff.tolist()[0]):\n",
    "            # Накопление \"момента\".\n",
    "            G[0,k] = beta * G[0,k] + (1 - beta) * w_grad[0,k]\n",
    "            # Шаг градиентного спуска.\n",
    "            w_step = - learning_rate[k] * G[0,k]\n",
    "            # Сбрасываем коэффициенты до ранее установленных.\n",
    "            w_coeff_1 = w_coeff.copy()\n",
    "            w_coeff_2 = w_coeff.copy()\n",
    "            # Модификация только k-го коэффициента:\n",
    "            w_coeff_1[0,k] = w + w_step\n",
    "            w_coeff_2[0,k] = w + 2*w_step\n",
    "\n",
    "            # Выборка случайных элементов (по индексам).\n",
    "            batch_indices = np.random.choice(len(X_train), size=batch_size, replace=True)\n",
    "            X_train_batch = [X_train[idx] for idx in batch_indices]\n",
    "            y_train_batch = [y_train[idx] for idx in batch_indices]\n",
    "            \n",
    "            # Формируем массив значений функции потерь, для вычисления градиента. Массив состоит из 3х элементов.\n",
    "            loss_func_grad = [loss_func(X_train_batch, y_train_batch, w_coeff  ),\n",
    "                              loss_func(X_train_batch, y_train_batch, w_coeff_1),\n",
    "                              loss_func(X_train_batch, y_train_batch, w_coeff_2)]\n",
    "            # Массив градиента состоит из 3х точек с индексами [0, 1, 2]. Берем предпоследнюю.\n",
    "            w_grad[0,k] = np.gradient(loss_func_grad, w_step)[1]\n",
    "            \n",
    "            # Обновление оного коэффициента в итоговом массиве.\n",
    "            w_coeff_buff[0,k] = w_coeff_1[0,k]\n",
    "        # Обновление всех коэффициентов.\n",
    "        w_coeff = w_coeff_buff.copy()\n",
    "        # Вычисление ошибки на полном датасете, при обновленных коэффициентах.\n",
    "        loss = loss_func(X_train, y_train, w_coeff)\n",
    "        # Накопление ошибки в отдельный массив для дальнейшей визуализации.\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print('Iteration:', i)\n",
    "            print('Gradient:', np.round(w_grad,4))\n",
    "            print('Weights:', np.round(w_coeff,4))\n",
    "            print('MSE loss:', np.round(loss,4))\n",
    "            if liveplot:\n",
    "                if loss < 1000:\n",
    "                    liveloss.update({'MSE loss': loss})\n",
    "                    liveloss.draw()\n",
    "    return w_coeff, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "add33c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100\n",
      "Gradient: [[1080.6926 1186.0099 1619.2532 1556.5892 3644.789 ]]\n",
      "Weights: [[-1.2749e+00 -1.6200e-01 -2.6800e-02 -4.8000e-03 -9.0000e-04]]\n",
      "MSE loss: 19483.4896\n",
      "Iteration: 200\n",
      "Gradient: [[  607.1605   451.006    197.2862 -1093.3716 -2748.1797]]\n",
      "Weights: [[-2.1385e+00 -2.5120e-01 -3.7700e-02 -5.8000e-03 -9.0000e-04]]\n",
      "MSE loss: 11322.2465\n",
      "Iteration: 300\n",
      "Gradient: [[  437.366    296.9573  -442.6372 -2916.8167 -8686.2125]]\n",
      "Weights: [[-2.6759e+00 -2.8890e-01 -3.7400e-02 -4.5000e-03 -3.0000e-04]]\n",
      "MSE loss: 8340.196\n",
      "Iteration: 400\n",
      "Gradient: [[ 3.0840130e+02 -5.8083000e+00 -7.7487580e+02 -1.6674791e+03\n",
      "  -7.2143797e+03]]\n",
      "Weights: [[-3.0318e+00 -2.9850e-01 -3.2200e-02 -1.9000e-03  6.0000e-04]]\n",
      "MSE loss: 6940.7896\n",
      "Iteration: 500\n",
      "Gradient: [[   132.6268   -168.1429   -949.7415  -2963.4915 -12005.8911]]\n",
      "Weights: [[-3.2762e+00 -2.9390e-01 -2.4300e-02  1.4000e-03  1.6000e-03]]\n",
      "MSE loss: 6079.4881\n",
      "Iteration: 600\n",
      "Gradient: [[  178.7013   -11.6328  -566.5923 -3124.5613 -8653.3476]]\n",
      "Weights: [[-3.4632e+00 -2.8150e-01 -1.5100e-02  4.7000e-03  2.7000e-03]]\n",
      "MSE loss: 5416.1958\n",
      "Iteration: 700\n",
      "Gradient: [[  130.5722    62.9744 -1077.7689 -2543.7523 -9455.7364]]\n",
      "Weights: [[-3.6166 -0.2687 -0.0056  0.008   0.0037]]\n",
      "MSE loss: 4868.6872\n",
      "Iteration: 800\n",
      "Gradient: [[  143.6019  -141.9506 -1091.6606 -2907.6657 -9913.1665]]\n",
      "Weights: [[-3.749e+00 -2.548e-01  3.300e-03  1.100e-02  4.700e-03]]\n",
      "MSE loss: 4400.824\n",
      "Iteration: 900\n",
      "Gradient: [[   104.0806   -144.3255   -973.3287  -3255.9048 -14411.3241]]\n",
      "Weights: [[-3.8622 -0.241   0.0113  0.0141  0.0056]]\n",
      "MSE loss: 3997.0728\n",
      "Iteration: 1000\n",
      "Gradient: [[  103.2009  -175.4579  -935.584  -3430.8801 -6872.9808]]\n",
      "Weights: [[-3.9643 -0.2275  0.0196  0.0169  0.0065]]\n",
      "MSE loss: 3638.2784\n",
      "Iteration: 1100\n",
      "Gradient: [[  113.138   -200.9631  -834.0218  -969.8768 -9617.9869]]\n",
      "Weights: [[-4.0566 -0.2142  0.0272  0.0197  0.0074]]\n",
      "MSE loss: 3320.8741\n",
      "Iteration: 1200\n",
      "Gradient: [[   55.0267  -159.2938  -788.0763 -1299.4197 -5507.2149]]\n",
      "Weights: [[-4.1491 -0.2019  0.0346  0.0223  0.0083]]\n",
      "MSE loss: 3036.9702\n",
      "Iteration: 1300\n",
      "Gradient: [[    86.9007   -132.7577   -942.3806  -2223.4359 -10053.7326]]\n",
      "Weights: [[-4.2309 -0.1906  0.0415  0.0246  0.009 ]]\n",
      "MSE loss: 2793.7304\n",
      "Iteration: 1400\n",
      "Gradient: [[  133.0732  -201.0302  -282.5313 -2083.5221 -7850.5941]]\n",
      "Weights: [[-4.3093 -0.181   0.048   0.0269  0.0098]]\n",
      "MSE loss: 2576.2073\n",
      "Iteration: 1500\n",
      "Gradient: [[    74.0232   -116.5048   -791.4773  -2411.1097 -10990.6616]]\n",
      "Weights: [[-4.3802 -0.1715  0.0539  0.0291  0.0105]]\n",
      "MSE loss: 2386.0991\n",
      "Iteration: 1600\n",
      "Gradient: [[   34.6374   -73.1336  -245.6464 -1902.2537 -7455.221 ]]\n",
      "Weights: [[-4.4459 -0.163   0.0595  0.0312  0.0111]]\n",
      "MSE loss: 2219.8133\n",
      "Iteration: 1700\n",
      "Gradient: [[   89.1494  -139.1093  -289.6666 -2095.1718 -7968.4695]]\n",
      "Weights: [[-4.5091 -0.1558  0.0648  0.0332  0.0118]]\n",
      "MSE loss: 2071.9508\n",
      "Iteration: 1800\n",
      "Gradient: [[   77.9027   -42.2502  -297.9049 -2249.2055 -7513.8486]]\n",
      "Weights: [[-4.5658 -0.1493  0.0696  0.0351  0.0123]]\n",
      "MSE loss: 1943.6772\n",
      "Iteration: 1900\n",
      "Gradient: [[   13.5313  -104.4529  -526.2122 -1702.2877 -6707.7632]]\n",
      "Weights: [[-4.6132 -0.1437  0.074   0.0369  0.0129]]\n",
      "MSE loss: 1833.8573\n",
      "Iteration: 2000\n",
      "Gradient: [[   49.0707   -84.1401  -367.039  -2630.1187 -5638.3878]]\n",
      "Weights: [[-4.662  -0.1393  0.0782  0.0386  0.0135]]\n",
      "MSE loss: 1733.3444\n",
      "Iteration: 2100\n",
      "Gradient: [[   55.2366   -12.4841  -265.9838 -1082.3893 -3450.5366]]\n",
      "Weights: [[-4.7049 -0.1343  0.082   0.0401  0.014 ]]\n",
      "MSE loss: 1647.9838\n",
      "Iteration: 2200\n",
      "Gradient: [[   32.3564   -75.4349  -411.4482 -1315.4247 -4908.3436]]\n",
      "Weights: [[-4.7488 -0.1313  0.0854  0.0415  0.0145]]\n",
      "MSE loss: 1571.1958\n",
      "Iteration: 2300\n",
      "Gradient: [[   41.3991  -118.351   -168.5212  -835.0573 -5560.5448]]\n",
      "Weights: [[-4.7842 -0.1288  0.089   0.0429  0.0149]]\n",
      "MSE loss: 1504.2035\n",
      "Iteration: 2400\n",
      "Gradient: [[   17.6908    32.5298  -445.4437 -1329.4647 -4083.169 ]]\n",
      "Weights: [[-4.8229 -0.1259  0.092   0.0441  0.0154]]\n",
      "MSE loss: 1443.6549\n",
      "Iteration: 2500\n",
      "Gradient: [[   34.3093   -77.7252  -177.5783 -1465.3082 -5247.8946]]\n",
      "Weights: [[-4.854  -0.1241  0.0947  0.0453  0.0158]]\n",
      "MSE loss: 1391.4991\n",
      "Iteration: 2600\n",
      "Gradient: [[   28.7652   -51.1966  -307.5152  -457.8441 -3632.7439]]\n",
      "Weights: [[-4.8878 -0.1216  0.0974  0.0465  0.0162]]\n",
      "MSE loss: 1343.1103\n",
      "Iteration: 2700\n",
      "Gradient: [[   19.1027    39.2292   -48.6421  -793.5313 -5212.4101]]\n",
      "Weights: [[-4.9175 -0.1196  0.1     0.0476  0.0166]]\n",
      "MSE loss: 1301.7346\n",
      "Iteration: 2800\n",
      "Gradient: [[   24.2824    21.389   -325.1668  -545.375  -4558.7559]]\n",
      "Weights: [[-4.9444 -0.1192  0.1024  0.0487  0.0169]]\n",
      "MSE loss: 1262.9832\n",
      "Iteration: 2900\n",
      "Gradient: [[   27.2745   -28.527    -38.1134  -924.3383 -2266.1051]]\n",
      "Weights: [[-4.9668 -0.1193  0.1045  0.0498  0.0173]]\n",
      "MSE loss: 1229.0085\n",
      "Iteration: 3000\n",
      "Gradient: [[    8.4598     6.191    -77.5788 -2139.2617 -2946.9097]]\n",
      "Weights: [[-4.9905 -0.1185  0.1064  0.0507  0.0176]]\n",
      "MSE loss: 1199.611\n",
      "Iteration: 3100\n",
      "Gradient: [[   20.2102    46.9829  -203.6299 -1396.7748 -1165.3599]]\n",
      "Weights: [[-5.0115 -0.1182  0.108   0.0516  0.018 ]]\n",
      "MSE loss: 1173.3362\n",
      "Iteration: 3200\n",
      "Gradient: [[    4.7515   -42.2518  -346.5007    71.1381 -4229.0327]]\n",
      "Weights: [[-5.033  -0.1182  0.1098  0.0524  0.0183]]\n",
      "MSE loss: 1148.1821\n",
      "Iteration: 3300\n",
      "Gradient: [[ 1.0095000e+00 -4.0954100e+01 -1.7232100e+01 -4.7976000e+02\n",
      "  -3.7588829e+03]]\n",
      "Weights: [[-5.0508 -0.1189  0.1114  0.0532  0.0186]]\n",
      "MSE loss: 1126.3235\n",
      "Iteration: 3400\n",
      "Gradient: [[-1.4812000e+00 -9.8957100e+01  3.5560000e+01 -1.1123814e+03\n",
      "  -4.8834330e+03]]\n",
      "Weights: [[-5.0658 -0.1197  0.1128  0.054   0.0189]]\n",
      "MSE loss: 1106.9597\n",
      "Iteration: 3500\n",
      "Gradient: [[    9.0153    32.9849  -162.6528  -984.9165 -1037.6948]]\n",
      "Weights: [[-5.0822 -0.1208  0.1141  0.0547  0.0192]]\n",
      "MSE loss: 1088.5639\n",
      "Iteration: 3600\n",
      "Gradient: [[   13.0305    26.3222  -151.5593  -833.7918 -2410.6615]]\n",
      "Weights: [[-5.1004 -0.1224  0.1152  0.0554  0.0195]]\n",
      "MSE loss: 1071.292\n",
      "Iteration: 3700\n",
      "Gradient: [[  -31.4415    -6.2808   -57.1644  -264.4117 -4240.2372]]\n",
      "Weights: [[-5.1129 -0.1241  0.1165  0.0561  0.0197]]\n",
      "MSE loss: 1055.9621\n",
      "Iteration: 3800\n",
      "Gradient: [[    8.2139    73.6441  -219.177   -917.4349 -3566.2054]]\n",
      "Weights: [[-5.1268 -0.1264  0.1177  0.0569  0.02  ]]\n",
      "MSE loss: 1041.2077\n",
      "Iteration: 3900\n",
      "Gradient: [[    7.7516   -28.4684   -35.7374 -1393.3148 -2348.5066]]\n",
      "Weights: [[-5.1358 -0.1286  0.1186  0.0575  0.0203]]\n",
      "MSE loss: 1028.2801\n",
      "Iteration: 4000\n",
      "Gradient: [[ 2.4059000e+00  1.1600310e+02 -2.3115640e+02 -6.8575300e+02\n",
      "  -2.7056361e+03]]\n",
      "Weights: [[-5.1458 -0.1311  0.1195  0.0582  0.0205]]\n",
      "MSE loss: 1016.127\n",
      "Iteration: 4100\n",
      "Gradient: [[   20.2852    10.0139    90.6349  -824.7303 -3325.5583]]\n",
      "Weights: [[-5.1523 -0.1344  0.1202  0.0588  0.0207]]\n",
      "MSE loss: 1005.0328\n",
      "Iteration: 4200\n",
      "Gradient: [[ 3.2870000e-01  2.1185700e+01 -1.5808330e+02 -4.0007660e+02\n",
      "  -1.7060996e+03]]\n",
      "Weights: [[-5.1589 -0.137   0.121   0.0594  0.021 ]]\n",
      "MSE loss: 995.0548\n",
      "Iteration: 4300\n",
      "Gradient: [[  -9.5121   61.9832 -130.9738 -390.2621 -906.9066]]\n",
      "Weights: [[-5.1662 -0.1394  0.1217  0.0599  0.0212]]\n",
      "MSE loss: 985.4665\n",
      "Iteration: 4400\n",
      "Gradient: [[  16.6883    8.4408  -29.2532 -518.0522 -731.1706]]\n",
      "Weights: [[-5.1693 -0.1433  0.1223  0.0604  0.0214]]\n",
      "MSE loss: 976.4278\n",
      "Iteration: 4500\n",
      "Gradient: [[ 9.5290000e-01  7.7357800e+01 -3.0004600e+01 -1.8107480e+02\n",
      "  -1.8117052e+03]]\n",
      "Weights: [[-5.1745 -0.1469  0.1228  0.061   0.0216]]\n",
      "MSE loss: 967.2468\n",
      "Iteration: 4600\n",
      "Gradient: [[  46.7986   68.3122  209.8517 -350.7842 -810.8538]]\n",
      "Weights: [[-5.1801 -0.1501  0.1233  0.0614  0.0218]]\n",
      "MSE loss: 959.1242\n",
      "Iteration: 4700\n",
      "Gradient: [[   11.253     19.1942  -174.9512  -359.8134 -3123.686 ]]\n",
      "Weights: [[-5.1856 -0.1539  0.1238  0.0619  0.022 ]]\n",
      "MSE loss: 951.0754\n",
      "Iteration: 4800\n",
      "Gradient: [[    6.9595   -34.7454    56.3839  -129.8019 -2135.0281]]\n",
      "Weights: [[-5.1871 -0.1567  0.1241  0.0623  0.0222]]\n",
      "MSE loss: 944.2093\n",
      "Iteration: 4900\n",
      "Gradient: [[-1.0044000e+00 -1.0304400e+01 -2.1286620e+02 -7.3746160e+02\n",
      "  -2.6156854e+03]]\n",
      "Weights: [[-5.1883 -0.16    0.1244  0.0627  0.0224]]\n",
      "MSE loss: 937.6246\n",
      "Iteration: 5000\n",
      "Gradient: [[   11.0003    53.644    -15.8079    25.0307 -1757.7493]]\n",
      "Weights: [[-5.1906 -0.1629  0.1247  0.0631  0.0226]]\n",
      "MSE loss: 931.2463\n",
      "Iteration: 5100\n",
      "Gradient: [[   -7.4762    33.1705    99.9027  -346.1588 -1078.9791]]\n",
      "Weights: [[-5.1935 -0.1661  0.1252  0.0635  0.0228]]\n",
      "MSE loss: 924.9102\n",
      "Iteration: 5200\n",
      "Gradient: [[ 1.6072000e+00 -3.9383000e+01 -8.1410600e+01 -3.5150660e+02\n",
      "  -2.8628006e+03]]\n",
      "Weights: [[-5.1981 -0.1698  0.1255  0.0639  0.023 ]]\n",
      "MSE loss: 918.4355\n",
      "Iteration: 5300\n",
      "Gradient: [[   14.503     66.951     15.9993  -178.9588 -2005.7445]]\n",
      "Weights: [[-5.2055 -0.1733  0.1256  0.0643  0.0232]]\n",
      "MSE loss: 912.587\n",
      "Iteration: 5400\n",
      "Gradient: [[  -22.735     27.695    -27.0966  -330.6546 -1186.6281]]\n",
      "Weights: [[-5.203  -0.1772  0.1257  0.0646  0.0233]]\n",
      "MSE loss: 906.6121\n",
      "Iteration: 5500\n",
      "Gradient: [[ -17.8869   60.9355    2.9184 -466.6845 -805.0118]]\n",
      "Weights: [[-5.2039 -0.1809  0.1261  0.065   0.0235]]\n",
      "MSE loss: 900.8678\n",
      "Iteration: 5600\n",
      "Gradient: [[   17.3366    38.1552   -24.5317    51.7232 -2057.4004]]\n",
      "Weights: [[-5.2025 -0.1853  0.1262  0.0654  0.0237]]\n",
      "MSE loss: 895.0993\n",
      "Iteration: 5700\n",
      "Gradient: [[   11.5473    22.229     71.7881   -57.9779 -2057.6443]]\n",
      "Weights: [[-5.2038 -0.1897  0.1264  0.0657  0.0238]]\n",
      "MSE loss: 889.6246\n",
      "Iteration: 5800\n",
      "Gradient: [[    9.9639    89.1715   103.0686   -99.8558 -1026.3572]]\n",
      "Weights: [[-5.2028 -0.1931  0.1266  0.066   0.024 ]]\n",
      "MSE loss: 884.1686\n",
      "Iteration: 5900\n",
      "Gradient: [[-1.2191000e+00  4.6401700e+01  5.0057000e+01 -1.6817850e+02\n",
      "  -1.4557375e+03]]\n",
      "Weights: [[-5.2029 -0.1965  0.1267  0.0664  0.0242]]\n",
      "MSE loss: 879.1614\n",
      "Iteration: 6000\n",
      "Gradient: [[   9.6287   31.5667 -218.9862   41.9757 -735.0888]]\n",
      "Weights: [[-5.2026 -0.2001  0.1269  0.0667  0.0243]]\n",
      "MSE loss: 874.2966\n",
      "Iteration: 6100\n",
      "Gradient: [[  -24.8045   -42.1123    73.12    -378.9008 -2226.2159]]\n",
      "Weights: [[-5.2021 -0.2049  0.1269  0.067   0.0245]]\n",
      "MSE loss: 868.8466\n",
      "Iteration: 6200\n",
      "Gradient: [[ 2.2570000e-01  5.4514800e+01 -6.6358600e+01 -4.1038010e+02\n",
      "  -1.6146676e+03]]\n",
      "Weights: [[-5.2008 -0.2092  0.1269  0.0673  0.0246]]\n",
      "MSE loss: 864.0501\n",
      "Iteration: 6300\n",
      "Gradient: [[   -2.4506    42.3535   -22.0823   234.7518 -1331.3035]]\n",
      "Weights: [[-5.1996 -0.2132  0.1269  0.0676  0.0248]]\n",
      "MSE loss: 859.2719\n",
      "Iteration: 6400\n",
      "Gradient: [[    3.9673    70.7681    31.6906  -577.494  -1199.2122]]\n",
      "Weights: [[-5.1983 -0.2173  0.127   0.0679  0.0249]]\n",
      "MSE loss: 854.445\n",
      "Iteration: 6500\n",
      "Gradient: [[  -12.0766    67.0936   133.6855  -673.4982 -1971.9276]]\n",
      "Weights: [[-5.1952 -0.2218  0.127   0.0682  0.0251]]\n",
      "MSE loss: 849.5347\n",
      "Iteration: 6600\n",
      "Gradient: [[-4.5000000e-03  6.0511400e+01  4.5767700e+01 -2.4530180e+02\n",
      "  -1.6405536e+03]]\n",
      "Weights: [[-5.1952 -0.2263  0.127   0.0685  0.0252]]\n",
      "MSE loss: 844.6006\n",
      "Iteration: 6700\n",
      "Gradient: [[   40.8683    74.6108    12.3002  -269.3777 -2395.5719]]\n",
      "Weights: [[-5.1936 -0.2306  0.1268  0.0687  0.0254]]\n",
      "MSE loss: 839.788\n",
      "Iteration: 6800\n",
      "Gradient: [[ 7.861000e-01  3.598210e+01  5.890710e+01 -8.364110e+01 -9.131925e+02]]\n",
      "Weights: [[-5.1925 -0.2348  0.1269  0.069   0.0255]]\n",
      "MSE loss: 835.3799\n",
      "Iteration: 6900\n",
      "Gradient: [[   -6.3549    66.4049    79.7816   104.1708 -2736.9604]]\n",
      "Weights: [[-5.1902 -0.238   0.1268  0.0693  0.0257]]\n",
      "MSE loss: 831.2057\n",
      "Iteration: 7000\n",
      "Gradient: [[  -1.5578  -30.0943 -163.8748 -274.7904 -765.5644]]\n",
      "Weights: [[-5.1888 -0.242   0.1268  0.0696  0.0258]]\n",
      "MSE loss: 826.7811\n",
      "Iteration: 7100\n",
      "Gradient: [[ -22.5737   32.5056  -14.5788 -724.652  -785.7395]]\n",
      "Weights: [[-5.1852 -0.2459  0.1268  0.0699  0.026 ]]\n",
      "MSE loss: 822.2469\n",
      "Iteration: 7200\n",
      "Gradient: [[    6.7276    37.2888   173.5884  -315.596  -1759.18  ]]\n",
      "Weights: [[-5.1824 -0.2495  0.1267  0.0701  0.0261]]\n",
      "MSE loss: 818.0631\n",
      "Iteration: 7300\n",
      "Gradient: [[ -32.2501   13.8079 -139.8332 -524.1746  -20.6628]]\n",
      "Weights: [[-5.1812 -0.2533  0.1265  0.0704  0.0262]]\n",
      "MSE loss: 813.8438\n",
      "Iteration: 7400\n",
      "Gradient: [[   14.8258    89.7473   177.8235   -23.3489 -1126.2758]]\n",
      "Weights: [[-5.1799 -0.2571  0.1262  0.0706  0.0264]]\n",
      "MSE loss: 809.7065\n",
      "Iteration: 7500\n",
      "Gradient: [[  -20.5139    23.9629     5.6036  -129.445  -1460.7493]]\n",
      "Weights: [[-5.1796 -0.2614  0.126   0.0709  0.0265]]\n",
      "MSE loss: 805.4997\n",
      "Iteration: 7600\n",
      "Gradient: [[   -6.2011    74.6003   -83.9815  -578.5912 -2144.8004]]\n",
      "Weights: [[-5.1773 -0.2657  0.1258  0.0712  0.0266]]\n",
      "MSE loss: 801.1425\n",
      "Iteration: 7700\n",
      "Gradient: [[  -9.8312  -15.5893  160.106  -617.2857 -836.6507]]\n",
      "Weights: [[-5.1724 -0.2701  0.1259  0.0715  0.0268]]\n",
      "MSE loss: 796.7522\n",
      "Iteration: 7800\n",
      "Gradient: [[  -14.9567    46.1359    66.6698  -367.2036 -1886.6541]]\n",
      "Weights: [[-5.1699 -0.2742  0.1259  0.0717  0.0269]]\n",
      "MSE loss: 792.6124\n",
      "Iteration: 7900\n",
      "Gradient: [[-5.616000e-01  2.694420e+01  1.233867e+02 -2.830111e+02 -1.812844e+03]]\n",
      "Weights: [[-5.1685 -0.2779  0.1257  0.0719  0.027 ]]\n",
      "MSE loss: 788.8038\n",
      "Iteration: 8000\n",
      "Gradient: [[  -20.7772     2.6842   256.4808   -63.2102 -1005.5069]]\n",
      "Weights: [[-5.1669 -0.2817  0.1255  0.0722  0.0272]]\n",
      "MSE loss: 784.8225\n",
      "Iteration: 8100\n",
      "Gradient: [[ -34.0788   93.9289  -40.2764 -104.743  -844.5184]]\n",
      "Weights: [[-5.1644 -0.2859  0.1253  0.0723  0.0273]]\n",
      "MSE loss: 780.9035\n",
      "Iteration: 8200\n",
      "Gradient: [[   4.0378   78.1932  129.4226 -400.7025 -669.4851]]\n",
      "Weights: [[-5.161  -0.2893  0.1252  0.0726  0.0274]]\n",
      "MSE loss: 777.0426\n",
      "Iteration: 8300\n",
      "Gradient: [[-5.7760000e-01  6.3056300e+01  1.3503450e+02 -1.0186019e+03\n",
      "  -2.2943650e+02]]\n",
      "Weights: [[-5.1585 -0.2937  0.1249  0.0728  0.0276]]\n",
      "MSE loss: 773.0706\n",
      "Iteration: 8400\n",
      "Gradient: [[ -16.1289   37.5663  180.5001  -79.0424 -836.3229]]\n",
      "Weights: [[-5.1556 -0.2977  0.1247  0.0731  0.0277]]\n",
      "MSE loss: 769.169\n",
      "Iteration: 8500\n",
      "Gradient: [[   9.6142   53.913     3.1997 -490.9499 -689.2077]]\n",
      "Weights: [[-5.1509 -0.3016  0.1246  0.0733  0.0278]]\n",
      "MSE loss: 765.3414\n",
      "Iteration: 8600\n",
      "Gradient: [[   34.1848    21.0196   -34.3866  -218.7099 -1246.243 ]]\n",
      "Weights: [[-5.1504 -0.3052  0.1244  0.0735  0.0279]]\n",
      "MSE loss: 761.8048\n",
      "Iteration: 8700\n",
      "Gradient: [[   -6.5845    30.5844    23.5923   -51.5513 -1029.96  ]]\n",
      "Weights: [[-5.1459 -0.3092  0.1242  0.0737  0.028 ]]\n",
      "MSE loss: 758.0178\n",
      "Iteration: 8800\n",
      "Gradient: [[   6.0429   30.0969   56.7576 -440.1596 -584.6226]]\n",
      "Weights: [[-5.1433 -0.3136  0.124   0.074   0.0282]]\n",
      "MSE loss: 754.0408\n",
      "Iteration: 8900\n",
      "Gradient: [[  -35.5298    89.0624    86.6896  -465.8556 -1470.8027]]\n",
      "Weights: [[-5.1389 -0.3176  0.1238  0.0742  0.0283]]\n",
      "MSE loss: 750.2638\n",
      "Iteration: 9000\n",
      "Gradient: [[-4.3680000e-01  6.4574800e+01 -5.0188700e+01  4.5910500e+01\n",
      "  -2.4747201e+03]]\n",
      "Weights: [[-5.1321 -0.3217  0.1234  0.0744  0.0284]]\n",
      "MSE loss: 746.4083\n",
      "Iteration: 9100\n",
      "Gradient: [[   3.0434   11.3136 -114.4895 -415.9867 -575.3187]]\n",
      "Weights: [[-5.1282 -0.326   0.123   0.0746  0.0285]]\n",
      "MSE loss: 742.6895\n",
      "Iteration: 9200\n",
      "Gradient: [[ -13.1951   -9.2533   -2.1675 -350.6919 -850.7734]]\n",
      "Weights: [[-5.1247 -0.3303  0.1228  0.0748  0.0286]]\n",
      "MSE loss: 738.8732\n",
      "Iteration: 9300\n",
      "Gradient: [[  -10.2913    21.3292    59.1621  -483.3701 -1342.987 ]]\n",
      "Weights: [[-5.1208 -0.3348  0.1227  0.075   0.0287]]\n",
      "MSE loss: 735.094\n",
      "Iteration: 9400\n",
      "Gradient: [[  -12.6531   -38.3975   -15.4534  -117.3789 -1052.8649]]\n",
      "Weights: [[-5.1174 -0.3391  0.1226  0.0752  0.0289]]\n",
      "MSE loss: 731.3333\n",
      "Iteration: 9500\n",
      "Gradient: [[  -37.3868    12.7317  -100.9261  -321.6351 -2037.5091]]\n",
      "Weights: [[-5.1147 -0.3431  0.1225  0.0755  0.029 ]]\n",
      "MSE loss: 727.8367\n",
      "Iteration: 9600\n",
      "Gradient: [[ -13.8183    7.2042   68.5315 -270.6414 -595.0657]]\n",
      "Weights: [[-5.1096 -0.3478  0.1224  0.0757  0.0291]]\n",
      "MSE loss: 723.7785\n",
      "Iteration: 9700\n",
      "Gradient: [[ 1.4558000e+00  4.9860400e+01 -1.1322520e+02 -3.5309600e+02\n",
      "  -1.8309972e+03]]\n",
      "Weights: [[-5.1054 -0.3518  0.1222  0.0759  0.0292]]\n",
      "MSE loss: 720.3764\n",
      "Iteration: 9800\n",
      "Gradient: [[  18.0266   19.0342   91.4434 -269.7632 -591.4477]]\n",
      "Weights: [[-5.1031 -0.3557  0.1219  0.076   0.0293]]\n",
      "MSE loss: 717.0108\n",
      "Iteration: 9900\n",
      "Gradient: [[ 7.5130000e-01  2.4721600e+01 -1.2972910e+02 -8.7301000e+00\n",
      "  -1.7492296e+03]]\n",
      "Weights: [[-5.1006 -0.3596  0.1216  0.0763  0.0294]]\n",
      "MSE loss: 713.4984\n",
      "Iteration: 10000\n",
      "Gradient: [[   -5.8644   103.8353    76.0318  -374.7112 -1856.6388]]\n",
      "Weights: [[-5.0953 -0.3638  0.1215  0.0765  0.0295]]\n",
      "MSE loss: 710.0266\n",
      "Iteration: 10100\n",
      "Gradient: [[   -5.8611     7.649   -116.3299   -85.1688 -1493.1279]]\n",
      "Weights: [[-5.0908 -0.3679  0.1213  0.0767  0.0297]]\n",
      "MSE loss: 706.441\n",
      "Iteration: 10200\n",
      "Gradient: [[   5.3408   41.8501  -39.1275 -593.1557 -684.1111]]\n",
      "Weights: [[-5.0889 -0.3719  0.121   0.0769  0.0298]]\n",
      "MSE loss: 703.0334\n",
      "Iteration: 10300\n",
      "Gradient: [[   -1.9231    68.5437    88.1507  -436.3306 -1238.6479]]\n",
      "Weights: [[-5.0862 -0.3757  0.1209  0.0771  0.0299]]\n",
      "MSE loss: 699.7057\n",
      "Iteration: 10400\n",
      "Gradient: [[  22.2678   41.9136   21.2657 -184.4716 -963.2717]]\n",
      "Weights: [[-5.0823 -0.3791  0.1207  0.0773  0.03  ]]\n",
      "MSE loss: 696.5559\n",
      "Iteration: 10500\n",
      "Gradient: [[  18.615    79.9079  127.8588 -589.3344 -725.4435]]\n",
      "Weights: [[-5.0774 -0.3839  0.1204  0.0775  0.0301]]\n",
      "MSE loss: 692.7898\n",
      "Iteration: 10600\n",
      "Gradient: [[  20.2623   27.5661  -33.0951  160.4777 -422.5906]]\n",
      "Weights: [[-5.0765 -0.3875  0.1202  0.0777  0.0302]]\n",
      "MSE loss: 689.5582\n",
      "Iteration: 10700\n",
      "Gradient: [[  -13.7156    71.1993     7.165   -299.6318 -1147.9509]]\n",
      "Weights: [[-5.0709 -0.3913  0.12    0.0779  0.0303]]\n",
      "MSE loss: 686.081\n",
      "Iteration: 10800\n",
      "Gradient: [[ -15.8274   20.0025   62.4713  -76.2939 -223.5746]]\n",
      "Weights: [[-5.0693 -0.3949  0.1198  0.078   0.0305]]\n",
      "MSE loss: 682.9789\n",
      "Iteration: 10900\n",
      "Gradient: [[  -10.535     96.3025   107.8965  -134.1968 -1889.9144]]\n",
      "Weights: [[-5.067  -0.3989  0.1194  0.0783  0.0306]]\n",
      "MSE loss: 679.4966\n",
      "Iteration: 11000\n",
      "Gradient: [[    2.1303    86.549    136.9167  -365.7366 -1857.3452]]\n",
      "Weights: [[-5.0628 -0.4032  0.1192  0.0784  0.0307]]\n",
      "MSE loss: 676.1387\n",
      "Iteration: 11100\n",
      "Gradient: [[ -17.5296   -9.2063  -59.9497 -458.1511 -666.0149]]\n",
      "Weights: [[-5.0583 -0.407   0.119   0.0786  0.0308]]\n",
      "MSE loss: 672.9107\n",
      "Iteration: 11200\n",
      "Gradient: [[  -35.7291    18.3212  -195.9222    73.2313 -1301.1814]]\n",
      "Weights: [[-5.0552 -0.4112  0.1188  0.0789  0.0309]]\n",
      "MSE loss: 669.5184\n",
      "Iteration: 11300\n",
      "Gradient: [[-2.808650e+01  5.759950e+01 -1.436300e+00 -2.116492e+02 -2.462851e+03]]\n",
      "Weights: [[-5.0535 -0.4153  0.1187  0.0791  0.031 ]]\n",
      "MSE loss: 666.2777\n",
      "Iteration: 11400\n",
      "Gradient: [[  -16.7181    40.673    -37.4042   -20.3816 -1391.952 ]]\n",
      "Weights: [[-5.0483 -0.4185  0.1184  0.0793  0.0311]]\n",
      "MSE loss: 663.0281\n",
      "Iteration: 11500\n",
      "Gradient: [[ -20.0199   -7.7238  -27.0416  -89.7115 -637.3083]]\n",
      "Weights: [[-5.0462 -0.4218  0.1182  0.0795  0.0312]]\n",
      "MSE loss: 660.0167\n",
      "Iteration: 11600\n",
      "Gradient: [[  -7.5252   29.9182  -12.6013 -381.0983  233.2611]]\n",
      "Weights: [[-5.0431 -0.4256  0.118   0.0797  0.0314]]\n",
      "MSE loss: 656.7338\n",
      "Iteration: 11700\n",
      "Gradient: [[  -12.5405    64.9186   103.1     -679.1083 -1045.4142]]\n",
      "Weights: [[-5.041  -0.4303  0.1178  0.0799  0.0315]]\n",
      "MSE loss: 653.2347\n",
      "Iteration: 11800\n",
      "Gradient: [[  -27.9452    52.3359    23.395   -353.2974 -1403.6801]]\n",
      "Weights: [[-5.039  -0.4337  0.1176  0.0801  0.0316]]\n",
      "MSE loss: 650.2678\n",
      "Iteration: 11900\n",
      "Gradient: [[  12.4311   43.686   151.5623 -456.7502 -946.073 ]]\n",
      "Weights: [[-5.0339 -0.4372  0.1174  0.0803  0.0317]]\n",
      "MSE loss: 647.2121\n",
      "Iteration: 12000\n",
      "Gradient: [[ -12.31     58.2678    5.9299 -183.912  -796.8412]]\n",
      "Weights: [[-5.0302 -0.4408  0.1173  0.0805  0.0318]]\n",
      "MSE loss: 644.2623\n",
      "Iteration: 12100\n",
      "Gradient: [[  -5.0816   25.5529  115.5454 -243.496  -407.5259]]\n",
      "Weights: [[-5.0279 -0.4442  0.117   0.0807  0.0319]]\n",
      "MSE loss: 641.3025\n",
      "Iteration: 12200\n",
      "Gradient: [[  -19.9206    46.7974   -75.9255   136.2363 -1485.0606]]\n",
      "Weights: [[-5.0255 -0.4474  0.1168  0.0809  0.032 ]]\n",
      "MSE loss: 638.5286\n",
      "Iteration: 12300\n",
      "Gradient: [[  26.9797   49.285   -31.7513  124.2892 -393.887 ]]\n",
      "Weights: [[-5.0218 -0.4507  0.1165  0.081   0.0321]]\n",
      "MSE loss: 635.7165\n",
      "Iteration: 12400\n",
      "Gradient: [[  -16.8272    67.3985   100.7266  -566.6268 -1740.7305]]\n",
      "Weights: [[-5.0192 -0.4545  0.1163  0.0812  0.0322]]\n",
      "MSE loss: 632.7856\n",
      "Iteration: 12500\n",
      "Gradient: [[  -16.5918    38.1346  -112.8474  -388.5459 -1826.9275]]\n",
      "Weights: [[-5.0147 -0.4581  0.1161  0.0814  0.0323]]\n",
      "MSE loss: 629.8341\n",
      "Iteration: 12600\n",
      "Gradient: [[ 1.4495900e+01  5.8430000e-01 -2.0876300e+01 -2.4553370e+02\n",
      "  -1.0105294e+03]]\n",
      "Weights: [[-5.0131 -0.4613  0.1159  0.0816  0.0324]]\n",
      "MSE loss: 627.1847\n",
      "Iteration: 12700\n",
      "Gradient: [[    2.2238    12.8432   187.5273  -378.8949 -1585.2629]]\n",
      "Weights: [[-5.0098 -0.4649  0.1158  0.0818  0.0325]]\n",
      "MSE loss: 624.1829\n",
      "Iteration: 12800\n",
      "Gradient: [[   -7.0437    59.92      85.44     -56.9952 -1165.1547]]\n",
      "Weights: [[-5.0086 -0.4686  0.1155  0.082   0.0326]]\n",
      "MSE loss: 621.339\n",
      "Iteration: 12900\n",
      "Gradient: [[  -12.0401    62.906    -18.5488  -334.1756 -1467.1613]]\n",
      "Weights: [[-5.0033 -0.4719  0.1152  0.0821  0.0328]]\n",
      "MSE loss: 618.5651\n",
      "Iteration: 13000\n",
      "Gradient: [[    1.9587    33.0807   -94.3944  -389.3911 -1920.2765]]\n",
      "Weights: [[-4.9998 -0.4755  0.1148  0.0823  0.0329]]\n",
      "MSE loss: 615.5865\n",
      "Iteration: 13100\n",
      "Gradient: [[  18.4105    3.2352   -0.7715 -359.0501 -116.0815]]\n",
      "Weights: [[-4.9969 -0.4793  0.1147  0.0825  0.033 ]]\n",
      "MSE loss: 612.7586\n",
      "Iteration: 13200\n",
      "Gradient: [[   -3.4219    49.4651    12.0588  -392.5806 -2200.8243]]\n",
      "Weights: [[-4.9945 -0.4829  0.1145  0.0826  0.0331]]\n",
      "MSE loss: 609.9058\n",
      "Iteration: 13300\n",
      "Gradient: [[    8.359     25.4064    55.9348   -76.7782 -1290.4243]]\n",
      "Weights: [[-4.9895 -0.486   0.1143  0.0828  0.0332]]\n",
      "MSE loss: 607.2907\n",
      "Iteration: 13400\n",
      "Gradient: [[  -21.275    106.6105    67.42    -146.6436 -1847.3421]]\n",
      "Weights: [[-4.9871 -0.4894  0.114   0.083   0.0333]]\n",
      "MSE loss: 604.5921\n",
      "Iteration: 13500\n",
      "Gradient: [[  -34.9995    32.7431    -8.2523  -259.0946 -1629.5033]]\n",
      "Weights: [[-4.9843 -0.493   0.1139  0.0831  0.0334]]\n",
      "MSE loss: 601.8527\n",
      "Iteration: 13600\n",
      "Gradient: [[   25.1919    42.346     44.9191   173.9653 -1600.2628]]\n",
      "Weights: [[-4.9801 -0.497   0.1138  0.0833  0.0335]]\n",
      "MSE loss: 598.8789\n",
      "Iteration: 13700\n",
      "Gradient: [[  -18.1286    39.6559    45.4391  -644.8114 -1477.8474]]\n",
      "Weights: [[-4.9772 -0.5007  0.1135  0.0834  0.0336]]\n",
      "MSE loss: 596.0706\n",
      "Iteration: 13800\n",
      "Gradient: [[  -4.0453   66.6477    4.1683  318.2941 -420.2195]]\n",
      "Weights: [[-4.9751 -0.5041  0.1134  0.0836  0.0337]]\n",
      "MSE loss: 593.4987\n",
      "Iteration: 13900\n",
      "Gradient: [[  -27.6533    80.7979   -51.8678  -355.504  -1063.4854]]\n",
      "Weights: [[-4.9709 -0.5073  0.1131  0.0838  0.0338]]\n",
      "MSE loss: 590.8935\n",
      "Iteration: 14000\n",
      "Gradient: [[ -17.1426   29.835    25.1593 -306.5289 -740.2489]]\n",
      "Weights: [[-4.9666 -0.5108  0.1129  0.0839  0.0339]]\n",
      "MSE loss: 588.2117\n",
      "Iteration: 14100\n",
      "Gradient: [[ 1.232300e+01 -6.251000e-01 -1.961270e+01 -7.261102e+02 -4.771968e+02]]\n",
      "Weights: [[-4.963  -0.5146  0.1127  0.0841  0.034 ]]\n",
      "MSE loss: 585.5304\n",
      "Iteration: 14200\n",
      "Gradient: [[ -25.4913    0.5757   95.1856  383.1568 -534.8202]]\n",
      "Weights: [[-4.9588 -0.518   0.1125  0.0843  0.0341]]\n",
      "MSE loss: 582.9183\n",
      "Iteration: 14300\n",
      "Gradient: [[   1.5949   44.8446  118.6632 -176.3831 -683.1485]]\n",
      "Weights: [[-4.9562 -0.5207  0.1123  0.0844  0.0342]]\n",
      "MSE loss: 580.6795\n",
      "Iteration: 14400\n",
      "Gradient: [[ 2.4294000e+01  9.6574000e+00 -4.6320000e-01  6.0064800e+01\n",
      "  -1.5233212e+03]]\n",
      "Weights: [[-4.9541 -0.5242  0.1121  0.0845  0.0343]]\n",
      "MSE loss: 578.2376\n",
      "Iteration: 14500\n",
      "Gradient: [[  18.6146   13.9147   75.0422 -606.869  -585.707 ]]\n",
      "Weights: [[-4.9533 -0.5273  0.1119  0.0847  0.0344]]\n",
      "MSE loss: 575.8342\n",
      "Iteration: 14600\n",
      "Gradient: [[   5.2434   38.0221   17.2505 -121.7688 -593.0273]]\n",
      "Weights: [[-4.951  -0.5303  0.1117  0.0849  0.0345]]\n",
      "MSE loss: 573.3791\n",
      "Iteration: 14700\n",
      "Gradient: [[ -17.2748  -18.4572   33.8111  -93.8992 -813.1761]]\n",
      "Weights: [[-4.9482 -0.5335  0.1114  0.085   0.0346]]\n",
      "MSE loss: 570.9459\n",
      "Iteration: 14800\n",
      "Gradient: [[   29.2763    -7.7026    12.2833   -71.0046 -1016.5345]]\n",
      "Weights: [[-4.9454 -0.5364  0.1113  0.0852  0.0347]]\n",
      "MSE loss: 568.6176\n",
      "Iteration: 14900\n",
      "Gradient: [[  13.1923  -62.2525  -24.1413 -180.0483 -823.6762]]\n",
      "Weights: [[-4.9444 -0.5395  0.1111  0.0854  0.0348]]\n",
      "MSE loss: 566.238\n",
      "Iteration: 15000\n",
      "Gradient: [[  -5.7399   63.6513   63.0639  121.2098 -272.4098]]\n",
      "Weights: [[-4.9412 -0.5424  0.111   0.0855  0.0349]]\n",
      "MSE loss: 563.9083\n",
      "Iteration: 15100\n",
      "Gradient: [[   0.4427   36.1795   28.8114 -147.7843 -215.9205]]\n",
      "Weights: [[-4.9397 -0.5457  0.1108  0.0857  0.035 ]]\n",
      "MSE loss: 561.4772\n",
      "Iteration: 15200\n",
      "Gradient: [[    5.8216    39.4721   -93.3884  -342.6248 -1268.1173]]\n",
      "Weights: [[-4.9345 -0.5494  0.1106  0.0859  0.0351]]\n",
      "MSE loss: 558.8515\n",
      "Iteration: 15300\n",
      "Gradient: [[  -9.913    75.7802  218.4323   54.2871 -916.5294]]\n",
      "Weights: [[-4.9306 -0.5526  0.1104  0.086   0.0352]]\n",
      "MSE loss: 556.4639\n",
      "Iteration: 15400\n",
      "Gradient: [[ -17.6834   17.0521  101.9461 -154.872  -294.3666]]\n",
      "Weights: [[-4.9264 -0.5559  0.1101  0.0862  0.0353]]\n",
      "MSE loss: 554.0656\n",
      "Iteration: 15500\n",
      "Gradient: [[ 1.3903000e+00  4.3366100e+01  6.1431000e+01 -6.8108800e+01\n",
      "  -2.0313879e+03]]\n",
      "Weights: [[-4.926  -0.559   0.1098  0.0864  0.0354]]\n",
      "MSE loss: 551.8834\n",
      "Iteration: 15600\n",
      "Gradient: [[    8.813      9.9413    48.9294   -44.3499 -1256.6935]]\n",
      "Weights: [[-4.9244 -0.5622  0.1096  0.0865  0.0355]]\n",
      "MSE loss: 549.495\n",
      "Iteration: 15700\n",
      "Gradient: [[  13.8562   25.1     148.1624  -36.051  -580.2178]]\n",
      "Weights: [[-4.9233 -0.5648  0.1093  0.0867  0.0356]]\n",
      "MSE loss: 547.402\n",
      "Iteration: 15800\n",
      "Gradient: [[  -25.4308   -34.7497   -23.7952  -383.5663 -1055.1201]]\n",
      "Weights: [[-4.9175 -0.568   0.109   0.0869  0.0357]]\n",
      "MSE loss: 544.8232\n",
      "Iteration: 15900\n",
      "Gradient: [[ -14.9879   24.5339 -296.1427   65.4382   66.0105]]\n",
      "Weights: [[-4.9131 -0.5712  0.1088  0.0871  0.0357]]\n",
      "MSE loss: 542.415\n",
      "Iteration: 16000\n",
      "Gradient: [[   -7.2248    62.6316    80.5216  -428.7222 -1210.7878]]\n",
      "Weights: [[-4.9113 -0.5749  0.1086  0.0872  0.0358]]\n",
      "MSE loss: 539.9871\n",
      "Iteration: 16100\n",
      "Gradient: [[  -4.537     7.3322  126.8803 -413.5348 -963.0205]]\n",
      "Weights: [[-4.911  -0.5781  0.1085  0.0873  0.0359]]\n",
      "MSE loss: 537.9113\n",
      "Iteration: 16200\n",
      "Gradient: [[ -2.4627 104.5741 -44.707  -14.6852 -76.3912]]\n",
      "Weights: [[-4.9082 -0.5816  0.1083  0.0875  0.036 ]]\n",
      "MSE loss: 535.5361\n",
      "Iteration: 16300\n",
      "Gradient: [[  -8.9193   -2.3024    2.9992 -302.5071 -403.9988]]\n",
      "Weights: [[-4.9045 -0.5848  0.1084  0.0877  0.0361]]\n",
      "MSE loss: 533.29\n",
      "Iteration: 16400\n",
      "Gradient: [[  -13.8037    44.1371  -118.4386   -75.4332 -1609.237 ]]\n",
      "Weights: [[-4.8997 -0.5881  0.1082  0.0879  0.0362]]\n",
      "MSE loss: 530.9589\n",
      "Iteration: 16500\n",
      "Gradient: [[   -2.1737    34.0966    17.9196  -122.7265 -1231.0621]]\n",
      "Weights: [[-4.8974 -0.5911  0.108   0.0881  0.0363]]\n",
      "MSE loss: 528.7783\n",
      "Iteration: 16600\n",
      "Gradient: [[-1.9806000e+00  7.4230000e+00  5.8610000e-01 -2.3976260e+02\n",
      "  -1.2105986e+03]]\n",
      "Weights: [[-4.8946 -0.5943  0.1079  0.0882  0.0364]]\n",
      "MSE loss: 526.54\n",
      "Iteration: 16700\n",
      "Gradient: [[  13.205    33.2625   20.1165  109.9201 -554.849 ]]\n",
      "Weights: [[-4.8912 -0.5968  0.1075  0.0884  0.0365]]\n",
      "MSE loss: 524.435\n",
      "Iteration: 16800\n",
      "Gradient: [[   -8.3567    38.0498   -88.6435   144.5038 -1033.0774]]\n",
      "Weights: [[-4.8878 -0.5999  0.1073  0.0885  0.0366]]\n",
      "MSE loss: 522.3109\n",
      "Iteration: 16900\n",
      "Gradient: [[   13.7435    43.2969   -10.028   -246.4114 -1109.9787]]\n",
      "Weights: [[-4.8853 -0.6029  0.1071  0.0886  0.0367]]\n",
      "MSE loss: 520.2098\n",
      "Iteration: 17000\n",
      "Gradient: [[ -15.7266    8.0318  -33.5279  193.6077 -648.6013]]\n",
      "Weights: [[-4.8807 -0.606   0.1067  0.0888  0.0368]]\n",
      "MSE loss: 518.0134\n",
      "Iteration: 17100\n",
      "Gradient: [[-6.053000e-01  1.324140e+01 -4.889790e+01 -3.371118e+02 -1.452907e+03]]\n",
      "Weights: [[-4.8773 -0.6089  0.1064  0.0889  0.0368]]\n",
      "MSE loss: 515.986\n",
      "Iteration: 17200\n",
      "Gradient: [[   -3.3792    32.7297    68.7769  -321.9596 -1362.7362]]\n",
      "Weights: [[-4.8733 -0.6119  0.1061  0.089   0.0369]]\n",
      "MSE loss: 513.962\n",
      "Iteration: 17300\n",
      "Gradient: [[  -10.6701    12.1025   -65.1838   -67.7867 -1333.2064]]\n",
      "Weights: [[-4.8714 -0.6148  0.1058  0.0892  0.037 ]]\n",
      "MSE loss: 511.8931\n",
      "Iteration: 17400\n",
      "Gradient: [[    6.2467    -5.9947    53.1631   210.4238 -1802.7486]]\n",
      "Weights: [[-4.8678 -0.6181  0.1056  0.0893  0.0371]]\n",
      "MSE loss: 509.7294\n",
      "Iteration: 17500\n",
      "Gradient: [[ 1.487000e-01  3.224380e+01 -6.831660e+01 -8.353960e+01 -9.267819e+02]]\n",
      "Weights: [[-4.865  -0.6207  0.1054  0.0895  0.0372]]\n",
      "MSE loss: 507.7178\n",
      "Iteration: 17600\n",
      "Gradient: [[ -21.4755    5.8392   74.8225  -45.118  -707.359 ]]\n",
      "Weights: [[-4.8643 -0.624   0.1052  0.0896  0.0373]]\n",
      "MSE loss: 505.7274\n",
      "Iteration: 17700\n",
      "Gradient: [[   5.9352   19.7043 -181.358    17.7605 -796.1089]]\n",
      "Weights: [[-4.8596 -0.6274  0.105   0.0898  0.0374]]\n",
      "MSE loss: 503.5285\n",
      "Iteration: 17800\n",
      "Gradient: [[   0.7792   87.8047   -7.4727  245.9928 -770.7721]]\n",
      "Weights: [[-4.858  -0.6306  0.1049  0.0899  0.0375]]\n",
      "MSE loss: 501.5009\n",
      "Iteration: 17900\n",
      "Gradient: [[  -13.2009    -8.5315   -50.5906  -233.5205 -1581.8479]]\n",
      "Weights: [[-4.8546 -0.6335  0.1046  0.0901  0.0375]]\n",
      "MSE loss: 499.4344\n",
      "Iteration: 18000\n",
      "Gradient: [[  10.898     8.5414   30.052  -612.412  -388.8255]]\n",
      "Weights: [[-4.8523 -0.6361  0.1044  0.0902  0.0376]]\n",
      "MSE loss: 497.6551\n",
      "Iteration: 18100\n",
      "Gradient: [[  14.054    42.4683   -2.254  -481.0106 -676.6958]]\n",
      "Weights: [[-4.8508 -0.6391  0.1041  0.0903  0.0377]]\n",
      "MSE loss: 495.7055\n",
      "Iteration: 18200\n",
      "Gradient: [[  -9.5667   41.2102  -20.4983 -221.8417 -458.2305]]\n",
      "Weights: [[-4.8483 -0.642   0.104   0.0904  0.0378]]\n",
      "MSE loss: 493.7869\n",
      "Iteration: 18300\n",
      "Gradient: [[  3.9781  47.1587   8.5019 223.8885 249.8898]]\n",
      "Weights: [[-4.8445 -0.6453  0.1039  0.0906  0.0379]]\n",
      "MSE loss: 491.7073\n",
      "Iteration: 18400\n",
      "Gradient: [[ 4.680000e-01 -2.434900e+00  2.557480e+01  1.407500e+00 -5.463221e+02]]\n",
      "Weights: [[-4.8419 -0.6483  0.1037  0.0907  0.038 ]]\n",
      "MSE loss: 489.7914\n",
      "Iteration: 18500\n",
      "Gradient: [[  -5.3725   15.6528   69.7285  144.867  -304.509 ]]\n",
      "Weights: [[-4.841  -0.6514  0.1036  0.0909  0.0381]]\n",
      "MSE loss: 487.8257\n",
      "Iteration: 18600\n",
      "Gradient: [[ -16.8671   -4.7567   35.781  -103.7093 -677.2729]]\n",
      "Weights: [[-4.837  -0.6543  0.1034  0.091   0.0382]]\n",
      "MSE loss: 485.8933\n",
      "Iteration: 18700\n",
      "Gradient: [[ -15.8506   29.9538   80.3683 -219.7292 -694.4901]]\n",
      "Weights: [[-4.8336 -0.6569  0.1031  0.0911  0.0382]]\n",
      "MSE loss: 484.0056\n",
      "Iteration: 18800\n",
      "Gradient: [[ -15.2578   32.8719  -24.9773  -65.5479 -124.1025]]\n",
      "Weights: [[-4.8316 -0.6597  0.1029  0.0913  0.0383]]\n",
      "MSE loss: 482.1469\n",
      "Iteration: 18900\n",
      "Gradient: [[   4.9009    1.4361  -37.7973 -191.4385 -818.2358]]\n",
      "Weights: [[-4.8302 -0.6625  0.1028  0.0914  0.0384]]\n",
      "MSE loss: 480.3467\n",
      "Iteration: 19000\n",
      "Gradient: [[   -4.1923   -13.4958    26.8445    85.8714 -2168.7748]]\n",
      "Weights: [[-4.8268 -0.6656  0.1026  0.0915  0.0385]]\n",
      "MSE loss: 478.4229\n",
      "Iteration: 19100\n",
      "Gradient: [[   -1.7292    26.4435    75.1197  -429.4033 -1359.415 ]]\n",
      "Weights: [[-4.8229 -0.6685  0.1024  0.0917  0.0386]]\n",
      "MSE loss: 476.5295\n",
      "Iteration: 19200\n",
      "Gradient: [[   1.2016   33.5829  110.2001  -39.1648 -240.6401]]\n",
      "Weights: [[-4.818  -0.6718  0.1023  0.0918  0.0387]]\n",
      "MSE loss: 474.5209\n",
      "Iteration: 19300\n",
      "Gradient: [[  -2.024    18.9946 -122.5196 -225.0668 -371.1385]]\n",
      "Weights: [[-4.8147 -0.6745  0.1019  0.092   0.0388]]\n",
      "MSE loss: 472.7084\n",
      "Iteration: 19400\n",
      "Gradient: [[   6.5614   75.5173   41.5519 -110.7562 -318.2308]]\n",
      "Weights: [[-4.8102 -0.6772  0.1017  0.0921  0.0388]]\n",
      "MSE loss: 470.9181\n",
      "Iteration: 19500\n",
      "Gradient: [[  -5.0256   76.8548   36.6884 -251.7533 -767.892 ]]\n",
      "Weights: [[-4.8072 -0.6803  0.1016  0.0922  0.0389]]\n",
      "MSE loss: 469.0785\n",
      "Iteration: 19600\n",
      "Gradient: [[  -10.8059    66.6054    85.734   -153.4666 -1102.173 ]]\n",
      "Weights: [[-4.8054 -0.6833  0.1014  0.0924  0.039 ]]\n",
      "MSE loss: 467.1628\n",
      "Iteration: 19700\n",
      "Gradient: [[   -4.1753    32.4311    35.1637  -205.5602 -1883.2229]]\n",
      "Weights: [[-4.8032 -0.6862  0.1012  0.0925  0.0391]]\n",
      "MSE loss: 465.3221\n",
      "Iteration: 19800\n",
      "Gradient: [[ -11.3834   50.5461   40.2985  175.0247 -843.5718]]\n",
      "Weights: [[-4.8007 -0.6889  0.101   0.0927  0.0392]]\n",
      "MSE loss: 463.5573\n",
      "Iteration: 19900\n",
      "Gradient: [[ 1.2815600e+01 -1.0866200e+01 -5.9090000e-01  5.1871300e+01\n",
      "  -1.7902249e+03]]\n",
      "Weights: [[-4.8012 -0.692   0.1007  0.0928  0.0393]]\n",
      "MSE loss: 461.6747\n",
      "Iteration: 20000\n",
      "Gradient: [[  -6.1201   23.2662  -11.3258   10.6391 -191.7112]]\n",
      "Weights: [[-4.7982 -0.6946  0.1006  0.093   0.0394]]\n",
      "MSE loss: 459.9548\n",
      "Iteration: 20100\n",
      "Gradient: [[  8.0512  27.925   27.6923  37.2345 170.0883]]\n",
      "Weights: [[-4.7919 -0.6976  0.1003  0.0931  0.0394]]\n",
      "MSE loss: 458.1649\n",
      "Iteration: 20200\n",
      "Gradient: [[ -11.2898  -12.9737  -61.1002  -63.7859 -893.4892]]\n",
      "Weights: [[-4.7914 -0.7     0.1001  0.0932  0.0395]]\n",
      "MSE loss: 456.5967\n",
      "Iteration: 20300\n",
      "Gradient: [[   6.9821   36.7429  -20.7592 -533.1331 -872.8494]]\n",
      "Weights: [[-4.7901 -0.7027  0.0999  0.0933  0.0396]]\n",
      "MSE loss: 454.9056\n",
      "Iteration: 20400\n",
      "Gradient: [[   -3.5668    59.1673    52.0613  -141.0199 -1659.1492]]\n",
      "Weights: [[-4.7891 -0.7051  0.0998  0.0935  0.0397]]\n",
      "MSE loss: 453.2939\n",
      "Iteration: 20500\n",
      "Gradient: [[  -6.2472   29.6677  -63.4495 -112.6366 -553.1503]]\n",
      "Weights: [[-4.7862 -0.7079  0.0996  0.0937  0.0398]]\n",
      "MSE loss: 451.5156\n",
      "Iteration: 20600\n",
      "Gradient: [[    5.3958     4.0314    69.5386  -112.6042 -1165.8396]]\n",
      "Weights: [[-4.7834 -0.7106  0.0993  0.0938  0.0399]]\n",
      "MSE loss: 449.7287\n",
      "Iteration: 20700\n",
      "Gradient: [[ -17.111    53.3185   47.1227 -166.5972 -802.9828]]\n",
      "Weights: [[-4.7814 -0.7132  0.0992  0.094   0.0399]]\n",
      "MSE loss: 448.1059\n",
      "Iteration: 20800\n",
      "Gradient: [[  -6.984    61.204   108.1684  -89.7002 -910.6091]]\n",
      "Weights: [[-4.7779 -0.7156  0.099   0.0941  0.04  ]]\n",
      "MSE loss: 446.4797\n",
      "Iteration: 20900\n",
      "Gradient: [[  12.0835   19.8729  221.2878 -265.9125 -839.0394]]\n",
      "Weights: [[-4.7756 -0.7183  0.0988  0.0943  0.0401]]\n",
      "MSE loss: 444.8184\n",
      "Iteration: 21000\n",
      "Gradient: [[   0.8384   45.438   -31.3962  218.793  -553.0235]]\n",
      "Weights: [[-4.7732 -0.7213  0.0986  0.0944  0.0402]]\n",
      "MSE loss: 443.0502\n",
      "Iteration: 21100\n",
      "Gradient: [[ -15.3917   -3.5377   -8.939  -202.6238 -322.8724]]\n",
      "Weights: [[-4.7719 -0.7242  0.0984  0.0946  0.0403]]\n",
      "MSE loss: 441.4175\n",
      "Iteration: 21200\n",
      "Gradient: [[ -24.0353  -14.7627  -40.6512   38.1189 -983.7258]]\n",
      "Weights: [[-4.7692 -0.727   0.0983  0.0947  0.0403]]\n",
      "MSE loss: 439.8567\n",
      "Iteration: 21300\n",
      "Gradient: [[  10.8881   45.6585   82.9781  -30.7373 -253.7986]]\n",
      "Weights: [[-4.7645 -0.7297  0.0981  0.0948  0.0404]]\n",
      "MSE loss: 438.1513\n",
      "Iteration: 21400\n",
      "Gradient: [[ -17.5258   52.3252   61.5855 -185.5518 -162.7525]]\n",
      "Weights: [[-4.7618 -0.7325  0.0979  0.0949  0.0405]]\n",
      "MSE loss: 436.3968\n",
      "Iteration: 21500\n",
      "Gradient: [[  21.1185   26.479     3.6058 -609.0477 -923.0085]]\n",
      "Weights: [[-4.7595 -0.7355  0.0976  0.0951  0.0406]]\n",
      "MSE loss: 434.7053\n",
      "Iteration: 21600\n",
      "Gradient: [[   -1.8905    22.3546   113.6154    61.2312 -1452.0478]]\n",
      "Weights: [[-4.7565 -0.7382  0.0974  0.0952  0.0407]]\n",
      "MSE loss: 433.1566\n",
      "Iteration: 21700\n",
      "Gradient: [[  44.788    47.5757  -35.0791 -596.388  -760.5793]]\n",
      "Weights: [[-4.7538 -0.741   0.0971  0.0954  0.0407]]\n",
      "MSE loss: 431.4988\n",
      "Iteration: 21800\n",
      "Gradient: [[   14.3405    20.0961    -3.1559  -209.5858 -1444.8327]]\n",
      "Weights: [[-4.7512 -0.7435  0.0969  0.0955  0.0408]]\n",
      "MSE loss: 430.057\n",
      "Iteration: 21900\n",
      "Gradient: [[  -7.7198   43.0275  114.3201  -69.7209 -157.8155]]\n",
      "Weights: [[-4.7492 -0.7462  0.0967  0.0957  0.0409]]\n",
      "MSE loss: 428.524\n",
      "Iteration: 22000\n",
      "Gradient: [[   16.9536   -18.7463    72.0419  -422.8348 -1055.0891]]\n",
      "Weights: [[-4.747  -0.7487  0.0965  0.0958  0.041 ]]\n",
      "MSE loss: 427.0375\n",
      "Iteration: 22100\n",
      "Gradient: [[  11.3947   75.9273    9.3575 -100.4101 -128.7583]]\n",
      "Weights: [[-4.7441 -0.7513  0.0963  0.0959  0.041 ]]\n",
      "MSE loss: 425.4629\n",
      "Iteration: 22200\n",
      "Gradient: [[ -22.2478   13.4256  174.5719 -198.731    73.5765]]\n",
      "Weights: [[-4.7412 -0.7545  0.0961  0.0961  0.0411]]\n",
      "MSE loss: 423.7626\n",
      "Iteration: 22300\n",
      "Gradient: [[-19.7487  19.7481  -5.3058 246.4871 169.3971]]\n",
      "Weights: [[-4.739  -0.7573  0.0959  0.0962  0.0412]]\n",
      "MSE loss: 422.1609\n",
      "Iteration: 22400\n",
      "Gradient: [[ 1.425400e+00  3.035600e+00  1.172620e+01  2.745170e+01 -1.623998e+03]]\n",
      "Weights: [[-4.7366 -0.7606  0.0958  0.0964  0.0413]]\n",
      "MSE loss: 420.4247\n",
      "Iteration: 22500\n",
      "Gradient: [[   13.0404    66.6839   -94.8675    37.8256 -1086.0677]]\n",
      "Weights: [[-4.7356 -0.7632  0.0957  0.0965  0.0414]]\n",
      "MSE loss: 418.9471\n",
      "Iteration: 22600\n",
      "Gradient: [[ 9.1940000e-01 -2.1817100e+01  6.2530800e+01 -2.5676300e+02\n",
      "  -1.5085361e+03]]\n",
      "Weights: [[-4.7328 -0.7653  0.0955  0.0966  0.0414]]\n",
      "MSE loss: 417.5364\n",
      "Iteration: 22700\n",
      "Gradient: [[  13.8892   10.227    64.1022 -145.8938 -766.3502]]\n",
      "Weights: [[-4.7299 -0.7679  0.0954  0.0967  0.0415]]\n",
      "MSE loss: 416.0775\n",
      "Iteration: 22800\n",
      "Gradient: [[-1.1739400e+01 -7.5490000e-01  4.3297900e+01 -2.9237060e+02\n",
      "  -1.5117357e+03]]\n",
      "Weights: [[-4.7292 -0.7705  0.0953  0.0968  0.0416]]\n",
      "MSE loss: 414.7397\n",
      "Iteration: 22900\n",
      "Gradient: [[   19.6761   -41.7491     4.0696  -207.2247 -1136.9797]]\n",
      "Weights: [[-4.7266 -0.7729  0.0951  0.097   0.0417]]\n",
      "MSE loss: 413.2627\n",
      "Iteration: 23000\n",
      "Gradient: [[  16.3906   29.8147   14.928   191.9606 -238.3904]]\n",
      "Weights: [[-4.7237 -0.7753  0.095   0.0971  0.0417]]\n",
      "MSE loss: 411.8476\n",
      "Iteration: 23100\n",
      "Gradient: [[ -12.9924  -34.8594   93.872     1.0198 -739.3484]]\n",
      "Weights: [[-4.7215 -0.7775  0.0948  0.0972  0.0418]]\n",
      "MSE loss: 410.5595\n",
      "Iteration: 23200\n",
      "Gradient: [[-4.333000e-01  2.153970e+01  4.113000e-01  4.167970e+01 -4.310609e+02]]\n",
      "Weights: [[-4.7189 -0.7796  0.0947  0.0973  0.0419]]\n",
      "MSE loss: 409.283\n",
      "Iteration: 23300\n",
      "Gradient: [[ -21.5122   73.2325  -20.3966 -146.5177  -36.2013]]\n",
      "Weights: [[-4.7175 -0.7819  0.0945  0.0974  0.042 ]]\n",
      "MSE loss: 407.9462\n",
      "Iteration: 23400\n",
      "Gradient: [[   1.3335   31.8717   70.4738  131.2581 -354.9598]]\n",
      "Weights: [[-4.7158 -0.7844  0.0942  0.0975  0.042 ]]\n",
      "MSE loss: 406.5693\n",
      "Iteration: 23500\n",
      "Gradient: [[   19.8544    48.4169    18.3719  -312.634  -1075.3242]]\n",
      "Weights: [[-4.7134 -0.7866  0.094   0.0976  0.0421]]\n",
      "MSE loss: 405.2227\n",
      "Iteration: 23600\n",
      "Gradient: [[  -0.6957   15.0841    3.0901  -23.6692 -689.5302]]\n",
      "Weights: [[-4.7112 -0.7892  0.0938  0.0978  0.0422]]\n",
      "MSE loss: 403.8155\n",
      "Iteration: 23700\n",
      "Gradient: [[  -4.211     8.0931  -70.2739 -301.2199  -86.5213]]\n",
      "Weights: [[-4.7101 -0.7913  0.0936  0.0979  0.0423]]\n",
      "MSE loss: 402.5712\n",
      "Iteration: 23800\n",
      "Gradient: [[   7.0763   51.2409 -180.0199  -88.2914 -795.1433]]\n",
      "Weights: [[-4.7072 -0.7935  0.0933  0.098   0.0423]]\n",
      "MSE loss: 401.2232\n",
      "Iteration: 23900\n",
      "Gradient: [[  13.7535  -21.5168   39.8935 -310.4045 -166.2452]]\n",
      "Weights: [[-4.7048 -0.7964  0.0932  0.0982  0.0424]]\n",
      "MSE loss: 399.6943\n",
      "Iteration: 24000\n",
      "Gradient: [[  -7.768    18.6507   14.2869  -18.856  -583.6767]]\n",
      "Weights: [[-4.7035 -0.7986  0.0931  0.0983  0.0425]]\n",
      "MSE loss: 398.4008\n",
      "Iteration: 24100\n",
      "Gradient: [[   -8.9408    56.5634   212.1359  -221.7656 -1131.32  ]]\n",
      "Weights: [[-4.7011 -0.801   0.0928  0.0984  0.0426]]\n",
      "MSE loss: 397.027\n",
      "Iteration: 24200\n",
      "Gradient: [[  14.9611   50.4517  -72.4171  -78.9064 -284.0825]]\n",
      "Weights: [[-4.6975 -0.8033  0.0927  0.0985  0.0427]]\n",
      "MSE loss: 395.7743\n",
      "Iteration: 24300\n",
      "Gradient: [[   -9.7277    22.1834   108.4347  -187.595  -1272.5299]]\n",
      "Weights: [[-4.6946 -0.8057  0.0925  0.0986  0.0427]]\n",
      "MSE loss: 394.4581\n",
      "Iteration: 24400\n",
      "Gradient: [[  -13.7559    21.7779    60.4517  -200.4965 -1004.0803]]\n",
      "Weights: [[-4.6925 -0.8081  0.0923  0.0987  0.0428]]\n",
      "MSE loss: 393.1797\n",
      "Iteration: 24500\n",
      "Gradient: [[  -4.7144  -33.2377   48.8653  -28.5444 -830.6185]]\n",
      "Weights: [[-4.6922 -0.8103  0.092   0.0989  0.0429]]\n",
      "MSE loss: 391.9329\n",
      "Iteration: 24600\n",
      "Gradient: [[  -19.3221     5.2147   -53.9242  -201.2273 -1450.0991]]\n",
      "Weights: [[-4.6866 -0.8125  0.0919  0.0989  0.0429]]\n",
      "MSE loss: 390.6889\n",
      "Iteration: 24700\n",
      "Gradient: [[  -10.2161    18.252     -2.2571   -17.5223 -1138.5075]]\n",
      "Weights: [[-4.6838 -0.8149  0.0918  0.0991  0.043 ]]\n",
      "MSE loss: 389.4426\n",
      "Iteration: 24800\n",
      "Gradient: [[    4.691      1.9839    60.3662  -221.239  -1588.157 ]]\n",
      "Weights: [[-4.6832 -0.8173  0.0915  0.0992  0.0431]]\n",
      "MSE loss: 388.2272\n",
      "Iteration: 24900\n",
      "Gradient: [[  -8.3412   33.8396   -7.2102 -116.8246 -312.798 ]]\n",
      "Weights: [[-4.6817 -0.8195  0.0914  0.0992  0.0432]]\n",
      "MSE loss: 387.0237\n",
      "Iteration: 25000\n",
      "Gradient: [[ -9.3368 -61.4302  63.6262  -3.2005   5.1785]]\n",
      "Weights: [[-4.6787 -0.822   0.0912  0.0994  0.0432]]\n",
      "MSE loss: 385.6774\n",
      "Iteration: 25100\n",
      "Gradient: [[  -2.2251  -13.2759   31.9289 -341.4015  243.4071]]\n",
      "Weights: [[-4.6771 -0.8244  0.0909  0.0995  0.0433]]\n",
      "MSE loss: 384.4426\n",
      "Iteration: 25200\n",
      "Gradient: [[  11.253    24.9232    5.0939 -114.7971 -916.886 ]]\n",
      "Weights: [[-4.6746 -0.8267  0.0907  0.0996  0.0434]]\n",
      "MSE loss: 383.2238\n",
      "Iteration: 25300\n",
      "Gradient: [[ -13.8593   28.197    26.1232 -254.2482 -896.9066]]\n",
      "Weights: [[-4.6741 -0.8287  0.0905  0.0998  0.0434]]\n",
      "MSE loss: 382.0372\n",
      "Iteration: 25400\n",
      "Gradient: [[  -24.4686    19.8502    94.433     33.95   -1266.617 ]]\n",
      "Weights: [[-4.6704 -0.8315  0.0903  0.0999  0.0435]]\n",
      "MSE loss: 380.6894\n",
      "Iteration: 25500\n",
      "Gradient: [[   9.8342    6.3674   93.0538 -456.1568 -367.8354]]\n",
      "Weights: [[-4.6686 -0.8337  0.0901  0.1     0.0436]]\n",
      "MSE loss: 379.5488\n",
      "Iteration: 25600\n",
      "Gradient: [[  -2.0386   31.9881   -7.5573 -106.3355 -505.8523]]\n",
      "Weights: [[-4.6669 -0.8361  0.0898  0.1001  0.0436]]\n",
      "MSE loss: 378.3354\n",
      "Iteration: 25700\n",
      "Gradient: [[  14.1516    3.6595   63.8904 -528.7303 -814.5365]]\n",
      "Weights: [[-4.6663 -0.8381  0.0897  0.1002  0.0437]]\n",
      "MSE loss: 377.2855\n",
      "Iteration: 25800\n",
      "Gradient: [[   4.8358   65.4122  -59.9695 -114.533  -761.5649]]\n",
      "Weights: [[-4.6655 -0.84    0.0894  0.1003  0.0438]]\n",
      "MSE loss: 376.1546\n",
      "Iteration: 25900\n",
      "Gradient: [[   2.2095   25.3122   83.1607  -60.7663 -830.9278]]\n",
      "Weights: [[-4.6636 -0.8419  0.0893  0.1005  0.0439]]\n",
      "MSE loss: 375.0437\n",
      "Iteration: 26000\n",
      "Gradient: [[   4.985    -2.6612   85.9363  229.4609 -527.6959]]\n",
      "Weights: [[-4.6626 -0.8439  0.0891  0.1006  0.0439]]\n",
      "MSE loss: 373.9792\n",
      "Iteration: 26100\n",
      "Gradient: [[-20.6025  27.201   58.8398 136.7167 -55.8687]]\n",
      "Weights: [[-4.6605 -0.8459  0.0889  0.1007  0.044 ]]\n",
      "MSE loss: 372.8922\n",
      "Iteration: 26200\n",
      "Gradient: [[ -29.1939   57.0765  -46.6902  -89.0827 -263.6307]]\n",
      "Weights: [[-4.6574 -0.8479  0.0887  0.1008  0.044 ]]\n",
      "MSE loss: 371.8144\n",
      "Iteration: 26300\n",
      "Gradient: [[  -6.1381   11.8414  -35.4004 -458.4898  100.5223]]\n",
      "Weights: [[-4.6539 -0.8498  0.0886  0.1009  0.0441]]\n",
      "MSE loss: 370.7298\n",
      "Iteration: 26400\n",
      "Gradient: [[    2.8651    30.3577   167.0982   -11.7081 -1116.7961]]\n",
      "Weights: [[-4.6516 -0.852   0.0884  0.101   0.0442]]\n",
      "MSE loss: 369.6692\n",
      "Iteration: 26500\n",
      "Gradient: [[ -3.5543   0.5538 -22.8144  27.3937 132.2949]]\n",
      "Weights: [[-4.6479 -0.8544  0.0881  0.1011  0.0442]]\n",
      "MSE loss: 368.4821\n",
      "Iteration: 26600\n",
      "Gradient: [[  15.3347   41.4363  -39.7201   -1.5108 -762.3152]]\n",
      "Weights: [[-4.6472 -0.8565  0.0879  0.1012  0.0443]]\n",
      "MSE loss: 367.4356\n",
      "Iteration: 26700\n",
      "Gradient: [[  -1.3367   31.4533  152.3774  -62.3112 -748.7604]]\n",
      "Weights: [[-4.645  -0.8583  0.0877  0.1013  0.0444]]\n",
      "MSE loss: 366.4515\n",
      "Iteration: 26800\n",
      "Gradient: [[   9.3112   18.3709   -6.1351 -213.6442 -562.8065]]\n",
      "Weights: [[-4.6441 -0.8601  0.0876  0.1014  0.0444]]\n",
      "MSE loss: 365.4025\n",
      "Iteration: 26900\n",
      "Gradient: [[  -7.1079   32.1485  -56.5267 -286.9585 -331.0824]]\n",
      "Weights: [[-4.6426 -0.8622  0.0875  0.1015  0.0445]]\n",
      "MSE loss: 364.3423\n",
      "Iteration: 27000\n",
      "Gradient: [[ -12.254    -1.9237   39.6015   98.4528 -404.4964]]\n",
      "Weights: [[-4.6403 -0.8643  0.0872  0.1016  0.0446]]\n",
      "MSE loss: 363.296\n",
      "Iteration: 27100\n",
      "Gradient: [[  18.2049   12.6163  126.7516  -90.5722 -815.0323]]\n",
      "Weights: [[-4.6394 -0.8665  0.0869  0.1017  0.0446]]\n",
      "MSE loss: 362.2659\n",
      "Iteration: 27200\n",
      "Gradient: [[  11.4315   33.4036  -53.4962 -349.8768 -551.0731]]\n",
      "Weights: [[-4.6382 -0.8682  0.0868  0.1018  0.0447]]\n",
      "MSE loss: 361.2862\n",
      "Iteration: 27300\n",
      "Gradient: [[  19.246    50.6633  -33.4483 -168.5475 -654.9282]]\n",
      "Weights: [[-4.6362 -0.87    0.0867  0.102   0.0448]]\n",
      "MSE loss: 360.2417\n",
      "Iteration: 27400\n",
      "Gradient: [[ 10.7183  43.1422  99.3568  48.9671 -61.9543]]\n",
      "Weights: [[-4.6347 -0.8721  0.0866  0.1021  0.0448]]\n",
      "MSE loss: 359.1985\n",
      "Iteration: 27500\n",
      "Gradient: [[  -4.505    28.5915  -51.1207  107.7423 -220.5798]]\n",
      "Weights: [[-4.6319 -0.8744  0.0864  0.1022  0.0449]]\n",
      "MSE loss: 358.1883\n",
      "Iteration: 27600\n",
      "Gradient: [[  21.805    -8.3935   96.2135 -242.4047  155.8758]]\n",
      "Weights: [[-4.6293 -0.8765  0.0862  0.1023  0.045 ]]\n",
      "MSE loss: 357.145\n",
      "Iteration: 27700\n",
      "Gradient: [[   9.285    54.0195  -57.2562   40.168  -734.184 ]]\n",
      "Weights: [[-4.6277 -0.8785  0.0861  0.1024  0.045 ]]\n",
      "MSE loss: 356.1768\n",
      "Iteration: 27800\n",
      "Gradient: [[   -4.0781    -3.1159    -8.5101  -233.5804 -1059.8254]]\n",
      "Weights: [[-4.6259 -0.8803  0.086   0.1025  0.0451]]\n",
      "MSE loss: 355.312\n",
      "Iteration: 27900\n",
      "Gradient: [[  14.7262   28.2451  -12.3945 -450.6542 -451.9071]]\n",
      "Weights: [[-4.6225 -0.8823  0.086   0.1026  0.0451]]\n",
      "MSE loss: 354.3012\n",
      "Iteration: 28000\n",
      "Gradient: [[  7.624   -8.4336 -32.7996 -62.2042 -25.1   ]]\n",
      "Weights: [[-4.6223 -0.8847  0.0859  0.1027  0.0452]]\n",
      "MSE loss: 353.2215\n",
      "Iteration: 28100\n",
      "Gradient: [[    7.4868    -8.9194   -75.6359  -113.2307 -1249.2678]]\n",
      "Weights: [[-4.6213 -0.887   0.0857  0.1028  0.0453]]\n",
      "MSE loss: 352.1996\n",
      "Iteration: 28200\n",
      "Gradient: [[-8.4470000e-01  5.8397000e+00  7.9335000e+00 -1.9859660e+02\n",
      "  -1.1200489e+03]]\n",
      "Weights: [[-4.6198 -0.889   0.0855  0.1028  0.0453]]\n",
      "MSE loss: 351.2223\n",
      "Iteration: 28300\n",
      "Gradient: [[   2.0407   -3.0318   45.1428  -76.57   -651.9849]]\n",
      "Weights: [[-4.6167 -0.8912  0.0854  0.1029  0.0454]]\n",
      "MSE loss: 350.1922\n",
      "Iteration: 28400\n",
      "Gradient: [[ -12.4913   35.1647   28.8677  -47.1373 -934.5794]]\n",
      "Weights: [[-4.6164 -0.8933  0.0852  0.103   0.0455]]\n",
      "MSE loss: 349.2759\n",
      "Iteration: 28500\n",
      "Gradient: [[   0.5234   35.274    42.9613 -153.8041  -64.7395]]\n",
      "Weights: [[-4.6149 -0.8949  0.085   0.1031  0.0455]]\n",
      "MSE loss: 348.3621\n",
      "Iteration: 28600\n",
      "Gradient: [[   4.1821   14.821   167.8295  119.3774 -426.5818]]\n",
      "Weights: [[-4.6131 -0.8967  0.0849  0.1032  0.0456]]\n",
      "MSE loss: 347.4427\n",
      "Iteration: 28700\n",
      "Gradient: [[-31.7624   6.589   34.9207 140.3217  95.6998]]\n",
      "Weights: [[-4.6102 -0.8986  0.0847  0.1033  0.0457]]\n",
      "MSE loss: 346.4618\n",
      "Iteration: 28800\n",
      "Gradient: [[  15.7092   -5.2378   63.2507 -394.3878 -542.265 ]]\n",
      "Weights: [[-4.6081 -0.9006  0.0845  0.1034  0.0457]]\n",
      "MSE loss: 345.4684\n",
      "Iteration: 28900\n",
      "Gradient: [[ -15.1592   -1.2152  152.978  -170.1855 -593.1453]]\n",
      "Weights: [[-4.6051 -0.9024  0.0844  0.1035  0.0458]]\n",
      "MSE loss: 344.5701\n",
      "Iteration: 29000\n",
      "Gradient: [[   -6.8378    15.3007    15.0337   -25.9159 -1167.0746]]\n",
      "Weights: [[-4.6042 -0.9044  0.0842  0.1036  0.0459]]\n",
      "MSE loss: 343.6541\n",
      "Iteration: 29100\n",
      "Gradient: [[  -4.0499   35.0303   59.84   -274.2438 -337.0416]]\n",
      "Weights: [[-4.6002 -0.9065  0.084   0.1037  0.0459]]\n",
      "MSE loss: 342.7048\n",
      "Iteration: 29200\n",
      "Gradient: [[    8.6214    28.5464    21.3893  -428.4297 -1233.8699]]\n",
      "Weights: [[-4.5989 -0.9084  0.0838  0.1038  0.046 ]]\n",
      "MSE loss: 341.782\n",
      "Iteration: 29300\n",
      "Gradient: [[   4.0557   -7.7976   25.6297  -66.4968 -609.6923]]\n",
      "Weights: [[-4.5976 -0.9105  0.0837  0.1039  0.046 ]]\n",
      "MSE loss: 340.8938\n",
      "Iteration: 29400\n",
      "Gradient: [[   -3.5535    46.2053    32.0341   105.6819 -1032.4462]]\n",
      "Weights: [[-4.5965 -0.9127  0.0835  0.104   0.0461]]\n",
      "MSE loss: 339.8952\n",
      "Iteration: 29500\n",
      "Gradient: [[   7.3617   19.6713  -32.7159   33.8173 -135.1738]]\n",
      "Weights: [[-4.595  -0.915   0.0834  0.1041  0.0462]]\n",
      "MSE loss: 338.9241\n",
      "Iteration: 29600\n",
      "Gradient: [[    4.7724    36.5205   123.2865  -136.9502 -1068.4942]]\n",
      "Weights: [[-4.5917 -0.9165  0.0832  0.1042  0.0462]]\n",
      "MSE loss: 338.0994\n",
      "Iteration: 29700\n",
      "Gradient: [[   4.5583   41.9453   45.9986 -224.0118 -881.2766]]\n",
      "Weights: [[-4.5889 -0.9187  0.0831  0.1042  0.0463]]\n",
      "MSE loss: 337.1724\n",
      "Iteration: 29800\n",
      "Gradient: [[ -1.2594  19.5141  23.1859  -9.7067 561.0976]]\n",
      "Weights: [[-4.5866 -0.9208  0.0829  0.1043  0.0464]]\n",
      "MSE loss: 336.2794\n",
      "Iteration: 29900\n",
      "Gradient: [[  11.0981    8.9077   98.8912 -115.242   113.9257]]\n",
      "Weights: [[-4.5846 -0.9227  0.0828  0.1044  0.0464]]\n",
      "MSE loss: 335.4308\n",
      "Iteration: 30000\n",
      "Gradient: [[  -2.4141   46.9388  -43.3892 -117.3132 -458.3926]]\n",
      "Weights: [[-4.5827 -0.9245  0.0825  0.1045  0.0465]]\n",
      "MSE loss: 334.5622\n",
      "Iteration: 30100\n",
      "Gradient: [[ -16.7204   17.0203   -9.387    16.3489 -207.4572]]\n",
      "Weights: [[-4.5805 -0.927   0.0824  0.1046  0.0465]]\n",
      "MSE loss: 333.5783\n",
      "Iteration: 30200\n",
      "Gradient: [[   4.0225   52.4393  -24.4677 -196.6184 -947.2575]]\n",
      "Weights: [[-4.5787 -0.9289  0.0822  0.1047  0.0466]]\n",
      "MSE loss: 332.6781\n",
      "Iteration: 30300\n",
      "Gradient: [[   6.6703  -26.7318  -64.1262   40.1152 -934.4289]]\n",
      "Weights: [[-4.5771 -0.9307  0.082   0.1048  0.0467]]\n",
      "MSE loss: 331.7904\n",
      "Iteration: 30400\n",
      "Gradient: [[  -6.4014   30.9213   49.0101   16.6928 -291.0615]]\n",
      "Weights: [[-4.5757 -0.9328  0.0818  0.1049  0.0467]]\n",
      "MSE loss: 330.8667\n",
      "Iteration: 30500\n",
      "Gradient: [[   11.7227   -23.0768   -34.114    193.3278 -1130.15  ]]\n",
      "Weights: [[-4.5736 -0.9349  0.0817  0.105   0.0468]]\n",
      "MSE loss: 329.9673\n",
      "Iteration: 30600\n",
      "Gradient: [[ -10.1416   56.7248    1.0836 -221.041  -346.9759]]\n",
      "Weights: [[-4.5728 -0.9369  0.0816  0.1051  0.0468]]\n",
      "MSE loss: 329.0826\n",
      "Iteration: 30700\n",
      "Gradient: [[  -3.0335   14.4904   19.9735  233.4813 -283.7085]]\n",
      "Weights: [[-4.5717 -0.9389  0.0814  0.1052  0.0469]]\n",
      "MSE loss: 328.2229\n",
      "Iteration: 30800\n",
      "Gradient: [[  0.8024 -37.3835  95.2106 -23.5264 -96.266 ]]\n",
      "Weights: [[-4.5691 -0.9409  0.0813  0.1053  0.047 ]]\n",
      "MSE loss: 327.3972\n",
      "Iteration: 30900\n",
      "Gradient: [[  -32.5452    22.0565    67.4682   -50.4284 -1032.125 ]]\n",
      "Weights: [[-4.5691 -0.9425  0.0812  0.1054  0.047 ]]\n",
      "MSE loss: 326.6515\n",
      "Iteration: 31000\n",
      "Gradient: [[    2.6274    10.3365   -17.6399  -299.938  -1340.2505]]\n",
      "Weights: [[-4.567  -0.9441  0.0811  0.1055  0.0471]]\n",
      "MSE loss: 325.8689\n",
      "Iteration: 31100\n",
      "Gradient: [[  -5.4289   -3.0237   33.9751 -291.7334 -129.1891]]\n",
      "Weights: [[-4.5663 -0.9457  0.081   0.1055  0.0472]]\n",
      "MSE loss: 325.1349\n",
      "Iteration: 31200\n",
      "Gradient: [[   4.323   -28.9585   35.1007 -120.4701 -120.3759]]\n",
      "Weights: [[-4.5664 -0.9475  0.0808  0.1056  0.0472]]\n",
      "MSE loss: 324.3737\n",
      "Iteration: 31300\n",
      "Gradient: [[  -5.7215    3.1389   47.4174 -241.0225 -407.7986]]\n",
      "Weights: [[-4.5648 -0.9489  0.0807  0.1057  0.0473]]\n",
      "MSE loss: 323.6505\n",
      "Iteration: 31400\n",
      "Gradient: [[  -2.5039   -0.7941   40.7943 -149.6249 -653.2777]]\n",
      "Weights: [[-4.5626 -0.9506  0.0806  0.1058  0.0473]]\n",
      "MSE loss: 322.8423\n",
      "Iteration: 31500\n",
      "Gradient: [[  14.4445   30.8427   47.272   -66.4633 -769.5107]]\n",
      "Weights: [[-4.5593 -0.9522  0.0805  0.1059  0.0474]]\n",
      "MSE loss: 322.0871\n",
      "Iteration: 31600\n",
      "Gradient: [[-2.2083000e+01  7.8340000e-01 -7.1046100e+01 -9.5521000e+01\n",
      "  -1.4469746e+03]]\n",
      "Weights: [[-4.5586 -0.9544  0.0802  0.106   0.0474]]\n",
      "MSE loss: 321.2319\n",
      "Iteration: 31700\n",
      "Gradient: [[   2.244    15.5326   -2.4015  230.8488 -327.4763]]\n",
      "Weights: [[-4.5569 -0.9561  0.0801  0.106   0.0475]]\n",
      "MSE loss: 320.4741\n",
      "Iteration: 31800\n",
      "Gradient: [[-10.846   53.5347 -32.9393 -80.1266  32.4608]]\n",
      "Weights: [[-4.5539 -0.9579  0.0799  0.1061  0.0476]]\n",
      "MSE loss: 319.6612\n",
      "Iteration: 31900\n",
      "Gradient: [[-7.8623000e+00 -1.1617000e+00  1.1145100e+01 -2.1188960e+02\n",
      "  -2.0353898e+03]]\n",
      "Weights: [[-4.5519 -0.9598  0.0797  0.1062  0.0476]]\n",
      "MSE loss: 318.8312\n",
      "Iteration: 32000\n",
      "Gradient: [[   1.0778    3.85    -44.3532 -270.329  -193.6935]]\n",
      "Weights: [[-4.5499 -0.9618  0.0796  0.1063  0.0477]]\n",
      "MSE loss: 317.9901\n",
      "Iteration: 32100\n",
      "Gradient: [[   0.885   -22.8045 -113.4979    4.5563 -326.4734]]\n",
      "Weights: [[-4.5493 -0.9635  0.0794  0.1064  0.0478]]\n",
      "MSE loss: 317.2608\n",
      "Iteration: 32200\n",
      "Gradient: [[  15.8782    8.5932  -42.0801 -104.5603 -179.4305]]\n",
      "Weights: [[-4.5471 -0.9649  0.0792  0.1065  0.0478]]\n",
      "MSE loss: 316.5723\n",
      "Iteration: 32300\n",
      "Gradient: [[ -10.6516   15.4944   -8.9884  140.9378 -153.3073]]\n",
      "Weights: [[-4.5453 -0.9667  0.0791  0.1065  0.0479]]\n",
      "MSE loss: 315.8566\n",
      "Iteration: 32400\n",
      "Gradient: [[   2.0191  -23.5984  -11.1304  168.5762 -256.7994]]\n",
      "Weights: [[-4.5437 -0.9683  0.079   0.1066  0.0479]]\n",
      "MSE loss: 315.1524\n",
      "Iteration: 32500\n",
      "Gradient: [[  -3.9381   17.3196   95.4572  -67.0868 -395.0125]]\n",
      "Weights: [[-4.5427 -0.9702  0.0788  0.1067  0.048 ]]\n",
      "MSE loss: 314.406\n",
      "Iteration: 32600\n",
      "Gradient: [[  9.2987   7.3342  21.6424 -44.485  121.6449]]\n",
      "Weights: [[-4.5424 -0.9717  0.0786  0.1068  0.048 ]]\n",
      "MSE loss: 313.7185\n",
      "Iteration: 32700\n",
      "Gradient: [[ 4.016000e-01  1.101870e+01 -5.412380e+01 -3.301964e+02 -7.828260e+02]]\n",
      "Weights: [[-4.5397 -0.9734  0.0785  0.1068  0.0481]]\n",
      "MSE loss: 313.0182\n",
      "Iteration: 32800\n",
      "Gradient: [[   8.4314   54.4488   -0.3373 -174.9013 -118.377 ]]\n",
      "Weights: [[-4.5375 -0.9748  0.0783  0.1069  0.0481]]\n",
      "MSE loss: 312.337\n",
      "Iteration: 32900\n",
      "Gradient: [[   3.5531   22.1468   72.5745  -38.2553 -646.0143]]\n",
      "Weights: [[-4.5358 -0.9767  0.0782  0.107   0.0482]]\n",
      "MSE loss: 311.618\n",
      "Iteration: 33000\n",
      "Gradient: [[   2.7231   14.2249   89.6739 -207.1823 -439.9637]]\n",
      "Weights: [[-4.5345 -0.9782  0.078   0.1071  0.0483]]\n",
      "MSE loss: 310.9324\n",
      "Iteration: 33100\n",
      "Gradient: [[  19.1581   18.8809  -52.0934 -260.5354 -524.8439]]\n",
      "Weights: [[-4.5325 -0.9797  0.0779  0.1072  0.0483]]\n",
      "MSE loss: 310.2655\n",
      "Iteration: 33200\n",
      "Gradient: [[   6.9321    7.9874   47.9103   16.4458 -857.6632]]\n",
      "Weights: [[-4.5313 -0.9814  0.0777  0.1073  0.0484]]\n",
      "MSE loss: 309.5714\n",
      "Iteration: 33300\n",
      "Gradient: [[ -11.06     44.722   -23.3958  -45.895  -911.7021]]\n",
      "Weights: [[-4.5301 -0.9831  0.0775  0.1074  0.0484]]\n",
      "MSE loss: 308.8643\n",
      "Iteration: 33400\n",
      "Gradient: [[  -3.6002   29.6723   60.1607  166.8963 -381.33  ]]\n",
      "Weights: [[-4.5281 -0.9848  0.0775  0.1074  0.0485]]\n",
      "MSE loss: 308.1928\n",
      "Iteration: 33500\n",
      "Gradient: [[  16.6091   32.486    -3.7935 -143.3978 -290.9529]]\n",
      "Weights: [[-4.5257 -0.9867  0.0774  0.1075  0.0485]]\n",
      "MSE loss: 307.4561\n",
      "Iteration: 33600\n",
      "Gradient: [[  -8.4339   27.4684  -38.3097 -503.9328 -834.1522]]\n",
      "Weights: [[-4.5257 -0.9881  0.0771  0.1076  0.0486]]\n",
      "MSE loss: 306.8225\n",
      "Iteration: 33700\n",
      "Gradient: [[   1.8436   -1.9581   84.7701 -408.2542 -459.253 ]]\n",
      "Weights: [[-4.5245 -0.9899  0.077   0.1077  0.0486]]\n",
      "MSE loss: 306.157\n",
      "Iteration: 33800\n",
      "Gradient: [[    8.4824    -8.5322   -70.8195    92.8425 -1016.7031]]\n",
      "Weights: [[-4.5208 -0.9917  0.0767  0.1078  0.0487]]\n",
      "MSE loss: 305.3953\n",
      "Iteration: 33900\n",
      "Gradient: [[  14.2847  -25.1919  104.6151   15.8225 -103.0973]]\n",
      "Weights: [[-4.5177 -0.9932  0.0765  0.1079  0.0487]]\n",
      "MSE loss: 304.7302\n",
      "Iteration: 34000\n",
      "Gradient: [[  -1.2707   -3.7806   24.1519   70.7555 -388.4231]]\n",
      "Weights: [[-4.5175 -0.9949  0.0764  0.108   0.0488]]\n",
      "MSE loss: 304.1005\n",
      "Iteration: 34100\n",
      "Gradient: [[  -9.2434   17.3028   10.26   -212.1885 -140.1236]]\n",
      "Weights: [[-4.5171 -0.9964  0.0762  0.1081  0.0488]]\n",
      "MSE loss: 303.5043\n",
      "Iteration: 34200\n",
      "Gradient: [[   5.4716   10.8531  -41.9837 -315.9613  467.4554]]\n",
      "Weights: [[-4.5156 -0.9979  0.0761  0.1081  0.0489]]\n",
      "MSE loss: 302.9095\n",
      "Iteration: 34300\n",
      "Gradient: [[ -20.3456   18.307   -11.3087 -333.7617 -829.0878]]\n",
      "Weights: [[-4.5132 -0.9996  0.0759  0.1083  0.0489]]\n",
      "MSE loss: 302.2397\n",
      "Iteration: 34400\n",
      "Gradient: [[  4.9073  37.3124 -33.8528 113.0972 -54.3785]]\n",
      "Weights: [[-4.5119 -1.0018  0.0758  0.1084  0.049 ]]\n",
      "MSE loss: 301.5275\n",
      "Iteration: 34500\n",
      "Gradient: [[  -10.2495    38.8919    19.437    -93.8323 -1165.1802]]\n",
      "Weights: [[-4.5092 -1.0035  0.0756  0.1084  0.049 ]]\n",
      "MSE loss: 300.8499\n",
      "Iteration: 34600\n",
      "Gradient: [[   3.4015   -5.4274   41.6607 -342.86    225.2744]]\n",
      "Weights: [[-4.5061 -1.0053  0.0754  0.1085  0.0491]]\n",
      "MSE loss: 300.1863\n",
      "Iteration: 34700\n",
      "Gradient: [[ 6.800000e-02  4.314120e+01  2.220880e+01 -1.187209e+02 -8.555441e+02]]\n",
      "Weights: [[-4.5049 -1.0074  0.0752  0.1086  0.0491]]\n",
      "MSE loss: 299.4738\n",
      "Iteration: 34800\n",
      "Gradient: [[  -8.2127   31.3675   85.8021 -174.5923 -479.8199]]\n",
      "Weights: [[-4.505  -1.009   0.0751  0.1086  0.0492]]\n",
      "MSE loss: 298.9066\n",
      "Iteration: 34900\n",
      "Gradient: [[  -15.0662    22.6754    45.2729    29.3158 -1213.2169]]\n",
      "Weights: [[-4.5028 -1.0108  0.075   0.1087  0.0492]]\n",
      "MSE loss: 298.1794\n",
      "Iteration: 35000\n",
      "Gradient: [[   -5.8168    25.4004    34.308   -163.1099 -1016.7724]]\n",
      "Weights: [[-4.5032 -1.0123  0.0749  0.1088  0.0493]]\n",
      "MSE loss: 297.6152\n",
      "Iteration: 35100\n",
      "Gradient: [[  -3.0751   43.9986  113.4603 -299.8341 -190.7941]]\n",
      "Weights: [[-4.502  -1.0137  0.0748  0.1089  0.0493]]\n",
      "MSE loss: 297.027\n",
      "Iteration: 35200\n",
      "Gradient: [[  -17.9052    27.4013    19.7312  -112.796  -1163.0591]]\n",
      "Weights: [[-4.5    -1.0153  0.0748  0.109   0.0494]]\n",
      "MSE loss: 296.4319\n",
      "Iteration: 35300\n",
      "Gradient: [[-12.225   31.7503 -26.0285 110.6963 -74.6106]]\n",
      "Weights: [[-4.4983 -1.0168  0.0747  0.109   0.0494]]\n",
      "MSE loss: 295.8481\n",
      "Iteration: 35400\n",
      "Gradient: [[    9.8445    12.7217     3.0041   -62.9134 -1037.2113]]\n",
      "Weights: [[-4.497  -1.0184  0.0746  0.1091  0.0495]]\n",
      "MSE loss: 295.2285\n",
      "Iteration: 35500\n",
      "Gradient: [[   8.4733   17.3966  -29.1771  -37.9683 -211.6322]]\n",
      "Weights: [[-4.4947 -1.0199  0.0744  0.1092  0.0496]]\n",
      "MSE loss: 294.5843\n",
      "Iteration: 35600\n",
      "Gradient: [[  -14.6802    29.2323   103.0905  -377.2112 -1250.376 ]]\n",
      "Weights: [[-4.4936 -1.0218  0.0742  0.1093  0.0496]]\n",
      "MSE loss: 293.936\n",
      "Iteration: 35700\n",
      "Gradient: [[    4.8372    12.8274   -43.3064   -34.8006 -1071.396 ]]\n",
      "Weights: [[-4.4912 -1.0233  0.0741  0.1094  0.0497]]\n",
      "MSE loss: 293.3385\n",
      "Iteration: 35800\n",
      "Gradient: [[-2.158000e-01  9.267700e+00  8.589580e+01 -6.743430e+01 -9.301639e+02]]\n",
      "Weights: [[-4.4891 -1.0249  0.0739  0.1095  0.0497]]\n",
      "MSE loss: 292.7628\n",
      "Iteration: 35900\n",
      "Gradient: [[  -0.7085   20.9214   52.4423   66.4428 -453.192 ]]\n",
      "Weights: [[-4.4887 -1.0263  0.0736  0.1095  0.0497]]\n",
      "MSE loss: 292.1981\n",
      "Iteration: 36000\n",
      "Gradient: [[  -3.6919   55.7294  -51.7172 -178.399  -640.8006]]\n",
      "Weights: [[-4.4882 -1.0277  0.0734  0.1096  0.0498]]\n",
      "MSE loss: 291.6401\n",
      "Iteration: 36100\n",
      "Gradient: [[  -9.9478  -16.5962  -46.4269 -141.6534 -354.3666]]\n",
      "Weights: [[-4.4872 -1.0286  0.0733  0.1096  0.0499]]\n",
      "MSE loss: 291.1844\n",
      "Iteration: 36200\n",
      "Gradient: [[  -1.1709    5.6033   62.3014   69.3018 -292.9332]]\n",
      "Weights: [[-4.4859 -1.0301  0.0732  0.1097  0.0499]]\n",
      "MSE loss: 290.6424\n",
      "Iteration: 36300\n",
      "Gradient: [[  -1.8928    8.341    90.4853  -36.3653 -846.1508]]\n",
      "Weights: [[-4.4848 -1.0317  0.0731  0.1098  0.0499]]\n",
      "MSE loss: 290.082\n",
      "Iteration: 36400\n",
      "Gradient: [[   6.9406   43.7949  -50.2557 -150.3705 -366.5519]]\n",
      "Weights: [[-4.4821 -1.0331  0.0729  0.1099  0.05  ]]\n",
      "MSE loss: 289.4782\n",
      "Iteration: 36500\n",
      "Gradient: [[  24.8786    4.4233  -48.8129 -169.1902 -312.2495]]\n",
      "Weights: [[-4.48   -1.0346  0.0728  0.1099  0.0501]]\n",
      "MSE loss: 288.8648\n",
      "Iteration: 36600\n",
      "Gradient: [[  -9.4267   -2.6663  -30.1411 -199.4995 -213.8897]]\n",
      "Weights: [[-4.4788 -1.0362  0.0726  0.11    0.0501]]\n",
      "MSE loss: 288.2785\n",
      "Iteration: 36700\n",
      "Gradient: [[  8.2751  27.0598 -88.205  -54.5559 -30.3558]]\n",
      "Weights: [[-4.4774 -1.0378  0.0725  0.1101  0.0502]]\n",
      "MSE loss: 287.7537\n",
      "Iteration: 36800\n",
      "Gradient: [[  14.4261  -11.3626  120.8979 -135.6898 -228.1548]]\n",
      "Weights: [[-4.4771 -1.0388  0.0723  0.1101  0.0502]]\n",
      "MSE loss: 287.318\n",
      "Iteration: 36900\n",
      "Gradient: [[-7.078000e-01  6.488000e+00  2.231880e+01 -1.940372e+02 -9.148486e+02]]\n",
      "Weights: [[-4.4758 -1.0404  0.0721  0.1102  0.0502]]\n",
      "MSE loss: 286.7601\n",
      "Iteration: 37000\n",
      "Gradient: [[   6.9994   28.2972   40.6014  -19.5705 -770.6221]]\n",
      "Weights: [[-4.4743 -1.0417  0.072   0.1103  0.0503]]\n",
      "MSE loss: 286.2541\n",
      "Iteration: 37100\n",
      "Gradient: [[   6.2515   20.7722   14.7624  -99.0892 -391.921 ]]\n",
      "Weights: [[-4.4737 -1.0429  0.0719  0.1104  0.0503]]\n",
      "MSE loss: 285.7389\n",
      "Iteration: 37200\n",
      "Gradient: [[   3.4756    8.1893   41.8665 -149.5671 -878.4669]]\n",
      "Weights: [[-4.4726 -1.0443  0.0717  0.1104  0.0504]]\n",
      "MSE loss: 285.2371\n",
      "Iteration: 37300\n",
      "Gradient: [[  -3.2389   90.1745    9.8751   35.1769 -824.0975]]\n",
      "Weights: [[-4.4716 -1.0456  0.0716  0.1105  0.0504]]\n",
      "MSE loss: 284.7378\n",
      "Iteration: 37400\n",
      "Gradient: [[ -2.4982   0.4548 107.9635  69.4516 146.2059]]\n",
      "Weights: [[-4.4703 -1.047   0.0714  0.1106  0.0505]]\n",
      "MSE loss: 284.192\n",
      "Iteration: 37500\n",
      "Gradient: [[   -8.0893    -5.6959    24.5059   227.1615 -1381.5009]]\n",
      "Weights: [[-4.4691 -1.0478  0.0712  0.1107  0.0505]]\n",
      "MSE loss: 283.7508\n",
      "Iteration: 37600\n",
      "Gradient: [[  -7.46      3.4361  -11.5505 -208.2618 -878.5626]]\n",
      "Weights: [[-4.4672 -1.0494  0.0711  0.1107  0.0506]]\n",
      "MSE loss: 283.1998\n",
      "Iteration: 37700\n",
      "Gradient: [[ -16.3367    8.7766   -5.1509 -161.8666 -988.6603]]\n",
      "Weights: [[-4.466  -1.0506  0.071   0.1108  0.0506]]\n",
      "MSE loss: 282.7309\n",
      "Iteration: 37800\n",
      "Gradient: [[ -12.984   -19.6358   71.1054  -44.9632 -458.7957]]\n",
      "Weights: [[-4.4653 -1.0524  0.0708  0.1109  0.0507]]\n",
      "MSE loss: 282.1601\n",
      "Iteration: 37900\n",
      "Gradient: [[ -16.4386   31.7416   70.2369  -17.0857 -643.1641]]\n",
      "Weights: [[-4.4619 -1.0541  0.0706  0.111   0.0507]]\n",
      "MSE loss: 281.6267\n",
      "Iteration: 38000\n",
      "Gradient: [[  13.6596   -5.4058   14.5979 -130.3724 -626.807 ]]\n",
      "Weights: [[-4.4597 -1.0551  0.0703  0.111   0.0508]]\n",
      "MSE loss: 281.1094\n",
      "Iteration: 38100\n",
      "Gradient: [[   -5.9898    17.2802     2.4794  -114.1593 -1160.5988]]\n",
      "Weights: [[-4.4595 -1.0566  0.0702  0.1111  0.0508]]\n",
      "MSE loss: 280.5968\n",
      "Iteration: 38200\n",
      "Gradient: [[ -20.6906   27.12     21.3002 -269.3942 -858.3268]]\n",
      "Weights: [[-4.4576 -1.058   0.0701  0.1112  0.0509]]\n",
      "MSE loss: 280.0943\n",
      "Iteration: 38300\n",
      "Gradient: [[  -5.3485   13.5854   29.915    -4.2461 -263.4777]]\n",
      "Weights: [[-4.4563 -1.0597  0.0699  0.1112  0.0509]]\n",
      "MSE loss: 279.5698\n",
      "Iteration: 38400\n",
      "Gradient: [[  -15.1265    35.5667   -75.3772  -137.3505 -1197.1887]]\n",
      "Weights: [[-4.4561 -1.0608  0.0697  0.1113  0.051 ]]\n",
      "MSE loss: 279.1402\n",
      "Iteration: 38500\n",
      "Gradient: [[   -9.7314    18.8477   -27.4889    80.4517 -1074.3917]]\n",
      "Weights: [[-4.4548 -1.0622  0.0696  0.1114  0.051 ]]\n",
      "MSE loss: 278.661\n",
      "Iteration: 38600\n",
      "Gradient: [[   7.5476    8.6334  -55.9318  336.8746 -748.4877]]\n",
      "Weights: [[-4.454  -1.0636  0.0694  0.1114  0.0511]]\n",
      "MSE loss: 278.1763\n",
      "Iteration: 38700\n",
      "Gradient: [[  -7.0965   19.1759    6.7499   24.4821 -326.6123]]\n",
      "Weights: [[-4.4534 -1.0649  0.0693  0.1115  0.0511]]\n",
      "MSE loss: 277.691\n",
      "Iteration: 38800\n",
      "Gradient: [[ -11.2202  -14.4876  -15.8421 -176.353  -267.3056]]\n",
      "Weights: [[-4.4516 -1.066   0.0692  0.1115  0.0512]]\n",
      "MSE loss: 277.2207\n",
      "Iteration: 38900\n",
      "Gradient: [[ -18.4628   25.4007    4.8375  -48.9042 -564.6111]]\n",
      "Weights: [[-4.4496 -1.067   0.069   0.1116  0.0512]]\n",
      "MSE loss: 276.7498\n",
      "Iteration: 39000\n",
      "Gradient: [[ -15.1196   -0.6474   60.6113  -15.3889 -633.0654]]\n",
      "Weights: [[-4.4478 -1.0683  0.0688  0.1117  0.0513]]\n",
      "MSE loss: 276.271\n",
      "Iteration: 39100\n",
      "Gradient: [[   8.1685   17.2563  -21.9567  167.3983 -172.638 ]]\n",
      "Weights: [[-4.4462 -1.0694  0.0687  0.1117  0.0513]]\n",
      "MSE loss: 275.8466\n",
      "Iteration: 39200\n",
      "Gradient: [[  4.8035   5.5208  -2.0778 -14.2801 -93.9193]]\n",
      "Weights: [[-4.4454 -1.071   0.0686  0.1118  0.0514]]\n",
      "MSE loss: 275.3737\n",
      "Iteration: 39300\n",
      "Gradient: [[ 2.316400e+00  3.618000e-01  2.828100e+01 -2.533831e+02 -4.561253e+02]]\n",
      "Weights: [[-4.4442 -1.0725  0.0684  0.1118  0.0514]]\n",
      "MSE loss: 274.908\n",
      "Iteration: 39400\n",
      "Gradient: [[  -7.313     1.8099  129.7959 -391.3807  322.6523]]\n",
      "Weights: [[-4.4428 -1.0736  0.0683  0.1119  0.0514]]\n",
      "MSE loss: 274.4975\n",
      "Iteration: 39500\n",
      "Gradient: [[  -2.178     6.1684   26.342   -27.0335 -115.6379]]\n",
      "Weights: [[-4.4403 -1.0747  0.0681  0.112   0.0515]]\n",
      "MSE loss: 274.0164\n",
      "Iteration: 39600\n",
      "Gradient: [[   3.6634   23.1468  -40.3151 -123.8339 -504.0532]]\n",
      "Weights: [[-4.4395 -1.0762  0.0678  0.112   0.0515]]\n",
      "MSE loss: 273.5111\n",
      "Iteration: 39700\n",
      "Gradient: [[  -0.3477   20.1475   26.4499 -138.3502 -236.077 ]]\n",
      "Weights: [[-4.4381 -1.078   0.0678  0.1121  0.0516]]\n",
      "MSE loss: 273.0105\n",
      "Iteration: 39800\n",
      "Gradient: [[ 5.047400e+00  2.648000e+01  3.425000e-01 -1.445978e+02 -3.584108e+02]]\n",
      "Weights: [[-4.4355 -1.0789  0.0677  0.1122  0.0516]]\n",
      "MSE loss: 272.6263\n",
      "Iteration: 39900\n",
      "Gradient: [[  5.7414 -15.4437  41.2805  21.8177 305.4739]]\n",
      "Weights: [[-4.4348 -1.0803  0.0675  0.1122  0.0517]]\n",
      "MSE loss: 272.1632\n",
      "Iteration: 40000\n",
      "Gradient: [[ -22.5474   -9.4833   25.2316  -98.8613 -366.9271]]\n",
      "Weights: [[-4.4353 -1.0817  0.0674  0.1123  0.0517]]\n",
      "MSE loss: 271.7417\n",
      "Iteration: 40100\n",
      "Gradient: [[ -12.0334    8.3003  103.324  -134.1662 -686.2122]]\n",
      "Weights: [[-4.4334 -1.0828  0.0672  0.1123  0.0518]]\n",
      "MSE loss: 271.3045\n",
      "Iteration: 40200\n",
      "Gradient: [[  8.9944  13.9927  74.5641 -73.3319  42.4127]]\n",
      "Weights: [[-4.4313 -1.0839  0.067   0.1124  0.0518]]\n",
      "MSE loss: 270.8582\n",
      "Iteration: 40300\n",
      "Gradient: [[  7.5206   2.5298 -25.7006   8.9756 -15.7052]]\n",
      "Weights: [[-4.4317 -1.0854  0.0669  0.1125  0.0519]]\n",
      "MSE loss: 270.4211\n",
      "Iteration: 40400\n",
      "Gradient: [[   6.8214   14.5255   27.4745 -203.3236   63.2593]]\n",
      "Weights: [[-4.4313 -1.0864  0.0667  0.1125  0.0519]]\n",
      "MSE loss: 270.026\n",
      "Iteration: 40500\n",
      "Gradient: [[ -23.9357   19.224   -26.5993 -193.781  -351.3489]]\n",
      "Weights: [[-4.4322 -1.0874  0.0665  0.1126  0.0519]]\n",
      "MSE loss: 269.6951\n",
      "Iteration: 40600\n",
      "Gradient: [[  -1.6453    8.6074   -7.536   -65.8687 -625.6466]]\n",
      "Weights: [[-4.4304 -1.0887  0.0665  0.1127  0.052 ]]\n",
      "MSE loss: 269.2577\n",
      "Iteration: 40700\n",
      "Gradient: [[  -3.3186   29.467    30.786    65.6712 -391.8185]]\n",
      "Weights: [[-4.4289 -1.09    0.0664  0.1128  0.052 ]]\n",
      "MSE loss: 268.8243\n",
      "Iteration: 40800\n",
      "Gradient: [[   1.3684  -26.1523   35.246   -29.4537 -257.0458]]\n",
      "Weights: [[-4.4266 -1.0913  0.0662  0.1128  0.0521]]\n",
      "MSE loss: 268.3807\n",
      "Iteration: 40900\n",
      "Gradient: [[   -4.474     26.7444   -81.4741   -68.7782 -1042.6266]]\n",
      "Weights: [[-4.4249 -1.0923  0.066   0.1129  0.0521]]\n",
      "MSE loss: 267.9859\n",
      "Iteration: 41000\n",
      "Gradient: [[-3.596000e-01  2.625240e+01 -2.314000e+01 -1.238448e+02 -5.619701e+02]]\n",
      "Weights: [[-4.4252 -1.0935  0.0658  0.113   0.0522]]\n",
      "MSE loss: 267.5886\n",
      "Iteration: 41100\n",
      "Gradient: [[   7.5148   17.1657   14.6688 -262.7136 -678.1275]]\n",
      "Weights: [[-4.4251 -1.0945  0.0657  0.113   0.0522]]\n",
      "MSE loss: 267.2132\n",
      "Iteration: 41200\n",
      "Gradient: [[  -9.1157    6.2967   78.9873   -2.0599 -470.1469]]\n",
      "Weights: [[-4.421  -1.0957  0.0655  0.1131  0.0522]]\n",
      "MSE loss: 266.7698\n",
      "Iteration: 41300\n",
      "Gradient: [[ -21.7331   19.351     1.1311 -115.6074 -459.0828]]\n",
      "Weights: [[-4.4198 -1.0968  0.0653  0.1132  0.0523]]\n",
      "MSE loss: 266.3823\n",
      "Iteration: 41400\n",
      "Gradient: [[  21.4408   25.6935    6.9355 -139.9333  -42.3811]]\n",
      "Weights: [[-4.4205 -1.0979  0.0652  0.1132  0.0523]]\n",
      "MSE loss: 266.0214\n",
      "Iteration: 41500\n",
      "Gradient: [[  -5.8721   37.0137   -7.456   -59.5499 -412.508 ]]\n",
      "Weights: [[-4.4195 -1.0989  0.065   0.1133  0.0524]]\n",
      "MSE loss: 265.6594\n",
      "Iteration: 41600\n",
      "Gradient: [[  7.2923  20.6746  39.1733 129.4326 -23.3901]]\n",
      "Weights: [[-4.4172 -1.1001  0.0648  0.1133  0.0524]]\n",
      "MSE loss: 265.2526\n",
      "Iteration: 41700\n",
      "Gradient: [[  11.1086   34.0954   26.8404   -9.8762 -807.2588]]\n",
      "Weights: [[-4.4155 -1.1013  0.0647  0.1134  0.0525]]\n",
      "MSE loss: 264.8553\n",
      "Iteration: 41800\n",
      "Gradient: [[ -6.1691  23.5652   4.2228 -33.3356 103.6632]]\n",
      "Weights: [[-4.4146 -1.1023  0.0645  0.1135  0.0525]]\n",
      "MSE loss: 264.4713\n",
      "Iteration: 41900\n",
      "Gradient: [[   9.4558   22.8353  -18.0793   67.0992 -627.8707]]\n",
      "Weights: [[-4.4129 -1.1037  0.0644  0.1135  0.0525]]\n",
      "MSE loss: 264.1008\n",
      "Iteration: 42000\n",
      "Gradient: [[   5.86     -8.9762   98.0375 -231.9833 -899.0453]]\n",
      "Weights: [[-4.4119 -1.105   0.0643  0.1136  0.0526]]\n",
      "MSE loss: 263.7349\n",
      "Iteration: 42100\n",
      "Gradient: [[   3.8751    8.8164   37.9595   11.8049 -474.6513]]\n",
      "Weights: [[-4.412  -1.1062  0.064   0.1136  0.0526]]\n",
      "MSE loss: 263.3469\n",
      "Iteration: 42200\n",
      "Gradient: [[   3.7843    6.9052 -107.1517 -189.8888 -179.5673]]\n",
      "Weights: [[-4.4103 -1.1074  0.0639  0.1137  0.0527]]\n",
      "MSE loss: 262.9402\n",
      "Iteration: 42300\n",
      "Gradient: [[-6.348300e+00 -1.217100e+00  1.422000e-01 -1.600766e+02 -4.928698e+02]]\n",
      "Weights: [[-4.4071 -1.1087  0.0638  0.1138  0.0527]]\n",
      "MSE loss: 262.5434\n",
      "Iteration: 42400\n",
      "Gradient: [[   3.63     38.1247  -50.4727  -39.7409 -554.2738]]\n",
      "Weights: [[-4.4067 -1.1096  0.0637  0.1138  0.0527]]\n",
      "MSE loss: 262.2261\n",
      "Iteration: 42500\n",
      "Gradient: [[  -1.4132  -19.1876   42.5367  127.9963 -216.8321]]\n",
      "Weights: [[-4.4074 -1.1108  0.0636  0.1139  0.0528]]\n",
      "MSE loss: 261.8597\n",
      "Iteration: 42600\n",
      "Gradient: [[  -4.3336   21.6055   60.0941 -256.3647  -70.3146]]\n",
      "Weights: [[-4.4073 -1.1117  0.0635  0.1139  0.0528]]\n",
      "MSE loss: 261.5201\n",
      "Iteration: 42700\n",
      "Gradient: [[ -16.1388   15.7643   16.1295 -121.6401 -836.4157]]\n",
      "Weights: [[-4.405  -1.1128  0.0634  0.114   0.0529]]\n",
      "MSE loss: 261.1631\n",
      "Iteration: 42800\n",
      "Gradient: [[   1.302    11.7708   48.9947   44.2526 -576.9784]]\n",
      "Weights: [[-4.4032 -1.1142  0.0633  0.1141  0.0529]]\n",
      "MSE loss: 260.7804\n",
      "Iteration: 42900\n",
      "Gradient: [[   6.3862   24.8592   12.8749  -35.5959 -356.8805]]\n",
      "Weights: [[-4.4028 -1.1155  0.0632  0.1141  0.0529]]\n",
      "MSE loss: 260.4116\n",
      "Iteration: 43000\n",
      "Gradient: [[  0.9272   8.8512 -31.9405 -50.5468 113.7436]]\n",
      "Weights: [[-4.4008 -1.1164  0.063   0.1141  0.053 ]]\n",
      "MSE loss: 260.0888\n",
      "Iteration: 43100\n",
      "Gradient: [[  -1.6363   13.4944  -51.7614  -25.0061 -391.6685]]\n",
      "Weights: [[-4.4003 -1.1174  0.0628  0.1142  0.053 ]]\n",
      "MSE loss: 259.7868\n",
      "Iteration: 43200\n",
      "Gradient: [[ 1.3853 23.6056 45.9007 51.2235 38.7981]]\n",
      "Weights: [[-4.3984 -1.1185  0.0627  0.1142  0.0531]]\n",
      "MSE loss: 259.4352\n",
      "Iteration: 43300\n",
      "Gradient: [[ -10.3167    6.8779  -79.2507 -168.2126 -248.2906]]\n",
      "Weights: [[-4.3994 -1.1196  0.0627  0.1143  0.0531]]\n",
      "MSE loss: 259.0943\n",
      "Iteration: 43400\n",
      "Gradient: [[ -4.0876   3.9793  35.4871 -10.6752  19.9134]]\n",
      "Weights: [[-4.3972 -1.1208  0.0626  0.1144  0.0531]]\n",
      "MSE loss: 258.7254\n",
      "Iteration: 43500\n",
      "Gradient: [[   2.454     9.2712    3.7948 -163.8222 -750.4645]]\n",
      "Weights: [[-4.3958 -1.1219  0.0624  0.1144  0.0532]]\n",
      "MSE loss: 258.38\n",
      "Iteration: 43600\n",
      "Gradient: [[   4.5475    4.7724  -48.8332 -199.9757 -278.836 ]]\n",
      "Weights: [[-4.3946 -1.1231  0.0622  0.1145  0.0532]]\n",
      "MSE loss: 258.0095\n",
      "Iteration: 43700\n",
      "Gradient: [[   7.0087    5.8633   10.0553  -17.7923 -575.09  ]]\n",
      "Weights: [[-4.3946 -1.1241  0.0621  0.1145  0.0533]]\n",
      "MSE loss: 257.7095\n",
      "Iteration: 43800\n",
      "Gradient: [[-3.224400e+00 -9.810000e-02  1.253067e+02 -2.438361e+02 -3.222317e+02]]\n",
      "Weights: [[-4.3937 -1.1252  0.0619  0.1146  0.0533]]\n",
      "MSE loss: 257.3958\n",
      "Iteration: 43900\n",
      "Gradient: [[  -6.2537   -3.835    44.1532   23.4639 -507.8758]]\n",
      "Weights: [[-4.3916 -1.1261  0.0617  0.1146  0.0533]]\n",
      "MSE loss: 257.0486\n",
      "Iteration: 44000\n",
      "Gradient: [[ -13.4182   12.7417    0.3858 -351.6026 -367.1799]]\n",
      "Weights: [[-4.39   -1.1274  0.0616  0.1147  0.0534]]\n",
      "MSE loss: 256.7393\n",
      "Iteration: 44100\n",
      "Gradient: [[  16.6091    2.7207   33.1111   47.5368 -437.8782]]\n",
      "Weights: [[-4.3894 -1.1282  0.0613  0.1147  0.0534]]\n",
      "MSE loss: 256.4219\n",
      "Iteration: 44200\n",
      "Gradient: [[  -6.6464   -3.9657   55.583  -193.6312 -706.3118]]\n",
      "Weights: [[-4.3892 -1.1288  0.0612  0.1148  0.0534]]\n",
      "MSE loss: 256.1566\n",
      "Iteration: 44300\n",
      "Gradient: [[  -4.1906   11.5128    7.4927 -107.1342  -99.9599]]\n",
      "Weights: [[-4.3885 -1.1295  0.0611  0.1148  0.0535]]\n",
      "MSE loss: 255.8769\n",
      "Iteration: 44400\n",
      "Gradient: [[  12.9035   -8.441    80.4841 -297.3362  177.0868]]\n",
      "Weights: [[-4.3858 -1.1305  0.061   0.1148  0.0535]]\n",
      "MSE loss: 255.5494\n",
      "Iteration: 44500\n",
      "Gradient: [[  13.571    19.2415  -27.3351   21.7063 -657.6462]]\n",
      "Weights: [[-4.3853 -1.1317  0.0607  0.1149  0.0536]]\n",
      "MSE loss: 255.1902\n",
      "Iteration: 44600\n",
      "Gradient: [[ -8.6065   1.5303  46.0129 -38.9287  86.0479]]\n",
      "Weights: [[-4.3835 -1.1327  0.0606  0.1149  0.0536]]\n",
      "MSE loss: 254.8358\n",
      "Iteration: 44700\n",
      "Gradient: [[  -2.6625    2.7417   45.9336 -230.809  -185.0128]]\n",
      "Weights: [[-4.3825 -1.1335  0.0604  0.115   0.0537]]\n",
      "MSE loss: 254.5482\n",
      "Iteration: 44800\n",
      "Gradient: [[ -14.832    24.3224   23.8416 -163.4725 -948.9501]]\n",
      "Weights: [[-4.3836 -1.1347  0.0603  0.115   0.0537]]\n",
      "MSE loss: 254.241\n",
      "Iteration: 44900\n",
      "Gradient: [[   8.993    46.8312  -14.8896 -189.1453 -435.6602]]\n",
      "Weights: [[-4.3822 -1.1357  0.0601  0.1151  0.0537]]\n",
      "MSE loss: 253.8962\n",
      "Iteration: 45000\n",
      "Gradient: [[ -15.4155  -11.5172  102.06    238.1474 -239.1095]]\n",
      "Weights: [[-4.3824 -1.1365  0.0599  0.1152  0.0538]]\n",
      "MSE loss: 253.613\n",
      "Iteration: 45100\n",
      "Gradient: [[  -1.9332   41.1559  -22.5863 -105.7032 -292.9942]]\n",
      "Weights: [[-4.3806 -1.1375  0.0598  0.1152  0.0538]]\n",
      "MSE loss: 253.2952\n",
      "Iteration: 45200\n",
      "Gradient: [[ -13.8182   25.2419  -24.1995 -124.5772 -177.8907]]\n",
      "Weights: [[-4.3799 -1.1388  0.0597  0.1153  0.0538]]\n",
      "MSE loss: 252.9609\n",
      "Iteration: 45300\n",
      "Gradient: [[ -5.2534  27.566  -23.0597  53.2635 -15.7271]]\n",
      "Weights: [[-4.3783 -1.1398  0.0595  0.1153  0.0539]]\n",
      "MSE loss: 252.6512\n",
      "Iteration: 45400\n",
      "Gradient: [[   13.8077     2.5988   -12.9418  -133.3404 -1034.8085]]\n",
      "Weights: [[-4.3771 -1.1408  0.0594  0.1154  0.0539]]\n",
      "MSE loss: 252.313\n",
      "Iteration: 45500\n",
      "Gradient: [[   0.7774   40.0524   62.0798  -35.43   -546.881 ]]\n",
      "Weights: [[-4.3758 -1.1419  0.0592  0.1154  0.054 ]]\n",
      "MSE loss: 252.0176\n",
      "Iteration: 45600\n",
      "Gradient: [[   5.7042   65.342   -36.2571  -95.5131 -155.0574]]\n",
      "Weights: [[-4.3741 -1.1428  0.0591  0.1154  0.054 ]]\n",
      "MSE loss: 251.7121\n",
      "Iteration: 45700\n",
      "Gradient: [[   5.9026   16.5391   15.0388   -7.0548 -322.2296]]\n",
      "Weights: [[-4.3732 -1.1441  0.059   0.1155  0.0541]]\n",
      "MSE loss: 251.3849\n",
      "Iteration: 45800\n",
      "Gradient: [[  18.6544    8.2891  -34.8848 -110.9416 -521.181 ]]\n",
      "Weights: [[-4.3716 -1.145   0.0589  0.1155  0.0541]]\n",
      "MSE loss: 251.1154\n",
      "Iteration: 45900\n",
      "Gradient: [[  -1.9393   18.804    11.2113   23.0215 -756.497 ]]\n",
      "Weights: [[-4.3717 -1.1458  0.0588  0.1156  0.0541]]\n",
      "MSE loss: 250.8428\n",
      "Iteration: 46000\n",
      "Gradient: [[   2.8414  -16.1798   25.8634    5.7459 -181.0943]]\n",
      "Weights: [[-4.3705 -1.1468  0.0587  0.1156  0.0542]]\n",
      "MSE loss: 250.5462\n",
      "Iteration: 46100\n",
      "Gradient: [[  -6.7921   -4.9406   56.1155  -10.116  -113.4902]]\n",
      "Weights: [[-4.3694 -1.1476  0.0585  0.1156  0.0542]]\n",
      "MSE loss: 250.2843\n",
      "Iteration: 46200\n",
      "Gradient: [[  18.0285    4.9314   -2.8007  -25.6385 -139.7402]]\n",
      "Weights: [[-4.3677 -1.1487  0.0583  0.1157  0.0542]]\n",
      "MSE loss: 249.9598\n",
      "Iteration: 46300\n",
      "Gradient: [[  -1.4347   -2.2101   -7.1218 -103.2663 -365.7994]]\n",
      "Weights: [[-4.3679 -1.1496  0.0582  0.1158  0.0543]]\n",
      "MSE loss: 249.685\n",
      "Iteration: 46400\n",
      "Gradient: [[   -4.7926    22.2566    26.6219  -123.2161 -1157.0808]]\n",
      "Weights: [[-4.3668 -1.1507  0.0581  0.1158  0.0543]]\n",
      "MSE loss: 249.4061\n",
      "Iteration: 46500\n",
      "Gradient: [[  -2.0824   25.834    78.3668 -121.3937   24.1653]]\n",
      "Weights: [[-4.365  -1.1518  0.0579  0.1159  0.0544]]\n",
      "MSE loss: 249.0967\n",
      "Iteration: 46600\n",
      "Gradient: [[  -3.1409   27.8358   31.1507  104.837  -157.3346]]\n",
      "Weights: [[-4.3629 -1.1529  0.0578  0.1159  0.0544]]\n",
      "MSE loss: 248.8235\n",
      "Iteration: 46700\n",
      "Gradient: [[  13.7171   22.6775  -57.6608   72.8039 -237.8575]]\n",
      "Weights: [[-4.3633 -1.154   0.0577  0.1159  0.0544]]\n",
      "MSE loss: 248.565\n",
      "Iteration: 46800\n",
      "Gradient: [[  -6.836    -2.5382   -3.3637  -55.7063 -951.3731]]\n",
      "Weights: [[-4.364  -1.1551  0.0577  0.116   0.0545]]\n",
      "MSE loss: 248.3017\n",
      "Iteration: 46900\n",
      "Gradient: [[  -4.9209  -32.2894  140.0055 -102.4338 -690.2813]]\n",
      "Weights: [[-4.3618 -1.1558  0.0576  0.116   0.0545]]\n",
      "MSE loss: 248.0433\n",
      "Iteration: 47000\n",
      "Gradient: [[   1.439    21.0762  -19.4358    3.091  -343.7938]]\n",
      "Weights: [[-4.3601 -1.1571  0.0574  0.1161  0.0545]]\n",
      "MSE loss: 247.7275\n",
      "Iteration: 47100\n",
      "Gradient: [[-14.2728 -25.2266   4.2646  32.2631 156.5124]]\n",
      "Weights: [[-4.3592 -1.1579  0.0573  0.1162  0.0546]]\n",
      "MSE loss: 247.4581\n",
      "Iteration: 47200\n",
      "Gradient: [[ -15.7629  -22.8331  -32.4495 -175.9484 -228.5022]]\n",
      "Weights: [[-4.357  -1.1588  0.0572  0.1162  0.0546]]\n",
      "MSE loss: 247.1909\n",
      "Iteration: 47300\n",
      "Gradient: [[  -1.0736   26.3676   34.5147  -67.5903 -325.7942]]\n",
      "Weights: [[-4.3577 -1.1595  0.0571  0.1163  0.0546]]\n",
      "MSE loss: 246.981\n",
      "Iteration: 47400\n",
      "Gradient: [[  0.5567  -5.2158   3.9159 -43.4964 -23.6771]]\n",
      "Weights: [[-4.3573 -1.1607  0.057   0.1163  0.0547]]\n",
      "MSE loss: 246.7031\n",
      "Iteration: 47500\n",
      "Gradient: [[  12.8509   19.7269   42.9334  -62.5548 -223.1767]]\n",
      "Weights: [[-4.356  -1.1622  0.0569  0.1163  0.0547]]\n",
      "MSE loss: 246.3885\n",
      "Iteration: 47600\n",
      "Gradient: [[   5.5826    9.3105 -111.744   -59.6751 -279.3535]]\n",
      "Weights: [[-4.3542 -1.163   0.0567  0.1164  0.0547]]\n",
      "MSE loss: 246.1474\n",
      "Iteration: 47700\n",
      "Gradient: [[ -7.4808   0.2977 -52.9563  10.062  192.4024]]\n",
      "Weights: [[-4.3553 -1.1638  0.0566  0.1164  0.0548]]\n",
      "MSE loss: 245.9258\n",
      "Iteration: 47800\n",
      "Gradient: [[  -3.0565  -11.0631   -7.752   -82.0353 -182.7563]]\n",
      "Weights: [[-4.355  -1.1645  0.0565  0.1165  0.0548]]\n",
      "MSE loss: 245.711\n",
      "Iteration: 47900\n",
      "Gradient: [[-5.345000e-01  2.919900e+01 -2.669630e+01 -2.590432e+02 -6.108856e+02]]\n",
      "Weights: [[-4.3534 -1.1653  0.0564  0.1165  0.0548]]\n",
      "MSE loss: 245.439\n",
      "Iteration: 48000\n",
      "Gradient: [[ -13.6801   27.0529  -12.5459 -174.9519 -835.2432]]\n",
      "Weights: [[-4.3518 -1.1667  0.0564  0.1166  0.0549]]\n",
      "MSE loss: 245.1409\n",
      "Iteration: 48100\n",
      "Gradient: [[ 11.2374 -29.2394  96.1144 -82.6011  80.4111]]\n",
      "Weights: [[-4.3511 -1.1676  0.0563  0.1166  0.0549]]\n",
      "MSE loss: 244.8713\n",
      "Iteration: 48200\n",
      "Gradient: [[   6.1528    9.3263   56.8719  -82.4415 -943.4454]]\n",
      "Weights: [[-4.3502 -1.1684  0.0562  0.1167  0.055 ]]\n",
      "MSE loss: 244.6323\n",
      "Iteration: 48300\n",
      "Gradient: [[   1.3258    9.5184  -58.275    11.6739 -469.9471]]\n",
      "Weights: [[-4.3494 -1.1692  0.056   0.1167  0.055 ]]\n",
      "MSE loss: 244.3773\n",
      "Iteration: 48400\n",
      "Gradient: [[  -2.986    13.0364   23.2304  -84.0064 -798.395 ]]\n",
      "Weights: [[-4.3473 -1.1703  0.0559  0.1167  0.055 ]]\n",
      "MSE loss: 244.0825\n",
      "Iteration: 48500\n",
      "Gradient: [[   2.4766   31.5758   -4.9235 -198.836  -546.9018]]\n",
      "Weights: [[-4.3466 -1.1713  0.0558  0.1168  0.0551]]\n",
      "MSE loss: 243.8234\n",
      "Iteration: 48600\n",
      "Gradient: [[-6.646200e+00  1.331000e-01  1.221340e+01  1.404563e+02 -5.202164e+02]]\n",
      "Weights: [[-4.3464 -1.1724  0.0557  0.1168  0.0551]]\n",
      "MSE loss: 243.565\n",
      "Iteration: 48700\n",
      "Gradient: [[ -0.1888 -10.9107  98.7429 -28.7015 -92.3041]]\n",
      "Weights: [[-4.3458 -1.1731  0.0556  0.1169  0.0551]]\n",
      "MSE loss: 243.3241\n",
      "Iteration: 48800\n",
      "Gradient: [[-2.02300e-01  3.60570e+01 -3.25067e+01  5.43729e+01 -2.76811e+02]]\n",
      "Weights: [[-4.3432 -1.1737  0.0555  0.1169  0.0552]]\n",
      "MSE loss: 243.1202\n",
      "Iteration: 48900\n",
      "Gradient: [[   0.6295   -8.6114  -26.0055 -153.013  -568.9365]]\n",
      "Weights: [[-4.3421 -1.1745  0.0554  0.1169  0.0552]]\n",
      "MSE loss: 242.8932\n",
      "Iteration: 49000\n",
      "Gradient: [[ -12.8906  -15.3216  -23.1985   19.5162 -657.9768]]\n",
      "Weights: [[-4.3426 -1.1753  0.0552  0.117   0.0552]]\n",
      "MSE loss: 242.6576\n",
      "Iteration: 49100\n",
      "Gradient: [[  12.5121    2.927    40.1593   11.4936 -374.3134]]\n",
      "Weights: [[-4.3428 -1.1763  0.0551  0.117   0.0553]]\n",
      "MSE loss: 242.4274\n",
      "Iteration: 49200\n",
      "Gradient: [[ 4.513000e-01 -1.691160e+01 -1.014880e+02  8.781990e+01 -6.651429e+02]]\n",
      "Weights: [[-4.3435 -1.1773  0.055   0.1171  0.0553]]\n",
      "MSE loss: 242.2043\n",
      "Iteration: 49300\n",
      "Gradient: [[   4.6347   27.5511   64.2222 -130.7426 -532.6914]]\n",
      "Weights: [[-4.3422 -1.1782  0.0549  0.1171  0.0553]]\n",
      "MSE loss: 241.9348\n",
      "Iteration: 49400\n",
      "Gradient: [[-3.696000e-01 -2.268590e+01  6.661550e+01 -3.976310e+01 -4.966752e+02]]\n",
      "Weights: [[-4.3424 -1.1788  0.0548  0.1172  0.0554]]\n",
      "MSE loss: 241.752\n",
      "Iteration: 49500\n",
      "Gradient: [[ -12.6632   16.2011   22.6045 -164.6291 -708.2457]]\n",
      "Weights: [[-4.3408 -1.1794  0.0547  0.1172  0.0554]]\n",
      "MSE loss: 241.5594\n",
      "Iteration: 49600\n",
      "Gradient: [[  15.84    -12.2442  -35.2355  104.4684 -317.2313]]\n",
      "Weights: [[-4.3401 -1.18    0.0546  0.1173  0.0554]]\n",
      "MSE loss: 241.3675\n",
      "Iteration: 49700\n",
      "Gradient: [[  14.7072  -25.8304    5.9006   62.5426 -145.3332]]\n",
      "Weights: [[-4.3388 -1.1806  0.0544  0.1173  0.0555]]\n",
      "MSE loss: 241.1442\n",
      "Iteration: 49800\n",
      "Gradient: [[   0.9108   16.3863   16.3788 -312.7918  -21.6009]]\n",
      "Weights: [[-4.3381 -1.1816  0.0543  0.1173  0.0555]]\n",
      "MSE loss: 240.905\n",
      "Iteration: 49900\n",
      "Gradient: [[   9.6181   29.2843 -110.876    -6.0414 -755.3181]]\n",
      "Weights: [[-4.3366 -1.1822  0.0541  0.1174  0.0555]]\n",
      "MSE loss: 240.6908\n",
      "Iteration: 50000\n",
      "Gradient: [[  16.5744   28.7037   10.3922  -17.2842 -358.9437]]\n",
      "Weights: [[-4.3361 -1.183   0.054   0.1174  0.0556]]\n",
      "MSE loss: 240.4612\n",
      "Iteration: 50100\n",
      "Gradient: [[   3.8368   25.6162   28.7649 -101.885  -911.0049]]\n",
      "Weights: [[-4.3339 -1.1841  0.0538  0.1174  0.0556]]\n",
      "MSE loss: 240.1976\n",
      "Iteration: 50200\n",
      "Gradient: [[-4.179000e-01  2.251600e+01  4.382050e+01 -1.858085e+02 -5.182672e+02]]\n",
      "Weights: [[-4.3327 -1.185   0.0536  0.1175  0.0556]]\n",
      "MSE loss: 239.9891\n",
      "Iteration: 50300\n",
      "Gradient: [[  -1.4177   14.2645    2.0488 -168.0835  159.7793]]\n",
      "Weights: [[-4.3317 -1.1859  0.0535  0.1175  0.0557]]\n",
      "MSE loss: 239.7506\n",
      "Iteration: 50400\n",
      "Gradient: [[ -4.7354  -4.4048 -57.5275 -53.5907  42.4341]]\n",
      "Weights: [[-4.3316 -1.1864  0.0533  0.1176  0.0557]]\n",
      "MSE loss: 239.5553\n",
      "Iteration: 50500\n",
      "Gradient: [[ -11.3085  -11.8447   36.4264 -260.8117 -711.1548]]\n",
      "Weights: [[-4.3297 -1.1872  0.0532  0.1176  0.0557]]\n",
      "MSE loss: 239.3407\n",
      "Iteration: 50600\n",
      "Gradient: [[ 9.0408 23.1809 54.5092 34.0582 74.6527]]\n",
      "Weights: [[-4.3293 -1.1882  0.053   0.1177  0.0558]]\n",
      "MSE loss: 239.1144\n",
      "Iteration: 50700\n",
      "Gradient: [[  11.7659   39.4057   56.1867  -90.4546 -994.5707]]\n",
      "Weights: [[-4.3287 -1.1891  0.0528  0.1177  0.0558]]\n",
      "MSE loss: 238.8709\n",
      "Iteration: 50800\n",
      "Gradient: [[-1.836810e+01 -3.540000e-01 -4.505170e+01  4.076110e+01 -4.881589e+02]]\n",
      "Weights: [[-4.3283 -1.1903  0.0527  0.1177  0.0558]]\n",
      "MSE loss: 238.6439\n",
      "Iteration: 50900\n",
      "Gradient: [[  23.2298  -10.3007   36.7783   36.8623 -206.7765]]\n",
      "Weights: [[-4.3273 -1.1912  0.0526  0.1178  0.0559]]\n",
      "MSE loss: 238.4186\n",
      "Iteration: 51000\n",
      "Gradient: [[ -3.4842 -15.2473  68.6138 -32.8094   1.7096]]\n",
      "Weights: [[-4.3272 -1.192   0.0525  0.1178  0.0559]]\n",
      "MSE loss: 238.2163\n",
      "Iteration: 51100\n",
      "Gradient: [[  11.3302    2.0845   49.7096   29.2185 -712.4816]]\n",
      "Weights: [[-4.3266 -1.1929  0.0523  0.1179  0.0559]]\n",
      "MSE loss: 238.0064\n",
      "Iteration: 51200\n",
      "Gradient: [[  -5.4549   -8.631   -31.6541 -151.1744 -405.9915]]\n",
      "Weights: [[-4.3263 -1.1935  0.0522  0.1179  0.056 ]]\n",
      "MSE loss: 237.7945\n",
      "Iteration: 51300\n",
      "Gradient: [[ -10.8184  -17.951    50.6333  -80.8052 -132.3439]]\n",
      "Weights: [[-4.3247 -1.194   0.0521  0.1179  0.056 ]]\n",
      "MSE loss: 237.5898\n",
      "Iteration: 51400\n",
      "Gradient: [[ -26.1711   47.739   -35.8925   52.7195 -531.44  ]]\n",
      "Weights: [[-4.3239 -1.1948  0.052   0.118   0.056 ]]\n",
      "MSE loss: 237.3871\n",
      "Iteration: 51500\n",
      "Gradient: [[   -7.557     13.4975   -21.0352     2.9707 -1032.0172]]\n",
      "Weights: [[-4.3226 -1.1954  0.0518  0.118   0.0561]]\n",
      "MSE loss: 237.1715\n",
      "Iteration: 51600\n",
      "Gradient: [[ -20.5937  -11.6975  -16.7386 -232.7571 -196.5242]]\n",
      "Weights: [[-4.323  -1.1958  0.0517  0.118   0.0561]]\n",
      "MSE loss: 237.0104\n",
      "Iteration: 51700\n",
      "Gradient: [[ -4.0083  14.0775 -35.7192 -35.3178 186.8984]]\n",
      "Weights: [[-4.323  -1.1966  0.0516  0.1181  0.0561]]\n",
      "MSE loss: 236.8139\n",
      "Iteration: 51800\n",
      "Gradient: [[   7.0174   20.9307  -10.2923  -62.41   -191.558 ]]\n",
      "Weights: [[-4.3209 -1.1971  0.0515  0.1181  0.0562]]\n",
      "MSE loss: 236.6285\n",
      "Iteration: 51900\n",
      "Gradient: [[   7.2764    6.1163  -34.473  -400.6582 -227.3295]]\n",
      "Weights: [[-4.3197 -1.1979  0.0513  0.1181  0.0562]]\n",
      "MSE loss: 236.4211\n",
      "Iteration: 52000\n",
      "Gradient: [[   5.3206   16.9522   42.7371  184.3712 -140.9058]]\n",
      "Weights: [[-4.3184 -1.1989  0.0512  0.1182  0.0562]]\n",
      "MSE loss: 236.2083\n",
      "Iteration: 52100\n",
      "Gradient: [[  10.9325   -6.1122  -16.3517  -41.91   -263.8529]]\n",
      "Weights: [[-4.3186 -1.1998  0.0512  0.1182  0.0563]]\n",
      "MSE loss: 236.0014\n",
      "Iteration: 52200\n",
      "Gradient: [[ -17.5983   18.0885   -3.6913  -62.2594 -647.1844]]\n",
      "Weights: [[-4.3174 -1.2004  0.051   0.1183  0.0563]]\n",
      "MSE loss: 235.8101\n",
      "Iteration: 52300\n",
      "Gradient: [[ -21.1532   23.3517   15.9144   22.864  -745.6388]]\n",
      "Weights: [[-4.317  -1.2013  0.0508  0.1183  0.0563]]\n",
      "MSE loss: 235.6002\n",
      "Iteration: 52400\n",
      "Gradient: [[  12.3081   -7.0541   -6.5925   72.1351 -782.4405]]\n",
      "Weights: [[-4.3166 -1.2021  0.0506  0.1183  0.0564]]\n",
      "MSE loss: 235.392\n",
      "Iteration: 52500\n",
      "Gradient: [[ -18.3644  -22.3631   93.1744  119.8427 -135.1193]]\n",
      "Weights: [[-4.3146 -1.2029  0.0505  0.1184  0.0564]]\n",
      "MSE loss: 235.1827\n",
      "Iteration: 52600\n",
      "Gradient: [[ -16.8647   38.7884  -40.327   -45.0223 -705.5878]]\n",
      "Weights: [[-4.315  -1.2037  0.0503  0.1184  0.0564]]\n",
      "MSE loss: 234.9909\n",
      "Iteration: 52700\n",
      "Gradient: [[ -15.5051   12.2732  -29.2349  -55.9492 -482.4308]]\n",
      "Weights: [[-4.314  -1.2043  0.0503  0.1185  0.0565]]\n",
      "MSE loss: 234.8025\n",
      "Iteration: 52800\n",
      "Gradient: [[ -15.8324   32.0319   20.0256 -151.482  -513.2022]]\n",
      "Weights: [[-4.3146 -1.205   0.0501  0.1185  0.0565]]\n",
      "MSE loss: 234.6165\n",
      "Iteration: 52900\n",
      "Gradient: [[  5.4232 -23.9875  80.8244 -89.8022  14.6053]]\n",
      "Weights: [[-4.3132 -1.2058  0.05    0.1185  0.0565]]\n",
      "MSE loss: 234.416\n",
      "Iteration: 53000\n",
      "Gradient: [[  -1.9067   33.3424   26.9109  -70.0754 -539.4076]]\n",
      "Weights: [[-4.3119 -1.2066  0.05    0.1186  0.0566]]\n",
      "MSE loss: 234.23\n",
      "Iteration: 53100\n",
      "Gradient: [[ 10.123  -26.4786 -27.2516  61.2279 -99.8952]]\n",
      "Weights: [[-4.3119 -1.2074  0.0498  0.1186  0.0566]]\n",
      "MSE loss: 234.0337\n",
      "Iteration: 53200\n",
      "Gradient: [[ -11.5692    0.2382   95.4914   20.3373 -146.0904]]\n",
      "Weights: [[-4.3113 -1.2082  0.0496  0.1187  0.0566]]\n",
      "MSE loss: 233.8283\n",
      "Iteration: 53300\n",
      "Gradient: [[ -8.545  -18.441   38.7079  43.8146 -81.6778]]\n",
      "Weights: [[-4.3104 -1.2088  0.0496  0.1187  0.0567]]\n",
      "MSE loss: 233.6536\n",
      "Iteration: 53400\n",
      "Gradient: [[  3.9656  14.7946  13.7109 -67.4734 212.0535]]\n",
      "Weights: [[-4.3106 -1.2095  0.0495  0.1187  0.0567]]\n",
      "MSE loss: 233.5072\n",
      "Iteration: 53500\n",
      "Gradient: [[   7.2098   24.5437  -16.9409  140.898  -282.4056]]\n",
      "Weights: [[-4.3099 -1.2102  0.0493  0.1188  0.0567]]\n",
      "MSE loss: 233.324\n",
      "Iteration: 53600\n",
      "Gradient: [[   8.4184   12.7136  -11.307   -54.6302 -102.6941]]\n",
      "Weights: [[-4.3081 -1.2106  0.0492  0.1188  0.0567]]\n",
      "MSE loss: 233.1723\n",
      "Iteration: 53700\n",
      "Gradient: [[  12.0818   -1.9002   61.6002  -23.7363 -411.7246]]\n",
      "Weights: [[-4.3083 -1.2117  0.0491  0.1189  0.0568]]\n",
      "MSE loss: 232.9781\n",
      "Iteration: 53800\n",
      "Gradient: [[ -18.7069  -14.6316  -31.688   216.2854 -297.7089]]\n",
      "Weights: [[-4.3068 -1.2122  0.049   0.1189  0.0568]]\n",
      "MSE loss: 232.8058\n",
      "Iteration: 53900\n",
      "Gradient: [[ -9.0427  13.9536 -13.1198 -20.9524 107.2613]]\n",
      "Weights: [[-4.3056 -1.2129  0.0489  0.1189  0.0568]]\n",
      "MSE loss: 232.6341\n",
      "Iteration: 54000\n",
      "Gradient: [[  -3.6641   17.7625   27.0067 -307.587  -213.0121]]\n",
      "Weights: [[-4.3049 -1.2137  0.0488  0.119   0.0569]]\n",
      "MSE loss: 232.4659\n",
      "Iteration: 54100\n",
      "Gradient: [[  9.5554  24.3609   1.0525 196.7077  46.52  ]]\n",
      "Weights: [[-4.3042 -1.2144  0.0487  0.119   0.0569]]\n",
      "MSE loss: 232.3019\n",
      "Iteration: 54200\n",
      "Gradient: [[   1.3002   10.1718   -2.1207   27.3953 -117.2998]]\n",
      "Weights: [[-4.3038 -1.2149  0.0485  0.119   0.0569]]\n",
      "MSE loss: 232.1273\n",
      "Iteration: 54300\n",
      "Gradient: [[ -6.5791  -6.8919  -5.8546 -41.0818 -83.5523]]\n",
      "Weights: [[-4.3027 -1.2154  0.0483  0.1191  0.057 ]]\n",
      "MSE loss: 231.9629\n",
      "Iteration: 54400\n",
      "Gradient: [[-7.725000e-01  4.221680e+01 -5.570000e-02  8.998800e+00 -4.870549e+02]]\n",
      "Weights: [[-4.3027 -1.2162  0.0481  0.1191  0.057 ]]\n",
      "MSE loss: 231.7767\n",
      "Iteration: 54500\n",
      "Gradient: [[ -17.5113  -14.774    -5.418     9.7838 -525.9718]]\n",
      "Weights: [[-4.3017 -1.2169  0.048   0.1191  0.057 ]]\n",
      "MSE loss: 231.6019\n",
      "Iteration: 54600\n",
      "Gradient: [[  -1.206     6.3336   47.6238   25.5386 -329.767 ]]\n",
      "Weights: [[-4.3016 -1.2177  0.048   0.1192  0.057 ]]\n",
      "MSE loss: 231.4584\n",
      "Iteration: 54700\n",
      "Gradient: [[-14.8531  -8.6022  18.5757  31.336  -72.5774]]\n",
      "Weights: [[-4.3007 -1.2184  0.0479  0.1192  0.0571]]\n",
      "MSE loss: 231.2833\n",
      "Iteration: 54800\n",
      "Gradient: [[   3.5939   -6.7071  -27.614   105.7048 -174.6146]]\n",
      "Weights: [[-4.2985 -1.2193  0.0477  0.1192  0.0571]]\n",
      "MSE loss: 231.095\n",
      "Iteration: 54900\n",
      "Gradient: [[ 3.072500e+00 -8.250000e-02  3.137440e+01  2.599900e+00 -7.662467e+02]]\n",
      "Weights: [[-4.2972 -1.2199  0.0476  0.1193  0.0571]]\n",
      "MSE loss: 230.9322\n",
      "Iteration: 55000\n",
      "Gradient: [[  6.0823 -19.7061 121.0844 -16.1835 143.2788]]\n",
      "Weights: [[-4.2968 -1.2203  0.0474  0.1193  0.0572]]\n",
      "MSE loss: 230.7943\n",
      "Iteration: 55100\n",
      "Gradient: [[   9.3381   -4.9398   30.649   101.6702 -322.4383]]\n",
      "Weights: [[-4.2961 -1.2211  0.0473  0.1193  0.0572]]\n",
      "MSE loss: 230.6224\n",
      "Iteration: 55200\n",
      "Gradient: [[  -0.3918   36.1619  -42.6226 -154.493   172.4749]]\n",
      "Weights: [[-4.295  -1.2218  0.0471  0.1194  0.0572]]\n",
      "MSE loss: 230.4705\n",
      "Iteration: 55300\n",
      "Gradient: [[  -10.0763     5.2907    53.4658  -249.2646 -1026.3305]]\n",
      "Weights: [[-4.295  -1.2226  0.047   0.1194  0.0572]]\n",
      "MSE loss: 230.3021\n",
      "Iteration: 55400\n",
      "Gradient: [[   0.7508   33.9283   41.1173 -380.1224 -174.1146]]\n",
      "Weights: [[-4.2936 -1.2233  0.0469  0.1194  0.0573]]\n",
      "MSE loss: 230.1303\n",
      "Iteration: 55500\n",
      "Gradient: [[  -0.6785    0.1405   13.3141 -105.872     4.9757]]\n",
      "Weights: [[-4.2933 -1.224   0.0468  0.1194  0.0573]]\n",
      "MSE loss: 229.9703\n",
      "Iteration: 55600\n",
      "Gradient: [[  10.7519   -2.1067   39.5842 -235.1734 -540.4869]]\n",
      "Weights: [[-4.293  -1.2244  0.0467  0.1195  0.0573]]\n",
      "MSE loss: 229.8146\n",
      "Iteration: 55700\n",
      "Gradient: [[ -17.2293   10.5611  -72.7029   74.4892 -642.9841]]\n",
      "Weights: [[-4.2923 -1.2246  0.0465  0.1195  0.0574]]\n",
      "MSE loss: 229.7018\n",
      "Iteration: 55800\n",
      "Gradient: [[  -0.8134   -5.666   -25.4849 -147.242   273.169 ]]\n",
      "Weights: [[-4.2926 -1.2249  0.0463  0.1195  0.0574]]\n",
      "MSE loss: 229.5641\n",
      "Iteration: 55900\n",
      "Gradient: [[   0.8919   10.8934   31.2744  -79.8455 -157.3036]]\n",
      "Weights: [[-4.2915 -1.2256  0.0463  0.1196  0.0574]]\n",
      "MSE loss: 229.4094\n",
      "Iteration: 56000\n",
      "Gradient: [[-16.812   12.0719  -0.8463  53.1137 113.7114]]\n",
      "Weights: [[-4.2914 -1.2263  0.0461  0.1196  0.0574]]\n",
      "MSE loss: 229.2411\n",
      "Iteration: 56100\n",
      "Gradient: [[   2.0146   11.8389    8.44     65.5903 -634.9249]]\n",
      "Weights: [[-4.291  -1.2267  0.046   0.1197  0.0575]]\n",
      "MSE loss: 229.103\n",
      "Iteration: 56200\n",
      "Gradient: [[ 16.7286  18.9102  35.3925 -62.6824   0.5092]]\n",
      "Weights: [[-4.2907 -1.2274  0.0459  0.1197  0.0575]]\n",
      "MSE loss: 228.9447\n",
      "Iteration: 56300\n",
      "Gradient: [[   8.9573    8.5925  -26.2791 -148.1264 -529.4894]]\n",
      "Weights: [[-4.29   -1.2281  0.0458  0.1197  0.0575]]\n",
      "MSE loss: 228.7857\n",
      "Iteration: 56400\n",
      "Gradient: [[   2.2424    2.8628   32.2124   77.7074 -604.6391]]\n",
      "Weights: [[-4.2894 -1.2289  0.0456  0.1198  0.0576]]\n",
      "MSE loss: 228.6162\n",
      "Iteration: 56500\n",
      "Gradient: [[-1.051830e+01  1.515920e+01 -4.233000e-01 -1.508960e+02 -6.069477e+02]]\n",
      "Weights: [[-4.288  -1.2297  0.0455  0.1198  0.0576]]\n",
      "MSE loss: 228.4442\n",
      "Iteration: 56600\n",
      "Gradient: [[   5.2735   11.4563  -19.1963 -172.2332 -226.4721]]\n",
      "Weights: [[-4.2869 -1.2304  0.0454  0.1199  0.0576]]\n",
      "MSE loss: 228.2929\n",
      "Iteration: 56700\n",
      "Gradient: [[   9.7104   -0.7624   69.3836  -38.0238 -552.7548]]\n",
      "Weights: [[-4.2863 -1.2309  0.0452  0.1199  0.0576]]\n",
      "MSE loss: 228.1603\n",
      "Iteration: 56800\n",
      "Gradient: [[  -7.2058    1.2529    6.948    -4.1071 -443.4901]]\n",
      "Weights: [[-4.2863 -1.2314  0.0451  0.1199  0.0577]]\n",
      "MSE loss: 228.0278\n",
      "Iteration: 56900\n",
      "Gradient: [[ -16.6251   -7.8203    3.6306 -190.871  -308.0015]]\n",
      "Weights: [[-4.2848 -1.2319  0.045   0.12    0.0577]]\n",
      "MSE loss: 227.8937\n",
      "Iteration: 57000\n",
      "Gradient: [[  -7.8831   13.0377  -81.7137   59.396  -306.6595]]\n",
      "Weights: [[-4.2834 -1.2324  0.0448  0.12    0.0577]]\n",
      "MSE loss: 227.761\n",
      "Iteration: 57100\n",
      "Gradient: [[  7.4768 -12.4225  16.9261 -34.4117 361.351 ]]\n",
      "Weights: [[-4.282  -1.2331  0.0446  0.12    0.0577]]\n",
      "MSE loss: 227.6033\n",
      "Iteration: 57200\n",
      "Gradient: [[   2.0746  -36.2293   52.7829   25.1265 -415.3912]]\n",
      "Weights: [[-4.2833 -1.2335  0.0445  0.12    0.0578]]\n",
      "MSE loss: 227.4825\n",
      "Iteration: 57300\n",
      "Gradient: [[   7.5943    7.7591   55.4094 -204.2729   25.0473]]\n",
      "Weights: [[-4.2824 -1.2342  0.0444  0.1201  0.0578]]\n",
      "MSE loss: 227.345\n",
      "Iteration: 57400\n",
      "Gradient: [[-14.4325  -4.3044 122.7681 -99.895  119.2971]]\n",
      "Weights: [[-4.2801 -1.235   0.0443  0.1201  0.0578]]\n",
      "MSE loss: 227.1784\n",
      "Iteration: 57500\n",
      "Gradient: [[  15.2603  -26.6137   57.5363  -85.4657 -910.1744]]\n",
      "Weights: [[-4.2793 -1.2359  0.0441  0.1201  0.0579]]\n",
      "MSE loss: 227.0008\n",
      "Iteration: 57600\n",
      "Gradient: [[ -0.6633  -0.7481  40.4365 -61.7342 170.4216]]\n",
      "Weights: [[-4.2787 -1.2366  0.044   0.1202  0.0579]]\n",
      "MSE loss: 226.8629\n",
      "Iteration: 57700\n",
      "Gradient: [[-13.1733   0.7689  11.0894 -27.6518 -67.3059]]\n",
      "Weights: [[-4.2776 -1.2372  0.0439  0.1202  0.0579]]\n",
      "MSE loss: 226.7219\n",
      "Iteration: 57800\n",
      "Gradient: [[-12.4502   3.768   -7.3776 -57.7676   5.8466]]\n",
      "Weights: [[-4.2774 -1.2377  0.0437  0.1202  0.0579]]\n",
      "MSE loss: 226.5941\n",
      "Iteration: 57900\n",
      "Gradient: [[  20.508    16.4523  -14.5127   -8.9565 -168.5051]]\n",
      "Weights: [[-4.2776 -1.2381  0.0435  0.1202  0.058 ]]\n",
      "MSE loss: 226.4661\n",
      "Iteration: 58000\n",
      "Gradient: [[   1.3083   31.3062   24.621    59.1183 -115.1031]]\n",
      "Weights: [[-4.2769 -1.2385  0.0434  0.1202  0.058 ]]\n",
      "MSE loss: 226.3464\n",
      "Iteration: 58100\n",
      "Gradient: [[13.5134 11.5358 75.8053 96.6157 46.5282]]\n",
      "Weights: [[-4.2752 -1.2391  0.0433  0.1203  0.058 ]]\n",
      "MSE loss: 226.2022\n",
      "Iteration: 58200\n",
      "Gradient: [[  -4.1945   20.575    34.5559  -55.1121 -338.178 ]]\n",
      "Weights: [[-4.2751 -1.2399  0.0432  0.1203  0.058 ]]\n",
      "MSE loss: 226.056\n",
      "Iteration: 58300\n",
      "Gradient: [[   1.2658    7.9266   40.3739  -17.2631 -619.5491]]\n",
      "Weights: [[-4.2746 -1.2401  0.0432  0.1203  0.0581]]\n",
      "MSE loss: 225.9459\n",
      "Iteration: 58400\n",
      "Gradient: [[  -8.9925    3.6608  -32.3411   -8.7426 -394.3949]]\n",
      "Weights: [[-4.2738 -1.2409  0.0431  0.1204  0.0581]]\n",
      "MSE loss: 225.795\n",
      "Iteration: 58500\n",
      "Gradient: [[  12.3473   -6.0679 -110.2634  129.5041 -466.176 ]]\n",
      "Weights: [[-4.2733 -1.2413  0.0429  0.1204  0.0581]]\n",
      "MSE loss: 225.6716\n",
      "Iteration: 58600\n",
      "Gradient: [[ -15.8262   32.2797   51.8096  -90.6503 -322.8239]]\n",
      "Weights: [[-4.2732 -1.2419  0.0428  0.1204  0.0582]]\n",
      "MSE loss: 225.5494\n",
      "Iteration: 58700\n",
      "Gradient: [[ 1.416480e+01  1.390000e-02 -4.808670e+01 -6.027620e+01 -2.137241e+02]]\n",
      "Weights: [[-4.2733 -1.2425  0.0427  0.1204  0.0582]]\n",
      "MSE loss: 225.418\n",
      "Iteration: 58800\n",
      "Gradient: [[  -1.9851   22.7567  -10.3774 -176.6884  167.7571]]\n",
      "Weights: [[-4.2731 -1.2429  0.0426  0.1205  0.0582]]\n",
      "MSE loss: 225.2816\n",
      "Iteration: 58900\n",
      "Gradient: [[13.2115 25.5314 76.4886 -9.1357 49.7209]]\n",
      "Weights: [[-4.2718 -1.2436  0.0425  0.1205  0.0583]]\n",
      "MSE loss: 225.1364\n",
      "Iteration: 59000\n",
      "Gradient: [[ 7.776100e+00 -5.719000e-01  3.835430e+01 -7.512110e+01 -6.084202e+02]]\n",
      "Weights: [[-4.2713 -1.2441  0.0424  0.1205  0.0583]]\n",
      "MSE loss: 225.0243\n",
      "Iteration: 59100\n",
      "Gradient: [[   7.5723    4.1867   52.1503 -108.1378 -432.8817]]\n",
      "Weights: [[-4.271  -1.2447  0.0422  0.1205  0.0583]]\n",
      "MSE loss: 224.8977\n",
      "Iteration: 59200\n",
      "Gradient: [[   1.3947    0.9465  -30.7096  -22.1525 -491.1795]]\n",
      "Weights: [[-4.2697 -1.2454  0.0421  0.1205  0.0583]]\n",
      "MSE loss: 224.7645\n",
      "Iteration: 59300\n",
      "Gradient: [[ -20.6845   35.413    11.6496  -82.4117 -790.1139]]\n",
      "Weights: [[-4.2691 -1.2459  0.042   0.1206  0.0583]]\n",
      "MSE loss: 224.6633\n",
      "Iteration: 59400\n",
      "Gradient: [[ 6.844   4.5774 24.6033 -2.4632 61.5732]]\n",
      "Weights: [[-4.2694 -1.2464  0.0418  0.1206  0.0584]]\n",
      "MSE loss: 224.5367\n",
      "Iteration: 59500\n",
      "Gradient: [[  8.3337  -9.0429  22.8495 -53.118  366.4459]]\n",
      "Weights: [[-4.2684 -1.2474  0.0417  0.1206  0.0584]]\n",
      "MSE loss: 224.3713\n",
      "Iteration: 59600\n",
      "Gradient: [[  16.0275   41.0886  -54.7031   98.1846 -325.4062]]\n",
      "Weights: [[-4.2687 -1.2478  0.0417  0.1206  0.0584]]\n",
      "MSE loss: 224.2664\n",
      "Iteration: 59700\n",
      "Gradient: [[   2.5945    4.6647   69.5722   33.155  -212.4729]]\n",
      "Weights: [[-4.2671 -1.2482  0.0416  0.1207  0.0585]]\n",
      "MSE loss: 224.1542\n",
      "Iteration: 59800\n",
      "Gradient: [[  7.1708   0.6359 -14.0585 -33.6232 250.264 ]]\n",
      "Weights: [[-4.2677 -1.2486  0.0414  0.1207  0.0585]]\n",
      "MSE loss: 224.0373\n",
      "Iteration: 59900\n",
      "Gradient: [[ -12.8656  -19.194   -18.6348  -80.7547 -164.9814]]\n",
      "Weights: [[-4.2674 -1.2492  0.0413  0.1207  0.0585]]\n",
      "MSE loss: 223.9022\n",
      "Iteration: 60000\n",
      "Gradient: [[  -8.0193   17.3944   79.6385   39.1653 -388.7852]]\n",
      "Weights: [[-4.2668 -1.2497  0.0412  0.1208  0.0585]]\n",
      "MSE loss: 223.7765\n",
      "Iteration: 60100\n",
      "Gradient: [[   -7.6392    38.7179    38.8578   -82.3544 -1005.3644]]\n",
      "Weights: [[-4.2662 -1.2502  0.041   0.1208  0.0586]]\n",
      "MSE loss: 223.6385\n",
      "Iteration: 60200\n",
      "Gradient: [[ 10.4206  24.7592  72.3819 -73.5984 177.8833]]\n",
      "Weights: [[-4.2646 -1.2508  0.0409  0.1208  0.0586]]\n",
      "MSE loss: 223.5167\n",
      "Iteration: 60300\n",
      "Gradient: [[  -2.9982  -24.8342   72.1385   50.0887 -353.7151]]\n",
      "Weights: [[-4.2628 -1.2513  0.0408  0.1209  0.0586]]\n",
      "MSE loss: 223.3802\n",
      "Iteration: 60400\n",
      "Gradient: [[ -9.1054   4.1899   6.7565 -19.1177 -21.1274]]\n",
      "Weights: [[-4.2633 -1.2521  0.0407  0.1209  0.0586]]\n",
      "MSE loss: 223.2639\n",
      "Iteration: 60500\n",
      "Gradient: [[  10.4337   -6.5052   22.7018   55.6009 -349.5387]]\n",
      "Weights: [[-4.2624 -1.2526  0.0406  0.1209  0.0587]]\n",
      "MSE loss: 223.1406\n",
      "Iteration: 60600\n",
      "Gradient: [[  11.2736   32.943    43.6882  -22.9929 -496.2833]]\n",
      "Weights: [[-4.262  -1.2534  0.0405  0.121   0.0587]]\n",
      "MSE loss: 222.9949\n",
      "Iteration: 60700\n",
      "Gradient: [[  11.3381   -1.4817  -10.4305   93.372  -127.2085]]\n",
      "Weights: [[-4.2602 -1.254   0.0403  0.121   0.0587]]\n",
      "MSE loss: 222.8687\n",
      "Iteration: 60800\n",
      "Gradient: [[  14.4608   18.6277  -32.8128 -121.0601 -521.1039]]\n",
      "Weights: [[-4.2614 -1.2546  0.0401  0.121   0.0588]]\n",
      "MSE loss: 222.7484\n",
      "Iteration: 60900\n",
      "Gradient: [[  10.9169   -5.1279   11.2541  -67.6503 -269.7537]]\n",
      "Weights: [[-4.2611 -1.2552  0.04    0.121   0.0588]]\n",
      "MSE loss: 222.6378\n",
      "Iteration: 61000\n",
      "Gradient: [[ -10.201    -5.9457  -19.9222    9.589  -808.0101]]\n",
      "Weights: [[-4.2603 -1.2557  0.04    0.1211  0.0588]]\n",
      "MSE loss: 222.5156\n",
      "Iteration: 61100\n",
      "Gradient: [[  -4.5645   11.1427   -2.0648   35.3875 -174.3443]]\n",
      "Weights: [[-4.2605 -1.2561  0.0399  0.1211  0.0588]]\n",
      "MSE loss: 222.4156\n",
      "Iteration: 61200\n",
      "Gradient: [[ -8.8803 -11.4505 -22.7221 262.5801 157.914 ]]\n",
      "Weights: [[-4.259  -1.2566  0.0399  0.1211  0.0589]]\n",
      "MSE loss: 222.3026\n",
      "Iteration: 61300\n",
      "Gradient: [[  -6.0053   24.6115   24.7163   56.5302 -780.1766]]\n",
      "Weights: [[-4.2572 -1.2573  0.0397  0.1211  0.0589]]\n",
      "MSE loss: 222.1656\n",
      "Iteration: 61400\n",
      "Gradient: [[  -9.5923   -4.4038   50.6854  -19.5672 -604.6342]]\n",
      "Weights: [[-4.2561 -1.2579  0.0396  0.1212  0.0589]]\n",
      "MSE loss: 222.0563\n",
      "Iteration: 61500\n",
      "Gradient: [[  12.4963   33.5534  -23.0327   41.495  -740.0688]]\n",
      "Weights: [[-4.2554 -1.2583  0.0395  0.1212  0.0589]]\n",
      "MSE loss: 221.9531\n",
      "Iteration: 61600\n",
      "Gradient: [[ -1.2691  20.6069   0.982   10.1087 -34.1211]]\n",
      "Weights: [[-4.2558 -1.2588  0.0393  0.1212  0.059 ]]\n",
      "MSE loss: 221.8482\n",
      "Iteration: 61700\n",
      "Gradient: [[   5.4856   25.447    22.4364  -71.9342 -407.4392]]\n",
      "Weights: [[-4.2552 -1.2593  0.0393  0.1212  0.059 ]]\n",
      "MSE loss: 221.7496\n",
      "Iteration: 61800\n",
      "Gradient: [[  -0.9683   14.1498  -87.1731 -205.2992  152.2662]]\n",
      "Weights: [[-4.2541 -1.2598  0.0392  0.1213  0.059 ]]\n",
      "MSE loss: 221.6357\n",
      "Iteration: 61900\n",
      "Gradient: [[  21.0727   18.7365   14.4469   36.1036 -574.381 ]]\n",
      "Weights: [[-4.2536 -1.2606  0.0392  0.1213  0.059 ]]\n",
      "MSE loss: 221.5201\n",
      "Iteration: 62000\n",
      "Gradient: [[ 5.0784 -0.9276 34.0844 46.0812 62.9339]]\n",
      "Weights: [[-4.253  -1.2611  0.0391  0.1213  0.0591]]\n",
      "MSE loss: 221.4141\n",
      "Iteration: 62100\n",
      "Gradient: [[   8.0519    2.5054   18.0291  124.8184 -259.1517]]\n",
      "Weights: [[-4.2547 -1.2615  0.0391  0.1213  0.0591]]\n",
      "MSE loss: 221.3398\n",
      "Iteration: 62200\n",
      "Gradient: [[  10.5996  -26.8469   39.2553 -138.652  -373.0762]]\n",
      "Weights: [[-4.2545 -1.262   0.0389  0.1213  0.0591]]\n",
      "MSE loss: 221.2354\n",
      "Iteration: 62300\n",
      "Gradient: [[ -3.1074  15.4869 -38.6084  43.0984 -67.2035]]\n",
      "Weights: [[-4.2535 -1.2623  0.0389  0.1213  0.0591]]\n",
      "MSE loss: 221.1183\n",
      "Iteration: 62400\n",
      "Gradient: [[   8.3763    7.6131 -102.701  -100.3389 -137.8951]]\n",
      "Weights: [[-4.2526 -1.2629  0.0387  0.1214  0.0592]]\n",
      "MSE loss: 221.0038\n",
      "Iteration: 62500\n",
      "Gradient: [[   4.7659   17.5115   37.0045 -137.0846 -233.336 ]]\n",
      "Weights: [[-4.2506 -1.2633  0.0386  0.1214  0.0592]]\n",
      "MSE loss: 220.9036\n",
      "Iteration: 62600\n",
      "Gradient: [[  -4.8519   21.2546   25.0066 -151.683  -305.6534]]\n",
      "Weights: [[-4.2496 -1.2641  0.0385  0.1214  0.0592]]\n",
      "MSE loss: 220.7701\n",
      "Iteration: 62700\n",
      "Gradient: [[   7.7038   38.1905   -7.2412   68.3535 -193.5736]]\n",
      "Weights: [[-4.251  -1.2647  0.0384  0.1214  0.0592]]\n",
      "MSE loss: 220.6742\n",
      "Iteration: 62800\n",
      "Gradient: [[   0.721     1.846    53.2131 -202.6681 -317.5893]]\n",
      "Weights: [[-4.2513 -1.2649  0.0383  0.1214  0.0593]]\n",
      "MSE loss: 220.5965\n",
      "Iteration: 62900\n",
      "Gradient: [[   3.2558  -13.3054   38.3753  -94.2301 -693.8946]]\n",
      "Weights: [[-4.2507 -1.2653  0.0383  0.1215  0.0593]]\n",
      "MSE loss: 220.4962\n",
      "Iteration: 63000\n",
      "Gradient: [[ -1.434   -4.7066   1.0165 125.4908  41.1183]]\n",
      "Weights: [[-4.2494 -1.2657  0.0382  0.1215  0.0593]]\n",
      "MSE loss: 220.3903\n",
      "Iteration: 63100\n",
      "Gradient: [[  10.1387  -13.1072   45.8152  -84.7194 -597.2683]]\n",
      "Weights: [[-4.2477 -1.2664  0.0381  0.1215  0.0593]]\n",
      "MSE loss: 220.2731\n",
      "Iteration: 63200\n",
      "Gradient: [[   7.6122  -11.3696   55.9768  -77.4255 -851.21  ]]\n",
      "Weights: [[-4.2472 -1.2667  0.0379  0.1216  0.0594]]\n",
      "MSE loss: 220.1745\n",
      "Iteration: 63300\n",
      "Gradient: [[  -2.8386   13.1331  -51.6249  -82.9071 -183.2433]]\n",
      "Weights: [[-4.2471 -1.267   0.0378  0.1216  0.0594]]\n",
      "MSE loss: 220.0822\n",
      "Iteration: 63400\n",
      "Gradient: [[  4.6473 -25.343    6.1502  11.9364 -31.2961]]\n",
      "Weights: [[-4.2458 -1.2672  0.0377  0.1216  0.0594]]\n",
      "MSE loss: 219.9976\n",
      "Iteration: 63500\n",
      "Gradient: [[  -4.385     9.6719   14.5558 -307.2644 -238.2984]]\n",
      "Weights: [[-4.2461 -1.2676  0.0375  0.1216  0.0594]]\n",
      "MSE loss: 219.9141\n",
      "Iteration: 63600\n",
      "Gradient: [[ 13.1058  -2.0292 -83.6434 -43.7973 151.4433]]\n",
      "Weights: [[-4.2458 -1.2683  0.0374  0.1217  0.0594]]\n",
      "MSE loss: 219.8161\n",
      "Iteration: 63700\n",
      "Gradient: [[   4.1992  -16.963     4.4001  -15.0721 -232.1163]]\n",
      "Weights: [[-4.2449 -1.2687  0.0374  0.1217  0.0595]]\n",
      "MSE loss: 219.7389\n",
      "Iteration: 63800\n",
      "Gradient: [[  13.306    28.8122    6.9694   93.6481 -205.5435]]\n",
      "Weights: [[-4.2433 -1.2691  0.0372  0.1217  0.0595]]\n",
      "MSE loss: 219.6451\n",
      "Iteration: 63900\n",
      "Gradient: [[  0.2425  23.7385  41.3794 -51.5057 109.2516]]\n",
      "Weights: [[-4.2439 -1.2697  0.0371  0.1217  0.0595]]\n",
      "MSE loss: 219.5368\n",
      "Iteration: 64000\n",
      "Gradient: [[3.374000e-01 6.761800e+00 1.909940e+01 2.281585e+02 3.476172e+02]]\n",
      "Weights: [[-4.2449 -1.2704  0.0371  0.1217  0.0595]]\n",
      "MSE loss: 219.4455\n",
      "Iteration: 64100\n",
      "Gradient: [[ 10.8071   6.0737  11.2348 -94.5603  47.8575]]\n",
      "Weights: [[-4.2439 -1.2709  0.037   0.1217  0.0596]]\n",
      "MSE loss: 219.3497\n",
      "Iteration: 64200\n",
      "Gradient: [[ -2.7884 -26.4035 -50.5298 -38.2067 459.8514]]\n",
      "Weights: [[-4.243  -1.2711  0.0369  0.1218  0.0596]]\n",
      "MSE loss: 219.2642\n",
      "Iteration: 64300\n",
      "Gradient: [[  -3.9883   -9.3      35.3163 -116.9457  118.5937]]\n",
      "Weights: [[-4.2424 -1.2714  0.0368  0.1218  0.0596]]\n",
      "MSE loss: 219.1709\n",
      "Iteration: 64400\n",
      "Gradient: [[  10.7122   11.9352   17.356  -128.9032 -363.5763]]\n",
      "Weights: [[-4.242  -1.2719  0.0367  0.1218  0.0596]]\n",
      "MSE loss: 219.0686\n",
      "Iteration: 64500\n",
      "Gradient: [[  -4.9621    6.235     9.5452  189.7403 -627.7726]]\n",
      "Weights: [[-4.2422 -1.2721  0.0366  0.1218  0.0597]]\n",
      "MSE loss: 218.994\n",
      "Iteration: 64600\n",
      "Gradient: [[  -3.2161    4.2949    7.9076  137.1013 -545.4341]]\n",
      "Weights: [[-4.2419 -1.2725  0.0365  0.1218  0.0597]]\n",
      "MSE loss: 218.9071\n",
      "Iteration: 64700\n",
      "Gradient: [[   8.9181  -13.6692  -32.8089 -106.3974 -195.7385]]\n",
      "Weights: [[-4.2418 -1.2732  0.0363  0.1219  0.0597]]\n",
      "MSE loss: 218.8015\n",
      "Iteration: 64800\n",
      "Gradient: [[ 3.00000e-02  3.69842e+01 -4.24640e+01  9.44329e+01 -1.20190e+02]]\n",
      "Weights: [[-4.2401 -1.2736  0.0363  0.1219  0.0597]]\n",
      "MSE loss: 218.7112\n",
      "Iteration: 64900\n",
      "Gradient: [[   5.5228    4.7046   47.3577   14.071  -546.6242]]\n",
      "Weights: [[-4.2396 -1.274   0.0362  0.1219  0.0597]]\n",
      "MSE loss: 218.6343\n",
      "Iteration: 65000\n",
      "Gradient: [[ 1.047000e-01  1.424500e+01  3.537850e+01 -1.862540e+02 -5.767002e+02]]\n",
      "Weights: [[-4.2387 -1.2742  0.036   0.1219  0.0598]]\n",
      "MSE loss: 218.5494\n",
      "Iteration: 65100\n",
      "Gradient: [[  12.3208   20.6537  -11.3056  153.2476 -198.4213]]\n",
      "Weights: [[-4.2384 -1.2749  0.0358  0.1219  0.0598]]\n",
      "MSE loss: 218.4348\n",
      "Iteration: 65200\n",
      "Gradient: [[  -6.0066  -20.1134   26.1214 -247.8029 -506.5891]]\n",
      "Weights: [[-4.2373 -1.2755  0.0357  0.122   0.0598]]\n",
      "MSE loss: 218.3398\n",
      "Iteration: 65300\n",
      "Gradient: [[   1.5892   -2.1053   83.5078   42.0882 -331.5946]]\n",
      "Weights: [[-4.2373 -1.2758  0.0355  0.122   0.0598]]\n",
      "MSE loss: 218.2423\n",
      "Iteration: 65400\n",
      "Gradient: [[   7.9565    4.1896   16.974   113.7982 -765.1818]]\n",
      "Weights: [[-4.2369 -1.2761  0.0354  0.122   0.0599]]\n",
      "MSE loss: 218.1639\n",
      "Iteration: 65500\n",
      "Gradient: [[  -2.5445   10.5553  -73.2838 -229.2024 -388.9578]]\n",
      "Weights: [[-4.2369 -1.2764  0.0352  0.122   0.0599]]\n",
      "MSE loss: 218.0735\n",
      "Iteration: 65600\n",
      "Gradient: [[  16.0955   -0.8216   18.9266 -262.6184 -665.3607]]\n",
      "Weights: [[-4.2357 -1.277   0.0352  0.122   0.0599]]\n",
      "MSE loss: 217.9841\n",
      "Iteration: 65700\n",
      "Gradient: [[ 4.333  17.123  51.1083 53.5516 81.1549]]\n",
      "Weights: [[-4.2362 -1.2777  0.0351  0.122   0.0599]]\n",
      "MSE loss: 217.8601\n",
      "Iteration: 65800\n",
      "Gradient: [[  -4.1411   -1.5922  -31.3692  -91.9412 -101.0437]]\n",
      "Weights: [[-4.2361 -1.2779  0.035   0.1221  0.06  ]]\n",
      "MSE loss: 217.7803\n",
      "Iteration: 65900\n",
      "Gradient: [[  -1.2158  -12.759    17.0987 -164.8726 -272.4366]]\n",
      "Weights: [[-4.2346 -1.2781  0.0349  0.1221  0.06  ]]\n",
      "MSE loss: 217.6949\n",
      "Iteration: 66000\n",
      "Gradient: [[ -6.864   23.241    0.4899 -43.4355  45.8326]]\n",
      "Weights: [[-4.2353 -1.2788  0.0348  0.1221  0.06  ]]\n",
      "MSE loss: 217.5906\n",
      "Iteration: 66100\n",
      "Gradient: [[  -0.9434  -43.4902   41.0373 -220.8703 -408.849 ]]\n",
      "Weights: [[-4.235  -1.2791  0.0347  0.1221  0.06  ]]\n",
      "MSE loss: 217.5149\n",
      "Iteration: 66200\n",
      "Gradient: [[  -4.5106   16.0593   55.3643   33.2794 -195.1724]]\n",
      "Weights: [[-4.2341 -1.2794  0.0346  0.1222  0.0601]]\n",
      "MSE loss: 217.4293\n",
      "Iteration: 66300\n",
      "Gradient: [[  -8.1719    6.6561  -23.0231 -150.9074    9.9295]]\n",
      "Weights: [[-4.2341 -1.2796  0.0344  0.1222  0.0601]]\n",
      "MSE loss: 217.3453\n",
      "Iteration: 66400\n",
      "Gradient: [[ 1.260000e-01 -1.738690e+01  4.601970e+01 -1.007976e+02  5.680739e+02]]\n",
      "Weights: [[-4.2334 -1.2801  0.0343  0.1222  0.0601]]\n",
      "MSE loss: 217.2496\n",
      "Iteration: 66500\n",
      "Gradient: [[  -4.975     1.7167   52.1769  -89.2033 -609.7559]]\n",
      "Weights: [[-4.2335 -1.2805  0.0342  0.1223  0.0601]]\n",
      "MSE loss: 217.167\n",
      "Iteration: 66600\n",
      "Gradient: [[ -20.5264  -12.911   -63.314    65.5181 -416.8791]]\n",
      "Weights: [[-4.2338 -1.2809  0.0341  0.1223  0.0601]]\n",
      "MSE loss: 217.0933\n",
      "Iteration: 66700\n",
      "Gradient: [[  -1.4491  -10.9576  -23.0145   56.4086 -795.2501]]\n",
      "Weights: [[-4.2324 -1.2812  0.034   0.1223  0.0602]]\n",
      "MSE loss: 217.0161\n",
      "Iteration: 66800\n",
      "Gradient: [[ -6.5055   0.5261  36.7665 103.6046  62.6728]]\n",
      "Weights: [[-4.2306 -1.2815  0.0339  0.1223  0.0602]]\n",
      "MSE loss: 216.9347\n",
      "Iteration: 66900\n",
      "Gradient: [[  -7.2113    1.7889   68.682    59.8681 -174.1246]]\n",
      "Weights: [[-4.2311 -1.282   0.0337  0.1223  0.0602]]\n",
      "MSE loss: 216.8424\n",
      "Iteration: 67000\n",
      "Gradient: [[   0.9802   19.0773   17.6106   67.2136 -378.33  ]]\n",
      "Weights: [[-4.2306 -1.2823  0.0336  0.1224  0.0602]]\n",
      "MSE loss: 216.7692\n",
      "Iteration: 67100\n",
      "Gradient: [[ -21.6515  -23.4133  -37.5939  103.6515 -276.8961]]\n",
      "Weights: [[-4.2294 -1.2827  0.0335  0.1224  0.0603]]\n",
      "MSE loss: 216.6799\n",
      "Iteration: 67200\n",
      "Gradient: [[  -9.7739    4.5799  -10.2265  -32.3088 -341.6853]]\n",
      "Weights: [[-4.229  -1.2828  0.0333  0.1224  0.0603]]\n",
      "MSE loss: 216.6034\n",
      "Iteration: 67300\n",
      "Gradient: [[-12.7937  26.2702  20.3796 -93.5581 472.6571]]\n",
      "Weights: [[-4.2282 -1.2835  0.0333  0.1224  0.0603]]\n",
      "MSE loss: 216.5008\n",
      "Iteration: 67400\n",
      "Gradient: [[   4.4561   -8.8983  -39.9384    8.3987 -148.7261]]\n",
      "Weights: [[-4.228  -1.284   0.0331  0.1224  0.0603]]\n",
      "MSE loss: 216.4231\n",
      "Iteration: 67500\n",
      "Gradient: [[  -0.6092   28.1123   -7.4137  143.5862 -544.502 ]]\n",
      "Weights: [[-4.2293 -1.2842  0.0331  0.1224  0.0603]]\n",
      "MSE loss: 216.3507\n",
      "Iteration: 67600\n",
      "Gradient: [[  16.465    11.2749   67.6245   -6.3114 -201.5204]]\n",
      "Weights: [[-4.2282 -1.2844  0.033   0.1225  0.0604]]\n",
      "MSE loss: 216.2705\n",
      "Iteration: 67700\n",
      "Gradient: [[   2.1138    9.8553   43.4647 -111.1819   -4.9228]]\n",
      "Weights: [[-4.2288 -1.285   0.0328  0.1225  0.0604]]\n",
      "MSE loss: 216.1836\n",
      "Iteration: 67800\n",
      "Gradient: [[   2.2364   -7.5067  -40.0853  -96.9081 -471.0735]]\n",
      "Weights: [[-4.2288 -1.285   0.0327  0.1225  0.0604]]\n",
      "MSE loss: 216.1094\n",
      "Iteration: 67900\n",
      "Gradient: [[  -9.079    -6.7595   -9.1385 -123.3599 -506.9199]]\n",
      "Weights: [[-4.2287 -1.2854  0.0326  0.1225  0.0604]]\n",
      "MSE loss: 216.0352\n",
      "Iteration: 68000\n",
      "Gradient: [[   1.2407  -11.0543   19.2063  -42.8774 -373.0371]]\n",
      "Weights: [[-4.2292 -1.2855  0.0325  0.1225  0.0605]]\n",
      "MSE loss: 215.9741\n",
      "Iteration: 68100\n",
      "Gradient: [[  -3.4823   28.9967  -19.9348 -108.2622 -317.2277]]\n",
      "Weights: [[-4.2277 -1.2858  0.0323  0.1225  0.0605]]\n",
      "MSE loss: 215.8879\n",
      "Iteration: 68200\n",
      "Gradient: [[   1.3466    5.4821   44.3031 -156.0524  122.243 ]]\n",
      "Weights: [[-4.2274 -1.2862  0.0322  0.1225  0.0605]]\n",
      "MSE loss: 215.8082\n",
      "Iteration: 68300\n",
      "Gradient: [[  -7.5684  -14.3728  -55.0534   60.8285 -362.4857]]\n",
      "Weights: [[-4.2272 -1.2861  0.0321  0.1226  0.0605]]\n",
      "MSE loss: 215.7515\n",
      "Iteration: 68400\n",
      "Gradient: [[-12.4525   9.775  -61.4451  -1.8243 -60.7492]]\n",
      "Weights: [[-4.2266 -1.2864  0.032   0.1226  0.0605]]\n",
      "MSE loss: 215.6828\n",
      "Iteration: 68500\n",
      "Gradient: [[ 14.7449  16.5046  31.3935 -69.0851 125.203 ]]\n",
      "Weights: [[-4.2256 -1.2868  0.0319  0.1226  0.0606]]\n",
      "MSE loss: 215.6042\n",
      "Iteration: 68600\n",
      "Gradient: [[  -9.9543   -9.6792   54.3695 -101.3847 -249.7737]]\n",
      "Weights: [[-4.2253 -1.2872  0.0318  0.1226  0.0606]]\n",
      "MSE loss: 215.5248\n",
      "Iteration: 68700\n",
      "Gradient: [[ -5.6192  32.7153  22.787  -59.8947  95.8609]]\n",
      "Weights: [[-4.2258 -1.2877  0.0316  0.1226  0.0606]]\n",
      "MSE loss: 215.4433\n",
      "Iteration: 68800\n",
      "Gradient: [[ -30.4385    2.7771  -30.4889   92.651  -296.3798]]\n",
      "Weights: [[-4.2248 -1.288   0.0315  0.1226  0.0606]]\n",
      "MSE loss: 215.3585\n",
      "Iteration: 68900\n",
      "Gradient: [[  9.0594  13.9775  31.9413 -51.7576 225.758 ]]\n",
      "Weights: [[-4.224  -1.2883  0.0314  0.1227  0.0607]]\n",
      "MSE loss: 215.2752\n",
      "Iteration: 69000\n",
      "Gradient: [[  -6.7343    6.5395   20.3505  187.2646 -494.8683]]\n",
      "Weights: [[-4.2229 -1.2888  0.0312  0.1227  0.0607]]\n",
      "MSE loss: 215.1891\n",
      "Iteration: 69100\n",
      "Gradient: [[   4.2756   -7.1646  -14.0982 -143.8789 -271.945 ]]\n",
      "Weights: [[-4.2222 -1.2893  0.0312  0.1227  0.0607]]\n",
      "MSE loss: 215.1128\n",
      "Iteration: 69200\n",
      "Gradient: [[ 14.3277   3.0581  27.1248 -58.8174 -90.7579]]\n",
      "Weights: [[-4.2216 -1.2897  0.031   0.1227  0.0607]]\n",
      "MSE loss: 215.0289\n",
      "Iteration: 69300\n",
      "Gradient: [[  23.6504   13.8912   14.0541  -49.0135 -361.084 ]]\n",
      "Weights: [[-4.2216 -1.2899  0.0309  0.1227  0.0608]]\n",
      "MSE loss: 214.9613\n",
      "Iteration: 69400\n",
      "Gradient: [[  -1.4281   18.6578   32.0005 -147.7764  547.5083]]\n",
      "Weights: [[-4.2215 -1.2904  0.0308  0.1227  0.0608]]\n",
      "MSE loss: 214.8774\n",
      "Iteration: 69500\n",
      "Gradient: [[  -1.6658   29.5063  -19.2423 -204.4347 -271.7022]]\n",
      "Weights: [[-4.2201 -1.2906  0.0307  0.1227  0.0608]]\n",
      "MSE loss: 214.816\n",
      "Iteration: 69600\n",
      "Gradient: [[  10.0186    2.5007  -63.9332   49.9647 -388.6033]]\n",
      "Weights: [[-4.2205 -1.2909  0.0306  0.1227  0.0608]]\n",
      "MSE loss: 214.7453\n",
      "Iteration: 69700\n",
      "Gradient: [[  -6.7061   -4.3096   88.4423   64.6189 -162.8124]]\n",
      "Weights: [[-4.2207 -1.2911  0.0305  0.1228  0.0608]]\n",
      "MSE loss: 214.6757\n",
      "Iteration: 69800\n",
      "Gradient: [[   7.2866  -10.1179   40.2267   12.7314 -274.3028]]\n",
      "Weights: [[-4.2198 -1.2912  0.0303  0.1228  0.0609]]\n",
      "MSE loss: 214.6073\n",
      "Iteration: 69900\n",
      "Gradient: [[  -7.8239   -5.6563   28.5225 -274.6286    0.9698]]\n",
      "Weights: [[-4.2202 -1.2917  0.0302  0.1228  0.0609]]\n",
      "MSE loss: 214.5473\n",
      "Iteration: 70000\n",
      "Gradient: [[  -0.4141  -29.0753   10.8113  -46.1574 -250.2248]]\n",
      "Weights: [[-4.2187 -1.2921  0.0301  0.1228  0.0609]]\n",
      "MSE loss: 214.4691\n",
      "Iteration: 70100\n",
      "Gradient: [[   5.8895   -3.5108  -42.2933  -67.876  -593.5337]]\n",
      "Weights: [[-4.2189 -1.2923  0.03    0.1228  0.0609]]\n",
      "MSE loss: 214.3951\n",
      "Iteration: 70200\n",
      "Gradient: [[  11.104    16.5638  -22.3645   17.6059 -186.8384]]\n",
      "Weights: [[-4.2176 -1.2929  0.0299  0.1228  0.0609]]\n",
      "MSE loss: 214.3202\n",
      "Iteration: 70300\n",
      "Gradient: [[  3.4753  -0.1963  64.3301 -75.4122  -9.6615]]\n",
      "Weights: [[-4.2162 -1.2931  0.0298  0.1228  0.061 ]]\n",
      "MSE loss: 214.2561\n",
      "Iteration: 70400\n",
      "Gradient: [[  -6.7964  -13.3181   15.5446   68.4811 -327.367 ]]\n",
      "Weights: [[-4.2156 -1.2934  0.0297  0.1228  0.061 ]]\n",
      "MSE loss: 214.2059\n",
      "Iteration: 70500\n",
      "Gradient: [[ -1.9616  -7.1032   4.969  -49.0952 -70.9333]]\n",
      "Weights: [[-4.2161 -1.2935  0.0295  0.1228  0.061 ]]\n",
      "MSE loss: 214.1322\n",
      "Iteration: 70600\n",
      "Gradient: [[  -6.5389  -15.323    55.2558  -58.0456 -479.7107]]\n",
      "Weights: [[-4.216  -1.2937  0.0294  0.1229  0.061 ]]\n",
      "MSE loss: 214.0738\n",
      "Iteration: 70700\n",
      "Gradient: [[ -3.3126   5.9715 -17.1362 -47.927    9.2241]]\n",
      "Weights: [[-4.2155 -1.2939  0.0292  0.1229  0.061 ]]\n",
      "MSE loss: 214.0056\n",
      "Iteration: 70800\n",
      "Gradient: [[  -8.7886   23.7163   42.8785  -32.4206 -185.2079]]\n",
      "Weights: [[-4.2153 -1.2944  0.0291  0.1229  0.0611]]\n",
      "MSE loss: 213.913\n",
      "Iteration: 70900\n",
      "Gradient: [[   4.5719  -14.7855  -21.8336   27.4665 -380.0195]]\n",
      "Weights: [[-4.2149 -1.2949  0.029   0.1229  0.0611]]\n",
      "MSE loss: 213.834\n",
      "Iteration: 71000\n",
      "Gradient: [[   8.5503   -7.833    26.5158  -74.4758 -139.67  ]]\n",
      "Weights: [[-4.215  -1.295   0.0288  0.1229  0.0611]]\n",
      "MSE loss: 213.7611\n",
      "Iteration: 71100\n",
      "Gradient: [[  -5.8165  -47.2511    5.148  -107.0545 -247.5094]]\n",
      "Weights: [[-4.2142 -1.2953  0.0287  0.1229  0.0611]]\n",
      "MSE loss: 213.6904\n",
      "Iteration: 71200\n",
      "Gradient: [[ -3.3253  -6.4097   7.1648 -13.3037  82.9867]]\n",
      "Weights: [[-4.2128 -1.2955  0.0285  0.123   0.0611]]\n",
      "MSE loss: 213.6241\n",
      "Iteration: 71300\n",
      "Gradient: [[ -14.2246   -7.4324  -10.8818 -193.1334 -387.9031]]\n",
      "Weights: [[-4.2131 -1.2961  0.0284  0.123   0.0612]]\n",
      "MSE loss: 213.553\n",
      "Iteration: 71400\n",
      "Gradient: [[  -0.9425  -61.3729  -13.6218   40.9913 -193.0185]]\n",
      "Weights: [[-4.2134 -1.2965  0.0283  0.123   0.0612]]\n",
      "MSE loss: 213.4827\n",
      "Iteration: 71500\n",
      "Gradient: [[   4.4064   -3.8229   13.3993 -101.5614 -473.324 ]]\n",
      "Weights: [[-4.2139 -1.2965  0.0282  0.123   0.0612]]\n",
      "MSE loss: 213.4187\n",
      "Iteration: 71600\n",
      "Gradient: [[   1.4653   -8.5044    9.8929 -172.8179 -544.136 ]]\n",
      "Weights: [[-4.2131 -1.2967  0.0281  0.123   0.0612]]\n",
      "MSE loss: 213.3458\n",
      "Iteration: 71700\n",
      "Gradient: [[ 14.1104  -6.939   22.7001   3.1691 276.9971]]\n",
      "Weights: [[-4.2134 -1.2971  0.0279  0.1231  0.0613]]\n",
      "MSE loss: 213.2718\n",
      "Iteration: 71800\n",
      "Gradient: [[  16.0836  -12.1607   75.3171  -73.1074 -690.7829]]\n",
      "Weights: [[-4.2129 -1.2973  0.0278  0.1231  0.0613]]\n",
      "MSE loss: 213.2068\n",
      "Iteration: 71900\n",
      "Gradient: [[ -6.2081 -17.5921 -24.5486  46.3896 233.849 ]]\n",
      "Weights: [[-4.2135 -1.2974  0.0277  0.1231  0.0613]]\n",
      "MSE loss: 213.1517\n",
      "Iteration: 72000\n",
      "Gradient: [[  0.7152   0.3325 -43.7056  62.4568  88.9519]]\n",
      "Weights: [[-4.2121 -1.2978  0.0276  0.1231  0.0613]]\n",
      "MSE loss: 213.0837\n",
      "Iteration: 72100\n",
      "Gradient: [[   3.2427    9.6043   21.9706 -118.1456 -260.5121]]\n",
      "Weights: [[-4.2124 -1.2981  0.0276  0.1231  0.0613]]\n",
      "MSE loss: 213.0427\n",
      "Iteration: 72200\n",
      "Gradient: [[-10.3617  21.9325   5.8387  -5.8086  63.8796]]\n",
      "Weights: [[-4.2108 -1.2983  0.0275  0.1232  0.0613]]\n",
      "MSE loss: 212.9834\n",
      "Iteration: 72300\n",
      "Gradient: [[  -0.4966   -4.5841  -42.2628   90.4021 -309.6118]]\n",
      "Weights: [[-4.2097 -1.2986  0.0273  0.1232  0.0614]]\n",
      "MSE loss: 212.9292\n",
      "Iteration: 72400\n",
      "Gradient: [[   8.9176  -12.3368   34.614   190.6511 -645.2522]]\n",
      "Weights: [[-4.2096 -1.299   0.0273  0.1232  0.0614]]\n",
      "MSE loss: 212.8641\n",
      "Iteration: 72500\n",
      "Gradient: [[ -2.1941  12.3997  14.8806  60.1515 128.1202]]\n",
      "Weights: [[-4.2085 -1.2992  0.0272  0.1232  0.0614]]\n",
      "MSE loss: 212.8082\n",
      "Iteration: 72600\n",
      "Gradient: [[  -4.3909    4.4955   -3.0192  -55.0385 -260.5428]]\n",
      "Weights: [[-4.2099 -1.2993  0.027   0.1232  0.0614]]\n",
      "MSE loss: 212.746\n",
      "Iteration: 72700\n",
      "Gradient: [[   1.7425   -0.5083   59.6777   -5.9784 -162.1488]]\n",
      "Weights: [[-4.2091 -1.2997  0.0268  0.1232  0.0614]]\n",
      "MSE loss: 212.6814\n",
      "Iteration: 72800\n",
      "Gradient: [[ 3.197400e+00 -1.772000e-01 -3.534270e+01  2.506410e+01 -2.000238e+02]]\n",
      "Weights: [[-4.2077 -1.2999  0.0267  0.1232  0.0615]]\n",
      "MSE loss: 212.6205\n",
      "Iteration: 72900\n",
      "Gradient: [[   1.3204   21.586   -27.8602   -9.7142 -303.6582]]\n",
      "Weights: [[-4.2081 -1.3001  0.0265  0.1233  0.0615]]\n",
      "MSE loss: 212.556\n",
      "Iteration: 73000\n",
      "Gradient: [[ 1.900000e-03 -2.149350e+01  3.245560e+01  2.318927e+02 -1.072166e+02]]\n",
      "Weights: [[-4.2078 -1.3004  0.0264  0.1233  0.0615]]\n",
      "MSE loss: 212.4946\n",
      "Iteration: 73100\n",
      "Gradient: [[ -6.225    3.444  -23.0555 187.0803 221.6645]]\n",
      "Weights: [[-4.2067 -1.3006  0.0263  0.1233  0.0615]]\n",
      "MSE loss: 212.4396\n",
      "Iteration: 73200\n",
      "Gradient: [[   2.7374  -31.0572  -27.8746 -145.8863  -12.4124]]\n",
      "Weights: [[-4.2048 -1.301   0.0262  0.1233  0.0615]]\n",
      "MSE loss: 212.3683\n",
      "Iteration: 73300\n",
      "Gradient: [[   4.8473   -5.018    28.904  -304.1452 -546.922 ]]\n",
      "Weights: [[-4.2059 -1.3013  0.026   0.1233  0.0616]]\n",
      "MSE loss: 212.3039\n",
      "Iteration: 73400\n",
      "Gradient: [[   4.6418   12.9488  -39.2478  -13.0222 -441.1595]]\n",
      "Weights: [[-4.2057 -1.3016  0.0259  0.1233  0.0616]]\n",
      "MSE loss: 212.2314\n",
      "Iteration: 73500\n",
      "Gradient: [[   4.5241   -0.2823   71.88     42.3649 -154.4957]]\n",
      "Weights: [[-4.2055 -1.302   0.0258  0.1234  0.0616]]\n",
      "MSE loss: 212.1683\n",
      "Iteration: 73600\n",
      "Gradient: [[  -5.8118  -11.0826   38.7296   83.2172 -283.0554]]\n",
      "Weights: [[-4.2055 -1.3024  0.0257  0.1234  0.0616]]\n",
      "MSE loss: 212.0991\n",
      "Iteration: 73700\n",
      "Gradient: [[  -1.4623    2.1254   29.8393  -66.5826 -369.0144]]\n",
      "Weights: [[-4.2037 -1.3025  0.0256  0.1234  0.0616]]\n",
      "MSE loss: 212.0524\n",
      "Iteration: 73800\n",
      "Gradient: [[ 0.4985  3.526  11.5188 -5.1    -4.247 ]]\n",
      "Weights: [[-4.2033 -1.3031  0.0255  0.1234  0.0617]]\n",
      "MSE loss: 211.9809\n",
      "Iteration: 73900\n",
      "Gradient: [[-16.5511  22.4082  60.3508  -6.3382 -52.2668]]\n",
      "Weights: [[-4.2028 -1.3034  0.0254  0.1234  0.0617]]\n",
      "MSE loss: 211.9266\n",
      "Iteration: 74000\n",
      "Gradient: [[  -1.8977   -5.709   -23.946   139.8454 -114.9266]]\n",
      "Weights: [[-4.2041 -1.3039  0.0253  0.1234  0.0617]]\n",
      "MSE loss: 211.8681\n",
      "Iteration: 74100\n",
      "Gradient: [[  13.0626    4.949    44.5419  -19.3906 -233.9149]]\n",
      "Weights: [[-4.2037 -1.304   0.0252  0.1234  0.0617]]\n",
      "MSE loss: 211.8193\n",
      "Iteration: 74200\n",
      "Gradient: [[  4.515   26.6043 -12.764   18.7411 316.2239]]\n",
      "Weights: [[-4.2024 -1.304   0.0251  0.1234  0.0617]]\n",
      "MSE loss: 211.7624\n",
      "Iteration: 74300\n",
      "Gradient: [[  13.8324   -7.017    17.9427  -92.8366 -350.3981]]\n",
      "Weights: [[-4.2012 -1.3043  0.0249  0.1234  0.0617]]\n",
      "MSE loss: 211.7046\n",
      "Iteration: 74400\n",
      "Gradient: [[ -2.8939  -3.1556 -16.1225 -96.9792 -65.3358]]\n",
      "Weights: [[-4.2016 -1.3046  0.0248  0.1234  0.0618]]\n",
      "MSE loss: 211.6596\n",
      "Iteration: 74500\n",
      "Gradient: [[ -0.8367 -16.7909 -57.6185  79.4356 -36.8557]]\n",
      "Weights: [[-4.2019 -1.3049  0.0247  0.1235  0.0618]]\n",
      "MSE loss: 211.6063\n",
      "Iteration: 74600\n",
      "Gradient: [[   2.5122    9.0517   11.8571   39.5628 -326.4505]]\n",
      "Weights: [[-4.2028 -1.3049  0.0246  0.1235  0.0618]]\n",
      "MSE loss: 211.5546\n",
      "Iteration: 74700\n",
      "Gradient: [[   8.6562  -23.8298   20.1878  -84.935  -591.1513]]\n",
      "Weights: [[-4.2035 -1.305   0.0244  0.1235  0.0618]]\n",
      "MSE loss: 211.4974\n",
      "Iteration: 74800\n",
      "Gradient: [[  -7.6059   -5.4035   38.5561   -3.8613 -511.391 ]]\n",
      "Weights: [[-4.2029 -1.3052  0.0243  0.1235  0.0618]]\n",
      "MSE loss: 211.4364\n",
      "Iteration: 74900\n",
      "Gradient: [[ -3.0136 -22.7958  -4.8992   9.3468  26.8626]]\n",
      "Weights: [[-4.2028 -1.3052  0.0243  0.1236  0.0619]]\n",
      "MSE loss: 211.3906\n",
      "Iteration: 75000\n",
      "Gradient: [[ -9.4249   0.7131   4.1123  39.6573 518.3493]]\n",
      "Weights: [[-4.2021 -1.3055  0.0241  0.1236  0.0619]]\n",
      "MSE loss: 211.3222\n",
      "Iteration: 75100\n",
      "Gradient: [[ -4.8651  17.5235  15.1135 -30.592  266.5519]]\n",
      "Weights: [[-4.2019 -1.3056  0.024   0.1236  0.0619]]\n",
      "MSE loss: 211.2712\n",
      "Iteration: 75200\n",
      "Gradient: [[ -12.2941  -12.1234   34.5603  -46.4043 -131.3379]]\n",
      "Weights: [[-4.2015 -1.3059  0.024   0.1236  0.0619]]\n",
      "MSE loss: 211.2224\n",
      "Iteration: 75300\n",
      "Gradient: [[  -5.8405   19.6871   20.9556 -101.4686 -648.7449]]\n",
      "Weights: [[-4.2003 -1.3061  0.0239  0.1236  0.0619]]\n",
      "MSE loss: 211.1758\n",
      "Iteration: 75400\n",
      "Gradient: [[  -4.113     8.6815   62.4839   41.0077 -695.3663]]\n",
      "Weights: [[-4.2005 -1.3065  0.0238  0.1236  0.0619]]\n",
      "MSE loss: 211.1215\n",
      "Iteration: 75500\n",
      "Gradient: [[   6.5556   -3.7024   14.4003    3.4526 -208.9106]]\n",
      "Weights: [[-4.1996 -1.3066  0.0236  0.1236  0.062 ]]\n",
      "MSE loss: 211.0655\n",
      "Iteration: 75600\n",
      "Gradient: [[ -19.9417   23.4085  -18.097    12.5537 -548.4524]]\n",
      "Weights: [[-4.1991 -1.3069  0.0236  0.1236  0.062 ]]\n",
      "MSE loss: 211.0162\n",
      "Iteration: 75700\n",
      "Gradient: [[   4.8757   -8.2402   58.5874 -183.5634 -387.4698]]\n",
      "Weights: [[-4.1987 -1.3071  0.0235  0.1236  0.062 ]]\n",
      "MSE loss: 210.9618\n",
      "Iteration: 75800\n",
      "Gradient: [[ -4.74     4.9327  44.5095 -33.9081  96.561 ]]\n",
      "Weights: [[-4.1992 -1.3074  0.0233  0.1237  0.062 ]]\n",
      "MSE loss: 210.905\n",
      "Iteration: 75900\n",
      "Gradient: [[  -1.6066  -33.0672  102.2844  -31.2643 -389.6344]]\n",
      "Weights: [[-4.1996 -1.3077  0.0233  0.1237  0.062 ]]\n",
      "MSE loss: 210.8499\n",
      "Iteration: 76000\n",
      "Gradient: [[   1.6959   -6.1236   31.2435   74.7192 -418.0343]]\n",
      "Weights: [[-4.1981 -1.3077  0.0232  0.1237  0.0621]]\n",
      "MSE loss: 210.7959\n",
      "Iteration: 76100\n",
      "Gradient: [[  -8.7973   11.6369   31.0398  -19.5378 -551.8218]]\n",
      "Weights: [[-4.198  -1.3078  0.023   0.1237  0.0621]]\n",
      "MSE loss: 210.732\n",
      "Iteration: 76200\n",
      "Gradient: [[ 4.7413  3.892   0.5438 38.7878 20.8541]]\n",
      "Weights: [[-4.1981 -1.3079  0.0229  0.1237  0.0621]]\n",
      "MSE loss: 210.6857\n",
      "Iteration: 76300\n",
      "Gradient: [[  -7.0813    8.4977  -80.0133   34.2067 -251.3854]]\n",
      "Weights: [[-4.1999 -1.3079  0.0227  0.1237  0.0621]]\n",
      "MSE loss: 210.6472\n",
      "Iteration: 76400\n",
      "Gradient: [[  -5.3873  -13.5669   76.8938  108.3619 -431.7349]]\n",
      "Weights: [[-4.1988 -1.308   0.0227  0.1237  0.0621]]\n",
      "MSE loss: 210.6006\n",
      "Iteration: 76500\n",
      "Gradient: [[ -23.2922   -3.0547   42.2804  -71.1495 -258.3051]]\n",
      "Weights: [[-4.1995 -1.3081  0.0226  0.1237  0.0622]]\n",
      "MSE loss: 210.5552\n",
      "Iteration: 76600\n",
      "Gradient: [[ -13.7876   20.2339   47.2217 -185.703  -522.9024]]\n",
      "Weights: [[-4.1988 -1.3083  0.0225  0.1238  0.0622]]\n",
      "MSE loss: 210.5028\n",
      "Iteration: 76700\n",
      "Gradient: [[  -6.6202   -5.3117   30.6562   72.2765 -385.4022]]\n",
      "Weights: [[-4.1972 -1.3086  0.0224  0.1238  0.0622]]\n",
      "MSE loss: 210.4424\n",
      "Iteration: 76800\n",
      "Gradient: [[   0.513    24.2576   12.876   -89.319  -412.6333]]\n",
      "Weights: [[-4.1953 -1.3091  0.0222  0.1238  0.0622]]\n",
      "MSE loss: 210.3781\n",
      "Iteration: 76900\n",
      "Gradient: [[  1.4781  -3.4989 -20.3406 -54.0202 -51.1564]]\n",
      "Weights: [[-4.1961 -1.3093  0.0221  0.1238  0.0622]]\n",
      "MSE loss: 210.3252\n",
      "Iteration: 77000\n",
      "Gradient: [[  -5.932   -17.7209   10.1001   64.885  -359.9888]]\n",
      "Weights: [[-4.1964 -1.3096  0.022   0.1238  0.0622]]\n",
      "MSE loss: 210.2691\n",
      "Iteration: 77100\n",
      "Gradient: [[ -20.8647    4.6363    4.0542 -168.8692 -117.4894]]\n",
      "Weights: [[-4.1964 -1.3099  0.0219  0.1238  0.0623]]\n",
      "MSE loss: 210.2263\n",
      "Iteration: 77200\n",
      "Gradient: [[ -19.2802    0.311   100.8033 -143.094    53.4646]]\n",
      "Weights: [[-4.1963 -1.3101  0.0217  0.1238  0.0623]]\n",
      "MSE loss: 210.167\n",
      "Iteration: 77300\n",
      "Gradient: [[ -3.9527  -8.6575  24.6362 -54.1242 -41.7372]]\n",
      "Weights: [[-4.1962 -1.3101  0.0217  0.1238  0.0623]]\n",
      "MSE loss: 210.1326\n",
      "Iteration: 77400\n",
      "Gradient: [[  10.9234   -8.2674   40.0308 -235.6273 -162.8416]]\n",
      "Weights: [[-4.1965 -1.3103  0.0216  0.1239  0.0623]]\n",
      "MSE loss: 210.0835\n",
      "Iteration: 77500\n",
      "Gradient: [[  -5.4608    6.0417   29.3416   43.9403 -665.0925]]\n",
      "Weights: [[-4.1972 -1.3107  0.0215  0.1239  0.0623]]\n",
      "MSE loss: 210.0462\n",
      "Iteration: 77600\n",
      "Gradient: [[  9.1802  18.3576 -10.755  -10.273  126.8241]]\n",
      "Weights: [[-4.1954 -1.311   0.0215  0.1239  0.0624]]\n",
      "MSE loss: 209.9823\n",
      "Iteration: 77700\n",
      "Gradient: [[  -8.3575   21.3102   59.2783  178.5814 -147.1503]]\n",
      "Weights: [[-4.1956 -1.3113  0.0214  0.1239  0.0624]]\n",
      "MSE loss: 209.9456\n",
      "Iteration: 77800\n",
      "Gradient: [[  -1.1296   -0.1832  -49.5536   21.3042 -151.1052]]\n",
      "Weights: [[-4.1943 -1.3115  0.0213  0.1239  0.0624]]\n",
      "MSE loss: 209.8933\n",
      "Iteration: 77900\n",
      "Gradient: [[   6.8317   -2.8037   24.2904 -258.3558 -144.2135]]\n",
      "Weights: [[-4.1942 -1.3116  0.0212  0.1239  0.0624]]\n",
      "MSE loss: 209.8511\n",
      "Iteration: 78000\n",
      "Gradient: [[ -11.0316   15.7256  -11.1269   63.9339 -582.2465]]\n",
      "Weights: [[-4.1927 -1.312   0.0211  0.1239  0.0624]]\n",
      "MSE loss: 209.7921\n",
      "Iteration: 78100\n",
      "Gradient: [[  -9.5167    8.3469   32.8545    0.9851 -117.0007]]\n",
      "Weights: [[-4.1904 -1.3123  0.0209  0.1239  0.0624]]\n",
      "MSE loss: 209.7387\n",
      "Iteration: 78200\n",
      "Gradient: [[  2.1819  10.1743  66.1654 -81.8503 112.5556]]\n",
      "Weights: [[-4.1905 -1.3127  0.0208  0.124   0.0625]]\n",
      "MSE loss: 209.6816\n",
      "Iteration: 78300\n",
      "Gradient: [[   8.9859   24.7277   24.9966  118.9812 -312.7526]]\n",
      "Weights: [[-4.1901 -1.3127  0.0207  0.124   0.0625]]\n",
      "MSE loss: 209.6381\n",
      "Iteration: 78400\n",
      "Gradient: [[  10.5724   -2.7946   24.9879   66.7112 -436.7501]]\n",
      "Weights: [[-4.191  -1.3129  0.0205  0.124   0.0625]]\n",
      "MSE loss: 209.5917\n",
      "Iteration: 78500\n",
      "Gradient: [[ 11.851   -6.2743 -38.3428 -11.8588 195.9443]]\n",
      "Weights: [[-4.1917 -1.3132  0.0204  0.124   0.0625]]\n",
      "MSE loss: 209.5369\n",
      "Iteration: 78600\n",
      "Gradient: [[   4.5412    7.63      6.7417  -82.501  -350.9963]]\n",
      "Weights: [[-4.1927 -1.3135  0.0203  0.124   0.0625]]\n",
      "MSE loss: 209.4975\n",
      "Iteration: 78700\n",
      "Gradient: [[ -11.0757   24.9035  -14.0054  -74.4761 -157.7495]]\n",
      "Weights: [[-4.191  -1.3137  0.0202  0.124   0.0625]]\n",
      "MSE loss: 209.4337\n",
      "Iteration: 78800\n",
      "Gradient: [[   7.3375   -7.835   -19.1162  -60.3434 -465.8096]]\n",
      "Weights: [[-4.1913 -1.314   0.02    0.124   0.0626]]\n",
      "MSE loss: 209.3892\n",
      "Iteration: 78900\n",
      "Gradient: [[-11.5028   0.1791  28.0104  27.083  -49.7248]]\n",
      "Weights: [[-4.1894 -1.3141  0.0199  0.124   0.0626]]\n",
      "MSE loss: 209.3361\n",
      "Iteration: 79000\n",
      "Gradient: [[   3.2451    3.5724  -34.1675  -88.1787 -403.8338]]\n",
      "Weights: [[-4.1904 -1.3141  0.0198  0.124   0.0626]]\n",
      "MSE loss: 209.2977\n",
      "Iteration: 79100\n",
      "Gradient: [[   4.1863   24.4525   31.5391  -60.8077 -493.3134]]\n",
      "Weights: [[-4.1901 -1.3142  0.0197  0.1241  0.0626]]\n",
      "MSE loss: 209.2583\n",
      "Iteration: 79200\n",
      "Gradient: [[-23.5569   6.4739  30.6168 -30.8121  51.4538]]\n",
      "Weights: [[-4.1899 -1.3145  0.0196  0.124   0.0626]]\n",
      "MSE loss: 209.2191\n",
      "Iteration: 79300\n",
      "Gradient: [[ 1.841000e-01  4.866500e+00 -3.934600e+00  1.884302e+02 -1.626055e+02]]\n",
      "Weights: [[-4.1899 -1.3147  0.0195  0.1241  0.0626]]\n",
      "MSE loss: 209.1761\n",
      "Iteration: 79400\n",
      "Gradient: [[  13.916    22.0798   49.5349   76.5851 -525.6816]]\n",
      "Weights: [[-4.1895 -1.3149  0.0194  0.1241  0.0627]]\n",
      "MSE loss: 209.1271\n",
      "Iteration: 79500\n",
      "Gradient: [[ -17.2548  -17.6921    9.9132   66.9181 -441.639 ]]\n",
      "Weights: [[-4.1896 -1.315   0.0193  0.1241  0.0627]]\n",
      "MSE loss: 209.0837\n",
      "Iteration: 79600\n",
      "Gradient: [[  -8.2015    2.1495    7.6154  -69.7989 -325.6663]]\n",
      "Weights: [[-4.1892 -1.3149  0.0192  0.1241  0.0627]]\n",
      "MSE loss: 209.045\n",
      "Iteration: 79700\n",
      "Gradient: [[  -5.2218   17.0976   65.918   -53.1262 -308.1888]]\n",
      "Weights: [[-4.1883 -1.315   0.0191  0.1241  0.0627]]\n",
      "MSE loss: 208.9993\n",
      "Iteration: 79800\n",
      "Gradient: [[   7.0708    2.9961  -36.9656  -68.6283 -444.6766]]\n",
      "Weights: [[-4.1877 -1.3153  0.0191  0.1241  0.0627]]\n",
      "MSE loss: 208.9614\n",
      "Iteration: 79900\n",
      "Gradient: [[  -6.558   -24.3044   26.611   100.6762 -266.6107]]\n",
      "Weights: [[-4.1882 -1.3155  0.019   0.1241  0.0627]]\n",
      "MSE loss: 208.9263\n",
      "Iteration: 80000\n",
      "Gradient: [[   7.6623   -6.9907   43.4592 -112.6137  123.7177]]\n",
      "Weights: [[-4.1881 -1.3157  0.0189  0.1241  0.0628]]\n",
      "MSE loss: 208.88\n",
      "Iteration: 80100\n",
      "Gradient: [[  21.1969   17.6418  100.3579  -39.8914 -474.1085]]\n",
      "Weights: [[-4.1888 -1.3157  0.0188  0.1242  0.0628]]\n",
      "MSE loss: 208.8405\n",
      "Iteration: 80200\n",
      "Gradient: [[ -2.0931  -9.779    7.7517 179.9719  36.2438]]\n",
      "Weights: [[-4.1892 -1.3157  0.0186  0.1242  0.0628]]\n",
      "MSE loss: 208.7961\n",
      "Iteration: 80300\n",
      "Gradient: [[  -7.664   -32.5914   -1.6808   -1.5961 -379.4113]]\n",
      "Weights: [[-4.189  -1.3156  0.0185  0.1242  0.0628]]\n",
      "MSE loss: 208.7584\n",
      "Iteration: 80400\n",
      "Gradient: [[   8.3064    0.8293   17.6497  -11.9586 -429.9396]]\n",
      "Weights: [[-4.189  -1.3158  0.0184  0.1242  0.0628]]\n",
      "MSE loss: 208.7084\n",
      "Iteration: 80500\n",
      "Gradient: [[  -6.5283  -16.9593    5.3813   27.2338 -233.1667]]\n",
      "Weights: [[-4.1874 -1.3162  0.0182  0.1242  0.0628]]\n",
      "MSE loss: 208.647\n",
      "Iteration: 80600\n",
      "Gradient: [[  1.2594  24.7745  48.8668 -83.1031 211.9762]]\n",
      "Weights: [[-4.187  -1.3165  0.0182  0.1242  0.0629]]\n",
      "MSE loss: 208.6156\n",
      "Iteration: 80700\n",
      "Gradient: [[  -3.4661   10.2325   -4.064   -62.0211 -187.0144]]\n",
      "Weights: [[-4.1867 -1.3167  0.018   0.1242  0.0629]]\n",
      "MSE loss: 208.5648\n",
      "Iteration: 80800\n",
      "Gradient: [[  6.5254  11.9749  65.2961  97.815  133.3203]]\n",
      "Weights: [[-4.1871 -1.3168  0.018   0.1242  0.0629]]\n",
      "MSE loss: 208.5303\n",
      "Iteration: 80900\n",
      "Gradient: [[ -14.8239  -15.5877  -63.8235  127.7011 -142.2877]]\n",
      "Weights: [[-4.1878 -1.3167  0.0179  0.1243  0.0629]]\n",
      "MSE loss: 208.4854\n",
      "Iteration: 81000\n",
      "Gradient: [[  4.3709  -6.9414  15.3304 -70.0803 -28.4594]]\n",
      "Weights: [[-4.1877 -1.3168  0.0178  0.1243  0.0629]]\n",
      "MSE loss: 208.4532\n",
      "Iteration: 81100\n",
      "Gradient: [[   7.9338   -8.5608    2.1555  -19.3859 -574.0996]]\n",
      "Weights: [[-4.1858 -1.317   0.0177  0.1243  0.0629]]\n",
      "MSE loss: 208.4023\n",
      "Iteration: 81200\n",
      "Gradient: [[ -16.8661  -17.164    -0.5243    9.636  -259.917 ]]\n",
      "Weights: [[-4.1852 -1.3172  0.0176  0.1243  0.063 ]]\n",
      "MSE loss: 208.3506\n",
      "Iteration: 81300\n",
      "Gradient: [[  -2.92    -14.976    16.0407   15.968  -632.1986]]\n",
      "Weights: [[-4.1851 -1.3174  0.0174  0.1243  0.063 ]]\n",
      "MSE loss: 208.305\n",
      "Iteration: 81400\n",
      "Gradient: [[  4.4695  -6.2567 -18.4291 -52.4521 -68.1867]]\n",
      "Weights: [[-4.1857 -1.3176  0.0173  0.1243  0.063 ]]\n",
      "MSE loss: 208.2565\n",
      "Iteration: 81500\n",
      "Gradient: [[  -3.8348   31.519   -10.0242   21.5159 -450.0781]]\n",
      "Weights: [[-4.187  -1.3174  0.0172  0.1243  0.063 ]]\n",
      "MSE loss: 208.2282\n",
      "Iteration: 81600\n",
      "Gradient: [[  14.4826   11.9508  -34.6412   44.0693 -272.0077]]\n",
      "Weights: [[-4.1858 -1.3172  0.0171  0.1243  0.063 ]]\n",
      "MSE loss: 208.1875\n",
      "Iteration: 81700\n",
      "Gradient: [[   1.5674  -13.8438   -4.7656   86.57   -240.2947]]\n",
      "Weights: [[-4.1853 -1.3173  0.017   0.1243  0.063 ]]\n",
      "MSE loss: 208.1543\n",
      "Iteration: 81800\n",
      "Gradient: [[  -3.6183  -15.392    82.2884  126.0191 -202.7696]]\n",
      "Weights: [[-4.185  -1.3175  0.0169  0.1243  0.0631]]\n",
      "MSE loss: 208.1074\n",
      "Iteration: 81900\n",
      "Gradient: [[  16.4487   10.839    33.7508   95.4489 -290.9277]]\n",
      "Weights: [[-4.1836 -1.3179  0.0167  0.1243  0.0631]]\n",
      "MSE loss: 208.0549\n",
      "Iteration: 82000\n",
      "Gradient: [[ -5.9233   9.8857  67.8677 156.7746 -32.9058]]\n",
      "Weights: [[-4.1836 -1.3181  0.0166  0.1243  0.0631]]\n",
      "MSE loss: 208.0107\n",
      "Iteration: 82100\n",
      "Gradient: [[   2.7146   10.0663  -36.0742  -31.2034 -182.8314]]\n",
      "Weights: [[-4.1832 -1.3185  0.0164  0.1243  0.0631]]\n",
      "MSE loss: 207.9575\n",
      "Iteration: 82200\n",
      "Gradient: [[   6.0023    0.6667  -17.7248  -82.3014 -428.7456]]\n",
      "Weights: [[-4.1832 -1.3184  0.0163  0.1244  0.0631]]\n",
      "MSE loss: 207.9156\n",
      "Iteration: 82300\n",
      "Gradient: [[  -1.1958    7.7537   13.7242  110.0949 -500.0662]]\n",
      "Weights: [[-4.1823 -1.3186  0.0161  0.1244  0.0631]]\n",
      "MSE loss: 207.8631\n",
      "Iteration: 82400\n",
      "Gradient: [[   7.1954    2.1379   -4.5482   23.6957 -124.7043]]\n",
      "Weights: [[-4.1816 -1.3185  0.016   0.1243  0.0632]]\n",
      "MSE loss: 207.8227\n",
      "Iteration: 82500\n",
      "Gradient: [[   1.8464   11.3129    2.5957   62.3808 -669.0815]]\n",
      "Weights: [[-4.1816 -1.3185  0.0159  0.1244  0.0632]]\n",
      "MSE loss: 207.7859\n",
      "Iteration: 82600\n",
      "Gradient: [[   8.0778    8.5694   38.4229   55.703  -137.027 ]]\n",
      "Weights: [[-4.1825 -1.319   0.0157  0.1244  0.0632]]\n",
      "MSE loss: 207.7248\n",
      "Iteration: 82700\n",
      "Gradient: [[  -2.7123   32.5192   39.4614  -22.5362 -457.2292]]\n",
      "Weights: [[-4.1822 -1.3194  0.0156  0.1244  0.0632]]\n",
      "MSE loss: 207.6769\n",
      "Iteration: 82800\n",
      "Gradient: [[   3.6047   21.085    18.4752 -110.0613    5.8932]]\n",
      "Weights: [[-4.1819 -1.3194  0.0155  0.1244  0.0632]]\n",
      "MSE loss: 207.6316\n",
      "Iteration: 82900\n",
      "Gradient: [[   3.249   -18.9704   -2.2502   47.8251 -168.2853]]\n",
      "Weights: [[-4.1823 -1.3197  0.0154  0.1244  0.0632]]\n",
      "MSE loss: 207.5941\n",
      "Iteration: 83000\n",
      "Gradient: [[-17.1789  -3.4391   3.3246 109.8525 163.5116]]\n",
      "Weights: [[-4.1829 -1.3197  0.0153  0.1244  0.0633]]\n",
      "MSE loss: 207.5625\n",
      "Iteration: 83100\n",
      "Gradient: [[  -1.7541   -1.5316    1.3156  -98.4488 -417.6014]]\n",
      "Weights: [[-4.1832 -1.3199  0.0152  0.1244  0.0633]]\n",
      "MSE loss: 207.5269\n",
      "Iteration: 83200\n",
      "Gradient: [[  -2.1807    0.5782   14.253   -98.0388 -472.9139]]\n",
      "Weights: [[-4.1822 -1.3201  0.0152  0.1244  0.0633]]\n",
      "MSE loss: 207.4736\n",
      "Iteration: 83300\n",
      "Gradient: [[ 1.873000e-01 -2.710050e+01 -1.673250e+01  1.192139e+02 -4.636902e+02]]\n",
      "Weights: [[-4.1825 -1.3202  0.015   0.1245  0.0633]]\n",
      "MSE loss: 207.4352\n",
      "Iteration: 83400\n",
      "Gradient: [[ -5.0117 -11.9514  27.2937 -39.7801  75.5474]]\n",
      "Weights: [[-4.1829 -1.3201  0.0149  0.1245  0.0633]]\n",
      "MSE loss: 207.4092\n",
      "Iteration: 83500\n",
      "Gradient: [[ -15.1803   -8.8765   14.8465 -138.1791 -268.4105]]\n",
      "Weights: [[-4.1827 -1.32    0.0148  0.1245  0.0633]]\n",
      "MSE loss: 207.3671\n",
      "Iteration: 83600\n",
      "Gradient: [[ -8.4566  12.8669   5.2924 148.9329 -68.4628]]\n",
      "Weights: [[-4.1825 -1.32    0.0148  0.1245  0.0634]]\n",
      "MSE loss: 207.3394\n",
      "Iteration: 83700\n",
      "Gradient: [[ -2.8623   2.7688  31.3075 -35.0482 181.9802]]\n",
      "Weights: [[-4.1808 -1.3201  0.0146  0.1245  0.0634]]\n",
      "MSE loss: 207.2786\n",
      "Iteration: 83800\n",
      "Gradient: [[-4.888000e-01 -1.625400e+01 -2.655550e+01 -3.791540e+01 -6.452081e+02]]\n",
      "Weights: [[-4.1796 -1.3204  0.0144  0.1245  0.0634]]\n",
      "MSE loss: 207.2322\n",
      "Iteration: 83900\n",
      "Gradient: [[   0.4674  -12.6437   99.1957 -143.1577   42.1024]]\n",
      "Weights: [[-4.1779 -1.3207  0.0143  0.1245  0.0634]]\n",
      "MSE loss: 207.188\n",
      "Iteration: 84000\n",
      "Gradient: [[   6.1662  -12.0516   22.2657  -22.8509 -205.7231]]\n",
      "Weights: [[-4.1772 -1.3209  0.0142  0.1245  0.0634]]\n",
      "MSE loss: 207.1514\n",
      "Iteration: 84100\n",
      "Gradient: [[  2.6764   2.0865  50.1905  77.2786 -55.0278]]\n",
      "Weights: [[-4.1769 -1.3212  0.014   0.1245  0.0634]]\n",
      "MSE loss: 207.1075\n",
      "Iteration: 84200\n",
      "Gradient: [[   3.2597   -3.1596   19.1513   65.7571 -153.4774]]\n",
      "Weights: [[-4.1774 -1.3213  0.0139  0.1245  0.0634]]\n",
      "MSE loss: 207.0813\n",
      "Iteration: 84300\n",
      "Gradient: [[ -12.5216  -12.959    21.674    -6.1947 -210.7635]]\n",
      "Weights: [[-4.1778 -1.3213  0.0139  0.1245  0.0635]]\n",
      "MSE loss: 207.0477\n",
      "Iteration: 84400\n",
      "Gradient: [[ 4.484000e+00 -9.410000e-02  4.046070e+01  2.573470e+01 -1.131518e+02]]\n",
      "Weights: [[-4.1774 -1.3215  0.0138  0.1245  0.0635]]\n",
      "MSE loss: 207.0159\n",
      "Iteration: 84500\n",
      "Gradient: [[ -4.583    6.6218  20.5061  69.6873 147.8085]]\n",
      "Weights: [[-4.1764 -1.3218  0.0137  0.1245  0.0635]]\n",
      "MSE loss: 206.9846\n",
      "Iteration: 84600\n",
      "Gradient: [[ -11.7011    3.5125  -65.9613   27.9718 -396.8814]]\n",
      "Weights: [[-4.177  -1.3216  0.0136  0.1245  0.0635]]\n",
      "MSE loss: 206.9497\n",
      "Iteration: 84700\n",
      "Gradient: [[ -6.0699  13.7788  33.9635 -14.656  -31.0262]]\n",
      "Weights: [[-4.1779 -1.3214  0.0135  0.1245  0.0635]]\n",
      "MSE loss: 206.914\n",
      "Iteration: 84800\n",
      "Gradient: [[   8.9596   17.3471    9.3575   84.1875 -692.2672]]\n",
      "Weights: [[-4.1783 -1.3215  0.0134  0.1245  0.0635]]\n",
      "MSE loss: 206.8813\n",
      "Iteration: 84900\n",
      "Gradient: [[ -13.8383    7.1636   75.6975  -37.5715 -331.5481]]\n",
      "Weights: [[-4.1777 -1.3214  0.0132  0.1246  0.0636]]\n",
      "MSE loss: 206.8329\n",
      "Iteration: 85000\n",
      "Gradient: [[  -6.7161    7.3056   -4.5844  -65.4049 -313.3577]]\n",
      "Weights: [[-4.1764 -1.3217  0.0132  0.1246  0.0636]]\n",
      "MSE loss: 206.8085\n",
      "Iteration: 85100\n",
      "Gradient: [[  -3.2276   19.2373  -10.1377   71.4288 -994.858 ]]\n",
      "Weights: [[-4.1761 -1.3216  0.0131  0.1246  0.0636]]\n",
      "MSE loss: 206.7775\n",
      "Iteration: 85200\n",
      "Gradient: [[ -6.3567  15.7689   6.5355  18.3689 -23.1044]]\n",
      "Weights: [[-4.1756 -1.3216  0.0129  0.1246  0.0636]]\n",
      "MSE loss: 206.7373\n",
      "Iteration: 85300\n",
      "Gradient: [[ -5.428    4.6078  49.4077   3.7802 171.0587]]\n",
      "Weights: [[-4.1767 -1.3217  0.0128  0.1246  0.0636]]\n",
      "MSE loss: 206.7036\n",
      "Iteration: 85400\n",
      "Gradient: [[  15.9169   -0.6002  -39.0195   15.7989 -272.1496]]\n",
      "Weights: [[-4.1762 -1.3218  0.0127  0.1246  0.0636]]\n",
      "MSE loss: 206.6552\n",
      "Iteration: 85500\n",
      "Gradient: [[  0.9516 -17.1639 -56.6881  67.6553 208.5018]]\n",
      "Weights: [[-4.1768 -1.3218  0.0126  0.1246  0.0636]]\n",
      "MSE loss: 206.623\n",
      "Iteration: 85600\n",
      "Gradient: [[   3.6743  -19.685    14.1625  -36.5886 -229.672 ]]\n",
      "Weights: [[-4.1757 -1.322   0.0125  0.1246  0.0637]]\n",
      "MSE loss: 206.583\n",
      "Iteration: 85700\n",
      "Gradient: [[  -0.7456    1.9804   23.2766   91.6207 -189.0957]]\n",
      "Weights: [[-4.1754 -1.3221  0.0124  0.1246  0.0637]]\n",
      "MSE loss: 206.5458\n",
      "Iteration: 85800\n",
      "Gradient: [[-11.3809  15.0098  34.1539  -5.8926 270.6154]]\n",
      "Weights: [[-4.1748 -1.3223  0.0122  0.1246  0.0637]]\n",
      "MSE loss: 206.5026\n",
      "Iteration: 85900\n",
      "Gradient: [[  -0.893    -3.9757   81.3417   14.1832 -703.8075]]\n",
      "Weights: [[-4.1746 -1.3225  0.0121  0.1246  0.0637]]\n",
      "MSE loss: 206.4663\n",
      "Iteration: 86000\n",
      "Gradient: [[   6.0134   13.8135   -2.351  -100.8669 -353.1385]]\n",
      "Weights: [[-4.1736 -1.3225  0.012   0.1246  0.0637]]\n",
      "MSE loss: 206.4355\n",
      "Iteration: 86100\n",
      "Gradient: [[ 10.3421 -21.797    3.5449 -73.4028 -90.2156]]\n",
      "Weights: [[-4.1746 -1.3227  0.0119  0.1246  0.0637]]\n",
      "MSE loss: 206.3992\n",
      "Iteration: 86200\n",
      "Gradient: [[  -8.3606   29.5836    7.2538  -28.1375 -637.4078]]\n",
      "Weights: [[-4.1753 -1.3229  0.0118  0.1246  0.0637]]\n",
      "MSE loss: 206.3663\n",
      "Iteration: 86300\n",
      "Gradient: [[   9.5059   11.3223  -14.4077    3.884  -208.2855]]\n",
      "Weights: [[-4.1749 -1.3229  0.0117  0.1246  0.0638]]\n",
      "MSE loss: 206.3249\n",
      "Iteration: 86400\n",
      "Gradient: [[  14.2051    9.7446  -33.4536 -133.8238 -427.0568]]\n",
      "Weights: [[-4.1749 -1.323   0.0116  0.1246  0.0638]]\n",
      "MSE loss: 206.2885\n",
      "Iteration: 86500\n",
      "Gradient: [[  -2.9975    7.2769   -7.2439  -46.9115 -295.3934]]\n",
      "Weights: [[-4.1741 -1.3232  0.0115  0.1246  0.0638]]\n",
      "MSE loss: 206.2488\n",
      "Iteration: 86600\n",
      "Gradient: [[  -5.2239   -3.1353  -11.5894  -31.7944 -382.543 ]]\n",
      "Weights: [[-4.1741 -1.3235  0.0114  0.1247  0.0638]]\n",
      "MSE loss: 206.2222\n",
      "Iteration: 86700\n",
      "Gradient: [[   6.6559    1.4487   24.4113 -104.113  -395.6359]]\n",
      "Weights: [[-4.1733 -1.3238  0.0113  0.1247  0.0638]]\n",
      "MSE loss: 206.188\n",
      "Iteration: 86800\n",
      "Gradient: [[   3.023     6.4009  -53.3309  139.8842 -283.0495]]\n",
      "Weights: [[-4.1725 -1.3238  0.0111  0.1247  0.0638]]\n",
      "MSE loss: 206.144\n",
      "Iteration: 86900\n",
      "Gradient: [[ -17.5959   24.5597   -3.8837  -72.7615 -266.3977]]\n",
      "Weights: [[-4.1713 -1.3242  0.011   0.1247  0.0638]]\n",
      "MSE loss: 206.1113\n",
      "Iteration: 87000\n",
      "Gradient: [[ -3.1617   6.3129 -22.3449   7.1471 146.5445]]\n",
      "Weights: [[-4.1723 -1.3242  0.0109  0.1247  0.0639]]\n",
      "MSE loss: 206.0772\n",
      "Iteration: 87100\n",
      "Gradient: [[  8.9656 -16.9238 -35.63    62.6474  20.3261]]\n",
      "Weights: [[-4.1731 -1.3243  0.0109  0.1247  0.0639]]\n",
      "MSE loss: 206.0509\n",
      "Iteration: 87200\n",
      "Gradient: [[  7.1319 -21.5305  13.0009  29.2755  32.787 ]]\n",
      "Weights: [[-4.1742 -1.3244  0.0108  0.1247  0.0639]]\n",
      "MSE loss: 206.0223\n",
      "Iteration: 87300\n",
      "Gradient: [[-10.2115  -7.2563 -33.1963 -61.213  -97.3453]]\n",
      "Weights: [[-4.1736 -1.3243  0.0107  0.1247  0.0639]]\n",
      "MSE loss: 205.9861\n",
      "Iteration: 87400\n",
      "Gradient: [[ -12.1127  -22.5972   19.982   -63.1048 -140.6532]]\n",
      "Weights: [[-4.1736 -1.3243  0.0106  0.1247  0.0639]]\n",
      "MSE loss: 205.9551\n",
      "Iteration: 87500\n",
      "Gradient: [[  -6.1872    3.1923   22.7226  -18.1084 -364.6608]]\n",
      "Weights: [[-4.1723 -1.3244  0.0105  0.1247  0.0639]]\n",
      "MSE loss: 205.9205\n",
      "Iteration: 87600\n",
      "Gradient: [[ 9.07000e-02  4.04270e+00 -1.28480e+01 -7.58734e+01 -2.82409e+02]]\n",
      "Weights: [[-4.1715 -1.3243  0.0104  0.1247  0.0639]]\n",
      "MSE loss: 205.8867\n",
      "Iteration: 87700\n",
      "Gradient: [[  17.7726   34.4869  -41.7528   59.8727 -464.796 ]]\n",
      "Weights: [[-4.1722 -1.3245  0.0103  0.1247  0.064 ]]\n",
      "MSE loss: 205.8516\n",
      "Iteration: 87800\n",
      "Gradient: [[ -14.2437    1.5565    6.3862 -112.6066  164.0864]]\n",
      "Weights: [[-4.1727 -1.3246  0.0102  0.1247  0.064 ]]\n",
      "MSE loss: 205.8202\n",
      "Iteration: 87900\n",
      "Gradient: [[  -8.6704    6.8642   23.8404   25.8442 -254.1384]]\n",
      "Weights: [[-4.173  -1.3246  0.0102  0.1247  0.064 ]]\n",
      "MSE loss: 205.7991\n",
      "Iteration: 88000\n",
      "Gradient: [[-1.648000e-01  1.991440e+01 -1.876220e+01 -9.561400e+00  2.889421e+02]]\n",
      "Weights: [[-4.1723 -1.3247  0.0101  0.1247  0.064 ]]\n",
      "MSE loss: 205.7607\n",
      "Iteration: 88100\n",
      "Gradient: [[-17.4421  -9.7479 -56.8338  68.9187 217.7482]]\n",
      "Weights: [[-4.1724 -1.3248  0.01    0.1247  0.064 ]]\n",
      "MSE loss: 205.7315\n",
      "Iteration: 88200\n",
      "Gradient: [[  -1.3461   -3.8953  -53.4902  -34.7198 -154.0967]]\n",
      "Weights: [[-4.1722 -1.3249  0.01    0.1247  0.064 ]]\n",
      "MSE loss: 205.7048\n",
      "Iteration: 88300\n",
      "Gradient: [[ 9.6553 12.5765  9.0691 30.9552 61.8581]]\n",
      "Weights: [[-4.1731 -1.325   0.0099  0.1247  0.064 ]]\n",
      "MSE loss: 205.6782\n",
      "Iteration: 88400\n",
      "Gradient: [[  2.2681  15.5746 -46.5508 -19.2059  16.8904]]\n",
      "Weights: [[-4.1721 -1.3251  0.0098  0.1248  0.0641]]\n",
      "MSE loss: 205.6413\n",
      "Iteration: 88500\n",
      "Gradient: [[   9.0379   -7.5221   60.7761  102.9298 -321.1599]]\n",
      "Weights: [[-4.172  -1.3253  0.0097  0.1248  0.0641]]\n",
      "MSE loss: 205.6076\n",
      "Iteration: 88600\n",
      "Gradient: [[   3.3812  -11.5035  -59.1285 -263.9416 -345.6896]]\n",
      "Weights: [[-4.1705 -1.3254  0.0096  0.1248  0.0641]]\n",
      "MSE loss: 205.5759\n",
      "Iteration: 88700\n",
      "Gradient: [[  2.6628  15.46   -20.4014  90.848  197.1778]]\n",
      "Weights: [[-4.1698 -1.3258  0.0095  0.1248  0.0641]]\n",
      "MSE loss: 205.5396\n",
      "Iteration: 88800\n",
      "Gradient: [[  0.3799  13.5531  53.1643   5.613  157.5678]]\n",
      "Weights: [[-4.1696 -1.3259  0.0094  0.1248  0.0641]]\n",
      "MSE loss: 205.5057\n",
      "Iteration: 88900\n",
      "Gradient: [[  18.945   -11.0535    8.7284  -97.4125 -115.9765]]\n",
      "Weights: [[-4.1697 -1.326   0.0093  0.1248  0.0641]]\n",
      "MSE loss: 205.4722\n",
      "Iteration: 89000\n",
      "Gradient: [[ 11.6385   9.0009  -6.7968 -69.5384  63.8157]]\n",
      "Weights: [[-4.1692 -1.3258  0.0092  0.1248  0.0642]]\n",
      "MSE loss: 205.4345\n",
      "Iteration: 89100\n",
      "Gradient: [[   2.145   -17.4783    7.7542    9.998  -182.3356]]\n",
      "Weights: [[-4.1683 -1.3259  0.0091  0.1248  0.0642]]\n",
      "MSE loss: 205.4095\n",
      "Iteration: 89200\n",
      "Gradient: [[  -1.3133    9.2193   66.1802   39.9807 -253.2897]]\n",
      "Weights: [[-4.1679 -1.3261  0.009   0.1248  0.0642]]\n",
      "MSE loss: 205.3719\n",
      "Iteration: 89300\n",
      "Gradient: [[-10.705    8.9285  18.5413 -51.9381 196.9535]]\n",
      "Weights: [[-4.1694 -1.3263  0.0088  0.1248  0.0642]]\n",
      "MSE loss: 205.3353\n",
      "Iteration: 89400\n",
      "Gradient: [[  3.4534   1.4714  29.3142 -31.604  -96.8097]]\n",
      "Weights: [[-4.1686 -1.3263  0.0088  0.1248  0.0642]]\n",
      "MSE loss: 205.3116\n",
      "Iteration: 89500\n",
      "Gradient: [[ -12.2654   11.2882  102.6154   84.3199 -250.5567]]\n",
      "Weights: [[-4.1676 -1.3264  0.0087  0.1248  0.0642]]\n",
      "MSE loss: 205.2836\n",
      "Iteration: 89600\n",
      "Gradient: [[  12.0854  -15.1012   59.4256   32.3587 -118.1304]]\n",
      "Weights: [[-4.1671 -1.3265  0.0086  0.1248  0.0642]]\n",
      "MSE loss: 205.2506\n",
      "Iteration: 89700\n",
      "Gradient: [[  -3.9616   30.9225  -49.7291   17.5958 -245.9382]]\n",
      "Weights: [[-4.1677 -1.3266  0.0085  0.1248  0.0642]]\n",
      "MSE loss: 205.2163\n",
      "Iteration: 89800\n",
      "Gradient: [[ 22.8217  -6.5639  64.0172 -97.1985  67.2353]]\n",
      "Weights: [[-4.1664 -1.3265  0.0083  0.1248  0.0643]]\n",
      "MSE loss: 205.1881\n",
      "Iteration: 89900\n",
      "Gradient: [[  20.9762    8.1633   49.8373  169.8777 -528.4809]]\n",
      "Weights: [[-4.1674 -1.3266  0.0082  0.1248  0.0643]]\n",
      "MSE loss: 205.1471\n",
      "Iteration: 90000\n",
      "Gradient: [[ 1.5857 19.1878 18.9924 -2.7071 68.3413]]\n",
      "Weights: [[-4.1671 -1.3266  0.0081  0.1248  0.0643]]\n",
      "MSE loss: 205.1123\n",
      "Iteration: 90100\n",
      "Gradient: [[  -6.9516    2.1576   20.3311  193.9895 -384.8398]]\n",
      "Weights: [[-4.1675 -1.3266  0.008   0.1248  0.0643]]\n",
      "MSE loss: 205.0842\n",
      "Iteration: 90200\n",
      "Gradient: [[   1.8912    1.9003   38.0334  126.2618 -462.7188]]\n",
      "Weights: [[-4.1668 -1.3268  0.0079  0.1248  0.0643]]\n",
      "MSE loss: 205.0556\n",
      "Iteration: 90300\n",
      "Gradient: [[ -5.7974  -5.4899 -19.639   -3.002  112.6375]]\n",
      "Weights: [[-4.166  -1.3269  0.0077  0.1248  0.0643]]\n",
      "MSE loss: 205.0183\n",
      "Iteration: 90400\n",
      "Gradient: [[-19.8394  29.3444  22.809  -48.6466 139.3522]]\n",
      "Weights: [[-4.1662 -1.3272  0.0077  0.1248  0.0643]]\n",
      "MSE loss: 204.984\n",
      "Iteration: 90500\n",
      "Gradient: [[  7.9064  36.5147   0.6894 111.8781 246.482 ]]\n",
      "Weights: [[-4.1659 -1.3274  0.0076  0.1248  0.0644]]\n",
      "MSE loss: 204.9596\n",
      "Iteration: 90600\n",
      "Gradient: [[-2.0794 20.1096 16.1075  0.4834 99.659 ]]\n",
      "Weights: [[-4.1663 -1.3275  0.0075  0.1248  0.0644]]\n",
      "MSE loss: 204.9295\n",
      "Iteration: 90700\n",
      "Gradient: [[   7.5254   26.1365    3.5128   39.5158 -226.0614]]\n",
      "Weights: [[-4.1666 -1.3275  0.0075  0.1248  0.0644]]\n",
      "MSE loss: 204.9022\n",
      "Iteration: 90800\n",
      "Gradient: [[-1.480000e-02 -4.195600e+00  2.963860e+01  8.677600e+01 -2.854426e+02]]\n",
      "Weights: [[-4.1654 -1.3276  0.0074  0.1248  0.0644]]\n",
      "MSE loss: 204.8665\n",
      "Iteration: 90900\n",
      "Gradient: [[  -2.0087   10.9024    0.2976    8.7602 -283.5666]]\n",
      "Weights: [[-4.1653 -1.3276  0.0073  0.1248  0.0644]]\n",
      "MSE loss: 204.8335\n",
      "Iteration: 91000\n",
      "Gradient: [[  10.5646   19.3278   30.0106   48.3715 -229.9106]]\n",
      "Weights: [[-4.1647 -1.3276  0.0071  0.1248  0.0644]]\n",
      "MSE loss: 204.8013\n",
      "Iteration: 91100\n",
      "Gradient: [[  10.5382  -12.5079   10.8126  118.1581 -672.5939]]\n",
      "Weights: [[-4.1643 -1.3276  0.0071  0.1248  0.0644]]\n",
      "MSE loss: 204.7798\n",
      "Iteration: 91200\n",
      "Gradient: [[   5.4536   16.2402  -25.8233 -118.3393  -15.9567]]\n",
      "Weights: [[-4.1644 -1.3279  0.0069  0.1248  0.0645]]\n",
      "MSE loss: 204.7411\n",
      "Iteration: 91300\n",
      "Gradient: [[ -2.6794   0.711    8.2274  67.5743 -78.5574]]\n",
      "Weights: [[-4.1652 -1.328   0.0069  0.1249  0.0645]]\n",
      "MSE loss: 204.71\n",
      "Iteration: 91400\n",
      "Gradient: [[   6.2944    6.1144   -0.6342  -36.1113 -576.7618]]\n",
      "Weights: [[-4.1648 -1.3281  0.0067  0.1249  0.0645]]\n",
      "MSE loss: 204.6748\n",
      "Iteration: 91500\n",
      "Gradient: [[   2.9551   26.0247   16.6307   15.7272 -686.142 ]]\n",
      "Weights: [[-4.1664 -1.3283  0.0066  0.1249  0.0645]]\n",
      "MSE loss: 204.6543\n",
      "Iteration: 91600\n",
      "Gradient: [[ 1.973000e-01 -3.367400e+00 -2.506600e+00 -5.907770e+01  3.899257e+02]]\n",
      "Weights: [[-4.1665 -1.3282  0.0065  0.1249  0.0645]]\n",
      "MSE loss: 204.6139\n",
      "Iteration: 91700\n",
      "Gradient: [[  -7.3939    7.5628    4.7342  159.7698 -402.4519]]\n",
      "Weights: [[-4.1652 -1.3281  0.0064  0.1249  0.0645]]\n",
      "MSE loss: 204.5657\n",
      "Iteration: 91800\n",
      "Gradient: [[ -13.8513  -14.2273   50.5384   53.5972 -446.978 ]]\n",
      "Weights: [[-4.1653 -1.3278  0.0063  0.1249  0.0646]]\n",
      "MSE loss: 204.5379\n",
      "Iteration: 91900\n",
      "Gradient: [[  6.2556 -15.1917  37.6384 -56.3398 182.4492]]\n",
      "Weights: [[-4.1657 -1.3278  0.0061  0.1249  0.0646]]\n",
      "MSE loss: 204.5058\n",
      "Iteration: 92000\n",
      "Gradient: [[ -12.9585  -14.2505   18.6892  131.7305 -129.7163]]\n",
      "Weights: [[-4.1646 -1.328   0.006   0.1249  0.0646]]\n",
      "MSE loss: 204.4694\n",
      "Iteration: 92100\n",
      "Gradient: [[  10.293    20.9833   16.3298   -7.914  -149.3887]]\n",
      "Weights: [[-4.1649 -1.3281  0.0059  0.1249  0.0646]]\n",
      "MSE loss: 204.4347\n",
      "Iteration: 92200\n",
      "Gradient: [[ -5.073    3.9842  -1.3019  37.534  208.155 ]]\n",
      "Weights: [[-4.1658 -1.3282  0.0058  0.1249  0.0646]]\n",
      "MSE loss: 204.4103\n",
      "Iteration: 92300\n",
      "Gradient: [[   2.6546   -8.4921   31.4737   42.9944 -184.7158]]\n",
      "Weights: [[-4.1651 -1.3282  0.0057  0.1249  0.0646]]\n",
      "MSE loss: 204.3676\n",
      "Iteration: 92400\n",
      "Gradient: [[  5.6288 -13.4734  -5.4245 -33.4819 -48.8615]]\n",
      "Weights: [[-4.1649 -1.3283  0.0056  0.1249  0.0646]]\n",
      "MSE loss: 204.3333\n",
      "Iteration: 92500\n",
      "Gradient: [[-10.5685 -26.2236  10.6603 187.9227 259.4058]]\n",
      "Weights: [[-4.1651 -1.3284  0.0055  0.1249  0.0647]]\n",
      "MSE loss: 204.3039\n",
      "Iteration: 92600\n",
      "Gradient: [[  -8.7927    4.2077   32.0819  -48.6707 -313.2419]]\n",
      "Weights: [[-4.1645 -1.3282  0.0054  0.1249  0.0647]]\n",
      "MSE loss: 204.269\n",
      "Iteration: 92700\n",
      "Gradient: [[ -11.6287  -13.5628   37.7792  -15.0358 -104.5947]]\n",
      "Weights: [[-4.1625 -1.3284  0.0052  0.1249  0.0647]]\n",
      "MSE loss: 204.237\n",
      "Iteration: 92800\n",
      "Gradient: [[ -4.8143  -2.6143  28.7701 -15.4663 -71.8144]]\n",
      "Weights: [[-4.1627 -1.3287  0.0051  0.1249  0.0647]]\n",
      "MSE loss: 204.2036\n",
      "Iteration: 92900\n",
      "Gradient: [[  7.202   12.2397  -5.4157 -62.0681 -18.0677]]\n",
      "Weights: [[-4.1624 -1.3289  0.005   0.125   0.0647]]\n",
      "MSE loss: 204.1683\n",
      "Iteration: 93000\n",
      "Gradient: [[   1.9256  -17.8604    7.9944   27.7507 -541.4093]]\n",
      "Weights: [[-4.1621 -1.3288  0.0049  0.125   0.0647]]\n",
      "MSE loss: 204.144\n",
      "Iteration: 93100\n",
      "Gradient: [[ 11.0656  19.9473  10.2427 -46.1976 211.217 ]]\n",
      "Weights: [[-4.1626 -1.3287  0.0048  0.125   0.0647]]\n",
      "MSE loss: 204.112\n",
      "Iteration: 93200\n",
      "Gradient: [[   1.1172    8.5179   22.8157  -38.3227 -333.1391]]\n",
      "Weights: [[-4.1622 -1.3286  0.0047  0.125   0.0648]]\n",
      "MSE loss: 204.0863\n",
      "Iteration: 93300\n",
      "Gradient: [[ -12.6623    5.1226  -46.9175   56.9492 -110.6099]]\n",
      "Weights: [[-4.1624 -1.3287  0.0046  0.125   0.0648]]\n",
      "MSE loss: 204.0568\n",
      "Iteration: 93400\n",
      "Gradient: [[   2.0256    6.6835   40.83    -93.2678 -106.9132]]\n",
      "Weights: [[-4.162  -1.3288  0.0045  0.125   0.0648]]\n",
      "MSE loss: 204.0327\n",
      "Iteration: 93500\n",
      "Gradient: [[   2.9031   26.1105   70.7584 -103.3538 -183.3309]]\n",
      "Weights: [[-4.1621 -1.3289  0.0044  0.125   0.0648]]\n",
      "MSE loss: 204.0016\n",
      "Iteration: 93600\n",
      "Gradient: [[  5.0607   5.0739   0.9868 -66.5146  -3.4722]]\n",
      "Weights: [[-4.1619 -1.3289  0.0043  0.125   0.0648]]\n",
      "MSE loss: 203.9789\n",
      "Iteration: 93700\n",
      "Gradient: [[  -5.9579   15.2315    9.9229   34.5349 -147.3292]]\n",
      "Weights: [[-4.1621 -1.329   0.0042  0.125   0.0648]]\n",
      "MSE loss: 203.9523\n",
      "Iteration: 93800\n",
      "Gradient: [[  -3.8281    2.0846   19.8428  -65.492  -395.4806]]\n",
      "Weights: [[-4.1607e+00 -1.3290e+00  4.1000e-03  1.2500e-01  6.4800e-02]]\n",
      "MSE loss: 203.9245\n",
      "Iteration: 93900\n",
      "Gradient: [[-21.8922 -27.4792  -5.0209  -3.2999   2.2202]]\n",
      "Weights: [[-4.1608e+00 -1.3291e+00  4.0000e-03  1.2500e-01  6.4800e-02]]\n",
      "MSE loss: 203.8956\n",
      "Iteration: 94000\n",
      "Gradient: [[  4.08    17.1883 -46.4992 -78.6106  39.1726]]\n",
      "Weights: [[-4.1611e+00 -1.3290e+00  3.9000e-03  1.2500e-01  6.4900e-02]]\n",
      "MSE loss: 203.8662\n",
      "Iteration: 94100\n",
      "Gradient: [[   6.6518   13.5548  -32.6939  -50.6035 -141.7473]]\n",
      "Weights: [[-4.1601e+00 -1.3291e+00  3.8000e-03  1.2500e-01  6.4900e-02]]\n",
      "MSE loss: 203.8443\n",
      "Iteration: 94200\n",
      "Gradient: [[-9.1289 30.7392 12.0657 69.8274 82.2159]]\n",
      "Weights: [[-4.1616e+00 -1.3291e+00  3.7000e-03  1.2500e-01  6.4900e-02]]\n",
      "MSE loss: 203.8197\n",
      "Iteration: 94300\n",
      "Gradient: [[  3.3556 -11.1098 -49.2389  -8.9643 -75.3097]]\n",
      "Weights: [[-4.1612e+00 -1.3293e+00  3.6000e-03  1.2500e-01  6.4900e-02]]\n",
      "MSE loss: 203.7888\n",
      "Iteration: 94400\n",
      "Gradient: [[ -7.663   31.1964  14.6257 121.9818  19.7237]]\n",
      "Weights: [[-4.1607e+00 -1.3296e+00  3.6000e-03  1.2500e-01  6.4900e-02]]\n",
      "MSE loss: 203.7664\n",
      "Iteration: 94500\n",
      "Gradient: [[  -0.4326   30.4827   -1.2552 -108.5502   23.871 ]]\n",
      "Weights: [[-4.1604e+00 -1.3296e+00  3.5000e-03  1.2500e-01  6.4900e-02]]\n",
      "MSE loss: 203.7439\n",
      "Iteration: 94600\n",
      "Gradient: [[ 6.001000e-01 -1.108690e+01  1.960850e+01 -6.134780e+01 -6.156396e+02]]\n",
      "Weights: [[-4.1601e+00 -1.3298e+00  3.4000e-03  1.2500e-01  6.4900e-02]]\n",
      "MSE loss: 203.7212\n",
      "Iteration: 94700\n",
      "Gradient: [[  10.3045   24.0443   45.2202   34.6246 -111.994 ]]\n",
      "Weights: [[-4.1618e+00 -1.3297e+00  3.3000e-03  1.2500e-01  6.4900e-02]]\n",
      "MSE loss: 203.6884\n",
      "Iteration: 94800\n",
      "Gradient: [[  -2.2046   -4.3077  -26.4914 -154.8473 -357.0449]]\n",
      "Weights: [[-4.1618e+00 -1.3296e+00  3.2000e-03  1.2500e-01  6.5000e-02]]\n",
      "MSE loss: 203.6596\n",
      "Iteration: 94900\n",
      "Gradient: [[  16.4822    8.9463  -18.0464   92.312  -562.0496]]\n",
      "Weights: [[-4.1608e+00 -1.3296e+00  3.1000e-03  1.2500e-01  6.5000e-02]]\n",
      "MSE loss: 203.6243\n",
      "Iteration: 95000\n",
      "Gradient: [[  -1.5708  -17.6893  -17.3721  146.8068 -318.9762]]\n",
      "Weights: [[-4.1610e+00 -1.3296e+00  3.0000e-03  1.2500e-01  6.5000e-02]]\n",
      "MSE loss: 203.5871\n",
      "Iteration: 95100\n",
      "Gradient: [[  -9.735    10.1132    5.6392  116.9012 -488.7838]]\n",
      "Weights: [[-4.1611e+00 -1.3296e+00  2.9000e-03  1.2500e-01  6.5000e-02]]\n",
      "MSE loss: 203.5539\n",
      "Iteration: 95200\n",
      "Gradient: [[  -6.6379  -18.5058   62.1952  -98.685  -381.1166]]\n",
      "Weights: [[-4.1605e+00 -1.3297e+00  2.8000e-03  1.2500e-01  6.5000e-02]]\n",
      "MSE loss: 203.524\n",
      "Iteration: 95300\n",
      "Gradient: [[  -9.9329   -4.3862   15.5005  -43.9094 -169.162 ]]\n",
      "Weights: [[-4.1590e+00 -1.3298e+00  2.7000e-03  1.2500e-01  6.5000e-02]]\n",
      "MSE loss: 203.4973\n",
      "Iteration: 95400\n",
      "Gradient: [[   8.8097    5.14     19.8769  127.7927 -146.2959]]\n",
      "Weights: [[-4.1599e+00 -1.3297e+00  2.6000e-03  1.2500e-01  6.5100e-02]]\n",
      "MSE loss: 203.4644\n",
      "Iteration: 95500\n",
      "Gradient: [[  -1.3482    1.6633  -15.4374  -42.2968 -227.6348]]\n",
      "Weights: [[-4.1604e+00 -1.3297e+00  2.5000e-03  1.2500e-01  6.5100e-02]]\n",
      "MSE loss: 203.4334\n",
      "Iteration: 95600\n",
      "Gradient: [[ 5.7585  7.6665  6.2253 65.2813 70.94  ]]\n",
      "Weights: [[-4.1607e+00 -1.3296e+00  2.4000e-03  1.2500e-01  6.5100e-02]]\n",
      "MSE loss: 203.4095\n",
      "Iteration: 95700\n",
      "Gradient: [[ -5.9079  16.7608  11.8146   9.3962 227.3665]]\n",
      "Weights: [[-4.1611e+00 -1.3295e+00  2.3000e-03  1.2500e-01  6.5100e-02]]\n",
      "MSE loss: 203.39\n",
      "Iteration: 95800\n",
      "Gradient: [[  -9.2774   -1.6244   16.9232   43.3573 -371.5214]]\n",
      "Weights: [[-4.1611e+00 -1.3298e+00  2.3000e-03  1.2500e-01  6.5100e-02]]\n",
      "MSE loss: 203.3652\n",
      "Iteration: 95900\n",
      "Gradient: [[   4.3573   15.9211   37.7134  219.669  -105.4496]]\n",
      "Weights: [[-4.1606e+00 -1.3298e+00  2.2000e-03  1.2500e-01  6.5100e-02]]\n",
      "MSE loss: 203.3386\n",
      "Iteration: 96000\n",
      "Gradient: [[ 10.8625 -28.2816 -16.1375  68.1493 294.9561]]\n",
      "Weights: [[-4.1593e+00 -1.3301e+00  2.1000e-03  1.2510e-01  6.5100e-02]]\n",
      "MSE loss: 203.3067\n",
      "Iteration: 96100\n",
      "Gradient: [[  -7.4093   -0.75     17.9824  -94.9893 -223.3301]]\n",
      "Weights: [[-4.1597e+00 -1.3303e+00  2.1000e-03  1.2510e-01  6.5200e-02]]\n",
      "MSE loss: 203.2841\n",
      "Iteration: 96200\n",
      "Gradient: [[ -10.6664    1.8073    9.2817 -104.0459    6.8439]]\n",
      "Weights: [[-4.1589e+00 -1.3304e+00  2.0000e-03  1.2510e-01  6.5200e-02]]\n",
      "MSE loss: 203.2516\n",
      "Iteration: 96300\n",
      "Gradient: [[  -7.6419    4.3587   45.7143  -50.9741 -355.9927]]\n",
      "Weights: [[-4.1587e+00 -1.3303e+00  1.9000e-03  1.2510e-01  6.5200e-02]]\n",
      "MSE loss: 203.2239\n",
      "Iteration: 96400\n",
      "Gradient: [[ -5.7388   4.2213  38.2234 225.2812  85.5606]]\n",
      "Weights: [[-4.1580e+00 -1.3303e+00  1.7000e-03  1.2500e-01  6.5200e-02]]\n",
      "MSE loss: 203.192\n",
      "Iteration: 96500\n",
      "Gradient: [[   5.8517   -2.717  -103.1483   24.0139  -67.3143]]\n",
      "Weights: [[-4.1576e+00 -1.3306e+00  1.6000e-03  1.2510e-01  6.5200e-02]]\n",
      "MSE loss: 203.1655\n",
      "Iteration: 96600\n",
      "Gradient: [[-7.09000e-02 -1.70800e+00 -3.30923e+01  3.81868e+01  9.70845e+01]]\n",
      "Weights: [[-4.1567e+00 -1.3307e+00  1.5000e-03  1.2500e-01  6.5200e-02]]\n",
      "MSE loss: 203.1408\n",
      "Iteration: 96700\n",
      "Gradient: [[  -3.7278   -4.7298  -35.7306    8.8172 -374.1319]]\n",
      "Weights: [[-4.1558e+00 -1.3308e+00  1.4000e-03  1.2500e-01  6.5200e-02]]\n",
      "MSE loss: 203.1146\n",
      "Iteration: 96800\n",
      "Gradient: [[ -19.9013   -1.0395  -42.5683 -137.9435   63.7162]]\n",
      "Weights: [[-4.1565e+00 -1.3309e+00  1.3000e-03  1.2500e-01  6.5200e-02]]\n",
      "MSE loss: 203.0894\n",
      "Iteration: 96900\n",
      "Gradient: [[ -12.5077   -8.8796    9.4403 -105.6862  289.6537]]\n",
      "Weights: [[-4.1570e+00 -1.3311e+00  1.2000e-03  1.2500e-01  6.5300e-02]]\n",
      "MSE loss: 203.063\n",
      "Iteration: 97000\n",
      "Gradient: [[  -8.2654   11.7359   41.5768    6.7096 -197.3202]]\n",
      "Weights: [[-4.1569e+00 -1.3310e+00  1.1000e-03  1.2500e-01  6.5300e-02]]\n",
      "MSE loss: 203.0352\n",
      "Iteration: 97100\n",
      "Gradient: [[ -2.6049   8.8813  19.7915 105.6674  -0.6341]]\n",
      "Weights: [[-4.1573e+00 -1.3310e+00  1.0000e-03  1.2500e-01  6.5300e-02]]\n",
      "MSE loss: 203.0079\n",
      "Iteration: 97200\n",
      "Gradient: [[  -6.5645   -1.3525   35.1646  -28.8515 -222.0898]]\n",
      "Weights: [[-4.1565e+00 -1.3309e+00  8.0000e-04  1.2500e-01  6.5300e-02]]\n",
      "MSE loss: 202.9748\n",
      "Iteration: 97300\n",
      "Gradient: [[  11.2403   29.1461   46.3104 -143.2182 -107.8274]]\n",
      "Weights: [[-4.1559e+00 -1.3312e+00  7.0000e-04  1.2500e-01  6.5300e-02]]\n",
      "MSE loss: 202.95\n",
      "Iteration: 97400\n",
      "Gradient: [[  -1.3066   14.8823    8.3501   35.3678 -125.4622]]\n",
      "Weights: [[-4.1557e+00 -1.3313e+00  6.0000e-04  1.2510e-01  6.5300e-02]]\n",
      "MSE loss: 202.9211\n",
      "Iteration: 97500\n",
      "Gradient: [[ 4.092000e-01 -2.313350e+01  5.915860e+01  9.327320e+01 -8.867625e+02]]\n",
      "Weights: [[-4.1542e+00 -1.3315e+00  5.0000e-04  1.2510e-01  6.5300e-02]]\n",
      "MSE loss: 202.9039\n",
      "Iteration: 97600\n",
      "Gradient: [[  -1.688    22.6251    8.7261   39.2058 -305.4179]]\n",
      "Weights: [[-4.1543e+00 -1.3317e+00  4.0000e-04  1.2510e-01  6.5300e-02]]\n",
      "MSE loss: 202.8764\n",
      "Iteration: 97700\n",
      "Gradient: [[   9.3931   -1.7783   10.2587   85.3284 -246.2947]]\n",
      "Weights: [[-4.1544e+00 -1.3318e+00  3.0000e-04  1.2510e-01  6.5300e-02]]\n",
      "MSE loss: 202.8492\n",
      "Iteration: 97800\n",
      "Gradient: [[   4.9795   -0.9092    9.1086   15.5812 -135.256 ]]\n",
      "Weights: [[-4.1539e+00 -1.3318e+00  2.0000e-04  1.2510e-01  6.5400e-02]]\n",
      "MSE loss: 202.82\n",
      "Iteration: 97900\n",
      "Gradient: [[   2.5781   -1.2506  -40.5981  -15.016  -456.4886]]\n",
      "Weights: [[-4.1543e+00 -1.3318e+00  1.0000e-04  1.2510e-01  6.5400e-02]]\n",
      "MSE loss: 202.7881\n",
      "Iteration: 98000\n",
      "Gradient: [[  -2.3323  -28.5867  -22.4539  -15.6882 -178.0799]]\n",
      "Weights: [[-4.1543 -1.3316  0.      0.1251  0.0654]]\n",
      "MSE loss: 202.7652\n",
      "Iteration: 98100\n",
      "Gradient: [[  -8.4048   -4.7369  -22.458   -13.3679 -116.2877]]\n",
      "Weights: [[-4.1555 -1.3315 -0.      0.1251  0.0654]]\n",
      "MSE loss: 202.7399\n",
      "Iteration: 98200\n",
      "Gradient: [[ -2.0034  -8.2511  56.8293 -80.1419   1.2779]]\n",
      "Weights: [[-4.1565e+00 -1.3315e+00 -1.0000e-04  1.2510e-01  6.5400e-02]]\n",
      "MSE loss: 202.7259\n",
      "Iteration: 98300\n",
      "Gradient: [[-10.3035  17.8053  -1.7121  36.4911 -94.1601]]\n",
      "Weights: [[-4.1552e+00 -1.3314e+00 -1.0000e-04  1.2510e-01  6.5400e-02]]\n",
      "MSE loss: 202.7002\n",
      "Iteration: 98400\n",
      "Gradient: [[  1.4986  -1.2053  55.9177 -19.5994  -5.2518]]\n",
      "Weights: [[-4.1559e+00 -1.3314e+00 -1.0000e-04  1.2510e-01  6.5400e-02]]\n",
      "MSE loss: 202.6812\n",
      "Iteration: 98500\n",
      "Gradient: [[  -8.1389   21.0313  -25.3495  -82.6966 -298.9482]]\n",
      "Weights: [[-4.1553e+00 -1.3312e+00 -2.0000e-04  1.2510e-01  6.5500e-02]]\n",
      "MSE loss: 202.6614\n",
      "Iteration: 98600\n",
      "Gradient: [[   1.621    -0.8567   34.2385  178.6773 -594.1539]]\n",
      "Weights: [[-4.1559e+00 -1.3314e+00 -3.0000e-04  1.2510e-01  6.5500e-02]]\n",
      "MSE loss: 202.6355\n",
      "Iteration: 98700\n",
      "Gradient: [[  -2.8664   -1.4614   66.9928   20.7041 -139.6765]]\n",
      "Weights: [[-4.1561e+00 -1.3315e+00 -5.0000e-04  1.2510e-01  6.5500e-02]]\n",
      "MSE loss: 202.6059\n",
      "Iteration: 98800\n",
      "Gradient: [[ -10.7762   -5.2216    0.4572   -6.137  -398.6452]]\n",
      "Weights: [[-4.1552e+00 -1.3316e+00 -6.0000e-04  1.2510e-01  6.5500e-02]]\n",
      "MSE loss: 202.5763\n",
      "Iteration: 98900\n",
      "Gradient: [[  2.0865 -11.2034  39.8578  45.0605 114.9386]]\n",
      "Weights: [[-4.1551e+00 -1.3317e+00 -6.0000e-04  1.2510e-01  6.5500e-02]]\n",
      "MSE loss: 202.5508\n",
      "Iteration: 99000\n",
      "Gradient: [[  -4.9738  -15.4272   42.3673 -104.419   209.7291]]\n",
      "Weights: [[-4.1552e+00 -1.3318e+00 -8.0000e-04  1.2510e-01  6.5500e-02]]\n",
      "MSE loss: 202.5238\n",
      "Iteration: 99100\n",
      "Gradient: [[   6.8902   -4.2276   -6.1154 -106.0991 -276.8369]]\n",
      "Weights: [[-4.1554e+00 -1.3317e+00 -9.0000e-04  1.2510e-01  6.5500e-02]]\n",
      "MSE loss: 202.4914\n",
      "Iteration: 99200\n",
      "Gradient: [[   4.4891   -0.6161   -1.7025 -129.7287 -416.4329]]\n",
      "Weights: [[-4.1556e+00 -1.3315e+00 -1.0000e-03  1.2510e-01  6.5500e-02]]\n",
      "MSE loss: 202.467\n",
      "Iteration: 99300\n",
      "Gradient: [[   5.9607   11.1397  -12.6598 -245.2041 -401.1237]]\n",
      "Weights: [[-4.1559e+00 -1.3315e+00 -1.1000e-03  1.2510e-01  6.5600e-02]]\n",
      "MSE loss: 202.4425\n",
      "Iteration: 99400\n",
      "Gradient: [[ -11.1069  -12.5562  -22.3481  -34.7384 -244.734 ]]\n",
      "Weights: [[-4.1559e+00 -1.3315e+00 -1.1000e-03  1.2510e-01  6.5600e-02]]\n",
      "MSE loss: 202.4208\n",
      "Iteration: 99500\n",
      "Gradient: [[   3.1889    4.9343  -32.1183 -116.0789 -480.964 ]]\n",
      "Weights: [[-4.1557e+00 -1.3314e+00 -1.2000e-03  1.2510e-01  6.5600e-02]]\n",
      "MSE loss: 202.3981\n",
      "Iteration: 99600\n",
      "Gradient: [[  19.1976   21.3334   66.2916   53.6354 -291.5968]]\n",
      "Weights: [[-4.1550e+00 -1.3314e+00 -1.3000e-03  1.2510e-01  6.5600e-02]]\n",
      "MSE loss: 202.3734\n",
      "Iteration: 99700\n",
      "Gradient: [[  6.6108  -4.5568  30.1464 -46.6046 203.5815]]\n",
      "Weights: [[-4.1548e+00 -1.3315e+00 -1.4000e-03  1.2520e-01  6.5600e-02]]\n",
      "MSE loss: 202.3472\n",
      "Iteration: 99800\n",
      "Gradient: [[-16.2861  -0.5213  53.5928  -8.5573  -7.273 ]]\n",
      "Weights: [[-4.1544e+00 -1.3314e+00 -1.5000e-03  1.2520e-01  6.5600e-02]]\n",
      "MSE loss: 202.329\n",
      "Iteration: 99900\n",
      "Gradient: [[  -3.8203  -15.3511   56.05    200.2538 -355.1806]]\n",
      "Weights: [[-4.1548e+00 -1.3315e+00 -1.6000e-03  1.2520e-01  6.5600e-02]]\n",
      "MSE loss: 202.3017\n",
      "Iteration: 100000\n",
      "Gradient: [[   1.9345    0.179     2.4334 -169.5332  -58.9952]]\n",
      "Weights: [[-4.1546e+00 -1.3316e+00 -1.7000e-03  1.2510e-01  6.5600e-02]]\n",
      "MSE loss: 202.2766\n",
      "Iteration: 100100\n",
      "Gradient: [[   3.122   -10.2646   67.8606  -22.4587 -487.9635]]\n",
      "Weights: [[-4.1550e+00 -1.3316e+00 -1.8000e-03  1.2520e-01  6.5600e-02]]\n",
      "MSE loss: 202.2586\n",
      "Iteration: 100200\n",
      "Gradient: [[ -4.7739  -9.1507  78.1402 -50.5674  58.1383]]\n",
      "Weights: [[-4.1536e+00 -1.3314e+00 -1.9000e-03  1.2520e-01  6.5700e-02]]\n",
      "MSE loss: 202.2334\n",
      "Iteration: 100300\n",
      "Gradient: [[  7.5494  -4.2722  -3.8575  65.1702 322.5113]]\n",
      "Weights: [[-4.1542e+00 -1.3314e+00 -1.9000e-03  1.2520e-01  6.5700e-02]]\n",
      "MSE loss: 202.2125\n",
      "Iteration: 100400\n",
      "Gradient: [[   4.2205   22.4647   13.6816  102.974  -275.033 ]]\n",
      "Weights: [[-4.1546e+00 -1.3314e+00 -2.1000e-03  1.2510e-01  6.5700e-02]]\n",
      "MSE loss: 202.1896\n",
      "Iteration: 100500\n",
      "Gradient: [[-15.9758 -16.435    8.5957 -42.1735 240.5882]]\n",
      "Weights: [[-4.1538e+00 -1.3314e+00 -2.1000e-03  1.2510e-01  6.5700e-02]]\n",
      "MSE loss: 202.1678\n",
      "Iteration: 100600\n",
      "Gradient: [[  -6.9979   -1.814    -5.3319   52.3118 -209.399 ]]\n",
      "Weights: [[-4.1536e+00 -1.3313e+00 -2.3000e-03  1.2520e-01  6.5700e-02]]\n",
      "MSE loss: 202.1374\n",
      "Iteration: 100700\n",
      "Gradient: [[  10.7267   -4.9525   63.288    79.7279 -318.9579]]\n",
      "Weights: [[-4.1534e+00 -1.3314e+00 -2.4000e-03  1.2510e-01  6.5700e-02]]\n",
      "MSE loss: 202.1037\n",
      "Iteration: 100800\n",
      "Gradient: [[  5.1582 -14.5922   4.6536  44.0003 176.3916]]\n",
      "Weights: [[-4.1532e+00 -1.3313e+00 -2.5000e-03  1.2510e-01  6.5700e-02]]\n",
      "MSE loss: 202.0762\n",
      "Iteration: 100900\n",
      "Gradient: [[  6.868   19.3282 -10.8316 122.2512 169.8172]]\n",
      "Weights: [[-4.1537e+00 -1.3312e+00 -2.7000e-03  1.2510e-01  6.5700e-02]]\n",
      "MSE loss: 202.046\n",
      "Iteration: 101000\n",
      "Gradient: [[ 2.351  31.0489 57.3487 -0.8951  1.8679]]\n",
      "Weights: [[-4.154e+00 -1.331e+00 -2.800e-03  1.252e-01  6.580e-02]]\n",
      "MSE loss: 202.0252\n",
      "Iteration: 101100\n",
      "Gradient: [[  5.625    5.5008   2.3238 -10.9484 -63.1181]]\n",
      "Weights: [[-4.1546e+00 -1.3311e+00 -2.9000e-03  1.2520e-01  6.5800e-02]]\n",
      "MSE loss: 201.9956\n",
      "Iteration: 101200\n",
      "Gradient: [[ 1.063000e-01  9.521000e-01  4.471420e+01 -6.251970e+01 -6.834579e+02]]\n",
      "Weights: [[-4.1547e+00 -1.3312e+00 -2.9000e-03  1.2520e-01  6.5800e-02]]\n",
      "MSE loss: 201.9791\n",
      "Iteration: 101300\n",
      "Gradient: [[   2.3112   -2.03     -6.7303  101.4261 -311.5606]]\n",
      "Weights: [[-4.1524e+00 -1.3313e+00 -3.0000e-03  1.2520e-01  6.5800e-02]]\n",
      "MSE loss: 201.9574\n",
      "Iteration: 101400\n",
      "Gradient: [[ -5.5084   9.9689  -9.9335 -28.2425  68.7349]]\n",
      "Weights: [[-4.1530e+00 -1.3311e+00 -3.1000e-03  1.2520e-01  6.5800e-02]]\n",
      "MSE loss: 201.9364\n",
      "Iteration: 101500\n",
      "Gradient: [[  2.5094  11.8883 -30.2609 -34.5758 -68.2274]]\n",
      "Weights: [[-4.1534e+00 -1.3311e+00 -3.2000e-03  1.2520e-01  6.5800e-02]]\n",
      "MSE loss: 201.9042\n",
      "Iteration: 101600\n",
      "Gradient: [[  -4.9618    1.9606  -21.5283 -106.4769    5.9909]]\n",
      "Weights: [[-4.1546e+00 -1.3311e+00 -3.3000e-03  1.2520e-01  6.5800e-02]]\n",
      "MSE loss: 201.8822\n",
      "Iteration: 101700\n",
      "Gradient: [[ 2.3544  1.5797 11.9602 61.1524 67.2369]]\n",
      "Weights: [[-4.1544e+00 -1.3310e+00 -3.5000e-03  1.2520e-01  6.5800e-02]]\n",
      "MSE loss: 201.8524\n",
      "Iteration: 101800\n",
      "Gradient: [[ -3.4081  13.7604  -0.994  -20.8128 162.5081]]\n",
      "Weights: [[-4.1538e+00 -1.3310e+00 -3.5000e-03  1.2520e-01  6.5900e-02]]\n",
      "MSE loss: 201.833\n",
      "Iteration: 101900\n",
      "Gradient: [[  -1.577    12.4955    3.7729  135.9523 -230.505 ]]\n",
      "Weights: [[-4.1529e+00 -1.3310e+00 -3.6000e-03  1.2520e-01  6.5900e-02]]\n",
      "MSE loss: 201.8125\n",
      "Iteration: 102000\n",
      "Gradient: [[  -6.108     3.6598   18.5411   65.6089 -211.0581]]\n",
      "Weights: [[-4.1534e+00 -1.3308e+00 -3.7000e-03  1.2520e-01  6.5900e-02]]\n",
      "MSE loss: 201.791\n",
      "Iteration: 102100\n",
      "Gradient: [[ -2.2194  12.0734 -10.4793  -0.9811  -2.9602]]\n",
      "Weights: [[-4.1539e+00 -1.3309e+00 -3.7000e-03  1.2520e-01  6.5900e-02]]\n",
      "MSE loss: 201.7764\n",
      "Iteration: 102200\n",
      "Gradient: [[  -3.0525   -8.5628   59.3922 -139.0921 -333.1244]]\n",
      "Weights: [[-4.1524e+00 -1.3307e+00 -3.8000e-03  1.2520e-01  6.5900e-02]]\n",
      "MSE loss: 201.7523\n",
      "Iteration: 102300\n",
      "Gradient: [[ 25.3325 -12.2657  15.8309  65.8805 -63.7228]]\n",
      "Weights: [[-4.1526e+00 -1.3309e+00 -3.9000e-03  1.2520e-01  6.5900e-02]]\n",
      "MSE loss: 201.7299\n",
      "Iteration: 102400\n",
      "Gradient: [[-21.246    3.7931   4.3601   0.9281 400.4442]]\n",
      "Weights: [[-4.1517e+00 -1.3310e+00 -4.1000e-03  1.2510e-01  6.5900e-02]]\n",
      "MSE loss: 201.7062\n",
      "Iteration: 102500\n",
      "Gradient: [[  -3.3457  -10.6877   11.7801  -21.1685 -428.5643]]\n",
      "Weights: [[-4.1523 -1.331  -0.0042  0.1251  0.0659]]\n",
      "MSE loss: 201.6815\n",
      "Iteration: 102600\n",
      "Gradient: [[  -2.5953   14.9721   67.3543  -63.7803 -368.2002]]\n",
      "Weights: [[-4.1514 -1.3311 -0.0043  0.1251  0.0659]]\n",
      "MSE loss: 201.6515\n",
      "Iteration: 102700\n",
      "Gradient: [[  5.3892   3.1355  24.5475  95.0342 315.9102]]\n",
      "Weights: [[-4.1512 -1.3311 -0.0044  0.1251  0.0659]]\n",
      "MSE loss: 201.6346\n",
      "Iteration: 102800\n",
      "Gradient: [[  -2.1641   -2.1573    6.4841  122.6277 -426.7873]]\n",
      "Weights: [[-4.1517 -1.3311 -0.0045  0.1251  0.066 ]]\n",
      "MSE loss: 201.6144\n",
      "Iteration: 102900\n",
      "Gradient: [[   9.9895   -9.5239   -1.5056  -78.1159 -145.2527]]\n",
      "Weights: [[-4.1519 -1.3307 -0.0046  0.1251  0.066 ]]\n",
      "MSE loss: 201.5825\n",
      "Iteration: 103000\n",
      "Gradient: [[   9.5799   -1.1525   29.0405  -95.6987 -506.802 ]]\n",
      "Weights: [[-4.152  -1.3308 -0.0047  0.1251  0.066 ]]\n",
      "MSE loss: 201.565\n",
      "Iteration: 103100\n",
      "Gradient: [[-9.061800e+00  3.765000e-01  5.745070e+01  9.737140e+01 -5.572546e+02]]\n",
      "Weights: [[-4.151  -1.3309 -0.0047  0.1252  0.066 ]]\n",
      "MSE loss: 201.5475\n",
      "Iteration: 103200\n",
      "Gradient: [[  10.3996    5.7328   -0.4326  -62.3476 -102.1353]]\n",
      "Weights: [[-4.1511 -1.3309 -0.0049  0.1251  0.066 ]]\n",
      "MSE loss: 201.5205\n",
      "Iteration: 103300\n",
      "Gradient: [[-10.4441   4.1439 -10.1918  30.2212 122.2687]]\n",
      "Weights: [[-4.1513 -1.3309 -0.005   0.1251  0.066 ]]\n",
      "MSE loss: 201.4934\n",
      "Iteration: 103400\n",
      "Gradient: [[ -2.2212  -5.6716  60.1029  34.3644 222.0831]]\n",
      "Weights: [[-4.1505 -1.331  -0.0051  0.1251  0.066 ]]\n",
      "MSE loss: 201.4743\n",
      "Iteration: 103500\n",
      "Gradient: [[ -11.1883    1.4933   15.6725 -125.4468 -278.7394]]\n",
      "Weights: [[-4.15   -1.3309 -0.0053  0.1251  0.066 ]]\n",
      "MSE loss: 201.4439\n",
      "Iteration: 103600\n",
      "Gradient: [[  2.2887  -4.162   32.8355 -79.9836 207.5531]]\n",
      "Weights: [[-4.1504 -1.3308 -0.0054  0.1251  0.066 ]]\n",
      "MSE loss: 201.412\n",
      "Iteration: 103700\n",
      "Gradient: [[  0.2846  -8.878    3.7516 139.4985 210.7461]]\n",
      "Weights: [[-4.1514 -1.3309 -0.0055  0.1251  0.0661]]\n",
      "MSE loss: 201.3897\n",
      "Iteration: 103800\n",
      "Gradient: [[  3.1232  -6.4214  28.6948 146.1219 382.9714]]\n",
      "Weights: [[-4.1507 -1.331  -0.0056  0.1252  0.0661]]\n",
      "MSE loss: 201.3596\n",
      "Iteration: 103900\n",
      "Gradient: [[-7.7435 27.3181  5.5926  9.4072 93.8931]]\n",
      "Weights: [[-4.1503 -1.331  -0.0057  0.1252  0.0661]]\n",
      "MSE loss: 201.3443\n",
      "Iteration: 104000\n",
      "Gradient: [[-16.0193 -23.4619   5.2633  35.7506   5.0793]]\n",
      "Weights: [[-4.151  -1.3308 -0.0058  0.1252  0.0661]]\n",
      "MSE loss: 201.3201\n",
      "Iteration: 104100\n",
      "Gradient: [[   3.9693  -10.2841   24.718   123.542  -447.102 ]]\n",
      "Weights: [[-4.1522 -1.3305 -0.0059  0.1252  0.0661]]\n",
      "MSE loss: 201.2987\n",
      "Iteration: 104200\n",
      "Gradient: [[   0.6577   15.1125   94.1035   37.6267 -516.8487]]\n",
      "Weights: [[-4.1515 -1.3303 -0.006   0.1252  0.0661]]\n",
      "MSE loss: 201.271\n",
      "Iteration: 104300\n",
      "Gradient: [[-10.8945  13.7396  24.2685 136.6656 573.1561]]\n",
      "Weights: [[-4.1513 -1.3302 -0.0061  0.1252  0.0661]]\n",
      "MSE loss: 201.2438\n",
      "Iteration: 104400\n",
      "Gradient: [[  -0.9607   12.9987   30.1109  -88.9829 -114.642 ]]\n",
      "Weights: [[-4.1516 -1.3302 -0.0062  0.1252  0.0661]]\n",
      "MSE loss: 201.2235\n",
      "Iteration: 104500\n",
      "Gradient: [[   9.2893    4.9507   21.7815   11.307  -168.7668]]\n",
      "Weights: [[-4.1507 -1.3301 -0.0063  0.1251  0.0662]]\n",
      "MSE loss: 201.1941\n",
      "Iteration: 104600\n",
      "Gradient: [[ 10.2047  15.5324 -10.2163  45.0564  73.5244]]\n",
      "Weights: [[-4.1506 -1.3302 -0.0064  0.1252  0.0662]]\n",
      "MSE loss: 201.1625\n",
      "Iteration: 104700\n",
      "Gradient: [[ 10.8509  11.2808  18.9464 121.089   97.0282]]\n",
      "Weights: [[-4.1507 -1.3303 -0.0065  0.1251  0.0662]]\n",
      "MSE loss: 201.1429\n",
      "Iteration: 104800\n",
      "Gradient: [[  4.1209 -10.8473 -23.2147  88.5201  21.3733]]\n",
      "Weights: [[-4.1491 -1.33   -0.0067  0.1251  0.0662]]\n",
      "MSE loss: 201.121\n",
      "Iteration: 104900\n",
      "Gradient: [[ -17.3442  -14.1693  -29.8036   84.2743 -650.5041]]\n",
      "Weights: [[-4.1504 -1.33   -0.0068  0.1251  0.0662]]\n",
      "MSE loss: 201.0959\n",
      "Iteration: 105000\n",
      "Gradient: [[  4.5183  36.8601 -16.2558  77.4124 -48.4204]]\n",
      "Weights: [[-4.1501 -1.3298 -0.0068  0.1251  0.0662]]\n",
      "MSE loss: 201.0754\n",
      "Iteration: 105100\n",
      "Gradient: [[  12.6684   -3.6497   59.4624  108.6634 -312.6083]]\n",
      "Weights: [[-4.1497 -1.3299 -0.0069  0.1251  0.0662]]\n",
      "MSE loss: 201.0566\n",
      "Iteration: 105200\n",
      "Gradient: [[-16.1505  -1.6983  -5.8432   0.8224 -26.2203]]\n",
      "Weights: [[-4.1505 -1.3301 -0.0071  0.1251  0.0662]]\n",
      "MSE loss: 201.03\n",
      "Iteration: 105300\n",
      "Gradient: [[  16.7963  -25.7007  -43.5935   37.6974 -123.8885]]\n",
      "Weights: [[-4.15   -1.33   -0.0072  0.1251  0.0662]]\n",
      "MSE loss: 201.0036\n",
      "Iteration: 105400\n",
      "Gradient: [[   4.173    -3.5903   -2.7279 -126.0032  256.7189]]\n",
      "Weights: [[-4.1492 -1.33   -0.0073  0.1251  0.0662]]\n",
      "MSE loss: 200.9811\n",
      "Iteration: 105500\n",
      "Gradient: [[ -11.0267   26.9247    0.9064 -102.2499 -259.4776]]\n",
      "Weights: [[-4.1492 -1.3301 -0.0074  0.1251  0.0663]]\n",
      "MSE loss: 200.9648\n",
      "Iteration: 105600\n",
      "Gradient: [[ -9.79    -7.4163  17.916  107.1438 205.6483]]\n",
      "Weights: [[-4.15   -1.3301 -0.0075  0.1252  0.0663]]\n",
      "MSE loss: 200.9402\n",
      "Iteration: 105700\n",
      "Gradient: [[  -1.6614  -34.2659   47.8392   98.2447 -627.9168]]\n",
      "Weights: [[-4.149  -1.33   -0.0076  0.1252  0.0663]]\n",
      "MSE loss: 200.9168\n",
      "Iteration: 105800\n",
      "Gradient: [[  3.014  -10.0302  10.2037  66.0114 189.4673]]\n",
      "Weights: [[-4.1481 -1.33   -0.0077  0.1252  0.0663]]\n",
      "MSE loss: 200.8954\n",
      "Iteration: 105900\n",
      "Gradient: [[ 13.1489   2.9356 -30.1965  54.6643 164.5861]]\n",
      "Weights: [[-4.1487 -1.33   -0.0078  0.1252  0.0663]]\n",
      "MSE loss: 200.8741\n",
      "Iteration: 106000\n",
      "Gradient: [[   5.4425  -10.5714  -19.4592   24.903  -212.8049]]\n",
      "Weights: [[-4.1507 -1.3299 -0.0079  0.1252  0.0663]]\n",
      "MSE loss: 200.8559\n",
      "Iteration: 106100\n",
      "Gradient: [[ -11.8384    2.7881   25.4152   48.7002 -253.4446]]\n",
      "Weights: [[-4.1495 -1.3298 -0.008   0.1252  0.0663]]\n",
      "MSE loss: 200.8212\n",
      "Iteration: 106200\n",
      "Gradient: [[   2.832    -1.1481    2.544    44.6161 -177.7376]]\n",
      "Weights: [[-4.1482 -1.3298 -0.0081  0.1251  0.0663]]\n",
      "MSE loss: 200.7954\n",
      "Iteration: 106300\n",
      "Gradient: [[ -14.6159   12.4045  -11.106   -40.2073 -275.6692]]\n",
      "Weights: [[-4.1493 -1.3298 -0.0082  0.1251  0.0664]]\n",
      "MSE loss: 200.78\n",
      "Iteration: 106400\n",
      "Gradient: [[ -7.9491 -13.5566  52.5073  -4.5539 -72.0529]]\n",
      "Weights: [[-4.1488 -1.3298 -0.0082  0.1251  0.0664]]\n",
      "MSE loss: 200.7608\n",
      "Iteration: 106500\n",
      "Gradient: [[  9.2326   5.1162 -38.3467  25.0421 297.6096]]\n",
      "Weights: [[-4.1482 -1.3296 -0.0083  0.1251  0.0664]]\n",
      "MSE loss: 200.7369\n",
      "Iteration: 106600\n",
      "Gradient: [[ 12.4279  -7.5534  19.6378 115.8554 144.6287]]\n",
      "Weights: [[-4.1488 -1.3297 -0.0084  0.1251  0.0664]]\n",
      "MSE loss: 200.7091\n",
      "Iteration: 106700\n",
      "Gradient: [[  18.7985   14.8815   28.2474   66.7552 -302.6351]]\n",
      "Weights: [[-4.1483 -1.3296 -0.0085  0.1251  0.0664]]\n",
      "MSE loss: 200.6863\n",
      "Iteration: 106800\n",
      "Gradient: [[ -13.7738    0.4827    9.0501   28.425  -211.3715]]\n",
      "Weights: [[-4.1488 -1.3295 -0.0086  0.1251  0.0664]]\n",
      "MSE loss: 200.6568\n",
      "Iteration: 106900\n",
      "Gradient: [[  -3.4512   -0.7749   -8.7159  -56.8684 -463.4239]]\n",
      "Weights: [[-4.148  -1.3295 -0.0087  0.1251  0.0664]]\n",
      "MSE loss: 200.6379\n",
      "Iteration: 107000\n",
      "Gradient: [[  7.4911  12.7621   1.4346 -98.3904 -24.8911]]\n",
      "Weights: [[-4.1482 -1.3295 -0.0088  0.1251  0.0664]]\n",
      "MSE loss: 200.6139\n",
      "Iteration: 107100\n",
      "Gradient: [[  2.7464  -6.3856  11.7865 -75.9783 262.6989]]\n",
      "Weights: [[-4.1479 -1.3296 -0.009   0.1251  0.0664]]\n",
      "MSE loss: 200.5882\n",
      "Iteration: 107200\n",
      "Gradient: [[  6.4818 -17.7414  13.3136 -23.0087 -42.5431]]\n",
      "Weights: [[-4.1487 -1.3295 -0.0091  0.1251  0.0665]]\n",
      "MSE loss: 200.564\n",
      "Iteration: 107300\n",
      "Gradient: [[  -6.6449   -2.3973   22.3319  -46.7099 -169.1219]]\n",
      "Weights: [[-4.1495 -1.3294 -0.0092  0.1251  0.0665]]\n",
      "MSE loss: 200.5406\n",
      "Iteration: 107400\n",
      "Gradient: [[ 20.5832 -16.4098  29.0504  23.0042  45.9096]]\n",
      "Weights: [[-4.1478 -1.3295 -0.0093  0.1251  0.0665]]\n",
      "MSE loss: 200.5122\n",
      "Iteration: 107500\n",
      "Gradient: [[  -8.0124   -4.5525   24.6266 -111.1516  191.3942]]\n",
      "Weights: [[-4.148  -1.3296 -0.0095  0.1251  0.0665]]\n",
      "MSE loss: 200.4859\n",
      "Iteration: 107600\n",
      "Gradient: [[ -2.9743  -5.0252  40.2458 -61.8687 -27.2968]]\n",
      "Weights: [[-4.1472 -1.3295 -0.0095  0.1251  0.0665]]\n",
      "MSE loss: 200.4698\n",
      "Iteration: 107700\n",
      "Gradient: [[  16.3803    6.8846   21.4148  -19.4179 -128.965 ]]\n",
      "Weights: [[-4.1461 -1.3294 -0.0097  0.1251  0.0665]]\n",
      "MSE loss: 200.4427\n",
      "Iteration: 107800\n",
      "Gradient: [[ 11.1751   7.3748 -20.7093 147.7505 -45.9914]]\n",
      "Weights: [[-4.1458 -1.3295 -0.0098  0.1251  0.0665]]\n",
      "MSE loss: 200.418\n",
      "Iteration: 107900\n",
      "Gradient: [[   8.122   -12.1066   23.8904 -138.0714  314.9538]]\n",
      "Weights: [[-4.147  -1.3297 -0.0099  0.1251  0.0665]]\n",
      "MSE loss: 200.4014\n",
      "Iteration: 108000\n",
      "Gradient: [[ -23.2738   -0.8504  -17.7436  -51.7177 -557.3771]]\n",
      "Weights: [[-4.1471 -1.3295 -0.0099  0.1251  0.0665]]\n",
      "MSE loss: 200.3807\n",
      "Iteration: 108100\n",
      "Gradient: [[-1.7615 16.2366  1.4843 58.5397 31.8927]]\n",
      "Weights: [[-4.1469 -1.3296 -0.01    0.1251  0.0666]]\n",
      "MSE loss: 200.3549\n",
      "Iteration: 108200\n",
      "Gradient: [[  -2.5697  -14.2791   71.3744   43.3826 -132.3416]]\n",
      "Weights: [[-4.1471 -1.3294 -0.0101  0.1251  0.0666]]\n",
      "MSE loss: 200.3349\n",
      "Iteration: 108300\n",
      "Gradient: [[  -2.2952    4.0472   68.1763  -54.6076 -316.1231]]\n",
      "Weights: [[-4.148  -1.3292 -0.0102  0.1251  0.0666]]\n",
      "MSE loss: 200.3111\n",
      "Iteration: 108400\n",
      "Gradient: [[  0.1463  -9.4906 -21.798   12.3945  15.23  ]]\n",
      "Weights: [[-4.1476 -1.3292 -0.0103  0.1251  0.0666]]\n",
      "MSE loss: 200.2863\n",
      "Iteration: 108500\n",
      "Gradient: [[ -9.5707  -8.7519  -5.5171   4.7033 -33.3259]]\n",
      "Weights: [[-4.1477 -1.3292 -0.0104  0.1251  0.0666]]\n",
      "MSE loss: 200.2592\n",
      "Iteration: 108600\n",
      "Gradient: [[ -9.0042 -12.6289 -13.8109 -60.2822 -70.0315]]\n",
      "Weights: [[-4.1475 -1.3293 -0.0105  0.1251  0.0666]]\n",
      "MSE loss: 200.2402\n",
      "Iteration: 108700\n",
      "Gradient: [[-3.1226  1.5114 73.9547 -9.5446  0.8   ]]\n",
      "Weights: [[-4.1463 -1.3291 -0.0105  0.1251  0.0666]]\n",
      "MSE loss: 200.2205\n",
      "Iteration: 108800\n",
      "Gradient: [[  -3.34      6.0082   26.7201 -186.5929 -165.0597]]\n",
      "Weights: [[-4.1459 -1.3291 -0.0107  0.1251  0.0666]]\n",
      "MSE loss: 200.1941\n",
      "Iteration: 108900\n",
      "Gradient: [[   8.8695    2.1399    7.4555   19.0776 -226.4827]]\n",
      "Weights: [[-4.1463 -1.329  -0.0108  0.1251  0.0667]]\n",
      "MSE loss: 200.1745\n",
      "Iteration: 109000\n",
      "Gradient: [[  17.8245    6.448    25.3506  -24.9628 -132.6233]]\n",
      "Weights: [[-4.145  -1.329  -0.0109  0.1251  0.0667]]\n",
      "MSE loss: 200.1612\n",
      "Iteration: 109100\n",
      "Gradient: [[  6.0434  19.5855  14.4577 -17.0808  77.6642]]\n",
      "Weights: [[-4.1456 -1.329  -0.011   0.1251  0.0667]]\n",
      "MSE loss: 200.1335\n",
      "Iteration: 109200\n",
      "Gradient: [[  2.4764 -13.6326  15.8843 -67.5188  43.6107]]\n",
      "Weights: [[-4.1458 -1.3292 -0.0111  0.1251  0.0667]]\n",
      "MSE loss: 200.1088\n",
      "Iteration: 109300\n",
      "Gradient: [[ -20.5255   12.9869   44.3802   65.7687 -264.1379]]\n",
      "Weights: [[-4.1456 -1.3292 -0.0112  0.1251  0.0667]]\n",
      "MSE loss: 200.0832\n",
      "Iteration: 109400\n",
      "Gradient: [[-12.9495  -9.1391  25.3291   4.3663 -61.4111]]\n",
      "Weights: [[-4.1464 -1.3292 -0.0113  0.1251  0.0667]]\n",
      "MSE loss: 200.0575\n",
      "Iteration: 109500\n",
      "Gradient: [[   3.9199  -40.14     31.0816   37.7512 -191.2302]]\n",
      "Weights: [[-4.146  -1.329  -0.0114  0.1251  0.0667]]\n",
      "MSE loss: 200.0378\n",
      "Iteration: 109600\n",
      "Gradient: [[-2.2346 -8.3883 18.4474 41.8    68.5521]]\n",
      "Weights: [[-4.1464 -1.3289 -0.0115  0.1251  0.0667]]\n",
      "MSE loss: 200.0116\n",
      "Iteration: 109700\n",
      "Gradient: [[  4.4381   4.1649 -78.1604 -23.2124  83.827 ]]\n",
      "Weights: [[-4.1464 -1.3288 -0.0117  0.1251  0.0667]]\n",
      "MSE loss: 199.9869\n",
      "Iteration: 109800\n",
      "Gradient: [[-1.586850e+01 -3.062000e-01 -2.746550e+01 -4.045070e+01 -5.956279e+02]]\n",
      "Weights: [[-4.1473 -1.3286 -0.0117  0.1251  0.0668]]\n",
      "MSE loss: 199.968\n",
      "Iteration: 109900\n",
      "Gradient: [[  8.4973  -4.6048  37.867  149.7053 -58.9317]]\n",
      "Weights: [[-4.1475 -1.3284 -0.0118  0.1251  0.0668]]\n",
      "MSE loss: 199.9477\n",
      "Iteration: 110000\n",
      "Gradient: [[ -0.964  -16.6357 -17.5719  90.6929 -70.1371]]\n",
      "Weights: [[-4.1466 -1.3283 -0.0118  0.1251  0.0668]]\n",
      "MSE loss: 199.9289\n",
      "Iteration: 110100\n",
      "Gradient: [[   4.5441   -1.9249   29.5888 -226.3335  -18.7081]]\n",
      "Weights: [[-4.147  -1.3282 -0.0119  0.1251  0.0668]]\n",
      "MSE loss: 199.905\n",
      "Iteration: 110200\n",
      "Gradient: [[   7.8165  -13.9891   70.8351  -59.125  -310.5005]]\n",
      "Weights: [[-4.1464 -1.3282 -0.012   0.1251  0.0668]]\n",
      "MSE loss: 199.8927\n",
      "Iteration: 110300\n",
      "Gradient: [[   9.7096    8.0849  -30.8866   -6.2974 -132.8173]]\n",
      "Weights: [[-4.148  -1.3279 -0.0121  0.1251  0.0668]]\n",
      "MSE loss: 199.8676\n",
      "Iteration: 110400\n",
      "Gradient: [[ -12.6302   -4.9913   15.4001  -69.6868 -432.7294]]\n",
      "Weights: [[-4.1479 -1.3277 -0.0122  0.1251  0.0668]]\n",
      "MSE loss: 199.8485\n",
      "Iteration: 110500\n",
      "Gradient: [[ -0.7885 -11.4112 -28.7165 -84.9813  65.3501]]\n",
      "Weights: [[-4.1466 -1.3277 -0.0123  0.1251  0.0668]]\n",
      "MSE loss: 199.8262\n",
      "Iteration: 110600\n",
      "Gradient: [[   2.3602   -0.9187   28.7522   70.3508 -144.3348]]\n",
      "Weights: [[-4.1466 -1.3278 -0.0123  0.1251  0.0668]]\n",
      "MSE loss: 199.8053\n",
      "Iteration: 110700\n",
      "Gradient: [[  5.3999  25.1863  20.5777  35.7079 -87.3164]]\n",
      "Weights: [[-4.1458 -1.3277 -0.0124  0.1251  0.0669]]\n",
      "MSE loss: 199.7871\n",
      "Iteration: 110800\n",
      "Gradient: [[  -5.819    -6.814   -43.3903   50.8027 -605.0381]]\n",
      "Weights: [[-4.1462 -1.3277 -0.0125  0.1251  0.0669]]\n",
      "MSE loss: 199.7631\n",
      "Iteration: 110900\n",
      "Gradient: [[  -4.4086  -22.7797    2.9086  -61.4436 -229.6994]]\n",
      "Weights: [[-4.1464 -1.3278 -0.0126  0.1251  0.0669]]\n",
      "MSE loss: 199.7427\n",
      "Iteration: 111000\n",
      "Gradient: [[   1.2031    9.7275   21.1327  -46.0266 -268.4358]]\n",
      "Weights: [[-4.1465 -1.3278 -0.0127  0.1251  0.0669]]\n",
      "MSE loss: 199.7211\n",
      "Iteration: 111100\n",
      "Gradient: [[   3.4865  -13.5133  -67.2743  138.8249 -163.9827]]\n",
      "Weights: [[-4.1469 -1.3278 -0.0129  0.1251  0.0669]]\n",
      "MSE loss: 199.6969\n",
      "Iteration: 111200\n",
      "Gradient: [[  -5.5398   -0.4068   25.7268  -15.0149 -218.1938]]\n",
      "Weights: [[-4.145  -1.3276 -0.013   0.1251  0.0669]]\n",
      "MSE loss: 199.6724\n",
      "Iteration: 111300\n",
      "Gradient: [[   1.5653   19.9106  -10.442    36.1555 -105.5787]]\n",
      "Weights: [[-4.1462 -1.3274 -0.0131  0.1251  0.0669]]\n",
      "MSE loss: 199.6474\n",
      "Iteration: 111400\n",
      "Gradient: [[  3.8409   1.31   100.0337 -88.9575 279.9018]]\n",
      "Weights: [[-4.1463 -1.3273 -0.0132  0.1251  0.0669]]\n",
      "MSE loss: 199.6266\n",
      "Iteration: 111500\n",
      "Gradient: [[   9.0313   13.3848   47.6535 -126.9822   54.9692]]\n",
      "Weights: [[-4.1465 -1.327  -0.0133  0.1251  0.0669]]\n",
      "MSE loss: 199.6027\n",
      "Iteration: 111600\n",
      "Gradient: [[ -3.8958  -2.2192  -6.2833 101.5673 159.0741]]\n",
      "Weights: [[-4.1461 -1.3271 -0.0135  0.1251  0.0669]]\n",
      "MSE loss: 199.5804\n",
      "Iteration: 111700\n",
      "Gradient: [[   5.6946  -13.2181  -68.4043   51.0926 -260.075 ]]\n",
      "Weights: [[-4.1458 -1.327  -0.0135  0.1251  0.067 ]]\n",
      "MSE loss: 199.5646\n",
      "Iteration: 111800\n",
      "Gradient: [[  -2.6898   -2.0129   -9.093   126.3161 -162.9082]]\n",
      "Weights: [[-4.1465 -1.327  -0.0137  0.1251  0.067 ]]\n",
      "MSE loss: 199.5357\n",
      "Iteration: 111900\n",
      "Gradient: [[-15.8431  16.1038  70.2938 -64.6277  72.0167]]\n",
      "Weights: [[-4.146  -1.327  -0.0137  0.1251  0.067 ]]\n",
      "MSE loss: 199.5161\n",
      "Iteration: 112000\n",
      "Gradient: [[ 11.6905  15.6422 -19.3227  72.9295 207.4627]]\n",
      "Weights: [[-4.1436 -1.327  -0.0138  0.1251  0.067 ]]\n",
      "MSE loss: 199.5038\n",
      "Iteration: 112100\n",
      "Gradient: [[   5.1569   19.1351   -5.2203  -65.5582 -172.3994]]\n",
      "Weights: [[-4.1442 -1.3269 -0.0139  0.1251  0.067 ]]\n",
      "MSE loss: 199.4813\n",
      "Iteration: 112200\n",
      "Gradient: [[  2.1247 -10.7     18.5788 174.7514  88.1199]]\n",
      "Weights: [[-4.1451 -1.327  -0.0141  0.1251  0.067 ]]\n",
      "MSE loss: 199.4506\n",
      "Iteration: 112300\n",
      "Gradient: [[ -2.9031 -15.7047 -34.7791  95.1508  -0.9082]]\n",
      "Weights: [[-4.1456 -1.3269 -0.0141  0.1251  0.067 ]]\n",
      "MSE loss: 199.4306\n",
      "Iteration: 112400\n",
      "Gradient: [[ -0.7117  -5.3438  30.4535 147.8579  20.8438]]\n",
      "Weights: [[-4.1447 -1.3269 -0.0142  0.1251  0.067 ]]\n",
      "MSE loss: 199.4115\n",
      "Iteration: 112500\n",
      "Gradient: [[   1.6192   -6.5783  -21.8869   25.257  -166.8653]]\n",
      "Weights: [[-4.1451 -1.3268 -0.0143  0.1251  0.067 ]]\n",
      "MSE loss: 199.3829\n",
      "Iteration: 112600\n",
      "Gradient: [[ -21.1405   16.3871   -8.0768  -27.8042 -214.3962]]\n",
      "Weights: [[-4.145  -1.3266 -0.0144  0.1251  0.0671]]\n",
      "MSE loss: 199.3663\n",
      "Iteration: 112700\n",
      "Gradient: [[ -4.2014   1.6193  -7.3722 -11.0753 104.0346]]\n",
      "Weights: [[-4.1448 -1.3268 -0.0145  0.1251  0.0671]]\n",
      "MSE loss: 199.3473\n",
      "Iteration: 112800\n",
      "Gradient: [[  7.2385 -10.939   46.1034  25.7587 256.4174]]\n",
      "Weights: [[-4.1438 -1.3268 -0.0146  0.1251  0.0671]]\n",
      "MSE loss: 199.3296\n",
      "Iteration: 112900\n",
      "Gradient: [[ -1.3906   6.6863 -52.8803 -73.5169  88.1624]]\n",
      "Weights: [[-4.1446 -1.3267 -0.0147  0.1251  0.0671]]\n",
      "MSE loss: 199.3046\n",
      "Iteration: 113000\n",
      "Gradient: [[  -7.3277   11.1528   -2.3264   47.8938 -550.5671]]\n",
      "Weights: [[-4.144  -1.3266 -0.0147  0.1251  0.0671]]\n",
      "MSE loss: 199.2889\n",
      "Iteration: 113100\n",
      "Gradient: [[  -6.8022   -4.3698  -51.1061 -152.3762   63.4435]]\n",
      "Weights: [[-4.1453 -1.3266 -0.0148  0.1251  0.0671]]\n",
      "MSE loss: 199.2661\n",
      "Iteration: 113200\n",
      "Gradient: [[  -4.2514  -24.7571   14.4389  -50.446  -330.6097]]\n",
      "Weights: [[-4.1447 -1.3265 -0.0149  0.1251  0.0671]]\n",
      "MSE loss: 199.2458\n",
      "Iteration: 113300\n",
      "Gradient: [[   2.5753   16.048     6.3161  -31.9338 -299.8771]]\n",
      "Weights: [[-4.1467 -1.3263 -0.0151  0.1251  0.0671]]\n",
      "MSE loss: 199.2186\n",
      "Iteration: 113400\n",
      "Gradient: [[   9.8594  -17.9789   40.559   -69.0031 -100.2781]]\n",
      "Weights: [[-4.1453 -1.3262 -0.0151  0.1251  0.0671]]\n",
      "MSE loss: 199.1955\n",
      "Iteration: 113500\n",
      "Gradient: [[  5.3783  -9.1575 -17.0242  63.9969 -74.1309]]\n",
      "Weights: [[-4.145  -1.3262 -0.0152  0.1251  0.0672]]\n",
      "MSE loss: 199.1737\n",
      "Iteration: 113600\n",
      "Gradient: [[  -1.9626   -5.1489  -15.5682  -13.0268 -580.845 ]]\n",
      "Weights: [[-4.1458 -1.3262 -0.0153  0.1251  0.0672]]\n",
      "MSE loss: 199.1515\n",
      "Iteration: 113700\n",
      "Gradient: [[ -1.2039 -22.0827 -41.3866 -57.5987  56.7854]]\n",
      "Weights: [[-4.1466 -1.326  -0.0154  0.1251  0.0672]]\n",
      "MSE loss: 199.1325\n",
      "Iteration: 113800\n",
      "Gradient: [[ 17.2431  21.1056   6.181  -31.2554 -89.8185]]\n",
      "Weights: [[-4.145  -1.3257 -0.0155  0.1251  0.0672]]\n",
      "MSE loss: 199.1116\n",
      "Iteration: 113900\n",
      "Gradient: [[  4.5687  11.4341  42.4404 161.4216 240.2428]]\n",
      "Weights: [[-4.1438 -1.3256 -0.0156  0.1251  0.0672]]\n",
      "MSE loss: 199.1009\n",
      "Iteration: 114000\n",
      "Gradient: [[   5.8901   -7.847    13.6719 -187.3546 -286.5958]]\n",
      "Weights: [[-4.1451 -1.3256 -0.0158  0.1251  0.0672]]\n",
      "MSE loss: 199.0696\n",
      "Iteration: 114100\n",
      "Gradient: [[   0.7527   -5.7472   39.3715  -26.4177 -199.6034]]\n",
      "Weights: [[-4.1449 -1.3255 -0.0159  0.1251  0.0672]]\n",
      "MSE loss: 199.0468\n",
      "Iteration: 114200\n",
      "Gradient: [[ 14.6262  -2.3692  -5.6901 123.0212  56.5053]]\n",
      "Weights: [[-4.1441 -1.3254 -0.016   0.1251  0.0672]]\n",
      "MSE loss: 199.026\n",
      "Iteration: 114300\n",
      "Gradient: [[ 10.2197   2.9485  26.8587 116.1453 264.7618]]\n",
      "Weights: [[-4.1455 -1.3254 -0.0161  0.1251  0.0672]]\n",
      "MSE loss: 199.0071\n",
      "Iteration: 114400\n",
      "Gradient: [[ -8.859    0.4     25.2387   0.309  115.2416]]\n",
      "Weights: [[-4.1469 -1.3253 -0.0162  0.1251  0.0672]]\n",
      "MSE loss: 198.9859\n",
      "Iteration: 114500\n",
      "Gradient: [[  23.4735   14.2014  -47.9453  -28.1818 -105.2852]]\n",
      "Weights: [[-4.1468 -1.3251 -0.0163  0.1251  0.0673]]\n",
      "MSE loss: 198.9627\n",
      "Iteration: 114600\n",
      "Gradient: [[  13.7856    7.4269   23.7589 -197.046  -162.2893]]\n",
      "Weights: [[-4.1464 -1.325  -0.0163  0.1251  0.0673]]\n",
      "MSE loss: 198.9458\n",
      "Iteration: 114700\n",
      "Gradient: [[  -7.1002  -15.1294  -44.0822   12.8376 -386.5933]]\n",
      "Weights: [[-4.1471 -1.3251 -0.0164  0.1251  0.0673]]\n",
      "MSE loss: 198.9386\n",
      "Iteration: 114800\n",
      "Gradient: [[  5.5961  -2.4568 -12.35    40.3534 -97.5726]]\n",
      "Weights: [[-4.1455 -1.3253 -0.0165  0.1251  0.0673]]\n",
      "MSE loss: 198.91\n",
      "Iteration: 114900\n",
      "Gradient: [[  -9.6049   11.2605   47.1273  -43.0271 -219.2372]]\n",
      "Weights: [[-4.1454 -1.3253 -0.0166  0.1251  0.0673]]\n",
      "MSE loss: 198.8902\n",
      "Iteration: 115000\n",
      "Gradient: [[  7.3378   7.9121  19.2969  20.0593 -63.0671]]\n",
      "Weights: [[-4.145  -1.3253 -0.0167  0.1251  0.0673]]\n",
      "MSE loss: 198.8712\n",
      "Iteration: 115100\n",
      "Gradient: [[ -5.3089   8.3141 -30.0414 -88.4774 -71.3092]]\n",
      "Weights: [[-4.145  -1.3253 -0.0168  0.1251  0.0673]]\n",
      "MSE loss: 198.8481\n",
      "Iteration: 115200\n",
      "Gradient: [[-10.1063  -3.3364 -13.7864 -54.946   62.2067]]\n",
      "Weights: [[-4.1446 -1.325  -0.0169  0.1251  0.0673]]\n",
      "MSE loss: 198.8238\n",
      "Iteration: 115300\n",
      "Gradient: [[  10.2463    6.3607    8.2787   84.576  -617.8231]]\n",
      "Weights: [[-4.1443 -1.3249 -0.017   0.1251  0.0673]]\n",
      "MSE loss: 198.8012\n",
      "Iteration: 115400\n",
      "Gradient: [[ 10.1505  -1.2331  27.9915  -1.5234 106.1411]]\n",
      "Weights: [[-4.1446 -1.3246 -0.0171  0.1251  0.0673]]\n",
      "MSE loss: 198.7852\n",
      "Iteration: 115500\n",
      "Gradient: [[   4.8683   14.2541    9.6455   81.7541 -149.4135]]\n",
      "Weights: [[-4.1433 -1.3244 -0.0172  0.1251  0.0674]]\n",
      "MSE loss: 198.7672\n",
      "Iteration: 115600\n",
      "Gradient: [[ -9.8007 -18.6576 -33.3031 -39.5741 103.6711]]\n",
      "Weights: [[-4.1447 -1.3243 -0.0173  0.125   0.0674]]\n",
      "MSE loss: 198.7403\n",
      "Iteration: 115700\n",
      "Gradient: [[-15.9429  15.8109  65.632  161.3489 243.0614]]\n",
      "Weights: [[-4.1441 -1.3242 -0.0174  0.125   0.0674]]\n",
      "MSE loss: 198.7184\n",
      "Iteration: 115800\n",
      "Gradient: [[   4.6506    0.8569    1.3994 -131.4424 -165.6905]]\n",
      "Weights: [[-4.145  -1.3242 -0.0175  0.125   0.0674]]\n",
      "MSE loss: 198.6933\n",
      "Iteration: 115900\n",
      "Gradient: [[  -3.2074    4.0135   33.4773 -105.7482   63.4729]]\n",
      "Weights: [[-4.1458 -1.3241 -0.0176  0.125   0.0674]]\n",
      "MSE loss: 198.6769\n",
      "Iteration: 116000\n",
      "Gradient: [[   1.1914   -1.368   -89.4311 -137.8335  124.4923]]\n",
      "Weights: [[-4.1458 -1.3237 -0.0176  0.125   0.0674]]\n",
      "MSE loss: 198.6504\n",
      "Iteration: 116100\n",
      "Gradient: [[  -6.1126  -12.4052   30.2924  -38.9255 -379.4182]]\n",
      "Weights: [[-4.1448 -1.3234 -0.0178  0.125   0.0674]]\n",
      "MSE loss: 198.6253\n",
      "Iteration: 116200\n",
      "Gradient: [[  5.5672  -2.0068 -22.0575  94.601  -76.6996]]\n",
      "Weights: [[-4.1452 -1.3233 -0.0179  0.125   0.0674]]\n",
      "MSE loss: 198.6052\n",
      "Iteration: 116300\n",
      "Gradient: [[-7.3302 -1.8854 49.0388 47.484  47.6909]]\n",
      "Weights: [[-4.1454 -1.3234 -0.018   0.125   0.0674]]\n",
      "MSE loss: 198.584\n",
      "Iteration: 116400\n",
      "Gradient: [[ 13.5873  -5.0911  31.3494 -21.2894   7.3076]]\n",
      "Weights: [[-4.1453 -1.3234 -0.0181  0.125   0.0674]]\n",
      "MSE loss: 198.5611\n",
      "Iteration: 116500\n",
      "Gradient: [[  -5.1813    3.6473  -82.239    -4.1127 -483.4599]]\n",
      "Weights: [[-4.1455 -1.3233 -0.0182  0.125   0.0675]]\n",
      "MSE loss: 198.5401\n",
      "Iteration: 116600\n",
      "Gradient: [[ -7.4437 -23.0838  22.4128 -74.9293 -82.8433]]\n",
      "Weights: [[-4.1457 -1.323  -0.0183  0.125   0.0675]]\n",
      "MSE loss: 198.5175\n",
      "Iteration: 116700\n",
      "Gradient: [[  6.1795   8.3034 -31.5105  27.3216 -99.1617]]\n",
      "Weights: [[-4.1473 -1.3227 -0.0184  0.125   0.0675]]\n",
      "MSE loss: 198.4977\n",
      "Iteration: 116800\n",
      "Gradient: [[ -5.9008  16.3669 -24.4322 -10.6106 243.2519]]\n",
      "Weights: [[-4.1459 -1.3224 -0.0185  0.125   0.0675]]\n",
      "MSE loss: 198.4714\n",
      "Iteration: 116900\n",
      "Gradient: [[   0.5732  -13.5983   38.5451 -121.7707 -232.9732]]\n",
      "Weights: [[-4.1456 -1.3226 -0.0186  0.125   0.0675]]\n",
      "MSE loss: 198.4492\n",
      "Iteration: 117000\n",
      "Gradient: [[  -0.9999  -14.1758    3.1799   35.5529 -281.0725]]\n",
      "Weights: [[-4.1457 -1.3224 -0.0187  0.125   0.0675]]\n",
      "MSE loss: 198.4239\n",
      "Iteration: 117100\n",
      "Gradient: [[ 14.2621  -9.0516 -23.0234 -10.3823 222.3063]]\n",
      "Weights: [[-4.1454 -1.3223 -0.0189  0.1249  0.0675]]\n",
      "MSE loss: 198.3946\n",
      "Iteration: 117200\n",
      "Gradient: [[  13.1862  -10.1202   60.0164  100.5734 -141.1987]]\n",
      "Weights: [[-4.1454 -1.3223 -0.019   0.125   0.0675]]\n",
      "MSE loss: 198.3746\n",
      "Iteration: 117300\n",
      "Gradient: [[ 6.762200e+00  7.130000e-02  2.000840e+01 -1.051437e+02 -4.869922e+02]]\n",
      "Weights: [[-4.1441 -1.3221 -0.0191  0.1249  0.0675]]\n",
      "MSE loss: 198.3493\n",
      "Iteration: 117400\n",
      "Gradient: [[ -9.141   23.2588  46.249    1.5498 166.3687]]\n",
      "Weights: [[-4.1445 -1.322  -0.0193  0.1249  0.0676]]\n",
      "MSE loss: 198.3237\n",
      "Iteration: 117500\n",
      "Gradient: [[ 8.0019 -7.7843 26.1513 16.5771 -0.79  ]]\n",
      "Weights: [[-4.145  -1.322  -0.0194  0.1249  0.0676]]\n",
      "MSE loss: 198.3053\n",
      "Iteration: 117600\n",
      "Gradient: [[ -0.441  -23.2769  43.253   51.3898 225.5983]]\n",
      "Weights: [[-4.1446 -1.3221 -0.0195  0.1249  0.0676]]\n",
      "MSE loss: 198.2851\n",
      "Iteration: 117700\n",
      "Gradient: [[-10.581  -21.6516  18.2057 143.816  -96.5541]]\n",
      "Weights: [[-4.1452 -1.3219 -0.0196  0.1249  0.0676]]\n",
      "MSE loss: 198.2647\n",
      "Iteration: 117800\n",
      "Gradient: [[  -5.3801   16.3254   53.8266   -2.7201 -123.8857]]\n",
      "Weights: [[-4.1452 -1.3216 -0.0196  0.1249  0.0676]]\n",
      "MSE loss: 198.2453\n",
      "Iteration: 117900\n",
      "Gradient: [[  -2.9375   -1.8851  102.8875   70.1715 -248.9541]]\n",
      "Weights: [[-4.1458 -1.3215 -0.0197  0.1249  0.0676]]\n",
      "MSE loss: 198.2247\n",
      "Iteration: 118000\n",
      "Gradient: [[ -5.6981  11.5111  -1.473  -70.438  370.1159]]\n",
      "Weights: [[-4.1446 -1.3215 -0.0199  0.1249  0.0676]]\n",
      "MSE loss: 198.1936\n",
      "Iteration: 118100\n",
      "Gradient: [[  6.807  -13.3653  30.182   47.0874 149.4122]]\n",
      "Weights: [[-4.144  -1.3214 -0.02    0.1249  0.0676]]\n",
      "MSE loss: 198.1771\n",
      "Iteration: 118200\n",
      "Gradient: [[   2.5453   17.6543  -47.7277   84.883  -191.926 ]]\n",
      "Weights: [[-4.1441 -1.3212 -0.0201  0.1249  0.0676]]\n",
      "MSE loss: 198.1573\n",
      "Iteration: 118300\n",
      "Gradient: [[ -0.2074  10.169   -9.1426 -40.4753 -69.0464]]\n",
      "Weights: [[-4.1453 -1.3211 -0.0202  0.1249  0.0676]]\n",
      "MSE loss: 198.1332\n",
      "Iteration: 118400\n",
      "Gradient: [[ -17.3774  -13.1798   20.666   -25.8545 -279.2152]]\n",
      "Weights: [[-4.1448 -1.3213 -0.0203  0.1249  0.0677]]\n",
      "MSE loss: 198.1163\n",
      "Iteration: 118500\n",
      "Gradient: [[ -4.5407  28.357  -12.8317  50.1433 143.646 ]]\n",
      "Weights: [[-4.145  -1.321  -0.0204  0.1249  0.0677]]\n",
      "MSE loss: 198.0969\n",
      "Iteration: 118600\n",
      "Gradient: [[ -8.4484   9.552   47.5214 137.2608 -76.7136]]\n",
      "Weights: [[-4.1445 -1.321  -0.0205  0.1249  0.0677]]\n",
      "MSE loss: 198.0843\n",
      "Iteration: 118700\n",
      "Gradient: [[ 2.96000e-02  2.11860e+00 -1.86989e+01  5.57675e+01 -6.73873e+01]]\n",
      "Weights: [[-4.1445 -1.3208 -0.0205  0.1249  0.0677]]\n",
      "MSE loss: 198.0682\n",
      "Iteration: 118800\n",
      "Gradient: [[  -3.2965   -3.1124   32.3701  140.3542 -127.6221]]\n",
      "Weights: [[-4.1437 -1.3209 -0.0207  0.1249  0.0677]]\n",
      "MSE loss: 198.0438\n",
      "Iteration: 118900\n",
      "Gradient: [[ -1.2029  14.2018   8.0727 -33.9347  28.3978]]\n",
      "Weights: [[-4.1436 -1.3208 -0.0208  0.1249  0.0677]]\n",
      "MSE loss: 198.0211\n",
      "Iteration: 119000\n",
      "Gradient: [[   8.1966  -19.2841   34.9644    7.965  -540.6887]]\n",
      "Weights: [[-4.1459 -1.3207 -0.0209  0.1249  0.0677]]\n",
      "MSE loss: 197.997\n",
      "Iteration: 119100\n",
      "Gradient: [[  -5.0138   11.4073    4.3599   84.1271 -410.7203]]\n",
      "Weights: [[-4.1467 -1.3205 -0.021   0.1249  0.0677]]\n",
      "MSE loss: 197.9803\n",
      "Iteration: 119200\n",
      "Gradient: [[  -1.1347  -15.0694   28.649  -225.5317  515.4312]]\n",
      "Weights: [[-4.1463 -1.3205 -0.0211  0.1249  0.0677]]\n",
      "MSE loss: 197.9558\n",
      "Iteration: 119300\n",
      "Gradient: [[  -4.4728   -1.7424  -13.4901 -110.6716 -246.4796]]\n",
      "Weights: [[-4.1466 -1.3201 -0.0211  0.1249  0.0678]]\n",
      "MSE loss: 197.9294\n",
      "Iteration: 119400\n",
      "Gradient: [[  7.372  -12.63    15.3885 -27.8323  49.6514]]\n",
      "Weights: [[-4.1467 -1.3199 -0.0212  0.1249  0.0678]]\n",
      "MSE loss: 197.9088\n",
      "Iteration: 119500\n",
      "Gradient: [[ -0.8674 -33.2852  -7.9424 -34.5881 150.6031]]\n",
      "Weights: [[-4.1477 -1.3196 -0.0213  0.1249  0.0678]]\n",
      "MSE loss: 197.8913\n",
      "Iteration: 119600\n",
      "Gradient: [[   2.9518   -7.611   -11.0287   23.9693 -619.2474]]\n",
      "Weights: [[-4.1464 -1.3193 -0.0214  0.1249  0.0678]]\n",
      "MSE loss: 197.8621\n",
      "Iteration: 119700\n",
      "Gradient: [[   0.6023   -7.5815  -29.1493  -26.2018 -126.1516]]\n",
      "Weights: [[-4.1476 -1.319  -0.0215  0.1249  0.0678]]\n",
      "MSE loss: 197.8423\n",
      "Iteration: 119800\n",
      "Gradient: [[   8.1328   -4.065   -76.1609   -9.7981 -176.757 ]]\n",
      "Weights: [[-4.1476 -1.3189 -0.0215  0.1249  0.0678]]\n",
      "MSE loss: 197.8251\n",
      "Iteration: 119900\n",
      "Gradient: [[ -3.1687  -2.9622 -24.1213  19.4431  70.8911]]\n",
      "Weights: [[-4.1475 -1.3189 -0.0216  0.1249  0.0678]]\n",
      "MSE loss: 197.8073\n",
      "Iteration: 120000\n",
      "Gradient: [[   2.7028   11.8595   40.8372  -69.9994 -583.8585]]\n",
      "Weights: [[-4.1481 -1.3188 -0.0217  0.1249  0.0678]]\n",
      "MSE loss: 197.7904\n",
      "Iteration: 120100\n",
      "Gradient: [[  -1.5458   15.3566   47.79    -35.493  -375.5462]]\n",
      "Weights: [[-4.1482 -1.3186 -0.0218  0.1249  0.0678]]\n",
      "MSE loss: 197.7758\n",
      "Iteration: 120200\n",
      "Gradient: [[ -2.148    4.6839  20.4785  63.8917 -84.4416]]\n",
      "Weights: [[-4.1479 -1.3185 -0.022   0.1249  0.0678]]\n",
      "MSE loss: 197.7488\n",
      "Iteration: 120300\n",
      "Gradient: [[   7.1146    4.1665  -21.6332   46.8398 -315.4943]]\n",
      "Weights: [[-4.1476 -1.3184 -0.0221  0.1249  0.0679]]\n",
      "MSE loss: 197.7182\n",
      "Iteration: 120400\n",
      "Gradient: [[ 4.79840e+00 -7.32780e+00  5.58078e+01 -8.03000e-02 -8.79832e+01]]\n",
      "Weights: [[-4.1468 -1.3183 -0.0223  0.1249  0.0679]]\n",
      "MSE loss: 197.688\n",
      "Iteration: 120500\n",
      "Gradient: [[ -14.394     7.5721  -22.3141   79.8831 -119.1892]]\n",
      "Weights: [[-4.1468 -1.3181 -0.0224  0.1249  0.0679]]\n",
      "MSE loss: 197.6684\n",
      "Iteration: 120600\n",
      "Gradient: [[ -14.5395    7.3408   23.7107   75.3822 -276.1571]]\n",
      "Weights: [[-4.1464 -1.3181 -0.0225  0.1249  0.0679]]\n",
      "MSE loss: 197.6519\n",
      "Iteration: 120700\n",
      "Gradient: [[ 13.8373  13.8604  49.4192 -75.9641  57.2026]]\n",
      "Weights: [[-4.1457 -1.3179 -0.0226  0.1249  0.0679]]\n",
      "MSE loss: 197.6302\n",
      "Iteration: 120800\n",
      "Gradient: [[ -13.5289  -20.6321    8.6163 -109.543  -178.7043]]\n",
      "Weights: [[-4.1449 -1.3179 -0.0228  0.1249  0.0679]]\n",
      "MSE loss: 197.6072\n",
      "Iteration: 120900\n",
      "Gradient: [[  7.9487  11.067   12.2036 -69.3512 293.6013]]\n",
      "Weights: [[-4.146  -1.3179 -0.0228  0.1249  0.0679]]\n",
      "MSE loss: 197.5923\n",
      "Iteration: 121000\n",
      "Gradient: [[ 18.3491 -25.4955 -23.0273 -94.4413 219.3143]]\n",
      "Weights: [[-4.1446 -1.3178 -0.023   0.1248  0.0679]]\n",
      "MSE loss: 197.5658\n",
      "Iteration: 121100\n",
      "Gradient: [[   0.2872   -8.0259  -49.4706   66.873  -122.7799]]\n",
      "Weights: [[-4.1452 -1.3178 -0.023   0.1249  0.0679]]\n",
      "MSE loss: 197.554\n",
      "Iteration: 121200\n",
      "Gradient: [[  -22.4819     3.9512    40.6239    77.353  -1021.119 ]]\n",
      "Weights: [[-4.1448 -1.3179 -0.0231  0.1248  0.0679]]\n",
      "MSE loss: 197.5387\n",
      "Iteration: 121300\n",
      "Gradient: [[-15.0982  14.6611   1.649  110.9734 348.5723]]\n",
      "Weights: [[-4.1455 -1.3175 -0.0232  0.1248  0.0679]]\n",
      "MSE loss: 197.5163\n",
      "Iteration: 121400\n",
      "Gradient: [[ -7.2832 -17.7247  23.0293 -27.4051 227.2249]]\n",
      "Weights: [[-4.1439 -1.3175 -0.0233  0.1248  0.0679]]\n",
      "MSE loss: 197.4945\n",
      "Iteration: 121500\n",
      "Gradient: [[  -4.3625   15.0626   -9.5454 -151.2098  155.466 ]]\n",
      "Weights: [[-4.1441 -1.3175 -0.0235  0.1248  0.068 ]]\n",
      "MSE loss: 197.4646\n",
      "Iteration: 121600\n",
      "Gradient: [[  7.194  -18.8909   1.6357 121.2939 -81.9102]]\n",
      "Weights: [[-4.1455 -1.3177 -0.0236  0.1248  0.068 ]]\n",
      "MSE loss: 197.4459\n",
      "Iteration: 121700\n",
      "Gradient: [[  16.4741   -9.0237    7.2017  117.715  -693.8298]]\n",
      "Weights: [[-4.145  -1.3177 -0.0236  0.1248  0.068 ]]\n",
      "MSE loss: 197.4298\n",
      "Iteration: 121800\n",
      "Gradient: [[   9.1136  -10.2106   -3.0068   53.0584 -466.8503]]\n",
      "Weights: [[-4.1455 -1.3174 -0.0238  0.1248  0.068 ]]\n",
      "MSE loss: 197.4009\n",
      "Iteration: 121900\n",
      "Gradient: [[   5.2771  -22.1742    4.3329  156.5655 -431.1682]]\n",
      "Weights: [[-4.1449 -1.3171 -0.0239  0.1248  0.068 ]]\n",
      "MSE loss: 197.3775\n",
      "Iteration: 122000\n",
      "Gradient: [[-10.5695 -25.1001 -66.2834 -15.5354  73.0463]]\n",
      "Weights: [[-4.1444 -1.317  -0.0239  0.1248  0.068 ]]\n",
      "MSE loss: 197.3576\n",
      "Iteration: 122100\n",
      "Gradient: [[   4.7559    9.1383  -20.524  -109.99    -23.708 ]]\n",
      "Weights: [[-4.1447 -1.3169 -0.024   0.1248  0.068 ]]\n",
      "MSE loss: 197.3401\n",
      "Iteration: 122200\n",
      "Gradient: [[ 17.7144 -12.4372  20.3177  10.0517 -90.3567]]\n",
      "Weights: [[-4.1438 -1.3169 -0.0241  0.1247  0.068 ]]\n",
      "MSE loss: 197.3262\n",
      "Iteration: 122300\n",
      "Gradient: [[   3.5943   -5.1863   24.9225  149.3953 -410.6393]]\n",
      "Weights: [[-4.1434 -1.317  -0.0242  0.1247  0.068 ]]\n",
      "MSE loss: 197.3069\n",
      "Iteration: 122400\n",
      "Gradient: [[ 8.3524 -0.8079 -3.9244 79.7742 86.8461]]\n",
      "Weights: [[-4.144  -1.3169 -0.0243  0.1247  0.0681]]\n",
      "MSE loss: 197.2821\n",
      "Iteration: 122500\n",
      "Gradient: [[   0.4566   -3.4342    8.448    67.0726 -132.7647]]\n",
      "Weights: [[-4.1442 -1.3168 -0.0243  0.1247  0.0681]]\n",
      "MSE loss: 197.2649\n",
      "Iteration: 122600\n",
      "Gradient: [[ -15.3717   13.0797  -19.7825 -158.5458   88.7562]]\n",
      "Weights: [[-4.1457 -1.3166 -0.0245  0.1247  0.0681]]\n",
      "MSE loss: 197.2435\n",
      "Iteration: 122700\n",
      "Gradient: [[  1.6139  -4.6002  42.6593  93.8022 394.5533]]\n",
      "Weights: [[-4.1455 -1.3166 -0.0245  0.1247  0.0681]]\n",
      "MSE loss: 197.227\n",
      "Iteration: 122800\n",
      "Gradient: [[  -7.5681  -31.9949   42.5801 -129.9874 -259.8163]]\n",
      "Weights: [[-4.1455 -1.3165 -0.0246  0.1247  0.0681]]\n",
      "MSE loss: 197.2114\n",
      "Iteration: 122900\n",
      "Gradient: [[ 9.090000e-02  1.261340e+01  2.798590e+01  9.625100e+00 -1.041407e+02]]\n",
      "Weights: [[-4.1441 -1.3167 -0.0247  0.1247  0.0681]]\n",
      "MSE loss: 197.1947\n",
      "Iteration: 123000\n",
      "Gradient: [[   1.7137   -1.8543   45.7573  163.5441 -248.2477]]\n",
      "Weights: [[-4.1442 -1.3166 -0.0248  0.1247  0.0681]]\n",
      "MSE loss: 197.1726\n",
      "Iteration: 123100\n",
      "Gradient: [[ -10.7012   -0.6701    1.1061   87.4422 -314.5437]]\n",
      "Weights: [[-4.144  -1.3166 -0.0249  0.1247  0.0681]]\n",
      "MSE loss: 197.1538\n",
      "Iteration: 123200\n",
      "Gradient: [[  -2.9643    9.701    -1.568    75.6071 -160.1078]]\n",
      "Weights: [[-4.1427 -1.3163 -0.025   0.1247  0.0681]]\n",
      "MSE loss: 197.1367\n",
      "Iteration: 123300\n",
      "Gradient: [[ -10.6278   -8.9143  -15.206  -220.9928   16.5606]]\n",
      "Weights: [[-4.1441 -1.3162 -0.0251  0.1246  0.0681]]\n",
      "MSE loss: 197.1153\n",
      "Iteration: 123400\n",
      "Gradient: [[   3.3078    4.2596  -19.4148 -123.292  -699.26  ]]\n",
      "Weights: [[-4.1453 -1.3162 -0.0252  0.1247  0.0681]]\n",
      "MSE loss: 197.1079\n",
      "Iteration: 123500\n",
      "Gradient: [[ -16.3439   -8.646    42.5686   22.4477 -595.7365]]\n",
      "Weights: [[-4.1457 -1.3158 -0.0253  0.1247  0.0682]]\n",
      "MSE loss: 197.0778\n",
      "Iteration: 123600\n",
      "Gradient: [[ 22.8635 -19.4575  30.5789  74.7174 -34.6977]]\n",
      "Weights: [[-4.1451 -1.3156 -0.0253  0.1247  0.0682]]\n",
      "MSE loss: 197.0553\n",
      "Iteration: 123700\n",
      "Gradient: [[-1.952100e+00  5.690000e-02  2.920540e+01  7.119290e+01 -1.401889e+02]]\n",
      "Weights: [[-4.145  -1.3153 -0.0254  0.1247  0.0682]]\n",
      "MSE loss: 197.0287\n",
      "Iteration: 123800\n",
      "Gradient: [[   0.1859  -11.2634  -43.7073 -112.9141 -137.7598]]\n",
      "Weights: [[-4.1446 -1.3153 -0.0256  0.1247  0.0682]]\n",
      "MSE loss: 197.0049\n",
      "Iteration: 123900\n",
      "Gradient: [[  -1.0108  -12.3466   19.8348  -57.8737 -163.3198]]\n",
      "Weights: [[-4.1441 -1.3151 -0.0257  0.1247  0.0682]]\n",
      "MSE loss: 196.9847\n",
      "Iteration: 124000\n",
      "Gradient: [[   5.893     5.1828   36.596  -134.2259  170.5622]]\n",
      "Weights: [[-4.1452 -1.315  -0.0258  0.1247  0.0682]]\n",
      "MSE loss: 196.9624\n",
      "Iteration: 124100\n",
      "Gradient: [[ 11.3765 -12.2943  -2.2175 -29.6127 -77.042 ]]\n",
      "Weights: [[-4.1446 -1.3145 -0.0259  0.1247  0.0682]]\n",
      "MSE loss: 196.9387\n",
      "Iteration: 124200\n",
      "Gradient: [[   0.9126    7.9641  -38.8131   97.452  -525.1611]]\n",
      "Weights: [[-4.1447 -1.3143 -0.026   0.1247  0.0682]]\n",
      "MSE loss: 196.9157\n",
      "Iteration: 124300\n",
      "Gradient: [[  -0.6249  -11.6439  -19.9265 -100.3971  -83.3336]]\n",
      "Weights: [[-4.1452 -1.3143 -0.0262  0.1247  0.0682]]\n",
      "MSE loss: 196.8943\n",
      "Iteration: 124400\n",
      "Gradient: [[  -6.2517    4.2191  113.4024   56.5422 -439.7769]]\n",
      "Weights: [[-4.1448 -1.3142 -0.0263  0.1247  0.0682]]\n",
      "MSE loss: 196.8692\n",
      "Iteration: 124500\n",
      "Gradient: [[   8.0952   10.3304    5.3017   25.7231 -504.336 ]]\n",
      "Weights: [[-4.1447 -1.3142 -0.0264  0.1247  0.0683]]\n",
      "MSE loss: 196.8472\n",
      "Iteration: 124600\n",
      "Gradient: [[-11.3282 -14.0995  21.3508 158.0867 -58.0952]]\n",
      "Weights: [[-4.1447 -1.3142 -0.0265  0.1246  0.0683]]\n",
      "MSE loss: 196.827\n",
      "Iteration: 124700\n",
      "Gradient: [[  6.2303 -12.9849  13.2935  30.7429  -0.4181]]\n",
      "Weights: [[-4.1447 -1.3141 -0.0267  0.1246  0.0683]]\n",
      "MSE loss: 196.8016\n",
      "Iteration: 124800\n",
      "Gradient: [[  -4.0047   -2.9567  -29.8498   75.3111 -695.2937]]\n",
      "Weights: [[-4.1441 -1.3138 -0.0268  0.1246  0.0683]]\n",
      "MSE loss: 196.7799\n",
      "Iteration: 124900\n",
      "Gradient: [[   3.7102   -4.4523   -6.1315  -31.8709 -172.6498]]\n",
      "Weights: [[-4.1451 -1.3139 -0.0268  0.1247  0.0683]]\n",
      "MSE loss: 196.7656\n",
      "Iteration: 125000\n",
      "Gradient: [[  -6.7651   15.5481    6.1284  -75.8186 -549.2208]]\n",
      "Weights: [[-4.1454 -1.3139 -0.0269  0.1246  0.0683]]\n",
      "MSE loss: 196.7492\n",
      "Iteration: 125100\n",
      "Gradient: [[  -9.835   -19.5195  100.2864 -120.892  -169.7191]]\n",
      "Weights: [[-4.1447 -1.3138 -0.027   0.1246  0.0683]]\n",
      "MSE loss: 196.7227\n",
      "Iteration: 125200\n",
      "Gradient: [[ -5.723   -4.1012  49.6522 -19.4014 287.6681]]\n",
      "Weights: [[-4.1439 -1.3136 -0.0272  0.1246  0.0683]]\n",
      "MSE loss: 196.7003\n",
      "Iteration: 125300\n",
      "Gradient: [[  -2.02    -28.0821   26.7785   12.6438 -358.7468]]\n",
      "Weights: [[-4.1435 -1.3137 -0.0272  0.1246  0.0683]]\n",
      "MSE loss: 196.6871\n",
      "Iteration: 125400\n",
      "Gradient: [[ -0.5363 -17.0727 -74.5985 -97.9546 -16.8308]]\n",
      "Weights: [[-4.1439 -1.3136 -0.0273  0.1246  0.0683]]\n",
      "MSE loss: 196.6692\n",
      "Iteration: 125500\n",
      "Gradient: [[ -11.9174   -5.2634   22.5005 -121.4544  -51.7769]]\n",
      "Weights: [[-4.1437 -1.3135 -0.0274  0.1246  0.0684]]\n",
      "MSE loss: 196.6471\n",
      "Iteration: 125600\n",
      "Gradient: [[  3.8239  -9.9887 -20.3642 145.3166   7.4211]]\n",
      "Weights: [[-4.1441 -1.3135 -0.0275  0.1246  0.0684]]\n",
      "MSE loss: 196.6291\n",
      "Iteration: 125700\n",
      "Gradient: [[ 10.6318   1.3477 -13.9463 -43.9949 106.0177]]\n",
      "Weights: [[-4.1437 -1.3132 -0.0276  0.1246  0.0684]]\n",
      "MSE loss: 196.61\n",
      "Iteration: 125800\n",
      "Gradient: [[   1.9028  -13.9705   -8.9548   59.3239 -397.5001]]\n",
      "Weights: [[-4.1445 -1.313  -0.0277  0.1246  0.0684]]\n",
      "MSE loss: 196.5901\n",
      "Iteration: 125900\n",
      "Gradient: [[  3.5513 -10.3817  50.6297 120.9916 166.1669]]\n",
      "Weights: [[-4.1441 -1.3127 -0.0278  0.1246  0.0684]]\n",
      "MSE loss: 196.56\n",
      "Iteration: 126000\n",
      "Gradient: [[  -2.3245    8.9006    5.2197   78.8625 -267.2917]]\n",
      "Weights: [[-4.1436 -1.3127 -0.0279  0.1246  0.0684]]\n",
      "MSE loss: 196.5457\n",
      "Iteration: 126100\n",
      "Gradient: [[  6.8962   5.0159  21.345   67.1909 138.6812]]\n",
      "Weights: [[-4.1434 -1.3126 -0.028   0.1246  0.0684]]\n",
      "MSE loss: 196.5181\n",
      "Iteration: 126200\n",
      "Gradient: [[-3.2462 14.7013 12.5239 81.0811 -8.9866]]\n",
      "Weights: [[-4.1437 -1.3127 -0.0281  0.1245  0.0684]]\n",
      "MSE loss: 196.4956\n",
      "Iteration: 126300\n",
      "Gradient: [[ 12.4466   0.0715  29.9462  69.9414 -30.0668]]\n",
      "Weights: [[-4.1439 -1.3125 -0.0282  0.1246  0.0684]]\n",
      "MSE loss: 196.475\n",
      "Iteration: 126400\n",
      "Gradient: [[   8.604     5.7292    1.5439  -15.5115 -510.005 ]]\n",
      "Weights: [[-4.144  -1.3125 -0.0284  0.1246  0.0685]]\n",
      "MSE loss: 196.4496\n",
      "Iteration: 126500\n",
      "Gradient: [[-22.3188 -12.7575  23.6027  44.9543 237.6805]]\n",
      "Weights: [[-4.1434 -1.3126 -0.0284  0.1246  0.0685]]\n",
      "MSE loss: 196.4391\n",
      "Iteration: 126600\n",
      "Gradient: [[ 14.9978  15.9942  31.1957 -22.7356 -68.9539]]\n",
      "Weights: [[-4.1449 -1.3123 -0.0285  0.1246  0.0685]]\n",
      "MSE loss: 196.424\n",
      "Iteration: 126700\n",
      "Gradient: [[ -2.5881   6.7165  17.0642 -98.7918  45.2764]]\n",
      "Weights: [[-4.1455 -1.3121 -0.0286  0.1246  0.0685]]\n",
      "MSE loss: 196.4058\n",
      "Iteration: 126800\n",
      "Gradient: [[ 1.569310e+01 -2.056260e+01  4.825320e+01  1.801148e+02 -4.200000e-03]]\n",
      "Weights: [[-4.1455 -1.3117 -0.0286  0.1246  0.0685]]\n",
      "MSE loss: 196.3821\n",
      "Iteration: 126900\n",
      "Gradient: [[ 14.9371  -3.1257  10.2397 -77.5885 350.9677]]\n",
      "Weights: [[-4.1474 -1.3112 -0.0288  0.1246  0.0685]]\n",
      "MSE loss: 196.364\n",
      "Iteration: 127000\n",
      "Gradient: [[ 5.351500e+00  1.329000e-01  6.529740e+01  1.475967e+02 -9.694940e+01]]\n",
      "Weights: [[-4.1482 -1.3107 -0.0288  0.1246  0.0685]]\n",
      "MSE loss: 196.3458\n",
      "Iteration: 127100\n",
      "Gradient: [[  -9.508     3.4862  -28.6562 -146.2028 -248.9053]]\n",
      "Weights: [[-4.1469 -1.3106 -0.0289  0.1246  0.0685]]\n",
      "MSE loss: 196.3196\n",
      "Iteration: 127200\n",
      "Gradient: [[   0.3869    3.2046   -8.4028  -83.6159 -211.8697]]\n",
      "Weights: [[-4.1473 -1.3105 -0.029   0.1245  0.0685]]\n",
      "MSE loss: 196.3031\n",
      "Iteration: 127300\n",
      "Gradient: [[  11.8356  -19.744    77.341   -91.392  -186.3941]]\n",
      "Weights: [[-4.1475 -1.3105 -0.0291  0.1245  0.0685]]\n",
      "MSE loss: 196.284\n",
      "Iteration: 127400\n",
      "Gradient: [[  -1.3107    5.7534  -33.8979  -97.2489 -270.6451]]\n",
      "Weights: [[-4.1465 -1.3104 -0.0292  0.1245  0.0685]]\n",
      "MSE loss: 196.261\n",
      "Iteration: 127500\n",
      "Gradient: [[  12.7488   -5.4953  -12.2019 -118.1358   33.0073]]\n",
      "Weights: [[-4.1462 -1.3103 -0.0293  0.1245  0.0686]]\n",
      "MSE loss: 196.2394\n",
      "Iteration: 127600\n",
      "Gradient: [[ -9.1785  -2.8634  20.993  104.8574 -57.167 ]]\n",
      "Weights: [[-4.1465 -1.3103 -0.0294  0.1245  0.0686]]\n",
      "MSE loss: 196.2205\n",
      "Iteration: 127700\n",
      "Gradient: [[  -7.4704   -0.3385   18.4525   67.6989 -100.5611]]\n",
      "Weights: [[-4.1456 -1.3103 -0.0295  0.1245  0.0686]]\n",
      "MSE loss: 196.2016\n",
      "Iteration: 127800\n",
      "Gradient: [[   8.4812    2.817     7.6228  -55.3275 -124.3839]]\n",
      "Weights: [[-4.1455 -1.3102 -0.0296  0.1245  0.0686]]\n",
      "MSE loss: 196.1833\n",
      "Iteration: 127900\n",
      "Gradient: [[ -2.203   -9.6652  29.3738 127.6018   2.6569]]\n",
      "Weights: [[-4.1462 -1.3102 -0.0297  0.1245  0.0686]]\n",
      "MSE loss: 196.162\n",
      "Iteration: 128000\n",
      "Gradient: [[-12.0223  -3.5729 -18.1723  37.3874 240.6915]]\n",
      "Weights: [[-4.1465 -1.3099 -0.0298  0.1245  0.0686]]\n",
      "MSE loss: 196.1387\n",
      "Iteration: 128100\n",
      "Gradient: [[ 18.3276   5.4941   7.9301 -32.0991  18.4015]]\n",
      "Weights: [[-4.1446 -1.3097 -0.0299  0.1245  0.0686]]\n",
      "MSE loss: 196.1166\n",
      "Iteration: 128200\n",
      "Gradient: [[  0.4526 -11.505  -25.9487  11.7666 396.5424]]\n",
      "Weights: [[-4.145  -1.3095 -0.0301  0.1245  0.0686]]\n",
      "MSE loss: 196.0891\n",
      "Iteration: 128300\n",
      "Gradient: [[   2.5752    8.2189  -23.5969   43.9622 -284.9552]]\n",
      "Weights: [[-4.1447 -1.3093 -0.0302  0.1245  0.0686]]\n",
      "MSE loss: 196.071\n",
      "Iteration: 128400\n",
      "Gradient: [[   4.2088   -1.6628   47.2883   46.0505 -184.7145]]\n",
      "Weights: [[-4.1445 -1.309  -0.0303  0.1245  0.0686]]\n",
      "MSE loss: 196.0552\n",
      "Iteration: 128500\n",
      "Gradient: [[  -4.3541   -9.3968   -1.7937  108.0491 -166.4778]]\n",
      "Weights: [[-4.1446 -1.3089 -0.0304  0.1245  0.0686]]\n",
      "MSE loss: 196.0321\n",
      "Iteration: 128600\n",
      "Gradient: [[  -7.831    -7.199    -8.5899 -115.981  -139.1105]]\n",
      "Weights: [[-4.1452 -1.3088 -0.0305  0.1245  0.0687]]\n",
      "MSE loss: 196.015\n",
      "Iteration: 128700\n",
      "Gradient: [[  -4.2645  -21.4787   56.2257   94.8295 -230.6648]]\n",
      "Weights: [[-4.1451 -1.3088 -0.0306  0.1245  0.0687]]\n",
      "MSE loss: 195.9923\n",
      "Iteration: 128800\n",
      "Gradient: [[  -9.706    18.4859   48.6401  -79.5943 -254.2326]]\n",
      "Weights: [[-4.1447 -1.3088 -0.0307  0.1245  0.0687]]\n",
      "MSE loss: 195.9756\n",
      "Iteration: 128900\n",
      "Gradient: [[-14.1147   3.4212  39.8489 113.1144 192.5792]]\n",
      "Weights: [[-4.1461 -1.3088 -0.0308  0.1245  0.0687]]\n",
      "MSE loss: 195.9561\n",
      "Iteration: 129000\n",
      "Gradient: [[   5.8426    2.8011   82.5577  -33.1937 -309.3627]]\n",
      "Weights: [[-4.1451 -1.3086 -0.0309  0.1245  0.0687]]\n",
      "MSE loss: 195.9359\n",
      "Iteration: 129100\n",
      "Gradient: [[ -1.3553  -1.7137 -14.4499  80.5311  69.3066]]\n",
      "Weights: [[-4.1447 -1.3083 -0.031   0.1245  0.0687]]\n",
      "MSE loss: 195.9155\n",
      "Iteration: 129200\n",
      "Gradient: [[-10.5728  -9.7205   9.885   69.5068 -99.7231]]\n",
      "Weights: [[-4.1443 -1.3082 -0.0311  0.1244  0.0687]]\n",
      "MSE loss: 195.9002\n",
      "Iteration: 129300\n",
      "Gradient: [[-5.649300e+00  2.045000e-01 -6.694500e+00 -4.334220e+01  3.053081e+02]]\n",
      "Weights: [[-4.144  -1.3081 -0.0312  0.1244  0.0687]]\n",
      "MSE loss: 195.8839\n",
      "Iteration: 129400\n",
      "Gradient: [[ 17.7115  14.2882  12.6065  -9.2717 164.8197]]\n",
      "Weights: [[-4.1442 -1.3078 -0.0313  0.1244  0.0687]]\n",
      "MSE loss: 195.8608\n",
      "Iteration: 129500\n",
      "Gradient: [[-13.4025  14.0837 -19.9121 -91.4483 171.5816]]\n",
      "Weights: [[-4.143  -1.3076 -0.0315  0.1244  0.0687]]\n",
      "MSE loss: 195.8472\n",
      "Iteration: 129600\n",
      "Gradient: [[ 5.8572  0.8384 29.0839 26.7373 56.2444]]\n",
      "Weights: [[-4.1433 -1.3077 -0.0315  0.1244  0.0687]]\n",
      "MSE loss: 195.83\n",
      "Iteration: 129700\n",
      "Gradient: [[   7.3975  -10.9647  -20.7548  -24.6627 -362.7525]]\n",
      "Weights: [[-4.1437 -1.3076 -0.0316  0.1244  0.0687]]\n",
      "MSE loss: 195.8135\n",
      "Iteration: 129800\n",
      "Gradient: [[ 11.0452   4.5582 -50.7028  40.402  -52.0912]]\n",
      "Weights: [[-4.1446 -1.3075 -0.0317  0.1244  0.0687]]\n",
      "MSE loss: 195.7926\n",
      "Iteration: 129900\n",
      "Gradient: [[ -4.426  -17.0196  23.8727  82.5988  -5.8981]]\n",
      "Weights: [[-4.1452 -1.3075 -0.0318  0.1244  0.0688]]\n",
      "MSE loss: 195.7783\n",
      "Iteration: 130000\n",
      "Gradient: [[  -2.7284   16.3539    1.9028  -33.9796 -318.0242]]\n",
      "Weights: [[-4.1454 -1.3074 -0.0318  0.1244  0.0688]]\n",
      "MSE loss: 195.76\n",
      "Iteration: 130100\n",
      "Gradient: [[   2.4207   -1.9242   33.5684  108.7764 -176.3763]]\n",
      "Weights: [[-4.145  -1.3072 -0.0319  0.1244  0.0688]]\n",
      "MSE loss: 195.7443\n",
      "Iteration: 130200\n",
      "Gradient: [[ 14.3954  -6.447    3.3487 -49.4251  75.0942]]\n",
      "Weights: [[-4.1455 -1.3071 -0.032   0.1244  0.0688]]\n",
      "MSE loss: 195.7296\n",
      "Iteration: 130300\n",
      "Gradient: [[   4.2713  -11.4317  -15.493    70.0605 -121.8011]]\n",
      "Weights: [[-4.1456 -1.3069 -0.0321  0.1244  0.0688]]\n",
      "MSE loss: 195.7076\n",
      "Iteration: 130400\n",
      "Gradient: [[ -1.6105  20.9622  35.303   23.2427 156.3166]]\n",
      "Weights: [[-4.1451 -1.3067 -0.0322  0.1244  0.0688]]\n",
      "MSE loss: 195.6888\n",
      "Iteration: 130500\n",
      "Gradient: [[  2.5516   7.4087  14.4484  49.4406 323.0108]]\n",
      "Weights: [[-4.1455 -1.3066 -0.0323  0.1244  0.0688]]\n",
      "MSE loss: 195.6748\n",
      "Iteration: 130600\n",
      "Gradient: [[  -4.2581   -2.1524  -20.11     93.0237 -240.4791]]\n",
      "Weights: [[-4.1464 -1.3065 -0.0324  0.1244  0.0688]]\n",
      "MSE loss: 195.6542\n",
      "Iteration: 130700\n",
      "Gradient: [[  4.4729 -23.3324 -41.8177   9.9655 144.434 ]]\n",
      "Weights: [[-4.1456 -1.3063 -0.0325  0.1244  0.0688]]\n",
      "MSE loss: 195.6342\n",
      "Iteration: 130800\n",
      "Gradient: [[   4.2648   -2.022     2.3585   20.3846 -203.0845]]\n",
      "Weights: [[-4.1453 -1.306  -0.0325  0.1244  0.0688]]\n",
      "MSE loss: 195.6179\n",
      "Iteration: 130900\n",
      "Gradient: [[16.5364 15.402  21.5676 62.2432 57.919 ]]\n",
      "Weights: [[-4.1449 -1.306  -0.0326  0.1244  0.0688]]\n",
      "MSE loss: 195.5996\n",
      "Iteration: 131000\n",
      "Gradient: [[   6.2835   18.173    20.5322    4.3636 -124.6457]]\n",
      "Weights: [[-4.1452 -1.306  -0.0327  0.1244  0.0688]]\n",
      "MSE loss: 195.5837\n",
      "Iteration: 131100\n",
      "Gradient: [[-11.1805   2.0306  16.282   53.2938  90.8199]]\n",
      "Weights: [[-4.1464 -1.3057 -0.0328  0.1244  0.0689]]\n",
      "MSE loss: 195.5632\n",
      "Iteration: 131200\n",
      "Gradient: [[-20.7927   0.3159   5.1292  -5.8952  26.2792]]\n",
      "Weights: [[-4.147  -1.3055 -0.0329  0.1244  0.0689]]\n",
      "MSE loss: 195.5454\n",
      "Iteration: 131300\n",
      "Gradient: [[  0.1544  18.7008   1.4223 108.9712  22.4354]]\n",
      "Weights: [[-4.1466 -1.3053 -0.033   0.1244  0.0689]]\n",
      "MSE loss: 195.5197\n",
      "Iteration: 131400\n",
      "Gradient: [[-10.0093  13.8443  19.149   18.4808 156.0472]]\n",
      "Weights: [[-4.1461 -1.3051 -0.0331  0.1244  0.0689]]\n",
      "MSE loss: 195.4964\n",
      "Iteration: 131500\n",
      "Gradient: [[  4.0978  15.2695  40.3282  51.7226 -76.4742]]\n",
      "Weights: [[-4.1452 -1.305  -0.0332  0.1244  0.0689]]\n",
      "MSE loss: 195.4775\n",
      "Iteration: 131600\n",
      "Gradient: [[  3.1461 -18.6252   3.0134 -44.5791  82.5022]]\n",
      "Weights: [[-4.1466 -1.305  -0.0333  0.1244  0.0689]]\n",
      "MSE loss: 195.4556\n",
      "Iteration: 131700\n",
      "Gradient: [[   9.7945    2.562   -20.2852 -120.5483 -311.0381]]\n",
      "Weights: [[-4.1466 -1.305  -0.0334  0.1243  0.0689]]\n",
      "MSE loss: 195.4399\n",
      "Iteration: 131800\n",
      "Gradient: [[  3.5537 -10.9558  51.5138  97.2301 -23.4427]]\n",
      "Weights: [[-4.1464 -1.3046 -0.0335  0.1244  0.0689]]\n",
      "MSE loss: 195.4147\n",
      "Iteration: 131900\n",
      "Gradient: [[-5.082  -8.1759 -7.6521  4.2712 25.5424]]\n",
      "Weights: [[-4.1468 -1.3043 -0.0336  0.1244  0.0689]]\n",
      "MSE loss: 195.3956\n",
      "Iteration: 132000\n",
      "Gradient: [[ -2.9911  12.79    24.4053 -94.4912 128.324 ]]\n",
      "Weights: [[-4.1459 -1.3042 -0.0337  0.1244  0.0689]]\n",
      "MSE loss: 195.3756\n",
      "Iteration: 132100\n",
      "Gradient: [[  -8.4796   12.0756  -17.0805   -7.3779 -272.7323]]\n",
      "Weights: [[-4.1466 -1.3042 -0.0339  0.1244  0.069 ]]\n",
      "MSE loss: 195.351\n",
      "Iteration: 132200\n",
      "Gradient: [[  5.4528  15.0776  19.978  -30.0609 171.5213]]\n",
      "Weights: [[-4.1452 -1.3041 -0.034   0.1244  0.069 ]]\n",
      "MSE loss: 195.3308\n",
      "Iteration: 132300\n",
      "Gradient: [[  7.6521  19.1328 -62.2206  68.6903 225.9744]]\n",
      "Weights: [[-4.1442 -1.3042 -0.0341  0.1244  0.069 ]]\n",
      "MSE loss: 195.3254\n",
      "Iteration: 132400\n",
      "Gradient: [[-10.1458   1.7387  20.9335  56.9515 185.7201]]\n",
      "Weights: [[-4.1452 -1.3042 -0.0342  0.1243  0.069 ]]\n",
      "MSE loss: 195.2929\n",
      "Iteration: 132500\n",
      "Gradient: [[ -7.0224  -1.8061  30.1394  43.153  133.9963]]\n",
      "Weights: [[-4.1464 -1.304  -0.0343  0.1243  0.069 ]]\n",
      "MSE loss: 195.2679\n",
      "Iteration: 132600\n",
      "Gradient: [[ 1.462180e+01 -2.018000e-01 -3.811700e+00  8.231490e+01 -5.562281e+02]]\n",
      "Weights: [[-4.1468 -1.3038 -0.0345  0.1243  0.069 ]]\n",
      "MSE loss: 195.2463\n",
      "Iteration: 132700\n",
      "Gradient: [[  2.0858  24.8066 -23.641   15.1259 -37.6641]]\n",
      "Weights: [[-4.1463 -1.3034 -0.0346  0.1243  0.069 ]]\n",
      "MSE loss: 195.2221\n",
      "Iteration: 132800\n",
      "Gradient: [[   9.0076  -17.5924   51.5155  -86.5231 -152.2911]]\n",
      "Weights: [[-4.1459 -1.3035 -0.0347  0.1243  0.069 ]]\n",
      "MSE loss: 195.2063\n",
      "Iteration: 132900\n",
      "Gradient: [[  0.1335  13.2623  33.9457  77.4291 -15.4617]]\n",
      "Weights: [[-4.1467 -1.3034 -0.0348  0.1243  0.069 ]]\n",
      "MSE loss: 195.191\n",
      "Iteration: 133000\n",
      "Gradient: [[  -2.5581   11.348     4.5596    9.9565 -222.2201]]\n",
      "Weights: [[-4.1455 -1.3032 -0.0349  0.1243  0.069 ]]\n",
      "MSE loss: 195.1696\n",
      "Iteration: 133100\n",
      "Gradient: [[-12.8475  -5.2775  36.4695  26.3096 137.5422]]\n",
      "Weights: [[-4.1462 -1.303  -0.035   0.1243  0.069 ]]\n",
      "MSE loss: 195.1498\n",
      "Iteration: 133200\n",
      "Gradient: [[   0.8052    3.8905    3.9712    7.4133 -151.6989]]\n",
      "Weights: [[-4.1451 -1.3028 -0.0351  0.1243  0.069 ]]\n",
      "MSE loss: 195.1355\n",
      "Iteration: 133300\n",
      "Gradient: [[  10.8342   15.7618  -24.0966  -54.7673 -306.6097]]\n",
      "Weights: [[-4.1446 -1.3027 -0.0352  0.1243  0.0691]]\n",
      "MSE loss: 195.1195\n",
      "Iteration: 133400\n",
      "Gradient: [[   3.8803    2.0944   47.6534 -118.638   343.2497]]\n",
      "Weights: [[-4.1444 -1.3027 -0.0352  0.1243  0.0691]]\n",
      "MSE loss: 195.1088\n",
      "Iteration: 133500\n",
      "Gradient: [[  11.7165   -7.8473  -69.6465   13.681  -233.7629]]\n",
      "Weights: [[-4.1461 -1.3026 -0.0353  0.1243  0.0691]]\n",
      "MSE loss: 195.0826\n",
      "Iteration: 133600\n",
      "Gradient: [[ -10.9992   14.8478    4.7958   67.8067 -201.8967]]\n",
      "Weights: [[-4.147  -1.3025 -0.0354  0.1243  0.0691]]\n",
      "MSE loss: 195.065\n",
      "Iteration: 133700\n",
      "Gradient: [[ -4.7555   1.3246  50.7411 165.4833 -48.1474]]\n",
      "Weights: [[-4.1462 -1.3024 -0.0355  0.1243  0.0691]]\n",
      "MSE loss: 195.0479\n",
      "Iteration: 133800\n",
      "Gradient: [[ -2.1012  -1.5294 -39.7046 142.2855 501.5568]]\n",
      "Weights: [[-4.1466 -1.3023 -0.0356  0.1243  0.0691]]\n",
      "MSE loss: 195.0261\n",
      "Iteration: 133900\n",
      "Gradient: [[ -2.4475   5.0831  23.8136 -98.5061  50.4961]]\n",
      "Weights: [[-4.1467 -1.3022 -0.0356  0.1243  0.0691]]\n",
      "MSE loss: 195.0094\n",
      "Iteration: 134000\n",
      "Gradient: [[   2.0183    9.3516    5.1869 -188.6437  -80.9317]]\n",
      "Weights: [[-4.1453 -1.3022 -0.0357  0.1243  0.0691]]\n",
      "MSE loss: 194.9923\n",
      "Iteration: 134100\n",
      "Gradient: [[  -5.7468   -0.7341    3.7097  -40.3415 -234.1567]]\n",
      "Weights: [[-4.146  -1.3021 -0.0359  0.1243  0.0691]]\n",
      "MSE loss: 194.9669\n",
      "Iteration: 134200\n",
      "Gradient: [[  -5.963    -4.845    82.6615  -11.1278 -323.2796]]\n",
      "Weights: [[-4.1453 -1.3021 -0.036   0.1243  0.0691]]\n",
      "MSE loss: 194.9495\n",
      "Iteration: 134300\n",
      "Gradient: [[  -5.9394    2.1158   28.1508 -138.4475 -110.346 ]]\n",
      "Weights: [[-4.1444 -1.3021 -0.0361  0.1243  0.0692]]\n",
      "MSE loss: 194.9299\n",
      "Iteration: 134400\n",
      "Gradient: [[  4.5758  -5.7616 -17.0755  64.289  -51.5459]]\n",
      "Weights: [[-4.1446 -1.302  -0.0362  0.1243  0.0692]]\n",
      "MSE loss: 194.9062\n",
      "Iteration: 134500\n",
      "Gradient: [[  -4.2868    9.7285    9.8677   -4.0781 -217.8521]]\n",
      "Weights: [[-4.1446 -1.302  -0.0363  0.1243  0.0692]]\n",
      "MSE loss: 194.889\n",
      "Iteration: 134600\n",
      "Gradient: [[   0.7091  -16.8422   -8.333  -114.6744  -49.0216]]\n",
      "Weights: [[-4.1437 -1.302  -0.0365  0.1243  0.0692]]\n",
      "MSE loss: 194.8701\n",
      "Iteration: 134700\n",
      "Gradient: [[  4.7935 -25.4821  58.2094 192.1904 -81.8714]]\n",
      "Weights: [[-4.1435 -1.3018 -0.0365  0.1242  0.0692]]\n",
      "MSE loss: 194.854\n",
      "Iteration: 134800\n",
      "Gradient: [[ 4.4715 -7.9649 43.3097 -6.0223  2.1222]]\n",
      "Weights: [[-4.1443 -1.3016 -0.0367  0.1242  0.0692]]\n",
      "MSE loss: 194.8267\n",
      "Iteration: 134900\n",
      "Gradient: [[  0.5828 -22.6163  28.4124 -47.9642 173.0597]]\n",
      "Weights: [[-4.1449 -1.3015 -0.0368  0.1242  0.0692]]\n",
      "MSE loss: 194.8044\n",
      "Iteration: 135000\n",
      "Gradient: [[   4.3456   24.8512   -0.8555  150.4026 -101.2881]]\n",
      "Weights: [[-4.145  -1.3012 -0.0369  0.1242  0.0692]]\n",
      "MSE loss: 194.7818\n",
      "Iteration: 135100\n",
      "Gradient: [[   2.9595    4.4234   48.4385 -183.324  -425.2002]]\n",
      "Weights: [[-4.146  -1.3008 -0.037   0.1242  0.0692]]\n",
      "MSE loss: 194.757\n",
      "Iteration: 135200\n",
      "Gradient: [[ -5.6207 -26.7616 -14.9449  18.903   64.8079]]\n",
      "Weights: [[-4.1458 -1.3006 -0.0371  0.1242  0.0692]]\n",
      "MSE loss: 194.7358\n",
      "Iteration: 135300\n",
      "Gradient: [[ -0.8782   6.8999   8.8563 -28.588   98.1124]]\n",
      "Weights: [[-4.1459 -1.3001 -0.0371  0.1242  0.0692]]\n",
      "MSE loss: 194.7257\n",
      "Iteration: 135400\n",
      "Gradient: [[  5.0182   3.4619   5.4017 149.9187 -92.3971]]\n",
      "Weights: [[-4.1474 -1.3    -0.0372  0.1242  0.0692]]\n",
      "MSE loss: 194.7054\n",
      "Iteration: 135500\n",
      "Gradient: [[  -2.4634   22.9883  -39.3796  -35.0461 -202.4171]]\n",
      "Weights: [[-4.1465 -1.2999 -0.0373  0.1242  0.0693]]\n",
      "MSE loss: 194.6873\n",
      "Iteration: 135600\n",
      "Gradient: [[  -4.4381   -4.6181   35.9513 -112.2739   -4.8945]]\n",
      "Weights: [[-4.1474 -1.2998 -0.0373  0.1242  0.0693]]\n",
      "MSE loss: 194.6721\n",
      "Iteration: 135700\n",
      "Gradient: [[   3.4569   11.2713    0.6664  -29.9652 -212.3623]]\n",
      "Weights: [[-4.1481 -1.2994 -0.0375  0.1242  0.0693]]\n",
      "MSE loss: 194.6504\n",
      "Iteration: 135800\n",
      "Gradient: [[  1.6697   5.819  -38.7881  44.554    0.4532]]\n",
      "Weights: [[-4.148  -1.2992 -0.0376  0.1242  0.0693]]\n",
      "MSE loss: 194.6303\n",
      "Iteration: 135900\n",
      "Gradient: [[  5.5462  12.7874  39.5652 -90.1367 308.3811]]\n",
      "Weights: [[-4.1484 -1.299  -0.0376  0.1242  0.0693]]\n",
      "MSE loss: 194.6124\n",
      "Iteration: 136000\n",
      "Gradient: [[  6.7097  12.1104  48.2835 -54.351  -27.7607]]\n",
      "Weights: [[-4.1483 -1.299  -0.0377  0.1242  0.0693]]\n",
      "MSE loss: 194.596\n",
      "Iteration: 136100\n",
      "Gradient: [[  3.268  -11.758   -8.2601  25.7768 -78.5347]]\n",
      "Weights: [[-4.1479 -1.2988 -0.0378  0.1242  0.0693]]\n",
      "MSE loss: 194.5748\n",
      "Iteration: 136200\n",
      "Gradient: [[   7.8466   -9.5277   -0.6878  -12.9186 -105.1668]]\n",
      "Weights: [[-4.1476 -1.2987 -0.0379  0.1242  0.0693]]\n",
      "MSE loss: 194.5583\n",
      "Iteration: 136300\n",
      "Gradient: [[  4.1367 -12.8258  36.3821  76.2762 146.0898]]\n",
      "Weights: [[-4.1475 -1.2984 -0.038   0.1242  0.0693]]\n",
      "MSE loss: 194.542\n",
      "Iteration: 136400\n",
      "Gradient: [[   9.9858    8.4578  -13.6834 -203.4818    7.6857]]\n",
      "Weights: [[-4.1476 -1.2985 -0.0382  0.1242  0.0693]]\n",
      "MSE loss: 194.5183\n",
      "Iteration: 136500\n",
      "Gradient: [[  6.0191  12.5132  33.2242  72.843  254.7257]]\n",
      "Weights: [[-4.1479 -1.2984 -0.0383  0.1242  0.0693]]\n",
      "MSE loss: 194.5017\n",
      "Iteration: 136600\n",
      "Gradient: [[  3.3593   2.4044  56.1268 -31.827  -32.4177]]\n",
      "Weights: [[-4.1478 -1.2982 -0.0384  0.1242  0.0693]]\n",
      "MSE loss: 194.4787\n",
      "Iteration: 136700\n",
      "Gradient: [[  -6.6499   -6.7456  -40.7034  -10.8465 -175.4105]]\n",
      "Weights: [[-4.1473 -1.2978 -0.0385  0.1242  0.0694]]\n",
      "MSE loss: 194.4537\n",
      "Iteration: 136800\n",
      "Gradient: [[   3.8771   -9.1531   17.9236 -253.4089 -390.4832]]\n",
      "Weights: [[-4.1456 -1.2976 -0.0386  0.1242  0.0694]]\n",
      "MSE loss: 194.4405\n",
      "Iteration: 136900\n",
      "Gradient: [[ -14.1598   21.9001   33.9655 -191.7937   26.25  ]]\n",
      "Weights: [[-4.1464 -1.2975 -0.0388  0.1242  0.0694]]\n",
      "MSE loss: 194.407\n",
      "Iteration: 137000\n",
      "Gradient: [[  13.733     7.358   -12.8839  107.1462 -456.6441]]\n",
      "Weights: [[-4.1467 -1.2974 -0.0389  0.1242  0.0694]]\n",
      "MSE loss: 194.3882\n",
      "Iteration: 137100\n",
      "Gradient: [[ -10.5993   13.007    85.0496 -243.3265   61.4439]]\n",
      "Weights: [[-4.1457 -1.2973 -0.039   0.1242  0.0694]]\n",
      "MSE loss: 194.3722\n",
      "Iteration: 137200\n",
      "Gradient: [[   4.3143   -8.142    -0.9619  -10.7112 -244.9592]]\n",
      "Weights: [[-4.147  -1.2973 -0.0391  0.1242  0.0694]]\n",
      "MSE loss: 194.3461\n",
      "Iteration: 137300\n",
      "Gradient: [[  7.7012 -15.1775   5.733   95.3657 205.1771]]\n",
      "Weights: [[-4.146  -1.2972 -0.0392  0.1242  0.0694]]\n",
      "MSE loss: 194.3245\n",
      "Iteration: 137400\n",
      "Gradient: [[   5.1791   12.5268   11.9352 -120.7798  -62.1135]]\n",
      "Weights: [[-4.1459 -1.2971 -0.0393  0.1242  0.0694]]\n",
      "MSE loss: 194.309\n",
      "Iteration: 137500\n",
      "Gradient: [[  -7.4248   -7.1255   -7.3016  -62.911  -729.632 ]]\n",
      "Weights: [[-4.1449 -1.2971 -0.0394  0.1241  0.0694]]\n",
      "MSE loss: 194.2948\n",
      "Iteration: 137600\n",
      "Gradient: [[   9.0669    8.1079   -5.3905    8.2078 -216.8224]]\n",
      "Weights: [[-4.1451 -1.2969 -0.0395  0.1241  0.0694]]\n",
      "MSE loss: 194.2737\n",
      "Iteration: 137700\n",
      "Gradient: [[ -6.5045  -6.598   23.3364  11.5957 231.3457]]\n",
      "Weights: [[-4.1466 -1.2969 -0.0396  0.1241  0.0694]]\n",
      "MSE loss: 194.2486\n",
      "Iteration: 137800\n",
      "Gradient: [[-21.4844  17.446   24.4996  44.3577 -69.7476]]\n",
      "Weights: [[-4.1473 -1.2966 -0.0397  0.1241  0.0695]]\n",
      "MSE loss: 194.2303\n",
      "Iteration: 137900\n",
      "Gradient: [[   3.9438  -19.5033  -23.4599  -46.3341 -117.7612]]\n",
      "Weights: [[-4.1485 -1.2964 -0.0398  0.1241  0.0695]]\n",
      "MSE loss: 194.2166\n",
      "Iteration: 138000\n",
      "Gradient: [[ 17.2588   8.7486  40.5154 -70.5689 -26.4427]]\n",
      "Weights: [[-4.1469 -1.2964 -0.0399  0.1241  0.0695]]\n",
      "MSE loss: 194.1915\n",
      "Iteration: 138100\n",
      "Gradient: [[  1.0298   9.6796  -8.6561 104.9613 -80.2077]]\n",
      "Weights: [[-4.1473 -1.2965 -0.04    0.1241  0.0695]]\n",
      "MSE loss: 194.1809\n",
      "Iteration: 138200\n",
      "Gradient: [[-12.9304   6.7473  -1.494  -74.9947 113.0041]]\n",
      "Weights: [[-4.1484 -1.2964 -0.04    0.1241  0.0695]]\n",
      "MSE loss: 194.1722\n",
      "Iteration: 138300\n",
      "Gradient: [[  -3.4054   15.0837  -14.2811  -64.1054 -101.6429]]\n",
      "Weights: [[-4.1485 -1.296  -0.0401  0.1241  0.0695]]\n",
      "MSE loss: 194.1496\n",
      "Iteration: 138400\n",
      "Gradient: [[  11.7254  -35.465   -17.6954  -61.1374 -258.8862]]\n",
      "Weights: [[-4.1482 -1.296  -0.0401  0.1241  0.0695]]\n",
      "MSE loss: 194.1328\n",
      "Iteration: 138500\n",
      "Gradient: [[ -6.3424 -12.9493  21.0707  45.5365 -14.9303]]\n",
      "Weights: [[-4.1475 -1.296  -0.0402  0.1241  0.0695]]\n",
      "MSE loss: 194.1159\n",
      "Iteration: 138600\n",
      "Gradient: [[  -3.1697   11.59    -12.4307  -11.066  -313.6133]]\n",
      "Weights: [[-4.1479 -1.2958 -0.0403  0.1241  0.0695]]\n",
      "MSE loss: 194.0966\n",
      "Iteration: 138700\n",
      "Gradient: [[ -15.4962   11.718    48.7489   71.4807 -497.8158]]\n",
      "Weights: [[-4.1463 -1.2957 -0.0404  0.1241  0.0695]]\n",
      "MSE loss: 194.0781\n",
      "Iteration: 138800\n",
      "Gradient: [[-9.418300e+00  3.522800e+00  5.163960e+01 -2.419000e-01  3.772353e+02]]\n",
      "Weights: [[-4.146  -1.2955 -0.0405  0.1241  0.0695]]\n",
      "MSE loss: 194.0537\n",
      "Iteration: 138900\n",
      "Gradient: [[  24.5466   -5.7822    8.8408  -50.7487 -320.3127]]\n",
      "Weights: [[-4.1475 -1.2954 -0.0407  0.1241  0.0696]]\n",
      "MSE loss: 194.0293\n",
      "Iteration: 139000\n",
      "Gradient: [[   2.7636   10.3374    8.0634  141.447  -506.3847]]\n",
      "Weights: [[-4.1479 -1.2952 -0.0408  0.1241  0.0696]]\n",
      "MSE loss: 194.004\n",
      "Iteration: 139100\n",
      "Gradient: [[25.728  -8.1714 -1.3627 43.6176 90.6914]]\n",
      "Weights: [[-4.1481 -1.2951 -0.0409  0.1241  0.0696]]\n",
      "MSE loss: 193.981\n",
      "Iteration: 139200\n",
      "Gradient: [[  0.9802  -3.6764 -81.4044  32.0046 230.4858]]\n",
      "Weights: [[-4.1498 -1.2948 -0.041   0.1241  0.0696]]\n",
      "MSE loss: 193.9707\n",
      "Iteration: 139300\n",
      "Gradient: [[-14.6119  -5.8319  -5.6582 -10.7801 129.7591]]\n",
      "Weights: [[-4.1492 -1.2944 -0.0411  0.1241  0.0696]]\n",
      "MSE loss: 193.9489\n",
      "Iteration: 139400\n",
      "Gradient: [[16.1774 12.2865  1.6371 30.1477 97.874 ]]\n",
      "Weights: [[-4.1483 -1.2942 -0.0411  0.1241  0.0696]]\n",
      "MSE loss: 193.9397\n",
      "Iteration: 139500\n",
      "Gradient: [[ -7.9479 -12.889  -28.5276   4.7432  59.4121]]\n",
      "Weights: [[-4.1485 -1.2941 -0.0411  0.124   0.0696]]\n",
      "MSE loss: 193.9271\n",
      "Iteration: 139600\n",
      "Gradient: [[  -1.7321    9.8776  -60.1259   25.2759 -172.3278]]\n",
      "Weights: [[-4.1479 -1.294  -0.0412  0.124   0.0696]]\n",
      "MSE loss: 193.9103\n",
      "Iteration: 139700\n",
      "Gradient: [[-2.7785 -4.5563 26.4569 42.7723 77.2347]]\n",
      "Weights: [[-4.147  -1.294  -0.0413  0.124   0.0696]]\n",
      "MSE loss: 193.8906\n",
      "Iteration: 139800\n",
      "Gradient: [[  2.774  -13.1062  13.7549   3.3399  38.7028]]\n",
      "Weights: [[-4.1473 -1.2937 -0.0414  0.124   0.0696]]\n",
      "MSE loss: 193.8717\n",
      "Iteration: 139900\n",
      "Gradient: [[ -8.0435 -14.9169  33.8015  -2.2198  81.2756]]\n",
      "Weights: [[-4.1471 -1.2936 -0.0415  0.124   0.0696]]\n",
      "MSE loss: 193.8555\n",
      "Iteration: 140000\n",
      "Gradient: [[  0.7986  -2.1657 -48.3958 -13.1032 472.1976]]\n",
      "Weights: [[-4.1484 -1.2934 -0.0415  0.124   0.0696]]\n",
      "MSE loss: 193.8424\n",
      "Iteration: 140100\n",
      "Gradient: [[ -15.463     5.0863  -12.945   -64.0466 -189.78  ]]\n",
      "Weights: [[-4.1481 -1.2932 -0.0416  0.124   0.0696]]\n",
      "MSE loss: 193.8314\n",
      "Iteration: 140200\n",
      "Gradient: [[   2.5509   -7.1244   26.2775  -31.348  -461.2309]]\n",
      "Weights: [[-4.1474 -1.2932 -0.0417  0.124   0.0697]]\n",
      "MSE loss: 193.8123\n",
      "Iteration: 140300\n",
      "Gradient: [[   3.9544   -5.152    -8.6309  -50.1368 -555.3158]]\n",
      "Weights: [[-4.1485 -1.2934 -0.0418  0.124   0.0697]]\n",
      "MSE loss: 193.7958\n",
      "Iteration: 140400\n",
      "Gradient: [[   6.2968    9.1763   80.4423  168.0194 -248.4904]]\n",
      "Weights: [[-4.1468 -1.2934 -0.0419  0.124   0.0697]]\n",
      "MSE loss: 193.7815\n",
      "Iteration: 140500\n",
      "Gradient: [[ 1.176580e+01  4.170000e-02 -1.661300e+01 -9.883800e+01 -1.796311e+02]]\n",
      "Weights: [[-4.1465 -1.2936 -0.0421  0.124   0.0697]]\n",
      "MSE loss: 193.7538\n",
      "Iteration: 140600\n",
      "Gradient: [[  8.4935  -2.9039  37.1174  11.6418 166.1874]]\n",
      "Weights: [[-4.1469 -1.2935 -0.0422  0.124   0.0697]]\n",
      "MSE loss: 193.732\n",
      "Iteration: 140700\n",
      "Gradient: [[  15.2515    7.0613   25.7679  -74.016  -196.4516]]\n",
      "Weights: [[-4.1473 -1.2934 -0.0423  0.124   0.0697]]\n",
      "MSE loss: 193.7088\n",
      "Iteration: 140800\n",
      "Gradient: [[   5.7618   -3.6711   22.1309  -60.1871 -597.3373]]\n",
      "Weights: [[-4.1477 -1.2931 -0.0423  0.124   0.0697]]\n",
      "MSE loss: 193.6922\n",
      "Iteration: 140900\n",
      "Gradient: [[ -7.8148  15.4523  10.624  -77.9747  59.1963]]\n",
      "Weights: [[-4.1477 -1.2929 -0.0425  0.124   0.0697]]\n",
      "MSE loss: 193.6698\n",
      "Iteration: 141000\n",
      "Gradient: [[  -4.0506   19.1673   13.0079  -47.5652 -170.3911]]\n",
      "Weights: [[-4.1479 -1.2929 -0.0425  0.124   0.0697]]\n",
      "MSE loss: 193.6565\n",
      "Iteration: 141100\n",
      "Gradient: [[   4.7989    2.0264    3.7975  -29.0622 -156.2558]]\n",
      "Weights: [[-4.1483 -1.2928 -0.0426  0.124   0.0697]]\n",
      "MSE loss: 193.642\n",
      "Iteration: 141200\n",
      "Gradient: [[ -11.8537   14.2117  -17.6666 -175.4715  157.0359]]\n",
      "Weights: [[-4.1477 -1.2924 -0.0427  0.1239  0.0697]]\n",
      "MSE loss: 193.6224\n",
      "Iteration: 141300\n",
      "Gradient: [[20.6034 -0.291  16.0808 55.2203 25.8477]]\n",
      "Weights: [[-4.1479 -1.2922 -0.0428  0.1239  0.0698]]\n",
      "MSE loss: 193.6013\n",
      "Iteration: 141400\n",
      "Gradient: [[  -0.7448  -23.9186   26.4617  207.0088 -146.4054]]\n",
      "Weights: [[-4.1473 -1.2922 -0.0429  0.1239  0.0698]]\n",
      "MSE loss: 193.5883\n",
      "Iteration: 141500\n",
      "Gradient: [[   7.1841   14.7763   78.0427 -146.3788  104.7047]]\n",
      "Weights: [[-4.1457 -1.2922 -0.043   0.1239  0.0698]]\n",
      "MSE loss: 193.5797\n",
      "Iteration: 141600\n",
      "Gradient: [[  2.0412  12.6396  28.1165 -30.5914 -11.5617]]\n",
      "Weights: [[-4.146  -1.2923 -0.0431  0.1239  0.0698]]\n",
      "MSE loss: 193.5615\n",
      "Iteration: 141700\n",
      "Gradient: [[  -3.9292    4.6935    1.2637  108.1688 -106.6275]]\n",
      "Weights: [[-4.1461 -1.2925 -0.0432  0.1239  0.0698]]\n",
      "MSE loss: 193.5471\n",
      "Iteration: 141800\n",
      "Gradient: [[   3.3008   -9.714    62.9955 -132.2251 -127.8055]]\n",
      "Weights: [[-4.1459 -1.2923 -0.0433  0.1239  0.0698]]\n",
      "MSE loss: 193.5239\n",
      "Iteration: 141900\n",
      "Gradient: [[ -10.8692    5.1469   -7.955  -109.1697 -482.6737]]\n",
      "Weights: [[-4.1469 -1.2921 -0.0435  0.1239  0.0698]]\n",
      "MSE loss: 193.5037\n",
      "Iteration: 142000\n",
      "Gradient: [[ 1.523650e+01 -2.714100e+01  1.786600e+00 -1.492000e-01 -2.972739e+02]]\n",
      "Weights: [[-4.1477 -1.2919 -0.0435  0.1239  0.0698]]\n",
      "MSE loss: 193.49\n",
      "Iteration: 142100\n",
      "Gradient: [[  23.1072    1.8814   35.6639   44.5128 -283.9964]]\n",
      "Weights: [[-4.148  -1.2916 -0.0435  0.124   0.0698]]\n",
      "MSE loss: 193.4764\n",
      "Iteration: 142200\n",
      "Gradient: [[   7.673    -8.8972   29.0079 -111.9302  -27.7763]]\n",
      "Weights: [[-4.1477 -1.2914 -0.0436  0.124   0.0698]]\n",
      "MSE loss: 193.4583\n",
      "Iteration: 142300\n",
      "Gradient: [[   7.6787  -26.0659   24.0018  159.5428 -143.0935]]\n",
      "Weights: [[-4.1477 -1.2912 -0.0437  0.1239  0.0698]]\n",
      "MSE loss: 193.4369\n",
      "Iteration: 142400\n",
      "Gradient: [[ 15.2629   9.967    0.4284  -1.4914 142.956 ]]\n",
      "Weights: [[-4.1471 -1.291  -0.0438  0.1239  0.0698]]\n",
      "MSE loss: 193.418\n",
      "Iteration: 142500\n",
      "Gradient: [[  8.8514  -5.6265  75.727  -73.0498  45.2328]]\n",
      "Weights: [[-4.1465 -1.2908 -0.0439  0.1239  0.0698]]\n",
      "MSE loss: 193.4059\n",
      "Iteration: 142600\n",
      "Gradient: [[  -5.2236   -3.5927   60.4999 -106.6719  -74.1347]]\n",
      "Weights: [[-4.147  -1.2908 -0.044   0.1239  0.0698]]\n",
      "MSE loss: 193.3871\n",
      "Iteration: 142700\n",
      "Gradient: [[ -4.6204  -6.908  -19.1063  63.186  -82.7764]]\n",
      "Weights: [[-4.1474 -1.2907 -0.0441  0.1239  0.0699]]\n",
      "MSE loss: 193.3652\n",
      "Iteration: 142800\n",
      "Gradient: [[ 10.5042  -6.2903   4.2295 138.0802  55.6268]]\n",
      "Weights: [[-4.1483 -1.2906 -0.0442  0.1239  0.0699]]\n",
      "MSE loss: 193.3481\n",
      "Iteration: 142900\n",
      "Gradient: [[-3.5796  9.4681 42.9365 65.1381 76.2611]]\n",
      "Weights: [[-4.1475 -1.2903 -0.0443  0.1239  0.0699]]\n",
      "MSE loss: 193.3258\n",
      "Iteration: 143000\n",
      "Gradient: [[   4.7137   -3.3087 -105.6649   10.4195   98.4265]]\n",
      "Weights: [[-4.1474 -1.2902 -0.0444  0.1239  0.0699]]\n",
      "MSE loss: 193.3064\n",
      "Iteration: 143100\n",
      "Gradient: [[   6.617   -29.8347   38.1731  -85.3733 -482.6886]]\n",
      "Weights: [[-4.1486 -1.2901 -0.0444  0.1239  0.0699]]\n",
      "MSE loss: 193.2911\n",
      "Iteration: 143200\n",
      "Gradient: [[-6.862300e+00 -6.310000e-02  3.984770e+01 -1.489610e+01 -3.449551e+02]]\n",
      "Weights: [[-4.1486 -1.2902 -0.0445  0.1239  0.0699]]\n",
      "MSE loss: 193.2752\n",
      "Iteration: 143300\n",
      "Gradient: [[ 15.6909  20.6133  16.8221  -0.8081 -83.0132]]\n",
      "Weights: [[-4.1492 -1.2898 -0.0447  0.1239  0.0699]]\n",
      "MSE loss: 193.2506\n",
      "Iteration: 143400\n",
      "Gradient: [[  -6.8169   -4.5375  -63.2806 -174.0733 -248.1749]]\n",
      "Weights: [[-4.1485 -1.2895 -0.0447  0.1239  0.0699]]\n",
      "MSE loss: 193.2306\n",
      "Iteration: 143500\n",
      "Gradient: [[ 19.7545  -7.3702 -29.1408 144.8827  50.56  ]]\n",
      "Weights: [[-4.1467 -1.2896 -0.0448  0.1239  0.0699]]\n",
      "MSE loss: 193.2195\n",
      "Iteration: 143600\n",
      "Gradient: [[ 1.7743 20.2736 36.9107  7.1521 41.2821]]\n",
      "Weights: [[-4.1474 -1.2896 -0.0449  0.1239  0.0699]]\n",
      "MSE loss: 193.2003\n",
      "Iteration: 143700\n",
      "Gradient: [[  2.2701   0.334   16.0728 -84.6061  23.8167]]\n",
      "Weights: [[-4.1467 -1.2896 -0.045   0.1239  0.07  ]]\n",
      "MSE loss: 193.1894\n",
      "Iteration: 143800\n",
      "Gradient: [[   4.6466  -10.376   -10.9753  -63.7476 -437.8061]]\n",
      "Weights: [[-4.1475 -1.2896 -0.045   0.1239  0.07  ]]\n",
      "MSE loss: 193.1724\n",
      "Iteration: 143900\n",
      "Gradient: [[  3.08    20.2762 -35.8679  34.8653 -78.4889]]\n",
      "Weights: [[-4.1483 -1.2894 -0.0451  0.1239  0.07  ]]\n",
      "MSE loss: 193.1528\n",
      "Iteration: 144000\n",
      "Gradient: [[  5.0008   5.0749  39.6249  20.7104 -82.9453]]\n",
      "Weights: [[-4.1483 -1.2895 -0.0452  0.1239  0.07  ]]\n",
      "MSE loss: 193.1332\n",
      "Iteration: 144100\n",
      "Gradient: [[ -3.5608   3.0718  15.369   69.0543 -24.369 ]]\n",
      "Weights: [[-4.1478 -1.2894 -0.0454  0.1239  0.07  ]]\n",
      "MSE loss: 193.1108\n",
      "Iteration: 144200\n",
      "Gradient: [[  4.003    6.0022  27.7786 105.4892 -74.2718]]\n",
      "Weights: [[-4.1476 -1.2894 -0.0455  0.1239  0.07  ]]\n",
      "MSE loss: 193.0925\n",
      "Iteration: 144300\n",
      "Gradient: [[  -3.6807    0.454    10.7257  -70.1338 -131.6734]]\n",
      "Weights: [[-4.1463 -1.2893 -0.0455  0.1238  0.07  ]]\n",
      "MSE loss: 193.0816\n",
      "Iteration: 144400\n",
      "Gradient: [[ -20.8269  -13.2246   50.504    77.6457 -127.1936]]\n",
      "Weights: [[-4.1454 -1.2894 -0.0456  0.1238  0.07  ]]\n",
      "MSE loss: 193.0715\n",
      "Iteration: 144500\n",
      "Gradient: [[ -0.5858   8.1671  28.6713 -55.5596 131.43  ]]\n",
      "Weights: [[-4.1463 -1.2891 -0.0458  0.1238  0.07  ]]\n",
      "MSE loss: 193.0409\n",
      "Iteration: 144600\n",
      "Gradient: [[  -5.5794   18.8596   21.5921 -198.7662   17.3781]]\n",
      "Weights: [[-4.1465 -1.289  -0.0459  0.1238  0.07  ]]\n",
      "MSE loss: 193.0231\n",
      "Iteration: 144700\n",
      "Gradient: [[  13.8823   18.5888   35.1964   55.2072 -419.4798]]\n",
      "Weights: [[-4.1462 -1.2888 -0.0459  0.1238  0.07  ]]\n",
      "MSE loss: 193.0055\n",
      "Iteration: 144800\n",
      "Gradient: [[-12.9077   8.723    8.3275 167.9745  33.8851]]\n",
      "Weights: [[-4.1465 -1.2887 -0.046   0.1238  0.0701]]\n",
      "MSE loss: 192.9862\n",
      "Iteration: 144900\n",
      "Gradient: [[ -5.4095 -12.2672   4.1901 -81.0812 -76.2643]]\n",
      "Weights: [[-4.1464 -1.2888 -0.0461  0.1238  0.0701]]\n",
      "MSE loss: 192.9676\n",
      "Iteration: 145000\n",
      "Gradient: [[   4.4479   12.6931  -25.0663  -88.8436 -150.5007]]\n",
      "Weights: [[-4.147  -1.2886 -0.0462  0.1238  0.0701]]\n",
      "MSE loss: 192.9501\n",
      "Iteration: 145100\n",
      "Gradient: [[-25.4806 -16.7308  27.4561 -64.3716 164.5124]]\n",
      "Weights: [[-4.1466 -1.2884 -0.0463  0.1238  0.0701]]\n",
      "MSE loss: 192.931\n",
      "Iteration: 145200\n",
      "Gradient: [[ -6.1502 -10.092   32.9008 -30.7643  80.6365]]\n",
      "Weights: [[-4.1465 -1.2882 -0.0464  0.1238  0.0701]]\n",
      "MSE loss: 192.9125\n",
      "Iteration: 145300\n",
      "Gradient: [[  -5.6816    4.9285   39.1143  -41.1925 -522.5774]]\n",
      "Weights: [[-4.1464 -1.2881 -0.0465  0.1238  0.0701]]\n",
      "MSE loss: 192.8984\n",
      "Iteration: 145400\n",
      "Gradient: [[  6.5023   9.8417 -13.3039  52.1577  11.6999]]\n",
      "Weights: [[-4.1471 -1.2878 -0.0465  0.1238  0.0701]]\n",
      "MSE loss: 192.8793\n",
      "Iteration: 145500\n",
      "Gradient: [[  7.4786 -13.9219   6.2005 111.0524 305.2438]]\n",
      "Weights: [[-4.1478 -1.2876 -0.0466  0.1238  0.0701]]\n",
      "MSE loss: 192.8628\n",
      "Iteration: 145600\n",
      "Gradient: [[ 3.8100e-02  5.0596e+00  7.6089e+00 -4.6862e+01 -2.7956e+02]]\n",
      "Weights: [[-4.1469 -1.2874 -0.0467  0.1238  0.0701]]\n",
      "MSE loss: 192.8479\n",
      "Iteration: 145700\n",
      "Gradient: [[   7.5926   14.3027   28.8454  -42.7239 -160.0729]]\n",
      "Weights: [[-4.1464 -1.2874 -0.0468  0.1237  0.0701]]\n",
      "MSE loss: 192.8308\n",
      "Iteration: 145800\n",
      "Gradient: [[  -5.0497    1.9829   17.1856   59.4155 -515.3495]]\n",
      "Weights: [[-4.1468 -1.2873 -0.0469  0.1237  0.0701]]\n",
      "MSE loss: 192.8115\n",
      "Iteration: 145900\n",
      "Gradient: [[ -9.9896  -6.8051 -13.5676 -44.916   32.6638]]\n",
      "Weights: [[-4.1477 -1.2872 -0.047   0.1237  0.0701]]\n",
      "MSE loss: 192.7916\n",
      "Iteration: 146000\n",
      "Gradient: [[  -1.9505   -7.0307   15.9831   68.4822 -578.4857]]\n",
      "Weights: [[-4.1488 -1.287  -0.047   0.1237  0.0702]]\n",
      "MSE loss: 192.7776\n",
      "Iteration: 146100\n",
      "Gradient: [[   9.3629   12.5042    6.0908 -234.4488  254.0425]]\n",
      "Weights: [[-4.1475 -1.2868 -0.0471  0.1237  0.0702]]\n",
      "MSE loss: 192.7608\n",
      "Iteration: 146200\n",
      "Gradient: [[  4.9641  -4.4372  52.0473  98.8367 241.3894]]\n",
      "Weights: [[-4.1472 -1.2867 -0.0472  0.1237  0.0702]]\n",
      "MSE loss: 192.7425\n",
      "Iteration: 146300\n",
      "Gradient: [[ -11.5865  -18.8346  -20.7807   -3.509  -131.0856]]\n",
      "Weights: [[-4.1474 -1.2865 -0.0473  0.1237  0.0702]]\n",
      "MSE loss: 192.7168\n",
      "Iteration: 146400\n",
      "Gradient: [[   2.8003   20.3587    4.7855 -124.7145  207.708 ]]\n",
      "Weights: [[-4.1476 -1.2863 -0.0474  0.1237  0.0702]]\n",
      "MSE loss: 192.6945\n",
      "Iteration: 146500\n",
      "Gradient: [[  14.4409    8.4884   -2.8     -47.4045 -343.8426]]\n",
      "Weights: [[-4.1476 -1.2861 -0.0475  0.1237  0.0702]]\n",
      "MSE loss: 192.6831\n",
      "Iteration: 146600\n",
      "Gradient: [[  -3.7985  -12.6326   21.2666 -172.6178 -592.7478]]\n",
      "Weights: [[-4.147  -1.2861 -0.0476  0.1237  0.0702]]\n",
      "MSE loss: 192.6677\n",
      "Iteration: 146700\n",
      "Gradient: [[-15.2185  -3.9303 -13.1776  37.1142  51.489 ]]\n",
      "Weights: [[-4.148  -1.2861 -0.0477  0.1237  0.0702]]\n",
      "MSE loss: 192.6453\n",
      "Iteration: 146800\n",
      "Gradient: [[   4.6574   -0.7001   -8.6773 -103.0977  -74.7868]]\n",
      "Weights: [[-4.1479 -1.2861 -0.0478  0.1237  0.0702]]\n",
      "MSE loss: 192.6255\n",
      "Iteration: 146900\n",
      "Gradient: [[ 4.5277 -5.0373 56.7467 95.3641 76.5941]]\n",
      "Weights: [[-4.1475 -1.2859 -0.0479  0.1237  0.0702]]\n",
      "MSE loss: 192.6058\n",
      "Iteration: 147000\n",
      "Gradient: [[  -6.4685   -3.9348   33.8585  -80.4605 -180.835 ]]\n",
      "Weights: [[-4.1464 -1.2859 -0.048   0.1237  0.0702]]\n",
      "MSE loss: 192.5895\n",
      "Iteration: 147100\n",
      "Gradient: [[  0.4583   6.0452  -8.6227 -88.4287 -48.8236]]\n",
      "Weights: [[-4.1479 -1.2857 -0.0481  0.1237  0.0703]]\n",
      "MSE loss: 192.5678\n",
      "Iteration: 147200\n",
      "Gradient: [[-7.206000e-01 -4.470000e-02 -2.978490e+01 -4.926220e+01 -4.961049e+02]]\n",
      "Weights: [[-4.1481 -1.2856 -0.0482  0.1237  0.0703]]\n",
      "MSE loss: 192.5499\n",
      "Iteration: 147300\n",
      "Gradient: [[ -6.4074  -1.434  -28.6799 -69.0375 128.0703]]\n",
      "Weights: [[-4.148  -1.2856 -0.0483  0.1237  0.0703]]\n",
      "MSE loss: 192.5317\n",
      "Iteration: 147400\n",
      "Gradient: [[ -9.614  -30.0766  -7.8381  -1.0591  86.9394]]\n",
      "Weights: [[-4.1478 -1.2853 -0.0484  0.1237  0.0703]]\n",
      "MSE loss: 192.5149\n",
      "Iteration: 147500\n",
      "Gradient: [[ -6.5022  30.6203  12.5159  40.5293 107.7436]]\n",
      "Weights: [[-4.1472 -1.2853 -0.0485  0.1237  0.0703]]\n",
      "MSE loss: 192.4975\n",
      "Iteration: 147600\n",
      "Gradient: [[-4.3553  4.2429 -3.1299 41.6034 62.5903]]\n",
      "Weights: [[-4.1472 -1.2851 -0.0486  0.1237  0.0703]]\n",
      "MSE loss: 192.4792\n",
      "Iteration: 147700\n",
      "Gradient: [[ -0.8294  -6.7854   6.5111 108.3692  38.4461]]\n",
      "Weights: [[-4.1482 -1.2849 -0.0487  0.1237  0.0703]]\n",
      "MSE loss: 192.4558\n",
      "Iteration: 147800\n",
      "Gradient: [[  15.8013   14.1563  -31.9995   74.3561 -114.3149]]\n",
      "Weights: [[-4.1474 -1.2847 -0.0488  0.1237  0.0703]]\n",
      "MSE loss: 192.4356\n",
      "Iteration: 147900\n",
      "Gradient: [[ -12.6986  -16.2248  -15.43    -75.0955 -505.5196]]\n",
      "Weights: [[-4.1474 -1.2844 -0.0489  0.1237  0.0703]]\n",
      "MSE loss: 192.4165\n",
      "Iteration: 148000\n",
      "Gradient: [[ -8.1224  12.8213  54.1661 -13.7445 104.1281]]\n",
      "Weights: [[-4.1475 -1.2841 -0.0491  0.1237  0.0703]]\n",
      "MSE loss: 192.3943\n",
      "Iteration: 148100\n",
      "Gradient: [[ 2.2261  7.8044 19.0992 29.6678  8.1682]]\n",
      "Weights: [[-4.1474 -1.2839 -0.0491  0.1236  0.0703]]\n",
      "MSE loss: 192.3768\n",
      "Iteration: 148200\n",
      "Gradient: [[  5.196   -0.861   14.7351 -52.4119 186.5179]]\n",
      "Weights: [[-4.1482 -1.2837 -0.0492  0.1236  0.0703]]\n",
      "MSE loss: 192.3572\n",
      "Iteration: 148300\n",
      "Gradient: [[ -8.3124 -11.8863 -51.0351  10.511  348.2381]]\n",
      "Weights: [[-4.1492 -1.2832 -0.0493  0.1236  0.0703]]\n",
      "MSE loss: 192.3346\n",
      "Iteration: 148400\n",
      "Gradient: [[ -6.7841 -29.694  -19.832   33.0464 -93.931 ]]\n",
      "Weights: [[-4.1495 -1.2831 -0.0494  0.1236  0.0704]]\n",
      "MSE loss: 192.3161\n",
      "Iteration: 148500\n",
      "Gradient: [[ 9.49000e-02 -1.83234e+01 -9.71000e-01 -7.83173e+01 -2.52095e+02]]\n",
      "Weights: [[-4.149  -1.2831 -0.0496  0.1236  0.0704]]\n",
      "MSE loss: 192.2937\n",
      "Iteration: 148600\n",
      "Gradient: [[-8.4109 10.076  54.5143 63.9807  6.3245]]\n",
      "Weights: [[-4.1482 -1.2828 -0.0497  0.1236  0.0704]]\n",
      "MSE loss: 192.2699\n",
      "Iteration: 148700\n",
      "Gradient: [[  4.5109  21.8056  21.4108 -82.1979 140.3535]]\n",
      "Weights: [[-4.1482 -1.2828 -0.0498  0.1236  0.0704]]\n",
      "MSE loss: 192.2557\n",
      "Iteration: 148800\n",
      "Gradient: [[  3.7911   6.5564  14.3351  90.1735 -83.6915]]\n",
      "Weights: [[-4.1467 -1.2827 -0.0499  0.1236  0.0704]]\n",
      "MSE loss: 192.2415\n",
      "Iteration: 148900\n",
      "Gradient: [[ -12.9348   21.3427   84.4607  -49.1192 -101.7062]]\n",
      "Weights: [[-4.1472 -1.2826 -0.0499  0.1236  0.0704]]\n",
      "MSE loss: 192.2248\n",
      "Iteration: 149000\n",
      "Gradient: [[  11.1817  -11.9032   35.2374  -41.3414 -152.1311]]\n",
      "Weights: [[-4.1468 -1.2826 -0.0501  0.1236  0.0704]]\n",
      "MSE loss: 192.2084\n",
      "Iteration: 149100\n",
      "Gradient: [[-10.7307   1.2291   9.6505 130.2189 -70.3267]]\n",
      "Weights: [[-4.1466 -1.2825 -0.0501  0.1236  0.0704]]\n",
      "MSE loss: 192.1936\n",
      "Iteration: 149200\n",
      "Gradient: [[ -11.6562  -10.8594   49.6556  -89.7672 -292.3993]]\n",
      "Weights: [[-4.1473 -1.2825 -0.0503  0.1236  0.0704]]\n",
      "MSE loss: 192.173\n",
      "Iteration: 149300\n",
      "Gradient: [[ 13.4002 -12.8481  37.1257  -7.3209  21.3568]]\n",
      "Weights: [[-4.1485 -1.2822 -0.0504  0.1236  0.0704]]\n",
      "MSE loss: 192.1473\n",
      "Iteration: 149400\n",
      "Gradient: [[  6.8505 -10.872   13.5679 -51.0437  73.68  ]]\n",
      "Weights: [[-4.1497 -1.2821 -0.0504  0.1236  0.0704]]\n",
      "MSE loss: 192.1361\n",
      "Iteration: 149500\n",
      "Gradient: [[  7.7489  -3.4717  19.7328  65.435  209.7255]]\n",
      "Weights: [[-4.149  -1.2821 -0.0506  0.1236  0.0705]]\n",
      "MSE loss: 192.1155\n",
      "Iteration: 149600\n",
      "Gradient: [[  -3.1576    3.4586   46.3023  -64.5826 -119.0182]]\n",
      "Weights: [[-4.1493 -1.282  -0.0507  0.1236  0.0705]]\n",
      "MSE loss: 192.0984\n",
      "Iteration: 149700\n",
      "Gradient: [[  -1.8908   -5.7325   -6.42    -36.8815 -293.0821]]\n",
      "Weights: [[-4.15   -1.2818 -0.0507  0.1236  0.0705]]\n",
      "MSE loss: 192.0883\n",
      "Iteration: 149800\n",
      "Gradient: [[   3.181    26.2361   34.3553  -70.3668 -463.6402]]\n",
      "Weights: [[-4.15   -1.2815 -0.0509  0.1236  0.0705]]\n",
      "MSE loss: 192.0625\n",
      "Iteration: 149900\n",
      "Gradient: [[  -5.4754   -0.2019   -7.549    33.1878 -160.4797]]\n",
      "Weights: [[-4.1485 -1.2814 -0.0509  0.1236  0.0705]]\n",
      "MSE loss: 192.0424\n",
      "Iteration: 150000\n",
      "Gradient: [[  -6.7363    4.4867  -15.6513  -39.0221 -685.5407]]\n",
      "Weights: [[-4.1478 -1.2811 -0.0511  0.1236  0.0705]]\n",
      "MSE loss: 192.0211\n",
      "Iteration: 150100\n",
      "Gradient: [[  -6.0608   -2.0242   18.8722   23.7983 -339.9934]]\n",
      "Weights: [[-4.1467 -1.2809 -0.0512  0.1236  0.0705]]\n",
      "MSE loss: 192.0077\n",
      "Iteration: 150200\n",
      "Gradient: [[   2.3703   14.4893  -15.4832   -9.9967 -429.2804]]\n",
      "Weights: [[-4.1472 -1.2808 -0.0513  0.1236  0.0705]]\n",
      "MSE loss: 191.9824\n",
      "Iteration: 150300\n",
      "Gradient: [[ 4.515200e+00  1.615000e-01  5.411210e+01 -4.908280e+01  1.664163e+02]]\n",
      "Weights: [[-4.1462 -1.2807 -0.0515  0.1236  0.0705]]\n",
      "MSE loss: 191.965\n",
      "Iteration: 150400\n",
      "Gradient: [[  -6.932    -4.5863   46.6305  -14.3989 -156.7397]]\n",
      "Weights: [[-4.1466 -1.2806 -0.0516  0.1236  0.0705]]\n",
      "MSE loss: 191.9449\n",
      "Iteration: 150500\n",
      "Gradient: [[   4.1978    3.897    14.7015 -128.0252  -67.6742]]\n",
      "Weights: [[-4.148  -1.2805 -0.0517  0.1235  0.0705]]\n",
      "MSE loss: 191.9153\n",
      "Iteration: 150600\n",
      "Gradient: [[   1.9059   20.9282    2.1544   99.6561 -101.0482]]\n",
      "Weights: [[-4.148  -1.2804 -0.0518  0.1235  0.0705]]\n",
      "MSE loss: 191.897\n",
      "Iteration: 150700\n",
      "Gradient: [[ -8.93   -14.9758  25.7433  44.02   305.4737]]\n",
      "Weights: [[-4.1486 -1.2804 -0.0519  0.1235  0.0705]]\n",
      "MSE loss: 191.8848\n",
      "Iteration: 150800\n",
      "Gradient: [[   2.5157   10.4324   -8.2436   68.6088 -184.7735]]\n",
      "Weights: [[-4.1495 -1.2802 -0.0519  0.1235  0.0706]]\n",
      "MSE loss: 191.8756\n",
      "Iteration: 150900\n",
      "Gradient: [[  -4.8709   20.4072   87.6308   -3.3449 -204.1534]]\n",
      "Weights: [[-4.1504 -1.2801 -0.052   0.1235  0.0706]]\n",
      "MSE loss: 191.8676\n",
      "Iteration: 151000\n",
      "Gradient: [[ -1.8826  -8.2737  75.7168 -18.2368  10.1207]]\n",
      "Weights: [[-4.1499 -1.2799 -0.0521  0.1235  0.0706]]\n",
      "MSE loss: 191.8451\n",
      "Iteration: 151100\n",
      "Gradient: [[  7.0521   5.1205  35.2998 140.1866 -47.9003]]\n",
      "Weights: [[-4.1484 -1.2797 -0.0522  0.1236  0.0706]]\n",
      "MSE loss: 191.828\n",
      "Iteration: 151200\n",
      "Gradient: [[   4.2244   36.5374   40.3208  -51.9076 -251.1627]]\n",
      "Weights: [[-4.1485 -1.2795 -0.0522  0.1235  0.0706]]\n",
      "MSE loss: 191.8099\n",
      "Iteration: 151300\n",
      "Gradient: [[ -5.0304   3.4592  20.1713 -64.693   93.4844]]\n",
      "Weights: [[-4.1499 -1.2793 -0.0523  0.1235  0.0706]]\n",
      "MSE loss: 191.789\n",
      "Iteration: 151400\n",
      "Gradient: [[   8.1374   -6.0562   40.5882 -104.6531 -155.7116]]\n",
      "Weights: [[-4.1491 -1.2793 -0.0524  0.1235  0.0706]]\n",
      "MSE loss: 191.774\n",
      "Iteration: 151500\n",
      "Gradient: [[  11.7557   -2.5723   86.6265  114.8476 -242.6404]]\n",
      "Weights: [[-4.1499 -1.2792 -0.0526  0.1235  0.0706]]\n",
      "MSE loss: 191.7533\n",
      "Iteration: 151600\n",
      "Gradient: [[ -3.4995 -14.3017  23.7689 -75.1508 -85.402 ]]\n",
      "Weights: [[-4.1492 -1.2791 -0.0527  0.1235  0.0706]]\n",
      "MSE loss: 191.7294\n",
      "Iteration: 151700\n",
      "Gradient: [[ -4.5686   4.3783  46.0587 -45.0368 -38.4851]]\n",
      "Weights: [[-4.1487 -1.2788 -0.0528  0.1235  0.0706]]\n",
      "MSE loss: 191.7049\n",
      "Iteration: 151800\n",
      "Gradient: [[ -12.1177   -1.2687  -25.3215  103.0195 -443.0434]]\n",
      "Weights: [[-4.1489 -1.2786 -0.0529  0.1235  0.0706]]\n",
      "MSE loss: 191.6824\n",
      "Iteration: 151900\n",
      "Gradient: [[  -1.3753   14.0172   34.5042  -16.1601 -584.478 ]]\n",
      "Weights: [[-4.1494 -1.2784 -0.053   0.1235  0.0706]]\n",
      "MSE loss: 191.6653\n",
      "Iteration: 152000\n",
      "Gradient: [[ 18.7368  -8.8855  12.3253  62.53   128.8642]]\n",
      "Weights: [[-4.1507 -1.278  -0.053   0.1235  0.0707]]\n",
      "MSE loss: 191.6476\n",
      "Iteration: 152100\n",
      "Gradient: [[  -0.7212  -11.814   -46.9897   39.5247 -225.4539]]\n",
      "Weights: [[-4.1516 -1.2776 -0.0531  0.1235  0.0707]]\n",
      "MSE loss: 191.6304\n",
      "Iteration: 152200\n",
      "Gradient: [[ -2.7528 -11.4652 -40.1434  46.6488 259.5349]]\n",
      "Weights: [[-4.1506 -1.2774 -0.0531  0.1235  0.0707]]\n",
      "MSE loss: 191.6131\n",
      "Iteration: 152300\n",
      "Gradient: [[  -1.9199    4.6486  -27.6137   84.1587 -629.5517]]\n",
      "Weights: [[-4.1513 -1.2774 -0.0532  0.1235  0.0707]]\n",
      "MSE loss: 191.6023\n",
      "Iteration: 152400\n",
      "Gradient: [[ 15.9608 -29.9622 -31.5561  16.1587 182.5557]]\n",
      "Weights: [[-4.1519 -1.2772 -0.0533  0.1234  0.0707]]\n",
      "MSE loss: 191.5877\n",
      "Iteration: 152500\n",
      "Gradient: [[ 23.0871 -15.5345  16.7744 -80.3181 167.4197]]\n",
      "Weights: [[-4.1521 -1.2769 -0.0533  0.1234  0.0707]]\n",
      "MSE loss: 191.5704\n",
      "Iteration: 152600\n",
      "Gradient: [[ -13.5371    8.4276   -0.9413   13.5322 -140.6534]]\n",
      "Weights: [[-4.1523 -1.2764 -0.0533  0.1234  0.0707]]\n",
      "MSE loss: 191.552\n",
      "Iteration: 152700\n",
      "Gradient: [[ -1.3741  -9.825   11.9341 -36.8715 254.4716]]\n",
      "Weights: [[-4.1499 -1.2764 -0.0534  0.1234  0.0707]]\n",
      "MSE loss: 191.5424\n",
      "Iteration: 152800\n",
      "Gradient: [[  15.5824    3.0984   27.7321  -89.7827 -186.3526]]\n",
      "Weights: [[-4.1495 -1.2765 -0.0536  0.1234  0.0707]]\n",
      "MSE loss: 191.5218\n",
      "Iteration: 152900\n",
      "Gradient: [[15.1335 -9.3068 12.2508 46.9872 -2.1322]]\n",
      "Weights: [[-4.1503 -1.2765 -0.0537  0.1234  0.0707]]\n",
      "MSE loss: 191.5038\n",
      "Iteration: 153000\n",
      "Gradient: [[   8.1427   13.2184  -19.066   104.2535 -131.7646]]\n",
      "Weights: [[-4.1506 -1.2763 -0.0538  0.1234  0.0707]]\n",
      "MSE loss: 191.4857\n",
      "Iteration: 153100\n",
      "Gradient: [[  -6.96      2.4916  -10.3125   36.6429 -110.404 ]]\n",
      "Weights: [[-4.1526 -1.2761 -0.0539  0.1234  0.0707]]\n",
      "MSE loss: 191.4638\n",
      "Iteration: 153200\n",
      "Gradient: [[ -8.5565  -9.5543   8.0521  16.7954 -88.0736]]\n",
      "Weights: [[-4.1525 -1.2759 -0.054   0.1234  0.0708]]\n",
      "MSE loss: 191.4392\n",
      "Iteration: 153300\n",
      "Gradient: [[   2.193   -12.3434   82.9898   56.0563 -523.9887]]\n",
      "Weights: [[-4.1525 -1.2757 -0.0541  0.1234  0.0708]]\n",
      "MSE loss: 191.421\n",
      "Iteration: 153400\n",
      "Gradient: [[   1.5803  -30.5654  -23.2821 -154.219  -233.5019]]\n",
      "Weights: [[-4.1528 -1.2755 -0.0542  0.1234  0.0708]]\n",
      "MSE loss: 191.4047\n",
      "Iteration: 153500\n",
      "Gradient: [[ -9.637  -29.1268  -4.161  -98.4084  85.785 ]]\n",
      "Weights: [[-4.1518 -1.2752 -0.0543  0.1234  0.0708]]\n",
      "MSE loss: 191.3779\n",
      "Iteration: 153600\n",
      "Gradient: [[   3.7347    4.1994   15.0346   23.3965 -107.3502]]\n",
      "Weights: [[-4.1509 -1.2752 -0.0544  0.1234  0.0708]]\n",
      "MSE loss: 191.3572\n",
      "Iteration: 153700\n",
      "Gradient: [[ -9.5559  15.0718  22.1714  72.3729 -96.1941]]\n",
      "Weights: [[-4.1509 -1.2752 -0.0545  0.1234  0.0708]]\n",
      "MSE loss: 191.3444\n",
      "Iteration: 153800\n",
      "Gradient: [[-14.7806  20.543   16.0554 184.6626  85.6485]]\n",
      "Weights: [[-4.1503 -1.2752 -0.0546  0.1234  0.0708]]\n",
      "MSE loss: 191.3272\n",
      "Iteration: 153900\n",
      "Gradient: [[  8.6823 -11.213   14.5174  70.1858 -48.5493]]\n",
      "Weights: [[-4.1514 -1.275  -0.0547  0.1233  0.0708]]\n",
      "MSE loss: 191.3101\n",
      "Iteration: 154000\n",
      "Gradient: [[  2.0751 -33.4084 -30.8715  70.929   89.8923]]\n",
      "Weights: [[-4.1507 -1.2749 -0.0548  0.1233  0.0708]]\n",
      "MSE loss: 191.2904\n",
      "Iteration: 154100\n",
      "Gradient: [[ -4.6424   8.9429  -7.8749   6.5023 231.5848]]\n",
      "Weights: [[-4.1509 -1.2746 -0.0548  0.1233  0.0708]]\n",
      "MSE loss: 191.2762\n",
      "Iteration: 154200\n",
      "Gradient: [[   1.9018   -8.5194    7.2771 -146.0667 -215.8987]]\n",
      "Weights: [[-4.153  -1.2744 -0.0549  0.1233  0.0708]]\n",
      "MSE loss: 191.2627\n",
      "Iteration: 154300\n",
      "Gradient: [[   2.916    34.8127    0.4578   28.0055 -163.1811]]\n",
      "Weights: [[-4.1519 -1.2742 -0.055   0.1233  0.0708]]\n",
      "MSE loss: 191.2391\n",
      "Iteration: 154400\n",
      "Gradient: [[  -5.5266    8.4394   -9.1606   61.4527 -503.9399]]\n",
      "Weights: [[-4.1512 -1.2741 -0.0551  0.1233  0.0709]]\n",
      "MSE loss: 191.2243\n",
      "Iteration: 154500\n",
      "Gradient: [[  -0.1705    2.0861  -34.3867   80.4704 -126.4475]]\n",
      "Weights: [[-4.1507 -1.2739 -0.0552  0.1233  0.0709]]\n",
      "MSE loss: 191.2051\n",
      "Iteration: 154600\n",
      "Gradient: [[  -1.5493  -11.9928  -12.9465  129.8029 -113.087 ]]\n",
      "Weights: [[-4.152  -1.2736 -0.0553  0.1233  0.0709]]\n",
      "MSE loss: 191.1836\n",
      "Iteration: 154700\n",
      "Gradient: [[  -1.6922   -7.3833    6.846    34.8046 -127.8078]]\n",
      "Weights: [[-4.1511 -1.2734 -0.0554  0.1233  0.0709]]\n",
      "MSE loss: 191.1668\n",
      "Iteration: 154800\n",
      "Gradient: [[  6.5993 -10.8922  78.1427 -67.6238   5.0912]]\n",
      "Weights: [[-4.1514 -1.2736 -0.0554  0.1233  0.0709]]\n",
      "MSE loss: 191.1552\n",
      "Iteration: 154900\n",
      "Gradient: [[  16.2816   -1.5147   20.3703   73.9665 -312.2922]]\n",
      "Weights: [[-4.1509 -1.2734 -0.0555  0.1233  0.0709]]\n",
      "MSE loss: 191.1395\n",
      "Iteration: 155000\n",
      "Gradient: [[  -0.8122    3.4026  -41.4132 -121.1627  322.6101]]\n",
      "Weights: [[-4.1514 -1.2734 -0.0556  0.1233  0.0709]]\n",
      "MSE loss: 191.1203\n",
      "Iteration: 155100\n",
      "Gradient: [[  10.2439   -4.9057  -25.3899  -17.3062 -220.3674]]\n",
      "Weights: [[-4.1503 -1.2732 -0.0557  0.1233  0.0709]]\n",
      "MSE loss: 191.1107\n",
      "Iteration: 155200\n",
      "Gradient: [[   4.0965   20.9145   -0.3177 -126.1144 -146.9804]]\n",
      "Weights: [[-4.1514 -1.273  -0.0557  0.1232  0.0709]]\n",
      "MSE loss: 191.0904\n",
      "Iteration: 155300\n",
      "Gradient: [[ -0.1834   4.5099   3.7169 106.2232 -67.6698]]\n",
      "Weights: [[-4.1512 -1.2729 -0.0558  0.1232  0.0709]]\n",
      "MSE loss: 191.0749\n",
      "Iteration: 155400\n",
      "Gradient: [[ -16.0056    1.604   -21.8634 -121.9352  464.8813]]\n",
      "Weights: [[-4.1506 -1.2727 -0.0559  0.1232  0.0709]]\n",
      "MSE loss: 191.0556\n",
      "Iteration: 155500\n",
      "Gradient: [[   8.6514   -8.3424   36.2281 -143.698  -107.2632]]\n",
      "Weights: [[-4.1513 -1.2726 -0.056   0.1232  0.0709]]\n",
      "MSE loss: 191.0303\n",
      "Iteration: 155600\n",
      "Gradient: [[  7.1422   4.3894 -15.6877  89.3621 -27.7592]]\n",
      "Weights: [[-4.1516 -1.2724 -0.0561  0.1232  0.0709]]\n",
      "MSE loss: 191.0127\n",
      "Iteration: 155700\n",
      "Gradient: [[   5.1981   -8.8977   14.6762   89.3389 -129.8748]]\n",
      "Weights: [[-4.1523 -1.2722 -0.0562  0.1232  0.071 ]]\n",
      "MSE loss: 190.9992\n",
      "Iteration: 155800\n",
      "Gradient: [[   8.1698    5.7723   54.5141 -122.412  -153.9835]]\n",
      "Weights: [[-4.1508 -1.2722 -0.0563  0.1232  0.071 ]]\n",
      "MSE loss: 190.9816\n",
      "Iteration: 155900\n",
      "Gradient: [[  -8.2484    4.5567   21.356  -102.0684  189.2094]]\n",
      "Weights: [[-4.1508 -1.2722 -0.0564  0.1232  0.071 ]]\n",
      "MSE loss: 190.9689\n",
      "Iteration: 156000\n",
      "Gradient: [[   2.005    -1.2784  -65.2847 -347.3097 -150.8711]]\n",
      "Weights: [[-4.15   -1.2722 -0.0565  0.1232  0.071 ]]\n",
      "MSE loss: 190.9583\n",
      "Iteration: 156100\n",
      "Gradient: [[  0.1948  -0.1909   3.1903 -26.4774 135.2366]]\n",
      "Weights: [[-4.1504 -1.2721 -0.0565  0.1232  0.071 ]]\n",
      "MSE loss: 190.9401\n",
      "Iteration: 156200\n",
      "Gradient: [[  -9.5242   -7.1365   -2.0107   58.879  -439.8787]]\n",
      "Weights: [[-4.1505 -1.2721 -0.0566  0.1232  0.071 ]]\n",
      "MSE loss: 190.9246\n",
      "Iteration: 156300\n",
      "Gradient: [[ -0.4048   5.0213 -29.1736  11.3943 322.0098]]\n",
      "Weights: [[-4.1507 -1.2718 -0.0567  0.1232  0.071 ]]\n",
      "MSE loss: 190.904\n",
      "Iteration: 156400\n",
      "Gradient: [[  0.7209  -2.2136 -38.256  -26.6703 -93.7217]]\n",
      "Weights: [[-4.1512 -1.2717 -0.0568  0.1232  0.071 ]]\n",
      "MSE loss: 190.8882\n",
      "Iteration: 156500\n",
      "Gradient: [[  1.6712   6.0675  30.1165 -38.4938 112.0154]]\n",
      "Weights: [[-4.1516 -1.2715 -0.0569  0.1231  0.071 ]]\n",
      "MSE loss: 190.8705\n",
      "Iteration: 156600\n",
      "Gradient: [[  6.3915  -9.3144   8.1804 -86.7515 187.8018]]\n",
      "Weights: [[-4.1519 -1.2713 -0.057   0.1231  0.071 ]]\n",
      "MSE loss: 190.85\n",
      "Iteration: 156700\n",
      "Gradient: [[ 4.4281  7.0332  4.7218 78.8073 -3.2143]]\n",
      "Weights: [[-4.1513 -1.2709 -0.0571  0.1231  0.071 ]]\n",
      "MSE loss: 190.8323\n",
      "Iteration: 156800\n",
      "Gradient: [[ -5.0232  10.3073  59.2349  96.6249 -88.1384]]\n",
      "Weights: [[-4.151  -1.2707 -0.0572  0.1231  0.071 ]]\n",
      "MSE loss: 190.8122\n",
      "Iteration: 156900\n",
      "Gradient: [[  -7.1272   24.154    19.9903  -39.8465 -165.608 ]]\n",
      "Weights: [[-4.1499 -1.2708 -0.0573  0.1231  0.071 ]]\n",
      "MSE loss: 190.8031\n",
      "Iteration: 157000\n",
      "Gradient: [[ -5.9458  -1.5235 -11.3469  44.2605 -98.2469]]\n",
      "Weights: [[-4.1501 -1.2707 -0.0574  0.1231  0.071 ]]\n",
      "MSE loss: 190.7808\n",
      "Iteration: 157100\n",
      "Gradient: [[   1.6648  -11.5549   -2.0765  -71.8979 -125.2764]]\n",
      "Weights: [[-4.1511 -1.2706 -0.0575  0.1231  0.0711]]\n",
      "MSE loss: 190.7623\n",
      "Iteration: 157200\n",
      "Gradient: [[  1.707    9.8357   2.432   28.4029 -34.1712]]\n",
      "Weights: [[-4.1502 -1.2701 -0.0576  0.1231  0.0711]]\n",
      "MSE loss: 190.7404\n",
      "Iteration: 157300\n",
      "Gradient: [[   3.8466   -2.6566   43.5503 -360.2998  -77.9856]]\n",
      "Weights: [[-4.1511 -1.27   -0.0577  0.1231  0.0711]]\n",
      "MSE loss: 190.7244\n",
      "Iteration: 157400\n",
      "Gradient: [[  -0.8033    5.0179  -23.4885   31.3805 -166.1456]]\n",
      "Weights: [[-4.1511 -1.2699 -0.0578  0.1231  0.0711]]\n",
      "MSE loss: 190.7049\n",
      "Iteration: 157500\n",
      "Gradient: [[  -6.7923   -6.6895   28.9485 -131.0216  154.3462]]\n",
      "Weights: [[-4.1518 -1.2697 -0.0579  0.1231  0.0711]]\n",
      "MSE loss: 190.6848\n",
      "Iteration: 157600\n",
      "Gradient: [[ -1.4828 -11.8014 -33.2482 110.3755 -15.1861]]\n",
      "Weights: [[-4.1522 -1.2694 -0.058   0.1231  0.0711]]\n",
      "MSE loss: 190.6637\n",
      "Iteration: 157700\n",
      "Gradient: [[ 15.0435   7.9088 -47.7708  78.9603 365.8012]]\n",
      "Weights: [[-4.1518 -1.2692 -0.0581  0.1231  0.0711]]\n",
      "MSE loss: 190.647\n",
      "Iteration: 157800\n",
      "Gradient: [[ 2.692000e-01  9.539600e+00  4.100860e+01  8.142940e+01 -3.148199e+02]]\n",
      "Weights: [[-4.1501 -1.2692 -0.0582  0.1231  0.0711]]\n",
      "MSE loss: 190.6412\n",
      "Iteration: 157900\n",
      "Gradient: [[-11.9252  -8.0676 -45.2969 -17.6499  82.4067]]\n",
      "Weights: [[-4.1502 -1.269  -0.0583  0.123   0.0711]]\n",
      "MSE loss: 190.6172\n",
      "Iteration: 158000\n",
      "Gradient: [[   6.8823   -7.7682   49.0868  -72.4534 -316.5165]]\n",
      "Weights: [[-4.151  -1.2688 -0.0584  0.123   0.0711]]\n",
      "MSE loss: 190.5942\n",
      "Iteration: 158100\n",
      "Gradient: [[  -9.9266    6.839   -21.5026   30.2265 -147.636 ]]\n",
      "Weights: [[-4.1514 -1.2685 -0.0585  0.123   0.0711]]\n",
      "MSE loss: 190.5769\n",
      "Iteration: 158200\n",
      "Gradient: [[-1.00000e-02  6.31700e+00  4.20460e+00  7.13980e+01 -9.44236e+01]]\n",
      "Weights: [[-4.1496 -1.2685 -0.0586  0.123   0.0711]]\n",
      "MSE loss: 190.5701\n",
      "Iteration: 158300\n",
      "Gradient: [[ 23.1825 -18.2541  57.4507  50.2857 -78.3905]]\n",
      "Weights: [[-4.1492 -1.2685 -0.0587  0.123   0.0711]]\n",
      "MSE loss: 190.5569\n",
      "Iteration: 158400\n",
      "Gradient: [[ 15.5626 -11.4524  -7.6721  91.4353   8.3841]]\n",
      "Weights: [[-4.1503 -1.2685 -0.0588  0.123   0.0711]]\n",
      "MSE loss: 190.535\n",
      "Iteration: 158500\n",
      "Gradient: [[   2.7876  -19.4037  -26.5476  101.3836 -165.0433]]\n",
      "Weights: [[-4.1514 -1.2684 -0.0589  0.123   0.0712]]\n",
      "MSE loss: 190.5139\n",
      "Iteration: 158600\n",
      "Gradient: [[-14.913  -40.587   21.5564  17.5716  73.7123]]\n",
      "Weights: [[-4.1514 -1.2682 -0.059   0.123   0.0712]]\n",
      "MSE loss: 190.4933\n",
      "Iteration: 158700\n",
      "Gradient: [[ -13.6697   -2.4617   38.9336  113.5258 -287.4205]]\n",
      "Weights: [[-4.1524 -1.2679 -0.0591  0.123   0.0712]]\n",
      "MSE loss: 190.4759\n",
      "Iteration: 158800\n",
      "Gradient: [[ -2.8399  -5.9963  29.3689 180.5966 125.9848]]\n",
      "Weights: [[-4.1521 -1.2676 -0.0592  0.123   0.0712]]\n",
      "MSE loss: 190.4573\n",
      "Iteration: 158900\n",
      "Gradient: [[  12.826     6.0268   19.5818  -53.6276 -382.4713]]\n",
      "Weights: [[-4.1525 -1.2673 -0.0592  0.123   0.0712]]\n",
      "MSE loss: 190.4393\n",
      "Iteration: 159000\n",
      "Gradient: [[  12.5475    0.5569    3.9243 -138.5435 -200.6897]]\n",
      "Weights: [[-4.1539 -1.2669 -0.0593  0.123   0.0712]]\n",
      "MSE loss: 190.4159\n",
      "Iteration: 159100\n",
      "Gradient: [[-12.1798 -10.8495 -15.3443  79.6595 -91.6755]]\n",
      "Weights: [[-4.1531 -1.2667 -0.0594  0.123   0.0712]]\n",
      "MSE loss: 190.4002\n",
      "Iteration: 159200\n",
      "Gradient: [[ -5.0724  14.9858  16.8428  47.5443 -11.1517]]\n",
      "Weights: [[-4.153  -1.2664 -0.0595  0.123   0.0712]]\n",
      "MSE loss: 190.3829\n",
      "Iteration: 159300\n",
      "Gradient: [[  2.8857 -17.3759  60.9939 -74.4154  49.0789]]\n",
      "Weights: [[-4.1524 -1.2664 -0.0596  0.123   0.0712]]\n",
      "MSE loss: 190.3651\n",
      "Iteration: 159400\n",
      "Gradient: [[  6.1343 -21.6712   1.0596  64.7709  17.6092]]\n",
      "Weights: [[-4.1522 -1.2664 -0.0597  0.123   0.0712]]\n",
      "MSE loss: 190.3434\n",
      "Iteration: 159500\n",
      "Gradient: [[ -3.9908 -16.9868 -19.6154  47.5666 -37.3628]]\n",
      "Weights: [[-4.1522 -1.2663 -0.0598  0.123   0.0712]]\n",
      "MSE loss: 190.3247\n",
      "Iteration: 159600\n",
      "Gradient: [[-13.9001  -7.1385   2.8489 -60.7979 430.9991]]\n",
      "Weights: [[-4.1528 -1.2663 -0.0599  0.1229  0.0712]]\n",
      "MSE loss: 190.3065\n",
      "Iteration: 159700\n",
      "Gradient: [[ -11.876    -8.2446   14.98    -48.6095 -135.8722]]\n",
      "Weights: [[-4.1523 -1.2661 -0.0601  0.1229  0.0713]]\n",
      "MSE loss: 190.2851\n",
      "Iteration: 159800\n",
      "Gradient: [[ -8.8589  -6.9291  -1.0927 -11.6932 227.1225]]\n",
      "Weights: [[-4.1511 -1.266  -0.0602  0.1229  0.0713]]\n",
      "MSE loss: 190.2657\n",
      "Iteration: 159900\n",
      "Gradient: [[  -0.9605   12.146   -36.2999  -16.519  -432.3185]]\n",
      "Weights: [[-4.1509 -1.266  -0.0603  0.1229  0.0713]]\n",
      "MSE loss: 190.2486\n",
      "Iteration: 160000\n",
      "Gradient: [[  -9.8387    8.2776   26.8049  -72.3266 -549.3811]]\n",
      "Weights: [[-4.1509 -1.2659 -0.0604  0.1229  0.0713]]\n",
      "MSE loss: 190.2267\n",
      "Iteration: 160100\n",
      "Gradient: [[   4.2446  -17.4158   13.2153   77.1227 -268.1916]]\n",
      "Weights: [[-4.1515 -1.2658 -0.0605  0.1229  0.0713]]\n",
      "MSE loss: 190.2075\n",
      "Iteration: 160200\n",
      "Gradient: [[  -5.6953  -32.2286   13.4479  -24.0484 -166.732 ]]\n",
      "Weights: [[-4.1515 -1.2656 -0.0606  0.1229  0.0713]]\n",
      "MSE loss: 190.185\n",
      "Iteration: 160300\n",
      "Gradient: [[   7.6044   18.546     1.0196 -160.1892  115.3527]]\n",
      "Weights: [[-4.1524 -1.2655 -0.0607  0.1229  0.0713]]\n",
      "MSE loss: 190.1705\n",
      "Iteration: 160400\n",
      "Gradient: [[   3.3124   21.9144  -16.5846   92.9804 -234.6728]]\n",
      "Weights: [[-4.1535 -1.2654 -0.0607  0.1229  0.0713]]\n",
      "MSE loss: 190.1636\n",
      "Iteration: 160500\n",
      "Gradient: [[  -6.4101    7.5904  -16.5114   92.6451 -112.2101]]\n",
      "Weights: [[-4.1526 -1.2654 -0.0607  0.1229  0.0713]]\n",
      "MSE loss: 190.1556\n",
      "Iteration: 160600\n",
      "Gradient: [[ 1.838070e+01 -2.519000e-01  1.762600e+01  1.145452e+02 -2.961648e+02]]\n",
      "Weights: [[-4.1522 -1.2651 -0.0607  0.1229  0.0713]]\n",
      "MSE loss: 190.1368\n",
      "Iteration: 160700\n",
      "Gradient: [[-12.9939  14.8425  56.3231 -24.3861 188.1954]]\n",
      "Weights: [[-4.1512 -1.2648 -0.0608  0.1229  0.0713]]\n",
      "MSE loss: 190.1242\n",
      "Iteration: 160800\n",
      "Gradient: [[  -6.9939    3.0661   56.1785 -125.3355  138.9732]]\n",
      "Weights: [[-4.15   -1.2649 -0.061   0.1229  0.0714]]\n",
      "MSE loss: 190.105\n",
      "Iteration: 160900\n",
      "Gradient: [[ 11.5987  -7.8908  17.0388 -19.8863  87.6297]]\n",
      "Weights: [[-4.1503 -1.2649 -0.0612  0.1229  0.0714]]\n",
      "MSE loss: 190.0814\n",
      "Iteration: 161000\n",
      "Gradient: [[ -12.2884    1.9412    9.253   -23.9847 -582.0951]]\n",
      "Weights: [[-4.1514 -1.2646 -0.0613  0.1228  0.0714]]\n",
      "MSE loss: 190.0515\n",
      "Iteration: 161100\n",
      "Gradient: [[  20.6375    4.4309  -24.0764   63.4558 -106.8234]]\n",
      "Weights: [[-4.1511 -1.2645 -0.0614  0.1228  0.0714]]\n",
      "MSE loss: 190.037\n",
      "Iteration: 161200\n",
      "Gradient: [[  5.1753  -0.7083  16.4111 -50.6968 246.1486]]\n",
      "Weights: [[-4.1509 -1.2643 -0.0614  0.1228  0.0714]]\n",
      "MSE loss: 190.0221\n",
      "Iteration: 161300\n",
      "Gradient: [[-6.4327 18.3307 31.8936 26.5544 11.789 ]]\n",
      "Weights: [[-4.1508 -1.2644 -0.0616  0.1228  0.0714]]\n",
      "MSE loss: 190.0066\n",
      "Iteration: 161400\n",
      "Gradient: [[ 9.720000e-02  3.138800e+00  3.148990e+01 -7.623870e+01 -2.708507e+02]]\n",
      "Weights: [[-4.1515 -1.2645 -0.0617  0.1228  0.0714]]\n",
      "MSE loss: 189.9842\n",
      "Iteration: 161500\n",
      "Gradient: [[  3.1946 -15.4978 -20.775    0.6064 147.0735]]\n",
      "Weights: [[-4.1517 -1.2642 -0.0617  0.1228  0.0714]]\n",
      "MSE loss: 189.9678\n",
      "Iteration: 161600\n",
      "Gradient: [[  -2.1307  -17.8856   26.7184   44.4931 -272.4246]]\n",
      "Weights: [[-4.1525 -1.264  -0.0618  0.1228  0.0714]]\n",
      "MSE loss: 189.9511\n",
      "Iteration: 161700\n",
      "Gradient: [[  5.9909   0.2164 -54.7152  71.0918  43.2922]]\n",
      "Weights: [[-4.1539 -1.2637 -0.0619  0.1228  0.0714]]\n",
      "MSE loss: 189.9344\n",
      "Iteration: 161800\n",
      "Gradient: [[   3.4538   -6.4407  -19.0317   60.8095 -238.6455]]\n",
      "Weights: [[-4.1512 -1.2636 -0.0619  0.1228  0.0714]]\n",
      "MSE loss: 189.9215\n",
      "Iteration: 161900\n",
      "Gradient: [[-12.4385   9.0661 -24.4706 -27.3902  47.439 ]]\n",
      "Weights: [[-4.1517 -1.2634 -0.062   0.1228  0.0715]]\n",
      "MSE loss: 189.8994\n",
      "Iteration: 162000\n",
      "Gradient: [[ 24.7101  11.5439 -15.884  -75.9287  14.8416]]\n",
      "Weights: [[-4.1526 -1.2631 -0.0621  0.1228  0.0715]]\n",
      "MSE loss: 189.8748\n",
      "Iteration: 162100\n",
      "Gradient: [[  -5.1479   -1.2755   55.6277   11.9392 -151.0874]]\n",
      "Weights: [[-4.1536 -1.2629 -0.0622  0.1228  0.0715]]\n",
      "MSE loss: 189.853\n",
      "Iteration: 162200\n",
      "Gradient: [[  9.2744  13.1146  14.5743  10.9834 -35.5438]]\n",
      "Weights: [[-4.1542 -1.2629 -0.0623  0.1228  0.0715]]\n",
      "MSE loss: 189.8397\n",
      "Iteration: 162300\n",
      "Gradient: [[  -7.9584    1.2872  -12.5319 -110.1591 -451.8962]]\n",
      "Weights: [[-4.1546 -1.2628 -0.0623  0.1228  0.0715]]\n",
      "MSE loss: 189.8289\n",
      "Iteration: 162400\n",
      "Gradient: [[  8.2505 -12.1904  -5.7825  80.6551 107.5582]]\n",
      "Weights: [[-4.1536 -1.2626 -0.0624  0.1228  0.0715]]\n",
      "MSE loss: 189.8089\n",
      "Iteration: 162500\n",
      "Gradient: [[-10.2527   3.912   27.6785  26.4614 -93.3231]]\n",
      "Weights: [[-4.155  -1.2624 -0.0625  0.1228  0.0715]]\n",
      "MSE loss: 189.7929\n",
      "Iteration: 162600\n",
      "Gradient: [[   7.3602    2.0229  -11.4105  117.5068 -121.9046]]\n",
      "Weights: [[-4.155  -1.262  -0.0626  0.1228  0.0715]]\n",
      "MSE loss: 189.7694\n",
      "Iteration: 162700\n",
      "Gradient: [[  -1.6279   -7.3274  -41.084    94.5747 -162.79  ]]\n",
      "Weights: [[-4.1544 -1.2618 -0.0627  0.1228  0.0715]]\n",
      "MSE loss: 189.7505\n",
      "Iteration: 162800\n",
      "Gradient: [[  -1.4963    6.1763  -46.0336   87.587  -350.2585]]\n",
      "Weights: [[-4.1542 -1.2618 -0.0628  0.1228  0.0715]]\n",
      "MSE loss: 189.729\n",
      "Iteration: 162900\n",
      "Gradient: [[ 2.1695 -4.0622 12.3962 66.8417 47.7495]]\n",
      "Weights: [[-4.1547 -1.2616 -0.0629  0.1227  0.0715]]\n",
      "MSE loss: 189.7154\n",
      "Iteration: 163000\n",
      "Gradient: [[  1.243   -4.7074   2.2593 -17.2584  54.4433]]\n",
      "Weights: [[-4.1535 -1.2615 -0.063   0.1228  0.0715]]\n",
      "MSE loss: 189.6994\n",
      "Iteration: 163100\n",
      "Gradient: [[   0.9517    2.4999   42.6684 -115.41   -159.3149]]\n",
      "Weights: [[-4.1517 -1.2615 -0.0631  0.1227  0.0715]]\n",
      "MSE loss: 189.6961\n",
      "Iteration: 163200\n",
      "Gradient: [[ -19.7542   22.1122   74.7684   91.5171 -421.8358]]\n",
      "Weights: [[-4.152  -1.2613 -0.0632  0.1227  0.0716]]\n",
      "MSE loss: 189.6671\n",
      "Iteration: 163300\n",
      "Gradient: [[   5.6405   14.1428   54.6111 -100.499  -381.71  ]]\n",
      "Weights: [[-4.1532 -1.2613 -0.0633  0.1227  0.0716]]\n",
      "MSE loss: 189.6515\n",
      "Iteration: 163400\n",
      "Gradient: [[  -0.7824  -10.9342   25.4815 -185.6575 -358.9913]]\n",
      "Weights: [[-4.1556 -1.2612 -0.0633  0.1227  0.0716]]\n",
      "MSE loss: 189.6445\n",
      "Iteration: 163500\n",
      "Gradient: [[   1.1435   -5.9076   39.589   127.6737 -136.9041]]\n",
      "Weights: [[-4.155  -1.2608 -0.0634  0.1227  0.0716]]\n",
      "MSE loss: 189.6214\n",
      "Iteration: 163600\n",
      "Gradient: [[  -4.9103    2.9955   -0.564    30.1305 -107.3384]]\n",
      "Weights: [[-4.1547 -1.2605 -0.0635  0.1227  0.0716]]\n",
      "MSE loss: 189.602\n",
      "Iteration: 163700\n",
      "Gradient: [[ 12.7366 -10.8751   6.5934  57.3923  27.7374]]\n",
      "Weights: [[-4.1553 -1.2603 -0.0636  0.1227  0.0716]]\n",
      "MSE loss: 189.5869\n",
      "Iteration: 163800\n",
      "Gradient: [[   6.605     2.8373   43.2824  -66.5554 -129.8795]]\n",
      "Weights: [[-4.1561 -1.2598 -0.0636  0.1227  0.0716]]\n",
      "MSE loss: 189.5619\n",
      "Iteration: 163900\n",
      "Gradient: [[  -7.25      1.6128   37.5182  -26.8722 -110.3817]]\n",
      "Weights: [[-4.1554 -1.2594 -0.0637  0.1227  0.0716]]\n",
      "MSE loss: 189.5435\n",
      "Iteration: 164000\n",
      "Gradient: [[-11.4572   0.3096   8.7009  68.2215   6.7145]]\n",
      "Weights: [[-4.1552 -1.2592 -0.0638  0.1227  0.0716]]\n",
      "MSE loss: 189.5309\n",
      "Iteration: 164100\n",
      "Gradient: [[   3.9069   22.3033   52.2814   30.138  -284.9962]]\n",
      "Weights: [[-4.1558 -1.259  -0.0639  0.1226  0.0716]]\n",
      "MSE loss: 189.5079\n",
      "Iteration: 164200\n",
      "Gradient: [[ -2.16    -4.1277 -17.0578 -40.6963 -99.3125]]\n",
      "Weights: [[-4.1574 -1.2588 -0.064   0.1226  0.0716]]\n",
      "MSE loss: 189.4927\n",
      "Iteration: 164300\n",
      "Gradient: [[  10.2262   -6.6852  -43.7427   56.8505 -142.4221]]\n",
      "Weights: [[-4.1559 -1.2585 -0.0641  0.1226  0.0716]]\n",
      "MSE loss: 189.463\n",
      "Iteration: 164400\n",
      "Gradient: [[ -5.7565  20.7379   3.0173 131.2768 121.192 ]]\n",
      "Weights: [[-4.1557 -1.2582 -0.0642  0.1226  0.0717]]\n",
      "MSE loss: 189.4465\n",
      "Iteration: 164500\n",
      "Gradient: [[  -1.9885    3.8656  -22.6148   45.4797 -223.3937]]\n",
      "Weights: [[-4.1556 -1.2583 -0.0643  0.1226  0.0717]]\n",
      "MSE loss: 189.4245\n",
      "Iteration: 164600\n",
      "Gradient: [[ -10.4034  -36.313    34.1305   11.8882 -292.454 ]]\n",
      "Weights: [[-4.1562 -1.2584 -0.0644  0.1226  0.0717]]\n",
      "MSE loss: 189.4112\n",
      "Iteration: 164700\n",
      "Gradient: [[ -2.3503 -11.3898  -2.549  -24.2112  56.168 ]]\n",
      "Weights: [[-4.1562 -1.2582 -0.0645  0.1226  0.0717]]\n",
      "MSE loss: 189.3924\n",
      "Iteration: 164800\n",
      "Gradient: [[ 2.0798  7.3104 32.0801 25.8807  3.4815]]\n",
      "Weights: [[-4.1544 -1.2581 -0.0646  0.1226  0.0717]]\n",
      "MSE loss: 189.3749\n",
      "Iteration: 164900\n",
      "Gradient: [[ -12.0825   -4.8477   24.7197   78.3855 -150.7739]]\n",
      "Weights: [[-4.1537 -1.2581 -0.0647  0.1226  0.0717]]\n",
      "MSE loss: 189.3594\n",
      "Iteration: 165000\n",
      "Gradient: [[  2.4186 -18.8727 -23.6657 162.1329 191.7656]]\n",
      "Weights: [[-4.1529 -1.258  -0.0648  0.1225  0.0717]]\n",
      "MSE loss: 189.3487\n",
      "Iteration: 165100\n",
      "Gradient: [[  14.4961   -5.9018   17.7704    9.411  -167.0771]]\n",
      "Weights: [[-4.1547 -1.2577 -0.0649  0.1225  0.0717]]\n",
      "MSE loss: 189.3132\n",
      "Iteration: 165200\n",
      "Gradient: [[ 3.4268 14.6867 45.6209 59.5967 51.3907]]\n",
      "Weights: [[-4.1548 -1.2575 -0.065   0.1225  0.0717]]\n",
      "MSE loss: 189.2959\n",
      "Iteration: 165300\n",
      "Gradient: [[  8.7687   4.7176  -8.4337 -25.5779 102.8458]]\n",
      "Weights: [[-4.1556 -1.2572 -0.0652  0.1225  0.0717]]\n",
      "MSE loss: 189.2669\n",
      "Iteration: 165400\n",
      "Gradient: [[ -3.7325 -10.2961  -7.5133  -1.7014 -86.392 ]]\n",
      "Weights: [[-4.1554 -1.2569 -0.0653  0.1225  0.0717]]\n",
      "MSE loss: 189.2431\n",
      "Iteration: 165500\n",
      "Gradient: [[   1.5422   -7.1965  -33.4406   10.2908 -260.5482]]\n",
      "Weights: [[-4.1561 -1.2565 -0.0654  0.1225  0.0718]]\n",
      "MSE loss: 189.2175\n",
      "Iteration: 165600\n",
      "Gradient: [[  15.8676  -39.9361   25.8514  -41.8573 -121.0397]]\n",
      "Weights: [[-4.1554 -1.2564 -0.0655  0.1225  0.0718]]\n",
      "MSE loss: 189.2005\n",
      "Iteration: 165700\n",
      "Gradient: [[   1.0868   -2.9875   53.5837   91.3154 -350.4144]]\n",
      "Weights: [[-4.1553 -1.2564 -0.0656  0.1225  0.0718]]\n",
      "MSE loss: 189.1872\n",
      "Iteration: 165800\n",
      "Gradient: [[  11.5979    8.6444   82.5539   53.27   -113.1876]]\n",
      "Weights: [[-4.1552 -1.2562 -0.0657  0.1225  0.0718]]\n",
      "MSE loss: 189.1692\n",
      "Iteration: 165900\n",
      "Gradient: [[-4.0456 20.3644 16.6633 -0.3895 19.0303]]\n",
      "Weights: [[-4.1555 -1.2558 -0.0657  0.1225  0.0718]]\n",
      "MSE loss: 189.1506\n",
      "Iteration: 166000\n",
      "Gradient: [[  -6.1463   -5.8793   18.3971   53.71   -116.9942]]\n",
      "Weights: [[-4.1559 -1.2557 -0.0658  0.1225  0.0718]]\n",
      "MSE loss: 189.1334\n",
      "Iteration: 166100\n",
      "Gradient: [[  2.4301   6.988   29.9778  40.1265 197.7461]]\n",
      "Weights: [[-4.1551 -1.2557 -0.0659  0.1225  0.0718]]\n",
      "MSE loss: 189.1181\n",
      "Iteration: 166200\n",
      "Gradient: [[  1.1417  -6.4594 -27.4159 -18.0017 -23.9178]]\n",
      "Weights: [[-4.1556 -1.2555 -0.0661  0.1225  0.0718]]\n",
      "MSE loss: 189.0915\n",
      "Iteration: 166300\n",
      "Gradient: [[ -18.251     4.9104  -16.4398   63.151  -146.5241]]\n",
      "Weights: [[-4.1555 -1.2552 -0.0662  0.1224  0.0718]]\n",
      "MSE loss: 189.0743\n",
      "Iteration: 166400\n",
      "Gradient: [[   2.4461   -9.3981   35.2232  -46.7733 -493.7578]]\n",
      "Weights: [[-4.1559 -1.2552 -0.0663  0.1224  0.0718]]\n",
      "MSE loss: 189.0575\n",
      "Iteration: 166500\n",
      "Gradient: [[  -7.8462  -15.9883   29.8793  -80.2744 -255.431 ]]\n",
      "Weights: [[-4.1559 -1.255  -0.0664  0.1224  0.0718]]\n",
      "MSE loss: 189.0415\n",
      "Iteration: 166600\n",
      "Gradient: [[ -5.4032  -0.8805  22.4795 -89.0963 394.2743]]\n",
      "Weights: [[-4.1566 -1.2549 -0.0665  0.1224  0.0718]]\n",
      "MSE loss: 189.0234\n",
      "Iteration: 166700\n",
      "Gradient: [[   4.8935   -3.5057   30.4342   29.239  -389.6831]]\n",
      "Weights: [[-4.1559 -1.2548 -0.0666  0.1224  0.0718]]\n",
      "MSE loss: 189.0042\n",
      "Iteration: 166800\n",
      "Gradient: [[ 13.2353 -18.1738 -20.4518  28.6419 -48.0029]]\n",
      "Weights: [[-4.1568 -1.2547 -0.0667  0.1224  0.0719]]\n",
      "MSE loss: 188.9831\n",
      "Iteration: 166900\n",
      "Gradient: [[ -2.3709  -6.2867  -9.5769 -17.1364  45.2404]]\n",
      "Weights: [[-4.1566 -1.2545 -0.0668  0.1224  0.0719]]\n",
      "MSE loss: 188.9606\n",
      "Iteration: 167000\n",
      "Gradient: [[  9.0383  13.726  -20.0886 -23.6734 -55.7313]]\n",
      "Weights: [[-4.1564 -1.2542 -0.0669  0.1224  0.0719]]\n",
      "MSE loss: 188.9398\n",
      "Iteration: 167100\n",
      "Gradient: [[ -6.4832   3.4702  17.1795  67.6779 -54.1578]]\n",
      "Weights: [[-4.158  -1.2539 -0.067   0.1224  0.0719]]\n",
      "MSE loss: 188.9213\n",
      "Iteration: 167200\n",
      "Gradient: [[   3.5873  -12.1321   60.7728 -102.2994 -341.3976]]\n",
      "Weights: [[-4.1568 -1.2537 -0.0671  0.1224  0.0719]]\n",
      "MSE loss: 188.903\n",
      "Iteration: 167300\n",
      "Gradient: [[  -5.7963   -8.0015    5.1957    4.1283 -134.5264]]\n",
      "Weights: [[-4.1578 -1.2536 -0.0672  0.1224  0.0719]]\n",
      "MSE loss: 188.8885\n",
      "Iteration: 167400\n",
      "Gradient: [[   9.5745   -5.6589  -58.0431  -34.7338 -211.5682]]\n",
      "Weights: [[-4.1577 -1.2533 -0.0673  0.1224  0.0719]]\n",
      "MSE loss: 188.8704\n",
      "Iteration: 167500\n",
      "Gradient: [[   1.927     2.8918   15.665   -12.5337 -408.9945]]\n",
      "Weights: [[-4.1578 -1.2532 -0.0673  0.1224  0.0719]]\n",
      "MSE loss: 188.8553\n",
      "Iteration: 167600\n",
      "Gradient: [[ -8.6913 -12.0113  -0.8743  24.0566 159.9633]]\n",
      "Weights: [[-4.1566 -1.2531 -0.0674  0.1224  0.0719]]\n",
      "MSE loss: 188.8372\n",
      "Iteration: 167700\n",
      "Gradient: [[  8.5836 -20.3289 -10.7167 157.3137 306.4764]]\n",
      "Weights: [[-4.1565 -1.253  -0.0675  0.1224  0.0719]]\n",
      "MSE loss: 188.8221\n",
      "Iteration: 167800\n",
      "Gradient: [[-3.947  -5.668  -5.9007 -2.7439 98.0582]]\n",
      "Weights: [[-4.1571 -1.2528 -0.0676  0.1224  0.0719]]\n",
      "MSE loss: 188.8029\n",
      "Iteration: 167900\n",
      "Gradient: [[-8.903700e+00 -7.790000e-02  6.963160e+01 -4.742200e+01 -2.753532e+02]]\n",
      "Weights: [[-4.1558 -1.2526 -0.0677  0.1223  0.0719]]\n",
      "MSE loss: 188.7849\n",
      "Iteration: 168000\n",
      "Gradient: [[ -13.7861   11.8732   -2.7674   64.823  -210.2242]]\n",
      "Weights: [[-4.1562 -1.2527 -0.0678  0.1223  0.072 ]]\n",
      "MSE loss: 188.7629\n",
      "Iteration: 168100\n",
      "Gradient: [[   4.0221    6.7682   12.2242  -18.939  -209.8939]]\n",
      "Weights: [[-4.1555 -1.2526 -0.0679  0.1223  0.072 ]]\n",
      "MSE loss: 188.7459\n",
      "Iteration: 168200\n",
      "Gradient: [[ 11.3276  -3.8956 -20.716   32.5163  -3.3542]]\n",
      "Weights: [[-4.1557 -1.2525 -0.068   0.1223  0.072 ]]\n",
      "MSE loss: 188.7301\n",
      "Iteration: 168300\n",
      "Gradient: [[ -16.2909  -22.3355    5.9071  -50.7091 -175.2563]]\n",
      "Weights: [[-4.156  -1.2523 -0.0681  0.1223  0.072 ]]\n",
      "MSE loss: 188.7156\n",
      "Iteration: 168400\n",
      "Gradient: [[ -8.8314  -8.9187  38.1669 -27.1663 113.6503]]\n",
      "Weights: [[-4.1563 -1.2518 -0.0682  0.1223  0.072 ]]\n",
      "MSE loss: 188.6948\n",
      "Iteration: 168500\n",
      "Gradient: [[  0.9993   5.8691  41.6595 -21.2731  40.3628]]\n",
      "Weights: [[-4.1564 -1.2518 -0.0682  0.1223  0.072 ]]\n",
      "MSE loss: 188.6763\n",
      "Iteration: 168600\n",
      "Gradient: [[ 2.2618  0.0705  5.7944  6.4263 -3.8981]]\n",
      "Weights: [[-4.1568 -1.2515 -0.0683  0.1223  0.072 ]]\n",
      "MSE loss: 188.6585\n",
      "Iteration: 168700\n",
      "Gradient: [[   3.7834    1.5125  -11.8939   39.5752 -275.0084]]\n",
      "Weights: [[-4.1573 -1.2513 -0.0684  0.1223  0.072 ]]\n",
      "MSE loss: 188.6381\n",
      "Iteration: 168800\n",
      "Gradient: [[  -7.2607    4.6219   11.5772   20.3871 -144.7001]]\n",
      "Weights: [[-4.1576 -1.2512 -0.0685  0.1223  0.072 ]]\n",
      "MSE loss: 188.6215\n",
      "Iteration: 168900\n",
      "Gradient: [[  -7.7695   -7.5533   52.0912   50.9595 -428.9884]]\n",
      "Weights: [[-4.1583 -1.251  -0.0686  0.1223  0.072 ]]\n",
      "MSE loss: 188.6082\n",
      "Iteration: 169000\n",
      "Gradient: [[  4.1277  -6.1997   3.3191  55.6317 256.6213]]\n",
      "Weights: [[-4.1577 -1.2507 -0.0687  0.1223  0.072 ]]\n",
      "MSE loss: 188.5862\n",
      "Iteration: 169100\n",
      "Gradient: [[  -4.3268    9.5309   30.8266   42.1995 -207.4341]]\n",
      "Weights: [[-4.1575 -1.2507 -0.0688  0.1223  0.072 ]]\n",
      "MSE loss: 188.5757\n",
      "Iteration: 169200\n",
      "Gradient: [[   3.8684   12.7418   -3.1819  -70.5875 -210.1004]]\n",
      "Weights: [[-4.1573 -1.2505 -0.0688  0.1223  0.072 ]]\n",
      "MSE loss: 188.5626\n",
      "Iteration: 169300\n",
      "Gradient: [[   4.9      -5.8353   -6.9926   22.2459 -218.2503]]\n",
      "Weights: [[-4.1578 -1.2504 -0.0689  0.1222  0.0721]]\n",
      "MSE loss: 188.5447\n",
      "Iteration: 169400\n",
      "Gradient: [[ -4.8676   0.39   -18.5913  25.0489  33.2676]]\n",
      "Weights: [[-4.1575 -1.2504 -0.069   0.1222  0.0721]]\n",
      "MSE loss: 188.5263\n",
      "Iteration: 169500\n",
      "Gradient: [[   1.1373   -1.8879   30.6062   51.4947 -290.6557]]\n",
      "Weights: [[-4.1575 -1.2503 -0.0691  0.1222  0.0721]]\n",
      "MSE loss: 188.5112\n",
      "Iteration: 169600\n",
      "Gradient: [[-10.0457 -22.062   37.3651 -33.4282 -64.5382]]\n",
      "Weights: [[-4.157  -1.2501 -0.0692  0.1222  0.0721]]\n",
      "MSE loss: 188.4959\n",
      "Iteration: 169700\n",
      "Gradient: [[ -4.7742 -12.2254 -28.2743  62.8638 254.7788]]\n",
      "Weights: [[-4.1572 -1.2501 -0.0693  0.1222  0.0721]]\n",
      "MSE loss: 188.4801\n",
      "Iteration: 169800\n",
      "Gradient: [[ -1.7935  11.5736  83.7512 -20.6813 -55.5682]]\n",
      "Weights: [[-4.1582 -1.2499 -0.0694  0.1222  0.0721]]\n",
      "MSE loss: 188.4633\n",
      "Iteration: 169900\n",
      "Gradient: [[   2.101   -15.3073   27.0787   39.9981 -477.4705]]\n",
      "Weights: [[-4.1584 -1.2498 -0.0694  0.1222  0.0721]]\n",
      "MSE loss: 188.4486\n",
      "Iteration: 170000\n",
      "Gradient: [[ 21.7006 -17.8764  13.8934 -54.4217 309.5083]]\n",
      "Weights: [[-4.1603 -1.2495 -0.0695  0.1222  0.0721]]\n",
      "MSE loss: 188.4368\n",
      "Iteration: 170100\n",
      "Gradient: [[ -9.3293 -13.186  -21.9762   9.8041   3.7796]]\n",
      "Weights: [[-4.1609 -1.249  -0.0696  0.1222  0.0721]]\n",
      "MSE loss: 188.4182\n",
      "Iteration: 170200\n",
      "Gradient: [[ -6.1379   8.9207 -23.5195  -8.8752  62.7924]]\n",
      "Weights: [[-4.1598 -1.2488 -0.0697  0.1222  0.0721]]\n",
      "MSE loss: 188.3925\n",
      "Iteration: 170300\n",
      "Gradient: [[ -0.6725  10.571   -6.0959 -80.6437 -88.7349]]\n",
      "Weights: [[-4.1586 -1.2487 -0.0698  0.1222  0.0721]]\n",
      "MSE loss: 188.3717\n",
      "Iteration: 170400\n",
      "Gradient: [[  13.0859   -1.3103   25.2034    5.6558 -137.5511]]\n",
      "Weights: [[-4.1584 -1.2485 -0.0699  0.1222  0.0721]]\n",
      "MSE loss: 188.3557\n",
      "Iteration: 170500\n",
      "Gradient: [[  9.7721 -26.2005 -10.5952 -21.5511 152.9807]]\n",
      "Weights: [[-4.1588 -1.2485 -0.07    0.1222  0.0721]]\n",
      "MSE loss: 188.3443\n",
      "Iteration: 170600\n",
      "Gradient: [[   4.4928   13.6469   34.1996 -127.7223  -37.5543]]\n",
      "Weights: [[-4.1594 -1.2483 -0.0701  0.1222  0.0721]]\n",
      "MSE loss: 188.324\n",
      "Iteration: 170700\n",
      "Gradient: [[   2.5184   16.0763   24.1404  169.6013 -364.5712]]\n",
      "Weights: [[-4.1598 -1.248  -0.0702  0.1222  0.0722]]\n",
      "MSE loss: 188.3038\n",
      "Iteration: 170800\n",
      "Gradient: [[ -8.5281 -14.4306  25.9644 -59.7098 -78.5297]]\n",
      "Weights: [[-4.1588 -1.2475 -0.0703  0.1222  0.0722]]\n",
      "MSE loss: 188.2737\n",
      "Iteration: 170900\n",
      "Gradient: [[  6.6736   0.8828  81.9248  -0.2692 -73.8793]]\n",
      "Weights: [[-4.1584 -1.2473 -0.0704  0.1221  0.0722]]\n",
      "MSE loss: 188.2567\n",
      "Iteration: 171000\n",
      "Gradient: [[ -1.1698  -6.4935 -27.4806  94.1053 -39.1277]]\n",
      "Weights: [[-4.1588 -1.2472 -0.0705  0.1221  0.0722]]\n",
      "MSE loss: 188.238\n",
      "Iteration: 171100\n",
      "Gradient: [[ -2.1079   3.2771  85.5978  42.0455 258.4795]]\n",
      "Weights: [[-4.1597 -1.247  -0.0706  0.1221  0.0722]]\n",
      "MSE loss: 188.2213\n",
      "Iteration: 171200\n",
      "Gradient: [[   3.7942   17.9893   23.1099  -39.7218 -179.6249]]\n",
      "Weights: [[-4.1602 -1.2468 -0.0707  0.1221  0.0722]]\n",
      "MSE loss: 188.2067\n",
      "Iteration: 171300\n",
      "Gradient: [[   3.6065   -0.7463    4.3532   18.7996 -169.2197]]\n",
      "Weights: [[-4.1605 -1.2464 -0.0708  0.1221  0.0722]]\n",
      "MSE loss: 188.1813\n",
      "Iteration: 171400\n",
      "Gradient: [[   3.1646  -11.8263   38.2047  -83.5658 -371.8914]]\n",
      "Weights: [[-4.1596 -1.2463 -0.0709  0.1221  0.0722]]\n",
      "MSE loss: 188.1651\n",
      "Iteration: 171500\n",
      "Gradient: [[ -6.7238  -1.4987  10.3783 -13.2436 234.7126]]\n",
      "Weights: [[-4.1596 -1.246  -0.071   0.1221  0.0722]]\n",
      "MSE loss: 188.1465\n",
      "Iteration: 171600\n",
      "Gradient: [[  -1.0315   10.6537  -55.0739   -9.4923 -114.4941]]\n",
      "Weights: [[-4.1591 -1.2459 -0.0711  0.1221  0.0722]]\n",
      "MSE loss: 188.1274\n",
      "Iteration: 171700\n",
      "Gradient: [[ 17.7771  24.3845  -4.1887 136.5563 228.5126]]\n",
      "Weights: [[-4.16   -1.2457 -0.0713  0.1221  0.0722]]\n",
      "MSE loss: 188.1062\n",
      "Iteration: 171800\n",
      "Gradient: [[  -8.5807   21.2957   15.1353   98.2017 -431.8233]]\n",
      "Weights: [[-4.1599 -1.2457 -0.0713  0.1221  0.0722]]\n",
      "MSE loss: 188.0911\n",
      "Iteration: 171900\n",
      "Gradient: [[  -6.6419   -8.1119   -1.9115 -131.5882 -380.8156]]\n",
      "Weights: [[-4.1601 -1.2455 -0.0714  0.1221  0.0722]]\n",
      "MSE loss: 188.0811\n",
      "Iteration: 172000\n",
      "Gradient: [[  -0.2633    8.3735   32.3198   72.7092 -151.7279]]\n",
      "Weights: [[-4.1598 -1.2454 -0.0715  0.1221  0.0723]]\n",
      "MSE loss: 188.067\n",
      "Iteration: 172100\n",
      "Gradient: [[   0.5888   -3.1478   10.6423   30.3277 -163.466 ]]\n",
      "Weights: [[-4.1611 -1.2451 -0.0716  0.1221  0.0723]]\n",
      "MSE loss: 188.0474\n",
      "Iteration: 172200\n",
      "Gradient: [[-2.2106  5.9411 16.8316 -8.8894 -5.5031]]\n",
      "Weights: [[-4.1601 -1.2449 -0.0717  0.1221  0.0723]]\n",
      "MSE loss: 188.0247\n",
      "Iteration: 172300\n",
      "Gradient: [[ -5.1974  -5.326  -28.5794 -21.9463 -40.9875]]\n",
      "Weights: [[-4.161  -1.2448 -0.0717  0.1221  0.0723]]\n",
      "MSE loss: 188.0131\n",
      "Iteration: 172400\n",
      "Gradient: [[  11.6546   12.1841   45.9463   58.89   -129.3235]]\n",
      "Weights: [[-4.1596 -1.2448 -0.0718  0.1221  0.0723]]\n",
      "MSE loss: 188.0002\n",
      "Iteration: 172500\n",
      "Gradient: [[-19.79     4.5287   8.753   46.4333 137.0542]]\n",
      "Weights: [[-4.1601 -1.2448 -0.0719  0.1221  0.0723]]\n",
      "MSE loss: 187.9892\n",
      "Iteration: 172600\n",
      "Gradient: [[   0.658   -20.8183    9.1103  113.7992 -214.2701]]\n",
      "Weights: [[-4.16   -1.2447 -0.072   0.1221  0.0723]]\n",
      "MSE loss: 187.9745\n",
      "Iteration: 172700\n",
      "Gradient: [[-9.4922  8.9154  5.4745 24.191  87.5961]]\n",
      "Weights: [[-4.1608 -1.2444 -0.0721  0.1221  0.0723]]\n",
      "MSE loss: 187.9535\n",
      "Iteration: 172800\n",
      "Gradient: [[ -4.84   -17.946  -13.7999 -60.7801 115.2632]]\n",
      "Weights: [[-4.1591 -1.2445 -0.0722  0.1221  0.0723]]\n",
      "MSE loss: 187.9351\n",
      "Iteration: 172900\n",
      "Gradient: [[   3.4144    2.2225   28.2005  -69.1547 -376.3638]]\n",
      "Weights: [[-4.1577 -1.2443 -0.0723  0.1221  0.0723]]\n",
      "MSE loss: 187.9239\n",
      "Iteration: 173000\n",
      "Gradient: [[  3.2267  -5.1278 -20.2137 -92.3233  41.5056]]\n",
      "Weights: [[-4.1597 -1.2441 -0.0725  0.122   0.0723]]\n",
      "MSE loss: 187.8933\n",
      "Iteration: 173100\n",
      "Gradient: [[   6.0797  -19.2003  -24.9961 -143.1615 -140.8426]]\n",
      "Weights: [[-4.1602 -1.2438 -0.0726  0.122   0.0723]]\n",
      "MSE loss: 187.8749\n",
      "Iteration: 173200\n",
      "Gradient: [[ -12.4288  -15.5003  -59.2035  -93.0024 -370.0569]]\n",
      "Weights: [[-4.1602 -1.2438 -0.0726  0.122   0.0723]]\n",
      "MSE loss: 187.8579\n",
      "Iteration: 173300\n",
      "Gradient: [[   2.2884    8.7271    3.9392  116.3046 -109.9769]]\n",
      "Weights: [[-4.1597 -1.2435 -0.0727  0.122   0.0723]]\n",
      "MSE loss: 187.8386\n",
      "Iteration: 173400\n",
      "Gradient: [[   8.2729   11.6048   39.8381  -60.6738 -389.258 ]]\n",
      "Weights: [[-4.1604 -1.2434 -0.0728  0.122   0.0724]]\n",
      "MSE loss: 187.8228\n",
      "Iteration: 173500\n",
      "Gradient: [[  -2.7132   30.2742    7.0843   34.2098 -167.24  ]]\n",
      "Weights: [[-4.1592 -1.2434 -0.0729  0.122   0.0724]]\n",
      "MSE loss: 187.8098\n",
      "Iteration: 173600\n",
      "Gradient: [[ 7.100400e+00  1.287680e+01 -6.760000e-02  1.097391e+02 -1.199961e+02]]\n",
      "Weights: [[-4.1593 -1.2432 -0.073   0.122   0.0724]]\n",
      "MSE loss: 187.785\n",
      "Iteration: 173700\n",
      "Gradient: [[   2.2591   22.7852   -9.7312  215.0935 -344.8113]]\n",
      "Weights: [[-4.1591 -1.243  -0.0731  0.122   0.0724]]\n",
      "MSE loss: 187.7686\n",
      "Iteration: 173800\n",
      "Gradient: [[ -2.1032 -17.1071  17.5745 -85.0394  67.9215]]\n",
      "Weights: [[-4.1606 -1.2427 -0.0732  0.1219  0.0724]]\n",
      "MSE loss: 187.7456\n",
      "Iteration: 173900\n",
      "Gradient: [[  -1.4949   -4.2397  -16.1675   41.5856 -457.4438]]\n",
      "Weights: [[-4.1606 -1.2426 -0.0733  0.122   0.0724]]\n",
      "MSE loss: 187.7316\n",
      "Iteration: 174000\n",
      "Gradient: [[ 0.8177 -8.1785  7.6708 60.8678 67.6855]]\n",
      "Weights: [[-4.1606 -1.2424 -0.0734  0.122   0.0724]]\n",
      "MSE loss: 187.7157\n",
      "Iteration: 174100\n",
      "Gradient: [[  1.4838  -2.525  -10.8088  31.1387 136.3455]]\n",
      "Weights: [[-4.1599 -1.2422 -0.0735  0.1219  0.0724]]\n",
      "MSE loss: 187.6987\n",
      "Iteration: 174200\n",
      "Gradient: [[ -11.2717    6.23     -6.2687 -123.1823 -319.3685]]\n",
      "Weights: [[-4.1599 -1.242  -0.0735  0.1219  0.0724]]\n",
      "MSE loss: 187.682\n",
      "Iteration: 174300\n",
      "Gradient: [[   0.5198   -1.9432   -4.0807  -34.5637 -272.2767]]\n",
      "Weights: [[-4.1607 -1.2416 -0.0736  0.1219  0.0724]]\n",
      "MSE loss: 187.6608\n",
      "Iteration: 174400\n",
      "Gradient: [[  -5.5358   11.3508  -23.3072  -56.4072 -207.7163]]\n",
      "Weights: [[-4.1606 -1.2414 -0.0738  0.1219  0.0724]]\n",
      "MSE loss: 187.6357\n",
      "Iteration: 174500\n",
      "Gradient: [[  -1.084    -4.2764  -14.4575   73.3862 -175.1242]]\n",
      "Weights: [[-4.162  -1.2412 -0.0739  0.1219  0.0724]]\n",
      "MSE loss: 187.6191\n",
      "Iteration: 174600\n",
      "Gradient: [[  0.7648  -6.622  -44.9968 -50.028   98.6744]]\n",
      "Weights: [[-4.1625 -1.2408 -0.0739  0.1219  0.0724]]\n",
      "MSE loss: 187.6012\n",
      "Iteration: 174700\n",
      "Gradient: [[ -9.2405  23.1768  -5.1395 -14.3348 129.0612]]\n",
      "Weights: [[-4.1627 -1.2405 -0.074   0.1219  0.0725]]\n",
      "MSE loss: 187.5869\n",
      "Iteration: 174800\n",
      "Gradient: [[ 10.9568 -15.5451  -9.2712 -34.701   67.2329]]\n",
      "Weights: [[-4.1632 -1.2405 -0.0741  0.1219  0.0725]]\n",
      "MSE loss: 187.5701\n",
      "Iteration: 174900\n",
      "Gradient: [[   5.2921   -7.0645   29.856   161.2339 -135.8909]]\n",
      "Weights: [[-4.1613 -1.2404 -0.0742  0.1219  0.0725]]\n",
      "MSE loss: 187.5497\n",
      "Iteration: 175000\n",
      "Gradient: [[   0.9916    1.6736  -19.2734   89.0501 -124.1997]]\n",
      "Weights: [[-4.1615 -1.2403 -0.0743  0.1219  0.0725]]\n",
      "MSE loss: 187.5336\n",
      "Iteration: 175100\n",
      "Gradient: [[ -7.0417   0.696   52.3313   2.1606 146.1501]]\n",
      "Weights: [[-4.1613 -1.2399 -0.0744  0.1219  0.0725]]\n",
      "MSE loss: 187.5103\n",
      "Iteration: 175200\n",
      "Gradient: [[-10.3952  12.6637  -4.6982  22.4074 -31.2705]]\n",
      "Weights: [[-4.1617 -1.2399 -0.0745  0.1219  0.0725]]\n",
      "MSE loss: 187.4946\n",
      "Iteration: 175300\n",
      "Gradient: [[ -15.1244    3.6682   40.2155  -17.4727 -316.6908]]\n",
      "Weights: [[-4.1615 -1.24   -0.0746  0.1219  0.0725]]\n",
      "MSE loss: 187.4791\n",
      "Iteration: 175400\n",
      "Gradient: [[ 12.8528  -5.1681   5.1358   1.9623 180.4085]]\n",
      "Weights: [[-4.1607 -1.2397 -0.0747  0.1219  0.0725]]\n",
      "MSE loss: 187.4628\n",
      "Iteration: 175500\n",
      "Gradient: [[  2.8666   0.3509  33.2438  64.8027 324.0234]]\n",
      "Weights: [[-4.161  -1.2393 -0.0748  0.1219  0.0725]]\n",
      "MSE loss: 187.4425\n",
      "Iteration: 175600\n",
      "Gradient: [[  0.5697   1.0287  -2.2791  16.8705 189.2191]]\n",
      "Weights: [[-4.1604 -1.2392 -0.0749  0.1218  0.0725]]\n",
      "MSE loss: 187.4319\n",
      "Iteration: 175700\n",
      "Gradient: [[-5.0783 -8.7558 -5.546  16.7524 19.6793]]\n",
      "Weights: [[-4.1597 -1.239  -0.0749  0.1218  0.0725]]\n",
      "MSE loss: 187.4235\n",
      "Iteration: 175800\n",
      "Gradient: [[   0.4321   -3.7343  -10.6743  109.6575 -125.6552]]\n",
      "Weights: [[-4.1607 -1.2389 -0.0751  0.1218  0.0725]]\n",
      "MSE loss: 187.3986\n",
      "Iteration: 175900\n",
      "Gradient: [[ -10.2648  -35.2847   56.7052  108.2123 -170.2364]]\n",
      "Weights: [[-4.1602 -1.2389 -0.0751  0.1218  0.0725]]\n",
      "MSE loss: 187.3875\n",
      "Iteration: 176000\n",
      "Gradient: [[ -5.7997 -12.0308  39.641  -58.2023 -41.9322]]\n",
      "Weights: [[-4.1606 -1.2387 -0.0752  0.1218  0.0725]]\n",
      "MSE loss: 187.3682\n",
      "Iteration: 176100\n",
      "Gradient: [[ 11.0978   4.1549  12.317  -30.6223  -8.0258]]\n",
      "Weights: [[-4.1599 -1.2384 -0.0753  0.1218  0.0725]]\n",
      "MSE loss: 187.3544\n",
      "Iteration: 176200\n",
      "Gradient: [[ -2.2483 -18.3163  77.743   75.212  313.9497]]\n",
      "Weights: [[-4.1593 -1.2384 -0.0754  0.1218  0.0726]]\n",
      "MSE loss: 187.3439\n",
      "Iteration: 176300\n",
      "Gradient: [[   1.7152   13.3657   17.9917   18.8608 -471.8537]]\n",
      "Weights: [[-4.1608 -1.2383 -0.0755  0.1218  0.0726]]\n",
      "MSE loss: 187.3245\n",
      "Iteration: 176400\n",
      "Gradient: [[  6.8528  -7.9303  23.36   -69.5284 148.9116]]\n",
      "Weights: [[-4.1614 -1.2381 -0.0755  0.1218  0.0726]]\n",
      "MSE loss: 187.3058\n",
      "Iteration: 176500\n",
      "Gradient: [[   1.4095   -0.2787   28.1347 -133.7464  -60.824 ]]\n",
      "Weights: [[-4.162  -1.238  -0.0757  0.1218  0.0726]]\n",
      "MSE loss: 187.2853\n",
      "Iteration: 176600\n",
      "Gradient: [[ 3.630000e-02  4.223700e+00  7.880000e-02 -1.416956e+02  2.760563e+02]]\n",
      "Weights: [[-4.163  -1.2381 -0.0757  0.1218  0.0726]]\n",
      "MSE loss: 187.2767\n",
      "Iteration: 176700\n",
      "Gradient: [[ -0.2485  11.1423  22.2349  75.9269 194.332 ]]\n",
      "Weights: [[-4.1632 -1.2377 -0.0758  0.1218  0.0726]]\n",
      "MSE loss: 187.2602\n",
      "Iteration: 176800\n",
      "Gradient: [[  0.5206 -12.7043  11.2315  61.9846 -29.4072]]\n",
      "Weights: [[-4.1622 -1.2375 -0.0758  0.1218  0.0726]]\n",
      "MSE loss: 187.2447\n",
      "Iteration: 176900\n",
      "Gradient: [[   1.1563  -40.4344    8.7358  -50.452  -328.3373]]\n",
      "Weights: [[-4.1622 -1.2373 -0.076   0.1217  0.0726]]\n",
      "MSE loss: 187.2213\n",
      "Iteration: 177000\n",
      "Gradient: [[ 6.118500e+00  2.110000e-02  3.124730e+01 -6.563590e+01 -3.752626e+02]]\n",
      "Weights: [[-4.1618 -1.2373 -0.076   0.1217  0.0726]]\n",
      "MSE loss: 187.2094\n",
      "Iteration: 177100\n",
      "Gradient: [[  -7.4976   14.6641   13.8789   79.236  -171.3402]]\n",
      "Weights: [[-4.1611 -1.2373 -0.0761  0.1217  0.0726]]\n",
      "MSE loss: 187.1963\n",
      "Iteration: 177200\n",
      "Gradient: [[-11.8448  -2.7773  37.2317 -47.6373  72.733 ]]\n",
      "Weights: [[-4.1615 -1.2376 -0.0762  0.1217  0.0726]]\n",
      "MSE loss: 187.1862\n",
      "Iteration: 177300\n",
      "Gradient: [[  6.523  -17.6654  24.723  -44.0302 -74.284 ]]\n",
      "Weights: [[-4.1631 -1.2375 -0.0762  0.1217  0.0726]]\n",
      "MSE loss: 187.1761\n",
      "Iteration: 177400\n",
      "Gradient: [[ -5.0012  -8.1502 -31.6699   2.8312  37.8261]]\n",
      "Weights: [[-4.1641 -1.2372 -0.0763  0.1217  0.0727]]\n",
      "MSE loss: 187.1632\n",
      "Iteration: 177500\n",
      "Gradient: [[ -21.7109  -11.167    24.0987  -15.9742 -308.9128]]\n",
      "Weights: [[-4.163  -1.237  -0.0764  0.1217  0.0727]]\n",
      "MSE loss: 187.1359\n",
      "Iteration: 177600\n",
      "Gradient: [[  0.1716   2.5892  82.8131   3.3035 -44.0958]]\n",
      "Weights: [[-4.1631 -1.2367 -0.0764  0.1217  0.0727]]\n",
      "MSE loss: 187.1172\n",
      "Iteration: 177700\n",
      "Gradient: [[  3.4692  -4.184   -7.3434 -78.4402  33.4926]]\n",
      "Weights: [[-4.1631 -1.2366 -0.0766  0.1217  0.0727]]\n",
      "MSE loss: 187.0987\n",
      "Iteration: 177800\n",
      "Gradient: [[ -1.8737   2.4885 -28.6511  21.8035 298.2604]]\n",
      "Weights: [[-4.1637 -1.2362 -0.0767  0.1217  0.0727]]\n",
      "MSE loss: 187.0787\n",
      "Iteration: 177900\n",
      "Gradient: [[   6.9658   -0.5602    3.3842  -41.2237 -134.9779]]\n",
      "Weights: [[-4.1627 -1.236  -0.0768  0.1217  0.0727]]\n",
      "MSE loss: 187.0581\n",
      "Iteration: 178000\n",
      "Gradient: [[  5.7529 -18.6686  -2.0987   5.2104 371.0084]]\n",
      "Weights: [[-4.1628 -1.2359 -0.0769  0.1217  0.0727]]\n",
      "MSE loss: 187.0404\n",
      "Iteration: 178100\n",
      "Gradient: [[  5.5675 -23.6198 -17.6273  68.334  105.4312]]\n",
      "Weights: [[-4.1619 -1.2357 -0.077   0.1217  0.0727]]\n",
      "MSE loss: 187.0249\n",
      "Iteration: 178200\n",
      "Gradient: [[ -6.9991   6.7773 -33.9908  64.2098 153.8573]]\n",
      "Weights: [[-4.163  -1.2355 -0.0771  0.1217  0.0727]]\n",
      "MSE loss: 187.0034\n",
      "Iteration: 178300\n",
      "Gradient: [[  4.3133   8.6056  25.8272  76.6948 101.9644]]\n",
      "Weights: [[-4.1626 -1.2352 -0.0772  0.1217  0.0727]]\n",
      "MSE loss: 186.9829\n",
      "Iteration: 178400\n",
      "Gradient: [[  6.2938  -0.4448 -13.9757 -84.0925  71.5255]]\n",
      "Weights: [[-4.1625 -1.2349 -0.0773  0.1216  0.0727]]\n",
      "MSE loss: 186.9671\n",
      "Iteration: 178500\n",
      "Gradient: [[-11.1544   4.0339 -40.8371   6.5689 -22.2331]]\n",
      "Weights: [[-4.1623 -1.2348 -0.0774  0.1216  0.0727]]\n",
      "MSE loss: 186.9501\n",
      "Iteration: 178600\n",
      "Gradient: [[ 1.906560e+01 -7.550000e-01 -9.570000e-02 -7.141980e+01 -5.303619e+02]]\n",
      "Weights: [[-4.1614 -1.2345 -0.0775  0.1216  0.0727]]\n",
      "MSE loss: 186.9363\n",
      "Iteration: 178700\n",
      "Gradient: [[  -6.7214    2.2713   53.2823  -48.1699 -197.161 ]]\n",
      "Weights: [[-4.1632 -1.2345 -0.0776  0.1216  0.0727]]\n",
      "MSE loss: 186.9224\n",
      "Iteration: 178800\n",
      "Gradient: [[  2.1966 -12.6476  30.1184  26.5476 -80.2927]]\n",
      "Weights: [[-4.1627 -1.2344 -0.0776  0.1216  0.0727]]\n",
      "MSE loss: 186.9067\n",
      "Iteration: 178900\n",
      "Gradient: [[ -2.4526   9.8712  10.5077  30.598  -53.1772]]\n",
      "Weights: [[-4.1629 -1.2342 -0.0778  0.1216  0.0728]]\n",
      "MSE loss: 186.8903\n",
      "Iteration: 179000\n",
      "Gradient: [[  3.0418  -5.5825 -11.2278  -6.789   90.7731]]\n",
      "Weights: [[-4.1628 -1.2341 -0.0778  0.1216  0.0728]]\n",
      "MSE loss: 186.8727\n",
      "Iteration: 179100\n",
      "Gradient: [[  -4.2094  -19.2753    3.6471   -4.1819 -229.0788]]\n",
      "Weights: [[-4.1619 -1.2341 -0.0779  0.1216  0.0728]]\n",
      "MSE loss: 186.8619\n",
      "Iteration: 179200\n",
      "Gradient: [[  -9.0452   31.352     0.4724  113.984  -261.464 ]]\n",
      "Weights: [[-4.162  -1.2338 -0.078   0.1216  0.0728]]\n",
      "MSE loss: 186.8471\n",
      "Iteration: 179300\n",
      "Gradient: [[ -7.2392  -2.4529 -28.778   41.6075 225.6389]]\n",
      "Weights: [[-4.161  -1.2339 -0.078   0.1216  0.0728]]\n",
      "MSE loss: 186.8391\n",
      "Iteration: 179400\n",
      "Gradient: [[-12.4039  -3.9674  -6.4726   8.8253 111.9299]]\n",
      "Weights: [[-4.1614 -1.2338 -0.0781  0.1216  0.0728]]\n",
      "MSE loss: 186.8252\n",
      "Iteration: 179500\n",
      "Gradient: [[-1.08683e+01  5.21000e-02  1.01636e+01  7.75537e+01  9.43045e+01]]\n",
      "Weights: [[-4.1626 -1.2337 -0.0782  0.1216  0.0728]]\n",
      "MSE loss: 186.8009\n",
      "Iteration: 179600\n",
      "Gradient: [[  -9.4991   -1.6325  -14.2302 -141.7558   63.4252]]\n",
      "Weights: [[-4.1628 -1.2334 -0.0783  0.1216  0.0728]]\n",
      "MSE loss: 186.7815\n",
      "Iteration: 179700\n",
      "Gradient: [[  10.4508   12.9445   26.254   -28.3344 -140.1107]]\n",
      "Weights: [[-4.1637 -1.2332 -0.0783  0.1215  0.0728]]\n",
      "MSE loss: 186.7639\n",
      "Iteration: 179800\n",
      "Gradient: [[  -9.446     7.8519   48.2652   -1.4331 -162.2074]]\n",
      "Weights: [[-4.1643 -1.2332 -0.0784  0.1215  0.0728]]\n",
      "MSE loss: 186.7543\n",
      "Iteration: 179900\n",
      "Gradient: [[   6.5254    0.8526   31.4897  114.1267 -226.0673]]\n",
      "Weights: [[-4.165  -1.2329 -0.0785  0.1215  0.0728]]\n",
      "MSE loss: 186.7387\n",
      "Iteration: 180000\n",
      "Gradient: [[  11.0848   -4.0847   62.0021  -12.5645 -150.8543]]\n",
      "Weights: [[-4.164  -1.2328 -0.0786  0.1215  0.0728]]\n",
      "MSE loss: 186.7208\n",
      "Iteration: 180100\n",
      "Gradient: [[-26.6108  40.1438  25.8523 -28.7016 -39.7916]]\n",
      "Weights: [[-4.1648 -1.2327 -0.0786  0.1215  0.0729]]\n",
      "MSE loss: 186.7092\n",
      "Iteration: 180200\n",
      "Gradient: [[ -5.6193  29.4006 -29.9708 -73.0631 248.3832]]\n",
      "Weights: [[-4.1625 -1.2326 -0.0787  0.1215  0.0729]]\n",
      "MSE loss: 186.6845\n",
      "Iteration: 180300\n",
      "Gradient: [[ 11.1158  -2.2613 -23.6117  59.475   59.1237]]\n",
      "Weights: [[-4.1635 -1.2325 -0.0789  0.1215  0.0729]]\n",
      "MSE loss: 186.6658\n",
      "Iteration: 180400\n",
      "Gradient: [[ 13.5265   0.2615   9.4836 -23.1099 123.6393]]\n",
      "Weights: [[-4.1631 -1.2323 -0.079   0.1215  0.0729]]\n",
      "MSE loss: 186.6459\n",
      "Iteration: 180500\n",
      "Gradient: [[ -14.3898    7.4837  -17.8227 -178.3319   58.5406]]\n",
      "Weights: [[-4.1624 -1.2323 -0.079   0.1215  0.0729]]\n",
      "MSE loss: 186.6342\n",
      "Iteration: 180600\n",
      "Gradient: [[-10.4924   2.4099  -5.1624 -72.627  -63.9073]]\n",
      "Weights: [[-4.1628 -1.2322 -0.0791  0.1215  0.0729]]\n",
      "MSE loss: 186.6224\n",
      "Iteration: 180700\n",
      "Gradient: [[  -3.3408   17.6519   -7.5007 -151.2054  135.3518]]\n",
      "Weights: [[-4.1636 -1.2318 -0.0792  0.1215  0.0729]]\n",
      "MSE loss: 186.5996\n",
      "Iteration: 180800\n",
      "Gradient: [[   8.877    -7.2011   12.0434   51.3798 -142.1228]]\n",
      "Weights: [[-4.1628 -1.2318 -0.0793  0.1215  0.0729]]\n",
      "MSE loss: 186.5848\n",
      "Iteration: 180900\n",
      "Gradient: [[   5.2222    5.7922  -13.6184    3.5399 -471.4519]]\n",
      "Weights: [[-4.1636 -1.2315 -0.0794  0.1215  0.0729]]\n",
      "MSE loss: 186.5613\n",
      "Iteration: 181000\n",
      "Gradient: [[  -9.058    -2.7067    3.0096  -28.3153 -262.3796]]\n",
      "Weights: [[-4.1638 -1.2312 -0.0795  0.1215  0.0729]]\n",
      "MSE loss: 186.5438\n",
      "Iteration: 181100\n",
      "Gradient: [[-12.1928  -0.9203 -59.492   60.6006  17.4759]]\n",
      "Weights: [[-4.1641 -1.2311 -0.0796  0.1215  0.0729]]\n",
      "MSE loss: 186.5235\n",
      "Iteration: 181200\n",
      "Gradient: [[  -1.7248   -0.8123   19.1999   32.5195 -738.0317]]\n",
      "Weights: [[-4.1646 -1.2311 -0.0797  0.1215  0.0729]]\n",
      "MSE loss: 186.5089\n",
      "Iteration: 181300\n",
      "Gradient: [[   1.1465  -16.134     5.0755   31.5359 -402.3932]]\n",
      "Weights: [[-4.1646 -1.2308 -0.0798  0.1215  0.073 ]]\n",
      "MSE loss: 186.4889\n",
      "Iteration: 181400\n",
      "Gradient: [[   2.6176    5.4017   47.7937 -211.0035  310.4182]]\n",
      "Weights: [[-4.1648 -1.2307 -0.0799  0.1215  0.073 ]]\n",
      "MSE loss: 186.4708\n",
      "Iteration: 181500\n",
      "Gradient: [[  21.2811   -5.8853   39.1666    5.6938 -265.3531]]\n",
      "Weights: [[-4.1646 -1.2304 -0.08    0.1214  0.073 ]]\n",
      "MSE loss: 186.4527\n",
      "Iteration: 181600\n",
      "Gradient: [[  -0.5153  -17.9008   40.533    87.5331 -106.1142]]\n",
      "Weights: [[-4.1645 -1.2305 -0.0801  0.1215  0.073 ]]\n",
      "MSE loss: 186.4409\n",
      "Iteration: 181700\n",
      "Gradient: [[11.767   3.0848  8.4213  1.7511 -4.7351]]\n",
      "Weights: [[-4.1635 -1.2302 -0.0802  0.1214  0.073 ]]\n",
      "MSE loss: 186.4156\n",
      "Iteration: 181800\n",
      "Gradient: [[  5.1947   1.9624  11.2151 106.3574  74.8555]]\n",
      "Weights: [[-4.1639 -1.2301 -0.0803  0.1214  0.073 ]]\n",
      "MSE loss: 186.3978\n",
      "Iteration: 181900\n",
      "Gradient: [[  -8.0449  -10.7583   16.5767  -41.505  -188.878 ]]\n",
      "Weights: [[-4.1638 -1.2299 -0.0804  0.1214  0.073 ]]\n",
      "MSE loss: 186.378\n",
      "Iteration: 182000\n",
      "Gradient: [[   7.4915  -12.2442   19.5661  -66.1314 -180.6123]]\n",
      "Weights: [[-4.164  -1.2297 -0.0806  0.1214  0.073 ]]\n",
      "MSE loss: 186.3552\n",
      "Iteration: 182100\n",
      "Gradient: [[  -1.8523    8.8919   12.0041   46.1853 -167.3249]]\n",
      "Weights: [[-4.1649 -1.2297 -0.0806  0.1214  0.073 ]]\n",
      "MSE loss: 186.3446\n",
      "Iteration: 182200\n",
      "Gradient: [[  6.3666   6.3412   4.7442 -41.1781 487.3757]]\n",
      "Weights: [[-4.1655 -1.2295 -0.0807  0.1214  0.073 ]]\n",
      "MSE loss: 186.3318\n",
      "Iteration: 182300\n",
      "Gradient: [[ -7.263   21.9301 -17.702    8.5911 140.6355]]\n",
      "Weights: [[-4.1648 -1.2292 -0.0808  0.1214  0.073 ]]\n",
      "MSE loss: 186.3056\n",
      "Iteration: 182400\n",
      "Gradient: [[  -6.3732   14.3981   10.7566  -11.734  -342.8246]]\n",
      "Weights: [[-4.1645 -1.229  -0.0809  0.1214  0.073 ]]\n",
      "MSE loss: 186.2836\n",
      "Iteration: 182500\n",
      "Gradient: [[  -9.3278   -6.6656   -7.6199  -33.9429 -415.6688]]\n",
      "Weights: [[-4.1648 -1.2288 -0.081   0.1214  0.0731]]\n",
      "MSE loss: 186.2626\n",
      "Iteration: 182600\n",
      "Gradient: [[ 10.3446 -13.2136 -23.7886  51.3079 393.1241]]\n",
      "Weights: [[-4.1655 -1.2286 -0.0811  0.1214  0.0731]]\n",
      "MSE loss: 186.2452\n",
      "Iteration: 182700\n",
      "Gradient: [[-18.8833   5.9749  20.7578  24.9309  22.0946]]\n",
      "Weights: [[-4.1674 -1.2284 -0.0811  0.1214  0.0731]]\n",
      "MSE loss: 186.2434\n",
      "Iteration: 182800\n",
      "Gradient: [[ -10.7374  -10.7613   12.5393  126.6684 -312.814 ]]\n",
      "Weights: [[-4.1663 -1.2281 -0.0812  0.1214  0.0731]]\n",
      "MSE loss: 186.2134\n",
      "Iteration: 182900\n",
      "Gradient: [[ 16.0053 -13.6699  45.7974  41.0576  65.6001]]\n",
      "Weights: [[-4.1657 -1.2279 -0.0813  0.1214  0.0731]]\n",
      "MSE loss: 186.1961\n",
      "Iteration: 183000\n",
      "Gradient: [[   3.1378   -0.3763   34.6213   46.4499 -217.673 ]]\n",
      "Weights: [[-4.1655 -1.2276 -0.0814  0.1214  0.0731]]\n",
      "MSE loss: 186.1759\n",
      "Iteration: 183100\n",
      "Gradient: [[ 15.9591  -9.6812  31.0806 -59.4101  -5.5004]]\n",
      "Weights: [[-4.1643 -1.2275 -0.0815  0.1213  0.0731]]\n",
      "MSE loss: 186.1654\n",
      "Iteration: 183200\n",
      "Gradient: [[ -6.8712  -0.9691 -18.2802 -66.3724 387.1801]]\n",
      "Weights: [[-4.1646 -1.2273 -0.0815  0.1213  0.0731]]\n",
      "MSE loss: 186.1466\n",
      "Iteration: 183300\n",
      "Gradient: [[   4.3149   11.4593  -10.7622 -121.2153   34.0917]]\n",
      "Weights: [[-4.166  -1.2271 -0.0816  0.1213  0.0731]]\n",
      "MSE loss: 186.1312\n",
      "Iteration: 183400\n",
      "Gradient: [[   6.9485   11.1822   39.5255  -95.618  -152.7795]]\n",
      "Weights: [[-4.165  -1.2271 -0.0817  0.1213  0.0731]]\n",
      "MSE loss: 186.115\n",
      "Iteration: 183500\n",
      "Gradient: [[  -4.5772    4.9885   41.1837  -15.7351 -553.5663]]\n",
      "Weights: [[-4.1658 -1.227  -0.0818  0.1213  0.0731]]\n",
      "MSE loss: 186.1011\n",
      "Iteration: 183600\n",
      "Gradient: [[   0.6652    6.3369   38.2805  -38.1104 -223.2827]]\n",
      "Weights: [[-4.1658 -1.2268 -0.0819  0.1213  0.0731]]\n",
      "MSE loss: 186.0808\n",
      "Iteration: 183700\n",
      "Gradient: [[  1.4481 -11.8271  45.8337  53.7573 111.6028]]\n",
      "Weights: [[-4.1661 -1.2267 -0.0819  0.1213  0.0731]]\n",
      "MSE loss: 186.0686\n",
      "Iteration: 183800\n",
      "Gradient: [[  2.5236  -3.2313 -48.7543  65.9709 -11.5106]]\n",
      "Weights: [[-4.1666 -1.2263 -0.082   0.1213  0.0732]]\n",
      "MSE loss: 186.0506\n",
      "Iteration: 183900\n",
      "Gradient: [[-7.225  13.3797 61.5872 20.4229  1.4783]]\n",
      "Weights: [[-4.1653 -1.2261 -0.0821  0.1213  0.0732]]\n",
      "MSE loss: 186.0338\n",
      "Iteration: 184000\n",
      "Gradient: [[ -8.9631 -11.0792  -1.5188 -80.8459 324.0611]]\n",
      "Weights: [[-4.165  -1.2259 -0.0822  0.1213  0.0732]]\n",
      "MSE loss: 186.0125\n",
      "Iteration: 184100\n",
      "Gradient: [[ -3.5021  -0.8075 -12.7309  88.3501  80.7774]]\n",
      "Weights: [[-4.1656 -1.2257 -0.0823  0.1213  0.0732]]\n",
      "MSE loss: 185.9945\n",
      "Iteration: 184200\n",
      "Gradient: [[  -9.285    22.5108   -7.8708  -17.5258 -292.25  ]]\n",
      "Weights: [[-4.1667 -1.2254 -0.0823  0.1213  0.0732]]\n",
      "MSE loss: 185.9829\n",
      "Iteration: 184300\n",
      "Gradient: [[ 7.1355  9.8476 -0.6658  7.5816 52.9846]]\n",
      "Weights: [[-4.1665 -1.2253 -0.0824  0.1212  0.0732]]\n",
      "MSE loss: 185.9654\n",
      "Iteration: 184400\n",
      "Gradient: [[  1.4552 -29.5009  14.3047  54.3981 334.8601]]\n",
      "Weights: [[-4.1661 -1.2251 -0.0826  0.1212  0.0732]]\n",
      "MSE loss: 185.9443\n",
      "Iteration: 184500\n",
      "Gradient: [[ 9.8502  0.0301  2.2391 -0.8012 -6.2926]]\n",
      "Weights: [[-4.1662 -1.225  -0.0826  0.1212  0.0732]]\n",
      "MSE loss: 185.9273\n",
      "Iteration: 184600\n",
      "Gradient: [[  -6.4334    3.5768   17.3277    6.3852 -186.6238]]\n",
      "Weights: [[-4.1663 -1.2248 -0.0827  0.1212  0.0732]]\n",
      "MSE loss: 185.9093\n",
      "Iteration: 184700\n",
      "Gradient: [[  4.1531  -1.4387  24.6074  78.195  153.5337]]\n",
      "Weights: [[-4.166  -1.2248 -0.0829  0.1212  0.0732]]\n",
      "MSE loss: 185.8909\n",
      "Iteration: 184800\n",
      "Gradient: [[   1.7871  -15.9417   11.5056  -38.1294 -182.0328]]\n",
      "Weights: [[-4.1658 -1.2246 -0.0829  0.1212  0.0732]]\n",
      "MSE loss: 185.8763\n",
      "Iteration: 184900\n",
      "Gradient: [[ -4.3043 -17.5622  10.054  -77.9253 -29.3794]]\n",
      "Weights: [[-4.1661 -1.2245 -0.083   0.1212  0.0732]]\n",
      "MSE loss: 185.859\n",
      "Iteration: 185000\n",
      "Gradient: [[ -0.792    2.6319  36.7849 -72.6727   8.3199]]\n",
      "Weights: [[-4.1671 -1.2242 -0.0831  0.1212  0.0732]]\n",
      "MSE loss: 185.8357\n",
      "Iteration: 185100\n",
      "Gradient: [[ -8.4876  -9.0869  39.6531  64.0861 -23.7287]]\n",
      "Weights: [[-4.1661 -1.224  -0.0832  0.1212  0.0733]]\n",
      "MSE loss: 185.8186\n",
      "Iteration: 185200\n",
      "Gradient: [[-4.87900e+00  1.37149e+01  1.44000e-02  7.34952e+01 -3.40710e+02]]\n",
      "Weights: [[-4.1664 -1.2237 -0.0833  0.1212  0.0733]]\n",
      "MSE loss: 185.7969\n",
      "Iteration: 185300\n",
      "Gradient: [[  -0.1838  -12.6967    5.2688   17.8633 -136.7368]]\n",
      "Weights: [[-4.1672 -1.2235 -0.0834  0.1211  0.0733]]\n",
      "MSE loss: 185.7805\n",
      "Iteration: 185400\n",
      "Gradient: [[  8.0052   4.377  -34.3736  23.7492 -25.8942]]\n",
      "Weights: [[-4.166  -1.2236 -0.0835  0.1211  0.0733]]\n",
      "MSE loss: 185.7669\n",
      "Iteration: 185500\n",
      "Gradient: [[  5.1402 -17.1938 -10.465   -9.9892 -23.9052]]\n",
      "Weights: [[-4.1664 -1.2235 -0.0836  0.1211  0.0733]]\n",
      "MSE loss: 185.7484\n",
      "Iteration: 185600\n",
      "Gradient: [[  0.9085 -13.1207 -13.1736  54.3556  47.1798]]\n",
      "Weights: [[-4.1688 -1.2231 -0.0837  0.1211  0.0733]]\n",
      "MSE loss: 185.7372\n",
      "Iteration: 185700\n",
      "Gradient: [[  8.8645  -7.0263  55.3633  11.4614 126.9548]]\n",
      "Weights: [[-4.1699 -1.2229 -0.0837  0.1211  0.0733]]\n",
      "MSE loss: 185.7328\n",
      "Iteration: 185800\n",
      "Gradient: [[ -8.7965  -3.0282  54.6715  61.4929 -97.0577]]\n",
      "Weights: [[-4.1687 -1.2224 -0.0838  0.1211  0.0733]]\n",
      "MSE loss: 185.7033\n",
      "Iteration: 185900\n",
      "Gradient: [[ -7.5373  19.6845  28.8938  15.0924 -33.9768]]\n",
      "Weights: [[-4.169  -1.2223 -0.0838  0.1211  0.0733]]\n",
      "MSE loss: 185.6896\n",
      "Iteration: 186000\n",
      "Gradient: [[   7.689    -6.1754  -22.5557  -90.5644 -509.6746]]\n",
      "Weights: [[-4.1686 -1.2224 -0.0839  0.1211  0.0733]]\n",
      "MSE loss: 185.6772\n",
      "Iteration: 186100\n",
      "Gradient: [[ -12.4082  -26.8625  -43.4451    4.8868 -139.8752]]\n",
      "Weights: [[-4.1675 -1.2223 -0.084   0.1211  0.0733]]\n",
      "MSE loss: 185.6592\n",
      "Iteration: 186200\n",
      "Gradient: [[-11.0804   9.6093  -2.3113  50.3345   8.6387]]\n",
      "Weights: [[-4.1672 -1.2222 -0.0841  0.1211  0.0733]]\n",
      "MSE loss: 185.6438\n",
      "Iteration: 186300\n",
      "Gradient: [[  1.0586  -0.8445  45.5336 102.1894   2.6997]]\n",
      "Weights: [[-4.1683 -1.222  -0.0842  0.1211  0.0733]]\n",
      "MSE loss: 185.6246\n",
      "Iteration: 186400\n",
      "Gradient: [[ -18.4263   -0.4703  -27.7942 -153.0848   74.3618]]\n",
      "Weights: [[-4.1684 -1.2217 -0.0844  0.1211  0.0734]]\n",
      "MSE loss: 185.6006\n",
      "Iteration: 186500\n",
      "Gradient: [[  -1.9687  -15.0522   81.0835   74.86   -540.9431]]\n",
      "Weights: [[-4.1687 -1.2213 -0.0845  0.1211  0.0734]]\n",
      "MSE loss: 185.5796\n",
      "Iteration: 186600\n",
      "Gradient: [[   9.121    -1.4476   -6.0776 -139.5858  220.3482]]\n",
      "Weights: [[-4.1687 -1.2211 -0.0845  0.1211  0.0734]]\n",
      "MSE loss: 185.56\n",
      "Iteration: 186700\n",
      "Gradient: [[  -1.2788   -0.5173   13.7977   10.5542 -123.7452]]\n",
      "Weights: [[-4.1679 -1.221  -0.0847  0.1211  0.0734]]\n",
      "MSE loss: 185.5406\n",
      "Iteration: 186800\n",
      "Gradient: [[  9.0527   1.3488 -51.2394 159.3076 -97.3161]]\n",
      "Weights: [[-4.1681 -1.2209 -0.0847  0.121   0.0734]]\n",
      "MSE loss: 185.5275\n",
      "Iteration: 186900\n",
      "Gradient: [[ 14.082    7.6768  -5.9605 -37.9884 -26.9886]]\n",
      "Weights: [[-4.1687 -1.2207 -0.0848  0.1211  0.0734]]\n",
      "MSE loss: 185.5143\n",
      "Iteration: 187000\n",
      "Gradient: [[ -4.8582   8.2544  11.6654 -21.5908  61.224 ]]\n",
      "Weights: [[-4.1703 -1.2204 -0.0849  0.1211  0.0734]]\n",
      "MSE loss: 185.5036\n",
      "Iteration: 187100\n",
      "Gradient: [[ -2.9197 -12.1309  34.1027  76.6427 -11.2646]]\n",
      "Weights: [[-4.1711 -1.2203 -0.0849  0.1211  0.0734]]\n",
      "MSE loss: 185.497\n",
      "Iteration: 187200\n",
      "Gradient: [[   3.9774   -8.3865   37.426    73.7379 -190.2555]]\n",
      "Weights: [[-4.1698 -1.2201 -0.085   0.1211  0.0734]]\n",
      "MSE loss: 185.4715\n",
      "Iteration: 187300\n",
      "Gradient: [[  -4.7675  -27.846    57.0609  180.5406 -104.6114]]\n",
      "Weights: [[-4.1687 -1.2199 -0.0851  0.121   0.0734]]\n",
      "MSE loss: 185.4543\n",
      "Iteration: 187400\n",
      "Gradient: [[  11.5017   21.5327   15.4039 -132.1042  140.7644]]\n",
      "Weights: [[-4.1681 -1.2198 -0.0852  0.121   0.0734]]\n",
      "MSE loss: 185.4395\n",
      "Iteration: 187500\n",
      "Gradient: [[ 13.6707   7.0426  27.6154 124.6772 -66.6935]]\n",
      "Weights: [[-4.1684 -1.2199 -0.0853  0.121   0.0734]]\n",
      "MSE loss: 185.4265\n",
      "Iteration: 187600\n",
      "Gradient: [[  13.0358   -3.305    29.047   152.8399 -105.0377]]\n",
      "Weights: [[-4.1688 -1.2196 -0.0854  0.121   0.0734]]\n",
      "MSE loss: 185.4098\n",
      "Iteration: 187700\n",
      "Gradient: [[ -7.3186  -6.3535  16.888   16.8245 133.1545]]\n",
      "Weights: [[-4.168  -1.2196 -0.0855  0.121   0.0734]]\n",
      "MSE loss: 185.3927\n",
      "Iteration: 187800\n",
      "Gradient: [[ -8.8462  -6.4547 -18.3447 -23.6138 -67.1413]]\n",
      "Weights: [[-4.169  -1.2192 -0.0856  0.121   0.0734]]\n",
      "MSE loss: 185.3669\n",
      "Iteration: 187900\n",
      "Gradient: [[  0.6641  -9.7359  14.1095  11.7901 143.9018]]\n",
      "Weights: [[-4.1693 -1.2191 -0.0857  0.121   0.0735]]\n",
      "MSE loss: 185.3512\n",
      "Iteration: 188000\n",
      "Gradient: [[   6.7041   -4.8099  -23.223    15.4384 -223.8335]]\n",
      "Weights: [[-4.1677 -1.2189 -0.0858  0.121   0.0735]]\n",
      "MSE loss: 185.3415\n",
      "Iteration: 188100\n",
      "Gradient: [[-2.0595  1.3439 39.3514 64.219  -2.8353]]\n",
      "Weights: [[-4.168  -1.2188 -0.0858  0.121   0.0735]]\n",
      "MSE loss: 185.3241\n",
      "Iteration: 188200\n",
      "Gradient: [[ -8.2351 -18.0062   8.6667  24.8131 149.5684]]\n",
      "Weights: [[-4.1676 -1.2188 -0.086   0.121   0.0735]]\n",
      "MSE loss: 185.307\n",
      "Iteration: 188300\n",
      "Gradient: [[ -3.063   -9.4827 -26.8753  94.2207  27.9175]]\n",
      "Weights: [[-4.1673 -1.2187 -0.0861  0.121   0.0735]]\n",
      "MSE loss: 185.2929\n",
      "Iteration: 188400\n",
      "Gradient: [[  5.1983 -16.6086 -20.7713 -70.9298  80.9811]]\n",
      "Weights: [[-4.1682 -1.2186 -0.0862  0.121   0.0735]]\n",
      "MSE loss: 185.2705\n",
      "Iteration: 188500\n",
      "Gradient: [[   1.4945  -16.5861    6.9365   69.6709 -175.2531]]\n",
      "Weights: [[-4.1681 -1.2181 -0.0863  0.121   0.0735]]\n",
      "MSE loss: 185.2499\n",
      "Iteration: 188600\n",
      "Gradient: [[  18.5216  -21.045   -14.4823  -59.2767 -167.091 ]]\n",
      "Weights: [[-4.169  -1.2181 -0.0864  0.121   0.0735]]\n",
      "MSE loss: 185.2375\n",
      "Iteration: 188700\n",
      "Gradient: [[   0.8727   -6.7918   37.4172  -24.1308 -268.8247]]\n",
      "Weights: [[-4.1686 -1.218  -0.0864  0.1209  0.0735]]\n",
      "MSE loss: 185.2263\n",
      "Iteration: 188800\n",
      "Gradient: [[  -9.5511  -13.413    27.5551   48.2477 -253.4955]]\n",
      "Weights: [[-4.1692 -1.2178 -0.0865  0.1209  0.0735]]\n",
      "MSE loss: 185.2036\n",
      "Iteration: 188900\n",
      "Gradient: [[-10.4828  -4.3747 -25.2322  46.191  -25.6333]]\n",
      "Weights: [[-4.1691 -1.2176 -0.0866  0.1209  0.0735]]\n",
      "MSE loss: 185.1896\n",
      "Iteration: 189000\n",
      "Gradient: [[ -2.1682   7.0978  25.4959 127.4477  66.3727]]\n",
      "Weights: [[-4.1701 -1.2173 -0.0867  0.1209  0.0735]]\n",
      "MSE loss: 185.1753\n",
      "Iteration: 189100\n",
      "Gradient: [[-4.548000e-01 -1.716730e+01 -1.657990e+01  5.777600e+01 -5.116989e+02]]\n",
      "Weights: [[-4.1695 -1.217  -0.0868  0.1209  0.0735]]\n",
      "MSE loss: 185.1513\n",
      "Iteration: 189200\n",
      "Gradient: [[  1.393  -17.6592  66.2943 -75.4586 -42.7037]]\n",
      "Weights: [[-4.1703 -1.2168 -0.0869  0.1209  0.0735]]\n",
      "MSE loss: 185.1377\n",
      "Iteration: 189300\n",
      "Gradient: [[  -3.7295   12.4593   -3.4504   77.7507 -372.0092]]\n",
      "Weights: [[-4.1693 -1.2166 -0.0869  0.1209  0.0735]]\n",
      "MSE loss: 185.1264\n",
      "Iteration: 189400\n",
      "Gradient: [[ -12.948     6.7188   71.2745   -8.5948 -306.7047]]\n",
      "Weights: [[-4.1696 -1.2165 -0.087   0.1209  0.0736]]\n",
      "MSE loss: 185.1099\n",
      "Iteration: 189500\n",
      "Gradient: [[ -2.099   -6.6413   4.0223 108.63   224.8532]]\n",
      "Weights: [[-4.1695 -1.2164 -0.0871  0.1209  0.0736]]\n",
      "MSE loss: 185.0935\n",
      "Iteration: 189600\n",
      "Gradient: [[  7.6798 -13.29     4.2871 -91.5405 -45.3778]]\n",
      "Weights: [[-4.1694 -1.2164 -0.0872  0.1209  0.0736]]\n",
      "MSE loss: 185.0764\n",
      "Iteration: 189700\n",
      "Gradient: [[  3.4876  13.2883 -28.1423  28.9523 239.6057]]\n",
      "Weights: [[-4.1694 -1.2162 -0.0873  0.1209  0.0736]]\n",
      "MSE loss: 185.0585\n",
      "Iteration: 189800\n",
      "Gradient: [[  14.539   -10.668    33.0978 -194.0258  -47.5157]]\n",
      "Weights: [[-4.1699 -1.2162 -0.0874  0.1209  0.0736]]\n",
      "MSE loss: 185.0473\n",
      "Iteration: 189900\n",
      "Gradient: [[  10.1401    7.9773    1.7441   -6.6018 -252.2633]]\n",
      "Weights: [[-4.1702 -1.2159 -0.0875  0.1209  0.0736]]\n",
      "MSE loss: 185.0302\n",
      "Iteration: 190000\n",
      "Gradient: [[ -14.5782    5.4239  -14.242   -24.0569 -231.4843]]\n",
      "Weights: [[-4.1704 -1.2156 -0.0876  0.1209  0.0736]]\n",
      "MSE loss: 185.0099\n",
      "Iteration: 190100\n",
      "Gradient: [[  4.2356  14.4801 -60.8625 -81.2504  62.4   ]]\n",
      "Weights: [[-4.1704 -1.2156 -0.0877  0.1209  0.0736]]\n",
      "MSE loss: 184.9934\n",
      "Iteration: 190200\n",
      "Gradient: [[   2.1361    9.8299   41.2574    1.2916 -243.7611]]\n",
      "Weights: [[-4.1694 -1.2154 -0.0878  0.1209  0.0736]]\n",
      "MSE loss: 184.98\n",
      "Iteration: 190300\n",
      "Gradient: [[  -2.5673  -11.2334    4.0389 -107.9976  112.8486]]\n",
      "Weights: [[-4.1691 -1.2152 -0.0879  0.1208  0.0736]]\n",
      "MSE loss: 184.9602\n",
      "Iteration: 190400\n",
      "Gradient: [[  -5.941   -15.1989   19.0808 -166.2734 -322.4202]]\n",
      "Weights: [[-4.1689 -1.215  -0.088   0.1208  0.0736]]\n",
      "MSE loss: 184.9409\n",
      "Iteration: 190500\n",
      "Gradient: [[   3.8502    0.7451  -25.8468 -116.2515  -74.394 ]]\n",
      "Weights: [[-4.1693 -1.2148 -0.088   0.1208  0.0736]]\n",
      "MSE loss: 184.9282\n",
      "Iteration: 190600\n",
      "Gradient: [[   0.8643  -15.7519   34.6599 -125.5164  -79.5982]]\n",
      "Weights: [[-4.1686 -1.2147 -0.0881  0.1208  0.0736]]\n",
      "MSE loss: 184.9149\n",
      "Iteration: 190700\n",
      "Gradient: [[ -5.3733   2.5465  34.9752 158.9816 -41.6383]]\n",
      "Weights: [[-4.1692 -1.2146 -0.0882  0.1208  0.0736]]\n",
      "MSE loss: 184.8893\n",
      "Iteration: 190800\n",
      "Gradient: [[  5.7221  -6.2989 -69.584   19.7423 208.1458]]\n",
      "Weights: [[-4.1702 -1.2144 -0.0883  0.1208  0.0737]]\n",
      "MSE loss: 184.8715\n",
      "Iteration: 190900\n",
      "Gradient: [[  5.113    1.123  -36.6471  74.5095  -8.3784]]\n",
      "Weights: [[-4.1707 -1.2144 -0.0884  0.1208  0.0737]]\n",
      "MSE loss: 184.858\n",
      "Iteration: 191000\n",
      "Gradient: [[ -3.3638   3.3333  36.7834  79.7301 140.1391]]\n",
      "Weights: [[-4.1704 -1.2141 -0.0885  0.1208  0.0737]]\n",
      "MSE loss: 184.8427\n",
      "Iteration: 191100\n",
      "Gradient: [[   5.6696  -10.6107  -25.3428   27.6812 -142.6905]]\n",
      "Weights: [[-4.1711 -1.2137 -0.0886  0.1208  0.0737]]\n",
      "MSE loss: 184.8198\n",
      "Iteration: 191200\n",
      "Gradient: [[-10.5311   3.809  -46.4147  26.1825 144.9824]]\n",
      "Weights: [[-4.1715 -1.2134 -0.0887  0.1208  0.0737]]\n",
      "MSE loss: 184.8\n",
      "Iteration: 191300\n",
      "Gradient: [[  14.6068   19.561    18.6813  -19.8112 -432.0199]]\n",
      "Weights: [[-4.1716 -1.2133 -0.0888  0.1208  0.0737]]\n",
      "MSE loss: 184.7854\n",
      "Iteration: 191400\n",
      "Gradient: [[   4.2538   -2.698    24.8015  -29.2381 -414.553 ]]\n",
      "Weights: [[-4.1711 -1.2132 -0.0889  0.1208  0.0737]]\n",
      "MSE loss: 184.7681\n",
      "Iteration: 191500\n",
      "Gradient: [[  -9.2186   -5.4706   69.1947   29.1547 -161.8419]]\n",
      "Weights: [[-4.1721 -1.213  -0.089   0.1208  0.0737]]\n",
      "MSE loss: 184.7526\n",
      "Iteration: 191600\n",
      "Gradient: [[ 10.3148  10.1805  14.3733 185.2641 449.0079]]\n",
      "Weights: [[-4.1726 -1.2128 -0.0891  0.1208  0.0737]]\n",
      "MSE loss: 184.7393\n",
      "Iteration: 191700\n",
      "Gradient: [[ -9.563   -1.8476 -16.3875  10.7888 -33.3979]]\n",
      "Weights: [[-4.1717 -1.2126 -0.0891  0.1208  0.0737]]\n",
      "MSE loss: 184.7173\n",
      "Iteration: 191800\n",
      "Gradient: [[ -6.4675  -9.6055  12.3005  45.5989 168.909 ]]\n",
      "Weights: [[-4.1703 -1.2124 -0.0893  0.1208  0.0737]]\n",
      "MSE loss: 184.6976\n",
      "Iteration: 191900\n",
      "Gradient: [[   3.9884   -1.7709   29.9033 -127.4371  -25.5554]]\n",
      "Weights: [[-4.1695 -1.2125 -0.0894  0.1207  0.0737]]\n",
      "MSE loss: 184.6787\n",
      "Iteration: 192000\n",
      "Gradient: [[   0.8097   -6.0128   41.1946   58.0458 -389.5798]]\n",
      "Weights: [[-4.1694 -1.2125 -0.0896  0.1207  0.0737]]\n",
      "MSE loss: 184.6567\n",
      "Iteration: 192100\n",
      "Gradient: [[   2.0371   10.9865   36.4934  -45.2602 -438.9984]]\n",
      "Weights: [[-4.1688 -1.2123 -0.0897  0.1207  0.0738]]\n",
      "MSE loss: 184.6335\n",
      "Iteration: 192200\n",
      "Gradient: [[   6.707   -16.7433  -27.5904 -172.6082 -294.8221]]\n",
      "Weights: [[-4.1694 -1.2123 -0.0899  0.1207  0.0738]]\n",
      "MSE loss: 184.6153\n",
      "Iteration: 192300\n",
      "Gradient: [[  5.8731 -21.1985 -58.2379  55.6055 -97.4986]]\n",
      "Weights: [[-4.1691 -1.2121 -0.0899  0.1207  0.0738]]\n",
      "MSE loss: 184.595\n",
      "Iteration: 192400\n",
      "Gradient: [[   4.3637   -6.1449   19.5389  -11.8979 -284.5419]]\n",
      "Weights: [[-4.1691 -1.2119 -0.09    0.1207  0.0738]]\n",
      "MSE loss: 184.5805\n",
      "Iteration: 192500\n",
      "Gradient: [[ 17.6242  -9.2103  42.1222 -59.1776 -89.6305]]\n",
      "Weights: [[-4.1683 -1.2119 -0.0901  0.1207  0.0738]]\n",
      "MSE loss: 184.566\n",
      "Iteration: 192600\n",
      "Gradient: [[   0.2631   -1.4718   55.4152 -113.0151  177.2109]]\n",
      "Weights: [[-4.1701 -1.2118 -0.0902  0.1206  0.0738]]\n",
      "MSE loss: 184.5473\n",
      "Iteration: 192700\n",
      "Gradient: [[ -3.6973 -15.4796  -0.7066 -41.6112 203.5104]]\n",
      "Weights: [[-4.1711 -1.2113 -0.0903  0.1206  0.0738]]\n",
      "MSE loss: 184.5248\n",
      "Iteration: 192800\n",
      "Gradient: [[  -6.8593   -3.2935    3.8795  -99.4211 -311.1665]]\n",
      "Weights: [[-4.1724 -1.2109 -0.0904  0.1207  0.0738]]\n",
      "MSE loss: 184.5078\n",
      "Iteration: 192900\n",
      "Gradient: [[  -9.256     7.7831   -6.0581   13.796  -402.9208]]\n",
      "Weights: [[-4.1723 -1.2108 -0.0904  0.1207  0.0738]]\n",
      "MSE loss: 184.4985\n",
      "Iteration: 193000\n",
      "Gradient: [[ 16.6938 -10.7515  17.6085 -74.5738 287.6163]]\n",
      "Weights: [[-4.1722 -1.2107 -0.0904  0.1207  0.0738]]\n",
      "MSE loss: 184.4898\n",
      "Iteration: 193100\n",
      "Gradient: [[ -11.4879  -14.1035    7.4058  102.329  -533.3473]]\n",
      "Weights: [[-4.1723 -1.2107 -0.0905  0.1206  0.0738]]\n",
      "MSE loss: 184.4817\n",
      "Iteration: 193200\n",
      "Gradient: [[   2.5281   -6.0675   66.3082  134.122  -749.1055]]\n",
      "Weights: [[-4.1724 -1.2104 -0.0906  0.1207  0.0738]]\n",
      "MSE loss: 184.4596\n",
      "Iteration: 193300\n",
      "Gradient: [[   2.6652    5.9454  -10.8919    8.4828 -329.567 ]]\n",
      "Weights: [[-4.1714 -1.2102 -0.0907  0.1206  0.0739]]\n",
      "MSE loss: 184.4366\n",
      "Iteration: 193400\n",
      "Gradient: [[  0.7179 -15.5024 -14.08   121.7762   0.7649]]\n",
      "Weights: [[-4.1723 -1.2101 -0.0908  0.1206  0.0739]]\n",
      "MSE loss: 184.425\n",
      "Iteration: 193500\n",
      "Gradient: [[ -11.0077   -4.4141   32.3675  -39.5885 -475.1383]]\n",
      "Weights: [[-4.1725 -1.2099 -0.0908  0.1206  0.0739]]\n",
      "MSE loss: 184.4117\n",
      "Iteration: 193600\n",
      "Gradient: [[   1.9767   -0.3744   76.0999  -11.3247 -280.3174]]\n",
      "Weights: [[-4.1721 -1.2095 -0.091   0.1206  0.0739]]\n",
      "MSE loss: 184.3831\n",
      "Iteration: 193700\n",
      "Gradient: [[  17.5314  -11.5258  -66.2937  122.6306 -305.2552]]\n",
      "Weights: [[-4.1729 -1.2094 -0.091   0.1206  0.0739]]\n",
      "MSE loss: 184.3695\n",
      "Iteration: 193800\n",
      "Gradient: [[ 17.0758   0.6713 -19.9359 -85.1828  98.8415]]\n",
      "Weights: [[-4.1723 -1.2091 -0.0912  0.1206  0.0739]]\n",
      "MSE loss: 184.3411\n",
      "Iteration: 193900\n",
      "Gradient: [[  1.5992  16.8828  -5.4669  -1.7096 -24.234 ]]\n",
      "Weights: [[-4.1718 -1.2089 -0.0912  0.1206  0.0739]]\n",
      "MSE loss: 184.3255\n",
      "Iteration: 194000\n",
      "Gradient: [[  -5.6914   -7.1566  -43.0214 -108.6313  -52.0744]]\n",
      "Weights: [[-4.1697 -1.2089 -0.0914  0.1206  0.0739]]\n",
      "MSE loss: 184.3169\n",
      "Iteration: 194100\n",
      "Gradient: [[   2.2957   18.7402  -26.0446  -49.726  -132.0616]]\n",
      "Weights: [[-4.1711 -1.209  -0.0914  0.1206  0.0739]]\n",
      "MSE loss: 184.2972\n",
      "Iteration: 194200\n",
      "Gradient: [[ -1.7086   3.0859  27.5141 -72.6095 -97.5397]]\n",
      "Weights: [[-4.1702 -1.2089 -0.0915  0.1206  0.0739]]\n",
      "MSE loss: 184.2837\n",
      "Iteration: 194300\n",
      "Gradient: [[   4.3096   19.4341   61.8543   -9.9138 -493.6431]]\n",
      "Weights: [[-4.1706 -1.2089 -0.0917  0.1206  0.0739]]\n",
      "MSE loss: 184.2637\n",
      "Iteration: 194400\n",
      "Gradient: [[  11.9563    1.7513   28.9988 -144.2014  202.8954]]\n",
      "Weights: [[-4.1712 -1.2087 -0.0918  0.1206  0.0739]]\n",
      "MSE loss: 184.2424\n",
      "Iteration: 194500\n",
      "Gradient: [[ 6.7395 -1.7255 59.3328 91.9296 18.6783]]\n",
      "Weights: [[-4.1717 -1.2087 -0.0918  0.1206  0.0739]]\n",
      "MSE loss: 184.2316\n",
      "Iteration: 194600\n",
      "Gradient: [[   5.863     5.2588  -13.7789  -61.7559 -228.6651]]\n",
      "Weights: [[-4.1714 -1.2085 -0.0919  0.1206  0.074 ]]\n",
      "MSE loss: 184.2163\n",
      "Iteration: 194700\n",
      "Gradient: [[ -9.36    -8.5402  64.9749 -73.8443  48.4578]]\n",
      "Weights: [[-4.1714 -1.2085 -0.092   0.1206  0.074 ]]\n",
      "MSE loss: 184.2024\n",
      "Iteration: 194800\n",
      "Gradient: [[  6.83     1.1834  20.7148 -31.0237 -23.0329]]\n",
      "Weights: [[-4.1713 -1.2083 -0.0921  0.1206  0.074 ]]\n",
      "MSE loss: 184.1853\n",
      "Iteration: 194900\n",
      "Gradient: [[ -2.6793 -19.6808   9.682  -18.144  176.5477]]\n",
      "Weights: [[-4.1704 -1.208  -0.0922  0.1206  0.074 ]]\n",
      "MSE loss: 184.1668\n",
      "Iteration: 195000\n",
      "Gradient: [[ -15.5642   10.3903   24.8968 -108.8581 -346.1726]]\n",
      "Weights: [[-4.1717 -1.2078 -0.0923  0.1206  0.074 ]]\n",
      "MSE loss: 184.1447\n",
      "Iteration: 195100\n",
      "Gradient: [[   1.7309   17.4321   82.3859  116.0405 -285.0167]]\n",
      "Weights: [[-4.1707 -1.2076 -0.0924  0.1206  0.074 ]]\n",
      "MSE loss: 184.1286\n",
      "Iteration: 195200\n",
      "Gradient: [[   1.9449   -5.2357    0.3374  -79.4929 -120.2538]]\n",
      "Weights: [[-4.1706 -1.2075 -0.0926  0.1205  0.074 ]]\n",
      "MSE loss: 184.108\n",
      "Iteration: 195300\n",
      "Gradient: [[   4.6901   19.7618   35.4494  -14.2027 -131.5681]]\n",
      "Weights: [[-4.171  -1.2073 -0.0927  0.1205  0.074 ]]\n",
      "MSE loss: 184.0895\n",
      "Iteration: 195400\n",
      "Gradient: [[ 10.7889   5.6938  14.7411  62.0296 256.7377]]\n",
      "Weights: [[-4.1721 -1.2071 -0.0928  0.1205  0.074 ]]\n",
      "MSE loss: 184.0665\n",
      "Iteration: 195500\n",
      "Gradient: [[-8.860000e-02 -3.468000e-01  1.466500e+00 -7.642800e+01 -3.365557e+02]]\n",
      "Weights: [[-4.1717 -1.207  -0.0929  0.1205  0.074 ]]\n",
      "MSE loss: 184.0529\n",
      "Iteration: 195600\n",
      "Gradient: [[  -9.7847  -11.0378  -16.954    26.1918 -138.7829]]\n",
      "Weights: [[-4.1719 -1.2071 -0.0929  0.1205  0.074 ]]\n",
      "MSE loss: 184.0426\n",
      "Iteration: 195700\n",
      "Gradient: [[  -0.8526    8.6247  -22.828   155.3548 -175.5523]]\n",
      "Weights: [[-4.1716 -1.2069 -0.093   0.1205  0.074 ]]\n",
      "MSE loss: 184.0257\n",
      "Iteration: 195800\n",
      "Gradient: [[   2.9111   25.7518   24.2041  -54.5248 -221.3859]]\n",
      "Weights: [[-4.1717 -1.2069 -0.0931  0.1205  0.074 ]]\n",
      "MSE loss: 184.0081\n",
      "Iteration: 195900\n",
      "Gradient: [[   9.4039    5.3937   27.1587 -139.7319 -331.7048]]\n",
      "Weights: [[-4.1727 -1.2067 -0.0932  0.1205  0.0741]]\n",
      "MSE loss: 183.9987\n",
      "Iteration: 196000\n",
      "Gradient: [[  1.9312  -1.168    4.5968 134.4412 257.8185]]\n",
      "Weights: [[-4.1718 -1.2066 -0.0932  0.1205  0.0741]]\n",
      "MSE loss: 183.9826\n",
      "Iteration: 196100\n",
      "Gradient: [[ -5.9912  32.1013 -36.8362  43.2456 -48.5955]]\n",
      "Weights: [[-4.1711 -1.2065 -0.0933  0.1205  0.0741]]\n",
      "MSE loss: 183.9734\n",
      "Iteration: 196200\n",
      "Gradient: [[  7.6436  10.9539 -24.2536  -3.4321  79.4162]]\n",
      "Weights: [[-4.1702 -1.2062 -0.0934  0.1205  0.0741]]\n",
      "MSE loss: 183.9597\n",
      "Iteration: 196300\n",
      "Gradient: [[ -5.868   -2.4609 -34.3626  87.205   54.308 ]]\n",
      "Weights: [[-4.1703 -1.2059 -0.0934  0.1205  0.0741]]\n",
      "MSE loss: 183.9425\n",
      "Iteration: 196400\n",
      "Gradient: [[  1.8135   1.0361 -79.357  -76.3498 -96.7037]]\n",
      "Weights: [[-4.171  -1.206  -0.0935  0.1205  0.0741]]\n",
      "MSE loss: 183.9259\n",
      "Iteration: 196500\n",
      "Gradient: [[  -4.5999    2.3724    4.6488  -45.4582 -305.2383]]\n",
      "Weights: [[-4.1707 -1.2059 -0.0936  0.1204  0.0741]]\n",
      "MSE loss: 183.9091\n",
      "Iteration: 196600\n",
      "Gradient: [[   0.3853   -4.2218    9.0849  -29.6512 -169.0196]]\n",
      "Weights: [[-4.1704 -1.2057 -0.0937  0.1204  0.0741]]\n",
      "MSE loss: 183.8986\n",
      "Iteration: 196700\n",
      "Gradient: [[  5.8426 -12.8284  25.2827  12.4278 -97.2313]]\n",
      "Weights: [[-4.17   -1.2054 -0.0938  0.1204  0.0741]]\n",
      "MSE loss: 183.8814\n",
      "Iteration: 196800\n",
      "Gradient: [[   4.7001    0.5026   17.3068   40.8952 -450.1719]]\n",
      "Weights: [[-4.1715 -1.2054 -0.0938  0.1204  0.0741]]\n",
      "MSE loss: 183.8616\n",
      "Iteration: 196900\n",
      "Gradient: [[  -9.2122   -5.2881  -10.8481  -54.3671 -295.7745]]\n",
      "Weights: [[-4.1715 -1.2054 -0.0939  0.1204  0.0741]]\n",
      "MSE loss: 183.8483\n",
      "Iteration: 197000\n",
      "Gradient: [[ -8.2245 -15.6367  89.0421 112.8397 251.6418]]\n",
      "Weights: [[-4.1717 -1.2053 -0.094   0.1204  0.0741]]\n",
      "MSE loss: 183.8323\n",
      "Iteration: 197100\n",
      "Gradient: [[   5.6768    6.1107    8.1341  -63.4414 -159.9094]]\n",
      "Weights: [[-4.172  -1.205  -0.094   0.1204  0.0741]]\n",
      "MSE loss: 183.8169\n",
      "Iteration: 197200\n",
      "Gradient: [[ -5.2697  16.3343  -0.4283 -13.8707 -43.911 ]]\n",
      "Weights: [[-4.172  -1.2049 -0.0941  0.1204  0.0741]]\n",
      "MSE loss: 183.8037\n",
      "Iteration: 197300\n",
      "Gradient: [[ -5.5693 -11.9287 -12.4559 -97.2139 364.288 ]]\n",
      "Weights: [[-4.1722 -1.2048 -0.0942  0.1204  0.0742]]\n",
      "MSE loss: 183.7824\n",
      "Iteration: 197400\n",
      "Gradient: [[   6.3027   12.8588    0.8369  -54.9529 -218.2302]]\n",
      "Weights: [[-4.1729 -1.2047 -0.0943  0.1204  0.0742]]\n",
      "MSE loss: 183.7708\n",
      "Iteration: 197500\n",
      "Gradient: [[-2.6463 -2.6832 35.3464 16.6417 40.2741]]\n",
      "Weights: [[-4.1726 -1.2045 -0.0944  0.1204  0.0742]]\n",
      "MSE loss: 183.751\n",
      "Iteration: 197600\n",
      "Gradient: [[ -9.3362   8.1837  23.9107  15.0855 103.9328]]\n",
      "Weights: [[-4.1729 -1.2041 -0.0945  0.1204  0.0742]]\n",
      "MSE loss: 183.7311\n",
      "Iteration: 197700\n",
      "Gradient: [[-10.1814   5.4529 -25.6364 -45.826   76.9755]]\n",
      "Weights: [[-4.1716 -1.204  -0.0946  0.1204  0.0742]]\n",
      "MSE loss: 183.7157\n",
      "Iteration: 197800\n",
      "Gradient: [[   7.9357   -1.1288    6.4183 -106.4449 -396.1253]]\n",
      "Weights: [[-4.1704 -1.2043 -0.0947  0.1203  0.0742]]\n",
      "MSE loss: 183.7081\n",
      "Iteration: 197900\n",
      "Gradient: [[ -3.1009  14.8152 -19.9612  27.4297 162.6832]]\n",
      "Weights: [[-4.1707 -1.2041 -0.0948  0.1203  0.0742]]\n",
      "MSE loss: 183.6895\n",
      "Iteration: 198000\n",
      "Gradient: [[  2.1909 -10.5346 -57.0597 -72.4064  35.8977]]\n",
      "Weights: [[-4.1713 -1.2039 -0.0949  0.1203  0.0742]]\n",
      "MSE loss: 183.6719\n",
      "Iteration: 198100\n",
      "Gradient: [[-4.692700e+00  1.174000e-01 -2.404260e+01  1.569227e+02 -1.802524e+02]]\n",
      "Weights: [[-4.1717 -1.2039 -0.0949  0.1203  0.0742]]\n",
      "MSE loss: 183.6585\n",
      "Iteration: 198200\n",
      "Gradient: [[  8.479   -8.2475  19.9174 -28.809  -64.04  ]]\n",
      "Weights: [[-4.1716 -1.2037 -0.095   0.1203  0.0742]]\n",
      "MSE loss: 183.6441\n",
      "Iteration: 198300\n",
      "Gradient: [[ -6.8754 -15.9768  40.0179 -31.769  131.9852]]\n",
      "Weights: [[-4.1721 -1.2036 -0.0951  0.1203  0.0742]]\n",
      "MSE loss: 183.6245\n",
      "Iteration: 198400\n",
      "Gradient: [[  14.3094  -27.0761  -28.4667  -30.5366 -212.9461]]\n",
      "Weights: [[-4.1719 -1.2035 -0.0952  0.1203  0.0742]]\n",
      "MSE loss: 183.6078\n",
      "Iteration: 198500\n",
      "Gradient: [[ 13.868   -0.6605 -14.4183  35.513  433.0734]]\n",
      "Weights: [[-4.1716 -1.2036 -0.0953  0.1203  0.0742]]\n",
      "MSE loss: 183.5976\n",
      "Iteration: 198600\n",
      "Gradient: [[ 2.360000e-02 -1.518110e+01  3.134000e-01  1.846852e+02 -9.186720e+01]]\n",
      "Weights: [[-4.171  -1.2036 -0.0953  0.1203  0.0743]]\n",
      "MSE loss: 183.5853\n",
      "Iteration: 198700\n",
      "Gradient: [[-14.991   20.7339  -0.9957 -61.2512 -89.2975]]\n",
      "Weights: [[-4.172  -1.2035 -0.0954  0.1203  0.0743]]\n",
      "MSE loss: 183.5736\n",
      "Iteration: 198800\n",
      "Gradient: [[  -7.5955    4.6133   40.9287   28.9663 -558.3204]]\n",
      "Weights: [[-4.172  -1.2033 -0.0955  0.1203  0.0743]]\n",
      "MSE loss: 183.5533\n",
      "Iteration: 198900\n",
      "Gradient: [[  -0.3174    3.0244   38.5296 -145.8491  127.9301]]\n",
      "Weights: [[-4.1705 -1.203  -0.0956  0.1203  0.0743]]\n",
      "MSE loss: 183.5346\n",
      "Iteration: 199000\n",
      "Gradient: [[  7.8431 -14.3914  41.6793  27.2509  -3.3629]]\n",
      "Weights: [[-4.1712 -1.2028 -0.0958  0.1203  0.0743]]\n",
      "MSE loss: 183.5141\n",
      "Iteration: 199100\n",
      "Gradient: [[   6.9066  -13.2163    5.9784  171.8924 -436.2537]]\n",
      "Weights: [[-4.1709 -1.2027 -0.0958  0.1203  0.0743]]\n",
      "MSE loss: 183.5034\n",
      "Iteration: 199200\n",
      "Gradient: [[  0.4502   8.1107 -10.8145 -88.4896 299.4965]]\n",
      "Weights: [[-4.1708 -1.2026 -0.0959  0.1202  0.0743]]\n",
      "MSE loss: 183.4845\n",
      "Iteration: 199300\n",
      "Gradient: [[  -3.4261    1.1606  -56.9437 -219.982  -172.0406]]\n",
      "Weights: [[-4.1708 -1.2026 -0.096   0.1202  0.0743]]\n",
      "MSE loss: 183.4709\n",
      "Iteration: 199400\n",
      "Gradient: [[ -10.8171    7.5362   41.7484  -50.0412 -538.2942]]\n",
      "Weights: [[-4.1717 -1.2023 -0.0961  0.1202  0.0743]]\n",
      "MSE loss: 183.452\n",
      "Iteration: 199500\n",
      "Gradient: [[3.430000e-02 1.803200e+00 1.693400e+00 1.440326e+02 2.673994e+02]]\n",
      "Weights: [[-4.1721 -1.2022 -0.0962  0.1202  0.0743]]\n",
      "MSE loss: 183.4347\n",
      "Iteration: 199600\n",
      "Gradient: [[  1.1828   7.348   18.8229 -35.44   310.9021]]\n",
      "Weights: [[-4.1728 -1.2019 -0.0963  0.1202  0.0743]]\n",
      "MSE loss: 183.4185\n",
      "Iteration: 199700\n",
      "Gradient: [[  5.373  -24.3114 -20.6351 -87.7229 199.2142]]\n",
      "Weights: [[-4.1704 -1.2017 -0.0964  0.1202  0.0743]]\n",
      "MSE loss: 183.3977\n",
      "Iteration: 199800\n",
      "Gradient: [[ 16.8887  -6.7091 -26.9561 -48.4964   0.4762]]\n",
      "Weights: [[-4.1699 -1.2016 -0.0966  0.1202  0.0743]]\n",
      "MSE loss: 183.3784\n",
      "Iteration: 199900\n",
      "Gradient: [[ -3.7543   1.3266  26.834  101.7804 -68.1094]]\n",
      "Weights: [[-4.1695 -1.2016 -0.0967  0.1202  0.0744]]\n",
      "MSE loss: 183.3643\n",
      "Iteration: 200000\n",
      "Gradient: [[  5.3956   9.7786 -15.4919 -71.5392  -3.5655]]\n",
      "Weights: [[-4.1695 -1.2014 -0.0968  0.1202  0.0744]]\n",
      "MSE loss: 183.3459\n",
      "Iteration: 200100\n",
      "Gradient: [[  3.8472 -16.2976  67.6701 -31.827   -2.1784]]\n",
      "Weights: [[-4.17   -1.201  -0.0969  0.1202  0.0744]]\n",
      "MSE loss: 183.3253\n",
      "Iteration: 200200\n",
      "Gradient: [[  6.6895  -1.3357  23.0207 -60.9425 -94.6842]]\n",
      "Weights: [[-4.1706 -1.2009 -0.097   0.1202  0.0744]]\n",
      "MSE loss: 183.3091\n",
      "Iteration: 200300\n",
      "Gradient: [[  -6.754     2.1168   -8.2473 -106.8632 -104.2623]]\n",
      "Weights: [[-4.1721 -1.2006 -0.097   0.1202  0.0744]]\n",
      "MSE loss: 183.2881\n",
      "Iteration: 200400\n",
      "Gradient: [[-2.0401 -5.1018 -6.6212 72.5486 13.5744]]\n",
      "Weights: [[-4.1736 -1.2001 -0.0971  0.1202  0.0744]]\n",
      "MSE loss: 183.2654\n",
      "Iteration: 200500\n",
      "Gradient: [[ 16.0552  -3.1038 -22.2377  -6.8448 313.9589]]\n",
      "Weights: [[-4.1729 -1.1997 -0.0972  0.1202  0.0744]]\n",
      "MSE loss: 183.2406\n",
      "Iteration: 200600\n",
      "Gradient: [[ -3.9701 -10.3972 -38.2688 -26.3627  40.2486]]\n",
      "Weights: [[-4.1735 -1.1995 -0.0973  0.1202  0.0744]]\n",
      "MSE loss: 183.2253\n",
      "Iteration: 200700\n",
      "Gradient: [[ -7.3296  -1.4928  16.298  137.2354  -7.8676]]\n",
      "Weights: [[-4.1727 -1.1994 -0.0974  0.1202  0.0744]]\n",
      "MSE loss: 183.2114\n",
      "Iteration: 200800\n",
      "Gradient: [[ -2.4993 -15.565    8.0356  41.2596  -7.2377]]\n",
      "Weights: [[-4.1734 -1.1993 -0.0975  0.1201  0.0744]]\n",
      "MSE loss: 183.1931\n",
      "Iteration: 200900\n",
      "Gradient: [[  -3.7888   13.6859   33.8528 -130.4196 -137.2352]]\n",
      "Weights: [[-4.1733 -1.1992 -0.0976  0.1201  0.0744]]\n",
      "MSE loss: 183.1792\n",
      "Iteration: 201000\n",
      "Gradient: [[   8.4978   15.7502    6.0345  -10.8498 -137.9929]]\n",
      "Weights: [[-4.1745 -1.199  -0.0976  0.1201  0.0744]]\n",
      "MSE loss: 183.1713\n",
      "Iteration: 201100\n",
      "Gradient: [[  2.2075   1.3832  11.4034 -28.3786 320.1174]]\n",
      "Weights: [[-4.1742 -1.1988 -0.0977  0.1201  0.0744]]\n",
      "MSE loss: 183.1515\n",
      "Iteration: 201200\n",
      "Gradient: [[  -0.8698  -24.4587  -14.0362  218.673  -172.005 ]]\n",
      "Weights: [[-4.1754 -1.1985 -0.0978  0.1201  0.0744]]\n",
      "MSE loss: 183.1369\n",
      "Iteration: 201300\n",
      "Gradient: [[  -6.8274  -18.7494   13.6381   59.1721 -374.6052]]\n",
      "Weights: [[-4.1745 -1.1982 -0.0978  0.1201  0.0745]]\n",
      "MSE loss: 183.1127\n",
      "Iteration: 201400\n",
      "Gradient: [[ -13.2476    5.0205   29.4291  -93.5265 -185.8141]]\n",
      "Weights: [[-4.1742 -1.1979 -0.0979  0.1201  0.0745]]\n",
      "MSE loss: 183.094\n",
      "Iteration: 201500\n",
      "Gradient: [[  9.9326 -25.1641 -19.2784  85.9939 333.5479]]\n",
      "Weights: [[-4.1732 -1.1978 -0.0981  0.1201  0.0745]]\n",
      "MSE loss: 183.0764\n",
      "Iteration: 201600\n",
      "Gradient: [[ -13.3564   11.1187   28.9143  -12.2086 -508.6654]]\n",
      "Weights: [[-4.1722 -1.1978 -0.0982  0.1201  0.0745]]\n",
      "MSE loss: 183.0616\n",
      "Iteration: 201700\n",
      "Gradient: [[  16.9157   22.9419    6.5725  172.9269 -231.4342]]\n",
      "Weights: [[-4.1723 -1.1978 -0.0983  0.1201  0.0745]]\n",
      "MSE loss: 183.0449\n",
      "Iteration: 201800\n",
      "Gradient: [[ -2.8883 -10.1822  60.9369  52.7191 194.0325]]\n",
      "Weights: [[-4.172  -1.1978 -0.0984  0.1201  0.0745]]\n",
      "MSE loss: 183.0301\n",
      "Iteration: 201900\n",
      "Gradient: [[  -5.8156  -15.167    -0.3459    8.4368 -261.1478]]\n",
      "Weights: [[-4.1719 -1.1977 -0.0985  0.1201  0.0745]]\n",
      "MSE loss: 183.0155\n",
      "Iteration: 202000\n",
      "Gradient: [[  2.7826  -6.7417  21.7355  74.8915 292.6143]]\n",
      "Weights: [[-4.1734 -1.1977 -0.0986  0.12    0.0745]]\n",
      "MSE loss: 182.9993\n",
      "Iteration: 202100\n",
      "Gradient: [[ -5.356    3.1213  -2.7727 -72.5647  45.9315]]\n",
      "Weights: [[-4.1737 -1.1976 -0.0987  0.12    0.0745]]\n",
      "MSE loss: 182.9819\n",
      "Iteration: 202200\n",
      "Gradient: [[  13.4033  -17.065    23.7318  -36.4526 -368.0669]]\n",
      "Weights: [[-4.1732 -1.1974 -0.0987  0.12    0.0745]]\n",
      "MSE loss: 182.9687\n",
      "Iteration: 202300\n",
      "Gradient: [[ -4.49   -27.0902   7.2629   2.423   57.4267]]\n",
      "Weights: [[-4.1732 -1.1971 -0.0988  0.12    0.0745]]\n",
      "MSE loss: 182.9502\n",
      "Iteration: 202400\n",
      "Gradient: [[   5.9642    3.8321   31.513   -76.5794 -234.418 ]]\n",
      "Weights: [[-4.1728 -1.197  -0.0989  0.12    0.0745]]\n",
      "MSE loss: 182.9374\n",
      "Iteration: 202500\n",
      "Gradient: [[ -2.3369  -7.3358  11.8205 -46.0123 140.8154]]\n",
      "Weights: [[-4.1738 -1.1967 -0.099   0.12    0.0745]]\n",
      "MSE loss: 182.9157\n",
      "Iteration: 202600\n",
      "Gradient: [[  -3.2205   -4.4115  -63.3078   22.3071 -435.8284]]\n",
      "Weights: [[-4.174  -1.1964 -0.0991  0.12    0.0746]]\n",
      "MSE loss: 182.8953\n",
      "Iteration: 202700\n",
      "Gradient: [[  7.2808  -5.1969   4.4454   8.9047 -65.39  ]]\n",
      "Weights: [[-4.1739 -1.1961 -0.0992  0.12    0.0746]]\n",
      "MSE loss: 182.878\n",
      "Iteration: 202800\n",
      "Gradient: [[  8.0375 -19.7329 -26.5725   6.1746 -42.1278]]\n",
      "Weights: [[-4.1733 -1.1957 -0.0993  0.12    0.0746]]\n",
      "MSE loss: 182.858\n",
      "Iteration: 202900\n",
      "Gradient: [[  -6.1069  -20.6439   37.1996   34.4202 -231.6929]]\n",
      "Weights: [[-4.1739 -1.1955 -0.0994  0.12    0.0746]]\n",
      "MSE loss: 182.8363\n",
      "Iteration: 203000\n",
      "Gradient: [[ -0.9222  -9.4715 -18.0188 -11.5651 -34.6255]]\n",
      "Weights: [[-4.1736 -1.1953 -0.0995  0.12    0.0746]]\n",
      "MSE loss: 182.8244\n",
      "Iteration: 203100\n",
      "Gradient: [[  10.4792   19.7536   65.593  -135.6193   14.573 ]]\n",
      "Weights: [[-4.1729 -1.1953 -0.0996  0.12    0.0746]]\n",
      "MSE loss: 182.8065\n",
      "Iteration: 203200\n",
      "Gradient: [[-11.6527   6.3073  76.9477  90.8102  26.456 ]]\n",
      "Weights: [[-4.1735 -1.1954 -0.0996  0.12    0.0746]]\n",
      "MSE loss: 182.7979\n",
      "Iteration: 203300\n",
      "Gradient: [[   7.2212  -14.7347   18.8038  117.5917 -133.315 ]]\n",
      "Weights: [[-4.1746 -1.1952 -0.0997  0.12    0.0746]]\n",
      "MSE loss: 182.7803\n",
      "Iteration: 203400\n",
      "Gradient: [[   6.8433    1.4364   30.0324  -48.8616 -642.6374]]\n",
      "Weights: [[-4.1736 -1.1951 -0.0998  0.12    0.0746]]\n",
      "MSE loss: 182.7637\n",
      "Iteration: 203500\n",
      "Gradient: [[ 16.9034   6.8706   0.9939 125.2149 120.7204]]\n",
      "Weights: [[-4.1742 -1.1951 -0.0999  0.12    0.0746]]\n",
      "MSE loss: 182.7495\n",
      "Iteration: 203600\n",
      "Gradient: [[  -0.7506   13.5302  -30.5255  111.791  -210.0815]]\n",
      "Weights: [[-4.1737 -1.195  -0.0999  0.12    0.0746]]\n",
      "MSE loss: 182.734\n",
      "Iteration: 203700\n",
      "Gradient: [[   5.5713   13.7087  -38.6624   34.0384 -100.8465]]\n",
      "Weights: [[-4.1742 -1.1947 -0.1001  0.12    0.0746]]\n",
      "MSE loss: 182.7129\n",
      "Iteration: 203800\n",
      "Gradient: [[-11.3423  -3.58   -12.2014 -52.258    1.4348]]\n",
      "Weights: [[-4.1745 -1.1947 -0.1002  0.12    0.0746]]\n",
      "MSE loss: 182.6993\n",
      "Iteration: 203900\n",
      "Gradient: [[  3.927  -12.9693  44.6155   1.0064 -12.6579]]\n",
      "Weights: [[-4.175  -1.1946 -0.1003  0.12    0.0747]]\n",
      "MSE loss: 182.6829\n",
      "Iteration: 204000\n",
      "Gradient: [[  -3.0655   12.9317   16.2938  -65.8565 -229.23  ]]\n",
      "Weights: [[-4.1753 -1.1945 -0.1004  0.12    0.0747]]\n",
      "MSE loss: 182.6677\n",
      "Iteration: 204100\n",
      "Gradient: [[ 9.6789 19.3623  7.6751 93.3724 48.8341]]\n",
      "Weights: [[-4.1757 -1.1946 -0.1004  0.12    0.0747]]\n",
      "MSE loss: 182.66\n",
      "Iteration: 204200\n",
      "Gradient: [[  -2.0022    7.388   -35.2924   74.4513 -295.7448]]\n",
      "Weights: [[-4.1745 -1.1946 -0.1005  0.12    0.0747]]\n",
      "MSE loss: 182.6387\n",
      "Iteration: 204300\n",
      "Gradient: [[   0.5926   21.8789   -2.1642  125.2047 -275.8873]]\n",
      "Weights: [[-4.1745 -1.1943 -0.1006  0.1199  0.0747]]\n",
      "MSE loss: 182.622\n",
      "Iteration: 204400\n",
      "Gradient: [[ -12.5178  -11.0319   25.5559   81.0707 -230.3618]]\n",
      "Weights: [[-4.1752 -1.194  -0.1007  0.1199  0.0747]]\n",
      "MSE loss: 182.6019\n",
      "Iteration: 204500\n",
      "Gradient: [[   3.6098   20.062    28.6306    6.5961 -478.2418]]\n",
      "Weights: [[-4.1762 -1.1936 -0.1007  0.1199  0.0747]]\n",
      "MSE loss: 182.585\n",
      "Iteration: 204600\n",
      "Gradient: [[ -15.0422   -4.6547   76.2935   48.7305 -428.8706]]\n",
      "Weights: [[-4.1758 -1.1934 -0.1008  0.1199  0.0747]]\n",
      "MSE loss: 182.5649\n",
      "Iteration: 204700\n",
      "Gradient: [[ -8.1874  -6.9921 -36.7157 -51.4017 174.8829]]\n",
      "Weights: [[-4.1759 -1.1933 -0.1009  0.1199  0.0747]]\n",
      "MSE loss: 182.5488\n",
      "Iteration: 204800\n",
      "Gradient: [[ 12.875  -23.7468  20.3433  38.3981 117.2801]]\n",
      "Weights: [[-4.1764 -1.1931 -0.101   0.1199  0.0747]]\n",
      "MSE loss: 182.531\n",
      "Iteration: 204900\n",
      "Gradient: [[ -2.3704   2.0237  49.5793 127.7834 175.7411]]\n",
      "Weights: [[-4.1767 -1.1927 -0.101   0.1199  0.0747]]\n",
      "MSE loss: 182.5158\n",
      "Iteration: 205000\n",
      "Gradient: [[ -18.6924    6.4857    2.2037  135.1728 -110.5245]]\n",
      "Weights: [[-4.1764 -1.1926 -0.1011  0.1199  0.0748]]\n",
      "MSE loss: 182.4953\n",
      "Iteration: 205100\n",
      "Gradient: [[  -9.0096   -3.8047  -15.6029  -47.5165 -494.7612]]\n",
      "Weights: [[-4.1768 -1.1925 -0.1012  0.1199  0.0748]]\n",
      "MSE loss: 182.4876\n",
      "Iteration: 205200\n",
      "Gradient: [[  8.9499  -6.1354  17.3902 133.8547 144.3844]]\n",
      "Weights: [[-4.176  -1.1923 -0.1012  0.1199  0.0748]]\n",
      "MSE loss: 182.4697\n",
      "Iteration: 205300\n",
      "Gradient: [[   8.0965   11.3025   11.0676  105.9321 -329.3552]]\n",
      "Weights: [[-4.1759 -1.1922 -0.1014  0.1199  0.0748]]\n",
      "MSE loss: 182.4498\n",
      "Iteration: 205400\n",
      "Gradient: [[ 10.2376   2.6661  32.6928  71.4032 -88.8664]]\n",
      "Weights: [[-4.1759 -1.1919 -0.1014  0.1199  0.0748]]\n",
      "MSE loss: 182.4354\n",
      "Iteration: 205500\n",
      "Gradient: [[-16.6313 -11.4553 -40.0745  17.2851  21.1063]]\n",
      "Weights: [[-4.1758 -1.1918 -0.1015  0.1199  0.0748]]\n",
      "MSE loss: 182.4234\n",
      "Iteration: 205600\n",
      "Gradient: [[ -2.805   -3.7574 -26.3395  72.2246  69.2324]]\n",
      "Weights: [[-4.1758 -1.1916 -0.1016  0.1199  0.0748]]\n",
      "MSE loss: 182.4098\n",
      "Iteration: 205700\n",
      "Gradient: [[-24.4523 -12.9242  51.5506  95.1724 147.3121]]\n",
      "Weights: [[-4.1765 -1.1915 -0.1016  0.1198  0.0748]]\n",
      "MSE loss: 182.3955\n",
      "Iteration: 205800\n",
      "Gradient: [[ -6.5628   4.0803  42.5931 -58.3862  -7.8527]]\n",
      "Weights: [[-4.1764 -1.1912 -0.1017  0.1198  0.0748]]\n",
      "MSE loss: 182.3739\n",
      "Iteration: 205900\n",
      "Gradient: [[  0.5228 -28.4098   3.7569 -85.9903  48.6318]]\n",
      "Weights: [[-4.1752 -1.1911 -0.1018  0.1198  0.0748]]\n",
      "MSE loss: 182.3613\n",
      "Iteration: 206000\n",
      "Gradient: [[  -6.5403   -2.357    23.515    91.4196 -467.1356]]\n",
      "Weights: [[-4.177  -1.1909 -0.1019  0.1198  0.0748]]\n",
      "MSE loss: 182.3408\n",
      "Iteration: 206100\n",
      "Gradient: [[-11.8034   4.3582  -4.1808 -36.1877 161.7612]]\n",
      "Weights: [[-4.177  -1.1909 -0.102   0.1198  0.0748]]\n",
      "MSE loss: 182.3338\n",
      "Iteration: 206200\n",
      "Gradient: [[   2.0789   35.8175  -30.4232 -166.8573 -238.2887]]\n",
      "Weights: [[-4.177  -1.1907 -0.102   0.1198  0.0748]]\n",
      "MSE loss: 182.32\n",
      "Iteration: 206300\n",
      "Gradient: [[  4.3549  -8.8101  11.4316 136.6053 426.1994]]\n",
      "Weights: [[-4.1765 -1.1905 -0.1021  0.1198  0.0748]]\n",
      "MSE loss: 182.3003\n",
      "Iteration: 206400\n",
      "Gradient: [[   7.8751   10.1475  -16.6179  -21.5064 -288.2554]]\n",
      "Weights: [[-4.1769 -1.1905 -0.1023  0.1198  0.0749]]\n",
      "MSE loss: 182.2812\n",
      "Iteration: 206500\n",
      "Gradient: [[   7.9118  -14.5016   19.1244   15.8707 -148.2428]]\n",
      "Weights: [[-4.1773 -1.1904 -0.1023  0.1198  0.0749]]\n",
      "MSE loss: 182.2694\n",
      "Iteration: 206600\n",
      "Gradient: [[  -0.4077  -25.3981   55.745  -185.0272 -270.259 ]]\n",
      "Weights: [[-4.1766 -1.1902 -0.1024  0.1198  0.0749]]\n",
      "MSE loss: 182.2555\n",
      "Iteration: 206700\n",
      "Gradient: [[-10.2165 -14.197  -65.4024 -43.3008 187.0911]]\n",
      "Weights: [[-4.1772 -1.19   -0.1024  0.1198  0.0749]]\n",
      "MSE loss: 182.2411\n",
      "Iteration: 206800\n",
      "Gradient: [[  6.4233   3.4294  38.8043 -19.2323 200.0871]]\n",
      "Weights: [[-4.1765 -1.19   -0.1025  0.1197  0.0749]]\n",
      "MSE loss: 182.23\n",
      "Iteration: 206900\n",
      "Gradient: [[ -0.5021 -15.6786 -13.6405 -54.7063  90.1715]]\n",
      "Weights: [[-4.1778 -1.1899 -0.1026  0.1197  0.0749]]\n",
      "MSE loss: 182.2114\n",
      "Iteration: 207000\n",
      "Gradient: [[  3.1315  -7.0183   6.2643 -44.621   15.9791]]\n",
      "Weights: [[-4.1778 -1.1898 -0.1027  0.1197  0.0749]]\n",
      "MSE loss: 182.1983\n",
      "Iteration: 207100\n",
      "Gradient: [[  -1.2758  -12.0666   30.5144   79.4728 -279.0257]]\n",
      "Weights: [[-4.1791 -1.1896 -0.1028  0.1197  0.0749]]\n",
      "MSE loss: 182.1864\n",
      "Iteration: 207200\n",
      "Gradient: [[  2.2662  14.8885  48.6812  -0.4225 -76.5443]]\n",
      "Weights: [[-4.1785 -1.1893 -0.1028  0.1197  0.0749]]\n",
      "MSE loss: 182.1628\n",
      "Iteration: 207300\n",
      "Gradient: [[ -19.9644    0.7989   15.3902   93.8408 -142.6892]]\n",
      "Weights: [[-4.178  -1.1891 -0.103   0.1197  0.0749]]\n",
      "MSE loss: 182.1397\n",
      "Iteration: 207400\n",
      "Gradient: [[   0.4955    7.0658  -42.8528   -5.201  -211.3757]]\n",
      "Weights: [[-4.1768 -1.189  -0.103   0.1197  0.0749]]\n",
      "MSE loss: 182.129\n",
      "Iteration: 207500\n",
      "Gradient: [[  3.0785 -11.0382 -30.803  -99.7696 239.0931]]\n",
      "Weights: [[-4.1764 -1.1888 -0.1031  0.1197  0.0749]]\n",
      "MSE loss: 182.1136\n",
      "Iteration: 207600\n",
      "Gradient: [[ -1.5413   2.2451  15.2417  19.4233 -30.0176]]\n",
      "Weights: [[-4.1767 -1.1888 -0.1032  0.1197  0.0749]]\n",
      "MSE loss: 182.0961\n",
      "Iteration: 207700\n",
      "Gradient: [[ 22.8092  -0.4431  39.8359  82.0775 108.3787]]\n",
      "Weights: [[-4.1768 -1.1886 -0.1033  0.1197  0.075 ]]\n",
      "MSE loss: 182.0727\n",
      "Iteration: 207800\n",
      "Gradient: [[  7.4766   1.904  -18.1636  53.4462  23.7088]]\n",
      "Weights: [[-4.1773 -1.1885 -0.1034  0.1197  0.075 ]]\n",
      "MSE loss: 182.0566\n",
      "Iteration: 207900\n",
      "Gradient: [[   7.2105   -9.2734   37.4035    3.0682 -123.2614]]\n",
      "Weights: [[-4.1768 -1.1883 -0.1035  0.1197  0.075 ]]\n",
      "MSE loss: 182.0368\n",
      "Iteration: 208000\n",
      "Gradient: [[ -6.5991   0.263   16.3934 -40.556  -73.5061]]\n",
      "Weights: [[-4.1763 -1.1882 -0.1037  0.1197  0.075 ]]\n",
      "MSE loss: 182.0177\n",
      "Iteration: 208100\n",
      "Gradient: [[  5.732  -26.0756 -12.7916  -8.7732  71.0126]]\n",
      "Weights: [[-4.1772 -1.188  -0.1037  0.1196  0.075 ]]\n",
      "MSE loss: 182.0032\n",
      "Iteration: 208200\n",
      "Gradient: [[ 7.07800e-01 -1.00011e+01  3.07200e-01 -4.25559e+01 -6.06632e+02]]\n",
      "Weights: [[-4.1764 -1.1878 -0.1038  0.1196  0.075 ]]\n",
      "MSE loss: 181.9826\n",
      "Iteration: 208300\n",
      "Gradient: [[  -0.6529   21.3093  -30.8393 -105.4991 -201.579 ]]\n",
      "Weights: [[-4.1768 -1.1878 -0.104   0.1196  0.075 ]]\n",
      "MSE loss: 181.9675\n",
      "Iteration: 208400\n",
      "Gradient: [[  -6.8554   -7.1893  -30.6187 -102.1237 -185.4684]]\n",
      "Weights: [[-4.1761 -1.1875 -0.1041  0.1196  0.075 ]]\n",
      "MSE loss: 181.9496\n",
      "Iteration: 208500\n",
      "Gradient: [[   8.44     12.3174  -16.2572 -104.9924 -103.739 ]]\n",
      "Weights: [[-4.1761 -1.1875 -0.1042  0.1196  0.075 ]]\n",
      "MSE loss: 181.9313\n",
      "Iteration: 208600\n",
      "Gradient: [[ 10.666   -0.5099  37.1642 -77.2306 -71.7501]]\n",
      "Weights: [[-4.1777 -1.1875 -0.1043  0.1196  0.075 ]]\n",
      "MSE loss: 181.9168\n",
      "Iteration: 208700\n",
      "Gradient: [[ -6.1743   6.8976 -11.1115 102.4604  53.0132]]\n",
      "Weights: [[-4.1781 -1.1872 -0.1044  0.1196  0.075 ]]\n",
      "MSE loss: 181.9003\n",
      "Iteration: 208800\n",
      "Gradient: [[ 11.335   15.5375 -20.1407  -5.3062 125.5562]]\n",
      "Weights: [[-4.1765 -1.1871 -0.1045  0.1196  0.075 ]]\n",
      "MSE loss: 181.8815\n",
      "Iteration: 208900\n",
      "Gradient: [[ -2.0836  23.7985 -12.2974 -63.1744  20.5751]]\n",
      "Weights: [[-4.1771 -1.1871 -0.1046  0.1196  0.075 ]]\n",
      "MSE loss: 181.8673\n",
      "Iteration: 209000\n",
      "Gradient: [[   7.8017    9.2194   12.5727  -41.8277 -100.9201]]\n",
      "Weights: [[-4.1766 -1.187  -0.1046  0.1196  0.0751]]\n",
      "MSE loss: 181.8525\n",
      "Iteration: 209100\n",
      "Gradient: [[ -2.7054  14.3401 -29.2442 -61.6083 185.0289]]\n",
      "Weights: [[-4.1764 -1.1869 -0.1048  0.1196  0.0751]]\n",
      "MSE loss: 181.8337\n",
      "Iteration: 209200\n",
      "Gradient: [[ -0.4921 -17.2903   1.0204 -27.5052 -52.3173]]\n",
      "Weights: [[-4.1766 -1.1867 -0.1049  0.1196  0.0751]]\n",
      "MSE loss: 181.8164\n",
      "Iteration: 209300\n",
      "Gradient: [[   6.3907  -29.469    -8.606  -183.2475   12.8076]]\n",
      "Weights: [[-4.1782 -1.1863 -0.105   0.1196  0.0751]]\n",
      "MSE loss: 181.801\n",
      "Iteration: 209400\n",
      "Gradient: [[   7.6952  -18.6      -7.8043   79.0219 -456.9154]]\n",
      "Weights: [[-4.1783 -1.1858 -0.105   0.1196  0.0751]]\n",
      "MSE loss: 181.7803\n",
      "Iteration: 209500\n",
      "Gradient: [[  7.6674  -4.9418  34.8668  85.4753 -47.5129]]\n",
      "Weights: [[-4.1774 -1.1856 -0.1051  0.1196  0.0751]]\n",
      "MSE loss: 181.7614\n",
      "Iteration: 209600\n",
      "Gradient: [[  9.1719  -4.156   27.6739 -40.0238 138.9351]]\n",
      "Weights: [[-4.1776 -1.1854 -0.1052  0.1196  0.0751]]\n",
      "MSE loss: 181.7415\n",
      "Iteration: 209700\n",
      "Gradient: [[  -3.1521   17.9796  -44.2997 -174.9472 -143.9766]]\n",
      "Weights: [[-4.1781 -1.1851 -0.1053  0.1196  0.0751]]\n",
      "MSE loss: 181.7213\n",
      "Iteration: 209800\n",
      "Gradient: [[-4.780000e-02  1.084730e+01  3.830100e+00  3.427400e+00 -1.186746e+02]]\n",
      "Weights: [[-4.1787 -1.1847 -0.1054  0.1195  0.0751]]\n",
      "MSE loss: 181.7048\n",
      "Iteration: 209900\n",
      "Gradient: [[   3.6715  -11.5915   62.2589  190.9398 -141.3927]]\n",
      "Weights: [[-4.1789 -1.1846 -0.1054  0.1195  0.0751]]\n",
      "MSE loss: 181.691\n",
      "Iteration: 210000\n",
      "Gradient: [[   3.1204   -2.2374  -73.7856 -130.1014  207.5491]]\n",
      "Weights: [[-4.179  -1.1845 -0.1055  0.1195  0.0751]]\n",
      "MSE loss: 181.6792\n",
      "Iteration: 210100\n",
      "Gradient: [[ -2.602   18.191   37.1179  41.4141 138.0305]]\n",
      "Weights: [[-4.1783 -1.1844 -0.1056  0.1195  0.0751]]\n",
      "MSE loss: 181.6613\n",
      "Iteration: 210200\n",
      "Gradient: [[ -2.088  -24.5797  62.4198  70.502  -58.7732]]\n",
      "Weights: [[-4.1787 -1.1842 -0.1057  0.1195  0.0751]]\n",
      "MSE loss: 181.6478\n",
      "Iteration: 210300\n",
      "Gradient: [[ -2.0669  26.7535  -3.1098 -75.6681  37.5976]]\n",
      "Weights: [[-4.1779 -1.1842 -0.1058  0.1195  0.0752]]\n",
      "MSE loss: 181.6302\n",
      "Iteration: 210400\n",
      "Gradient: [[  11.0599   -4.9383   66.1886  -74.3154 -167.1596]]\n",
      "Weights: [[-4.1769 -1.1841 -0.1059  0.1195  0.0752]]\n",
      "MSE loss: 181.6132\n",
      "Iteration: 210500\n",
      "Gradient: [[ -2.1775 -14.1278   8.7129  74.5907 316.5018]]\n",
      "Weights: [[-4.1784 -1.1841 -0.106   0.1195  0.0752]]\n",
      "MSE loss: 181.5998\n",
      "Iteration: 210600\n",
      "Gradient: [[  -4.5832  -16.8403  -47.7297  -24.1883 -208.9913]]\n",
      "Weights: [[-4.1769 -1.1839 -0.1061  0.1195  0.0752]]\n",
      "MSE loss: 181.5862\n",
      "Iteration: 210700\n",
      "Gradient: [[-16.3361  -1.7036 -20.424  147.5892 -58.8376]]\n",
      "Weights: [[-4.1762 -1.1839 -0.1062  0.1195  0.0752]]\n",
      "MSE loss: 181.571\n",
      "Iteration: 210800\n",
      "Gradient: [[  7.0925 -15.7647 -24.3901  27.1006 166.4087]]\n",
      "Weights: [[-4.1767 -1.184  -0.1063  0.1195  0.0752]]\n",
      "MSE loss: 181.5501\n",
      "Iteration: 210900\n",
      "Gradient: [[ -2.1438  -1.6631 -31.9107 -55.3633 -15.7441]]\n",
      "Weights: [[-4.1757 -1.1838 -0.1064  0.1195  0.0752]]\n",
      "MSE loss: 181.5392\n",
      "Iteration: 211000\n",
      "Gradient: [[ 15.2959 -26.3219  42.3011 -58.1184 422.1309]]\n",
      "Weights: [[-4.1763 -1.1838 -0.1064  0.1194  0.0752]]\n",
      "MSE loss: 181.5206\n",
      "Iteration: 211100\n",
      "Gradient: [[  11.8828   -9.7141    4.4706   32.8316 -350.2585]]\n",
      "Weights: [[-4.1772 -1.1838 -0.1065  0.1194  0.0752]]\n",
      "MSE loss: 181.5055\n",
      "Iteration: 211200\n",
      "Gradient: [[  9.0766  16.5244  -3.8155 -46.0396  16.5709]]\n",
      "Weights: [[-4.1764 -1.1835 -0.1066  0.1194  0.0752]]\n",
      "MSE loss: 181.4832\n",
      "Iteration: 211300\n",
      "Gradient: [[  -4.7751    6.9518   15.6922    3.9866 -307.8215]]\n",
      "Weights: [[-4.1768 -1.1833 -0.1068  0.1194  0.0752]]\n",
      "MSE loss: 181.4629\n",
      "Iteration: 211400\n",
      "Gradient: [[-6.630000e-02 -6.114300e+00  1.821750e+01 -1.092750e+01 -1.743509e+02]]\n",
      "Weights: [[-4.1768 -1.183  -0.1069  0.1194  0.0752]]\n",
      "MSE loss: 181.4472\n",
      "Iteration: 211500\n",
      "Gradient: [[-1.843000e-01 -9.047500e+00  2.467590e+01 -1.525060e+01 -2.996724e+02]]\n",
      "Weights: [[-4.1775 -1.1828 -0.107   0.1194  0.0752]]\n",
      "MSE loss: 181.4243\n",
      "Iteration: 211600\n",
      "Gradient: [[  18.9571    5.991    18.1352  -33.6198 -692.061 ]]\n",
      "Weights: [[-4.1771 -1.1828 -0.1071  0.1194  0.0752]]\n",
      "MSE loss: 181.4166\n",
      "Iteration: 211700\n",
      "Gradient: [[   1.472   -17.8161    8.4217 -114.7426  -16.2451]]\n",
      "Weights: [[-4.178  -1.1826 -0.1072  0.1194  0.0753]]\n",
      "MSE loss: 181.4015\n",
      "Iteration: 211800\n",
      "Gradient: [[  -2.5901    2.9004  -13.8161   16.8098 -464.1559]]\n",
      "Weights: [[-4.1776 -1.1824 -0.1072  0.1194  0.0753]]\n",
      "MSE loss: 181.3827\n",
      "Iteration: 211900\n",
      "Gradient: [[  -6.9611   -7.7668   59.717     7.0328 -187.0289]]\n",
      "Weights: [[-4.1768 -1.1823 -0.1073  0.1194  0.0753]]\n",
      "MSE loss: 181.3712\n",
      "Iteration: 212000\n",
      "Gradient: [[  6.699    8.0586 -41.6391 -11.433  -37.9254]]\n",
      "Weights: [[-4.1767 -1.1823 -0.1074  0.1194  0.0753]]\n",
      "MSE loss: 181.3515\n",
      "Iteration: 212100\n",
      "Gradient: [[ -7.2558  10.1716  25.3522  -7.9448 317.9583]]\n",
      "Weights: [[-4.1765 -1.1821 -0.1075  0.1194  0.0753]]\n",
      "MSE loss: 181.3376\n",
      "Iteration: 212200\n",
      "Gradient: [[-12.5832  -8.5528  24.488   67.1306 -85.6119]]\n",
      "Weights: [[-4.1767 -1.1821 -0.1076  0.1194  0.0753]]\n",
      "MSE loss: 181.3215\n",
      "Iteration: 212300\n",
      "Gradient: [[ -0.2758  -2.04   -30.5986  -6.2811  38.8942]]\n",
      "Weights: [[-4.1771 -1.182  -0.1077  0.1194  0.0753]]\n",
      "MSE loss: 181.3032\n",
      "Iteration: 212400\n",
      "Gradient: [[ 10.5368  14.0067   2.0398 -87.5464  -3.9843]]\n",
      "Weights: [[-4.1773 -1.1818 -0.1078  0.1194  0.0753]]\n",
      "MSE loss: 181.2895\n",
      "Iteration: 212500\n",
      "Gradient: [[   3.5063    5.9353   24.6635 -100.66   -456.6894]]\n",
      "Weights: [[-4.1768 -1.1818 -0.1079  0.1194  0.0753]]\n",
      "MSE loss: 181.2722\n",
      "Iteration: 212600\n",
      "Gradient: [[  -5.8432    2.6772    7.7065   77.8851 -220.6983]]\n",
      "Weights: [[-4.1763 -1.1814 -0.108   0.1194  0.0753]]\n",
      "MSE loss: 181.2521\n",
      "Iteration: 212700\n",
      "Gradient: [[   5.7737    2.9945  -21.7953   46.1163 -360.099 ]]\n",
      "Weights: [[-4.1772 -1.1812 -0.1081  0.1194  0.0753]]\n",
      "MSE loss: 181.2312\n",
      "Iteration: 212800\n",
      "Gradient: [[ -13.6223    7.2805   81.405     6.0201 -214.9469]]\n",
      "Weights: [[-4.1768 -1.181  -0.1082  0.1193  0.0753]]\n",
      "MSE loss: 181.2146\n",
      "Iteration: 212900\n",
      "Gradient: [[  2.504  -13.2047  27.4519 -32.067   64.7673]]\n",
      "Weights: [[-4.1765 -1.1808 -0.1083  0.1193  0.0754]]\n",
      "MSE loss: 181.1974\n",
      "Iteration: 213000\n",
      "Gradient: [[   2.5998    4.5019   -7.3222  -81.8179 -149.0789]]\n",
      "Weights: [[-4.1785 -1.1806 -0.1084  0.1193  0.0754]]\n",
      "MSE loss: 181.1752\n",
      "Iteration: 213100\n",
      "Gradient: [[  0.6832  -8.5591  23.539   13.6196 -28.7469]]\n",
      "Weights: [[-4.1778 -1.1803 -0.1085  0.1193  0.0754]]\n",
      "MSE loss: 181.1605\n",
      "Iteration: 213200\n",
      "Gradient: [[ -2.6822  -5.3983  44.908   75.826  -41.8349]]\n",
      "Weights: [[-4.1779 -1.1803 -0.1085  0.1193  0.0754]]\n",
      "MSE loss: 181.1456\n",
      "Iteration: 213300\n",
      "Gradient: [[   0.3871  -14.8666   11.9613  108.9672 -384.3224]]\n",
      "Weights: [[-4.1786 -1.1802 -0.1087  0.1193  0.0754]]\n",
      "MSE loss: 181.1289\n",
      "Iteration: 213400\n",
      "Gradient: [[ 17.3647 -20.6071 -20.5896   9.468   36.9289]]\n",
      "Weights: [[-4.1783 -1.18   -0.1087  0.1193  0.0754]]\n",
      "MSE loss: 181.1158\n",
      "Iteration: 213500\n",
      "Gradient: [[  -0.6307  -14.0128  -24.2249 -128.9831 -304.9506]]\n",
      "Weights: [[-4.1789 -1.1797 -0.1088  0.1193  0.0754]]\n",
      "MSE loss: 181.0965\n",
      "Iteration: 213600\n",
      "Gradient: [[  -9.6903  -11.0338   15.7422  -39.3599 -676.7411]]\n",
      "Weights: [[-4.179  -1.1796 -0.1089  0.1193  0.0754]]\n",
      "MSE loss: 181.0829\n",
      "Iteration: 213700\n",
      "Gradient: [[   7.9328    6.3983    3.4059 -151.7193 -150.7819]]\n",
      "Weights: [[-4.1782 -1.1795 -0.1089  0.1193  0.0754]]\n",
      "MSE loss: 181.0716\n",
      "Iteration: 213800\n",
      "Gradient: [[  -8.8819    2.1064   71.5227   55.5014 -169.5146]]\n",
      "Weights: [[-4.1782 -1.1795 -0.109   0.1193  0.0754]]\n",
      "MSE loss: 181.0587\n",
      "Iteration: 213900\n",
      "Gradient: [[  10.1995    4.2324   25.8383  -36.9759 -115.2545]]\n",
      "Weights: [[-4.1777 -1.1795 -0.1091  0.1193  0.0754]]\n",
      "MSE loss: 181.048\n",
      "Iteration: 214000\n",
      "Gradient: [[  3.5784 -20.4676  -9.0843  37.9414  45.2994]]\n",
      "Weights: [[-4.1795 -1.1794 -0.1092  0.1193  0.0754]]\n",
      "MSE loss: 181.0375\n",
      "Iteration: 214100\n",
      "Gradient: [[ -2.6299  -2.1686   4.64   -27.5174 -38.4997]]\n",
      "Weights: [[-4.1791 -1.1792 -0.1092  0.1193  0.0754]]\n",
      "MSE loss: 181.0221\n",
      "Iteration: 214200\n",
      "Gradient: [[ -7.6649  -5.5012 -55.8145  38.1475 -86.8599]]\n",
      "Weights: [[-4.1795 -1.179  -0.1093  0.1193  0.0754]]\n",
      "MSE loss: 181.0066\n",
      "Iteration: 214300\n",
      "Gradient: [[ -14.6941   -8.2787  -39.9343   19.1573 -497.8729]]\n",
      "Weights: [[-4.1796 -1.1789 -0.1094  0.1193  0.0754]]\n",
      "MSE loss: 180.9956\n",
      "Iteration: 214400\n",
      "Gradient: [[  3.1981  12.2558   3.9154 -31.4143 -24.0655]]\n",
      "Weights: [[-4.1791 -1.1785 -0.1095  0.1193  0.0755]]\n",
      "MSE loss: 180.9728\n",
      "Iteration: 214500\n",
      "Gradient: [[ -5.303   35.8574  27.588  -22.1137  73.8261]]\n",
      "Weights: [[-4.1783 -1.1785 -0.1096  0.1193  0.0755]]\n",
      "MSE loss: 180.9614\n",
      "Iteration: 214600\n",
      "Gradient: [[  -5.3779   17.3203   13.4891 -150.0724   80.7437]]\n",
      "Weights: [[-4.179  -1.1787 -0.1097  0.1193  0.0755]]\n",
      "MSE loss: 180.9481\n",
      "Iteration: 214700\n",
      "Gradient: [[  10.1384  -16.6615   52.1116  -48.2232 -284.7931]]\n",
      "Weights: [[-4.1787 -1.1784 -0.1098  0.1193  0.0755]]\n",
      "MSE loss: 180.9305\n",
      "Iteration: 214800\n",
      "Gradient: [[ -0.3446   1.6278 -13.0794 -13.5505 -70.5468]]\n",
      "Weights: [[-4.1788 -1.1784 -0.1099  0.1193  0.0755]]\n",
      "MSE loss: 180.9159\n",
      "Iteration: 214900\n",
      "Gradient: [[ 8.7662 -3.2348 -8.5025 82.6437 22.3043]]\n",
      "Weights: [[-4.1794 -1.178  -0.11    0.1193  0.0755]]\n",
      "MSE loss: 180.8955\n",
      "Iteration: 215000\n",
      "Gradient: [[ -6.4788  -4.3095 -16.9838 -52.0165 213.8936]]\n",
      "Weights: [[-4.1797 -1.1776 -0.1101  0.1193  0.0755]]\n",
      "MSE loss: 180.8721\n",
      "Iteration: 215100\n",
      "Gradient: [[  1.6896 -11.0523  12.3728   5.1289 -33.7143]]\n",
      "Weights: [[-4.1803 -1.1775 -0.1102  0.1193  0.0755]]\n",
      "MSE loss: 180.8521\n",
      "Iteration: 215200\n",
      "Gradient: [[  -2.8156    0.3544   -1.5703 -128.8095 -351.9582]]\n",
      "Weights: [[-4.1799 -1.1772 -0.1104  0.1193  0.0755]]\n",
      "MSE loss: 180.8254\n",
      "Iteration: 215300\n",
      "Gradient: [[  -5.0338   -5.2556   16.4227  -42.6847 -203.3403]]\n",
      "Weights: [[-4.1794 -1.1769 -0.1105  0.1193  0.0755]]\n",
      "MSE loss: 180.8039\n",
      "Iteration: 215400\n",
      "Gradient: [[  7.6181 -29.3807 -41.6067  89.3208 135.8066]]\n",
      "Weights: [[-4.1793 -1.1766 -0.1106  0.1193  0.0755]]\n",
      "MSE loss: 180.7905\n",
      "Iteration: 215500\n",
      "Gradient: [[-10.3648  20.078  -10.9912  50.4873 186.3559]]\n",
      "Weights: [[-4.1793 -1.1764 -0.1107  0.1192  0.0755]]\n",
      "MSE loss: 180.7736\n",
      "Iteration: 215600\n",
      "Gradient: [[  -1.0243    1.2771  -40.6016   28.0587 -318.837 ]]\n",
      "Weights: [[-4.179  -1.1763 -0.1108  0.1192  0.0755]]\n",
      "MSE loss: 180.7584\n",
      "Iteration: 215700\n",
      "Gradient: [[ -2.1573 -39.2854 -20.081  -66.5172 -48.4673]]\n",
      "Weights: [[-4.1796 -1.1762 -0.1109  0.1192  0.0756]]\n",
      "MSE loss: 180.7427\n",
      "Iteration: 215800\n",
      "Gradient: [[  16.3847    1.1249  -32.5141   72.2532 -664.5356]]\n",
      "Weights: [[-4.1792 -1.1761 -0.111   0.1192  0.0756]]\n",
      "MSE loss: 180.7289\n",
      "Iteration: 215900\n",
      "Gradient: [[-11.1378  16.9572  21.4431 -26.4417 144.6664]]\n",
      "Weights: [[-4.1801 -1.1758 -0.1111  0.1192  0.0756]]\n",
      "MSE loss: 180.7076\n",
      "Iteration: 216000\n",
      "Gradient: [[  6.183   22.6024  22.5695  47.9037 340.6484]]\n",
      "Weights: [[-4.1802 -1.1757 -0.1112  0.1192  0.0756]]\n",
      "MSE loss: 180.6948\n",
      "Iteration: 216100\n",
      "Gradient: [[   7.6315    3.8007    5.8517  154.1271 -350.6889]]\n",
      "Weights: [[-4.1796 -1.1756 -0.1113  0.1192  0.0756]]\n",
      "MSE loss: 180.676\n",
      "Iteration: 216200\n",
      "Gradient: [[  10.82    -13.8875   -7.8666 -164.3797  -65.0106]]\n",
      "Weights: [[-4.1793 -1.1755 -0.1113  0.1192  0.0756]]\n",
      "MSE loss: 180.6607\n",
      "Iteration: 216300\n",
      "Gradient: [[   3.872     9.3474  -23.6377 -211.9767 -223.5009]]\n",
      "Weights: [[-4.1789 -1.1756 -0.1114  0.1192  0.0756]]\n",
      "MSE loss: 180.6484\n",
      "Iteration: 216400\n",
      "Gradient: [[  9.9181 -10.7692  27.9439  85.1678 248.5675]]\n",
      "Weights: [[-4.1798 -1.1753 -0.1115  0.1192  0.0756]]\n",
      "MSE loss: 180.6299\n",
      "Iteration: 216500\n",
      "Gradient: [[   3.3408  -13.0061   21.2605   99.9714 -198.1871]]\n",
      "Weights: [[-4.18   -1.1751 -0.1116  0.1192  0.0756]]\n",
      "MSE loss: 180.6143\n",
      "Iteration: 216600\n",
      "Gradient: [[  2.6064  -2.026   19.2002 -53.6328 -23.474 ]]\n",
      "Weights: [[-4.1792 -1.1751 -0.1117  0.1192  0.0756]]\n",
      "MSE loss: 180.6021\n",
      "Iteration: 216700\n",
      "Gradient: [[  -5.5001  -19.4711    9.4026  117.28   -395.1993]]\n",
      "Weights: [[-4.1792 -1.175  -0.1117  0.1192  0.0756]]\n",
      "MSE loss: 180.5946\n",
      "Iteration: 216800\n",
      "Gradient: [[-11.7335   1.5368  -4.5141 103.9976 -93.0519]]\n",
      "Weights: [[-4.1798 -1.1747 -0.1118  0.1192  0.0756]]\n",
      "MSE loss: 180.5677\n",
      "Iteration: 216900\n",
      "Gradient: [[  0.4816  23.8284 -24.9426  37.8794  61.4114]]\n",
      "Weights: [[-4.1801 -1.1748 -0.112   0.1192  0.0756]]\n",
      "MSE loss: 180.5485\n",
      "Iteration: 217000\n",
      "Gradient: [[ 11.0323   7.0883 -12.5609 164.829   53.0204]]\n",
      "Weights: [[-4.1802 -1.1748 -0.1121  0.1192  0.0757]]\n",
      "MSE loss: 180.5316\n",
      "Iteration: 217100\n",
      "Gradient: [[  -4.317     3.9552  -13.2009  -46.9791 -241.4637]]\n",
      "Weights: [[-4.18   -1.1747 -0.1122  0.1192  0.0757]]\n",
      "MSE loss: 180.517\n",
      "Iteration: 217200\n",
      "Gradient: [[ -7.3218   9.0448  59.9782  -5.6157 302.9782]]\n",
      "Weights: [[-4.1802 -1.1744 -0.1122  0.1192  0.0757]]\n",
      "MSE loss: 180.4992\n",
      "Iteration: 217300\n",
      "Gradient: [[ -3.4976 -18.5945 -24.5845  37.859    5.4595]]\n",
      "Weights: [[-4.1798 -1.1742 -0.1123  0.1191  0.0757]]\n",
      "MSE loss: 180.4846\n",
      "Iteration: 217400\n",
      "Gradient: [[  -0.9623   22.1456   41.7662   -5.8518 -568.621 ]]\n",
      "Weights: [[-4.1792 -1.1742 -0.1124  0.1191  0.0757]]\n",
      "MSE loss: 180.4706\n",
      "Iteration: 217500\n",
      "Gradient: [[ 6.0658  3.1651  1.1971 10.2664 85.7084]]\n",
      "Weights: [[-4.1787 -1.1743 -0.1125  0.1191  0.0757]]\n",
      "MSE loss: 180.4624\n",
      "Iteration: 217600\n",
      "Gradient: [[   0.9748   23.8075   -3.4064   -4.1998 -161.3672]]\n",
      "Weights: [[-4.1799 -1.1742 -0.1125  0.1191  0.0757]]\n",
      "MSE loss: 180.4464\n",
      "Iteration: 217700\n",
      "Gradient: [[  -7.4844  -13.5071   16.538  -136.789   402.8739]]\n",
      "Weights: [[-4.1792 -1.1742 -0.1126  0.1191  0.0757]]\n",
      "MSE loss: 180.4326\n",
      "Iteration: 217800\n",
      "Gradient: [[ -21.9266  -17.0118   -0.3039 -129.2305 -131.888 ]]\n",
      "Weights: [[-4.18   -1.1739 -0.1127  0.1191  0.0757]]\n",
      "MSE loss: 180.411\n",
      "Iteration: 217900\n",
      "Gradient: [[ -13.435    23.4862   -1.4088  -96.1061 -212.382 ]]\n",
      "Weights: [[-4.179  -1.1738 -0.1128  0.1191  0.0757]]\n",
      "MSE loss: 180.3955\n",
      "Iteration: 218000\n",
      "Gradient: [[ -1.9799  -0.7779  20.0295  75.751  223.5861]]\n",
      "Weights: [[-4.1792 -1.1734 -0.1129  0.1191  0.0757]]\n",
      "MSE loss: 180.3772\n",
      "Iteration: 218100\n",
      "Gradient: [[   6.5376    4.3129    8.9569 -153.7686  216.0574]]\n",
      "Weights: [[-4.1788 -1.1732 -0.1131  0.1191  0.0757]]\n",
      "MSE loss: 180.3572\n",
      "Iteration: 218200\n",
      "Gradient: [[-10.5155 -11.671   11.066   11.097  159.8003]]\n",
      "Weights: [[-4.1776 -1.1731 -0.1132  0.1191  0.0758]]\n",
      "MSE loss: 180.3456\n",
      "Iteration: 218300\n",
      "Gradient: [[   0.3114    6.5081   -5.2033  112.8133 -276.037 ]]\n",
      "Weights: [[-4.178  -1.173  -0.1133  0.1191  0.0758]]\n",
      "MSE loss: 180.3233\n",
      "Iteration: 218400\n",
      "Gradient: [[   4.5013   14.0614    9.13    -22.8078 -418.4905]]\n",
      "Weights: [[-4.178  -1.1729 -0.1134  0.1191  0.0758]]\n",
      "MSE loss: 180.3029\n",
      "Iteration: 218500\n",
      "Gradient: [[  6.2259 -11.4362  42.9377   3.2244 257.6307]]\n",
      "Weights: [[-4.1787 -1.1727 -0.1136  0.1191  0.0758]]\n",
      "MSE loss: 180.2763\n",
      "Iteration: 218600\n",
      "Gradient: [[ -19.1302   -3.9957   23.8365   18.9415 -196.3093]]\n",
      "Weights: [[-4.1794 -1.1723 -0.1137  0.1191  0.0758]]\n",
      "MSE loss: 180.2572\n",
      "Iteration: 218700\n",
      "Gradient: [[ 15.3836 -11.1716 -17.5217  49.7766 262.7263]]\n",
      "Weights: [[-4.1792 -1.172  -0.1138  0.1191  0.0758]]\n",
      "MSE loss: 180.2337\n",
      "Iteration: 218800\n",
      "Gradient: [[  -6.3809   -8.4203   -6.3692  -24.5375 -104.2178]]\n",
      "Weights: [[-4.1813 -1.1717 -0.1139  0.1191  0.0758]]\n",
      "MSE loss: 180.2145\n",
      "Iteration: 218900\n",
      "Gradient: [[   0.9713   31.9303    5.6726  -95.3348 -137.434 ]]\n",
      "Weights: [[-4.1802 -1.1715 -0.114   0.1191  0.0758]]\n",
      "MSE loss: 180.1949\n",
      "Iteration: 219000\n",
      "Gradient: [[ -0.5591  -1.1067 -24.1736  74.2219 -25.5886]]\n",
      "Weights: [[-4.1804 -1.1716 -0.114   0.119   0.0758]]\n",
      "MSE loss: 180.1861\n",
      "Iteration: 219100\n",
      "Gradient: [[ 0.5902 10.7272 34.1139 40.8422 -5.9117]]\n",
      "Weights: [[-4.181  -1.1714 -0.114   0.119   0.0758]]\n",
      "MSE loss: 180.1777\n",
      "Iteration: 219200\n",
      "Gradient: [[ -7.1261   1.9537  63.136  -12.9587  65.2598]]\n",
      "Weights: [[-4.1806 -1.1713 -0.1141  0.119   0.0758]]\n",
      "MSE loss: 180.1591\n",
      "Iteration: 219300\n",
      "Gradient: [[  -6.4612   24.6798   57.9982   32.858  -291.2838]]\n",
      "Weights: [[-4.1807 -1.1713 -0.1142  0.1191  0.0758]]\n",
      "MSE loss: 180.1448\n",
      "Iteration: 219400\n",
      "Gradient: [[  6.7172   0.8855  13.4809 -62.1545  27.6742]]\n",
      "Weights: [[-4.1799 -1.1711 -0.1143  0.1191  0.0758]]\n",
      "MSE loss: 180.1295\n",
      "Iteration: 219500\n",
      "Gradient: [[-18.5308 -13.4674 -40.1645  32.9822 -73.5427]]\n",
      "Weights: [[-4.18   -1.1709 -0.1144  0.119   0.0759]]\n",
      "MSE loss: 180.1164\n",
      "Iteration: 219600\n",
      "Gradient: [[   4.1594   18.4263   -3.2976  -19.6201 -206.3561]]\n",
      "Weights: [[-4.1799 -1.1706 -0.1145  0.119   0.0759]]\n",
      "MSE loss: 180.0994\n",
      "Iteration: 219700\n",
      "Gradient: [[ -5.8008 -37.728   38.2732  21.1247 133.6419]]\n",
      "Weights: [[-4.1808 -1.1703 -0.1145  0.119   0.0759]]\n",
      "MSE loss: 180.0857\n",
      "Iteration: 219800\n",
      "Gradient: [[ -0.9579  14.1101  74.6102 106.8574 -36.1007]]\n",
      "Weights: [[-4.1809 -1.1701 -0.1146  0.119   0.0759]]\n",
      "MSE loss: 180.0703\n",
      "Iteration: 219900\n",
      "Gradient: [[  -4.2624   14.4002   56.6149  118.3113 -209.8215]]\n",
      "Weights: [[-4.1805 -1.1697 -0.1147  0.119   0.0759]]\n",
      "MSE loss: 180.049\n",
      "Iteration: 220000\n",
      "Gradient: [[  -7.0147   -1.8006   37.5322  156.4425 -266.4447]]\n",
      "Weights: [[-4.1808 -1.1695 -0.1148  0.119   0.0759]]\n",
      "MSE loss: 180.0311\n",
      "Iteration: 220100\n",
      "Gradient: [[  15.4432   18.7597   55.6738   -4.871  -106.534 ]]\n",
      "Weights: [[-4.181  -1.1695 -0.1149  0.119   0.0759]]\n",
      "MSE loss: 180.0138\n",
      "Iteration: 220200\n",
      "Gradient: [[ 24.172   -5.4594 -10.8136 124.2538 196.5491]]\n",
      "Weights: [[-4.1816 -1.1695 -0.115   0.119   0.0759]]\n",
      "MSE loss: 180.0011\n",
      "Iteration: 220300\n",
      "Gradient: [[ -7.7356   3.9939 -10.8097 -30.0024  15.8523]]\n",
      "Weights: [[-4.1815 -1.1694 -0.115   0.119   0.0759]]\n",
      "MSE loss: 179.9897\n",
      "Iteration: 220400\n",
      "Gradient: [[ -3.2469 -23.449  -13.8944 111.4855 244.39  ]]\n",
      "Weights: [[-4.1808 -1.1694 -0.1151  0.119   0.0759]]\n",
      "MSE loss: 179.975\n",
      "Iteration: 220500\n",
      "Gradient: [[  2.1063  10.3131  50.9693 120.4998 -65.779 ]]\n",
      "Weights: [[-4.1808 -1.1692 -0.1152  0.119   0.0759]]\n",
      "MSE loss: 179.9617\n",
      "Iteration: 220600\n",
      "Gradient: [[   0.4697  -16.2204   25.1768  -88.2898 -200.1291]]\n",
      "Weights: [[-4.1816 -1.169  -0.1152  0.119   0.0759]]\n",
      "MSE loss: 179.9476\n",
      "Iteration: 220700\n",
      "Gradient: [[  2.3588   0.5113   2.4795 -42.9952 230.1009]]\n",
      "Weights: [[-4.1824 -1.1689 -0.1153  0.119   0.0759]]\n",
      "MSE loss: 179.9315\n",
      "Iteration: 220800\n",
      "Gradient: [[  2.1598   6.5341 -41.4136 107.9806  60.9611]]\n",
      "Weights: [[-4.1839 -1.1688 -0.1154  0.119   0.076 ]]\n",
      "MSE loss: 179.9238\n",
      "Iteration: 220900\n",
      "Gradient: [[  5.4947 -28.1883  12.6622  80.3917 -33.9011]]\n",
      "Weights: [[-4.1844 -1.1685 -0.1155  0.1189  0.076 ]]\n",
      "MSE loss: 179.9063\n",
      "Iteration: 221000\n",
      "Gradient: [[   5.4709    2.5868   34.2168   14.8469 -506.0453]]\n",
      "Weights: [[-4.1842 -1.1683 -0.1156  0.1189  0.076 ]]\n",
      "MSE loss: 179.8856\n",
      "Iteration: 221100\n",
      "Gradient: [[  7.1512 -20.1236 -11.7816 -10.4075 -92.9633]]\n",
      "Weights: [[-4.1843 -1.168  -0.1156  0.1189  0.076 ]]\n",
      "MSE loss: 179.8706\n",
      "Iteration: 221200\n",
      "Gradient: [[ -3.9711  -5.1008 -15.1883  46.9242 266.0256]]\n",
      "Weights: [[-4.183  -1.1678 -0.1157  0.1189  0.076 ]]\n",
      "MSE loss: 179.8472\n",
      "Iteration: 221300\n",
      "Gradient: [[  -8.8773    7.9133   22.6187    0.9083 -157.0113]]\n",
      "Weights: [[-4.1828 -1.1676 -0.1158  0.1189  0.076 ]]\n",
      "MSE loss: 179.8348\n",
      "Iteration: 221400\n",
      "Gradient: [[  6.5264  -1.8057  17.2871 -94.6734 176.6758]]\n",
      "Weights: [[-4.1821 -1.1673 -0.1158  0.1189  0.076 ]]\n",
      "MSE loss: 179.8246\n",
      "Iteration: 221500\n",
      "Gradient: [[  -7.0557   16.9702  -44.9931  -81.992  -689.3068]]\n",
      "Weights: [[-4.1819 -1.1672 -0.1159  0.1189  0.076 ]]\n",
      "MSE loss: 179.8129\n",
      "Iteration: 221600\n",
      "Gradient: [[  -4.3747  -20.6154   57.0201  -54.7935 -146.9154]]\n",
      "Weights: [[-4.1817 -1.1669 -0.116   0.1189  0.076 ]]\n",
      "MSE loss: 179.7977\n",
      "Iteration: 221700\n",
      "Gradient: [[ -8.2545  -7.5683  13.3335 102.6247 -51.2929]]\n",
      "Weights: [[-4.1826 -1.1667 -0.1161  0.1189  0.076 ]]\n",
      "MSE loss: 179.7728\n",
      "Iteration: 221800\n",
      "Gradient: [[  10.9248    7.3017    1.7078   13.1594 -217.9736]]\n",
      "Weights: [[-4.1827 -1.1665 -0.1162  0.1189  0.076 ]]\n",
      "MSE loss: 179.7561\n",
      "Iteration: 221900\n",
      "Gradient: [[   0.829     0.784   -23.5423  -48.23   -116.202 ]]\n",
      "Weights: [[-4.1828 -1.1665 -0.1163  0.1188  0.076 ]]\n",
      "MSE loss: 179.7363\n",
      "Iteration: 222000\n",
      "Gradient: [[  -1.4101   -5.7047  -13.114    85.0547 -431.9013]]\n",
      "Weights: [[-4.1832 -1.1662 -0.1164  0.1188  0.076 ]]\n",
      "MSE loss: 179.717\n",
      "Iteration: 222100\n",
      "Gradient: [[  0.5498  -2.5767 -44.685   52.2138  90.0091]]\n",
      "Weights: [[-4.1825 -1.1659 -0.1165  0.1188  0.076 ]]\n",
      "MSE loss: 179.6959\n",
      "Iteration: 222200\n",
      "Gradient: [[   3.3289   14.7954  -54.9317  -14.5333 -424.262 ]]\n",
      "Weights: [[-4.1826 -1.1658 -0.1166  0.1188  0.0761]]\n",
      "MSE loss: 179.6814\n",
      "Iteration: 222300\n",
      "Gradient: [[   1.4029  -12.4592   -8.3424   61.1621 -281.3654]]\n",
      "Weights: [[-4.1831 -1.1657 -0.1167  0.1188  0.0761]]\n",
      "MSE loss: 179.6657\n",
      "Iteration: 222400\n",
      "Gradient: [[ 14.4129   1.4411  32.5816 -13.3975  29.5502]]\n",
      "Weights: [[-4.1835 -1.1656 -0.1167  0.1188  0.0761]]\n",
      "MSE loss: 179.6536\n",
      "Iteration: 222500\n",
      "Gradient: [[  12.2135   15.7657   -7.7591   -0.9596 -129.1312]]\n",
      "Weights: [[-4.183  -1.1654 -0.1168  0.1188  0.0761]]\n",
      "MSE loss: 179.6387\n",
      "Iteration: 222600\n",
      "Gradient: [[   1.0585    2.5032   -7.1872   55.2097 -572.116 ]]\n",
      "Weights: [[-4.1828 -1.1654 -0.117   0.1188  0.0761]]\n",
      "MSE loss: 179.6277\n",
      "Iteration: 222700\n",
      "Gradient: [[ 10.8892   0.7147  19.9338 -53.4869  98.319 ]]\n",
      "Weights: [[-4.1833 -1.1652 -0.117   0.1188  0.0761]]\n",
      "MSE loss: 179.6127\n",
      "Iteration: 222800\n",
      "Gradient: [[   9.3084    1.9876   12.3311 -228.8825 -260.2472]]\n",
      "Weights: [[-4.185  -1.1651 -0.1171  0.1188  0.0761]]\n",
      "MSE loss: 179.5998\n",
      "Iteration: 222900\n",
      "Gradient: [[   2.7921    3.9617   61.9648   25.0054 -214.0345]]\n",
      "Weights: [[-4.1849 -1.1649 -0.1172  0.1188  0.0761]]\n",
      "MSE loss: 179.5827\n",
      "Iteration: 223000\n",
      "Gradient: [[  -6.1634   -0.8704    8.8585    1.6029 -245.4038]]\n",
      "Weights: [[-4.1849 -1.1647 -0.1173  0.1188  0.0761]]\n",
      "MSE loss: 179.5681\n",
      "Iteration: 223100\n",
      "Gradient: [[-2.513   3.7934 38.2473 69.4629 22.3074]]\n",
      "Weights: [[-4.1858 -1.1646 -0.1173  0.1188  0.0761]]\n",
      "MSE loss: 179.5586\n",
      "Iteration: 223200\n",
      "Gradient: [[ -4.1581   9.6868   4.88    18.1687 -76.7157]]\n",
      "Weights: [[-4.1845 -1.1643 -0.1174  0.1188  0.0761]]\n",
      "MSE loss: 179.5341\n",
      "Iteration: 223300\n",
      "Gradient: [[ -8.3813  -0.6601  34.2877  38.3884 -52.9553]]\n",
      "Weights: [[-4.1849 -1.1641 -0.1176  0.1188  0.0761]]\n",
      "MSE loss: 179.5173\n",
      "Iteration: 223400\n",
      "Gradient: [[ 14.4626   0.8038  -6.6563  -5.9829 186.7058]]\n",
      "Weights: [[-4.1846 -1.1641 -0.1177  0.1188  0.0761]]\n",
      "MSE loss: 179.5011\n",
      "Iteration: 223500\n",
      "Gradient: [[  -8.3843   -0.9825  -13.8306    4.7014 -360.0485]]\n",
      "Weights: [[-4.1835 -1.1639 -0.1178  0.1188  0.0761]]\n",
      "MSE loss: 179.4837\n",
      "Iteration: 223600\n",
      "Gradient: [[  -1.418    -6.5085  -27.6822   43.4873 -300.1062]]\n",
      "Weights: [[-4.1833 -1.1639 -0.1179  0.1188  0.0762]]\n",
      "MSE loss: 179.4686\n",
      "Iteration: 223700\n",
      "Gradient: [[ -0.9363 -16.819   15.988  -14.0885 -47.0338]]\n",
      "Weights: [[-4.1836 -1.1637 -0.1179  0.1188  0.0762]]\n",
      "MSE loss: 179.4583\n",
      "Iteration: 223800\n",
      "Gradient: [[-14.5851 -11.7015  50.0604   2.3258  36.4449]]\n",
      "Weights: [[-4.1849 -1.1633 -0.118   0.1188  0.0762]]\n",
      "MSE loss: 179.4369\n",
      "Iteration: 223900\n",
      "Gradient: [[-1.286000e-01  9.608100e+00  4.905790e+01 -2.938500e+00 -2.023938e+02]]\n",
      "Weights: [[-4.1856 -1.1631 -0.1181  0.1188  0.0762]]\n",
      "MSE loss: 179.4238\n",
      "Iteration: 224000\n",
      "Gradient: [[-1.00300e-01  2.69000e-01  4.14544e+01  1.22441e+01 -4.35937e+02]]\n",
      "Weights: [[-4.1853 -1.1631 -0.1182  0.1188  0.0762]]\n",
      "MSE loss: 179.4124\n",
      "Iteration: 224100\n",
      "Gradient: [[   6.5927   14.5368  -42.6099  -67.0537 -299.5899]]\n",
      "Weights: [[-4.1857 -1.1627 -0.1183  0.1188  0.0762]]\n",
      "MSE loss: 179.3937\n",
      "Iteration: 224200\n",
      "Gradient: [[  8.2851  20.8932  33.1081 111.3172 250.118 ]]\n",
      "Weights: [[-4.1862 -1.1625 -0.1183  0.1188  0.0762]]\n",
      "MSE loss: 179.3816\n",
      "Iteration: 224300\n",
      "Gradient: [[  9.2346   0.275   46.8541 -65.5487 -22.3095]]\n",
      "Weights: [[-4.1848 -1.1623 -0.1184  0.1188  0.0762]]\n",
      "MSE loss: 179.3695\n",
      "Iteration: 224400\n",
      "Gradient: [[-9.2348 16.7486 43.6919 99.7359 50.6539]]\n",
      "Weights: [[-4.1864 -1.1623 -0.1185  0.1188  0.0762]]\n",
      "MSE loss: 179.3541\n",
      "Iteration: 224500\n",
      "Gradient: [[  -7.2149  -23.6894  -37.6505   24.5751 -552.8198]]\n",
      "Weights: [[-4.1856 -1.1621 -0.1186  0.1188  0.0762]]\n",
      "MSE loss: 179.3309\n",
      "Iteration: 224600\n",
      "Gradient: [[   2.6602   -1.9339   -2.4091  -23.9119 -414.7509]]\n",
      "Weights: [[-4.1848 -1.1618 -0.1187  0.1188  0.0762]]\n",
      "MSE loss: 179.3119\n",
      "Iteration: 224700\n",
      "Gradient: [[  2.2082 -15.2131  20.3738  47.2587 108.9888]]\n",
      "Weights: [[-4.1853 -1.1617 -0.1188  0.1188  0.0762]]\n",
      "MSE loss: 179.2989\n",
      "Iteration: 224800\n",
      "Gradient: [[  3.6785  -9.8393  30.352  -82.3484 130.7161]]\n",
      "Weights: [[-4.186  -1.1615 -0.1189  0.1188  0.0762]]\n",
      "MSE loss: 179.2825\n",
      "Iteration: 224900\n",
      "Gradient: [[ -9.0601 -10.3221  -3.4046  39.8235 150.737 ]]\n",
      "Weights: [[-4.1869 -1.1614 -0.119   0.1188  0.0762]]\n",
      "MSE loss: 179.2736\n",
      "Iteration: 225000\n",
      "Gradient: [[  -1.8827  -25.8799   16.9852   88.0525 -368.4624]]\n",
      "Weights: [[-4.1866 -1.1612 -0.1191  0.1188  0.0762]]\n",
      "MSE loss: 179.2552\n",
      "Iteration: 225100\n",
      "Gradient: [[  13.6186    3.8838   11.8737 -122.9406   66.5312]]\n",
      "Weights: [[-4.186  -1.161  -0.1192  0.1188  0.0762]]\n",
      "MSE loss: 179.2372\n",
      "Iteration: 225200\n",
      "Gradient: [[  1.5941  -2.924   34.78   -23.6474  46.6291]]\n",
      "Weights: [[-4.1865 -1.1607 -0.1193  0.1188  0.0763]]\n",
      "MSE loss: 179.2229\n",
      "Iteration: 225300\n",
      "Gradient: [[-11.8569  -9.446   22.4517 -51.1939  30.6131]]\n",
      "Weights: [[-4.1861 -1.1605 -0.1193  0.1188  0.0763]]\n",
      "MSE loss: 179.2048\n",
      "Iteration: 225400\n",
      "Gradient: [[  7.8385   5.7048  52.844   40.7115 137.8398]]\n",
      "Weights: [[-4.1862 -1.1605 -0.1195  0.1188  0.0763]]\n",
      "MSE loss: 179.1878\n",
      "Iteration: 225500\n",
      "Gradient: [[   7.3429    0.5364   45.486   -76.3422 -182.1796]]\n",
      "Weights: [[-4.1856 -1.1604 -0.1196  0.1188  0.0763]]\n",
      "MSE loss: 179.1727\n",
      "Iteration: 225600\n",
      "Gradient: [[   5.9707  -18.7942  -19.3812  -19.7384 -364.8053]]\n",
      "Weights: [[-4.1853 -1.1603 -0.1197  0.1188  0.0763]]\n",
      "MSE loss: 179.1513\n",
      "Iteration: 225700\n",
      "Gradient: [[ -8.0007  -0.309   20.0226 -74.9209  29.7165]]\n",
      "Weights: [[-4.1851 -1.1601 -0.1198  0.1187  0.0763]]\n",
      "MSE loss: 179.1379\n",
      "Iteration: 225800\n",
      "Gradient: [[   3.0482   -0.9307   57.61    -50.4113 -104.3627]]\n",
      "Weights: [[-4.1852 -1.1601 -0.1199  0.1187  0.0763]]\n",
      "MSE loss: 179.1268\n",
      "Iteration: 225900\n",
      "Gradient: [[  -5.6933    1.3596   -2.0954  -57.5147 -234.2315]]\n",
      "Weights: [[-4.1844 -1.16   -0.12    0.1187  0.0763]]\n",
      "MSE loss: 179.109\n",
      "Iteration: 226000\n",
      "Gradient: [[   1.0117  -16.4698   69.543    24.3107 -323.7339]]\n",
      "Weights: [[-4.1837 -1.1601 -0.1201  0.1187  0.0763]]\n",
      "MSE loss: 179.1052\n",
      "Iteration: 226100\n",
      "Gradient: [[  -7.0358    8.3627  -16.6976  134.3357 -120.6755]]\n",
      "Weights: [[-4.1835 -1.1601 -0.1201  0.1187  0.0763]]\n",
      "MSE loss: 179.0971\n",
      "Iteration: 226200\n",
      "Gradient: [[ -13.4151   10.4474   30.2787   63.0263 -189.5276]]\n",
      "Weights: [[-4.1847 -1.16   -0.1202  0.1187  0.0763]]\n",
      "MSE loss: 179.0774\n",
      "Iteration: 226300\n",
      "Gradient: [[ 11.7076  14.1671  25.4724  71.2963 122.0665]]\n",
      "Weights: [[-4.185  -1.1597 -0.1202  0.1187  0.0763]]\n",
      "MSE loss: 179.0635\n",
      "Iteration: 226400\n",
      "Gradient: [[   2.4555    1.0898   34.141   215.7309 -321.3848]]\n",
      "Weights: [[-4.1863 -1.1595 -0.1203  0.1187  0.0763]]\n",
      "MSE loss: 179.0489\n",
      "Iteration: 226500\n",
      "Gradient: [[  1.7769  -5.0865  28.144  -22.4757 -84.3838]]\n",
      "Weights: [[-4.1859 -1.1596 -0.1204  0.1187  0.0763]]\n",
      "MSE loss: 179.0383\n",
      "Iteration: 226600\n",
      "Gradient: [[ -10.8163    5.7071   59.3269   19.1018 -209.1199]]\n",
      "Weights: [[-4.1859 -1.1592 -0.1205  0.1187  0.0763]]\n",
      "MSE loss: 179.0194\n",
      "Iteration: 226700\n",
      "Gradient: [[ 11.9653   3.2592  27.6819  84.3573 322.141 ]]\n",
      "Weights: [[-4.1849 -1.159  -0.1206  0.1187  0.0763]]\n",
      "MSE loss: 179.0046\n",
      "Iteration: 226800\n",
      "Gradient: [[ -3.5628   2.7136  -8.3913   9.7357 235.1004]]\n",
      "Weights: [[-4.1841 -1.1589 -0.1208  0.1187  0.0763]]\n",
      "MSE loss: 178.9847\n",
      "Iteration: 226900\n",
      "Gradient: [[   1.6799  -13.8831   36.8542  -62.5173 -131.0486]]\n",
      "Weights: [[-4.1839 -1.1588 -0.1209  0.1187  0.0764]]\n",
      "MSE loss: 178.9697\n",
      "Iteration: 227000\n",
      "Gradient: [[  8.875    0.303   34.444   95.4582 194.617 ]]\n",
      "Weights: [[-4.184  -1.1585 -0.1209  0.1187  0.0764]]\n",
      "MSE loss: 178.9562\n",
      "Iteration: 227100\n",
      "Gradient: [[  -6.7277   -2.6936   54.307   106.5867 -157.1618]]\n",
      "Weights: [[-4.184  -1.1585 -0.1211  0.1186  0.0764]]\n",
      "MSE loss: 178.9397\n",
      "Iteration: 227200\n",
      "Gradient: [[ 14.2734   3.5347  -4.5563  62.4949 259.3231]]\n",
      "Weights: [[-4.1836 -1.1586 -0.1212  0.1186  0.0764]]\n",
      "MSE loss: 178.9292\n",
      "Iteration: 227300\n",
      "Gradient: [[-13.8643   7.2863   8.6233 107.5755  20.5849]]\n",
      "Weights: [[-4.1834 -1.1585 -0.1212  0.1186  0.0764]]\n",
      "MSE loss: 178.9222\n",
      "Iteration: 227400\n",
      "Gradient: [[  8.8237   2.9708 -14.0476  48.7054 386.2875]]\n",
      "Weights: [[-4.1835 -1.1583 -0.1213  0.1186  0.0764]]\n",
      "MSE loss: 178.9068\n",
      "Iteration: 227500\n",
      "Gradient: [[  -1.6062  -13.1742  -22.5879    8.4678 -342.8097]]\n",
      "Weights: [[-4.1841 -1.1581 -0.1213  0.1186  0.0764]]\n",
      "MSE loss: 178.8912\n",
      "Iteration: 227600\n",
      "Gradient: [[ -1.1912  15.4336  -2.4359  35.1351 216.5704]]\n",
      "Weights: [[-4.1851 -1.1578 -0.1214  0.1186  0.0764]]\n",
      "MSE loss: 178.8767\n",
      "Iteration: 227700\n",
      "Gradient: [[  16.5872  -10.0732  -12.5058 -138.6795  151.9667]]\n",
      "Weights: [[-4.1851 -1.1579 -0.1214  0.1186  0.0764]]\n",
      "MSE loss: 178.8652\n",
      "Iteration: 227800\n",
      "Gradient: [[  -5.8236  -16.8249   31.2657  -69.8752 -593.5816]]\n",
      "Weights: [[-4.1855 -1.1577 -0.1215  0.1186  0.0764]]\n",
      "MSE loss: 178.8506\n",
      "Iteration: 227900\n",
      "Gradient: [[   0.7649   -7.7992   28.7296 -147.9103 -610.8274]]\n",
      "Weights: [[-4.1865 -1.1574 -0.1215  0.1186  0.0764]]\n",
      "MSE loss: 178.8376\n",
      "Iteration: 228000\n",
      "Gradient: [[ -6.7305  23.0805  65.4371 -95.251  -89.9229]]\n",
      "Weights: [[-4.1862 -1.1572 -0.1216  0.1186  0.0764]]\n",
      "MSE loss: 178.8187\n",
      "Iteration: 228100\n",
      "Gradient: [[  -8.5874   -3.3358   17.4952  -15.8032 -391.6796]]\n",
      "Weights: [[-4.1847 -1.157  -0.1217  0.1186  0.0764]]\n",
      "MSE loss: 178.8054\n",
      "Iteration: 228200\n",
      "Gradient: [[  0.6721 -14.0348  19.8116  13.6137 -90.7822]]\n",
      "Weights: [[-4.1854 -1.1569 -0.1218  0.1186  0.0764]]\n",
      "MSE loss: 178.7887\n",
      "Iteration: 228300\n",
      "Gradient: [[  -6.2726    9.8012   25.5145   24.1355 -543.951 ]]\n",
      "Weights: [[-4.1858 -1.1567 -0.1219  0.1185  0.0764]]\n",
      "MSE loss: 178.7709\n",
      "Iteration: 228400\n",
      "Gradient: [[  9.9889   4.5035  78.6669  43.8311 -76.8178]]\n",
      "Weights: [[-4.1847 -1.1567 -0.122   0.1185  0.0765]]\n",
      "MSE loss: 178.7593\n",
      "Iteration: 228500\n",
      "Gradient: [[10.0909  2.1697 12.0189 21.2886 31.5569]]\n",
      "Weights: [[-4.1855 -1.1568 -0.1221  0.1185  0.0765]]\n",
      "MSE loss: 178.7434\n",
      "Iteration: 228600\n",
      "Gradient: [[-9.203300e+00 -2.580800e+00 -3.890000e-02  4.923430e+01 -6.195957e+02]]\n",
      "Weights: [[-4.1861 -1.1567 -0.1222  0.1185  0.0765]]\n",
      "MSE loss: 178.7317\n",
      "Iteration: 228700\n",
      "Gradient: [[  10.1599    4.1951  -13.6942   27.897  -258.2862]]\n",
      "Weights: [[-4.1871 -1.1565 -0.1222  0.1185  0.0765]]\n",
      "MSE loss: 178.7204\n",
      "Iteration: 228800\n",
      "Gradient: [[ -9.3244 -19.2359 -15.3207 142.0336  71.6087]]\n",
      "Weights: [[-4.1865 -1.156  -0.1223  0.1185  0.0765]]\n",
      "MSE loss: 178.6946\n",
      "Iteration: 228900\n",
      "Gradient: [[  -3.7529   -2.486    39.45     15.8071 -292.2357]]\n",
      "Weights: [[-4.1854 -1.1558 -0.1224  0.1185  0.0765]]\n",
      "MSE loss: 178.6778\n",
      "Iteration: 229000\n",
      "Gradient: [[  -2.8091  -18.5178   37.4528 -104.5538 -349.4689]]\n",
      "Weights: [[-4.1849 -1.1559 -0.1225  0.1185  0.0765]]\n",
      "MSE loss: 178.6691\n",
      "Iteration: 229100\n",
      "Gradient: [[  -8.8071   24.3263   47.3552   53.9794 -526.6613]]\n",
      "Weights: [[-4.1852 -1.1555 -0.1226  0.1185  0.0765]]\n",
      "MSE loss: 178.6472\n",
      "Iteration: 229200\n",
      "Gradient: [[   5.3342    5.4419   32.6545  -29.8005 -219.1296]]\n",
      "Weights: [[-4.1855 -1.1553 -0.1227  0.1185  0.0765]]\n",
      "MSE loss: 178.6247\n",
      "Iteration: 229300\n",
      "Gradient: [[   7.9343  -37.6073    1.4125    7.2748 -126.2584]]\n",
      "Weights: [[-4.1871 -1.1552 -0.1228  0.1185  0.0765]]\n",
      "MSE loss: 178.6105\n",
      "Iteration: 229400\n",
      "Gradient: [[-5.125700e+00 -1.207000e-01  5.931080e+01 -2.659870e+01 -5.597857e+02]]\n",
      "Weights: [[-4.1873 -1.1547 -0.1229  0.1185  0.0765]]\n",
      "MSE loss: 178.5856\n",
      "Iteration: 229500\n",
      "Gradient: [[  12.6298  -11.8212  -26.6775   51.7682 -441.779 ]]\n",
      "Weights: [[-4.1878 -1.1544 -0.123   0.1185  0.0765]]\n",
      "MSE loss: 178.5658\n",
      "Iteration: 229600\n",
      "Gradient: [[  -0.6197   -0.5502   68.547   -12.7723 -508.2065]]\n",
      "Weights: [[-4.1879 -1.1541 -0.1231  0.1185  0.0765]]\n",
      "MSE loss: 178.5449\n",
      "Iteration: 229700\n",
      "Gradient: [[ -20.3649  -19.3131  -25.27    121.7782 -193.4367]]\n",
      "Weights: [[-4.1872 -1.1538 -0.1232  0.1185  0.0766]]\n",
      "MSE loss: 178.5269\n",
      "Iteration: 229800\n",
      "Gradient: [[   0.7933   -3.1235    6.6555   75.4155 -222.2062]]\n",
      "Weights: [[-4.1875 -1.1537 -0.1233  0.1184  0.0766]]\n",
      "MSE loss: 178.5072\n",
      "Iteration: 229900\n",
      "Gradient: [[-11.6723  -1.1175  67.7965   8.0063 -21.3534]]\n",
      "Weights: [[-4.1868 -1.1535 -0.1234  0.1184  0.0766]]\n",
      "MSE loss: 178.4921\n",
      "Iteration: 230000\n",
      "Gradient: [[-12.379   13.1684 -17.2893 -86.691   85.7908]]\n",
      "Weights: [[-4.1872 -1.1533 -0.1235  0.1184  0.0766]]\n",
      "MSE loss: 178.4781\n",
      "Iteration: 230100\n",
      "Gradient: [[   4.4596   13.8777    2.7248  119.5491 -331.3379]]\n",
      "Weights: [[-4.1869 -1.1532 -0.1236  0.1184  0.0766]]\n",
      "MSE loss: 178.4632\n",
      "Iteration: 230200\n",
      "Gradient: [[ -3.8527 -19.903    6.9179  30.8556 223.5713]]\n",
      "Weights: [[-4.1882 -1.1529 -0.1236  0.1184  0.0766]]\n",
      "MSE loss: 178.4462\n",
      "Iteration: 230300\n",
      "Gradient: [[ -1.4128 -12.4806   8.2932 103.0619 -30.2438]]\n",
      "Weights: [[-4.1887 -1.1528 -0.1238  0.1184  0.0766]]\n",
      "MSE loss: 178.4259\n",
      "Iteration: 230400\n",
      "Gradient: [[   7.8659   -2.325   -17.6023  -38.6267 -214.3313]]\n",
      "Weights: [[-4.1879 -1.1526 -0.1238  0.1184  0.0766]]\n",
      "MSE loss: 178.4077\n",
      "Iteration: 230500\n",
      "Gradient: [[  4.4846   1.9711  -2.6117 -72.5271 260.2193]]\n",
      "Weights: [[-4.1868 -1.1524 -0.1239  0.1184  0.0766]]\n",
      "MSE loss: 178.3898\n",
      "Iteration: 230600\n",
      "Gradient: [[ -9.5865  -6.7363 -49.609   30.3083 -32.0528]]\n",
      "Weights: [[-4.1869 -1.1524 -0.124   0.1183  0.0766]]\n",
      "MSE loss: 178.3718\n",
      "Iteration: 230700\n",
      "Gradient: [[  -1.8765  -13.2107   25.1434  104.1341 -104.9171]]\n",
      "Weights: [[-4.1878 -1.152  -0.1242  0.1183  0.0766]]\n",
      "MSE loss: 178.3482\n",
      "Iteration: 230800\n",
      "Gradient: [[  19.8323   -3.9539   59.4714  -30.1404 -106.932 ]]\n",
      "Weights: [[-4.1884 -1.152  -0.1242  0.1183  0.0766]]\n",
      "MSE loss: 178.3343\n",
      "Iteration: 230900\n",
      "Gradient: [[-21.027   16.2992  11.7921 -30.1075 -51.88  ]]\n",
      "Weights: [[-4.189  -1.1516 -0.1243  0.1183  0.0767]]\n",
      "MSE loss: 178.3181\n",
      "Iteration: 231000\n",
      "Gradient: [[  4.8912 -13.4169  58.045   28.0872  48.9765]]\n",
      "Weights: [[-4.1885 -1.1514 -0.1244  0.1183  0.0767]]\n",
      "MSE loss: 178.2995\n",
      "Iteration: 231100\n",
      "Gradient: [[ -4.8988 -17.187   12.1218 -67.105  147.4425]]\n",
      "Weights: [[-4.1876 -1.1512 -0.1245  0.1183  0.0767]]\n",
      "MSE loss: 178.2826\n",
      "Iteration: 231200\n",
      "Gradient: [[ 12.5593  26.7559  22.5395 -46.1123  -6.164 ]]\n",
      "Weights: [[-4.1856 -1.1508 -0.1246  0.1183  0.0767]]\n",
      "MSE loss: 178.2843\n",
      "Iteration: 231300\n",
      "Gradient: [[ 1.8019  1.4324 15.5862 25.8114 38.1093]]\n",
      "Weights: [[-4.1856 -1.1508 -0.1247  0.1183  0.0767]]\n",
      "MSE loss: 178.2657\n",
      "Iteration: 231400\n",
      "Gradient: [[ -5.7545 -22.7426 -22.0592  21.2696  51.1427]]\n",
      "Weights: [[-4.1867 -1.1509 -0.1249  0.1183  0.0767]]\n",
      "MSE loss: 178.2378\n",
      "Iteration: 231500\n",
      "Gradient: [[  -1.975    17.9681    4.579  -135.2161  213.9302]]\n",
      "Weights: [[-4.1855 -1.1506 -0.1249  0.1183  0.0767]]\n",
      "MSE loss: 178.232\n",
      "Iteration: 231600\n",
      "Gradient: [[   0.6512    8.1928  -21.2552  -12.18   -202.0391]]\n",
      "Weights: [[-4.1852 -1.1506 -0.125   0.1182  0.0767]]\n",
      "MSE loss: 178.2159\n",
      "Iteration: 231700\n",
      "Gradient: [[  -4.9535  -21.6429   12.7934   45.6178 -210.2438]]\n",
      "Weights: [[-4.1847 -1.1505 -0.1251  0.1182  0.0767]]\n",
      "MSE loss: 178.209\n",
      "Iteration: 231800\n",
      "Gradient: [[-11.3313 -11.1797   9.1733 -40.8498 369.3721]]\n",
      "Weights: [[-4.1841 -1.1506 -0.1252  0.1182  0.0767]]\n",
      "MSE loss: 178.2038\n",
      "Iteration: 231900\n",
      "Gradient: [[ 14.395  -29.422    4.3117 -45.8763  62.0217]]\n",
      "Weights: [[-4.185  -1.1507 -0.1253  0.1182  0.0767]]\n",
      "MSE loss: 178.1832\n",
      "Iteration: 232000\n",
      "Gradient: [[   1.1396    6.7804   31.259    18.651  -568.791 ]]\n",
      "Weights: [[-4.1845 -1.1506 -0.1254  0.1182  0.0767]]\n",
      "MSE loss: 178.1699\n",
      "Iteration: 232100\n",
      "Gradient: [[ -2.6848  16.0395   0.5638 109.7194 182.4688]]\n",
      "Weights: [[-4.1851 -1.1505 -0.1255  0.1182  0.0767]]\n",
      "MSE loss: 178.1482\n",
      "Iteration: 232200\n",
      "Gradient: [[11.8124 -3.7765 22.0883 -5.4825 -0.2136]]\n",
      "Weights: [[-4.1857 -1.1505 -0.1255  0.1182  0.0767]]\n",
      "MSE loss: 178.1363\n",
      "Iteration: 232300\n",
      "Gradient: [[ -10.4265  -18.2755  -14.8219  -50.8437 -157.7679]]\n",
      "Weights: [[-4.1858 -1.1503 -0.1256  0.1182  0.0767]]\n",
      "MSE loss: 178.1233\n",
      "Iteration: 232400\n",
      "Gradient: [[ -1.1139 -16.3866  59.7221  33.7923 -85.4843]]\n",
      "Weights: [[-4.1858 -1.1502 -0.1257  0.1182  0.0767]]\n",
      "MSE loss: 178.1074\n",
      "Iteration: 232500\n",
      "Gradient: [[-15.4711  -6.7428  18.0281  12.5987 162.3694]]\n",
      "Weights: [[-4.187  -1.1502 -0.1257  0.1182  0.0768]]\n",
      "MSE loss: 178.0985\n",
      "Iteration: 232600\n",
      "Gradient: [[ -2.8463   3.721   -0.2244  73.5993 173.119 ]]\n",
      "Weights: [[-4.1871 -1.1501 -0.1258  0.1182  0.0768]]\n",
      "MSE loss: 178.0831\n",
      "Iteration: 232700\n",
      "Gradient: [[ -3.7448  -0.8102  -2.3671 -75.8969 144.569 ]]\n",
      "Weights: [[-4.1869 -1.15   -0.1259  0.1182  0.0768]]\n",
      "MSE loss: 178.07\n",
      "Iteration: 232800\n",
      "Gradient: [[  4.8773  11.1526  30.2537 -28.9683 -38.378 ]]\n",
      "Weights: [[-4.1878 -1.1497 -0.126   0.1182  0.0768]]\n",
      "MSE loss: 178.0525\n",
      "Iteration: 232900\n",
      "Gradient: [[   4.499   -16.6257   -1.6342  -62.1898 -244.1339]]\n",
      "Weights: [[-4.1883 -1.1493 -0.126   0.1182  0.0768]]\n",
      "MSE loss: 178.0335\n",
      "Iteration: 233000\n",
      "Gradient: [[  -4.169     2.475   -24.1049   12.0765 -253.4224]]\n",
      "Weights: [[-4.1891 -1.1492 -0.1261  0.1182  0.0768]]\n",
      "MSE loss: 178.0208\n",
      "Iteration: 233100\n",
      "Gradient: [[ 1.6168 -8.5475 26.4392 85.6416 44.0231]]\n",
      "Weights: [[-4.1881 -1.1488 -0.1262  0.1182  0.0768]]\n",
      "MSE loss: 177.9976\n",
      "Iteration: 233200\n",
      "Gradient: [[  1.9263   3.755  -20.9861  44.8168 161.3089]]\n",
      "Weights: [[-4.1872 -1.1489 -0.1263  0.1182  0.0768]]\n",
      "MSE loss: 177.9877\n",
      "Iteration: 233300\n",
      "Gradient: [[ -4.8334 -14.8326 -31.8838 -33.2076 -48.538 ]]\n",
      "Weights: [[-4.1882 -1.1486 -0.1264  0.1182  0.0768]]\n",
      "MSE loss: 177.9689\n",
      "Iteration: 233400\n",
      "Gradient: [[ -2.8491  -2.905   31.7527 113.3313 -47.4216]]\n",
      "Weights: [[-4.187  -1.1485 -0.1265  0.1182  0.0768]]\n",
      "MSE loss: 177.9534\n",
      "Iteration: 233500\n",
      "Gradient: [[-12.7319  20.0721   8.4966  14.6874 297.976 ]]\n",
      "Weights: [[-4.1876 -1.1482 -0.1265  0.1182  0.0768]]\n",
      "MSE loss: 177.9354\n",
      "Iteration: 233600\n",
      "Gradient: [[  -4.0077  -20.7191    8.8542    7.8342 -120.2031]]\n",
      "Weights: [[-4.1881 -1.148  -0.1266  0.1181  0.0768]]\n",
      "MSE loss: 177.9223\n",
      "Iteration: 233700\n",
      "Gradient: [[  -1.3184  -17.0547    3.2456 -128.2127  285.2158]]\n",
      "Weights: [[-4.1881 -1.1479 -0.1267  0.1181  0.0768]]\n",
      "MSE loss: 177.9079\n",
      "Iteration: 233800\n",
      "Gradient: [[ -9.6354  -3.5592   8.151  -17.9174 156.5988]]\n",
      "Weights: [[-4.1881 -1.1477 -0.1268  0.1181  0.0768]]\n",
      "MSE loss: 177.8962\n",
      "Iteration: 233900\n",
      "Gradient: [[  -9.727   -14.9215   -2.5855  -66.3729 -130.1448]]\n",
      "Weights: [[-4.1891 -1.1476 -0.1268  0.1181  0.0768]]\n",
      "MSE loss: 177.8861\n",
      "Iteration: 234000\n",
      "Gradient: [[  -3.537     1.107    -2.9452   63.976  -124.729 ]]\n",
      "Weights: [[-4.189  -1.1474 -0.1269  0.1181  0.0769]]\n",
      "MSE loss: 177.8702\n",
      "Iteration: 234100\n",
      "Gradient: [[ 0.9746 16.633  15.5797 99.8014 75.8943]]\n",
      "Weights: [[-4.1887 -1.1473 -0.1269  0.1181  0.0769]]\n",
      "MSE loss: 177.859\n",
      "Iteration: 234200\n",
      "Gradient: [[  -6.3805  -10.994   -59.6534  132.7416 -222.8144]]\n",
      "Weights: [[-4.1898 -1.1471 -0.1269  0.1181  0.0769]]\n",
      "MSE loss: 177.8445\n",
      "Iteration: 234300\n",
      "Gradient: [[ -1.2996  14.4872 -25.8432 -87.6996  80.6353]]\n",
      "Weights: [[-4.1898 -1.1467 -0.127   0.1181  0.0769]]\n",
      "MSE loss: 177.8298\n",
      "Iteration: 234400\n",
      "Gradient: [[ -4.1911  -4.8298  16.4355 -30.2417 -46.8164]]\n",
      "Weights: [[-4.1896 -1.1465 -0.1271  0.1181  0.0769]]\n",
      "MSE loss: 177.8136\n",
      "Iteration: 234500\n",
      "Gradient: [[  1.203   -5.4345 -30.0269  91.752  -86.9956]]\n",
      "Weights: [[-4.1907 -1.1463 -0.1272  0.1181  0.0769]]\n",
      "MSE loss: 177.7962\n",
      "Iteration: 234600\n",
      "Gradient: [[-3.3917 11.591  37.1459 40.4024 -1.0988]]\n",
      "Weights: [[-4.1899 -1.1462 -0.1272  0.1181  0.0769]]\n",
      "MSE loss: 177.779\n",
      "Iteration: 234700\n",
      "Gradient: [[   5.6368  -17.779    56.9447   60.1372 -287.4636]]\n",
      "Weights: [[-4.1898 -1.1459 -0.1273  0.1181  0.0769]]\n",
      "MSE loss: 177.7614\n",
      "Iteration: 234800\n",
      "Gradient: [[  -7.6149  -13.5092   75.129    29.0773 -281.962 ]]\n",
      "Weights: [[-4.1899 -1.1457 -0.1274  0.118   0.0769]]\n",
      "MSE loss: 177.7446\n",
      "Iteration: 234900\n",
      "Gradient: [[   9.8511  -12.2249   -0.5975 -184.1325 -128.1586]]\n",
      "Weights: [[-4.1887 -1.1456 -0.1276  0.118   0.0769]]\n",
      "MSE loss: 177.7284\n",
      "Iteration: 235000\n",
      "Gradient: [[ -3.0532 -17.875   63.186   81.8374 210.3266]]\n",
      "Weights: [[-4.1891 -1.1453 -0.1276  0.118   0.0769]]\n",
      "MSE loss: 177.7093\n",
      "Iteration: 235100\n",
      "Gradient: [[-1.0426  0.5067 27.1923  2.2496 60.2745]]\n",
      "Weights: [[-4.1892 -1.1453 -0.1277  0.118   0.0769]]\n",
      "MSE loss: 177.6945\n",
      "Iteration: 235200\n",
      "Gradient: [[  2.2084   0.4428  30.2038  -7.4487 318.9044]]\n",
      "Weights: [[-4.1886 -1.1454 -0.1278  0.118   0.0769]]\n",
      "MSE loss: 177.6868\n",
      "Iteration: 235300\n",
      "Gradient: [[   3.6881  -11.3628  -16.0839   96.9169 -352.8928]]\n",
      "Weights: [[-4.1903 -1.1452 -0.1279  0.118   0.077 ]]\n",
      "MSE loss: 177.6687\n",
      "Iteration: 235400\n",
      "Gradient: [[  -4.201    -9.5896  -47.1752   54.5606 -224.0082]]\n",
      "Weights: [[-4.191  -1.1446 -0.128   0.118   0.077 ]]\n",
      "MSE loss: 177.6491\n",
      "Iteration: 235500\n",
      "Gradient: [[ 22.3849 -12.3604   6.7803  19.8555 -51.7502]]\n",
      "Weights: [[-4.1916 -1.1443 -0.128   0.118   0.077 ]]\n",
      "MSE loss: 177.6327\n",
      "Iteration: 235600\n",
      "Gradient: [[ -8.6439  23.2534  -7.8363  -0.6027 -12.1688]]\n",
      "Weights: [[-4.1924 -1.1441 -0.128   0.118   0.077 ]]\n",
      "MSE loss: 177.6229\n",
      "Iteration: 235700\n",
      "Gradient: [[   8.2298  -11.1098   10.9382   70.5307 -245.4268]]\n",
      "Weights: [[-4.1918 -1.1438 -0.1281  0.118   0.077 ]]\n",
      "MSE loss: 177.6039\n",
      "Iteration: 235800\n",
      "Gradient: [[   5.805    19.8245  -27.0576   52.0019 -148.5197]]\n",
      "Weights: [[-4.1913 -1.1435 -0.1282  0.118   0.077 ]]\n",
      "MSE loss: 177.5839\n",
      "Iteration: 235900\n",
      "Gradient: [[  10.9689   11.7542   -0.9442  101.9786 -298.6615]]\n",
      "Weights: [[-4.1925 -1.1433 -0.1282  0.118   0.077 ]]\n",
      "MSE loss: 177.575\n",
      "Iteration: 236000\n",
      "Gradient: [[  5.2981  -2.1683 -57.7227 -85.3757 -38.3643]]\n",
      "Weights: [[-4.192  -1.1432 -0.1283  0.118   0.077 ]]\n",
      "MSE loss: 177.5572\n",
      "Iteration: 236100\n",
      "Gradient: [[ -12.5115  -11.4074  -10.9769   59.6133 -150.7549]]\n",
      "Weights: [[-4.193  -1.1432 -0.1284  0.1179  0.077 ]]\n",
      "MSE loss: 177.5479\n",
      "Iteration: 236200\n",
      "Gradient: [[   3.8819  -20.2443  -14.1446  -52.168  -398.6785]]\n",
      "Weights: [[-4.1914 -1.1429 -0.1285  0.1179  0.077 ]]\n",
      "MSE loss: 177.5266\n",
      "Iteration: 236300\n",
      "Gradient: [[ 14.1934  -6.565   -0.2211 112.3934 122.1121]]\n",
      "Weights: [[-4.1913 -1.1428 -0.1286  0.1179  0.077 ]]\n",
      "MSE loss: 177.5072\n",
      "Iteration: 236400\n",
      "Gradient: [[ -7.115  -20.8416  21.0963 -32.1702  77.2114]]\n",
      "Weights: [[-4.1904 -1.1428 -0.1287  0.1179  0.077 ]]\n",
      "MSE loss: 177.4959\n",
      "Iteration: 236500\n",
      "Gradient: [[  4.8181 -19.4073   5.5784  77.0103  25.4423]]\n",
      "Weights: [[-4.1905 -1.1428 -0.1288  0.1179  0.077 ]]\n",
      "MSE loss: 177.4835\n",
      "Iteration: 236600\n",
      "Gradient: [[ -11.3406    2.415    27.1263  -46.802  -200.0764]]\n",
      "Weights: [[-4.1917 -1.1425 -0.1289  0.1179  0.0771]]\n",
      "MSE loss: 177.4638\n",
      "Iteration: 236700\n",
      "Gradient: [[ -12.3684    2.8191   12.2649  -95.6548 -306.2373]]\n",
      "Weights: [[-4.1912 -1.1423 -0.1289  0.1179  0.0771]]\n",
      "MSE loss: 177.4522\n",
      "Iteration: 236800\n",
      "Gradient: [[  -2.6873   -8.3969   39.1294   99.8794 -415.9071]]\n",
      "Weights: [[-4.1918 -1.1422 -0.1291  0.1179  0.0771]]\n",
      "MSE loss: 177.4324\n",
      "Iteration: 236900\n",
      "Gradient: [[ -12.0979    1.3936    7.164   109.1938 -265.0367]]\n",
      "Weights: [[-4.1915 -1.142  -0.1292  0.1179  0.0771]]\n",
      "MSE loss: 177.4152\n",
      "Iteration: 237000\n",
      "Gradient: [[  4.6166 -21.1513 -31.6803  46.6596 -88.5027]]\n",
      "Weights: [[-4.1916 -1.1419 -0.1293  0.1179  0.0771]]\n",
      "MSE loss: 177.4004\n",
      "Iteration: 237100\n",
      "Gradient: [[  -5.0069   16.8419   43.9458  126.6405 -208.8809]]\n",
      "Weights: [[-4.192  -1.1417 -0.1294  0.1179  0.0771]]\n",
      "MSE loss: 177.3817\n",
      "Iteration: 237200\n",
      "Gradient: [[-4.2977 13.1854 -0.2775 11.0097 90.8636]]\n",
      "Weights: [[-4.1917 -1.1414 -0.1295  0.1179  0.0771]]\n",
      "MSE loss: 177.3622\n",
      "Iteration: 237300\n",
      "Gradient: [[ -1.3794  -1.9193  37.167  134.0166 189.2636]]\n",
      "Weights: [[-4.1919 -1.1411 -0.1296  0.1179  0.0771]]\n",
      "MSE loss: 177.3388\n",
      "Iteration: 237400\n",
      "Gradient: [[ 5.745500e+00  1.507000e-01  4.727390e+01  2.043960e+01 -2.106615e+02]]\n",
      "Weights: [[-4.1914 -1.1411 -0.1297  0.1178  0.0771]]\n",
      "MSE loss: 177.3307\n",
      "Iteration: 237500\n",
      "Gradient: [[  2.7284  -2.2713 -38.3283  43.0775 -30.4344]]\n",
      "Weights: [[-4.1909 -1.1409 -0.1298  0.1178  0.0771]]\n",
      "MSE loss: 177.3159\n",
      "Iteration: 237600\n",
      "Gradient: [[   0.6473  -10.2463  -30.39     24.5927 -100.9867]]\n",
      "Weights: [[-4.1913 -1.1406 -0.1299  0.1178  0.0771]]\n",
      "MSE loss: 177.2958\n",
      "Iteration: 237700\n",
      "Gradient: [[  6.7136   2.9737  58.7946 -90.0594  30.1839]]\n",
      "Weights: [[-4.1928 -1.1403 -0.13    0.1178  0.0771]]\n",
      "MSE loss: 177.2735\n",
      "Iteration: 237800\n",
      "Gradient: [[  7.6582   7.0819  34.658   20.1101 335.1224]]\n",
      "Weights: [[-4.1934 -1.14   -0.1301  0.1178  0.0771]]\n",
      "MSE loss: 177.2603\n",
      "Iteration: 237900\n",
      "Gradient: [[ -11.543    -8.1012   44.0756   30.9183 -418.1367]]\n",
      "Weights: [[-4.1937 -1.1398 -0.1301  0.1178  0.0771]]\n",
      "MSE loss: 177.248\n",
      "Iteration: 238000\n",
      "Gradient: [[-7.272900e+00 -1.182500e+00 -3.050550e+01  9.840000e-02 -4.590145e+02]]\n",
      "Weights: [[-4.1938 -1.1394 -0.1302  0.1178  0.0771]]\n",
      "MSE loss: 177.2261\n",
      "Iteration: 238100\n",
      "Gradient: [[ 2.1865 11.1606 25.906  82.9276  5.5569]]\n",
      "Weights: [[-4.1946 -1.139  -0.1304  0.1178  0.0772]]\n",
      "MSE loss: 177.204\n",
      "Iteration: 238200\n",
      "Gradient: [[ 6.958000e-01  1.119000e-01  1.116440e+01 -3.201360e+01 -1.169268e+02]]\n",
      "Weights: [[-4.1936 -1.1389 -0.1305  0.1178  0.0772]]\n",
      "MSE loss: 177.1855\n",
      "Iteration: 238300\n",
      "Gradient: [[ -4.2645  -2.7885  -5.6648  -8.4428 -42.0772]]\n",
      "Weights: [[-4.1932 -1.1386 -0.1306  0.1178  0.0772]]\n",
      "MSE loss: 177.1677\n",
      "Iteration: 238400\n",
      "Gradient: [[ -6.5555  -9.1927  27.0434 -22.6305 201.4746]]\n",
      "Weights: [[-4.194  -1.1385 -0.1307  0.1178  0.0772]]\n",
      "MSE loss: 177.1464\n",
      "Iteration: 238500\n",
      "Gradient: [[ 14.7673   6.9727  -8.9002 121.6983   8.0338]]\n",
      "Weights: [[-4.1935 -1.1383 -0.1308  0.1178  0.0772]]\n",
      "MSE loss: 177.1305\n",
      "Iteration: 238600\n",
      "Gradient: [[  -1.1495  -10.4357   10.5919  121.0814 -133.9069]]\n",
      "Weights: [[-4.193  -1.1381 -0.1309  0.1178  0.0772]]\n",
      "MSE loss: 177.1116\n",
      "Iteration: 238700\n",
      "Gradient: [[ -13.6704    4.5065    1.7839  -22.6335 -240.755 ]]\n",
      "Weights: [[-4.1941 -1.138  -0.131   0.1177  0.0772]]\n",
      "MSE loss: 177.1015\n",
      "Iteration: 238800\n",
      "Gradient: [[ -6.1333   0.4904  19.2001 -53.0611 338.4443]]\n",
      "Weights: [[-4.1943 -1.1377 -0.131   0.1177  0.0772]]\n",
      "MSE loss: 177.0884\n",
      "Iteration: 238900\n",
      "Gradient: [[  -5.3779   -6.6609   61.318   -10.0999 -199.1025]]\n",
      "Weights: [[-4.1941 -1.1376 -0.1311  0.1177  0.0772]]\n",
      "MSE loss: 177.075\n",
      "Iteration: 239000\n",
      "Gradient: [[  -2.2792   -7.7009   59.5476 -110.2111 -111.2024]]\n",
      "Weights: [[-4.1942 -1.1374 -0.1312  0.1177  0.0772]]\n",
      "MSE loss: 177.0542\n",
      "Iteration: 239100\n",
      "Gradient: [[  -9.0715    9.9953   48.6166   91.0623 -395.2534]]\n",
      "Weights: [[-4.1917 -1.1374 -0.1313  0.1177  0.0772]]\n",
      "MSE loss: 177.047\n",
      "Iteration: 239200\n",
      "Gradient: [[ -4.6008   2.0094 -20.6485 -57.5181  17.188 ]]\n",
      "Weights: [[-4.1926 -1.1374 -0.1313  0.1177  0.0772]]\n",
      "MSE loss: 177.0313\n",
      "Iteration: 239300\n",
      "Gradient: [[ 9.825  10.8265 -0.7773  5.9672  1.7289]]\n",
      "Weights: [[-4.193  -1.1373 -0.1314  0.1177  0.0772]]\n",
      "MSE loss: 177.0138\n",
      "Iteration: 239400\n",
      "Gradient: [[  2.5466 -13.3532  -9.1156 -46.3527 101.1786]]\n",
      "Weights: [[-4.1937 -1.1371 -0.1315  0.1177  0.0773]]\n",
      "MSE loss: 176.9946\n",
      "Iteration: 239500\n",
      "Gradient: [[   9.6119   20.9584  -27.0717 -139.8085  -81.4818]]\n",
      "Weights: [[-4.195  -1.1368 -0.1316  0.1177  0.0773]]\n",
      "MSE loss: 176.9819\n",
      "Iteration: 239600\n",
      "Gradient: [[ -2.2726   1.5167   2.3759  -2.0062 248.4213]]\n",
      "Weights: [[-4.1942 -1.1363 -0.1317  0.1177  0.0773]]\n",
      "MSE loss: 176.958\n",
      "Iteration: 239700\n",
      "Gradient: [[   8.2659   -9.4614  -25.5671 -120.9696  -51.0357]]\n",
      "Weights: [[-4.1946 -1.1362 -0.1318  0.1177  0.0773]]\n",
      "MSE loss: 176.9459\n",
      "Iteration: 239800\n",
      "Gradient: [[  1.0531   0.545   52.0323 -62.2859  47.5408]]\n",
      "Weights: [[-4.1932 -1.1362 -0.1319  0.1177  0.0773]]\n",
      "MSE loss: 176.9337\n",
      "Iteration: 239900\n",
      "Gradient: [[-3.1783  5.3041  6.3065  9.0712 46.2786]]\n",
      "Weights: [[-4.1938 -1.136  -0.132   0.1177  0.0773]]\n",
      "MSE loss: 176.9145\n",
      "Iteration: 240000\n",
      "Gradient: [[ -0.3133  -4.5125  13.6967 127.816  -32.6651]]\n",
      "Weights: [[-4.1948 -1.1359 -0.1321  0.1177  0.0773]]\n",
      "MSE loss: 176.9014\n",
      "Iteration: 240100\n",
      "Gradient: [[   1.6611   15.6548  -21.8004   12.0405 -183.8195]]\n",
      "Weights: [[-4.1932 -1.1358 -0.1322  0.1176  0.0773]]\n",
      "MSE loss: 176.8811\n",
      "Iteration: 240200\n",
      "Gradient: [[ -2.8242 -10.3022   9.017   60.7301 -38.3553]]\n",
      "Weights: [[-4.1933 -1.1356 -0.1323  0.1176  0.0773]]\n",
      "MSE loss: 176.8612\n",
      "Iteration: 240300\n",
      "Gradient: [[   3.4678  -12.6323   12.8921 -175.8421 -481.3508]]\n",
      "Weights: [[-4.193  -1.1355 -0.1324  0.1176  0.0773]]\n",
      "MSE loss: 176.847\n",
      "Iteration: 240400\n",
      "Gradient: [[-13.0452  17.949   41.8103 -73.3612 -31.855 ]]\n",
      "Weights: [[-4.1944 -1.1355 -0.1325  0.1176  0.0773]]\n",
      "MSE loss: 176.8388\n",
      "Iteration: 240500\n",
      "Gradient: [[  2.5883   9.8294  16.0607 117.8646 257.7062]]\n",
      "Weights: [[-4.1937 -1.1355 -0.1326  0.1176  0.0773]]\n",
      "MSE loss: 176.822\n",
      "Iteration: 240600\n",
      "Gradient: [[ -0.5392 -13.065   75.6448  29.6469 -84.8052]]\n",
      "Weights: [[-4.1938 -1.1355 -0.1327  0.1176  0.0773]]\n",
      "MSE loss: 176.808\n",
      "Iteration: 240700\n",
      "Gradient: [[  -7.7737   -8.8066    3.178    57.5932 -220.7681]]\n",
      "Weights: [[-4.1942 -1.1354 -0.1328  0.1176  0.0773]]\n",
      "MSE loss: 176.7934\n",
      "Iteration: 240800\n",
      "Gradient: [[   9.4463   -1.4006   27.0468 -115.7101  -99.9006]]\n",
      "Weights: [[-4.1947 -1.1353 -0.1329  0.1176  0.0773]]\n",
      "MSE loss: 176.7833\n",
      "Iteration: 240900\n",
      "Gradient: [[  5.4071  -0.6326 -46.7589  98.45   285.5146]]\n",
      "Weights: [[-4.1942 -1.1351 -0.1329  0.1176  0.0774]]\n",
      "MSE loss: 176.7667\n",
      "Iteration: 241000\n",
      "Gradient: [[ -13.3214   -4.7926    3.6246   13.4498 -375.6242]]\n",
      "Weights: [[-4.1943 -1.1347 -0.133   0.1176  0.0774]]\n",
      "MSE loss: 176.7479\n",
      "Iteration: 241100\n",
      "Gradient: [[ -10.2351   -4.1118   -8.3793    2.5667 -150.84  ]]\n",
      "Weights: [[-4.1944 -1.1345 -0.1331  0.1176  0.0774]]\n",
      "MSE loss: 176.7349\n",
      "Iteration: 241200\n",
      "Gradient: [[   0.356    12.7664  -12.7877   56.0414 -288.121 ]]\n",
      "Weights: [[-4.195  -1.1343 -0.1332  0.1176  0.0774]]\n",
      "MSE loss: 176.7191\n",
      "Iteration: 241300\n",
      "Gradient: [[   4.0098  -16.0133   27.8349  -27.9312 -119.3746]]\n",
      "Weights: [[-4.1941 -1.134  -0.1333  0.1176  0.0774]]\n",
      "MSE loss: 176.6958\n",
      "Iteration: 241400\n",
      "Gradient: [[  -3.8031   -6.5805   -4.5974   19.4001 -210.2063]]\n",
      "Weights: [[-4.1951 -1.1337 -0.1334  0.1176  0.0774]]\n",
      "MSE loss: 176.6826\n",
      "Iteration: 241500\n",
      "Gradient: [[  0.7954  -8.9864  55.8291  62.6638 343.5486]]\n",
      "Weights: [[-4.1945 -1.1334 -0.1335  0.1176  0.0774]]\n",
      "MSE loss: 176.6649\n",
      "Iteration: 241600\n",
      "Gradient: [[  -7.9786    6.3666    9.8314   16.3826 -133.5575]]\n",
      "Weights: [[-4.1943 -1.133  -0.1336  0.1176  0.0774]]\n",
      "MSE loss: 176.6472\n",
      "Iteration: 241700\n",
      "Gradient: [[   8.5381   13.9986   14.6756  -26.5664 -356.0433]]\n",
      "Weights: [[-4.194  -1.1327 -0.1337  0.1175  0.0774]]\n",
      "MSE loss: 176.6296\n",
      "Iteration: 241800\n",
      "Gradient: [[ -6.6899   2.7193  53.4348  48.5626 100.0032]]\n",
      "Weights: [[-4.1938 -1.1326 -0.1338  0.1175  0.0774]]\n",
      "MSE loss: 176.6126\n",
      "Iteration: 241900\n",
      "Gradient: [[-11.5644  15.1617  13.0261 -11.5809 -13.2205]]\n",
      "Weights: [[-4.1949 -1.1325 -0.1339  0.1175  0.0774]]\n",
      "MSE loss: 176.5974\n",
      "Iteration: 242000\n",
      "Gradient: [[ -1.2445  -0.7952  14.0694  82.7179 103.1935]]\n",
      "Weights: [[-4.1944 -1.1324 -0.134   0.1175  0.0774]]\n",
      "MSE loss: 176.5788\n",
      "Iteration: 242100\n",
      "Gradient: [[  2.3411  13.2235  -4.9952  28.58   169.8052]]\n",
      "Weights: [[-4.194  -1.1323 -0.1341  0.1175  0.0774]]\n",
      "MSE loss: 176.5653\n",
      "Iteration: 242200\n",
      "Gradient: [[ 20.5536  -9.0536  29.3711 139.6439 283.7115]]\n",
      "Weights: [[-4.1943 -1.1323 -0.1342  0.1175  0.0774]]\n",
      "MSE loss: 176.5498\n",
      "Iteration: 242300\n",
      "Gradient: [[-5.100000e-02  2.367100e+00 -7.778000e+00 -9.819200e+00  1.010661e+02]]\n",
      "Weights: [[-4.1958 -1.1323 -0.1343  0.1175  0.0774]]\n",
      "MSE loss: 176.5397\n",
      "Iteration: 242400\n",
      "Gradient: [[  7.4845   6.1709  56.9254  16.6418 -20.5401]]\n",
      "Weights: [[-4.1954 -1.1319 -0.1343  0.1175  0.0775]]\n",
      "MSE loss: 176.5188\n",
      "Iteration: 242500\n",
      "Gradient: [[ -5.8274   8.9398  14.3988  -9.5328 199.7275]]\n",
      "Weights: [[-4.1963 -1.1316 -0.1344  0.1175  0.0775]]\n",
      "MSE loss: 176.5025\n",
      "Iteration: 242600\n",
      "Gradient: [[  7.4459 -32.7259  -7.3994  10.93    73.319 ]]\n",
      "Weights: [[-4.1951 -1.1315 -0.1345  0.1175  0.0775]]\n",
      "MSE loss: 176.4826\n",
      "Iteration: 242700\n",
      "Gradient: [[ -6.3494   5.7601  -8.4002  30.2685 250.3914]]\n",
      "Weights: [[-4.1949 -1.1313 -0.1345  0.1175  0.0775]]\n",
      "MSE loss: 176.4751\n",
      "Iteration: 242800\n",
      "Gradient: [[  -4.8989   18.4781   16.5274    4.32   -119.2466]]\n",
      "Weights: [[-4.195  -1.1312 -0.1346  0.1175  0.0775]]\n",
      "MSE loss: 176.462\n",
      "Iteration: 242900\n",
      "Gradient: [[  12.4058  -10.5795   12.2484   -3.9412 -288.2492]]\n",
      "Weights: [[-4.1957 -1.131  -0.1347  0.1175  0.0775]]\n",
      "MSE loss: 176.4459\n",
      "Iteration: 243000\n",
      "Gradient: [[  17.0428  -11.6787   49.2721  -50.1463 -191.6653]]\n",
      "Weights: [[-4.1961 -1.1309 -0.1348  0.1174  0.0775]]\n",
      "MSE loss: 176.4325\n",
      "Iteration: 243100\n",
      "Gradient: [[  -5.6369  -10.7987   18.5915   20.5502 -166.4042]]\n",
      "Weights: [[-4.1965 -1.131  -0.1349  0.1174  0.0775]]\n",
      "MSE loss: 176.4198\n",
      "Iteration: 243200\n",
      "Gradient: [[ 0.6272 14.1558 22.4114 -1.0139 19.1211]]\n",
      "Weights: [[-4.1973 -1.1304 -0.1349  0.1174  0.0775]]\n",
      "MSE loss: 176.4006\n",
      "Iteration: 243300\n",
      "Gradient: [[  -8.7724  -11.2393   30.9758  -45.3677 -265.933 ]]\n",
      "Weights: [[-4.197  -1.1302 -0.135   0.1174  0.0775]]\n",
      "MSE loss: 176.3826\n",
      "Iteration: 243400\n",
      "Gradient: [[   4.6699   -8.0516   45.9586   37.9195 -224.9644]]\n",
      "Weights: [[-4.1962 -1.1301 -0.1351  0.1174  0.0775]]\n",
      "MSE loss: 176.3701\n",
      "Iteration: 243500\n",
      "Gradient: [[  5.196   10.6847  -8.8296 -25.5193  33.869 ]]\n",
      "Weights: [[-4.1957 -1.1302 -0.1352  0.1174  0.0775]]\n",
      "MSE loss: 176.3563\n",
      "Iteration: 243600\n",
      "Gradient: [[ 11.3435   7.2266   2.8544   1.6513 349.8539]]\n",
      "Weights: [[-4.1966 -1.1301 -0.1353  0.1174  0.0775]]\n",
      "MSE loss: 176.3409\n",
      "Iteration: 243700\n",
      "Gradient: [[   7.916   -15.004    25.3213  -28.2735 -242.4589]]\n",
      "Weights: [[-4.1965 -1.1299 -0.1353  0.1174  0.0776]]\n",
      "MSE loss: 176.3238\n",
      "Iteration: 243800\n",
      "Gradient: [[-10.4029  -3.2295  51.483   -6.699  -15.9886]]\n",
      "Weights: [[-4.1966 -1.1297 -0.1355  0.1174  0.0776]]\n",
      "MSE loss: 176.3058\n",
      "Iteration: 243900\n",
      "Gradient: [[-0.69   -4.1216 24.243  54.0351 22.2976]]\n",
      "Weights: [[-4.1968 -1.1294 -0.1355  0.1174  0.0776]]\n",
      "MSE loss: 176.2874\n",
      "Iteration: 244000\n",
      "Gradient: [[15.1776 14.1467  7.495  40.2629 25.5239]]\n",
      "Weights: [[-4.1964 -1.1293 -0.1356  0.1174  0.0776]]\n",
      "MSE loss: 176.2728\n",
      "Iteration: 244100\n",
      "Gradient: [[ -15.6508    5.4385   28.0794   36.5344 -213.4006]]\n",
      "Weights: [[-4.1976 -1.1291 -0.1357  0.1174  0.0776]]\n",
      "MSE loss: 176.2587\n",
      "Iteration: 244200\n",
      "Gradient: [[ 16.4537   0.7208  41.277   23.8463 262.966 ]]\n",
      "Weights: [[-4.1969 -1.1287 -0.1358  0.1173  0.0776]]\n",
      "MSE loss: 176.2364\n",
      "Iteration: 244300\n",
      "Gradient: [[   3.9782  -16.6434    4.2529   -3.1545 -160.4519]]\n",
      "Weights: [[-4.1984 -1.1286 -0.1358  0.1173  0.0776]]\n",
      "MSE loss: 176.2271\n",
      "Iteration: 244400\n",
      "Gradient: [[  12.8026    4.7317   -4.2765   56.1927 -188.1163]]\n",
      "Weights: [[-4.1979 -1.1285 -0.1359  0.1173  0.0776]]\n",
      "MSE loss: 176.2116\n",
      "Iteration: 244500\n",
      "Gradient: [[ -10.2591   10.0765  -70.8529   61.8295 -381.8149]]\n",
      "Weights: [[-4.1965 -1.1284 -0.136   0.1173  0.0776]]\n",
      "MSE loss: 176.1913\n",
      "Iteration: 244600\n",
      "Gradient: [[-1.5602  6.7543  4.5398 -8.5053 28.6221]]\n",
      "Weights: [[-4.1962 -1.1284 -0.1361  0.1173  0.0776]]\n",
      "MSE loss: 176.1812\n",
      "Iteration: 244700\n",
      "Gradient: [[ -12.0942  -17.9      -1.3463  -18.3345 -261.9608]]\n",
      "Weights: [[-4.195  -1.1284 -0.1362  0.1173  0.0776]]\n",
      "MSE loss: 176.1737\n",
      "Iteration: 244800\n",
      "Gradient: [[ -3.6655 -16.5558 -64.1552 -11.5067 113.5071]]\n",
      "Weights: [[-4.1951 -1.1281 -0.1363  0.1173  0.0776]]\n",
      "MSE loss: 176.1553\n",
      "Iteration: 244900\n",
      "Gradient: [[-14.6782 -16.9096 -10.3297  69.427  185.1443]]\n",
      "Weights: [[-4.1968 -1.1277 -0.1364  0.1173  0.0776]]\n",
      "MSE loss: 176.1351\n",
      "Iteration: 245000\n",
      "Gradient: [[ 1.981000e-01 -1.773000e-01 -1.048220e+01 -9.267910e+01 -4.396394e+02]]\n",
      "Weights: [[-4.1956 -1.1275 -0.1365  0.1173  0.0776]]\n",
      "MSE loss: 176.1189\n",
      "Iteration: 245100\n",
      "Gradient: [[  -0.7743   -9.8434   15.2667  113.2302 -388.7832]]\n",
      "Weights: [[-4.1967 -1.1274 -0.1365  0.1173  0.0776]]\n",
      "MSE loss: 176.1036\n",
      "Iteration: 245200\n",
      "Gradient: [[ -6.5469 -16.0234  26.8506  16.2604 -82.8051]]\n",
      "Weights: [[-4.1966 -1.1275 -0.1366  0.1173  0.0777]]\n",
      "MSE loss: 176.0915\n",
      "Iteration: 245300\n",
      "Gradient: [[   4.5073   17.5482    0.671    54.4484 -102.1962]]\n",
      "Weights: [[-4.1956 -1.1273 -0.1368  0.1173  0.0777]]\n",
      "MSE loss: 176.0722\n",
      "Iteration: 245400\n",
      "Gradient: [[  -2.0857  -22.4179    7.6241   51.4721 -250.5681]]\n",
      "Weights: [[-4.196  -1.1273 -0.1369  0.1173  0.0777]]\n",
      "MSE loss: 176.0566\n",
      "Iteration: 245500\n",
      "Gradient: [[  -5.2913  -10.6134    4.5754   55.5336 -151.2635]]\n",
      "Weights: [[-4.1956 -1.1272 -0.137   0.1173  0.0777]]\n",
      "MSE loss: 176.0423\n",
      "Iteration: 245600\n",
      "Gradient: [[ -7.6958  -4.6452  16.5626 -47.4047 345.4698]]\n",
      "Weights: [[-4.1951 -1.1271 -0.1371  0.1173  0.0777]]\n",
      "MSE loss: 176.0286\n",
      "Iteration: 245700\n",
      "Gradient: [[  -5.1588   -4.7486  -11.9407 -143.4688  155.8466]]\n",
      "Weights: [[-4.1956 -1.1272 -0.1372  0.1172  0.0777]]\n",
      "MSE loss: 176.0124\n",
      "Iteration: 245800\n",
      "Gradient: [[-21.2439  13.3142 -20.4206 116.1588  76.3936]]\n",
      "Weights: [[-4.1954 -1.1269 -0.1373  0.1172  0.0777]]\n",
      "MSE loss: 175.9916\n",
      "Iteration: 245900\n",
      "Gradient: [[ -3.5805  22.0448  44.6734 120.597   22.0611]]\n",
      "Weights: [[-4.1955 -1.1268 -0.1374  0.1172  0.0777]]\n",
      "MSE loss: 175.9763\n",
      "Iteration: 246000\n",
      "Gradient: [[  -5.9754   21.6137   39.9851 -230.0423  209.9263]]\n",
      "Weights: [[-4.196  -1.1266 -0.1374  0.1172  0.0777]]\n",
      "MSE loss: 175.964\n",
      "Iteration: 246100\n",
      "Gradient: [[-18.5134  16.3845  27.077  -63.5777 -78.1713]]\n",
      "Weights: [[-4.1945 -1.1267 -0.1376  0.1172  0.0777]]\n",
      "MSE loss: 175.9511\n",
      "Iteration: 246200\n",
      "Gradient: [[  -1.1833    9.1228   26.8871 -114.9204 -285.8165]]\n",
      "Weights: [[-4.1954 -1.1264 -0.1376  0.1172  0.0777]]\n",
      "MSE loss: 175.9328\n",
      "Iteration: 246300\n",
      "Gradient: [[  -6.9271  -18.2347   39.1165 -152.05   -216.2873]]\n",
      "Weights: [[-4.1963 -1.1261 -0.1377  0.1172  0.0777]]\n",
      "MSE loss: 175.9159\n",
      "Iteration: 246400\n",
      "Gradient: [[  -4.534   -29.9792   -0.2473 -187.1907  153.3753]]\n",
      "Weights: [[-4.196  -1.1258 -0.1378  0.1172  0.0777]]\n",
      "MSE loss: 175.8964\n",
      "Iteration: 246500\n",
      "Gradient: [[ -1.3539   4.9874  -2.8279 -40.2299 -32.0193]]\n",
      "Weights: [[-4.1969 -1.1255 -0.1379  0.1172  0.0778]]\n",
      "MSE loss: 175.8793\n",
      "Iteration: 246600\n",
      "Gradient: [[ -0.6794 -30.6119  19.7894  60.7828  25.1579]]\n",
      "Weights: [[-4.1967 -1.1255 -0.138   0.1172  0.0778]]\n",
      "MSE loss: 175.8693\n",
      "Iteration: 246700\n",
      "Gradient: [[-10.6156  14.7798 -21.6286  -1.2119 561.6096]]\n",
      "Weights: [[-4.1968 -1.1254 -0.138   0.1172  0.0778]]\n",
      "MSE loss: 175.8575\n",
      "Iteration: 246800\n",
      "Gradient: [[  7.0905  -1.3088  -6.5409 -67.8286 -23.7023]]\n",
      "Weights: [[-4.1966 -1.1254 -0.138   0.1172  0.0778]]\n",
      "MSE loss: 175.8544\n",
      "Iteration: 246900\n",
      "Gradient: [[  -4.4446   -4.7928  -33.323    48.4001 -219.4546]]\n",
      "Weights: [[-4.1975 -1.1253 -0.1381  0.1172  0.0778]]\n",
      "MSE loss: 175.8421\n",
      "Iteration: 247000\n",
      "Gradient: [[-13.6235  15.3391 -38.0194 -95.2457  -2.1915]]\n",
      "Weights: [[-4.1973 -1.1252 -0.1382  0.1172  0.0778]]\n",
      "MSE loss: 175.8278\n",
      "Iteration: 247100\n",
      "Gradient: [[  2.2233  13.92     4.6069 -44.4819  18.3502]]\n",
      "Weights: [[-4.1965 -1.1252 -0.1383  0.1172  0.0778]]\n",
      "MSE loss: 175.8157\n",
      "Iteration: 247200\n",
      "Gradient: [[  -4.1972   15.1388    2.2459  110.0523 -257.3631]]\n",
      "Weights: [[-4.1966 -1.1248 -0.1384  0.1172  0.0778]]\n",
      "MSE loss: 175.7992\n",
      "Iteration: 247300\n",
      "Gradient: [[  -4.8251   -3.0393   13.4069   94.6036 -228.2551]]\n",
      "Weights: [[-4.1964 -1.1245 -0.1385  0.1172  0.0778]]\n",
      "MSE loss: 175.776\n",
      "Iteration: 247400\n",
      "Gradient: [[  0.4198   0.1215 -41.5515 -68.098  -55.9055]]\n",
      "Weights: [[-4.1964 -1.1243 -0.1386  0.1172  0.0778]]\n",
      "MSE loss: 175.7602\n",
      "Iteration: 247500\n",
      "Gradient: [[ 11.0059  24.0688 -18.6668  10.742  -49.0735]]\n",
      "Weights: [[-4.1977 -1.1243 -0.1387  0.1172  0.0778]]\n",
      "MSE loss: 175.745\n",
      "Iteration: 247600\n",
      "Gradient: [[ -2.7624 -16.4099  15.8377 -74.3233 109.1942]]\n",
      "Weights: [[-4.1992 -1.124  -0.1388  0.1172  0.0778]]\n",
      "MSE loss: 175.7318\n",
      "Iteration: 247700\n",
      "Gradient: [[  2.3538 -17.1811 -25.9622 -29.8093 -96.5739]]\n",
      "Weights: [[-4.199  -1.1235 -0.1389  0.1172  0.0778]]\n",
      "MSE loss: 175.7051\n",
      "Iteration: 247800\n",
      "Gradient: [[   3.1934   -0.6862    5.709   -81.0027 -169.5221]]\n",
      "Weights: [[-4.199  -1.1233 -0.1389  0.1172  0.0778]]\n",
      "MSE loss: 175.6899\n",
      "Iteration: 247900\n",
      "Gradient: [[  -2.7092    2.5986   -0.4873  -34.7099 -221.512 ]]\n",
      "Weights: [[-4.1993 -1.1231 -0.139   0.1171  0.0779]]\n",
      "MSE loss: 175.6786\n",
      "Iteration: 248000\n",
      "Gradient: [[  9.7263  -1.6892  21.868  134.5822 201.5201]]\n",
      "Weights: [[-4.1992 -1.123  -0.1391  0.1171  0.0779]]\n",
      "MSE loss: 175.6623\n",
      "Iteration: 248100\n",
      "Gradient: [[  12.6356    3.977    13.3747   38.4762 -471.429 ]]\n",
      "Weights: [[-4.1982 -1.1229 -0.1391  0.1171  0.0779]]\n",
      "MSE loss: 175.6455\n",
      "Iteration: 248200\n",
      "Gradient: [[   2.7904   13.4927    1.6238  149.9855 -128.5254]]\n",
      "Weights: [[-4.1977 -1.1229 -0.1392  0.1171  0.0779]]\n",
      "MSE loss: 175.635\n",
      "Iteration: 248300\n",
      "Gradient: [[ 15.1087   8.5569  36.6271 112.5097 -55.0137]]\n",
      "Weights: [[-4.1989 -1.1228 -0.1393  0.1171  0.0779]]\n",
      "MSE loss: 175.6196\n",
      "Iteration: 248400\n",
      "Gradient: [[-14.0555  17.6321   5.3983 -43.5611   6.9313]]\n",
      "Weights: [[-4.198  -1.1227 -0.1394  0.1171  0.0779]]\n",
      "MSE loss: 175.6082\n",
      "Iteration: 248500\n",
      "Gradient: [[  -4.5472   15.743    27.8756  110.1369 -335.006 ]]\n",
      "Weights: [[-4.1976 -1.1226 -0.1395  0.1171  0.0779]]\n",
      "MSE loss: 175.592\n",
      "Iteration: 248600\n",
      "Gradient: [[   1.99     -5.4009   26.2137  -47.8581 -200.0584]]\n",
      "Weights: [[-4.1974 -1.1226 -0.1396  0.1171  0.0779]]\n",
      "MSE loss: 175.5783\n",
      "Iteration: 248700\n",
      "Gradient: [[ -5.9886 -30.6574  70.9376  72.5838 124.7084]]\n",
      "Weights: [[-4.1987 -1.1222 -0.1397  0.1171  0.0779]]\n",
      "MSE loss: 175.5583\n",
      "Iteration: 248800\n",
      "Gradient: [[ -4.4858  13.5104  31.589   12.183  -24.5892]]\n",
      "Weights: [[-4.198  -1.1222 -0.1398  0.1171  0.0779]]\n",
      "MSE loss: 175.5465\n",
      "Iteration: 248900\n",
      "Gradient: [[   1.021    -9.4171   -9.0586 -140.2936   15.9685]]\n",
      "Weights: [[-4.198  -1.1218 -0.1399  0.1171  0.0779]]\n",
      "MSE loss: 175.5266\n",
      "Iteration: 249000\n",
      "Gradient: [[ 6.5942 -5.562  -3.4073 62.6493 80.6383]]\n",
      "Weights: [[-4.1975 -1.1217 -0.14    0.1171  0.0779]]\n",
      "MSE loss: 175.5137\n",
      "Iteration: 249100\n",
      "Gradient: [[-10.8525   5.4065 -13.8896  73.6539  97.1208]]\n",
      "Weights: [[-4.197  -1.1217 -0.1401  0.1171  0.0779]]\n",
      "MSE loss: 175.5004\n",
      "Iteration: 249200\n",
      "Gradient: [[  5.134   18.9507 -14.5241 -14.3095  86.6077]]\n",
      "Weights: [[-4.1972 -1.1215 -0.1401  0.1171  0.0779]]\n",
      "MSE loss: 175.4872\n",
      "Iteration: 249300\n",
      "Gradient: [[  4.1022  17.8678   0.7118  63.0287 -75.5047]]\n",
      "Weights: [[-4.196  -1.1215 -0.1402  0.1171  0.0779]]\n",
      "MSE loss: 175.4849\n",
      "Iteration: 249400\n",
      "Gradient: [[ 1.6823 17.9903  8.9692 17.4418 31.3346]]\n",
      "Weights: [[-4.1969 -1.1214 -0.1403  0.117   0.0779]]\n",
      "MSE loss: 175.4673\n",
      "Iteration: 249500\n",
      "Gradient: [[ -17.1631   16.6082   37.668    41.0258 -115.3717]]\n",
      "Weights: [[-4.198  -1.1213 -0.1403  0.117   0.078 ]]\n",
      "MSE loss: 175.4542\n",
      "Iteration: 249600\n",
      "Gradient: [[  2.465   -3.3741 -14.1106 135.9861 -33.6675]]\n",
      "Weights: [[-4.1979 -1.121  -0.1404  0.117   0.078 ]]\n",
      "MSE loss: 175.4375\n",
      "Iteration: 249700\n",
      "Gradient: [[-3.853300e+00 -4.637700e+00 -7.700000e-03 -1.067047e+02  6.210100e+00]]\n",
      "Weights: [[-4.1984 -1.1207 -0.1405  0.117   0.078 ]]\n",
      "MSE loss: 175.4206\n",
      "Iteration: 249800\n",
      "Gradient: [[ -0.3662  13.1112  46.7612 -24.42    35.5398]]\n",
      "Weights: [[-4.198  -1.1207 -0.1406  0.117   0.078 ]]\n",
      "MSE loss: 175.4029\n",
      "Iteration: 249900\n",
      "Gradient: [[  -0.4123   16.0585   23.7255   39.462  -271.8471]]\n",
      "Weights: [[-4.1974 -1.1207 -0.1407  0.117   0.078 ]]\n",
      "MSE loss: 175.3897\n",
      "Iteration: 250000\n",
      "Gradient: [[  -2.7426   -8.1226   11.6422   63.63   -198.0985]]\n",
      "Weights: [[-4.1967 -1.1206 -0.1408  0.117   0.078 ]]\n",
      "MSE loss: 175.3734\n",
      "Iteration: 250100\n",
      "Gradient: [[   7.5798   14.5943   27.7128 -140.6564  -56.9216]]\n",
      "Weights: [[-4.1964 -1.1204 -0.141   0.117   0.078 ]]\n",
      "MSE loss: 175.3565\n",
      "Iteration: 250200\n",
      "Gradient: [[  -7.7906   15.604    15.3058  -15.2918 -326.4481]]\n",
      "Weights: [[-4.1978 -1.1202 -0.141   0.117   0.078 ]]\n",
      "MSE loss: 175.3369\n",
      "Iteration: 250300\n",
      "Gradient: [[ -1.9774  -0.7776   1.2971 -23.2751  77.0418]]\n",
      "Weights: [[-4.197  -1.1202 -0.1412  0.117   0.078 ]]\n",
      "MSE loss: 175.3184\n",
      "Iteration: 250400\n",
      "Gradient: [[  4.3514 -43.3709  83.6006 -53.4897 557.8072]]\n",
      "Weights: [[-4.1972 -1.12   -0.1412  0.117   0.078 ]]\n",
      "MSE loss: 175.305\n",
      "Iteration: 250500\n",
      "Gradient: [[  4.9075  -2.6244  38.2451 -36.979  111.011 ]]\n",
      "Weights: [[-4.1975 -1.1198 -0.1413  0.1169  0.078 ]]\n",
      "MSE loss: 175.29\n",
      "Iteration: 250600\n",
      "Gradient: [[ 11.8174 -19.137   43.5426 -98.3516 421.9113]]\n",
      "Weights: [[-4.197  -1.1195 -0.1414  0.1169  0.078 ]]\n",
      "MSE loss: 175.2771\n",
      "Iteration: 250700\n",
      "Gradient: [[  9.7327  12.4822 -24.9466  25.1695  47.5696]]\n",
      "Weights: [[-4.1975 -1.1194 -0.1414  0.1169  0.078 ]]\n",
      "MSE loss: 175.2648\n",
      "Iteration: 250800\n",
      "Gradient: [[ -30.1717  -16.4998    2.7342  -39.3099 -129.9709]]\n",
      "Weights: [[-4.1975 -1.1192 -0.1415  0.1169  0.078 ]]\n",
      "MSE loss: 175.2509\n",
      "Iteration: 250900\n",
      "Gradient: [[  -7.3825   -7.0704   34.8011   -6.8064 -137.3626]]\n",
      "Weights: [[-4.1982 -1.1191 -0.1416  0.1169  0.0781]]\n",
      "MSE loss: 175.2383\n",
      "Iteration: 251000\n",
      "Gradient: [[ 11.918  -27.9953   1.5008  61.2248  70.8079]]\n",
      "Weights: [[-4.1979 -1.119  -0.1416  0.1169  0.0781]]\n",
      "MSE loss: 175.227\n",
      "Iteration: 251100\n",
      "Gradient: [[-16.0336   1.5824 -33.6061 -92.7013 -99.284 ]]\n",
      "Weights: [[-4.1983 -1.1186 -0.1417  0.1169  0.0781]]\n",
      "MSE loss: 175.2083\n",
      "Iteration: 251200\n",
      "Gradient: [[  10.6141  -22.9019  -31.5511  -76.6336 -113.1131]]\n",
      "Weights: [[-4.1975 -1.1185 -0.1418  0.1169  0.0781]]\n",
      "MSE loss: 175.1964\n",
      "Iteration: 251300\n",
      "Gradient: [[  10.6678  -16.8691   39.1354 -116.0209 -181.8713]]\n",
      "Weights: [[-4.1981 -1.1186 -0.1418  0.1169  0.0781]]\n",
      "MSE loss: 175.185\n",
      "Iteration: 251400\n",
      "Gradient: [[   7.2942   -6.2425  -15.8172  141.3698 -295.4897]]\n",
      "Weights: [[-4.1978 -1.1185 -0.1419  0.1169  0.0781]]\n",
      "MSE loss: 175.1777\n",
      "Iteration: 251500\n",
      "Gradient: [[  -8.0142   -5.9602  -40.2993    5.4119 -212.5295]]\n",
      "Weights: [[-4.1983 -1.1183 -0.142   0.1169  0.0781]]\n",
      "MSE loss: 175.164\n",
      "Iteration: 251600\n",
      "Gradient: [[  0.9586  -9.5513   0.2702 105.1392 222.7576]]\n",
      "Weights: [[-4.1983 -1.118  -0.1421  0.1168  0.0781]]\n",
      "MSE loss: 175.1466\n",
      "Iteration: 251700\n",
      "Gradient: [[ 1.640000e-02  2.007710e+01  6.013050e+01  7.619360e+01 -1.080652e+02]]\n",
      "Weights: [[-4.1984 -1.1179 -0.1421  0.1168  0.0781]]\n",
      "MSE loss: 175.1304\n",
      "Iteration: 251800\n",
      "Gradient: [[ -1.2613  18.7506 -25.7817 -29.0973 -50.0721]]\n",
      "Weights: [[-4.1986 -1.1177 -0.1422  0.1168  0.0781]]\n",
      "MSE loss: 175.1121\n",
      "Iteration: 251900\n",
      "Gradient: [[  -6.5558  -15.3841  -10.3657  -82.6386 -140.4705]]\n",
      "Weights: [[-4.2001 -1.1175 -0.1423  0.1168  0.0781]]\n",
      "MSE loss: 175.0999\n",
      "Iteration: 252000\n",
      "Gradient: [[ -15.6708    6.0731   31.5491   72.8282 -195.2209]]\n",
      "Weights: [[-4.201  -1.1173 -0.1423  0.1169  0.0781]]\n",
      "MSE loss: 175.0888\n",
      "Iteration: 252100\n",
      "Gradient: [[  3.5569  17.8513  42.1777 114.333  -56.7688]]\n",
      "Weights: [[-4.2007 -1.117  -0.1423  0.1168  0.0781]]\n",
      "MSE loss: 175.0735\n",
      "Iteration: 252200\n",
      "Gradient: [[   7.404   -15.4792  -30.0367   36.6984 -557.0938]]\n",
      "Weights: [[-4.1998 -1.1168 -0.1424  0.1168  0.0781]]\n",
      "MSE loss: 175.0589\n",
      "Iteration: 252300\n",
      "Gradient: [[   7.0933   19.2725  -19.597    40.8463 -302.4862]]\n",
      "Weights: [[-4.1998 -1.1167 -0.1424  0.1168  0.0782]]\n",
      "MSE loss: 175.0456\n",
      "Iteration: 252400\n",
      "Gradient: [[  -0.271    -1.7268    2.3543   -3.7841 -237.3446]]\n",
      "Weights: [[-4.1999 -1.1165 -0.1425  0.1168  0.0782]]\n",
      "MSE loss: 175.0308\n",
      "Iteration: 252500\n",
      "Gradient: [[  -0.8414   -2.0416  -25.509    12.307  -440.9743]]\n",
      "Weights: [[-4.2004 -1.1162 -0.1426  0.1168  0.0782]]\n",
      "MSE loss: 175.0179\n",
      "Iteration: 252600\n",
      "Gradient: [[ -9.2652   5.3617  54.6321 -89.2338 138.9947]]\n",
      "Weights: [[-4.2015 -1.116  -0.1427  0.1168  0.0782]]\n",
      "MSE loss: 175.002\n",
      "Iteration: 252700\n",
      "Gradient: [[  -8.7666    4.8956   14.2272 -131.825  -491.4062]]\n",
      "Weights: [[-4.2013 -1.116  -0.1427  0.1168  0.0782]]\n",
      "MSE loss: 174.9901\n",
      "Iteration: 252800\n",
      "Gradient: [[-22.4208   6.1354  16.5155 -59.5648 163.4223]]\n",
      "Weights: [[-4.2023 -1.1159 -0.1428  0.1168  0.0782]]\n",
      "MSE loss: 174.9772\n",
      "Iteration: 252900\n",
      "Gradient: [[ -5.4817   3.964   43.6209 -27.9448 403.0084]]\n",
      "Weights: [[-4.2011 -1.1155 -0.1429  0.1168  0.0782]]\n",
      "MSE loss: 174.957\n",
      "Iteration: 253000\n",
      "Gradient: [[  2.7755 -28.0229  16.8672 223.7215  50.3498]]\n",
      "Weights: [[-4.2003 -1.1151 -0.143   0.1168  0.0782]]\n",
      "MSE loss: 174.9372\n",
      "Iteration: 253100\n",
      "Gradient: [[ 1.436030e+01 -7.950000e-02  1.355200e+01  1.028827e+02 -1.190110e+02]]\n",
      "Weights: [[-4.2019 -1.115  -0.1432  0.1168  0.0782]]\n",
      "MSE loss: 174.9144\n",
      "Iteration: 253200\n",
      "Gradient: [[ -25.5145  -15.367    18.884   135.3218 -351.0068]]\n",
      "Weights: [[-4.2026 -1.115  -0.1433  0.1168  0.0782]]\n",
      "MSE loss: 174.908\n",
      "Iteration: 253300\n",
      "Gradient: [[-2.3333 -5.0227 -3.6358 23.9517 46.9898]]\n",
      "Weights: [[-4.2016 -1.1147 -0.1433  0.1168  0.0782]]\n",
      "MSE loss: 174.8866\n",
      "Iteration: 253400\n",
      "Gradient: [[  -9.4806   -3.545     7.4924 -138.3958  176.5392]]\n",
      "Weights: [[-4.2019 -1.1144 -0.1434  0.1168  0.0782]]\n",
      "MSE loss: 174.8716\n",
      "Iteration: 253500\n",
      "Gradient: [[  -0.4295   -1.2457  -20.4659  -60.1514 -129.3858]]\n",
      "Weights: [[-4.2012 -1.1142 -0.1434  0.1168  0.0782]]\n",
      "MSE loss: 174.8628\n",
      "Iteration: 253600\n",
      "Gradient: [[ 3.377900e+00 -5.220000e-02  1.001110e+01 -3.307550e+01 -1.754736e+02]]\n",
      "Weights: [[-4.2022 -1.1141 -0.1435  0.1167  0.0782]]\n",
      "MSE loss: 174.8468\n",
      "Iteration: 253700\n",
      "Gradient: [[  4.1489  16.4593   8.7373  17.7384 339.1884]]\n",
      "Weights: [[-4.2032 -1.114  -0.1436  0.1167  0.0782]]\n",
      "MSE loss: 174.8363\n",
      "Iteration: 253800\n",
      "Gradient: [[ -1.7447   0.6253  13.3332  46.9388 -93.4644]]\n",
      "Weights: [[-4.2028 -1.1138 -0.1437  0.1167  0.0783]]\n",
      "MSE loss: 174.8212\n",
      "Iteration: 253900\n",
      "Gradient: [[  4.7783  13.5411  41.037   32.4824 -90.1394]]\n",
      "Weights: [[-4.2018 -1.1137 -0.1438  0.1167  0.0783]]\n",
      "MSE loss: 174.8062\n",
      "Iteration: 254000\n",
      "Gradient: [[  -1.9875   -7.0915   26.5198   12.1716 -489.3679]]\n",
      "Weights: [[-4.2014 -1.1136 -0.1438  0.1167  0.0783]]\n",
      "MSE loss: 174.7939\n",
      "Iteration: 254100\n",
      "Gradient: [[   4.5441  -15.3617   34.5504  107.0899 -224.2852]]\n",
      "Weights: [[-4.2008 -1.1134 -0.144   0.1167  0.0783]]\n",
      "MSE loss: 174.7741\n",
      "Iteration: 254200\n",
      "Gradient: [[ -9.4434  -9.1521  53.5696  56.4725 323.0946]]\n",
      "Weights: [[-4.2021 -1.1134 -0.144   0.1167  0.0783]]\n",
      "MSE loss: 174.7647\n",
      "Iteration: 254300\n",
      "Gradient: [[   2.7317    7.23      0.5855  200.479  -532.2496]]\n",
      "Weights: [[-4.2028 -1.113  -0.1441  0.1167  0.0783]]\n",
      "MSE loss: 174.7498\n",
      "Iteration: 254400\n",
      "Gradient: [[   4.5644   -4.3244  -32.3558   59.8943 -429.5332]]\n",
      "Weights: [[-4.2023 -1.1129 -0.1442  0.1167  0.0783]]\n",
      "MSE loss: 174.7368\n",
      "Iteration: 254500\n",
      "Gradient: [[  3.9726  16.0054 -65.8374   3.4526 107.6167]]\n",
      "Weights: [[-4.2018 -1.1129 -0.1442  0.1166  0.0783]]\n",
      "MSE loss: 174.7255\n",
      "Iteration: 254600\n",
      "Gradient: [[  16.8253  -15.3511  -39.3189  108.9191 -204.206 ]]\n",
      "Weights: [[-4.2009 -1.1129 -0.1443  0.1166  0.0783]]\n",
      "MSE loss: 174.7112\n",
      "Iteration: 254700\n",
      "Gradient: [[  14.0575   -9.0503   12.5525  129.1513 -471.1742]]\n",
      "Weights: [[-4.2017 -1.1125 -0.1444  0.1166  0.0783]]\n",
      "MSE loss: 174.6961\n",
      "Iteration: 254800\n",
      "Gradient: [[   6.4793  -23.8629   18.2911 -104.1978 -158.2365]]\n",
      "Weights: [[-4.2026 -1.1123 -0.1445  0.1166  0.0783]]\n",
      "MSE loss: 174.683\n",
      "Iteration: 254900\n",
      "Gradient: [[ -18.149     1.1805    5.9471 -158.8449  224.4184]]\n",
      "Weights: [[-4.2025 -1.1122 -0.1445  0.1166  0.0783]]\n",
      "MSE loss: 174.6698\n",
      "Iteration: 255000\n",
      "Gradient: [[   6.3871   12.4456  -44.9588   46.6412 -272.968 ]]\n",
      "Weights: [[-4.2018 -1.1122 -0.1447  0.1166  0.0783]]\n",
      "MSE loss: 174.6544\n",
      "Iteration: 255100\n",
      "Gradient: [[-14.8214  -0.9774 -39.4943 -31.3861 411.4806]]\n",
      "Weights: [[-4.2018 -1.1121 -0.1447  0.1166  0.0783]]\n",
      "MSE loss: 174.6386\n",
      "Iteration: 255200\n",
      "Gradient: [[ -1.0725 -15.7984  -3.4929 -17.4288 206.7465]]\n",
      "Weights: [[-4.2011 -1.112  -0.1448  0.1166  0.0783]]\n",
      "MSE loss: 174.6268\n",
      "Iteration: 255300\n",
      "Gradient: [[ 12.2911  -9.4853   2.25     3.9245 -63.5859]]\n",
      "Weights: [[-4.2021 -1.1119 -0.145   0.1166  0.0783]]\n",
      "MSE loss: 174.6114\n",
      "Iteration: 255400\n",
      "Gradient: [[  -1.0674  -22.027   -31.5395  192.6249 -331.0301]]\n",
      "Weights: [[-4.2015 -1.1116 -0.145   0.1165  0.0783]]\n",
      "MSE loss: 174.5934\n",
      "Iteration: 255500\n",
      "Gradient: [[  1.9937 -12.7786  -1.7486 -56.4564 -24.8992]]\n",
      "Weights: [[-4.2016 -1.1115 -0.1451  0.1165  0.0784]]\n",
      "MSE loss: 174.5799\n",
      "Iteration: 255600\n",
      "Gradient: [[  0.5946   0.8955   8.3688 -70.4669  89.4106]]\n",
      "Weights: [[-4.2017 -1.1114 -0.1451  0.1165  0.0784]]\n",
      "MSE loss: 174.5674\n",
      "Iteration: 255700\n",
      "Gradient: [[-10.0729   7.8035 -15.5007  69.7804 208.5442]]\n",
      "Weights: [[-4.2019 -1.1112 -0.1452  0.1165  0.0784]]\n",
      "MSE loss: 174.5574\n",
      "Iteration: 255800\n",
      "Gradient: [[ -8.9622   5.0466  26.396  106.2105 -20.2377]]\n",
      "Weights: [[-4.2016 -1.1111 -0.1453  0.1165  0.0784]]\n",
      "MSE loss: 174.5432\n",
      "Iteration: 255900\n",
      "Gradient: [[ -9.766   23.1529  -8.1744   8.6074 169.2077]]\n",
      "Weights: [[-4.2013 -1.111  -0.1453  0.1165  0.0784]]\n",
      "MSE loss: 174.5298\n",
      "Iteration: 256000\n",
      "Gradient: [[   3.9898  -11.9619   31.0327   -4.9909 -220.3052]]\n",
      "Weights: [[-4.2015 -1.1111 -0.1454  0.1165  0.0784]]\n",
      "MSE loss: 174.5166\n",
      "Iteration: 256100\n",
      "Gradient: [[  4.398   16.7689 -14.3379 -97.269   27.3251]]\n",
      "Weights: [[-4.2019 -1.111  -0.1455  0.1165  0.0784]]\n",
      "MSE loss: 174.5016\n",
      "Iteration: 256200\n",
      "Gradient: [[  -2.1741   -9.7838   17.8588   48.8293 -380.3797]]\n",
      "Weights: [[-4.2028 -1.111  -0.1456  0.1165  0.0784]]\n",
      "MSE loss: 174.4964\n",
      "Iteration: 256300\n",
      "Gradient: [[ -3.0643  -9.5799 -61.1732 -13.0521 -71.3516]]\n",
      "Weights: [[-4.2037 -1.1107 -0.1456  0.1165  0.0784]]\n",
      "MSE loss: 174.4879\n",
      "Iteration: 256400\n",
      "Gradient: [[   0.7981   -1.5742  -11.7326  -28.1496 -182.0575]]\n",
      "Weights: [[-4.2024 -1.1103 -0.1457  0.1165  0.0784]]\n",
      "MSE loss: 174.4595\n",
      "Iteration: 256500\n",
      "Gradient: [[   8.6006   -4.7099  -19.6798  -10.7226 -211.3341]]\n",
      "Weights: [[-4.2041 -1.1102 -0.1458  0.1165  0.0784]]\n",
      "MSE loss: 174.4509\n",
      "Iteration: 256600\n",
      "Gradient: [[  2.062  -10.3736  18.9068  82.7646 140.1092]]\n",
      "Weights: [[-4.2043 -1.1099 -0.1459  0.1165  0.0784]]\n",
      "MSE loss: 174.4371\n",
      "Iteration: 256700\n",
      "Gradient: [[  -7.5117   -2.3412   25.441    12.765  -370.836 ]]\n",
      "Weights: [[-4.2031 -1.1099 -0.1459  0.1165  0.0784]]\n",
      "MSE loss: 174.4246\n",
      "Iteration: 256800\n",
      "Gradient: [[ 18.1954  19.9367  30.443   24.5403 332.1238]]\n",
      "Weights: [[-4.202  -1.1099 -0.146   0.1165  0.0784]]\n",
      "MSE loss: 174.4105\n",
      "Iteration: 256900\n",
      "Gradient: [[  4.9471   4.926   33.5134 -65.8107 229.8433]]\n",
      "Weights: [[-4.2016 -1.1097 -0.1461  0.1164  0.0785]]\n",
      "MSE loss: 174.3953\n",
      "Iteration: 257000\n",
      "Gradient: [[  -6.3474   -1.5611    6.6317   94.8945 -182.434 ]]\n",
      "Weights: [[-4.201  -1.1096 -0.1462  0.1164  0.0785]]\n",
      "MSE loss: 174.3818\n",
      "Iteration: 257100\n",
      "Gradient: [[  -7.5641    8.6983   14.3776   79.8005 -361.9389]]\n",
      "Weights: [[-4.2019 -1.1096 -0.1462  0.1164  0.0785]]\n",
      "MSE loss: 174.3674\n",
      "Iteration: 257200\n",
      "Gradient: [[ 10.4865  -9.168   12.1734  -9.0067 114.3282]]\n",
      "Weights: [[-4.2017 -1.1092 -0.1463  0.1164  0.0785]]\n",
      "MSE loss: 174.3512\n",
      "Iteration: 257300\n",
      "Gradient: [[ -2.0027 -13.7627  17.1222 -28.816  -27.0695]]\n",
      "Weights: [[-4.2026 -1.1091 -0.1464  0.1164  0.0785]]\n",
      "MSE loss: 174.3329\n",
      "Iteration: 257400\n",
      "Gradient: [[  9.1196   4.432  -58.5209 -63.1315 268.7053]]\n",
      "Weights: [[-4.203  -1.1089 -0.1465  0.1164  0.0785]]\n",
      "MSE loss: 174.3147\n",
      "Iteration: 257500\n",
      "Gradient: [[ -1.5508 -17.7991  31.3634 -89.0125 -37.4992]]\n",
      "Weights: [[-4.2035 -1.1086 -0.1466  0.1164  0.0785]]\n",
      "MSE loss: 174.3032\n",
      "Iteration: 257600\n",
      "Gradient: [[-8.6673 11.677  10.3468 74.2829 35.8782]]\n",
      "Weights: [[-4.2043 -1.1084 -0.1466  0.1164  0.0785]]\n",
      "MSE loss: 174.294\n",
      "Iteration: 257700\n",
      "Gradient: [[  -7.0299  -16.4138   14.5472   44.5914 -136.4846]]\n",
      "Weights: [[-4.2029 -1.1082 -0.1466  0.1164  0.0785]]\n",
      "MSE loss: 174.2781\n",
      "Iteration: 257800\n",
      "Gradient: [[ -15.1926  -15.6582   -6.7442  -44.5729 -260.7101]]\n",
      "Weights: [[-4.2031 -1.1082 -0.1467  0.1164  0.0785]]\n",
      "MSE loss: 174.2621\n",
      "Iteration: 257900\n",
      "Gradient: [[ 3.110000e-02 -1.520410e+01  2.190350e+01 -6.243020e+01 -1.003676e+02]]\n",
      "Weights: [[-4.2038 -1.108  -0.1468  0.1163  0.0785]]\n",
      "MSE loss: 174.2447\n",
      "Iteration: 258000\n",
      "Gradient: [[  3.933  -21.3099  41.8768  42.4308 199.5655]]\n",
      "Weights: [[-4.2041 -1.1078 -0.1469  0.1163  0.0785]]\n",
      "MSE loss: 174.2298\n",
      "Iteration: 258100\n",
      "Gradient: [[ 8.3516 -7.8884 20.1606 -8.1617 28.6264]]\n",
      "Weights: [[-4.2035 -1.1074 -0.147   0.1163  0.0785]]\n",
      "MSE loss: 174.2069\n",
      "Iteration: 258200\n",
      "Gradient: [[  -9.8066  -13.1465  -23.3504  -16.8266 -483.2989]]\n",
      "Weights: [[-4.2042 -1.1071 -0.1471  0.1163  0.0786]]\n",
      "MSE loss: 174.188\n",
      "Iteration: 258300\n",
      "Gradient: [[ 19.0896   1.3525 -26.8677 139.4009  30.4307]]\n",
      "Weights: [[-4.2048 -1.107  -0.1472  0.1163  0.0786]]\n",
      "MSE loss: 174.1734\n",
      "Iteration: 258400\n",
      "Gradient: [[-15.1266  15.3466  83.7752  49.5467  33.3194]]\n",
      "Weights: [[-4.2052 -1.1067 -0.1472  0.1163  0.0786]]\n",
      "MSE loss: 174.1581\n",
      "Iteration: 258500\n",
      "Gradient: [[ -4.4962   3.3728  -4.6152 -58.0354 268.1121]]\n",
      "Weights: [[-4.2051 -1.1066 -0.1474  0.1163  0.0786]]\n",
      "MSE loss: 174.1416\n",
      "Iteration: 258600\n",
      "Gradient: [[  -5.1539  -25.9982   50.5589  107.0457 -271.7283]]\n",
      "Weights: [[-4.2051 -1.1065 -0.1474  0.1163  0.0786]]\n",
      "MSE loss: 174.1305\n",
      "Iteration: 258700\n",
      "Gradient: [[  12.2271   11.3382   -3.0587  -58.8718 -151.1513]]\n",
      "Weights: [[-4.2044 -1.1061 -0.1475  0.1163  0.0786]]\n",
      "MSE loss: 174.1073\n",
      "Iteration: 258800\n",
      "Gradient: [[ -4.7304 -14.4574  32.5862  10.455  116.147 ]]\n",
      "Weights: [[-4.205  -1.1059 -0.1476  0.1163  0.0786]]\n",
      "MSE loss: 174.0946\n",
      "Iteration: 258900\n",
      "Gradient: [[  3.0939  18.734   16.342  -10.365   48.2965]]\n",
      "Weights: [[-4.2043 -1.1058 -0.1477  0.1162  0.0786]]\n",
      "MSE loss: 174.0797\n",
      "Iteration: 259000\n",
      "Gradient: [[   6.3975   -1.7313   -9.3377 -142.4039  -34.7208]]\n",
      "Weights: [[-4.2058 -1.1055 -0.1478  0.1162  0.0786]]\n",
      "MSE loss: 174.0675\n",
      "Iteration: 259100\n",
      "Gradient: [[ -2.3939   7.0903  17.5958 -57.5256  42.6564]]\n",
      "Weights: [[-4.205  -1.1053 -0.1478  0.1162  0.0786]]\n",
      "MSE loss: 174.0478\n",
      "Iteration: 259200\n",
      "Gradient: [[-10.4592 -12.8654 -48.9188  55.2105  34.9664]]\n",
      "Weights: [[-4.205  -1.1051 -0.1479  0.1162  0.0786]]\n",
      "MSE loss: 174.0299\n",
      "Iteration: 259300\n",
      "Gradient: [[-2.4432 11.6806 51.6435  1.1935 48.6911]]\n",
      "Weights: [[-4.205  -1.105  -0.148   0.1162  0.0786]]\n",
      "MSE loss: 174.0172\n",
      "Iteration: 259400\n",
      "Gradient: [[  13.6129  -11.087    24.671    40.8448 -145.9867]]\n",
      "Weights: [[-4.2055 -1.1049 -0.1481  0.1162  0.0786]]\n",
      "MSE loss: 174.0041\n",
      "Iteration: 259500\n",
      "Gradient: [[  0.9813 -10.8019 -22.4698  16.5756  36.8322]]\n",
      "Weights: [[-4.2057 -1.1046 -0.1482  0.1162  0.0786]]\n",
      "MSE loss: 173.9892\n",
      "Iteration: 259600\n",
      "Gradient: [[   5.2215   11.1778   17.4303  126.2294 -186.2023]]\n",
      "Weights: [[-4.205  -1.1043 -0.1483  0.1162  0.0787]]\n",
      "MSE loss: 173.9715\n",
      "Iteration: 259700\n",
      "Gradient: [[ -1.965  -24.9116   8.907  -41.3447  97.9197]]\n",
      "Weights: [[-4.2056 -1.1044 -0.1484  0.1162  0.0787]]\n",
      "MSE loss: 173.9605\n",
      "Iteration: 259800\n",
      "Gradient: [[  0.9368  -5.6868  48.9465  52.1495 -38.7979]]\n",
      "Weights: [[-4.2047 -1.1042 -0.1485  0.1162  0.0787]]\n",
      "MSE loss: 173.9431\n",
      "Iteration: 259900\n",
      "Gradient: [[  8.7318   0.7069  18.4971 115.7196  87.6855]]\n",
      "Weights: [[-4.2054 -1.1038 -0.1486  0.1162  0.0787]]\n",
      "MSE loss: 173.9265\n",
      "Iteration: 260000\n",
      "Gradient: [[   1.7834    3.7802   11.2927  -33.9147 -100.4351]]\n",
      "Weights: [[-4.2054 -1.1036 -0.1486  0.1162  0.0787]]\n",
      "MSE loss: 173.9093\n",
      "Iteration: 260100\n",
      "Gradient: [[   7.7851    2.2481   -2.8264   57.6242 -217.811 ]]\n",
      "Weights: [[-4.2064 -1.1034 -0.1487  0.1162  0.0787]]\n",
      "MSE loss: 173.8933\n",
      "Iteration: 260200\n",
      "Gradient: [[ -1.6174 -11.3481 -10.2946  98.9402 -49.1574]]\n",
      "Weights: [[-4.2058 -1.1033 -0.1488  0.1162  0.0787]]\n",
      "MSE loss: 173.877\n",
      "Iteration: 260300\n",
      "Gradient: [[ -22.1286  -14.3634   35.9551 -191.6746   81.1756]]\n",
      "Weights: [[-4.2058 -1.1029 -0.1489  0.1162  0.0787]]\n",
      "MSE loss: 173.8592\n",
      "Iteration: 260400\n",
      "Gradient: [[  10.438     8.0317    5.9929  -88.7219 -137.9019]]\n",
      "Weights: [[-4.2063 -1.103  -0.149   0.1162  0.0787]]\n",
      "MSE loss: 173.8464\n",
      "Iteration: 260500\n",
      "Gradient: [[  -0.5058   -6.5446   17.0673  145.63   -430.6296]]\n",
      "Weights: [[-4.2066 -1.1027 -0.1491  0.1162  0.0787]]\n",
      "MSE loss: 173.8315\n",
      "Iteration: 260600\n",
      "Gradient: [[  3.687  -26.6971   7.0107  -8.9604  -9.0504]]\n",
      "Weights: [[-4.2068 -1.1026 -0.1492  0.1161  0.0787]]\n",
      "MSE loss: 173.8188\n",
      "Iteration: 260700\n",
      "Gradient: [[-10.2571  -3.4798   2.6121  35.3497 -22.4685]]\n",
      "Weights: [[-4.2066 -1.1026 -0.1493  0.1161  0.0787]]\n",
      "MSE loss: 173.8052\n",
      "Iteration: 260800\n",
      "Gradient: [[ -1.975   -1.7032  60.571   18.6707 106.6984]]\n",
      "Weights: [[-4.2058 -1.1023 -0.1494  0.1161  0.0787]]\n",
      "MSE loss: 173.7842\n",
      "Iteration: 260900\n",
      "Gradient: [[  -3.5581  -13.9755   15.5666  151.6186 -269.106 ]]\n",
      "Weights: [[-4.2053 -1.1023 -0.1495  0.1161  0.0787]]\n",
      "MSE loss: 173.7686\n",
      "Iteration: 261000\n",
      "Gradient: [[   1.489    -6.4311    4.3562 -157.368   458.3403]]\n",
      "Weights: [[-4.2052 -1.1022 -0.1496  0.1161  0.0788]]\n",
      "MSE loss: 173.7501\n",
      "Iteration: 261100\n",
      "Gradient: [[  1.8681  -7.0273 -47.3391 157.672  203.9076]]\n",
      "Weights: [[-4.2058 -1.102  -0.1497  0.1161  0.0788]]\n",
      "MSE loss: 173.7364\n",
      "Iteration: 261200\n",
      "Gradient: [[ -0.737  -10.7564  -9.7394  11.4993 170.9579]]\n",
      "Weights: [[-4.2064 -1.1017 -0.1497  0.1161  0.0788]]\n",
      "MSE loss: 173.7222\n",
      "Iteration: 261300\n",
      "Gradient: [[  5.2375 -15.6636 -15.1474  -6.5872  26.7373]]\n",
      "Weights: [[-4.2064 -1.1014 -0.1498  0.1161  0.0788]]\n",
      "MSE loss: 173.7059\n",
      "Iteration: 261400\n",
      "Gradient: [[ -0.861    9.5575  24.7986  31.7774 -86.8728]]\n",
      "Weights: [[-4.2062 -1.1014 -0.1499  0.1161  0.0788]]\n",
      "MSE loss: 173.6887\n",
      "Iteration: 261500\n",
      "Gradient: [[  15.4852   11.1213  -30.556  -156.0906 -226.9129]]\n",
      "Weights: [[-4.2062 -1.1013 -0.15    0.1161  0.0788]]\n",
      "MSE loss: 173.6782\n",
      "Iteration: 261600\n",
      "Gradient: [[ -1.7918  -3.6201  39.3904  61.9948 119.3311]]\n",
      "Weights: [[-4.206  -1.1011 -0.1501  0.1161  0.0788]]\n",
      "MSE loss: 173.6595\n",
      "Iteration: 261700\n",
      "Gradient: [[   6.4265    3.3544   11.0374 -149.6571   -9.1019]]\n",
      "Weights: [[-4.2058 -1.1008 -0.1502  0.1161  0.0788]]\n",
      "MSE loss: 173.6422\n",
      "Iteration: 261800\n",
      "Gradient: [[ -12.3402   -7.537     8.9203 -129.7637  220.2301]]\n",
      "Weights: [[-4.2061 -1.1007 -0.1503  0.116   0.0788]]\n",
      "MSE loss: 173.6247\n",
      "Iteration: 261900\n",
      "Gradient: [[-11.9734 -13.1107   2.268    9.4273  -8.0808]]\n",
      "Weights: [[-4.2071 -1.1007 -0.1504  0.116   0.0788]]\n",
      "MSE loss: 173.6159\n",
      "Iteration: 262000\n",
      "Gradient: [[13.8589 -4.4569 34.3526 15.2591 56.1686]]\n",
      "Weights: [[-4.2066 -1.1006 -0.1505  0.116   0.0788]]\n",
      "MSE loss: 173.5962\n",
      "Iteration: 262100\n",
      "Gradient: [[ -5.7837 -11.9912  -3.6866  -6.0401 152.932 ]]\n",
      "Weights: [[-4.2062 -1.1006 -0.1506  0.116   0.0788]]\n",
      "MSE loss: 173.5847\n",
      "Iteration: 262200\n",
      "Gradient: [[  12.6612  -14.9772  -13.9873 -123.7047  136.8235]]\n",
      "Weights: [[-4.2053 -1.1006 -0.1507  0.116   0.0788]]\n",
      "MSE loss: 173.5741\n",
      "Iteration: 262300\n",
      "Gradient: [[ -11.073    21.2179    0.7918    3.3096 -133.7813]]\n",
      "Weights: [[-4.2055 -1.1004 -0.1508  0.116   0.0788]]\n",
      "MSE loss: 173.5552\n",
      "Iteration: 262400\n",
      "Gradient: [[   9.2502    3.0373   50.4246 -127.8561   81.5482]]\n",
      "Weights: [[-4.2041 -1.1003 -0.1509  0.116   0.0788]]\n",
      "MSE loss: 173.5456\n",
      "Iteration: 262500\n",
      "Gradient: [[  8.8714  -9.8929  -4.2044 165.2115 253.2987]]\n",
      "Weights: [[-4.205  -1.1002 -0.151   0.116   0.0789]]\n",
      "MSE loss: 173.5241\n",
      "Iteration: 262600\n",
      "Gradient: [[  6.0343  -7.0995 -36.8531 -61.9339 -41.6997]]\n",
      "Weights: [[-4.2061 -1.1001 -0.1511  0.116   0.0789]]\n",
      "MSE loss: 173.5125\n",
      "Iteration: 262700\n",
      "Gradient: [[  -0.9832   -3.1209  -19.1003 -168.9955  153.2089]]\n",
      "Weights: [[-4.206  -1.0998 -0.1511  0.116   0.0789]]\n",
      "MSE loss: 173.4959\n",
      "Iteration: 262800\n",
      "Gradient: [[  9.3252 -22.8245 -30.5145 -48.3086 -77.8116]]\n",
      "Weights: [[-4.2052 -1.0998 -0.1512  0.116   0.0789]]\n",
      "MSE loss: 173.4862\n",
      "Iteration: 262900\n",
      "Gradient: [[ 8.157  -7.2225  1.7558 -1.7653 10.0145]]\n",
      "Weights: [[-4.2049 -1.0996 -0.1512  0.116   0.0789]]\n",
      "MSE loss: 173.4766\n",
      "Iteration: 263000\n",
      "Gradient: [[ -2.4666   7.4314   2.7559  79.5809 -21.818 ]]\n",
      "Weights: [[-4.2048 -1.0995 -0.1514  0.116   0.0789]]\n",
      "MSE loss: 173.4565\n",
      "Iteration: 263100\n",
      "Gradient: [[   6.0142   -2.5083  -13.7554 -127.1537 -207.2164]]\n",
      "Weights: [[-4.2048 -1.0994 -0.1514  0.116   0.0789]]\n",
      "MSE loss: 173.4425\n",
      "Iteration: 263200\n",
      "Gradient: [[   3.9112   -0.3449   11.5728   52.4853 -133.5323]]\n",
      "Weights: [[-4.2055 -1.0992 -0.1515  0.116   0.0789]]\n",
      "MSE loss: 173.4251\n",
      "Iteration: 263300\n",
      "Gradient: [[-9.1681 -0.9797 -3.3851 -5.9181 16.3082]]\n",
      "Weights: [[-4.205  -1.0991 -0.1515  0.116   0.0789]]\n",
      "MSE loss: 173.4194\n",
      "Iteration: 263400\n",
      "Gradient: [[   1.8866    5.2217   26.9204  -94.0911 -115.8676]]\n",
      "Weights: [[-4.2051 -1.0992 -0.1517  0.116   0.0789]]\n",
      "MSE loss: 173.4061\n",
      "Iteration: 263500\n",
      "Gradient: [[ 10.7806   9.4466   5.7021  86.2682 146.9019]]\n",
      "Weights: [[-4.205  -1.0989 -0.1518  0.116   0.0789]]\n",
      "MSE loss: 173.3892\n",
      "Iteration: 263600\n",
      "Gradient: [[-5.1323  2.7383 16.9907 96.1814 -8.0183]]\n",
      "Weights: [[-4.2062 -1.0988 -0.1518  0.116   0.0789]]\n",
      "MSE loss: 173.3727\n",
      "Iteration: 263700\n",
      "Gradient: [[   7.9289   -8.8828   28.6826   43.3732 -177.7056]]\n",
      "Weights: [[-4.2044 -1.0988 -0.1519  0.116   0.0789]]\n",
      "MSE loss: 173.3604\n",
      "Iteration: 263800\n",
      "Gradient: [[ 11.1375  14.0339 -44.0892 -56.5497 384.386 ]]\n",
      "Weights: [[-4.204  -1.0987 -0.152   0.1159  0.0789]]\n",
      "MSE loss: 173.3477\n",
      "Iteration: 263900\n",
      "Gradient: [[ -1.736   16.9159   7.6907 -67.6004 118.4389]]\n",
      "Weights: [[-4.2045 -1.0987 -0.1521  0.1159  0.079 ]]\n",
      "MSE loss: 173.3381\n",
      "Iteration: 264000\n",
      "Gradient: [[-14.8758 -14.7634 -18.9993 -54.4698 217.2458]]\n",
      "Weights: [[-4.2044 -1.0986 -0.1521  0.1159  0.079 ]]\n",
      "MSE loss: 173.3275\n",
      "Iteration: 264100\n",
      "Gradient: [[  13.35     28.4036   34.3987   60.2933 -288.3754]]\n",
      "Weights: [[-4.2047 -1.0983 -0.1523  0.1159  0.079 ]]\n",
      "MSE loss: 173.306\n",
      "Iteration: 264200\n",
      "Gradient: [[ 3.7383 -5.1474  3.7691 48.1489 50.4198]]\n",
      "Weights: [[-4.2041 -1.0982 -0.1523  0.1159  0.079 ]]\n",
      "MSE loss: 173.2959\n",
      "Iteration: 264300\n",
      "Gradient: [[ -10.714   -14.5039   18.1792  -92.264  -113.6517]]\n",
      "Weights: [[-4.204  -1.0982 -0.1524  0.1159  0.079 ]]\n",
      "MSE loss: 173.2874\n",
      "Iteration: 264400\n",
      "Gradient: [[  -4.842    20.7959    9.3597 -111.0751 -251.2276]]\n",
      "Weights: [[-4.2052 -1.0982 -0.1524  0.1159  0.079 ]]\n",
      "MSE loss: 173.2745\n",
      "Iteration: 264500\n",
      "Gradient: [[  1.1185  -3.6765  -9.5299 -24.8717  81.9528]]\n",
      "Weights: [[-4.208  -1.098  -0.1525  0.1159  0.079 ]]\n",
      "MSE loss: 173.2726\n",
      "Iteration: 264600\n",
      "Gradient: [[   1.8835   -7.1244  -10.0452   40.3841 -453.1537]]\n",
      "Weights: [[-4.2071 -1.0978 -0.1525  0.1159  0.079 ]]\n",
      "MSE loss: 173.2488\n",
      "Iteration: 264700\n",
      "Gradient: [[   7.2329   25.0999   13.372   -24.7927 -107.6928]]\n",
      "Weights: [[-4.2064 -1.0976 -0.1526  0.1159  0.079 ]]\n",
      "MSE loss: 173.2325\n",
      "Iteration: 264800\n",
      "Gradient: [[-2.472000e-01  6.697000e-01  1.584200e+00 -3.270700e+01 -6.501076e+02]]\n",
      "Weights: [[-4.2071 -1.0976 -0.1527  0.1159  0.079 ]]\n",
      "MSE loss: 173.2228\n",
      "Iteration: 264900\n",
      "Gradient: [[ -2.3412  -6.4583 -30.8109  -1.8537  73.6772]]\n",
      "Weights: [[-4.2075 -1.0972 -0.1527  0.1159  0.079 ]]\n",
      "MSE loss: 173.206\n",
      "Iteration: 265000\n",
      "Gradient: [[  2.2295 -17.2006 -32.1745  59.3511 244.0794]]\n",
      "Weights: [[-4.2084 -1.0968 -0.1528  0.1159  0.079 ]]\n",
      "MSE loss: 173.1903\n",
      "Iteration: 265100\n",
      "Gradient: [[ 1.365000e-01 -6.944900e+00 -5.891620e+01  6.707700e+00  3.767722e+02]]\n",
      "Weights: [[-4.2077 -1.0967 -0.1529  0.1159  0.079 ]]\n",
      "MSE loss: 173.1755\n",
      "Iteration: 265200\n",
      "Gradient: [[ 11.1682  18.9715  21.2499 -24.694   36.0218]]\n",
      "Weights: [[-4.2055 -1.0966 -0.153   0.1159  0.079 ]]\n",
      "MSE loss: 173.1632\n",
      "Iteration: 265300\n",
      "Gradient: [[   2.5602   -5.0011  -37.8243   65.7243 -170.1029]]\n",
      "Weights: [[-4.2072 -1.0963 -0.1531  0.1159  0.079 ]]\n",
      "MSE loss: 173.1421\n",
      "Iteration: 265400\n",
      "Gradient: [[  3.8006  -1.813   31.5502 -75.273  201.9627]]\n",
      "Weights: [[-4.2066 -1.0962 -0.1532  0.1159  0.0791]]\n",
      "MSE loss: 173.126\n",
      "Iteration: 265500\n",
      "Gradient: [[ -1.662   21.1264 -11.376   50.8232  18.039 ]]\n",
      "Weights: [[-4.207  -1.0958 -0.1532  0.1158  0.0791]]\n",
      "MSE loss: 173.1135\n",
      "Iteration: 265600\n",
      "Gradient: [[ -2.7605  -5.8792 -23.7882  31.9788 172.753 ]]\n",
      "Weights: [[-4.2063 -1.0958 -0.1533  0.1158  0.0791]]\n",
      "MSE loss: 173.1019\n",
      "Iteration: 265700\n",
      "Gradient: [[ -12.293    -1.6589   18.6158   27.1882 -187.6568]]\n",
      "Weights: [[-4.2062 -1.0958 -0.1534  0.1158  0.0791]]\n",
      "MSE loss: 173.0948\n",
      "Iteration: 265800\n",
      "Gradient: [[ -8.844  -28.5843  -3.1554  70.1839 139.9328]]\n",
      "Weights: [[-4.2059 -1.0957 -0.1535  0.1158  0.0791]]\n",
      "MSE loss: 173.0803\n",
      "Iteration: 265900\n",
      "Gradient: [[  -4.7414  -20.0828    9.836    25.3214 -309.7319]]\n",
      "Weights: [[-4.2062 -1.0952 -0.1536  0.1158  0.0791]]\n",
      "MSE loss: 173.0669\n",
      "Iteration: 266000\n",
      "Gradient: [[ 14.2719  -0.4864 -29.3106 -39.5723 -74.035 ]]\n",
      "Weights: [[-4.2069 -1.095  -0.1536  0.1158  0.0791]]\n",
      "MSE loss: 173.056\n",
      "Iteration: 266100\n",
      "Gradient: [[-12.1328  18.8809  49.4963  27.5825 136.7688]]\n",
      "Weights: [[-4.2073 -1.0948 -0.1537  0.1158  0.0791]]\n",
      "MSE loss: 173.0423\n",
      "Iteration: 266200\n",
      "Gradient: [[ -13.1072  -13.4126    6.4649   77.6935 -464.5182]]\n",
      "Weights: [[-4.2086 -1.0947 -0.1538  0.1158  0.0791]]\n",
      "MSE loss: 173.0264\n",
      "Iteration: 266300\n",
      "Gradient: [[   0.2017   -8.4061  -32.384    25.7546 -166.7788]]\n",
      "Weights: [[-4.2069 -1.0946 -0.1539  0.1158  0.0791]]\n",
      "MSE loss: 173.0077\n",
      "Iteration: 266400\n",
      "Gradient: [[ -14.2358   10.6159   22.5461 -131.3883 -377.333 ]]\n",
      "Weights: [[-4.2074 -1.0945 -0.154   0.1158  0.0791]]\n",
      "MSE loss: 172.9924\n",
      "Iteration: 266500\n",
      "Gradient: [[  0.8438 -11.6144  45.2436 -47.4455 -95.7589]]\n",
      "Weights: [[-4.2066 -1.0944 -0.1541  0.1158  0.0791]]\n",
      "MSE loss: 172.981\n",
      "Iteration: 266600\n",
      "Gradient: [[   1.6966   -0.8968   28.3008  -32.1019 -219.0433]]\n",
      "Weights: [[-4.2069 -1.0943 -0.1542  0.1158  0.0791]]\n",
      "MSE loss: 172.9666\n",
      "Iteration: 266700\n",
      "Gradient: [[  -3.7303    7.3866  -15.638    11.9988 -166.4162]]\n",
      "Weights: [[-4.2072 -1.0943 -0.1542  0.1158  0.0791]]\n",
      "MSE loss: 172.9525\n",
      "Iteration: 266800\n",
      "Gradient: [[  -6.2487  -15.158    -7.2277 -101.4114   43.0729]]\n",
      "Weights: [[-4.2076 -1.094  -0.1543  0.1158  0.0791]]\n",
      "MSE loss: 172.938\n",
      "Iteration: 266900\n",
      "Gradient: [[  17.5585    1.1759   50.4279  -20.3295 -107.9465]]\n",
      "Weights: [[-4.207  -1.0937 -0.1544  0.1158  0.0791]]\n",
      "MSE loss: 172.9185\n",
      "Iteration: 267000\n",
      "Gradient: [[ -8.792   -5.2423 -23.0751 113.6208 -22.5079]]\n",
      "Weights: [[-4.2069 -1.0936 -0.1546  0.1158  0.0792]]\n",
      "MSE loss: 172.8992\n",
      "Iteration: 267100\n",
      "Gradient: [[ 17.6796  14.1466  -6.4746 -99.7565 169.119 ]]\n",
      "Weights: [[-4.2064 -1.0933 -0.1547  0.1157  0.0792]]\n",
      "MSE loss: 172.8834\n",
      "Iteration: 267200\n",
      "Gradient: [[   2.3922    0.7509   19.6013  -52.8042 -301.6218]]\n",
      "Weights: [[-4.2067 -1.0932 -0.1547  0.1157  0.0792]]\n",
      "MSE loss: 172.8671\n",
      "Iteration: 267300\n",
      "Gradient: [[ -1.56   -14.3129  -3.7655   2.3301 291.2048]]\n",
      "Weights: [[-4.2068 -1.093  -0.1548  0.1157  0.0792]]\n",
      "MSE loss: 172.852\n",
      "Iteration: 267400\n",
      "Gradient: [[  3.5425   9.9105   3.2062   7.4812 -30.822 ]]\n",
      "Weights: [[-4.205  -1.093  -0.1549  0.1157  0.0792]]\n",
      "MSE loss: 172.8508\n",
      "Iteration: 267500\n",
      "Gradient: [[  -5.8661    7.0859   13.1822   85.3922 -118.151 ]]\n",
      "Weights: [[-4.2058 -1.0929 -0.155   0.1157  0.0792]]\n",
      "MSE loss: 172.825\n",
      "Iteration: 267600\n",
      "Gradient: [[-10.8699  -4.8616 -16.6343 -80.1027  53.9579]]\n",
      "Weights: [[-4.2058 -1.0928 -0.1551  0.1157  0.0792]]\n",
      "MSE loss: 172.8158\n",
      "Iteration: 267700\n",
      "Gradient: [[   0.8171   13.6262  -40.9167   29.6058 -555.553 ]]\n",
      "Weights: [[-4.207  -1.0924 -0.1551  0.1156  0.0792]]\n",
      "MSE loss: 172.7994\n",
      "Iteration: 267800\n",
      "Gradient: [[  0.8867 -11.8039 -13.4444 126.1022  76.2069]]\n",
      "Weights: [[-4.2076 -1.0923 -0.1551  0.1156  0.0792]]\n",
      "MSE loss: 172.7861\n",
      "Iteration: 267900\n",
      "Gradient: [[ -9.3686  15.5539  47.8625 109.277  115.3231]]\n",
      "Weights: [[-4.2072 -1.0918 -0.1552  0.1156  0.0792]]\n",
      "MSE loss: 172.7727\n",
      "Iteration: 268000\n",
      "Gradient: [[ -14.9613    3.202    38.2694 -134.195  -123.5452]]\n",
      "Weights: [[-4.2073 -1.0918 -0.1553  0.1156  0.0792]]\n",
      "MSE loss: 172.7586\n",
      "Iteration: 268100\n",
      "Gradient: [[ 14.2313 -10.6346  67.5378  -8.2865  34.4884]]\n",
      "Weights: [[-4.2071 -1.0918 -0.1554  0.1156  0.0792]]\n",
      "MSE loss: 172.7459\n",
      "Iteration: 268200\n",
      "Gradient: [[  1.6652   5.1949 -27.489   48.3667  -9.052 ]]\n",
      "Weights: [[-4.207  -1.0917 -0.1555  0.1156  0.0792]]\n",
      "MSE loss: 172.7317\n",
      "Iteration: 268300\n",
      "Gradient: [[ -2.6172  -7.8176  21.805   14.0282 -29.1706]]\n",
      "Weights: [[-4.2081 -1.0914 -0.1556  0.1156  0.0792]]\n",
      "MSE loss: 172.7105\n",
      "Iteration: 268400\n",
      "Gradient: [[   8.1866   -1.4704  -50.5977   28.1279 -239.2918]]\n",
      "Weights: [[-4.2078 -1.0913 -0.1557  0.1156  0.0792]]\n",
      "MSE loss: 172.6934\n",
      "Iteration: 268500\n",
      "Gradient: [[-6.347400e+00  2.155760e+01  3.719190e+01  3.950000e-02  1.060054e+02]]\n",
      "Weights: [[-4.2077 -1.0912 -0.1558  0.1156  0.0793]]\n",
      "MSE loss: 172.678\n",
      "Iteration: 268600\n",
      "Gradient: [[   1.765    -8.3829  -21.9985  202.8358 -301.7508]]\n",
      "Weights: [[-4.2071 -1.0911 -0.1559  0.1156  0.0793]]\n",
      "MSE loss: 172.6666\n",
      "Iteration: 268700\n",
      "Gradient: [[  -3.109   -35.9781  -62.9074  -28.0653 -160.198 ]]\n",
      "Weights: [[-4.2065 -1.0908 -0.1559  0.1156  0.0793]]\n",
      "MSE loss: 172.6568\n",
      "Iteration: 268800\n",
      "Gradient: [[  12.5316  -11.8795   26.3862   64.2938 -379.9838]]\n",
      "Weights: [[-4.2064 -1.0906 -0.156   0.1156  0.0793]]\n",
      "MSE loss: 172.6466\n",
      "Iteration: 268900\n",
      "Gradient: [[ -1.2743  -6.6442 -35.8598 -13.572  120.6996]]\n",
      "Weights: [[-4.2067 -1.0908 -0.1561  0.1156  0.0793]]\n",
      "MSE loss: 172.6316\n",
      "Iteration: 269000\n",
      "Gradient: [[ 7.0544 17.839  31.5348 -7.2644 13.2387]]\n",
      "Weights: [[-4.2072 -1.0906 -0.1562  0.1156  0.0793]]\n",
      "MSE loss: 172.616\n",
      "Iteration: 269100\n",
      "Gradient: [[ -17.396   -23.4295  -15.1895  -14.7256 -434.7027]]\n",
      "Weights: [[-4.2071 -1.0907 -0.1563  0.1156  0.0793]]\n",
      "MSE loss: 172.6029\n",
      "Iteration: 269200\n",
      "Gradient: [[ -6.1652   2.4606 -14.8244  96.3826  56.1823]]\n",
      "Weights: [[-4.2067 -1.0907 -0.1564  0.1156  0.0793]]\n",
      "MSE loss: 172.5908\n",
      "Iteration: 269300\n",
      "Gradient: [[  3.8098  -4.1427  57.1627  30.4451 -39.7453]]\n",
      "Weights: [[-4.2065 -1.0903 -0.1565  0.1155  0.0793]]\n",
      "MSE loss: 172.5706\n",
      "Iteration: 269400\n",
      "Gradient: [[  20.673   -14.172     3.7817 -131.8068 -360.3598]]\n",
      "Weights: [[-4.206  -1.0903 -0.1566  0.1155  0.0793]]\n",
      "MSE loss: 172.5623\n",
      "Iteration: 269500\n",
      "Gradient: [[  8.4983   7.4175 -65.3799  48.7067 161.7175]]\n",
      "Weights: [[-4.206  -1.0901 -0.1567  0.1155  0.0793]]\n",
      "MSE loss: 172.5424\n",
      "Iteration: 269600\n",
      "Gradient: [[ -7.6697  -1.1446 -15.8649  39.4647  81.2803]]\n",
      "Weights: [[-4.2064 -1.0899 -0.1567  0.1155  0.0793]]\n",
      "MSE loss: 172.5333\n",
      "Iteration: 269700\n",
      "Gradient: [[-9.420800e+00 -2.082000e-01  3.356100e+01  8.670150e+01 -3.155213e+02]]\n",
      "Weights: [[-4.2067 -1.0898 -0.1568  0.1155  0.0793]]\n",
      "MSE loss: 172.5206\n",
      "Iteration: 269800\n",
      "Gradient: [[  2.7518  10.2179  19.4653 -60.4157 -91.4336]]\n",
      "Weights: [[-4.2065 -1.0897 -0.1568  0.1155  0.0793]]\n",
      "MSE loss: 172.5092\n",
      "Iteration: 269900\n",
      "Gradient: [[  5.2656  -5.1596  43.189  -88.7727   0.1048]]\n",
      "Weights: [[-4.2075 -1.0895 -0.1569  0.1155  0.0793]]\n",
      "MSE loss: 172.4883\n",
      "Iteration: 270000\n",
      "Gradient: [[ 13.6849  -6.012  -11.678  114.025   44.8074]]\n",
      "Weights: [[-4.2076 -1.0894 -0.157   0.1155  0.0793]]\n",
      "MSE loss: 172.4768\n",
      "Iteration: 270100\n",
      "Gradient: [[  6.1468   4.2566 -47.4983 133.4115 136.7236]]\n",
      "Weights: [[-4.2079 -1.0892 -0.1571  0.1155  0.0794]]\n",
      "MSE loss: 172.4598\n",
      "Iteration: 270200\n",
      "Gradient: [[ -3.5708 -16.4046  34.7572 -55.4177 -36.2006]]\n",
      "Weights: [[-4.2082 -1.0893 -0.1572  0.1155  0.0794]]\n",
      "MSE loss: 172.4558\n",
      "Iteration: 270300\n",
      "Gradient: [[  7.1651  -5.1017 -16.9859 -26.687    2.56  ]]\n",
      "Weights: [[-4.2084 -1.0891 -0.1572  0.1155  0.0794]]\n",
      "MSE loss: 172.4425\n",
      "Iteration: 270400\n",
      "Gradient: [[   2.2115   13.3418   54.685    -0.4761 -261.6884]]\n",
      "Weights: [[-4.2081 -1.0887 -0.1573  0.1155  0.0794]]\n",
      "MSE loss: 172.4224\n",
      "Iteration: 270500\n",
      "Gradient: [[ -4.1555   7.5483 -24.8619 -65.2436  38.9548]]\n",
      "Weights: [[-4.208  -1.0884 -0.1574  0.1155  0.0794]]\n",
      "MSE loss: 172.4047\n",
      "Iteration: 270600\n",
      "Gradient: [[  7.8825  -0.4504  37.6317  64.1884 351.1206]]\n",
      "Weights: [[-4.2088 -1.0882 -0.1575  0.1155  0.0794]]\n",
      "MSE loss: 172.3917\n",
      "Iteration: 270700\n",
      "Gradient: [[ -10.279     3.36     28.9052  161.3873 -222.3288]]\n",
      "Weights: [[-4.2088 -1.088  -0.1575  0.1155  0.0794]]\n",
      "MSE loss: 172.3795\n",
      "Iteration: 270800\n",
      "Gradient: [[   6.5019   -6.8487   -0.4568  -88.8133 -376.9429]]\n",
      "Weights: [[-4.2097 -1.0878 -0.1576  0.1154  0.0794]]\n",
      "MSE loss: 172.3598\n",
      "Iteration: 270900\n",
      "Gradient: [[  3.7291 -18.9805  37.1108  51.7877 -81.5355]]\n",
      "Weights: [[-4.2097 -1.0875 -0.1577  0.1154  0.0794]]\n",
      "MSE loss: 172.3424\n",
      "Iteration: 271000\n",
      "Gradient: [[-13.4693 -10.5613  18.0625  68.2336 -25.8619]]\n",
      "Weights: [[-4.2102 -1.0875 -0.1578  0.1154  0.0794]]\n",
      "MSE loss: 172.3309\n",
      "Iteration: 271100\n",
      "Gradient: [[  -9.4518   -1.6292  -72.6498   23.8553 -225.8386]]\n",
      "Weights: [[-4.2089 -1.0874 -0.1579  0.1154  0.0794]]\n",
      "MSE loss: 172.3157\n",
      "Iteration: 271200\n",
      "Gradient: [[  11.799    13.7943  -41.9478   95.9805 -221.697 ]]\n",
      "Weights: [[-4.2094 -1.0872 -0.158   0.1154  0.0794]]\n",
      "MSE loss: 172.2993\n",
      "Iteration: 271300\n",
      "Gradient: [[  -8.4535  -20.6248   42.0847  -27.0864 -370.3422]]\n",
      "Weights: [[-4.2096 -1.0871 -0.1581  0.1154  0.0794]]\n",
      "MSE loss: 172.2865\n",
      "Iteration: 271400\n",
      "Gradient: [[  4.7334  11.0637   9.5766 125.6674 -16.5516]]\n",
      "Weights: [[-4.2086 -1.0869 -0.1581  0.1154  0.0794]]\n",
      "MSE loss: 172.2728\n",
      "Iteration: 271500\n",
      "Gradient: [[  3.6183  13.7691   8.369  -37.0785 -13.2239]]\n",
      "Weights: [[-4.2077 -1.0869 -0.1582  0.1154  0.0795]]\n",
      "MSE loss: 172.2632\n",
      "Iteration: 271600\n",
      "Gradient: [[-11.1723  -5.1804 -62.7023  95.0693  57.024 ]]\n",
      "Weights: [[-4.207  -1.0867 -0.1583  0.1154  0.0795]]\n",
      "MSE loss: 172.2516\n",
      "Iteration: 271700\n",
      "Gradient: [[  1.2692  19.6368  48.7089  -4.5043 -63.618 ]]\n",
      "Weights: [[-4.2078 -1.0866 -0.1584  0.1154  0.0795]]\n",
      "MSE loss: 172.2371\n",
      "Iteration: 271800\n",
      "Gradient: [[ -1.7917 -10.8805  69.6574 139.5619  88.3746]]\n",
      "Weights: [[-4.2087 -1.0865 -0.1584  0.1154  0.0795]]\n",
      "MSE loss: 172.2226\n",
      "Iteration: 271900\n",
      "Gradient: [[  0.9489  -4.4406  37.82   -82.701   86.3586]]\n",
      "Weights: [[-4.2087 -1.0862 -0.1586  0.1153  0.0795]]\n",
      "MSE loss: 172.2001\n",
      "Iteration: 272000\n",
      "Gradient: [[  5.3884 -38.4243   2.6434   5.3546 -61.4436]]\n",
      "Weights: [[-4.2088 -1.0856 -0.1587  0.1153  0.0795]]\n",
      "MSE loss: 172.1808\n",
      "Iteration: 272100\n",
      "Gradient: [[ -4.907    9.4832   0.6485 -54.4098  35.4262]]\n",
      "Weights: [[-4.2094 -1.0854 -0.1587  0.1153  0.0795]]\n",
      "MSE loss: 172.1647\n",
      "Iteration: 272200\n",
      "Gradient: [[  -0.7736   23.3032    5.1234 -103.4673 -287.9395]]\n",
      "Weights: [[-4.2096 -1.0852 -0.1588  0.1153  0.0795]]\n",
      "MSE loss: 172.1474\n",
      "Iteration: 272300\n",
      "Gradient: [[  6.1898 -19.0009  40.7785 -51.2077 337.4831]]\n",
      "Weights: [[-4.2086 -1.0851 -0.159   0.1153  0.0795]]\n",
      "MSE loss: 172.1284\n",
      "Iteration: 272400\n",
      "Gradient: [[ -3.3257  -3.4895  35.4713 -65.5511  48.2359]]\n",
      "Weights: [[-4.2092 -1.085  -0.1591  0.1153  0.0795]]\n",
      "MSE loss: 172.1137\n",
      "Iteration: 272500\n",
      "Gradient: [[ 13.1851 -11.0336  27.5496  72.9686 -69.0552]]\n",
      "Weights: [[-4.2096 -1.0847 -0.1591  0.1153  0.0795]]\n",
      "MSE loss: 172.0958\n",
      "Iteration: 272600\n",
      "Gradient: [[-9.9534  0.2853 54.9314  0.1774 34.8714]]\n",
      "Weights: [[-4.2096 -1.0846 -0.1592  0.1153  0.0795]]\n",
      "MSE loss: 172.0801\n",
      "Iteration: 272700\n",
      "Gradient: [[   2.0612  -18.5446  -27.6403  113.3682 -394.9377]]\n",
      "Weights: [[-4.2106 -1.0845 -0.1593  0.1153  0.0795]]\n",
      "MSE loss: 172.0687\n",
      "Iteration: 272800\n",
      "Gradient: [[  -4.0381   -5.9399  -21.3647  -64.7275 -333.4362]]\n",
      "Weights: [[-4.2098 -1.0845 -0.1593  0.1153  0.0795]]\n",
      "MSE loss: 172.0579\n",
      "Iteration: 272900\n",
      "Gradient: [[-11.3518  -5.4358  39.0834   4.1184  10.5316]]\n",
      "Weights: [[-4.2098 -1.0843 -0.1594  0.1153  0.0796]]\n",
      "MSE loss: 172.0487\n",
      "Iteration: 273000\n",
      "Gradient: [[ -17.299    -9.5451    8.5492 -106.2353 -103.6594]]\n",
      "Weights: [[-4.2101 -1.0843 -0.1594  0.1153  0.0796]]\n",
      "MSE loss: 172.0367\n",
      "Iteration: 273100\n",
      "Gradient: [[  -1.0239   21.6368   15.6167  -23.5128 -427.4702]]\n",
      "Weights: [[-4.2098 -1.0841 -0.1595  0.1153  0.0796]]\n",
      "MSE loss: 172.0206\n",
      "Iteration: 273200\n",
      "Gradient: [[  1.2468  -1.9936  26.0749 -97.0549 -79.6875]]\n",
      "Weights: [[-4.2101 -1.0841 -0.1597  0.1153  0.0796]]\n",
      "MSE loss: 172.0047\n",
      "Iteration: 273300\n",
      "Gradient: [[ -3.1865  -2.9788 -30.2133 -19.9041 -35.4164]]\n",
      "Weights: [[-4.2108 -1.084  -0.1597  0.1153  0.0796]]\n",
      "MSE loss: 171.9956\n",
      "Iteration: 273400\n",
      "Gradient: [[  13.482    17.7854   42.1043   78.8242 -111.3033]]\n",
      "Weights: [[-4.2094 -1.084  -0.1598  0.1153  0.0796]]\n",
      "MSE loss: 171.9868\n",
      "Iteration: 273500\n",
      "Gradient: [[   4.9995    2.2336   24.3328   85.1019 -520.3454]]\n",
      "Weights: [[-4.2095 -1.084  -0.1599  0.1153  0.0796]]\n",
      "MSE loss: 171.9765\n",
      "Iteration: 273600\n",
      "Gradient: [[  1.7685  20.7023  16.9335 137.2307 210.7398]]\n",
      "Weights: [[-4.2084 -1.0841 -0.16    0.1153  0.0796]]\n",
      "MSE loss: 171.9646\n",
      "Iteration: 273700\n",
      "Gradient: [[ -1.6083  35.3186   8.9542 -75.4952  75.7546]]\n",
      "Weights: [[-4.2092 -1.0839 -0.16    0.1153  0.0796]]\n",
      "MSE loss: 171.9467\n",
      "Iteration: 273800\n",
      "Gradient: [[  -7.5639    6.7339    7.7979   91.4225 -265.0386]]\n",
      "Weights: [[-4.21   -1.0839 -0.1601  0.1153  0.0796]]\n",
      "MSE loss: 171.9394\n",
      "Iteration: 273900\n",
      "Gradient: [[ -10.908     6.1021  -75.7709 -201.3095 -340.1264]]\n",
      "Weights: [[-4.2107 -1.0837 -0.1602  0.1153  0.0796]]\n",
      "MSE loss: 171.9263\n",
      "Iteration: 274000\n",
      "Gradient: [[-8.600000e-02 -8.825000e-01  1.821610e+01  3.578040e+01 -1.213807e+02]]\n",
      "Weights: [[-4.2099 -1.0835 -0.1602  0.1153  0.0796]]\n",
      "MSE loss: 171.9053\n",
      "Iteration: 274100\n",
      "Gradient: [[  -1.4653   -2.9281  -10.7714  -29.1964 -273.4041]]\n",
      "Weights: [[-4.2092 -1.0832 -0.1603  0.1153  0.0796]]\n",
      "MSE loss: 171.8924\n",
      "Iteration: 274200\n",
      "Gradient: [[   2.9325   -1.53     34.5059   15.126  -366.9886]]\n",
      "Weights: [[-4.2086 -1.0831 -0.1604  0.1152  0.0796]]\n",
      "MSE loss: 171.8804\n",
      "Iteration: 274300\n",
      "Gradient: [[   4.6248  -14.3037    8.2185   -6.1066 -346.7563]]\n",
      "Weights: [[-4.2097 -1.0831 -0.1605  0.1152  0.0797]]\n",
      "MSE loss: 171.8664\n",
      "Iteration: 274400\n",
      "Gradient: [[ -2.325   -6.7498   6.6507 152.9095  57.5017]]\n",
      "Weights: [[-4.2098 -1.0828 -0.1606  0.1152  0.0797]]\n",
      "MSE loss: 171.8505\n",
      "Iteration: 274500\n",
      "Gradient: [[  -7.4037    0.2271   57.011  -107.859  -209.5964]]\n",
      "Weights: [[-4.2107 -1.0826 -0.1606  0.1152  0.0797]]\n",
      "MSE loss: 171.8353\n",
      "Iteration: 274600\n",
      "Gradient: [[ -0.4954 -17.1764 -19.0114 -70.6557 161.3701]]\n",
      "Weights: [[-4.2104 -1.0822 -0.1608  0.1152  0.0797]]\n",
      "MSE loss: 171.8115\n",
      "Iteration: 274700\n",
      "Gradient: [[ 23.5512  -2.2716  28.9024 125.6713   4.0992]]\n",
      "Weights: [[-4.2095 -1.0822 -0.1609  0.1152  0.0797]]\n",
      "MSE loss: 171.8028\n",
      "Iteration: 274800\n",
      "Gradient: [[  -2.558    -1.8483   20.1796  -92.65   -115.5547]]\n",
      "Weights: [[-4.2108 -1.0819 -0.1609  0.1152  0.0797]]\n",
      "MSE loss: 171.7844\n",
      "Iteration: 274900\n",
      "Gradient: [[   6.6279   13.1779  -23.3553    9.9453 -118.1538]]\n",
      "Weights: [[-4.2106 -1.0817 -0.161   0.1152  0.0797]]\n",
      "MSE loss: 171.7677\n",
      "Iteration: 275000\n",
      "Gradient: [[  3.7796  10.5836 -27.4222 119.8876 561.8904]]\n",
      "Weights: [[-4.2102 -1.0817 -0.1611  0.1152  0.0797]]\n",
      "MSE loss: 171.7537\n",
      "Iteration: 275100\n",
      "Gradient: [[  -6.1899   -0.8994   -8.6257   79.8152 -595.9878]]\n",
      "Weights: [[-4.2113 -1.0816 -0.1612  0.1152  0.0797]]\n",
      "MSE loss: 171.7411\n",
      "Iteration: 275200\n",
      "Gradient: [[  -3.0367   22.2458   14.4155    8.8678 -475.2499]]\n",
      "Weights: [[-4.2111 -1.0814 -0.1613  0.1152  0.0797]]\n",
      "MSE loss: 171.7255\n",
      "Iteration: 275300\n",
      "Gradient: [[ 16.2268 -40.7461  32.9316  40.2138  64.9339]]\n",
      "Weights: [[-4.2103 -1.0814 -0.1614  0.1152  0.0797]]\n",
      "MSE loss: 171.7117\n",
      "Iteration: 275400\n",
      "Gradient: [[ -0.4054 -10.3757  12.5403  21.5083 -47.5983]]\n",
      "Weights: [[-4.2101 -1.0813 -0.1615  0.1152  0.0797]]\n",
      "MSE loss: 171.6956\n",
      "Iteration: 275500\n",
      "Gradient: [[  2.4077  -0.4655  54.9409 -60.2603 308.3376]]\n",
      "Weights: [[-4.2106 -1.0812 -0.1616  0.1152  0.0797]]\n",
      "MSE loss: 171.6815\n",
      "Iteration: 275600\n",
      "Gradient: [[   3.5757  -35.8659    8.181   102.7782 -230.5418]]\n",
      "Weights: [[-4.2097 -1.0812 -0.1617  0.1152  0.0797]]\n",
      "MSE loss: 171.6713\n",
      "Iteration: 275700\n",
      "Gradient: [[  4.5411 -26.9578  34.2871 -10.7396 206.86  ]]\n",
      "Weights: [[-4.2098 -1.0811 -0.1617  0.1152  0.0798]]\n",
      "MSE loss: 171.6572\n",
      "Iteration: 275800\n",
      "Gradient: [[ -12.5798    1.6668  -42.2455  -15.392  -294.6289]]\n",
      "Weights: [[-4.2105 -1.081  -0.1618  0.1152  0.0798]]\n",
      "MSE loss: 171.6466\n",
      "Iteration: 275900\n",
      "Gradient: [[ -1.6859  12.7825 -14.7968 162.5763 290.379 ]]\n",
      "Weights: [[-4.2113 -1.0809 -0.1618  0.1152  0.0798]]\n",
      "MSE loss: 171.6344\n",
      "Iteration: 276000\n",
      "Gradient: [[  -3.9688  -13.5644   18.9533  -81.2225 -250.6502]]\n",
      "Weights: [[-4.2112 -1.0809 -0.1619  0.1152  0.0798]]\n",
      "MSE loss: 171.6196\n",
      "Iteration: 276100\n",
      "Gradient: [[  -4.8584  -11.8736   30.0025  -33.0307 -317.4747]]\n",
      "Weights: [[-4.2118 -1.0806 -0.162   0.1151  0.0798]]\n",
      "MSE loss: 171.6086\n",
      "Iteration: 276200\n",
      "Gradient: [[   1.8172   18.9908   31.0564  -71.2195 -267.1024]]\n",
      "Weights: [[-4.2121 -1.0803 -0.162   0.1151  0.0798]]\n",
      "MSE loss: 171.5914\n",
      "Iteration: 276300\n",
      "Gradient: [[   3.3661   10.0096   51.551  -123.3038 -114.3346]]\n",
      "Weights: [[-4.2125 -1.08   -0.162   0.1151  0.0798]]\n",
      "MSE loss: 171.5798\n",
      "Iteration: 276400\n",
      "Gradient: [[  16.6918   -1.6356    1.6433  -29.9281 -877.1773]]\n",
      "Weights: [[-4.2125 -1.0797 -0.1621  0.1151  0.0798]]\n",
      "MSE loss: 171.5588\n",
      "Iteration: 276500\n",
      "Gradient: [[  0.7797  -4.0675  57.228   64.6735 -44.7073]]\n",
      "Weights: [[-4.2125 -1.0795 -0.1622  0.1151  0.0798]]\n",
      "MSE loss: 171.5438\n",
      "Iteration: 276600\n",
      "Gradient: [[  3.3523   8.603    3.9451 -54.5253  13.7529]]\n",
      "Weights: [[-4.2122 -1.0794 -0.1623  0.1151  0.0798]]\n",
      "MSE loss: 171.5287\n",
      "Iteration: 276700\n",
      "Gradient: [[   1.6013   -4.9494    8.5248 -101.4984   10.2182]]\n",
      "Weights: [[-4.2119 -1.0792 -0.1624  0.1151  0.0798]]\n",
      "MSE loss: 171.5121\n",
      "Iteration: 276800\n",
      "Gradient: [[  8.6381  -1.3679 -19.2038 152.732  154.5602]]\n",
      "Weights: [[-4.2116 -1.079  -0.1625  0.1151  0.0798]]\n",
      "MSE loss: 171.4947\n",
      "Iteration: 276900\n",
      "Gradient: [[  1.3829  -1.5917  56.8407  89.6116 146.5153]]\n",
      "Weights: [[-4.2106 -1.0786 -0.1626  0.1151  0.0798]]\n",
      "MSE loss: 171.4826\n",
      "Iteration: 277000\n",
      "Gradient: [[   3.2332    9.9325    4.018  -121.6828  -20.5181]]\n",
      "Weights: [[-4.2116 -1.0783 -0.1628  0.1151  0.0798]]\n",
      "MSE loss: 171.4565\n",
      "Iteration: 277100\n",
      "Gradient: [[ -5.9602  -3.6    -40.0919 -47.9858 -47.708 ]]\n",
      "Weights: [[-4.2113 -1.078  -0.1629  0.1151  0.0799]]\n",
      "MSE loss: 171.446\n",
      "Iteration: 277200\n",
      "Gradient: [[  -4.509    15.4647   15.3429  -40.6433 -166.4041]]\n",
      "Weights: [[-4.2122 -1.0778 -0.1629  0.1151  0.0799]]\n",
      "MSE loss: 171.4279\n",
      "Iteration: 277300\n",
      "Gradient: [[  10.2186  -17.6749  -39.959    47.7397 -295.2173]]\n",
      "Weights: [[-4.2123 -1.0777 -0.163   0.1151  0.0799]]\n",
      "MSE loss: 171.4146\n",
      "Iteration: 277400\n",
      "Gradient: [[  2.5134   3.7548  -3.7868  62.8703 104.6921]]\n",
      "Weights: [[-4.2123 -1.0777 -0.1631  0.1151  0.0799]]\n",
      "MSE loss: 171.3984\n",
      "Iteration: 277500\n",
      "Gradient: [[  -4.8541   -6.138     8.5993   -6.0771 -143.8127]]\n",
      "Weights: [[-4.2124 -1.0777 -0.1632  0.115   0.0799]]\n",
      "MSE loss: 171.3875\n",
      "Iteration: 277600\n",
      "Gradient: [[ -13.6803   -0.7119  -36.3496    5.3196 -553.9508]]\n",
      "Weights: [[-4.2129 -1.0775 -0.1633  0.115   0.0799]]\n",
      "MSE loss: 171.3733\n",
      "Iteration: 277700\n",
      "Gradient: [[  2.1572  18.5241  38.5717 137.4785  -3.1543]]\n",
      "Weights: [[-4.2127 -1.0775 -0.1634  0.115   0.0799]]\n",
      "MSE loss: 171.3549\n",
      "Iteration: 277800\n",
      "Gradient: [[  6.5944 -16.0293  56.6923  38.4099 -72.5236]]\n",
      "Weights: [[-4.2123 -1.0773 -0.1635  0.115   0.0799]]\n",
      "MSE loss: 171.3467\n",
      "Iteration: 277900\n",
      "Gradient: [[  6.8769  13.2905  -2.7738 -73.3986 -31.9744]]\n",
      "Weights: [[-4.2122 -1.077  -0.1636  0.115   0.0799]]\n",
      "MSE loss: 171.328\n",
      "Iteration: 278000\n",
      "Gradient: [[  -4.4812    4.3523  -48.2149  124.9948 -118.4204]]\n",
      "Weights: [[-4.2126 -1.0768 -0.1637  0.115   0.0799]]\n",
      "MSE loss: 171.3102\n",
      "Iteration: 278100\n",
      "Gradient: [[  -0.6016    1.6978   14.5101   86.0347 -129.2499]]\n",
      "Weights: [[-4.2124 -1.0766 -0.1638  0.115   0.0799]]\n",
      "MSE loss: 171.2952\n",
      "Iteration: 278200\n",
      "Gradient: [[-4.9811  9.1672 11.6196 34.498  20.5529]]\n",
      "Weights: [[-4.2123 -1.0764 -0.1638  0.115   0.0799]]\n",
      "MSE loss: 171.2805\n",
      "Iteration: 278300\n",
      "Gradient: [[ -4.8854   1.8623  -1.5793  81.46   -44.2513]]\n",
      "Weights: [[-4.2124 -1.0764 -0.164   0.115   0.0799]]\n",
      "MSE loss: 171.2654\n",
      "Iteration: 278400\n",
      "Gradient: [[-14.3706  -9.9403  19.1241 102.3504 -79.055 ]]\n",
      "Weights: [[-4.2125 -1.0761 -0.1641  0.115   0.0799]]\n",
      "MSE loss: 171.2472\n",
      "Iteration: 278500\n",
      "Gradient: [[ -17.2929   11.3079   43.3787 -114.0728  -35.1141]]\n",
      "Weights: [[-4.2119 -1.0762 -0.1642  0.115   0.0799]]\n",
      "MSE loss: 171.2332\n",
      "Iteration: 278600\n",
      "Gradient: [[  -5.2631   17.6069   37.3879  -95.4919 -349.9985]]\n",
      "Weights: [[-4.2112 -1.076  -0.1643  0.115   0.08  ]]\n",
      "MSE loss: 171.2174\n",
      "Iteration: 278700\n",
      "Gradient: [[   0.2907    4.2229  -15.6161   15.8676 -187.5583]]\n",
      "Weights: [[-4.2113 -1.0758 -0.1644  0.1149  0.08  ]]\n",
      "MSE loss: 171.1974\n",
      "Iteration: 278800\n",
      "Gradient: [[  -4.4914   -9.238    -6.5923 -163.1798  -14.8217]]\n",
      "Weights: [[-4.2122 -1.0756 -0.1645  0.1149  0.08  ]]\n",
      "MSE loss: 171.1834\n",
      "Iteration: 278900\n",
      "Gradient: [[   0.4937    7.8759  -25.6893   34.9568 -428.0586]]\n",
      "Weights: [[-4.2126 -1.0755 -0.1645  0.115   0.08  ]]\n",
      "MSE loss: 171.1735\n",
      "Iteration: 279000\n",
      "Gradient: [[ -1.8424 -14.3189  31.4851 -43.7165  83.2726]]\n",
      "Weights: [[-4.2124 -1.0756 -0.1646  0.1149  0.08  ]]\n",
      "MSE loss: 171.1641\n",
      "Iteration: 279100\n",
      "Gradient: [[  17.5741   10.7982   46.6295  -64.2738 -255.6733]]\n",
      "Weights: [[-4.2118 -1.0755 -0.1647  0.1149  0.08  ]]\n",
      "MSE loss: 171.1499\n",
      "Iteration: 279200\n",
      "Gradient: [[ 16.4624  -8.1215  11.1874 -86.537  -62.9731]]\n",
      "Weights: [[-4.2111 -1.0754 -0.1648  0.1149  0.08  ]]\n",
      "MSE loss: 171.1392\n",
      "Iteration: 279300\n",
      "Gradient: [[  -2.4237  -14.8724  -38.1606   39.4009 -130.7047]]\n",
      "Weights: [[-4.2115 -1.0751 -0.1649  0.1149  0.08  ]]\n",
      "MSE loss: 171.124\n",
      "Iteration: 279400\n",
      "Gradient: [[-1.217000e-01 -2.258680e+01  3.449720e+01  9.870980e+01 -2.099531e+02]]\n",
      "Weights: [[-4.211  -1.075  -0.165   0.1149  0.08  ]]\n",
      "MSE loss: 171.1139\n",
      "Iteration: 279500\n",
      "Gradient: [[  -3.2175   13.1055  -33.9771  150.3717 -203.4707]]\n",
      "Weights: [[-4.2116 -1.0749 -0.1651  0.1149  0.08  ]]\n",
      "MSE loss: 171.0958\n",
      "Iteration: 279600\n",
      "Gradient: [[  -9.0108   14.4269   16.4398 -126.4978 -105.4649]]\n",
      "Weights: [[-4.2113 -1.0748 -0.1652  0.1149  0.08  ]]\n",
      "MSE loss: 171.0831\n",
      "Iteration: 279700\n",
      "Gradient: [[  3.1731  28.9335 -20.3499 154.7851 -12.0442]]\n",
      "Weights: [[-4.2113 -1.0749 -0.1652  0.1149  0.08  ]]\n",
      "MSE loss: 171.0767\n",
      "Iteration: 279800\n",
      "Gradient: [[  -4.2423   11.5271   -1.3094  109.179  -488.5035]]\n",
      "Weights: [[-4.2116 -1.0747 -0.1653  0.1149  0.08  ]]\n",
      "MSE loss: 171.0626\n",
      "Iteration: 279900\n",
      "Gradient: [[ -3.1777  -9.2258  29.2963 -74.7478 123.7641]]\n",
      "Weights: [[-4.2119 -1.0748 -0.1654  0.1149  0.08  ]]\n",
      "MSE loss: 171.0501\n",
      "Iteration: 280000\n",
      "Gradient: [[ -4.5227  -3.2621  -1.4792  85.9476 279.1567]]\n",
      "Weights: [[-4.2117 -1.0746 -0.1655  0.1149  0.08  ]]\n",
      "MSE loss: 171.0324\n",
      "Iteration: 280100\n",
      "Gradient: [[  -1.1717   19.9592   15.0546  -91.2039 -308.1334]]\n",
      "Weights: [[-4.2128 -1.0743 -0.1656  0.1149  0.0801]]\n",
      "MSE loss: 171.0157\n",
      "Iteration: 280200\n",
      "Gradient: [[  -6.78    -10.6485  -16.3745    8.2659 -183.2087]]\n",
      "Weights: [[-4.2131 -1.0742 -0.1657  0.1149  0.0801]]\n",
      "MSE loss: 170.9975\n",
      "Iteration: 280300\n",
      "Gradient: [[  3.0404  -2.9103  -0.6128 158.6853  26.0815]]\n",
      "Weights: [[-4.2131 -1.074  -0.1658  0.1149  0.0801]]\n",
      "MSE loss: 170.9802\n",
      "Iteration: 280400\n",
      "Gradient: [[ -1.8727 -12.2644 -19.3639 121.0769 -95.9674]]\n",
      "Weights: [[-4.2122 -1.0737 -0.1659  0.1149  0.0801]]\n",
      "MSE loss: 170.962\n",
      "Iteration: 280500\n",
      "Gradient: [[  -4.9508   16.0475   66.6121   33.819  -311.122 ]]\n",
      "Weights: [[-4.2123 -1.0734 -0.166   0.1149  0.0801]]\n",
      "MSE loss: 170.9461\n",
      "Iteration: 280600\n",
      "Gradient: [[ -5.5919   3.3132 -18.3293  31.8356 -69.8228]]\n",
      "Weights: [[-4.2121 -1.0734 -0.166   0.1149  0.0801]]\n",
      "MSE loss: 170.9358\n",
      "Iteration: 280700\n",
      "Gradient: [[   1.1647   -2.8379   10.177   133.8901 -122.6656]]\n",
      "Weights: [[-4.2122 -1.0731 -0.1661  0.1149  0.0801]]\n",
      "MSE loss: 170.9208\n",
      "Iteration: 280800\n",
      "Gradient: [[  13.5342   -6.398    -5.5612    8.2656 -179.9604]]\n",
      "Weights: [[-4.213  -1.0731 -0.1662  0.1149  0.0801]]\n",
      "MSE loss: 170.9056\n",
      "Iteration: 280900\n",
      "Gradient: [[ -0.2824  -3.6862  16.162  -70.4381 -97.6014]]\n",
      "Weights: [[-4.2126 -1.0729 -0.1662  0.1149  0.0801]]\n",
      "MSE loss: 170.8902\n",
      "Iteration: 281000\n",
      "Gradient: [[ -4.169   -7.6702  46.6504  49.6499 184.4909]]\n",
      "Weights: [[-4.2132 -1.0727 -0.1663  0.1149  0.0801]]\n",
      "MSE loss: 170.8754\n",
      "Iteration: 281100\n",
      "Gradient: [[  10.7333   16.3895   13.0647   -7.0725 -500.1293]]\n",
      "Weights: [[-4.2134 -1.0727 -0.1664  0.1149  0.0801]]\n",
      "MSE loss: 170.8692\n",
      "Iteration: 281200\n",
      "Gradient: [[  0.1477  17.5248 -12.6265 -82.4701 -12.175 ]]\n",
      "Weights: [[-4.2151 -1.0726 -0.1664  0.1149  0.0801]]\n",
      "MSE loss: 170.8599\n",
      "Iteration: 281300\n",
      "Gradient: [[12.8513  0.8167 63.5402 56.0786 50.3331]]\n",
      "Weights: [[-4.2146 -1.0722 -0.1665  0.1149  0.0801]]\n",
      "MSE loss: 170.8403\n",
      "Iteration: 281400\n",
      "Gradient: [[  -5.888   -11.6547  -49.8716   23.9371 -421.1428]]\n",
      "Weights: [[-4.2138 -1.072  -0.1666  0.1149  0.0801]]\n",
      "MSE loss: 170.8236\n",
      "Iteration: 281500\n",
      "Gradient: [[  -3.1397   19.8625    2.2445   75.1347 -448.0592]]\n",
      "Weights: [[-4.214  -1.0718 -0.1667  0.1149  0.0802]]\n",
      "MSE loss: 170.8092\n",
      "Iteration: 281600\n",
      "Gradient: [[ -3.3482   4.0905  -5.5706 -35.4388 -72.8939]]\n",
      "Weights: [[-4.2146 -1.0718 -0.1668  0.1149  0.0802]]\n",
      "MSE loss: 170.7967\n",
      "Iteration: 281700\n",
      "Gradient: [[-6.8072 -5.768  39.9635 87.9515 30.6961]]\n",
      "Weights: [[-4.215  -1.0715 -0.1668  0.1149  0.0802]]\n",
      "MSE loss: 170.782\n",
      "Iteration: 281800\n",
      "Gradient: [[   7.2681  -28.0226   62.4526  -11.5623 -191.983 ]]\n",
      "Weights: [[-4.2151 -1.0714 -0.1669  0.1148  0.0802]]\n",
      "MSE loss: 170.77\n",
      "Iteration: 281900\n",
      "Gradient: [[ -11.4527  -16.1418   24.3467  105.0997 -133.7915]]\n",
      "Weights: [[-4.2156 -1.0711 -0.167   0.1148  0.0802]]\n",
      "MSE loss: 170.7549\n",
      "Iteration: 282000\n",
      "Gradient: [[   7.7203   26.55     18.0916  -50.6003 -154.1675]]\n",
      "Weights: [[-4.2148 -1.0711 -0.167   0.1148  0.0802]]\n",
      "MSE loss: 170.7458\n",
      "Iteration: 282100\n",
      "Gradient: [[  1.0719 -16.3693  20.2308 -79.6969 107.7931]]\n",
      "Weights: [[-4.2154 -1.071  -0.1671  0.1148  0.0802]]\n",
      "MSE loss: 170.7355\n",
      "Iteration: 282200\n",
      "Gradient: [[  -1.667    33.6742  -18.0134   -8.3837 -382.6909]]\n",
      "Weights: [[-4.2144 -1.0709 -0.1671  0.1148  0.0802]]\n",
      "MSE loss: 170.7244\n",
      "Iteration: 282300\n",
      "Gradient: [[   0.612    16.6701   47.7397   32.0081 -298.9129]]\n",
      "Weights: [[-4.2147 -1.0708 -0.1672  0.1148  0.0802]]\n",
      "MSE loss: 170.713\n",
      "Iteration: 282400\n",
      "Gradient: [[ -0.2988 -35.5308  -6.3066  43.8141 -41.4908]]\n",
      "Weights: [[-4.2144 -1.0708 -0.1672  0.1148  0.0802]]\n",
      "MSE loss: 170.7024\n",
      "Iteration: 282500\n",
      "Gradient: [[   3.8826    5.2159   11.5137   20.6628 -243.1984]]\n",
      "Weights: [[-4.2149 -1.0707 -0.1673  0.1148  0.0802]]\n",
      "MSE loss: 170.6879\n",
      "Iteration: 282600\n",
      "Gradient: [[  7.1596  -1.7428 -16.9506  84.3351 -24.347 ]]\n",
      "Weights: [[-4.215  -1.0706 -0.1674  0.1148  0.0802]]\n",
      "MSE loss: 170.6781\n",
      "Iteration: 282700\n",
      "Gradient: [[  6.2259  -8.5076  17.1514  -1.8932 -28.9062]]\n",
      "Weights: [[-4.2157 -1.0705 -0.1674  0.1148  0.0802]]\n",
      "MSE loss: 170.6684\n",
      "Iteration: 282800\n",
      "Gradient: [[  6.4198 -16.4625  -1.6831  18.9474 112.3756]]\n",
      "Weights: [[-4.2156 -1.0702 -0.1675  0.1148  0.0802]]\n",
      "MSE loss: 170.6527\n",
      "Iteration: 282900\n",
      "Gradient: [[  -5.5763   18.262    -5.6726   71.2924 -137.0584]]\n",
      "Weights: [[-4.2162 -1.0698 -0.1675  0.1148  0.0803]]\n",
      "MSE loss: 170.639\n",
      "Iteration: 283000\n",
      "Gradient: [[-21.4397  -0.188   31.9246 -74.4673 -42.6887]]\n",
      "Weights: [[-4.2154 -1.0696 -0.1675  0.1148  0.0803]]\n",
      "MSE loss: 170.6272\n",
      "Iteration: 283100\n",
      "Gradient: [[   6.5108    3.178     3.5439  -23.7744 -192.6134]]\n",
      "Weights: [[-4.2151 -1.0693 -0.1676  0.1147  0.0803]]\n",
      "MSE loss: 170.6096\n",
      "Iteration: 283200\n",
      "Gradient: [[   1.8167  -11.4894   45.0198  -28.2732 -431.8676]]\n",
      "Weights: [[-4.2145 -1.0693 -0.1678  0.1147  0.0803]]\n",
      "MSE loss: 170.5925\n",
      "Iteration: 283300\n",
      "Gradient: [[   6.3224  -23.1204   31.3822 -107.0809 -311.1353]]\n",
      "Weights: [[-4.2148 -1.0694 -0.1679  0.1147  0.0803]]\n",
      "MSE loss: 170.576\n",
      "Iteration: 283400\n",
      "Gradient: [[   2.2798    8.0792  -14.1785  117.2901 -270.0831]]\n",
      "Weights: [[-4.2157 -1.0692 -0.1679  0.1147  0.0803]]\n",
      "MSE loss: 170.564\n",
      "Iteration: 283500\n",
      "Gradient: [[   6.0428   -9.4032   58.5117  -26.8475 -406.8017]]\n",
      "Weights: [[-4.2152 -1.069  -0.168   0.1147  0.0803]]\n",
      "MSE loss: 170.5506\n",
      "Iteration: 283600\n",
      "Gradient: [[  9.2344   0.9543  -5.2128  62.3946 197.0238]]\n",
      "Weights: [[-4.2159 -1.069  -0.1681  0.1147  0.0803]]\n",
      "MSE loss: 170.5419\n",
      "Iteration: 283700\n",
      "Gradient: [[   5.3457  -15.5395  -25.161  -128.1088 -220.8178]]\n",
      "Weights: [[-4.2158 -1.0687 -0.1682  0.1147  0.0803]]\n",
      "MSE loss: 170.5226\n",
      "Iteration: 283800\n",
      "Gradient: [[ -15.7442   -8.0482   28.4445   20.7007 -444.8557]]\n",
      "Weights: [[-4.2158 -1.0685 -0.1682  0.1147  0.0803]]\n",
      "MSE loss: 170.5129\n",
      "Iteration: 283900\n",
      "Gradient: [[ 25.4671  -2.7337  42.0747   9.7525 128.4273]]\n",
      "Weights: [[-4.216  -1.0685 -0.1683  0.1147  0.0803]]\n",
      "MSE loss: 170.5002\n",
      "Iteration: 284000\n",
      "Gradient: [[  5.3636 -30.9936  19.7377  55.0466  64.9589]]\n",
      "Weights: [[-4.2144 -1.0683 -0.1684  0.1147  0.0803]]\n",
      "MSE loss: 170.4874\n",
      "Iteration: 284100\n",
      "Gradient: [[  -8.9736   -4.668   -41.2336   45.5792 -122.4679]]\n",
      "Weights: [[-4.2147 -1.0683 -0.1685  0.1147  0.0803]]\n",
      "MSE loss: 170.473\n",
      "Iteration: 284200\n",
      "Gradient: [[   1.6024    2.8985   14.2426   59.1913 -167.6158]]\n",
      "Weights: [[-4.215  -1.068  -0.1686  0.1147  0.0803]]\n",
      "MSE loss: 170.4555\n",
      "Iteration: 284300\n",
      "Gradient: [[  2.3101   8.1056 -16.1769  -9.0054 137.8312]]\n",
      "Weights: [[-4.2157 -1.0679 -0.1687  0.1147  0.0803]]\n",
      "MSE loss: 170.4399\n",
      "Iteration: 284400\n",
      "Gradient: [[  -6.0366   -1.1406    1.7125 -179.6821   75.299 ]]\n",
      "Weights: [[-4.2155 -1.0678 -0.1688  0.1147  0.0803]]\n",
      "MSE loss: 170.4259\n",
      "Iteration: 284500\n",
      "Gradient: [[  -1.6494  -11.0569  -27.5658  -89.2997 -474.5627]]\n",
      "Weights: [[-4.2154 -1.0677 -0.1689  0.1147  0.0804]]\n",
      "MSE loss: 170.4175\n",
      "Iteration: 284600\n",
      "Gradient: [[  -7.0673   22.5429    9.496   -25.7063 -163.921 ]]\n",
      "Weights: [[-4.2149 -1.0677 -0.169   0.1147  0.0804]]\n",
      "MSE loss: 170.4044\n",
      "Iteration: 284700\n",
      "Gradient: [[  4.8023  -5.2202  17.2818 -69.1375 -22.6239]]\n",
      "Weights: [[-4.2152 -1.0676 -0.1691  0.1147  0.0804]]\n",
      "MSE loss: 170.3903\n",
      "Iteration: 284800\n",
      "Gradient: [[  -5.8135  -13.8187    4.4397   62.8815 -218.6187]]\n",
      "Weights: [[-4.2158 -1.0674 -0.1692  0.1147  0.0804]]\n",
      "MSE loss: 170.3699\n",
      "Iteration: 284900\n",
      "Gradient: [[  4.6908 -13.8896   3.99   104.3863 -17.5697]]\n",
      "Weights: [[-4.2162 -1.0674 -0.1693  0.1146  0.0804]]\n",
      "MSE loss: 170.3601\n",
      "Iteration: 285000\n",
      "Gradient: [[  4.8462  17.6681  -4.1457 113.1631 119.77  ]]\n",
      "Weights: [[-4.2158 -1.0673 -0.1693  0.1146  0.0804]]\n",
      "MSE loss: 170.3457\n",
      "Iteration: 285100\n",
      "Gradient: [[ -1.1937 -19.0731 -54.5173  49.6744 117.8595]]\n",
      "Weights: [[-4.2162 -1.067  -0.1694  0.1146  0.0804]]\n",
      "MSE loss: 170.3307\n",
      "Iteration: 285200\n",
      "Gradient: [[ -5.1698  -3.4416 -16.1842  96.5616  32.0914]]\n",
      "Weights: [[-4.2165 -1.0671 -0.1695  0.1146  0.0804]]\n",
      "MSE loss: 170.3209\n",
      "Iteration: 285300\n",
      "Gradient: [[ -0.2643 -10.6216   5.3564 -47.1487 -39.4335]]\n",
      "Weights: [[-4.2148 -1.0671 -0.1695  0.1146  0.0804]]\n",
      "MSE loss: 170.3122\n",
      "Iteration: 285400\n",
      "Gradient: [[  4.6693 -20.7759   5.576   40.2795 -56.8644]]\n",
      "Weights: [[-4.2146 -1.0669 -0.1696  0.1146  0.0804]]\n",
      "MSE loss: 170.2994\n",
      "Iteration: 285500\n",
      "Gradient: [[   2.1149   -7.0163  -35.6795  -51.2093 -104.4834]]\n",
      "Weights: [[-4.216  -1.0669 -0.1697  0.1146  0.0804]]\n",
      "MSE loss: 170.286\n",
      "Iteration: 285600\n",
      "Gradient: [[  -8.5316    7.1125  -18.453    -8.7543 -294.5264]]\n",
      "Weights: [[-4.2166 -1.0667 -0.1698  0.1146  0.0804]]\n",
      "MSE loss: 170.2727\n",
      "Iteration: 285700\n",
      "Gradient: [[ -6.3831 -24.0252  34.7517 -57.2464 292.6107]]\n",
      "Weights: [[-4.2153 -1.0666 -0.1698  0.1146  0.0804]]\n",
      "MSE loss: 170.2615\n",
      "Iteration: 285800\n",
      "Gradient: [[ -6.9192  20.4608  -8.6986 -12.5879 -33.3981]]\n",
      "Weights: [[-4.2141 -1.0666 -0.1698  0.1146  0.0804]]\n",
      "MSE loss: 170.2567\n",
      "Iteration: 285900\n",
      "Gradient: [[   3.1748   -3.3382  -27.8311   44.8271 -102.8289]]\n",
      "Weights: [[-4.2158 -1.0664 -0.17    0.1146  0.0805]]\n",
      "MSE loss: 170.2375\n",
      "Iteration: 286000\n",
      "Gradient: [[   7.0921    2.726   -16.322    15.5294 -444.4776]]\n",
      "Weights: [[-4.2155 -1.0662 -0.17    0.1146  0.0805]]\n",
      "MSE loss: 170.2269\n",
      "Iteration: 286100\n",
      "Gradient: [[ -7.5565   5.8568  38.9765 127.3807  39.3899]]\n",
      "Weights: [[-4.2161 -1.0659 -0.1701  0.1146  0.0805]]\n",
      "MSE loss: 170.2091\n",
      "Iteration: 286200\n",
      "Gradient: [[   3.6241   -9.6731   49.9825  -66.3954 -428.4037]]\n",
      "Weights: [[-4.2157 -1.0658 -0.1702  0.1146  0.0805]]\n",
      "MSE loss: 170.1978\n",
      "Iteration: 286300\n",
      "Gradient: [[   8.0461   -4.9114  -40.0924    5.163  -128.9382]]\n",
      "Weights: [[-4.2154 -1.0655 -0.1703  0.1146  0.0805]]\n",
      "MSE loss: 170.1778\n",
      "Iteration: 286400\n",
      "Gradient: [[  -1.3145   21.1823  -63.4412  -64.9135 -166.9033]]\n",
      "Weights: [[-4.2162 -1.0653 -0.1704  0.1146  0.0805]]\n",
      "MSE loss: 170.1598\n",
      "Iteration: 286500\n",
      "Gradient: [[ -2.9294  -0.3968  42.2234 126.0208 -33.4631]]\n",
      "Weights: [[-4.2151 -1.0651 -0.1704  0.1145  0.0805]]\n",
      "MSE loss: 170.1496\n",
      "Iteration: 286600\n",
      "Gradient: [[ -15.5425   -7.0638   25.5116   68.3518 -139.1058]]\n",
      "Weights: [[-4.2159 -1.065  -0.1705  0.1145  0.0805]]\n",
      "MSE loss: 170.1405\n",
      "Iteration: 286700\n",
      "Gradient: [[  -4.9189   10.5738   25.1098   44.8274 -267.1814]]\n",
      "Weights: [[-4.2171 -1.0648 -0.1706  0.1145  0.0805]]\n",
      "MSE loss: 170.1249\n",
      "Iteration: 286800\n",
      "Gradient: [[ 11.4757 -12.1082 -33.8161 168.4039 -76.0306]]\n",
      "Weights: [[-4.2174 -1.0646 -0.1706  0.1145  0.0805]]\n",
      "MSE loss: 170.1122\n",
      "Iteration: 286900\n",
      "Gradient: [[ -2.3838  18.3109   8.5659  37.9755 -23.6308]]\n",
      "Weights: [[-4.2169 -1.0645 -0.1707  0.1145  0.0805]]\n",
      "MSE loss: 170.1068\n",
      "Iteration: 287000\n",
      "Gradient: [[   3.347     2.5949  -46.9705  -18.3706 -118.6445]]\n",
      "Weights: [[-4.217  -1.0645 -0.1707  0.1145  0.0805]]\n",
      "MSE loss: 170.0969\n",
      "Iteration: 287100\n",
      "Gradient: [[ -4.8295  -6.9271 -43.6469 -29.9324 398.8309]]\n",
      "Weights: [[-4.2168 -1.0644 -0.1708  0.1145  0.0805]]\n",
      "MSE loss: 170.0813\n",
      "Iteration: 287200\n",
      "Gradient: [[  -7.9371   18.0055   34.6312  205.7409 -143.2617]]\n",
      "Weights: [[-4.2167 -1.0645 -0.1709  0.1145  0.0805]]\n",
      "MSE loss: 170.0709\n",
      "Iteration: 287300\n",
      "Gradient: [[ -7.9637   4.7654   4.223   17.9314 148.0022]]\n",
      "Weights: [[-4.2165 -1.0642 -0.171   0.1145  0.0805]]\n",
      "MSE loss: 170.0539\n",
      "Iteration: 287400\n",
      "Gradient: [[  -9.919     3.1957  -12.0517  -24.5211 -411.3665]]\n",
      "Weights: [[-4.2155 -1.0641 -0.171   0.1145  0.0805]]\n",
      "MSE loss: 170.0431\n",
      "Iteration: 287500\n",
      "Gradient: [[ 10.707   -0.6883  35.3667 147.7797 101.401 ]]\n",
      "Weights: [[-4.2151 -1.0639 -0.1711  0.1145  0.0806]]\n",
      "MSE loss: 170.0269\n",
      "Iteration: 287600\n",
      "Gradient: [[  4.2727 -17.5919  -8.6402  69.1189  79.0554]]\n",
      "Weights: [[-4.216  -1.0637 -0.1713  0.1145  0.0806]]\n",
      "MSE loss: 170.0087\n",
      "Iteration: 287700\n",
      "Gradient: [[-14.5876  -4.4497  39.5845  85.1239 -92.5768]]\n",
      "Weights: [[-4.2164 -1.0636 -0.1713  0.1145  0.0806]]\n",
      "MSE loss: 169.9957\n",
      "Iteration: 287800\n",
      "Gradient: [[  6.4662 -14.244   35.4584  79.4035 200.9559]]\n",
      "Weights: [[-4.2173 -1.0634 -0.1714  0.1144  0.0806]]\n",
      "MSE loss: 169.9779\n",
      "Iteration: 287900\n",
      "Gradient: [[  -0.2001   -4.145   -38.5445   -7.941  -195.2226]]\n",
      "Weights: [[-4.2169 -1.0631 -0.1714  0.1144  0.0806]]\n",
      "MSE loss: 169.9658\n",
      "Iteration: 288000\n",
      "Gradient: [[ -10.8976   -6.2763   43.2368  187.5532 -141.297 ]]\n",
      "Weights: [[-4.217  -1.0631 -0.1715  0.1144  0.0806]]\n",
      "MSE loss: 169.9549\n",
      "Iteration: 288100\n",
      "Gradient: [[  2.3922 -10.8564 -11.3842   3.0489 -56.2207]]\n",
      "Weights: [[-4.218  -1.0629 -0.1716  0.1144  0.0806]]\n",
      "MSE loss: 169.9419\n",
      "Iteration: 288200\n",
      "Gradient: [[  6.8222 -13.0027  43.9046  24.6225  52.816 ]]\n",
      "Weights: [[-4.2182 -1.0624 -0.1716  0.1144  0.0806]]\n",
      "MSE loss: 169.9233\n",
      "Iteration: 288300\n",
      "Gradient: [[ -0.5006 -11.1502  76.6593  62.2803  88.9357]]\n",
      "Weights: [[-4.219  -1.0622 -0.1717  0.1144  0.0806]]\n",
      "MSE loss: 169.9096\n",
      "Iteration: 288400\n",
      "Gradient: [[-13.8139   9.8816  -7.8069 -80.4038 -21.9056]]\n",
      "Weights: [[-4.2186 -1.0621 -0.1718  0.1144  0.0806]]\n",
      "MSE loss: 169.8955\n",
      "Iteration: 288500\n",
      "Gradient: [[ -0.9447   3.7212  18.3357 -60.5005  -4.7387]]\n",
      "Weights: [[-4.2185 -1.062  -0.1719  0.1144  0.0806]]\n",
      "MSE loss: 169.882\n",
      "Iteration: 288600\n",
      "Gradient: [[ -5.0764 -16.3263  16.7743 -34.8324 154.3756]]\n",
      "Weights: [[-4.2177 -1.0617 -0.172   0.1144  0.0806]]\n",
      "MSE loss: 169.859\n",
      "Iteration: 288700\n",
      "Gradient: [[ -11.3699  -11.0145   33.7744   18.1476 -182.4304]]\n",
      "Weights: [[-4.2166 -1.0616 -0.1721  0.1143  0.0806]]\n",
      "MSE loss: 169.845\n",
      "Iteration: 288800\n",
      "Gradient: [[  -5.3461    9.3513   36.8311   27.9391 -166.8015]]\n",
      "Weights: [[-4.2162 -1.0615 -0.1722  0.1143  0.0806]]\n",
      "MSE loss: 169.8298\n",
      "Iteration: 288900\n",
      "Gradient: [[  2.9209  14.8659  87.413  -63.8146 -51.4525]]\n",
      "Weights: [[-4.2151 -1.0615 -0.1724  0.1143  0.0807]]\n",
      "MSE loss: 169.8167\n",
      "Iteration: 289000\n",
      "Gradient: [[-1.005640e+01  1.637000e-01  2.355160e+01 -3.027860e+01 -2.847597e+02]]\n",
      "Weights: [[-4.2149 -1.0614 -0.1725  0.1143  0.0807]]\n",
      "MSE loss: 169.8037\n",
      "Iteration: 289100\n",
      "Gradient: [[ 19.5075   4.8628  31.6653  88.6586 104.669 ]]\n",
      "Weights: [[-4.2139 -1.0613 -0.1726  0.1143  0.0807]]\n",
      "MSE loss: 169.7964\n",
      "Iteration: 289200\n",
      "Gradient: [[   3.9609    5.2578   57.3434   -6.6337 -548.3625]]\n",
      "Weights: [[-4.2147 -1.0612 -0.1727  0.1143  0.0807]]\n",
      "MSE loss: 169.7769\n",
      "Iteration: 289300\n",
      "Gradient: [[  -0.5558    1.6928   23.9411 -125.05     -6.2401]]\n",
      "Weights: [[-4.2157 -1.0611 -0.1727  0.1143  0.0807]]\n",
      "MSE loss: 169.7552\n",
      "Iteration: 289400\n",
      "Gradient: [[   4.2364  -19.3502  -19.7327   28.4142 -105.162 ]]\n",
      "Weights: [[-4.2168 -1.0608 -0.1729  0.1143  0.0807]]\n",
      "MSE loss: 169.7331\n",
      "Iteration: 289500\n",
      "Gradient: [[   3.5735  -16.7714  -21.4114  -73.1493 -135.44  ]]\n",
      "Weights: [[-4.2174 -1.0606 -0.1729  0.1143  0.0807]]\n",
      "MSE loss: 169.7207\n",
      "Iteration: 289600\n",
      "Gradient: [[  11.9092  -23.9694  -30.076    72.2958 -290.6044]]\n",
      "Weights: [[-4.2181 -1.0602 -0.173   0.1143  0.0807]]\n",
      "MSE loss: 169.7\n",
      "Iteration: 289700\n",
      "Gradient: [[   3.4008   -5.4761   -7.2489   12.9345 -230.0884]]\n",
      "Weights: [[-4.217  -1.0599 -0.1731  0.1142  0.0807]]\n",
      "MSE loss: 169.683\n",
      "Iteration: 289800\n",
      "Gradient: [[   2.3893   13.8971   15.5352   48.528  -408.4554]]\n",
      "Weights: [[-4.2164 -1.0598 -0.1732  0.1142  0.0807]]\n",
      "MSE loss: 169.669\n",
      "Iteration: 289900\n",
      "Gradient: [[ 14.151   -3.2021  -4.0037  -0.2818 -29.3615]]\n",
      "Weights: [[-4.217  -1.0598 -0.1733  0.1142  0.0807]]\n",
      "MSE loss: 169.656\n",
      "Iteration: 290000\n",
      "Gradient: [[  -1.3521   -8.7542   -4.3767    3.816  -390.3704]]\n",
      "Weights: [[-4.2168 -1.0597 -0.1733  0.1142  0.0807]]\n",
      "MSE loss: 169.6425\n",
      "Iteration: 290100\n",
      "Gradient: [[  0.8907  -7.8021   4.2346 -67.3425 133.4854]]\n",
      "Weights: [[-4.2168 -1.0596 -0.1734  0.1142  0.0807]]\n",
      "MSE loss: 169.6281\n",
      "Iteration: 290200\n",
      "Gradient: [[  1.7967  -5.4345  10.0316 -65.8276 -57.1936]]\n",
      "Weights: [[-4.2167 -1.0593 -0.1735  0.1142  0.0807]]\n",
      "MSE loss: 169.6158\n",
      "Iteration: 290300\n",
      "Gradient: [[  6.7544 -10.4742   0.6236  75.7907 435.1655]]\n",
      "Weights: [[-4.2177 -1.059  -0.1736  0.1142  0.0808]]\n",
      "MSE loss: 169.5946\n",
      "Iteration: 290400\n",
      "Gradient: [[  -5.5858   15.7176  -11.5977  -98.768  -173.8842]]\n",
      "Weights: [[-4.2173 -1.059  -0.1738  0.1142  0.0808]]\n",
      "MSE loss: 169.5803\n",
      "Iteration: 290500\n",
      "Gradient: [[  0.433   -1.1933  33.9279 115.6511  58.781 ]]\n",
      "Weights: [[-4.2168 -1.0587 -0.1738  0.1142  0.0808]]\n",
      "MSE loss: 169.5689\n",
      "Iteration: 290600\n",
      "Gradient: [[-10.445    2.0234  30.0358  26.0768 -88.3762]]\n",
      "Weights: [[-4.2162 -1.0587 -0.1739  0.1142  0.0808]]\n",
      "MSE loss: 169.5613\n",
      "Iteration: 290700\n",
      "Gradient: [[ -0.8816 -14.4013 -23.9711 -69.4023 -13.0592]]\n",
      "Weights: [[-4.2173 -1.0586 -0.1739  0.1142  0.0808]]\n",
      "MSE loss: 169.5471\n",
      "Iteration: 290800\n",
      "Gradient: [[ 10.1145  -7.9849  43.577  -52.3801  45.1505]]\n",
      "Weights: [[-4.2182 -1.0584 -0.1739  0.1142  0.0808]]\n",
      "MSE loss: 169.5318\n",
      "Iteration: 290900\n",
      "Gradient: [[ -9.912   -8.3674  18.844   46.7387 -67.5299]]\n",
      "Weights: [[-4.2184 -1.0582 -0.174   0.1142  0.0808]]\n",
      "MSE loss: 169.5154\n",
      "Iteration: 291000\n",
      "Gradient: [[  3.5258 -50.4401  36.0661  12.9592 -41.8435]]\n",
      "Weights: [[-4.2184 -1.0578 -0.1741  0.1142  0.0808]]\n",
      "MSE loss: 169.499\n",
      "Iteration: 291100\n",
      "Gradient: [[  -2.0738   -5.5147    6.5723  -15.7597 -207.72  ]]\n",
      "Weights: [[-4.2186 -1.0575 -0.1742  0.1142  0.0808]]\n",
      "MSE loss: 169.4825\n",
      "Iteration: 291200\n",
      "Gradient: [[ -3.1541  -9.4091 -21.2669 132.8556 107.1059]]\n",
      "Weights: [[-4.2194 -1.0577 -0.1743  0.1142  0.0808]]\n",
      "MSE loss: 169.4729\n",
      "Iteration: 291300\n",
      "Gradient: [[   3.8832    4.4248  -24.7752  124.6991 -355.4064]]\n",
      "Weights: [[-4.2197 -1.0576 -0.1743  0.1142  0.0808]]\n",
      "MSE loss: 169.4641\n",
      "Iteration: 291400\n",
      "Gradient: [[  -2.2289    1.0062    4.5797   68.4464 -242.998 ]]\n",
      "Weights: [[-4.2201 -1.0573 -0.1744  0.1142  0.0808]]\n",
      "MSE loss: 169.4514\n",
      "Iteration: 291500\n",
      "Gradient: [[  2.8997   7.5069  -9.1031 133.034   60.9212]]\n",
      "Weights: [[-4.2191 -1.0571 -0.1744  0.1141  0.0808]]\n",
      "MSE loss: 169.4357\n",
      "Iteration: 291600\n",
      "Gradient: [[  18.7009   -9.0478    3.5697   38.203  -319.5514]]\n",
      "Weights: [[-4.22   -1.057  -0.1745  0.1141  0.0808]]\n",
      "MSE loss: 169.4275\n",
      "Iteration: 291700\n",
      "Gradient: [[ -4.1173  -5.6034  18.3624  55.7694 178.7947]]\n",
      "Weights: [[-4.2197 -1.0569 -0.1745  0.1141  0.0809]]\n",
      "MSE loss: 169.4114\n",
      "Iteration: 291800\n",
      "Gradient: [[ -6.428    6.2555 -46.6674 -56.4263 -68.462 ]]\n",
      "Weights: [[-4.2178 -1.0566 -0.1747  0.1141  0.0809]]\n",
      "MSE loss: 169.3989\n",
      "Iteration: 291900\n",
      "Gradient: [[ -5.3655  -4.622   50.053   23.9711 240.3949]]\n",
      "Weights: [[-4.2197 -1.0564 -0.1748  0.1141  0.0809]]\n",
      "MSE loss: 169.3775\n",
      "Iteration: 292000\n",
      "Gradient: [[-11.2595  21.6526  78.8412  47.817  -52.0805]]\n",
      "Weights: [[-4.2192 -1.0563 -0.1748  0.1141  0.0809]]\n",
      "MSE loss: 169.3668\n",
      "Iteration: 292100\n",
      "Gradient: [[ -12.8347    5.5977  -16.1801   27.1686 -185.8782]]\n",
      "Weights: [[-4.2185 -1.0561 -0.1749  0.1141  0.0809]]\n",
      "MSE loss: 169.3507\n",
      "Iteration: 292200\n",
      "Gradient: [[  8.708   -8.6801  23.8647 -76.0775 -43.4951]]\n",
      "Weights: [[-4.219  -1.0561 -0.1751  0.1141  0.0809]]\n",
      "MSE loss: 169.3318\n",
      "Iteration: 292300\n",
      "Gradient: [[  -2.3591   20.9844   20.464   -25.2845 -186.9621]]\n",
      "Weights: [[-4.2189 -1.0559 -0.1752  0.1141  0.0809]]\n",
      "MSE loss: 169.3132\n",
      "Iteration: 292400\n",
      "Gradient: [[  9.1635  25.1359 -21.2342  24.8733 -85.0382]]\n",
      "Weights: [[-4.2189 -1.0556 -0.1752  0.1141  0.0809]]\n",
      "MSE loss: 169.2986\n",
      "Iteration: 292500\n",
      "Gradient: [[  17.9165   14.6049   32.9556   24.4801 -225.293 ]]\n",
      "Weights: [[-4.2192 -1.0552 -0.1753  0.114   0.0809]]\n",
      "MSE loss: 169.2823\n",
      "Iteration: 292600\n",
      "Gradient: [[ -4.6988  -5.7792   2.2909  39.1863 136.4923]]\n",
      "Weights: [[-4.2198 -1.0551 -0.1755  0.114   0.0809]]\n",
      "MSE loss: 169.2666\n",
      "Iteration: 292700\n",
      "Gradient: [[  -9.8827   17.6796   10.8455  107.9207 -214.7498]]\n",
      "Weights: [[-4.2197 -1.0551 -0.1755  0.114   0.0809]]\n",
      "MSE loss: 169.2556\n",
      "Iteration: 292800\n",
      "Gradient: [[-15.5918   2.8607   9.4079 104.7127  38.8144]]\n",
      "Weights: [[-4.2202 -1.0548 -0.1756  0.114   0.0809]]\n",
      "MSE loss: 169.2387\n",
      "Iteration: 292900\n",
      "Gradient: [[ -4.4272  -4.1754   4.7451  51.7103 109.1456]]\n",
      "Weights: [[-4.218  -1.0547 -0.1757  0.114   0.0809]]\n",
      "MSE loss: 169.2349\n",
      "Iteration: 293000\n",
      "Gradient: [[ 14.238    0.8613 -18.7573 -32.8821  83.3504]]\n",
      "Weights: [[-4.2188 -1.0547 -0.1758  0.114   0.0809]]\n",
      "MSE loss: 169.2159\n",
      "Iteration: 293100\n",
      "Gradient: [[ 13.0431 -19.1023 -44.6031 -77.0265  57.0056]]\n",
      "Weights: [[-4.2195 -1.0546 -0.1759  0.114   0.0809]]\n",
      "MSE loss: 169.2027\n",
      "Iteration: 293200\n",
      "Gradient: [[-16.6774   6.3667  -1.4565 -80.1499 -49.8559]]\n",
      "Weights: [[-4.2197 -1.0543 -0.176   0.114   0.0809]]\n",
      "MSE loss: 169.1856\n",
      "Iteration: 293300\n",
      "Gradient: [[ 21.5859  -3.2481  26.6798  18.7962 214.2785]]\n",
      "Weights: [[-4.2203 -1.0543 -0.176   0.114   0.081 ]]\n",
      "MSE loss: 169.1766\n",
      "Iteration: 293400\n",
      "Gradient: [[   3.699     8.6421   47.3409   10.7578 -179.4296]]\n",
      "Weights: [[-4.2197 -1.0538 -0.1761  0.114   0.081 ]]\n",
      "MSE loss: 169.1557\n",
      "Iteration: 293500\n",
      "Gradient: [[-3.54000e-02  1.99466e+01  4.66900e+00  6.27171e+01 -3.75428e+02]]\n",
      "Weights: [[-4.219  -1.0537 -0.1762  0.114   0.081 ]]\n",
      "MSE loss: 169.1456\n",
      "Iteration: 293600\n",
      "Gradient: [[ -11.2785  -30.042    -3.6635   95.3445 -367.7237]]\n",
      "Weights: [[-4.2175 -1.0538 -0.1763  0.114   0.081 ]]\n",
      "MSE loss: 169.1404\n",
      "Iteration: 293700\n",
      "Gradient: [[  11.7711  -11.0299   29.5481  -40.8228 -202.9112]]\n",
      "Weights: [[-4.2178 -1.0536 -0.1764  0.114   0.081 ]]\n",
      "MSE loss: 169.1205\n",
      "Iteration: 293800\n",
      "Gradient: [[  -5.7062    7.5782    3.194   100.2317 -277.8226]]\n",
      "Weights: [[-4.2177 -1.0535 -0.1765  0.114   0.081 ]]\n",
      "MSE loss: 169.1094\n",
      "Iteration: 293900\n",
      "Gradient: [[  -4.7654    9.3123  -34.9177   11.5003 -259.2046]]\n",
      "Weights: [[-4.2178 -1.0536 -0.1766  0.1139  0.081 ]]\n",
      "MSE loss: 169.0919\n",
      "Iteration: 294000\n",
      "Gradient: [[ 0.4208 29.7892  9.2074 -9.4427 55.2413]]\n",
      "Weights: [[-4.2183 -1.0534 -0.1767  0.1139  0.081 ]]\n",
      "MSE loss: 169.0727\n",
      "Iteration: 294100\n",
      "Gradient: [[ -19.25      4.5772   57.7071  -68.7736 -326.6887]]\n",
      "Weights: [[-4.2174 -1.0534 -0.1768  0.1139  0.081 ]]\n",
      "MSE loss: 169.0633\n",
      "Iteration: 294200\n",
      "Gradient: [[-19.2749 -19.064    0.5739  12.3345 -27.2082]]\n",
      "Weights: [[-4.2157 -1.0534 -0.1769  0.1139  0.081 ]]\n",
      "MSE loss: 169.064\n",
      "Iteration: 294300\n",
      "Gradient: [[   1.821     4.1738  -35.3742   72.446  -385.752 ]]\n",
      "Weights: [[-4.2164 -1.0532 -0.1771  0.1139  0.081 ]]\n",
      "MSE loss: 169.0372\n",
      "Iteration: 294400\n",
      "Gradient: [[  5.6991 -15.8611  -4.8403  -0.4342 -26.0546]]\n",
      "Weights: [[-4.2172 -1.0531 -0.1771  0.1139  0.081 ]]\n",
      "MSE loss: 169.0151\n",
      "Iteration: 294500\n",
      "Gradient: [[   7.6985  -18.4594   29.3403  104.1915 -298.3117]]\n",
      "Weights: [[-4.2181 -1.0527 -0.1772  0.1139  0.081 ]]\n",
      "MSE loss: 168.9973\n",
      "Iteration: 294600\n",
      "Gradient: [[ -0.4785  10.4918   0.4625 -19.5057 336.9873]]\n",
      "Weights: [[-4.2181 -1.0526 -0.1773  0.1139  0.081 ]]\n",
      "MSE loss: 168.9803\n",
      "Iteration: 294700\n",
      "Gradient: [[   5.3803   -1.9981   57.4365   16.5493 -170.7078]]\n",
      "Weights: [[-4.2172 -1.0525 -0.1774  0.1139  0.0811]]\n",
      "MSE loss: 168.9745\n",
      "Iteration: 294800\n",
      "Gradient: [[ -2.052   10.6655 -47.3162  43.1074   5.9922]]\n",
      "Weights: [[-4.2179 -1.0522 -0.1774  0.1139  0.0811]]\n",
      "MSE loss: 168.9552\n",
      "Iteration: 294900\n",
      "Gradient: [[ -5.9342 -19.634   33.9909  12.1472  90.3722]]\n",
      "Weights: [[-4.2186 -1.0521 -0.1775  0.1139  0.0811]]\n",
      "MSE loss: 168.935\n",
      "Iteration: 295000\n",
      "Gradient: [[  -3.6765    4.5362  -14.1394  -31.8343 -216.2496]]\n",
      "Weights: [[-4.2173 -1.0516 -0.1776  0.1138  0.0811]]\n",
      "MSE loss: 168.9282\n",
      "Iteration: 295100\n",
      "Gradient: [[ -2.2634   4.7398  30.034   58.9276 205.9908]]\n",
      "Weights: [[-4.2182 -1.0515 -0.1777  0.1138  0.0811]]\n",
      "MSE loss: 168.9089\n",
      "Iteration: 295200\n",
      "Gradient: [[-7.7531  0.7193 42.3677 28.1707 13.1757]]\n",
      "Weights: [[-4.2192 -1.0514 -0.1777  0.1138  0.0811]]\n",
      "MSE loss: 168.8909\n",
      "Iteration: 295300\n",
      "Gradient: [[  -5.0393  -11.3681   18.1     -68.6312 -224.2722]]\n",
      "Weights: [[-4.2193 -1.0513 -0.1779  0.1138  0.0811]]\n",
      "MSE loss: 168.8749\n",
      "Iteration: 295400\n",
      "Gradient: [[   0.2493    1.818   -38.05     20.7702 -221.6508]]\n",
      "Weights: [[-4.2193 -1.0511 -0.178   0.1138  0.0811]]\n",
      "MSE loss: 168.8607\n",
      "Iteration: 295500\n",
      "Gradient: [[  -2.1726   -5.0667  -27.9735   81.3151 -201.4344]]\n",
      "Weights: [[-4.2199 -1.0511 -0.178   0.1138  0.0811]]\n",
      "MSE loss: 168.8502\n",
      "Iteration: 295600\n",
      "Gradient: [[   4.1551    7.3229   57.7587   61.1147 -675.732 ]]\n",
      "Weights: [[-4.2203 -1.0511 -0.178   0.1138  0.0811]]\n",
      "MSE loss: 168.8385\n",
      "Iteration: 295700\n",
      "Gradient: [[ 26.0586 -14.397   20.3773 -14.026   -9.5476]]\n",
      "Weights: [[-4.2205 -1.0509 -0.1781  0.1138  0.0811]]\n",
      "MSE loss: 168.8256\n",
      "Iteration: 295800\n",
      "Gradient: [[   1.9699    3.5786   12.3098   -8.3652 -291.4075]]\n",
      "Weights: [[-4.2212 -1.0507 -0.1782  0.1138  0.0811]]\n",
      "MSE loss: 168.8139\n",
      "Iteration: 295900\n",
      "Gradient: [[  18.9252  -26.6224   32.3068   27.7498 -209.0626]]\n",
      "Weights: [[-4.2214 -1.0505 -0.1782  0.1138  0.0811]]\n",
      "MSE loss: 168.7966\n",
      "Iteration: 296000\n",
      "Gradient: [[-15.3482   2.6983  22.5297  34.5421 248.5399]]\n",
      "Weights: [[-4.2205 -1.05   -0.1783  0.1138  0.0812]]\n",
      "MSE loss: 168.7749\n",
      "Iteration: 296100\n",
      "Gradient: [[ 9.029200e+00 -9.509100e+00  1.746000e-01 -4.587070e+01 -3.485058e+02]]\n",
      "Weights: [[-4.2209 -1.0497 -0.1784  0.1138  0.0812]]\n",
      "MSE loss: 168.7564\n",
      "Iteration: 296200\n",
      "Gradient: [[  -0.4273   -6.4363   20.5164    9.1758 -149.7437]]\n",
      "Weights: [[-4.2222 -1.0495 -0.1785  0.1138  0.0812]]\n",
      "MSE loss: 168.7434\n",
      "Iteration: 296300\n",
      "Gradient: [[  -7.582     2.8202   15.417   128.6226 -199.4806]]\n",
      "Weights: [[-4.2228 -1.0494 -0.1785  0.1138  0.0812]]\n",
      "MSE loss: 168.7321\n",
      "Iteration: 296400\n",
      "Gradient: [[ -12.6312   -2.7296  -22.9823   -8.0389 -236.1661]]\n",
      "Weights: [[-4.2238 -1.0491 -0.1786  0.1138  0.0812]]\n",
      "MSE loss: 168.7224\n",
      "Iteration: 296500\n",
      "Gradient: [[ 0.5689  5.4882 19.0942 45.6942 74.2251]]\n",
      "Weights: [[-4.2234 -1.0491 -0.1787  0.1138  0.0812]]\n",
      "MSE loss: 168.7107\n",
      "Iteration: 296600\n",
      "Gradient: [[-12.5264   6.796   43.257   -2.9595 -70.0458]]\n",
      "Weights: [[-4.2222 -1.0491 -0.1787  0.1138  0.0812]]\n",
      "MSE loss: 168.6962\n",
      "Iteration: 296700\n",
      "Gradient: [[  -5.62     20.6505    7.9713  -81.7638 -142.2254]]\n",
      "Weights: [[-4.2217 -1.0491 -0.1789  0.1138  0.0812]]\n",
      "MSE loss: 168.6806\n",
      "Iteration: 296800\n",
      "Gradient: [[  -2.0019    7.2867  -31.3803  -94.0497 -469.0064]]\n",
      "Weights: [[-4.222  -1.0488 -0.1789  0.1138  0.0812]]\n",
      "MSE loss: 168.6651\n",
      "Iteration: 296900\n",
      "Gradient: [[  -1.8602    6.2114   66.0207   -9.5369 -481.044 ]]\n",
      "Weights: [[-4.2208 -1.0485 -0.1791  0.1138  0.0812]]\n",
      "MSE loss: 168.6462\n",
      "Iteration: 297000\n",
      "Gradient: [[   0.4973   -0.9103  -11.0681   -7.0806 -283.7796]]\n",
      "Weights: [[-4.2212 -1.0482 -0.1792  0.1137  0.0812]]\n",
      "MSE loss: 168.626\n",
      "Iteration: 297100\n",
      "Gradient: [[ -6.2895   4.5232  18.1108  40.7374 -11.6169]]\n",
      "Weights: [[-4.222  -1.0483 -0.1793  0.1137  0.0812]]\n",
      "MSE loss: 168.6118\n",
      "Iteration: 297200\n",
      "Gradient: [[ -3.4783  19.8094 -16.0475  96.9998 -30.319 ]]\n",
      "Weights: [[-4.2217 -1.0484 -0.1794  0.1137  0.0812]]\n",
      "MSE loss: 168.5998\n",
      "Iteration: 297300\n",
      "Gradient: [[  9.9516  26.6081  -0.994  -12.8295 -87.1129]]\n",
      "Weights: [[-4.2214 -1.0484 -0.1794  0.1137  0.0813]]\n",
      "MSE loss: 168.5881\n",
      "Iteration: 297400\n",
      "Gradient: [[  9.6966   1.6104  41.1782 -18.4033  28.2067]]\n",
      "Weights: [[-4.221  -1.0481 -0.1795  0.1137  0.0813]]\n",
      "MSE loss: 168.5723\n",
      "Iteration: 297500\n",
      "Gradient: [[ -2.2663  15.5497  33.4278  56.9877 -14.6642]]\n",
      "Weights: [[-4.2226 -1.0479 -0.1796  0.1137  0.0813]]\n",
      "MSE loss: 168.5603\n",
      "Iteration: 297600\n",
      "Gradient: [[ -0.7973  20.3185 -26.6109  15.1185 -58.2345]]\n",
      "Weights: [[-4.2227 -1.0474 -0.1797  0.1137  0.0813]]\n",
      "MSE loss: 168.5378\n",
      "Iteration: 297700\n",
      "Gradient: [[  1.1455 -15.8355  15.2918  15.1743 -60.3181]]\n",
      "Weights: [[-4.223  -1.047  -0.1798  0.1137  0.0813]]\n",
      "MSE loss: 168.5212\n",
      "Iteration: 297800\n",
      "Gradient: [[   7.8935   -2.5235  -23.8848  -82.968  -405.3687]]\n",
      "Weights: [[-4.2232 -1.0467 -0.1798  0.1137  0.0813]]\n",
      "MSE loss: 168.5025\n",
      "Iteration: 297900\n",
      "Gradient: [[   0.1668   -6.1562   17.7292  -54.0124 -136.2758]]\n",
      "Weights: [[-4.2229 -1.0464 -0.1799  0.1137  0.0813]]\n",
      "MSE loss: 168.4851\n",
      "Iteration: 298000\n",
      "Gradient: [[ -4.9721   3.8886  -6.6931 -89.652  101.5708]]\n",
      "Weights: [[-4.2224 -1.0463 -0.18    0.1137  0.0813]]\n",
      "MSE loss: 168.4751\n",
      "Iteration: 298100\n",
      "Gradient: [[  -1.2869  -14.0599    0.8652   60.6849 -183.3859]]\n",
      "Weights: [[-4.2227 -1.0462 -0.1801  0.1137  0.0813]]\n",
      "MSE loss: 168.4643\n",
      "Iteration: 298200\n",
      "Gradient: [[  -7.6741    0.2626   41.8143  -90.013  -150.0348]]\n",
      "Weights: [[-4.2222 -1.046  -0.1802  0.1137  0.0813]]\n",
      "MSE loss: 168.4503\n",
      "Iteration: 298300\n",
      "Gradient: [[-12.3597  -5.7821  20.8101 129.3124 176.2702]]\n",
      "Weights: [[-4.2225 -1.0459 -0.1803  0.1137  0.0813]]\n",
      "MSE loss: 168.4343\n",
      "Iteration: 298400\n",
      "Gradient: [[   0.4078   -1.3605    3.8089   87.1173 -172.936 ]]\n",
      "Weights: [[-4.2231 -1.0457 -0.1804  0.1136  0.0813]]\n",
      "MSE loss: 168.4156\n",
      "Iteration: 298500\n",
      "Gradient: [[ -5.9042 -13.1994  67.5608  -1.9756  -9.0788]]\n",
      "Weights: [[-4.2233 -1.0457 -0.1805  0.1136  0.0813]]\n",
      "MSE loss: 168.4001\n",
      "Iteration: 298600\n",
      "Gradient: [[   2.5282   22.151     8.9713  -76.2724 -166.2562]]\n",
      "Weights: [[-4.2252 -1.0453 -0.1806  0.1136  0.0813]]\n",
      "MSE loss: 168.3924\n",
      "Iteration: 298700\n",
      "Gradient: [[  -0.2484    5.2529   42.6535  130.264  -177.5298]]\n",
      "Weights: [[-4.2257 -1.0451 -0.1807  0.1137  0.0813]]\n",
      "MSE loss: 168.3779\n",
      "Iteration: 298800\n",
      "Gradient: [[   2.0902   14.0976   11.6126  -65.9974 -216.0729]]\n",
      "Weights: [[-4.2248 -1.0449 -0.1807  0.1136  0.0814]]\n",
      "MSE loss: 168.3578\n",
      "Iteration: 298900\n",
      "Gradient: [[  -3.0604   18.1654   18.4144   48.2205 -488.8499]]\n",
      "Weights: [[-4.2234 -1.0446 -0.1808  0.1136  0.0814]]\n",
      "MSE loss: 168.3399\n",
      "Iteration: 299000\n",
      "Gradient: [[  1.0976  12.7537 -12.5558  -2.0617  92.5871]]\n",
      "Weights: [[-4.2228 -1.0445 -0.1809  0.1136  0.0814]]\n",
      "MSE loss: 168.3257\n",
      "Iteration: 299100\n",
      "Gradient: [[ -3.6851  -2.7502  41.3695 145.4354  83.617 ]]\n",
      "Weights: [[-4.2229 -1.0443 -0.181   0.1136  0.0814]]\n",
      "MSE loss: 168.3114\n",
      "Iteration: 299200\n",
      "Gradient: [[  -4.2982    6.7298   58.9618   18.5276 -102.0687]]\n",
      "Weights: [[-4.223  -1.0442 -0.1811  0.1136  0.0814]]\n",
      "MSE loss: 168.2985\n",
      "Iteration: 299300\n",
      "Gradient: [[  16.2405   -8.6339   -7.7607  -31.8223 -284.0457]]\n",
      "Weights: [[-4.2229 -1.0441 -0.1812  0.1136  0.0814]]\n",
      "MSE loss: 168.2902\n",
      "Iteration: 299400\n",
      "Gradient: [[  -6.3656   14.7268   18.7887 -172.0333  -25.6553]]\n",
      "Weights: [[-4.2238 -1.0439 -0.1812  0.1136  0.0814]]\n",
      "MSE loss: 168.278\n",
      "Iteration: 299500\n",
      "Gradient: [[  17.4885   -1.3129    8.013  -109.5523 -211.7504]]\n",
      "Weights: [[-4.2242 -1.0437 -0.1813  0.1136  0.0814]]\n",
      "MSE loss: 168.2662\n",
      "Iteration: 299600\n",
      "Gradient: [[12.3489  4.7645 10.6246 45.0184 83.4508]]\n",
      "Weights: [[-4.2237 -1.0435 -0.1814  0.1136  0.0814]]\n",
      "MSE loss: 168.2537\n",
      "Iteration: 299700\n",
      "Gradient: [[ -21.7195   21.5605   29.4767   19.1178 -414.2386]]\n",
      "Weights: [[-4.2236 -1.0434 -0.1814  0.1136  0.0814]]\n",
      "MSE loss: 168.2395\n",
      "Iteration: 299800\n",
      "Gradient: [[ -4.1331   5.159   28.7954  90.0496 -83.8158]]\n",
      "Weights: [[-4.2246 -1.0431 -0.1815  0.1136  0.0814]]\n",
      "MSE loss: 168.2253\n",
      "Iteration: 299900\n",
      "Gradient: [[  14.7063  -28.3319   -2.4241   44.9218 -291.1031]]\n",
      "Weights: [[-4.2242 -1.043  -0.1816  0.1136  0.0814]]\n",
      "MSE loss: 168.2107\n",
      "Iteration: 300000\n",
      "Gradient: [[  13.2138   12.7962   -0.7349   -9.7942 -283.4637]]\n",
      "Weights: [[-4.2249 -1.0427 -0.1817  0.1135  0.0814]]\n",
      "MSE loss: 168.195\n",
      "Iteration: 300100\n",
      "Gradient: [[-15.3686   0.4765 -31.0331  80.9264 176.7189]]\n",
      "Weights: [[-4.2246 -1.0426 -0.1818  0.1135  0.0814]]\n",
      "MSE loss: 168.1846\n",
      "Iteration: 300200\n",
      "Gradient: [[-10.1396   3.7259  58.1729  65.9356 225.9243]]\n",
      "Weights: [[-4.2251 -1.0423 -0.1818  0.1135  0.0814]]\n",
      "MSE loss: 168.1749\n",
      "Iteration: 300300\n",
      "Gradient: [[  1.8844  -1.2708  22.7563 -24.4884  -3.7055]]\n",
      "Weights: [[-4.2245 -1.0422 -0.1819  0.1135  0.0814]]\n",
      "MSE loss: 168.1539\n",
      "Iteration: 300400\n",
      "Gradient: [[ -7.7728  -2.1584  31.8633  23.1611 -41.9236]]\n",
      "Weights: [[-4.2241 -1.0419 -0.182   0.1135  0.0815]]\n",
      "MSE loss: 168.1379\n",
      "Iteration: 300500\n",
      "Gradient: [[ -3.6741  -7.7135  27.8941 115.3307  83.563 ]]\n",
      "Weights: [[-4.2247 -1.0419 -0.1821  0.1135  0.0815]]\n",
      "MSE loss: 168.1243\n",
      "Iteration: 300600\n",
      "Gradient: [[  6.1011   3.58    17.7611 -66.2461 -62.6391]]\n",
      "Weights: [[-4.2253 -1.0418 -0.1821  0.1135  0.0815]]\n",
      "MSE loss: 168.1126\n",
      "Iteration: 300700\n",
      "Gradient: [[  3.0814  12.5961 -32.6045  51.9412 371.8261]]\n",
      "Weights: [[-4.2248 -1.0416 -0.1821  0.1135  0.0815]]\n",
      "MSE loss: 168.1061\n",
      "Iteration: 300800\n",
      "Gradient: [[  1.2488   9.1621   5.7061  59.3997 -69.7749]]\n",
      "Weights: [[-4.2248 -1.0416 -0.1822  0.1135  0.0815]]\n",
      "MSE loss: 168.0936\n",
      "Iteration: 300900\n",
      "Gradient: [[  -0.9394    2.9154    6.7779   64.4912 -217.9411]]\n",
      "Weights: [[-4.2252 -1.0413 -0.1823  0.1135  0.0815]]\n",
      "MSE loss: 168.0808\n",
      "Iteration: 301000\n",
      "Gradient: [[  -8.7909   -4.6107   34.1112  -15.7796 -574.6671]]\n",
      "Weights: [[-4.2258 -1.0413 -0.1823  0.1135  0.0815]]\n",
      "MSE loss: 168.0713\n",
      "Iteration: 301100\n",
      "Gradient: [[  -5.6334  -18.3963   45.9921   10.5808 -144.454 ]]\n",
      "Weights: [[-4.2261 -1.0412 -0.1824  0.1135  0.0815]]\n",
      "MSE loss: 168.0596\n",
      "Iteration: 301200\n",
      "Gradient: [[ -8.6497  13.1321 -60.3328 -43.7112 -10.4852]]\n",
      "Weights: [[-4.2268 -1.0409 -0.1825  0.1135  0.0815]]\n",
      "MSE loss: 168.0461\n",
      "Iteration: 301300\n",
      "Gradient: [[  -5.5105    7.2407   29.6414 -207.628    -4.7866]]\n",
      "Weights: [[-4.2271 -1.0407 -0.1825  0.1135  0.0815]]\n",
      "MSE loss: 168.0338\n",
      "Iteration: 301400\n",
      "Gradient: [[-3.1549 14.5085 21.5942 67.3147 15.8527]]\n",
      "Weights: [[-4.2262 -1.0407 -0.1826  0.1134  0.0815]]\n",
      "MSE loss: 168.0164\n",
      "Iteration: 301500\n",
      "Gradient: [[   4.4915   -8.2478   69.9385  100.7973 -230.6551]]\n",
      "Weights: [[-4.2259 -1.0405 -0.1827  0.1134  0.0815]]\n",
      "MSE loss: 168.0026\n",
      "Iteration: 301600\n",
      "Gradient: [[  -5.6012   -1.0572   27.1284   40.4272 -193.8384]]\n",
      "Weights: [[-4.2248 -1.0405 -0.1828  0.1134  0.0815]]\n",
      "MSE loss: 167.991\n",
      "Iteration: 301700\n",
      "Gradient: [[  0.3598 -10.4275  25.3816  40.897  167.4158]]\n",
      "Weights: [[-4.2244 -1.0403 -0.1829  0.1134  0.0815]]\n",
      "MSE loss: 167.9788\n",
      "Iteration: 301800\n",
      "Gradient: [[  -5.3156    7.0369    6.8755   12.7221 -156.1304]]\n",
      "Weights: [[-4.2233 -1.0401 -0.183   0.1134  0.0815]]\n",
      "MSE loss: 167.97\n",
      "Iteration: 301900\n",
      "Gradient: [[13.875  -8.9981  5.9762 61.0552 -4.8595]]\n",
      "Weights: [[-4.2241 -1.0401 -0.1831  0.1134  0.0816]]\n",
      "MSE loss: 167.9507\n",
      "Iteration: 302000\n",
      "Gradient: [[   2.3272    6.4463   24.6025   19.4824 -254.9224]]\n",
      "Weights: [[-4.2245 -1.0402 -0.1832  0.1134  0.0816]]\n",
      "MSE loss: 167.9352\n",
      "Iteration: 302100\n",
      "Gradient: [[   9.6021   -1.4809   23.2253 -149.2746  147.9985]]\n",
      "Weights: [[-4.2249 -1.04   -0.1832  0.1134  0.0816]]\n",
      "MSE loss: 167.922\n",
      "Iteration: 302200\n",
      "Gradient: [[  -4.7939  -35.5352  -18.6791   13.2229 -116.5659]]\n",
      "Weights: [[-4.2258 -1.0398 -0.1833  0.1134  0.0816]]\n",
      "MSE loss: 167.9068\n",
      "Iteration: 302300\n",
      "Gradient: [[   1.5973  -15.3845   18.9258   10.2533 -344.8879]]\n",
      "Weights: [[-4.2244 -1.0398 -0.1834  0.1134  0.0816]]\n",
      "MSE loss: 167.8997\n",
      "Iteration: 302400\n",
      "Gradient: [[ 11.3952   5.4091  62.1773  29.1871 -56.2967]]\n",
      "Weights: [[-4.2243 -1.0397 -0.1834  0.1134  0.0816]]\n",
      "MSE loss: 167.8905\n",
      "Iteration: 302500\n",
      "Gradient: [[ -3.6869   1.7433 -18.491  -21.2496  -6.2017]]\n",
      "Weights: [[-4.2246 -1.0396 -0.1835  0.1133  0.0816]]\n",
      "MSE loss: 167.8734\n",
      "Iteration: 302600\n",
      "Gradient: [[ -1.7319  -2.0333  43.6672  36.3483 102.8625]]\n",
      "Weights: [[-4.2244 -1.0393 -0.1836  0.1133  0.0816]]\n",
      "MSE loss: 167.8576\n",
      "Iteration: 302700\n",
      "Gradient: [[  -3.652    13.2242   24.7221  -73.2868 -454.8363]]\n",
      "Weights: [[-4.2247 -1.0392 -0.1837  0.1133  0.0816]]\n",
      "MSE loss: 167.8467\n",
      "Iteration: 302800\n",
      "Gradient: [[ 10.3998   6.2388  -9.8835  73.547  269.1586]]\n",
      "Weights: [[-4.2245 -1.0393 -0.1838  0.1133  0.0816]]\n",
      "MSE loss: 167.838\n",
      "Iteration: 302900\n",
      "Gradient: [[   6.1073    8.4658   33.2548   -6.8271 -269.8633]]\n",
      "Weights: [[-4.2251 -1.0391 -0.1839  0.1133  0.0816]]\n",
      "MSE loss: 167.8259\n",
      "Iteration: 303000\n",
      "Gradient: [[  14.0839   -7.3337   12.7461   98.2652 -121.0569]]\n",
      "Weights: [[-4.2251 -1.0388 -0.1839  0.1133  0.0816]]\n",
      "MSE loss: 167.8094\n",
      "Iteration: 303100\n",
      "Gradient: [[  -5.1647   -5.148   -24.906    19.1312 -278.6624]]\n",
      "Weights: [[-4.2244 -1.0387 -0.184   0.1133  0.0816]]\n",
      "MSE loss: 167.7959\n",
      "Iteration: 303200\n",
      "Gradient: [[  4.0037  22.2186 -54.8013  91.3142 176.7019]]\n",
      "Weights: [[-4.2242 -1.0386 -0.1841  0.1133  0.0816]]\n",
      "MSE loss: 167.7898\n",
      "Iteration: 303300\n",
      "Gradient: [[   4.518    16.446    48.7328   28.8096 -117.036 ]]\n",
      "Weights: [[-4.225  -1.0386 -0.1841  0.1133  0.0816]]\n",
      "MSE loss: 167.7787\n",
      "Iteration: 303400\n",
      "Gradient: [[   4.2487  -12.2434  -66.59     41.8    -112.9334]]\n",
      "Weights: [[-4.2252 -1.0385 -0.1842  0.1133  0.0816]]\n",
      "MSE loss: 167.7643\n",
      "Iteration: 303500\n",
      "Gradient: [[ -2.4238  23.3727   4.4598 -64.9801   2.2979]]\n",
      "Weights: [[-4.2251 -1.0382 -0.1842  0.1133  0.0817]]\n",
      "MSE loss: 167.7486\n",
      "Iteration: 303600\n",
      "Gradient: [[  -6.8916   -2.2496   41.1943   -1.8245 -347.7759]]\n",
      "Weights: [[-4.225  -1.0381 -0.1843  0.1133  0.0817]]\n",
      "MSE loss: 167.7372\n",
      "Iteration: 303700\n",
      "Gradient: [[  4.6766  19.3992  -2.2879  84.7474 111.9388]]\n",
      "Weights: [[-4.2261 -1.0378 -0.1844  0.1132  0.0817]]\n",
      "MSE loss: 167.7207\n",
      "Iteration: 303800\n",
      "Gradient: [[ 2.3303  1.014  27.9403  8.7962 63.2688]]\n",
      "Weights: [[-4.2254 -1.0375 -0.1845  0.1132  0.0817]]\n",
      "MSE loss: 167.7054\n",
      "Iteration: 303900\n",
      "Gradient: [[ 11.2      2.448   32.7038 166.8244  38.2891]]\n",
      "Weights: [[-4.2248 -1.0375 -0.1846  0.1132  0.0817]]\n",
      "MSE loss: 167.6941\n",
      "Iteration: 304000\n",
      "Gradient: [[  19.6215  -18.9413   27.5082  103.3483 -218.3779]]\n",
      "Weights: [[-4.2246 -1.0372 -0.1846  0.1132  0.0817]]\n",
      "MSE loss: 167.68\n",
      "Iteration: 304100\n",
      "Gradient: [[ -7.1191  -2.0363  -2.2654 -86.4422 185.8272]]\n",
      "Weights: [[-4.2255 -1.0369 -0.1847  0.1132  0.0817]]\n",
      "MSE loss: 167.6606\n",
      "Iteration: 304200\n",
      "Gradient: [[ 16.6833 -31.0261  59.6565  28.6665 -26.2844]]\n",
      "Weights: [[-4.2265 -1.0367 -0.1848  0.1132  0.0817]]\n",
      "MSE loss: 167.6471\n",
      "Iteration: 304300\n",
      "Gradient: [[  18.4794    7.9932    6.2747   -6.4508 -261.4558]]\n",
      "Weights: [[-4.2261 -1.0367 -0.1849  0.1132  0.0817]]\n",
      "MSE loss: 167.6363\n",
      "Iteration: 304400\n",
      "Gradient: [[ -11.9753  -10.7018   38.8696  129.3493 -308.7445]]\n",
      "Weights: [[-4.2262 -1.0365 -0.1849  0.1132  0.0817]]\n",
      "MSE loss: 167.6244\n",
      "Iteration: 304500\n",
      "Gradient: [[-6.4069  6.4888 33.4782 84.6604 91.3568]]\n",
      "Weights: [[-4.2262 -1.0362 -0.185   0.1132  0.0817]]\n",
      "MSE loss: 167.606\n",
      "Iteration: 304600\n",
      "Gradient: [[   4.2787    4.3516   50.6533 -138.9065   35.2495]]\n",
      "Weights: [[-4.2268 -1.036  -0.1851  0.1132  0.0817]]\n",
      "MSE loss: 167.5908\n",
      "Iteration: 304700\n",
      "Gradient: [[ -7.4252 -10.2558 -18.0058 -10.6149 -50.9711]]\n",
      "Weights: [[-4.2272 -1.0358 -0.1851  0.1132  0.0817]]\n",
      "MSE loss: 167.5803\n",
      "Iteration: 304800\n",
      "Gradient: [[ 11.7356  -3.4002  -2.7742 -31.4009 -75.0993]]\n",
      "Weights: [[-4.2258 -1.0358 -0.1851  0.1132  0.0817]]\n",
      "MSE loss: 167.576\n",
      "Iteration: 304900\n",
      "Gradient: [[  4.6538  -5.1316 -37.2855 -47.7202 -69.3986]]\n",
      "Weights: [[-4.2244 -1.0357 -0.1852  0.1131  0.0817]]\n",
      "MSE loss: 167.571\n",
      "Iteration: 305000\n",
      "Gradient: [[   3.4757   25.0624   13.198   -20.1695 -509.7979]]\n",
      "Weights: [[-4.2258 -1.0355 -0.1854  0.1131  0.0818]]\n",
      "MSE loss: 167.5423\n",
      "Iteration: 305100\n",
      "Gradient: [[ -1.7143  -4.9232  42.2594 -44.1561 -88.6232]]\n",
      "Weights: [[-4.2266 -1.0354 -0.1855  0.1131  0.0818]]\n",
      "MSE loss: 167.5217\n",
      "Iteration: 305200\n",
      "Gradient: [[-13.9021  16.4051  -4.2126  22.6799 260.0817]]\n",
      "Weights: [[-4.2267 -1.0351 -0.1856  0.1131  0.0818]]\n",
      "MSE loss: 167.5065\n",
      "Iteration: 305300\n",
      "Gradient: [[  -4.071    -4.5044  -14.5837  153.0266 -253.1967]]\n",
      "Weights: [[-4.2281 -1.0349 -0.1857  0.1131  0.0818]]\n",
      "MSE loss: 167.4955\n",
      "Iteration: 305400\n",
      "Gradient: [[-10.1217  10.0881  18.5473 -66.188  -37.797 ]]\n",
      "Weights: [[-4.2282 -1.0346 -0.1858  0.1131  0.0818]]\n",
      "MSE loss: 167.4811\n",
      "Iteration: 305500\n",
      "Gradient: [[  -5.4479   -5.1195  -26.0651   20.8184 -343.1467]]\n",
      "Weights: [[-4.2281 -1.0345 -0.1859  0.1132  0.0818]]\n",
      "MSE loss: 167.4642\n",
      "Iteration: 305600\n",
      "Gradient: [[-4.8538  8.8277  3.6331 17.3625 54.8507]]\n",
      "Weights: [[-4.2274 -1.0345 -0.186   0.1131  0.0818]]\n",
      "MSE loss: 167.4499\n",
      "Iteration: 305700\n",
      "Gradient: [[-3.60280e+00  1.66000e-02 -5.13505e+01  2.28054e+01  4.18165e+01]]\n",
      "Weights: [[-4.2274 -1.0344 -0.1861  0.1131  0.0818]]\n",
      "MSE loss: 167.437\n",
      "Iteration: 305800\n",
      "Gradient: [[  11.9434  -23.3973    6.9079  194.9936 -290.4082]]\n",
      "Weights: [[-4.2276 -1.0342 -0.1862  0.1131  0.0818]]\n",
      "MSE loss: 167.4195\n",
      "Iteration: 305900\n",
      "Gradient: [[   4.5426    8.0552   18.6808 -137.4237   56.1707]]\n",
      "Weights: [[-4.2272 -1.034  -0.1863  0.1131  0.0818]]\n",
      "MSE loss: 167.404\n",
      "Iteration: 306000\n",
      "Gradient: [[  15.7527    9.7449   32.7138   30.4872 -199.6975]]\n",
      "Weights: [[-4.2265 -1.0341 -0.1863  0.1131  0.0818]]\n",
      "MSE loss: 167.3929\n",
      "Iteration: 306100\n",
      "Gradient: [[-1.389100e+00 -5.325100e+00  9.532700e+00  2.696000e-01 -2.865616e+02]]\n",
      "Weights: [[-4.2256 -1.0342 -0.1864  0.1131  0.0818]]\n",
      "MSE loss: 167.3893\n",
      "Iteration: 306200\n",
      "Gradient: [[ 12.4829  -1.8633 -27.7824   6.4417 -40.475 ]]\n",
      "Weights: [[-4.2252 -1.034  -0.1864  0.1131  0.0818]]\n",
      "MSE loss: 167.3809\n",
      "Iteration: 306300\n",
      "Gradient: [[-7.9545  0.0697  5.2214 25.1159  2.047 ]]\n",
      "Weights: [[-4.226  -1.0339 -0.1865  0.1131  0.0818]]\n",
      "MSE loss: 167.3647\n",
      "Iteration: 306400\n",
      "Gradient: [[ -5.7761  12.1294 -38.3496 112.3859  73.5939]]\n",
      "Weights: [[-4.2264 -1.034  -0.1866  0.1131  0.0818]]\n",
      "MSE loss: 167.3511\n",
      "Iteration: 306500\n",
      "Gradient: [[  11.9538  -34.6049   52.6145   46.05   -230.8578]]\n",
      "Weights: [[-4.2253 -1.0338 -0.1867  0.1131  0.0819]]\n",
      "MSE loss: 167.3396\n",
      "Iteration: 306600\n",
      "Gradient: [[  -8.5323   -8.9604  -61.0578   37.2837 -177.9089]]\n",
      "Weights: [[-4.226  -1.0336 -0.1868  0.113   0.0819]]\n",
      "MSE loss: 167.3227\n",
      "Iteration: 306700\n",
      "Gradient: [[ -3.2163 -14.1954 -26.7663 -53.2864 250.073 ]]\n",
      "Weights: [[-4.226  -1.0332 -0.1869  0.113   0.0819]]\n",
      "MSE loss: 167.3038\n",
      "Iteration: 306800\n",
      "Gradient: [[  3.1388 -16.042  -23.4539 -54.1929  66.2857]]\n",
      "Weights: [[-4.2261 -1.0332 -0.187   0.113   0.0819]]\n",
      "MSE loss: 167.291\n",
      "Iteration: 306900\n",
      "Gradient: [[ -5.312   -3.7678  33.9535 -80.1181 -32.9859]]\n",
      "Weights: [[-4.2266 -1.033  -0.1871  0.113   0.0819]]\n",
      "MSE loss: 167.2773\n",
      "Iteration: 307000\n",
      "Gradient: [[  -4.9847  -12.9763   36.4595  -96.6765 -330.6088]]\n",
      "Weights: [[-4.2258 -1.0331 -0.1871  0.113   0.0819]]\n",
      "MSE loss: 167.2699\n",
      "Iteration: 307100\n",
      "Gradient: [[  6.5289  13.2444  20.7185 -66.6739 219.7403]]\n",
      "Weights: [[-4.2257 -1.0331 -0.1872  0.113   0.0819]]\n",
      "MSE loss: 167.2561\n",
      "Iteration: 307200\n",
      "Gradient: [[ 1.2619 17.4368 11.4172 12.1317 -3.156 ]]\n",
      "Weights: [[-4.2261 -1.0328 -0.1873  0.113   0.0819]]\n",
      "MSE loss: 167.2412\n",
      "Iteration: 307300\n",
      "Gradient: [[ -16.2167   -9.7645   65.1235  -62.5142 -398.2958]]\n",
      "Weights: [[-4.226  -1.0327 -0.1874  0.113   0.0819]]\n",
      "MSE loss: 167.2269\n",
      "Iteration: 307400\n",
      "Gradient: [[  11.556   -26.1249   24.6385   36.0997 -192.1507]]\n",
      "Weights: [[-4.226  -1.0327 -0.1875  0.113   0.0819]]\n",
      "MSE loss: 167.2151\n",
      "Iteration: 307500\n",
      "Gradient: [[  6.696   -3.0866  11.9038 132.5555 -24.3538]]\n",
      "Weights: [[-4.2264 -1.0325 -0.1876  0.113   0.0819]]\n",
      "MSE loss: 167.2044\n",
      "Iteration: 307600\n",
      "Gradient: [[-11.1311   1.587    0.6559  75.7737 -33.6812]]\n",
      "Weights: [[-4.226  -1.0323 -0.1876  0.113   0.0819]]\n",
      "MSE loss: 167.1906\n",
      "Iteration: 307700\n",
      "Gradient: [[ 11.3807   3.8182 -48.0872  17.8756 103.9403]]\n",
      "Weights: [[-4.2246 -1.0321 -0.1877  0.113   0.0819]]\n",
      "MSE loss: 167.1848\n",
      "Iteration: 307800\n",
      "Gradient: [[    2.0731   -10.909    -39.9877    19.7655 -1111.3859]]\n",
      "Weights: [[-4.225  -1.0321 -0.1879  0.113   0.0819]]\n",
      "MSE loss: 167.1683\n",
      "Iteration: 307900\n",
      "Gradient: [[ -8.5971   7.154    4.5916 -27.3193 -61.4908]]\n",
      "Weights: [[-4.2251 -1.032  -0.1879  0.113   0.0819]]\n",
      "MSE loss: 167.1525\n",
      "Iteration: 308000\n",
      "Gradient: [[ 17.5053  -4.0255   5.8638 137.0472  90.7958]]\n",
      "Weights: [[-4.2253 -1.0319 -0.188   0.113   0.0819]]\n",
      "MSE loss: 167.1393\n",
      "Iteration: 308100\n",
      "Gradient: [[  0.2967 -17.1236 -15.0922 -21.9825 197.8642]]\n",
      "Weights: [[-4.2259 -1.0317 -0.1881  0.113   0.082 ]]\n",
      "MSE loss: 167.1201\n",
      "Iteration: 308200\n",
      "Gradient: [[ 12.6968 -14.3311  61.7091 -38.0687   5.5726]]\n",
      "Weights: [[-4.2259 -1.0318 -0.1882  0.1129  0.082 ]]\n",
      "MSE loss: 167.1037\n",
      "Iteration: 308300\n",
      "Gradient: [[ -4.5982  12.809   44.1301  37.1419 -93.2242]]\n",
      "Weights: [[-4.2251 -1.0314 -0.1883  0.1129  0.082 ]]\n",
      "MSE loss: 167.0882\n",
      "Iteration: 308400\n",
      "Gradient: [[ -1.4382 -15.8687 -25.6705  83.5195  99.507 ]]\n",
      "Weights: [[-4.2244 -1.0312 -0.1884  0.1129  0.082 ]]\n",
      "MSE loss: 167.0822\n",
      "Iteration: 308500\n",
      "Gradient: [[ 5.9307 29.064  12.3307 34.4722 69.1628]]\n",
      "Weights: [[-4.2261 -1.031  -0.1884  0.1129  0.082 ]]\n",
      "MSE loss: 167.0589\n",
      "Iteration: 308600\n",
      "Gradient: [[  0.3094  -4.6098 -36.7639 -57.3755 -57.9885]]\n",
      "Weights: [[-4.226  -1.0309 -0.1885  0.1129  0.082 ]]\n",
      "MSE loss: 167.0475\n",
      "Iteration: 308700\n",
      "Gradient: [[   4.8777    0.4921   28.99   -138.5615   46.1733]]\n",
      "Weights: [[-4.2253 -1.0308 -0.1886  0.1129  0.082 ]]\n",
      "MSE loss: 167.0361\n",
      "Iteration: 308800\n",
      "Gradient: [[-9.400000e-03  1.258310e+01  5.510270e+01  3.295400e+00 -2.483513e+02]]\n",
      "Weights: [[-4.2259 -1.0305 -0.1887  0.1129  0.082 ]]\n",
      "MSE loss: 167.0166\n",
      "Iteration: 308900\n",
      "Gradient: [[  -2.7289  -12.2151   17.3925   68.6133 -307.8369]]\n",
      "Weights: [[-4.2264 -1.0306 -0.1888  0.1129  0.082 ]]\n",
      "MSE loss: 167.0023\n",
      "Iteration: 309000\n",
      "Gradient: [[   4.1163  -12.6782  -31.5767  189.3943 -163.3832]]\n",
      "Weights: [[-4.2286 -1.0303 -0.1889  0.1129  0.082 ]]\n",
      "MSE loss: 166.9854\n",
      "Iteration: 309100\n",
      "Gradient: [[   5.489    12.0994   12.2077 -111.4427  -70.6464]]\n",
      "Weights: [[-4.2289 -1.0302 -0.189   0.1129  0.082 ]]\n",
      "MSE loss: 166.975\n",
      "Iteration: 309200\n",
      "Gradient: [[  -0.6823   -2.4248  -17.9628    3.7706 -169.5468]]\n",
      "Weights: [[-4.2282 -1.03   -0.189   0.1129  0.082 ]]\n",
      "MSE loss: 166.9552\n",
      "Iteration: 309300\n",
      "Gradient: [[  -8.2601   -6.0018   -5.7912   21.0013 -638.9984]]\n",
      "Weights: [[-4.2275 -1.0297 -0.1891  0.1129  0.0821]]\n",
      "MSE loss: 166.9417\n",
      "Iteration: 309400\n",
      "Gradient: [[-16.2556  -9.0935  12.9439 129.003  -17.9769]]\n",
      "Weights: [[-4.2283 -1.0296 -0.1892  0.1129  0.0821]]\n",
      "MSE loss: 166.9275\n",
      "Iteration: 309500\n",
      "Gradient: [[ 22.1813  -5.818  -11.5236 198.7349 329.2442]]\n",
      "Weights: [[-4.2255 -1.0293 -0.1893  0.1129  0.0821]]\n",
      "MSE loss: 166.9236\n",
      "Iteration: 309600\n",
      "Gradient: [[   8.1701   -5.6703    1.9265  -32.7043 -300.0594]]\n",
      "Weights: [[-4.2255 -1.0294 -0.1894  0.1129  0.0821]]\n",
      "MSE loss: 166.9077\n",
      "Iteration: 309700\n",
      "Gradient: [[  5.568  -15.6413 -10.6203 -36.2254  15.2875]]\n",
      "Weights: [[-4.2267 -1.0294 -0.1895  0.1129  0.0821]]\n",
      "MSE loss: 166.8858\n",
      "Iteration: 309800\n",
      "Gradient: [[   8.0091   -5.1943   13.1271  106.7292 -269.5766]]\n",
      "Weights: [[-4.2271 -1.0291 -0.1895  0.1129  0.0821]]\n",
      "MSE loss: 166.8746\n",
      "Iteration: 309900\n",
      "Gradient: [[ -1.269    4.1497  25.5002  39.905  260.9844]]\n",
      "Weights: [[-4.228  -1.029  -0.1896  0.1129  0.0821]]\n",
      "MSE loss: 166.86\n",
      "Iteration: 310000\n",
      "Gradient: [[  -9.4142  -18.6953   46.2219  -66.7889 -547.0897]]\n",
      "Weights: [[-4.2281 -1.0287 -0.1897  0.1129  0.0821]]\n",
      "MSE loss: 166.8457\n",
      "Iteration: 310100\n",
      "Gradient: [[ 10.5486 -14.6987  76.6568  -1.1296  33.5731]]\n",
      "Weights: [[-4.2287 -1.0284 -0.1897  0.1129  0.0821]]\n",
      "MSE loss: 166.8311\n",
      "Iteration: 310200\n",
      "Gradient: [[ -4.9575   6.9392  28.1079 -30.4896 156.2717]]\n",
      "Weights: [[-4.2289 -1.028  -0.1898  0.1129  0.0821]]\n",
      "MSE loss: 166.8178\n",
      "Iteration: 310300\n",
      "Gradient: [[ -5.7529  -4.8896  26.4607 -18.3208 288.473 ]]\n",
      "Weights: [[-4.2274 -1.0279 -0.1899  0.1129  0.0821]]\n",
      "MSE loss: 166.8085\n",
      "Iteration: 310400\n",
      "Gradient: [[ -8.431  -10.7237  15.8868 -40.7846  97.3523]]\n",
      "Weights: [[-4.2275 -1.0279 -0.19    0.1129  0.0821]]\n",
      "MSE loss: 166.7893\n",
      "Iteration: 310500\n",
      "Gradient: [[  -9.4229   -4.1231   33.0631  -60.8684 -179.5212]]\n",
      "Weights: [[-4.2278 -1.0278 -0.1901  0.1128  0.0821]]\n",
      "MSE loss: 166.7788\n",
      "Iteration: 310600\n",
      "Gradient: [[ -11.2547    0.2413   29.295    66.7076 -113.5707]]\n",
      "Weights: [[-4.2277 -1.0278 -0.1901  0.1128  0.0821]]\n",
      "MSE loss: 166.7703\n",
      "Iteration: 310700\n",
      "Gradient: [[  8.9262   4.847   -2.7746  55.5481 290.1205]]\n",
      "Weights: [[-4.2279 -1.0277 -0.1902  0.1128  0.0821]]\n",
      "MSE loss: 166.7585\n",
      "Iteration: 310800\n",
      "Gradient: [[  -9.9612    6.4779   19.0358   29.8021 -210.1429]]\n",
      "Weights: [[-4.2284 -1.0275 -0.1903  0.1128  0.0821]]\n",
      "MSE loss: 166.745\n",
      "Iteration: 310900\n",
      "Gradient: [[ -7.0608   8.7614  40.9132 -46.17   177.3152]]\n",
      "Weights: [[-4.2291 -1.0274 -0.1903  0.1128  0.0822]]\n",
      "MSE loss: 166.7345\n",
      "Iteration: 311000\n",
      "Gradient: [[  -0.6564    9.0081    4.6491  -46.4753 -267.691 ]]\n",
      "Weights: [[-4.2286 -1.0273 -0.1904  0.1128  0.0822]]\n",
      "MSE loss: 166.7217\n",
      "Iteration: 311100\n",
      "Gradient: [[  -8.4205   12.1662  -11.6682   93.3679 -205.6515]]\n",
      "Weights: [[-4.2291 -1.0271 -0.1905  0.1128  0.0822]]\n",
      "MSE loss: 166.7088\n",
      "Iteration: 311200\n",
      "Gradient: [[   1.5123   -1.6897  -13.1099   57.8105 -108.21  ]]\n",
      "Weights: [[-4.2293 -1.027  -0.1905  0.1128  0.0822]]\n",
      "MSE loss: 166.6977\n",
      "Iteration: 311300\n",
      "Gradient: [[ -1.7134 -22.8026   5.8011  32.2767 -55.0203]]\n",
      "Weights: [[-4.2297 -1.0268 -0.1906  0.1128  0.0822]]\n",
      "MSE loss: 166.6823\n",
      "Iteration: 311400\n",
      "Gradient: [[ -5.7324   9.796  -20.9203   0.7874 130.6911]]\n",
      "Weights: [[-4.2294 -1.0268 -0.1907  0.1128  0.0822]]\n",
      "MSE loss: 166.6691\n",
      "Iteration: 311500\n",
      "Gradient: [[  10.0812   -1.8206    9.7377  -75.117  -433.3655]]\n",
      "Weights: [[-4.2287 -1.0268 -0.1908  0.1128  0.0822]]\n",
      "MSE loss: 166.6596\n",
      "Iteration: 311600\n",
      "Gradient: [[  4.8297  26.009   14.6065  32.914  -73.9598]]\n",
      "Weights: [[-4.2289 -1.0266 -0.1909  0.1128  0.0822]]\n",
      "MSE loss: 166.644\n",
      "Iteration: 311700\n",
      "Gradient: [[  3.5525   0.4355  37.249  -43.42   -69.7514]]\n",
      "Weights: [[-4.2291 -1.0264 -0.191   0.1128  0.0822]]\n",
      "MSE loss: 166.6309\n",
      "Iteration: 311800\n",
      "Gradient: [[-11.8903   1.0095  -3.6112  45.63   242.6166]]\n",
      "Weights: [[-4.2299 -1.0261 -0.1911  0.1128  0.0822]]\n",
      "MSE loss: 166.6128\n",
      "Iteration: 311900\n",
      "Gradient: [[  -8.6527   -7.0774   -1.2714  134.2064 -100.4189]]\n",
      "Weights: [[-4.2296 -1.0259 -0.1912  0.1128  0.0822]]\n",
      "MSE loss: 166.5928\n",
      "Iteration: 312000\n",
      "Gradient: [[   9.0134   12.2731   52.0483  -29.067  -275.184 ]]\n",
      "Weights: [[-4.2302 -1.0257 -0.1913  0.1128  0.0822]]\n",
      "MSE loss: 166.5811\n",
      "Iteration: 312100\n",
      "Gradient: [[-13.8291 -23.0318 -26.567   81.8602 -43.1059]]\n",
      "Weights: [[-4.2301 -1.0257 -0.1913  0.1128  0.0822]]\n",
      "MSE loss: 166.5714\n",
      "Iteration: 312200\n",
      "Gradient: [[ -5.3823 -17.2178  53.9489 -15.9824 204.5198]]\n",
      "Weights: [[-4.2295 -1.0256 -0.1914  0.1128  0.0822]]\n",
      "MSE loss: 166.5616\n",
      "Iteration: 312300\n",
      "Gradient: [[  0.7009   4.5805   4.2034  -8.2335 240.6215]]\n",
      "Weights: [[-4.2295 -1.0252 -0.1915  0.1128  0.0822]]\n",
      "MSE loss: 166.5403\n",
      "Iteration: 312400\n",
      "Gradient: [[ 1.182000e-01  1.825600e+00  4.605400e+00 -1.555224e+02  1.265081e+02]]\n",
      "Weights: [[-4.2291 -1.025  -0.1916  0.1127  0.0823]]\n",
      "MSE loss: 166.5267\n",
      "Iteration: 312500\n",
      "Gradient: [[  -5.796     0.9739   -8.4249  -34.794  -148.2207]]\n",
      "Weights: [[-4.2296 -1.0248 -0.1917  0.1127  0.0823]]\n",
      "MSE loss: 166.5105\n",
      "Iteration: 312600\n",
      "Gradient: [[-2.766  -4.1408 -9.2696 45.3879 77.899 ]]\n",
      "Weights: [[-4.2299 -1.0246 -0.1918  0.1127  0.0823]]\n",
      "MSE loss: 166.4917\n",
      "Iteration: 312700\n",
      "Gradient: [[ -4.4751  16.0836 -33.0687  20.6479 160.3781]]\n",
      "Weights: [[-4.2307 -1.0245 -0.1918  0.1127  0.0823]]\n",
      "MSE loss: 166.4806\n",
      "Iteration: 312800\n",
      "Gradient: [[ -0.848    1.6897   1.5776  62.2841 -66.6805]]\n",
      "Weights: [[-4.2307 -1.0245 -0.1919  0.1127  0.0823]]\n",
      "MSE loss: 166.4708\n",
      "Iteration: 312900\n",
      "Gradient: [[ -9.3178   2.5122 -13.945   82.4953  -1.6245]]\n",
      "Weights: [[-4.2307 -1.0243 -0.192   0.1127  0.0823]]\n",
      "MSE loss: 166.4577\n",
      "Iteration: 313000\n",
      "Gradient: [[   4.5438    6.3833  -28.9027   32.1894 -305.1952]]\n",
      "Weights: [[-4.231  -1.0239 -0.192   0.1127  0.0823]]\n",
      "MSE loss: 166.4456\n",
      "Iteration: 313100\n",
      "Gradient: [[-2.699000e-01 -2.893700e+00  2.222130e+01  2.995250e+01 -4.253558e+02]]\n",
      "Weights: [[-4.2318 -1.0238 -0.1921  0.1127  0.0823]]\n",
      "MSE loss: 166.4396\n",
      "Iteration: 313200\n",
      "Gradient: [[ -11.7429    8.9041   45.9512   58.6739 -450.0945]]\n",
      "Weights: [[-4.2305 -1.0237 -0.1921  0.1127  0.0823]]\n",
      "MSE loss: 166.4225\n",
      "Iteration: 313300\n",
      "Gradient: [[  4.5332   1.0211  14.3889  38.7418 -62.5177]]\n",
      "Weights: [[-4.2306 -1.0234 -0.1922  0.1127  0.0823]]\n",
      "MSE loss: 166.4062\n",
      "Iteration: 313400\n",
      "Gradient: [[ -2.4058   2.8293  78.9386 -91.7042  -5.199 ]]\n",
      "Weights: [[-4.2311 -1.0231 -0.1923  0.1127  0.0823]]\n",
      "MSE loss: 166.3898\n",
      "Iteration: 313500\n",
      "Gradient: [[  -2.4921   -3.5492   -4.5799  -96.8982 -425.0747]]\n",
      "Weights: [[-4.2307 -1.0232 -0.1924  0.1127  0.0823]]\n",
      "MSE loss: 166.3848\n",
      "Iteration: 313600\n",
      "Gradient: [[  -0.9493   -3.2725   41.628    12.238  -346.1608]]\n",
      "Weights: [[-4.231  -1.0229 -0.1925  0.1127  0.0823]]\n",
      "MSE loss: 166.3691\n",
      "Iteration: 313700\n",
      "Gradient: [[ -3.5816  -1.581   69.7212 -14.1898 -54.9575]]\n",
      "Weights: [[-4.2302 -1.0228 -0.1926  0.1127  0.0823]]\n",
      "MSE loss: 166.3556\n",
      "Iteration: 313800\n",
      "Gradient: [[ 1.474000e-01  9.515000e-01  2.416700e+01  2.169040e+01 -5.515732e+02]]\n",
      "Weights: [[-4.2304 -1.0227 -0.1927  0.1127  0.0823]]\n",
      "MSE loss: 166.3369\n",
      "Iteration: 313900\n",
      "Gradient: [[ -2.6118 -11.9028  41.6522  65.3998 306.2899]]\n",
      "Weights: [[-4.2304 -1.0224 -0.1928  0.1127  0.0823]]\n",
      "MSE loss: 166.3218\n",
      "Iteration: 314000\n",
      "Gradient: [[ -6.2363  13.497   43.3397  63.174  -43.939 ]]\n",
      "Weights: [[-4.2303 -1.0225 -0.1929  0.1127  0.0824]]\n",
      "MSE loss: 166.3113\n",
      "Iteration: 314100\n",
      "Gradient: [[  6.2797  -1.3449 -65.8346  25.6593  37.1898]]\n",
      "Weights: [[-4.2296 -1.0224 -0.193   0.1127  0.0824]]\n",
      "MSE loss: 166.2974\n",
      "Iteration: 314200\n",
      "Gradient: [[   7.1278    2.0099   16.6125  -33.2921 -282.0973]]\n",
      "Weights: [[-4.2301 -1.0224 -0.1931  0.1127  0.0824]]\n",
      "MSE loss: 166.2854\n",
      "Iteration: 314300\n",
      "Gradient: [[ -10.7064   -0.7544   51.771   -58.1279 -151.2567]]\n",
      "Weights: [[-4.2304 -1.0223 -0.1932  0.1126  0.0824]]\n",
      "MSE loss: 166.2729\n",
      "Iteration: 314400\n",
      "Gradient: [[-5.0261 11.0141 11.9    -9.3004 40.3988]]\n",
      "Weights: [[-4.23   -1.0222 -0.1932  0.1127  0.0824]]\n",
      "MSE loss: 166.2611\n",
      "Iteration: 314500\n",
      "Gradient: [[ -1.7331  -6.0006   1.296  -57.6893 -20.681 ]]\n",
      "Weights: [[-4.2304 -1.022  -0.1933  0.1126  0.0824]]\n",
      "MSE loss: 166.2474\n",
      "Iteration: 314600\n",
      "Gradient: [[   1.344   -10.4773    7.5399  -70.496  -322.3861]]\n",
      "Weights: [[-4.2304 -1.0219 -0.1934  0.1126  0.0824]]\n",
      "MSE loss: 166.2332\n",
      "Iteration: 314700\n",
      "Gradient: [[  -9.9004   18.8688    4.2167  113.7767 -510.7381]]\n",
      "Weights: [[-4.2309 -1.0216 -0.1935  0.1126  0.0824]]\n",
      "MSE loss: 166.2149\n",
      "Iteration: 314800\n",
      "Gradient: [[  16.91     -2.8995   -2.5191   23.9809 -127.9224]]\n",
      "Weights: [[-4.2314 -1.0215 -0.1936  0.1126  0.0824]]\n",
      "MSE loss: 166.2024\n",
      "Iteration: 314900\n",
      "Gradient: [[ -11.3479   -8.0553  -30.1192 -121.0343 -157.7579]]\n",
      "Weights: [[-4.2317 -1.0212 -0.1936  0.1126  0.0824]]\n",
      "MSE loss: 166.185\n",
      "Iteration: 315000\n",
      "Gradient: [[-3.627900e+00  4.970000e-02  5.222060e+01  1.054472e+02 -2.022101e+02]]\n",
      "Weights: [[-4.2303 -1.021  -0.1937  0.1126  0.0824]]\n",
      "MSE loss: 166.1681\n",
      "Iteration: 315100\n",
      "Gradient: [[ 23.6312  -0.3288  46.1685 128.3956  29.7173]]\n",
      "Weights: [[-4.2308 -1.0209 -0.1938  0.1126  0.0824]]\n",
      "MSE loss: 166.1544\n",
      "Iteration: 315200\n",
      "Gradient: [[   0.4075    1.1893   50.2932  164.0273 -132.7275]]\n",
      "Weights: [[-4.2302 -1.0207 -0.1939  0.1126  0.0824]]\n",
      "MSE loss: 166.1411\n",
      "Iteration: 315300\n",
      "Gradient: [[  -0.7254  -16.5799   17.1728   27.6827 -604.3467]]\n",
      "Weights: [[-4.2284 -1.0205 -0.194   0.1126  0.0824]]\n",
      "MSE loss: 166.1386\n",
      "Iteration: 315400\n",
      "Gradient: [[ 4.003500e+00  6.650000e-02 -5.049400e+01 -1.002134e+02 -9.605500e+01]]\n",
      "Weights: [[-4.2297 -1.0204 -0.1941  0.1126  0.0825]]\n",
      "MSE loss: 166.1201\n",
      "Iteration: 315500\n",
      "Gradient: [[  0.7857  -2.5914  13.5529  -3.3978 101.1033]]\n",
      "Weights: [[-4.2306 -1.0202 -0.1942  0.1126  0.0825]]\n",
      "MSE loss: 166.1024\n",
      "Iteration: 315600\n",
      "Gradient: [[ -5.8914 -24.2187  66.9716  83.6495 293.069 ]]\n",
      "Weights: [[-4.2313 -1.0202 -0.1943  0.1126  0.0825]]\n",
      "MSE loss: 166.0925\n",
      "Iteration: 315700\n",
      "Gradient: [[  1.4465  -4.5531  -9.1086   2.5681 145.9387]]\n",
      "Weights: [[-4.2307 -1.02   -0.1944  0.1126  0.0825]]\n",
      "MSE loss: 166.076\n",
      "Iteration: 315800\n",
      "Gradient: [[  -7.5735  -14.6965   47.8932  111.8831 -152.6714]]\n",
      "Weights: [[-4.2311 -1.0199 -0.1945  0.1126  0.0825]]\n",
      "MSE loss: 166.064\n",
      "Iteration: 315900\n",
      "Gradient: [[ -2.2194 -13.0422  -2.1321 -67.0771   6.3743]]\n",
      "Weights: [[-4.2305 -1.0196 -0.1946  0.1125  0.0825]]\n",
      "MSE loss: 166.0461\n",
      "Iteration: 316000\n",
      "Gradient: [[  -6.9434   23.5703    3.4045   48.5641 -274.2762]]\n",
      "Weights: [[-4.2303 -1.0194 -0.1946  0.1125  0.0825]]\n",
      "MSE loss: 166.0361\n",
      "Iteration: 316100\n",
      "Gradient: [[-2.48000e-02 -2.63910e+00 -7.25400e+00 -9.95492e+01  9.71860e+00]]\n",
      "Weights: [[-4.2295 -1.0193 -0.1947  0.1125  0.0825]]\n",
      "MSE loss: 166.0246\n",
      "Iteration: 316200\n",
      "Gradient: [[-6.0486  8.9136 21.3746 -7.7596 31.3663]]\n",
      "Weights: [[-4.2303 -1.0191 -0.1947  0.1125  0.0825]]\n",
      "MSE loss: 166.0099\n",
      "Iteration: 316300\n",
      "Gradient: [[-14.3699   1.015   14.1718  28.0244 145.4066]]\n",
      "Weights: [[-4.2292 -1.0191 -0.1948  0.1125  0.0825]]\n",
      "MSE loss: 166.0035\n",
      "Iteration: 316400\n",
      "Gradient: [[ -11.6192  -20.4244  -15.4346   27.5818 -251.6001]]\n",
      "Weights: [[-4.23   -1.0191 -0.1949  0.1125  0.0825]]\n",
      "MSE loss: 165.9911\n",
      "Iteration: 316500\n",
      "Gradient: [[ 11.0047   0.1013 -11.4452  15.4848 -95.2137]]\n",
      "Weights: [[-4.2301 -1.0189 -0.195   0.1125  0.0825]]\n",
      "MSE loss: 165.9746\n",
      "Iteration: 316600\n",
      "Gradient: [[ 12.4048 -14.5732  88.9999 -12.8031  59.2805]]\n",
      "Weights: [[-4.2306 -1.0188 -0.195   0.1125  0.0825]]\n",
      "MSE loss: 165.961\n",
      "Iteration: 316700\n",
      "Gradient: [[ -10.2014    5.7708   20.0915  -38.8979 -234.5829]]\n",
      "Weights: [[-4.2311 -1.0186 -0.1951  0.1125  0.0825]]\n",
      "MSE loss: 165.9463\n",
      "Iteration: 316800\n",
      "Gradient: [[-2.4102 -5.265  23.6758  2.0257 68.8235]]\n",
      "Weights: [[-4.2307 -1.0185 -0.1951  0.1125  0.0825]]\n",
      "MSE loss: 165.9391\n",
      "Iteration: 316900\n",
      "Gradient: [[   5.834    -0.9577   11.0197 -102.0027  250.1258]]\n",
      "Weights: [[-4.2311 -1.0183 -0.1952  0.1125  0.0825]]\n",
      "MSE loss: 165.9262\n",
      "Iteration: 317000\n",
      "Gradient: [[   7.9625  -14.6566   -2.6691    6.3909 -409.0908]]\n",
      "Weights: [[-4.2302 -1.018  -0.1952  0.1124  0.0826]]\n",
      "MSE loss: 165.9191\n",
      "Iteration: 317100\n",
      "Gradient: [[ -2.0346   9.8663   5.6748 -73.9855  -1.6221]]\n",
      "Weights: [[-4.2317 -1.0178 -0.1953  0.1124  0.0826]]\n",
      "MSE loss: 165.9016\n",
      "Iteration: 317200\n",
      "Gradient: [[  -8.1675   -6.177   -10.6673   16.2295 -142.6437]]\n",
      "Weights: [[-4.2324 -1.0177 -0.1954  0.1124  0.0826]]\n",
      "MSE loss: 165.8898\n",
      "Iteration: 317300\n",
      "Gradient: [[ -7.33    12.7501  27.1729 -46.7354 -35.0918]]\n",
      "Weights: [[-4.2325 -1.0176 -0.1954  0.1124  0.0826]]\n",
      "MSE loss: 165.8797\n",
      "Iteration: 317400\n",
      "Gradient: [[-3.3936  6.7781 -1.3054 17.9264  9.465 ]]\n",
      "Weights: [[-4.2318 -1.0174 -0.1955  0.1124  0.0826]]\n",
      "MSE loss: 165.8689\n",
      "Iteration: 317500\n",
      "Gradient: [[ -8.4334 -22.2856   7.2347 -76.0121   3.8421]]\n",
      "Weights: [[-4.2318 -1.0172 -0.1955  0.1124  0.0826]]\n",
      "MSE loss: 165.8597\n",
      "Iteration: 317600\n",
      "Gradient: [[ -10.2031   -1.0469   17.1877  -14.8086 -282.9964]]\n",
      "Weights: [[-4.2325 -1.0169 -0.1956  0.1124  0.0826]]\n",
      "MSE loss: 165.8422\n",
      "Iteration: 317700\n",
      "Gradient: [[  -1.5162   14.9723   32.2108  -57.2319 -107.899 ]]\n",
      "Weights: [[-4.2328 -1.0166 -0.1957  0.1124  0.0826]]\n",
      "MSE loss: 165.8259\n",
      "Iteration: 317800\n",
      "Gradient: [[  -0.6684  -13.2044   23.3392   28.3017 -137.7108]]\n",
      "Weights: [[-4.2335 -1.0164 -0.1958  0.1124  0.0826]]\n",
      "MSE loss: 165.8151\n",
      "Iteration: 317900\n",
      "Gradient: [[  0.72   -18.3078  17.4082 -68.3089  22.262 ]]\n",
      "Weights: [[-4.2327 -1.0163 -0.1959  0.1124  0.0826]]\n",
      "MSE loss: 165.7982\n",
      "Iteration: 318000\n",
      "Gradient: [[ -7.0502  20.5014 -30.3415  14.2038 -77.0501]]\n",
      "Weights: [[-4.2328 -1.0161 -0.196   0.1124  0.0826]]\n",
      "MSE loss: 165.7884\n",
      "Iteration: 318100\n",
      "Gradient: [[ -3.7192   7.2227  -9.5796 -28.7607 -45.8358]]\n",
      "Weights: [[-4.2335 -1.0159 -0.1961  0.1124  0.0826]]\n",
      "MSE loss: 165.7736\n",
      "Iteration: 318200\n",
      "Gradient: [[  -7.4169   -0.5696   15.7031   36.8492 -261.1845]]\n",
      "Weights: [[-4.2334 -1.0155 -0.1961  0.1124  0.0826]]\n",
      "MSE loss: 165.7578\n",
      "Iteration: 318300\n",
      "Gradient: [[ -2.7425   8.3029  -5.3031 -53.5025 180.5883]]\n",
      "Weights: [[-4.234  -1.0153 -0.1963  0.1124  0.0826]]\n",
      "MSE loss: 165.7402\n",
      "Iteration: 318400\n",
      "Gradient: [[ -3.6955 -26.7001 -11.0715  17.2683 -34.3866]]\n",
      "Weights: [[-4.2338 -1.0151 -0.1963  0.1124  0.0826]]\n",
      "MSE loss: 165.7276\n",
      "Iteration: 318500\n",
      "Gradient: [[  11.1027   -3.4651   32.1316   -7.2597 -134.3724]]\n",
      "Weights: [[-4.2346 -1.0148 -0.1965  0.1124  0.0826]]\n",
      "MSE loss: 165.7102\n",
      "Iteration: 318600\n",
      "Gradient: [[   9.7261    2.5492   16.6951    3.1337 -257.3994]]\n",
      "Weights: [[-4.2352 -1.0147 -0.1965  0.1124  0.0826]]\n",
      "MSE loss: 165.7025\n",
      "Iteration: 318700\n",
      "Gradient: [[  -3.0103   -9.6784   18.8464   84.154  -186.1021]]\n",
      "Weights: [[-4.2348 -1.0145 -0.1966  0.1123  0.0826]]\n",
      "MSE loss: 165.6873\n",
      "Iteration: 318800\n",
      "Gradient: [[ -5.134    3.8196 -19.4207 -40.1341 223.1519]]\n",
      "Weights: [[-4.2353 -1.0142 -0.1966  0.1123  0.0827]]\n",
      "MSE loss: 165.6746\n",
      "Iteration: 318900\n",
      "Gradient: [[  -2.6404    5.1291   30.7822 -121.4017 -155.5113]]\n",
      "Weights: [[-4.2354 -1.0141 -0.1967  0.1123  0.0827]]\n",
      "MSE loss: 165.6616\n",
      "Iteration: 319000\n",
      "Gradient: [[  -2.9384    5.2537   -1.8726   95.225  -387.8674]]\n",
      "Weights: [[-4.236  -1.0138 -0.1968  0.1123  0.0827]]\n",
      "MSE loss: 165.6441\n",
      "Iteration: 319100\n",
      "Gradient: [[ -7.1722   7.9483 -15.9519  63.883  445.8652]]\n",
      "Weights: [[-4.2367 -1.0136 -0.1969  0.1123  0.0827]]\n",
      "MSE loss: 165.6313\n",
      "Iteration: 319200\n",
      "Gradient: [[ -6.7941   7.4824  -7.207   56.1215 102.6104]]\n",
      "Weights: [[-4.236  -1.0132 -0.1969  0.1123  0.0827]]\n",
      "MSE loss: 165.6142\n",
      "Iteration: 319300\n",
      "Gradient: [[  -3.4026   13.0655   -1.4398   47.5556 -134.3085]]\n",
      "Weights: [[-4.2354 -1.0131 -0.197   0.1123  0.0827]]\n",
      "MSE loss: 165.6013\n",
      "Iteration: 319400\n",
      "Gradient: [[  4.5079  -0.4186 -27.0472 -14.0907  45.5372]]\n",
      "Weights: [[-4.2357 -1.0129 -0.1971  0.1123  0.0827]]\n",
      "MSE loss: 165.5878\n",
      "Iteration: 319500\n",
      "Gradient: [[ -4.2868 -17.8329  69.676  -33.824  121.7573]]\n",
      "Weights: [[-4.2367 -1.0128 -0.1971  0.1123  0.0827]]\n",
      "MSE loss: 165.5763\n",
      "Iteration: 319600\n",
      "Gradient: [[ 2.0586 13.2228 16.3038 58.9964 -8.6422]]\n",
      "Weights: [[-4.2369 -1.0124 -0.1972  0.1123  0.0827]]\n",
      "MSE loss: 165.5588\n",
      "Iteration: 319700\n",
      "Gradient: [[  -8.2472    0.6444   23.6947 -124.6613  210.5073]]\n",
      "Weights: [[-4.2361 -1.0123 -0.1973  0.1123  0.0827]]\n",
      "MSE loss: 165.5422\n",
      "Iteration: 319800\n",
      "Gradient: [[  3.9464  -8.9865   3.2772  10.2845 -66.5501]]\n",
      "Weights: [[-4.2358 -1.0124 -0.1974  0.1123  0.0827]]\n",
      "MSE loss: 165.5301\n",
      "Iteration: 319900\n",
      "Gradient: [[  16.2441    2.4212  -17.1254  -12.1408 -148.8104]]\n",
      "Weights: [[-4.2352 -1.0123 -0.1975  0.1123  0.0827]]\n",
      "MSE loss: 165.5176\n",
      "Iteration: 320000\n",
      "Gradient: [[  6.892   -9.8164  10.9468  85.723  -18.548 ]]\n",
      "Weights: [[-4.2365 -1.0121 -0.1976  0.1123  0.0827]]\n",
      "MSE loss: 165.5028\n",
      "Iteration: 320100\n",
      "Gradient: [[ -8.075   -9.7445  17.0084  28.2159 -48.3267]]\n",
      "Weights: [[-4.2367 -1.0119 -0.1977  0.1123  0.0828]]\n",
      "MSE loss: 165.4878\n",
      "Iteration: 320200\n",
      "Gradient: [[  -4.0207  -11.405    -3.907    68.2697 -317.8839]]\n",
      "Weights: [[-4.2363 -1.0117 -0.1978  0.1123  0.0828]]\n",
      "MSE loss: 165.4734\n",
      "Iteration: 320300\n",
      "Gradient: [[ -4.8046  -4.0701 -47.78   -51.4711  57.4038]]\n",
      "Weights: [[-4.2365 -1.0115 -0.1979  0.1122  0.0828]]\n",
      "MSE loss: 165.4607\n",
      "Iteration: 320400\n",
      "Gradient: [[  7.0899   7.8276  18.1948 -29.6286 -61.7849]]\n",
      "Weights: [[-4.2366 -1.0113 -0.198   0.1122  0.0828]]\n",
      "MSE loss: 165.4438\n",
      "Iteration: 320500\n",
      "Gradient: [[  -3.3691    0.1988   40.0228   23.1438 -130.5129]]\n",
      "Weights: [[-4.2362 -1.0111 -0.198   0.1122  0.0828]]\n",
      "MSE loss: 165.4283\n",
      "Iteration: 320600\n",
      "Gradient: [[  5.0843  -3.9561 -37.4158  68.8033 137.5137]]\n",
      "Weights: [[-4.2375 -1.0109 -0.1981  0.1122  0.0828]]\n",
      "MSE loss: 165.4196\n",
      "Iteration: 320700\n",
      "Gradient: [[ -0.9752  -5.0169  23.656  -73.9716  64.4054]]\n",
      "Weights: [[-4.2374 -1.0106 -0.1981  0.1122  0.0828]]\n",
      "MSE loss: 165.4047\n",
      "Iteration: 320800\n",
      "Gradient: [[  -4.5005   -7.5614   56.857    20.0368 -120.3544]]\n",
      "Weights: [[-4.2361 -1.0104 -0.1982  0.1122  0.0828]]\n",
      "MSE loss: 165.3885\n",
      "Iteration: 320900\n",
      "Gradient: [[  -3.6357   16.4431   47.1643   17.0437 -123.7243]]\n",
      "Weights: [[-4.2374 -1.0101 -0.1983  0.1122  0.0828]]\n",
      "MSE loss: 165.3725\n",
      "Iteration: 321000\n",
      "Gradient: [[ 14.7797  42.5251 -27.0464 -15.8047 -46.3697]]\n",
      "Weights: [[-4.237  -1.0098 -0.1984  0.1122  0.0828]]\n",
      "MSE loss: 165.3536\n",
      "Iteration: 321100\n",
      "Gradient: [[  2.1412  -6.9421  20.5384 -53.7475  83.0378]]\n",
      "Weights: [[-4.2356 -1.0096 -0.1985  0.1122  0.0828]]\n",
      "MSE loss: 165.3438\n",
      "Iteration: 321200\n",
      "Gradient: [[   8.3876   13.104    11.6965  -51.0737 -306.6598]]\n",
      "Weights: [[-4.235  -1.0097 -0.1986  0.1122  0.0828]]\n",
      "MSE loss: 165.3315\n",
      "Iteration: 321300\n",
      "Gradient: [[ 11.8316  13.2887  22.9237  59.3205 185.1021]]\n",
      "Weights: [[-4.2346 -1.0095 -0.1987  0.1121  0.0828]]\n",
      "MSE loss: 165.3224\n",
      "Iteration: 321400\n",
      "Gradient: [[ 5.4463 15.7496 24.2723 78.7405 24.1257]]\n",
      "Weights: [[-4.2346 -1.0095 -0.1988  0.1121  0.0828]]\n",
      "MSE loss: 165.3069\n",
      "Iteration: 321500\n",
      "Gradient: [[   3.9137  -25.9751   26.5959  -51.3547 -100.703 ]]\n",
      "Weights: [[-4.2359 -1.0093 -0.1989  0.1121  0.0828]]\n",
      "MSE loss: 165.2909\n",
      "Iteration: 321600\n",
      "Gradient: [[  -0.3046   -6.7782   17.2465 -161.8478  136.0085]]\n",
      "Weights: [[-4.2363 -1.0091 -0.199   0.1121  0.0828]]\n",
      "MSE loss: 165.2747\n",
      "Iteration: 321700\n",
      "Gradient: [[  5.33     4.3842 -27.8909  11.8441 439.8894]]\n",
      "Weights: [[-4.2372 -1.0089 -0.1991  0.1121  0.0828]]\n",
      "MSE loss: 165.2649\n",
      "Iteration: 321800\n",
      "Gradient: [[  -3.0848    1.0333   37.3301  120.4022 -233.9176]]\n",
      "Weights: [[-4.2363 -1.0085 -0.1991  0.1121  0.0828]]\n",
      "MSE loss: 165.252\n",
      "Iteration: 321900\n",
      "Gradient: [[  6.8534  -7.5563  38.5451 -36.5079 -10.3858]]\n",
      "Weights: [[-4.2373 -1.0084 -0.1992  0.1121  0.0829]]\n",
      "MSE loss: 165.233\n",
      "Iteration: 322000\n",
      "Gradient: [[-11.7961   2.8198  38.928   -5.0235 -33.6353]]\n",
      "Weights: [[-4.2368 -1.0083 -0.1993  0.1121  0.0829]]\n",
      "MSE loss: 165.2198\n",
      "Iteration: 322100\n",
      "Gradient: [[   4.7886    2.8272  -13.0425 -109.1623  124.9992]]\n",
      "Weights: [[-4.2369 -1.0083 -0.1994  0.1121  0.0829]]\n",
      "MSE loss: 165.2081\n",
      "Iteration: 322200\n",
      "Gradient: [[  8.0652  -1.5318   6.7003   6.799  123.3523]]\n",
      "Weights: [[-4.237  -1.0081 -0.1995  0.1121  0.0829]]\n",
      "MSE loss: 165.1946\n",
      "Iteration: 322300\n",
      "Gradient: [[ 12.7838   1.78   -13.6854 -84.6914  10.739 ]]\n",
      "Weights: [[-4.237  -1.008  -0.1995  0.1121  0.0829]]\n",
      "MSE loss: 165.1843\n",
      "Iteration: 322400\n",
      "Gradient: [[ -9.5457   2.8976  52.2907 118.894  152.0671]]\n",
      "Weights: [[-4.2366 -1.0078 -0.1996  0.1121  0.0829]]\n",
      "MSE loss: 165.1685\n",
      "Iteration: 322500\n",
      "Gradient: [[ 13.1999   0.4999 -23.7097  19.108   77.894 ]]\n",
      "Weights: [[-4.2368 -1.0076 -0.1997  0.112   0.0829]]\n",
      "MSE loss: 165.1559\n",
      "Iteration: 322600\n",
      "Gradient: [[ 12.0088  -9.9956   1.9673  58.3922 129.8476]]\n",
      "Weights: [[-4.2365 -1.0074 -0.1997  0.112   0.0829]]\n",
      "MSE loss: 165.1456\n",
      "Iteration: 322700\n",
      "Gradient: [[   2.5235   16.1129  -40.0766    5.1537 -356.8031]]\n",
      "Weights: [[-4.2361 -1.0071 -0.1999  0.112   0.0829]]\n",
      "MSE loss: 165.1254\n",
      "Iteration: 322800\n",
      "Gradient: [[  -8.5021   -6.7891   28.5182  -33.858  -262.8805]]\n",
      "Weights: [[-4.2361 -1.0072 -0.1999  0.112   0.0829]]\n",
      "MSE loss: 165.112\n",
      "Iteration: 322900\n",
      "Gradient: [[  2.5716 -10.4023 -41.6342  61.2639  61.854 ]]\n",
      "Weights: [[-4.2362 -1.0072 -0.2001  0.112   0.0829]]\n",
      "MSE loss: 165.0957\n",
      "Iteration: 323000\n",
      "Gradient: [[  -7.0036   26.7519  -31.7929  -40.9707 -231.9305]]\n",
      "Weights: [[-4.2373 -1.007  -0.2001  0.112   0.0829]]\n",
      "MSE loss: 165.0844\n",
      "Iteration: 323100\n",
      "Gradient: [[  -7.2826   -5.0009  -12.1991  149.9038 -150.2763]]\n",
      "Weights: [[-4.2362 -1.0071 -0.2002  0.112   0.0829]]\n",
      "MSE loss: 165.0761\n",
      "Iteration: 323200\n",
      "Gradient: [[  -1.1265   17.5671   23.2775 -114.1482  120.3935]]\n",
      "Weights: [[-4.237  -1.0071 -0.2003  0.112   0.0829]]\n",
      "MSE loss: 165.0645\n",
      "Iteration: 323300\n",
      "Gradient: [[-15.0198  -0.5054  25.9027 -22.3228 158.7046]]\n",
      "Weights: [[-4.2365 -1.0068 -0.2003  0.112   0.0829]]\n",
      "MSE loss: 165.0518\n",
      "Iteration: 323400\n",
      "Gradient: [[ -5.1292  13.6353  63.9526 128.5885  -3.3941]]\n",
      "Weights: [[-4.2357 -1.0066 -0.2004  0.112   0.0829]]\n",
      "MSE loss: 165.0409\n",
      "Iteration: 323500\n",
      "Gradient: [[ 10.1138  -9.3441   1.1833 -70.0418 116.7261]]\n",
      "Weights: [[-4.236  -1.0065 -0.2005  0.1119  0.083 ]]\n",
      "MSE loss: 165.0236\n",
      "Iteration: 323600\n",
      "Gradient: [[  -2.3233    9.7962    4.3182  -34.0353 -189.9713]]\n",
      "Weights: [[-4.2363 -1.0063 -0.2006  0.1119  0.083 ]]\n",
      "MSE loss: 165.0088\n",
      "Iteration: 323700\n",
      "Gradient: [[ -0.3442 -31.7843 -34.6751  -7.7743 -37.5894]]\n",
      "Weights: [[-4.2361 -1.006  -0.2006  0.1119  0.083 ]]\n",
      "MSE loss: 165.0012\n",
      "Iteration: 323800\n",
      "Gradient: [[  3.6284   6.9494 -55.0781 141.8652 -94.036 ]]\n",
      "Weights: [[-4.2365 -1.0059 -0.2007  0.1119  0.083 ]]\n",
      "MSE loss: 164.987\n",
      "Iteration: 323900\n",
      "Gradient: [[  11.1523   -6.4818   31.9037  -19.9641 -128.8989]]\n",
      "Weights: [[-4.2359 -1.0057 -0.2008  0.1119  0.083 ]]\n",
      "MSE loss: 164.9718\n",
      "Iteration: 324000\n",
      "Gradient: [[  9.123    6.1948  15.4126  -1.247  171.4369]]\n",
      "Weights: [[-4.2364 -1.0055 -0.2009  0.1119  0.083 ]]\n",
      "MSE loss: 164.9549\n",
      "Iteration: 324100\n",
      "Gradient: [[ -7.2      8.4072  40.7942 -69.544  108.4771]]\n",
      "Weights: [[-4.2369 -1.0054 -0.2009  0.1119  0.083 ]]\n",
      "MSE loss: 164.9455\n",
      "Iteration: 324200\n",
      "Gradient: [[ -4.0849  -1.0526 -43.9319  65.7974   4.5913]]\n",
      "Weights: [[-4.2373 -1.0052 -0.2011  0.1119  0.083 ]]\n",
      "MSE loss: 164.9293\n",
      "Iteration: 324300\n",
      "Gradient: [[  8.847  -12.2812  33.6211 -91.0361  16.2793]]\n",
      "Weights: [[-4.2382 -1.0051 -0.2012  0.1119  0.083 ]]\n",
      "MSE loss: 164.9134\n",
      "Iteration: 324400\n",
      "Gradient: [[  -1.1699   -2.8831   39.5726  147.784  -135.6409]]\n",
      "Weights: [[-4.2366 -1.0048 -0.2013  0.1119  0.083 ]]\n",
      "MSE loss: 164.8948\n",
      "Iteration: 324500\n",
      "Gradient: [[  -2.0077    0.985    22.2271    1.9057 -236.4738]]\n",
      "Weights: [[-4.237  -1.0048 -0.2014  0.1119  0.083 ]]\n",
      "MSE loss: 164.8813\n",
      "Iteration: 324600\n",
      "Gradient: [[ 14.0964  -0.084   39.3733  46.4538 -20.786 ]]\n",
      "Weights: [[-4.2373 -1.0047 -0.2015  0.1119  0.083 ]]\n",
      "MSE loss: 164.8647\n",
      "Iteration: 324700\n",
      "Gradient: [[  6.1215 -10.2828 -17.6886 -24.5487  90.8293]]\n",
      "Weights: [[-4.2368 -1.0045 -0.2016  0.1119  0.083 ]]\n",
      "MSE loss: 164.8496\n",
      "Iteration: 324800\n",
      "Gradient: [[  3.6566   9.1858  33.1923 -15.1984 360.5682]]\n",
      "Weights: [[-4.2365 -1.0045 -0.2017  0.1119  0.083 ]]\n",
      "MSE loss: 164.8378\n",
      "Iteration: 324900\n",
      "Gradient: [[  0.1032  -3.4659   4.8516 -74.6762 -12.2535]]\n",
      "Weights: [[-4.2351 -1.0043 -0.2018  0.1119  0.0831]]\n",
      "MSE loss: 164.8316\n",
      "Iteration: 325000\n",
      "Gradient: [[  2.1102   4.4505  28.0219   6.1316 202.5674]]\n",
      "Weights: [[-4.2358 -1.0042 -0.2018  0.1118  0.0831]]\n",
      "MSE loss: 164.822\n",
      "Iteration: 325100\n",
      "Gradient: [[  -5.3865    7.3718   24.1449  -17.1477 -175.1779]]\n",
      "Weights: [[-4.2361 -1.0041 -0.2019  0.1118  0.0831]]\n",
      "MSE loss: 164.8086\n",
      "Iteration: 325200\n",
      "Gradient: [[  8.7593 -14.1891  76.3583 -55.5096 186.1144]]\n",
      "Weights: [[-4.2349 -1.004  -0.202   0.1118  0.0831]]\n",
      "MSE loss: 164.802\n",
      "Iteration: 325300\n",
      "Gradient: [[ 15.4231   7.3615  22.2389 -23.2084 364.759 ]]\n",
      "Weights: [[-4.2372 -1.0038 -0.202   0.1118  0.0831]]\n",
      "MSE loss: 164.7819\n",
      "Iteration: 325400\n",
      "Gradient: [[ -6.0815  15.5734  61.8204 106.3667  71.6818]]\n",
      "Weights: [[-4.238  -1.0037 -0.202   0.1118  0.0831]]\n",
      "MSE loss: 164.7703\n",
      "Iteration: 325500\n",
      "Gradient: [[  14.4518   -8.1157  -27.0254   94.1697 -108.5975]]\n",
      "Weights: [[-4.2389 -1.0034 -0.2021  0.1118  0.0831]]\n",
      "MSE loss: 164.7586\n",
      "Iteration: 325600\n",
      "Gradient: [[  8.523    3.1188  -5.9171  41.5749 133.1489]]\n",
      "Weights: [[-4.2377 -1.0032 -0.2022  0.1118  0.0831]]\n",
      "MSE loss: 164.7369\n",
      "Iteration: 325700\n",
      "Gradient: [[ -14.0815    3.2078   17.2105 -151.2782 -130.9021]]\n",
      "Weights: [[-4.2372 -1.0029 -0.2022  0.1118  0.0831]]\n",
      "MSE loss: 164.7251\n",
      "Iteration: 325800\n",
      "Gradient: [[  -8.5455    8.7314   23.5435   37.4492 -246.3844]]\n",
      "Weights: [[-4.2376 -1.0026 -0.2023  0.1118  0.0831]]\n",
      "MSE loss: 164.7066\n",
      "Iteration: 325900\n",
      "Gradient: [[   9.6857  -20.5636  -23.2323  138.3828 -137.8695]]\n",
      "Weights: [[-4.2385 -1.0024 -0.2024  0.1118  0.0831]]\n",
      "MSE loss: 164.6933\n",
      "Iteration: 326000\n",
      "Gradient: [[  6.6374  32.1415 -23.3466  31.0625 244.1455]]\n",
      "Weights: [[-4.2379 -1.0021 -0.2025  0.1117  0.0831]]\n",
      "MSE loss: 164.6785\n",
      "Iteration: 326100\n",
      "Gradient: [[ -7.3334 -14.3816  -7.3737  45.7325 -30.637 ]]\n",
      "Weights: [[-4.2377 -1.0022 -0.2026  0.1117  0.0831]]\n",
      "MSE loss: 164.6684\n",
      "Iteration: 326200\n",
      "Gradient: [[ -0.8058 -13.5581  18.8507 -34.044  284.8784]]\n",
      "Weights: [[-4.238  -1.002  -0.2027  0.1117  0.0831]]\n",
      "MSE loss: 164.6566\n",
      "Iteration: 326300\n",
      "Gradient: [[  6.3531  -9.2885  67.9682 100.7152 107.9616]]\n",
      "Weights: [[-4.2378 -1.0018 -0.2027  0.1117  0.0831]]\n",
      "MSE loss: 164.6472\n",
      "Iteration: 326400\n",
      "Gradient: [[ 3.5433  3.8218 54.8037 41.1195 46.3566]]\n",
      "Weights: [[-4.2386 -1.0017 -0.2028  0.1117  0.0832]]\n",
      "MSE loss: 164.6294\n",
      "Iteration: 326500\n",
      "Gradient: [[  -5.1886    4.0586  -17.9001  -16.9367 -311.7049]]\n",
      "Weights: [[-4.2388 -1.0017 -0.2029  0.1117  0.0832]]\n",
      "MSE loss: 164.6189\n",
      "Iteration: 326600\n",
      "Gradient: [[ 13.9072 -26.038    1.3697  29.3591 149.3638]]\n",
      "Weights: [[-4.2387 -1.0014 -0.2029  0.1117  0.0832]]\n",
      "MSE loss: 164.6081\n",
      "Iteration: 326700\n",
      "Gradient: [[   1.6564    9.4364    3.7152 -182.0221  329.7275]]\n",
      "Weights: [[-4.2383 -1.0011 -0.203   0.1117  0.0832]]\n",
      "MSE loss: 164.5939\n",
      "Iteration: 326800\n",
      "Gradient: [[  2.3082 -14.3871   7.8582 -55.8293 -58.3739]]\n",
      "Weights: [[-4.238  -1.001  -0.2031  0.1117  0.0832]]\n",
      "MSE loss: 164.5833\n",
      "Iteration: 326900\n",
      "Gradient: [[   4.717     9.181    -3.5859  -19.5582 -458.1636]]\n",
      "Weights: [[-4.2388 -1.001  -0.2032  0.1117  0.0832]]\n",
      "MSE loss: 164.5735\n",
      "Iteration: 327000\n",
      "Gradient: [[-14.8498   6.5046 -30.9236 -39.1868 -73.4877]]\n",
      "Weights: [[-4.239  -1.001  -0.2032  0.1117  0.0832]]\n",
      "MSE loss: 164.5633\n",
      "Iteration: 327100\n",
      "Gradient: [[  6.4645 -28.0251  34.2564 -28.6212 -12.7845]]\n",
      "Weights: [[-4.2384 -1.0007 -0.2034  0.1117  0.0832]]\n",
      "MSE loss: 164.542\n",
      "Iteration: 327200\n",
      "Gradient: [[-16.0537   7.4361  24.0662  -6.6296  54.4291]]\n",
      "Weights: [[-4.2389 -1.0004 -0.2035  0.1117  0.0832]]\n",
      "MSE loss: 164.5225\n",
      "Iteration: 327300\n",
      "Gradient: [[  -6.2745  -22.5762  -14.7999   28.6007 -242.558 ]]\n",
      "Weights: [[-4.2382 -1.0002 -0.2036  0.1117  0.0832]]\n",
      "MSE loss: 164.5115\n",
      "Iteration: 327400\n",
      "Gradient: [[ -2.2087  13.3382   8.3547  37.8302 103.1697]]\n",
      "Weights: [[-4.2381 -1.0001 -0.2036  0.1117  0.0832]]\n",
      "MSE loss: 164.4989\n",
      "Iteration: 327500\n",
      "Gradient: [[ -2.3811  25.7942  24.5783 -29.4045 170.9021]]\n",
      "Weights: [[-4.2384 -1.0002 -0.2038  0.1117  0.0832]]\n",
      "MSE loss: 164.484\n",
      "Iteration: 327600\n",
      "Gradient: [[  -6.7019    2.6067   26.9767 -121.1967 -347.3366]]\n",
      "Weights: [[-4.2382 -0.9999 -0.2039  0.1117  0.0832]]\n",
      "MSE loss: 164.4693\n",
      "Iteration: 327700\n",
      "Gradient: [[  5.4785  -2.8601  -6.0522 -97.4517 134.7618]]\n",
      "Weights: [[-4.2382 -0.9999 -0.204   0.1117  0.0832]]\n",
      "MSE loss: 164.4504\n",
      "Iteration: 327800\n",
      "Gradient: [[-12.7575  -2.7344 -42.2575  37.2812 -94.8763]]\n",
      "Weights: [[-4.2394 -0.9997 -0.204   0.1117  0.0832]]\n",
      "MSE loss: 164.4387\n",
      "Iteration: 327900\n",
      "Gradient: [[   8.26     15.9327   -2.9931  -55.3082 -480.8755]]\n",
      "Weights: [[-4.2385 -0.9994 -0.2041  0.1117  0.0833]]\n",
      "MSE loss: 164.4251\n",
      "Iteration: 328000\n",
      "Gradient: [[ -1.7371 -15.4716  -1.775   70.1231 120.1227]]\n",
      "Weights: [[-4.2379 -0.9993 -0.2042  0.1116  0.0833]]\n",
      "MSE loss: 164.4134\n",
      "Iteration: 328100\n",
      "Gradient: [[ -9.9692  -6.154   21.8984 118.3776 310.7214]]\n",
      "Weights: [[-4.2387 -0.999  -0.2043  0.1116  0.0833]]\n",
      "MSE loss: 164.3975\n",
      "Iteration: 328200\n",
      "Gradient: [[  11.5269  -24.4053  -11.6456   50.0684 -475.456 ]]\n",
      "Weights: [[-4.2391 -0.9989 -0.2044  0.1116  0.0833]]\n",
      "MSE loss: 164.3835\n",
      "Iteration: 328300\n",
      "Gradient: [[  6.5786   2.2941  16.5855 -60.0591 -60.9354]]\n",
      "Weights: [[-4.2391 -0.9988 -0.2045  0.1116  0.0833]]\n",
      "MSE loss: 164.372\n",
      "Iteration: 328400\n",
      "Gradient: [[ -16.6086   14.6102   70.1559 -104.0268  197.0186]]\n",
      "Weights: [[-4.2404 -0.9986 -0.2046  0.1116  0.0833]]\n",
      "MSE loss: 164.3573\n",
      "Iteration: 328500\n",
      "Gradient: [[  -6.6168  -16.9408   -0.284  -100.844   -58.8238]]\n",
      "Weights: [[-4.2396 -0.9985 -0.2047  0.1116  0.0833]]\n",
      "MSE loss: 164.339\n",
      "Iteration: 328600\n",
      "Gradient: [[ 19.3939   4.8818  40.2376 -14.8519  71.7792]]\n",
      "Weights: [[-4.2385 -0.9984 -0.2047  0.1116  0.0833]]\n",
      "MSE loss: 164.3285\n",
      "Iteration: 328700\n",
      "Gradient: [[ 10.3571 -15.4574  17.8107  73.4238 237.6907]]\n",
      "Weights: [[-4.2385 -0.9981 -0.2048  0.1116  0.0833]]\n",
      "MSE loss: 164.3174\n",
      "Iteration: 328800\n",
      "Gradient: [[ -2.3919  -4.3264  15.6899 -19.4056  -5.0998]]\n",
      "Weights: [[-4.2382 -0.9982 -0.2049  0.1116  0.0833]]\n",
      "MSE loss: 164.3058\n",
      "Iteration: 328900\n",
      "Gradient: [[  4.5835 -29.7673  35.7752 -56.832  223.2399]]\n",
      "Weights: [[-4.237  -0.9981 -0.205   0.1116  0.0833]]\n",
      "MSE loss: 164.2981\n",
      "Iteration: 329000\n",
      "Gradient: [[  13.4366    7.0927   45.5312 -174.4756  140.0065]]\n",
      "Weights: [[-4.237  -0.9979 -0.2051  0.1116  0.0833]]\n",
      "MSE loss: 164.2779\n",
      "Iteration: 329100\n",
      "Gradient: [[  18.8419   -0.3744   51.5587  -50.5737 -266.2587]]\n",
      "Weights: [[-4.2377 -0.9977 -0.2052  0.1115  0.0833]]\n",
      "MSE loss: 164.2568\n",
      "Iteration: 329200\n",
      "Gradient: [[  -1.1127   -6.4177  -25.0651 -141.1873   46.2763]]\n",
      "Weights: [[-4.2373 -0.9975 -0.2053  0.1115  0.0833]]\n",
      "MSE loss: 164.2486\n",
      "Iteration: 329300\n",
      "Gradient: [[  16.9919   -1.5802   22.6396 -122.0401  284.8647]]\n",
      "Weights: [[-4.2388 -0.9976 -0.2054  0.1115  0.0833]]\n",
      "MSE loss: 164.2329\n",
      "Iteration: 329400\n",
      "Gradient: [[ -1.3716  -0.1908 -21.044  -11.6461  12.6719]]\n",
      "Weights: [[-4.2389 -0.9975 -0.2054  0.1116  0.0834]]\n",
      "MSE loss: 164.224\n",
      "Iteration: 329500\n",
      "Gradient: [[  -5.3156  -13.2811   27.9262  -20.9314 -115.2468]]\n",
      "Weights: [[-4.2386 -0.9974 -0.2055  0.1116  0.0834]]\n",
      "MSE loss: 164.2104\n",
      "Iteration: 329600\n",
      "Gradient: [[13.0294  7.1608 40.8267 39.4839 10.4002]]\n",
      "Weights: [[-4.2393 -0.9969 -0.2055  0.1116  0.0834]]\n",
      "MSE loss: 164.1916\n",
      "Iteration: 329700\n",
      "Gradient: [[  -2.6686    1.1093   28.4987   44.6275 -374.6779]]\n",
      "Weights: [[-4.2389 -0.9968 -0.2056  0.1116  0.0834]]\n",
      "MSE loss: 164.1829\n",
      "Iteration: 329800\n",
      "Gradient: [[   4.5118  -12.5609   -1.8032    3.0645 -235.6779]]\n",
      "Weights: [[-4.2384 -0.9967 -0.2057  0.1115  0.0834]]\n",
      "MSE loss: 164.1742\n",
      "Iteration: 329900\n",
      "Gradient: [[  5.053    4.7476  53.9353 127.296  204.1769]]\n",
      "Weights: [[-4.24   -0.9965 -0.2058  0.1115  0.0834]]\n",
      "MSE loss: 164.1582\n",
      "Iteration: 330000\n",
      "Gradient: [[  -1.8174  -14.0578   -3.5877  -21.093  -254.5939]]\n",
      "Weights: [[-4.2402 -0.9964 -0.2059  0.1115  0.0834]]\n",
      "MSE loss: 164.1454\n",
      "Iteration: 330100\n",
      "Gradient: [[   5.538   -13.1489   -8.7975   20.8667 -293.3471]]\n",
      "Weights: [[-4.2408 -0.9963 -0.2059  0.1115  0.0834]]\n",
      "MSE loss: 164.1347\n",
      "Iteration: 330200\n",
      "Gradient: [[ 20.9147  -6.6199  50.158  -61.8278 -13.6609]]\n",
      "Weights: [[-4.242  -0.9964 -0.206   0.1115  0.0834]]\n",
      "MSE loss: 164.1352\n",
      "Iteration: 330300\n",
      "Gradient: [[   9.4936   16.1725  -38.4937  -37.9136 -162.8255]]\n",
      "Weights: [[-4.2409 -0.9961 -0.206   0.1115  0.0834]]\n",
      "MSE loss: 164.1149\n",
      "Iteration: 330400\n",
      "Gradient: [[ -4.5364  -2.4403  -0.3864 -26.735   60.3109]]\n",
      "Weights: [[-4.2419 -0.9959 -0.2061  0.1115  0.0834]]\n",
      "MSE loss: 164.1056\n",
      "Iteration: 330500\n",
      "Gradient: [[  6.3652   1.1872   6.1693 -25.1085 311.9448]]\n",
      "Weights: [[-4.2417 -0.9957 -0.2062  0.1115  0.0834]]\n",
      "MSE loss: 164.0902\n",
      "Iteration: 330600\n",
      "Gradient: [[   6.5843  -19.9967   -1.5229   88.8784 -449.6864]]\n",
      "Weights: [[-4.2423 -0.9952 -0.2063  0.1115  0.0834]]\n",
      "MSE loss: 164.0716\n",
      "Iteration: 330700\n",
      "Gradient: [[  -7.3373  -14.8795   27.0505 -168.5547 -361.1911]]\n",
      "Weights: [[-4.2413 -0.995  -0.2063  0.1115  0.0834]]\n",
      "MSE loss: 164.0546\n",
      "Iteration: 330800\n",
      "Gradient: [[ -0.1525 -24.3886   8.3083  66.3331  12.4582]]\n",
      "Weights: [[-4.2415 -0.9947 -0.2064  0.1115  0.0834]]\n",
      "MSE loss: 164.0391\n",
      "Iteration: 330900\n",
      "Gradient: [[   4.1892   -8.7472  -15.1814  -20.2564 -460.1463]]\n",
      "Weights: [[-4.2409 -0.9947 -0.2065  0.1115  0.0834]]\n",
      "MSE loss: 164.0264\n",
      "Iteration: 331000\n",
      "Gradient: [[  -0.888    -9.4124   10.6965  -10.2089 -356.1507]]\n",
      "Weights: [[-4.2401 -0.9947 -0.2066  0.1115  0.0835]]\n",
      "MSE loss: 164.0114\n",
      "Iteration: 331100\n",
      "Gradient: [[ -10.4417   -5.3969    4.1437   16.7964 -110.1424]]\n",
      "Weights: [[-4.2392 -0.9947 -0.2068  0.1115  0.0835]]\n",
      "MSE loss: 163.9997\n",
      "Iteration: 331200\n",
      "Gradient: [[ -1.2667   0.2676  22.7521 118.9686 -48.9998]]\n",
      "Weights: [[-4.238  -0.9948 -0.2068  0.1115  0.0835]]\n",
      "MSE loss: 163.9968\n",
      "Iteration: 331300\n",
      "Gradient: [[   9.968    11.1757    5.1094  -28.3114 -493.0266]]\n",
      "Weights: [[-4.2378 -0.9949 -0.2069  0.1114  0.0835]]\n",
      "MSE loss: 163.9891\n",
      "Iteration: 331400\n",
      "Gradient: [[ -7.2204 -11.671   17.8333 -11.9437 135.8231]]\n",
      "Weights: [[-4.2382 -0.9946 -0.2069  0.1114  0.0835]]\n",
      "MSE loss: 163.9752\n",
      "Iteration: 331500\n",
      "Gradient: [[  6.2177   2.5095 -30.8423  10.6754 134.8391]]\n",
      "Weights: [[-4.2397 -0.9943 -0.207   0.1114  0.0835]]\n",
      "MSE loss: 163.9547\n",
      "Iteration: 331600\n",
      "Gradient: [[-12.1286   1.4015 -16.7707  86.7474 -85.6232]]\n",
      "Weights: [[-4.2409 -0.9941 -0.2071  0.1114  0.0835]]\n",
      "MSE loss: 163.9404\n",
      "Iteration: 331700\n",
      "Gradient: [[  3.6069   1.3506   9.1313   6.9586 144.2519]]\n",
      "Weights: [[-4.2409 -0.9939 -0.2072  0.1114  0.0835]]\n",
      "MSE loss: 163.9279\n",
      "Iteration: 331800\n",
      "Gradient: [[-20.7933 -21.0199   4.6003 111.935   -3.611 ]]\n",
      "Weights: [[-4.2409 -0.9937 -0.2073  0.1114  0.0835]]\n",
      "MSE loss: 163.909\n",
      "Iteration: 331900\n",
      "Gradient: [[ 0.7427 13.7204 39.43    5.9467 -2.0156]]\n",
      "Weights: [[-4.2392 -0.9935 -0.2073  0.1114  0.0835]]\n",
      "MSE loss: 163.9017\n",
      "Iteration: 332000\n",
      "Gradient: [[  14.101     1.4772  -36.3861  -45.1974 -281.4419]]\n",
      "Weights: [[-4.2401 -0.9935 -0.2074  0.1114  0.0835]]\n",
      "MSE loss: 163.8883\n",
      "Iteration: 332100\n",
      "Gradient: [[  3.7265  -9.8775  20.4997  66.9134 243.9421]]\n",
      "Weights: [[-4.24   -0.9934 -0.2074  0.1114  0.0835]]\n",
      "MSE loss: 163.8769\n",
      "Iteration: 332200\n",
      "Gradient: [[  2.8717  -9.7977 -33.9367 -26.3517  87.4242]]\n",
      "Weights: [[-4.2409 -0.9932 -0.2075  0.1114  0.0835]]\n",
      "MSE loss: 163.8631\n",
      "Iteration: 332300\n",
      "Gradient: [[  5.0182 -21.9181  45.1819 115.6059  55.3426]]\n",
      "Weights: [[-4.2403 -0.9931 -0.2075  0.1114  0.0835]]\n",
      "MSE loss: 163.8555\n",
      "Iteration: 332400\n",
      "Gradient: [[ 11.084   -9.6727 -20.7117  37.8786  56.2878]]\n",
      "Weights: [[-4.2395 -0.993  -0.2076  0.1113  0.0835]]\n",
      "MSE loss: 163.8501\n",
      "Iteration: 332500\n",
      "Gradient: [[  -0.3339  -17.3891   96.4814  -80.6313 -148.8645]]\n",
      "Weights: [[-4.2402 -0.9929 -0.2077  0.1113  0.0835]]\n",
      "MSE loss: 163.833\n",
      "Iteration: 332600\n",
      "Gradient: [[  1.0512  12.0296  22.085   42.0913 290.1944]]\n",
      "Weights: [[-4.2402 -0.9927 -0.2078  0.1113  0.0836]]\n",
      "MSE loss: 163.8162\n",
      "Iteration: 332700\n",
      "Gradient: [[-10.6039 -17.9413 -40.0149 -61.6015  73.0357]]\n",
      "Weights: [[-4.2415 -0.9926 -0.2079  0.1113  0.0836]]\n",
      "MSE loss: 163.8052\n",
      "Iteration: 332800\n",
      "Gradient: [[  -7.4561   -7.0864   14.8755 -169.4477   -5.8454]]\n",
      "Weights: [[-4.2418 -0.9923 -0.2079  0.1113  0.0836]]\n",
      "MSE loss: 163.7948\n",
      "Iteration: 332900\n",
      "Gradient: [[ 14.0703  -8.4103  14.3371 -15.5785  60.7849]]\n",
      "Weights: [[-4.2422 -0.9922 -0.2079  0.1113  0.0836]]\n",
      "MSE loss: 163.7872\n",
      "Iteration: 333000\n",
      "Gradient: [[  -9.8442   -8.8876  -33.9165  -18.7052 -354.4181]]\n",
      "Weights: [[-4.242  -0.9919 -0.208   0.1113  0.0836]]\n",
      "MSE loss: 163.7717\n",
      "Iteration: 333100\n",
      "Gradient: [[ 19.4949  -1.5121 -19.9623   0.1725  26.3963]]\n",
      "Weights: [[-4.2429 -0.9917 -0.208   0.1113  0.0836]]\n",
      "MSE loss: 163.7599\n",
      "Iteration: 333200\n",
      "Gradient: [[ -12.0568    5.3928   -3.078    -2.6026 -516.2505]]\n",
      "Weights: [[-4.2429 -0.9917 -0.2081  0.1113  0.0836]]\n",
      "MSE loss: 163.7521\n",
      "Iteration: 333300\n",
      "Gradient: [[ -12.5345   -6.5364    9.1026 -109.3937 -368.0564]]\n",
      "Weights: [[-4.2419 -0.9916 -0.2082  0.1113  0.0836]]\n",
      "MSE loss: 163.7355\n",
      "Iteration: 333400\n",
      "Gradient: [[  6.1209  -2.4487  12.7036 -22.2534 -82.8834]]\n",
      "Weights: [[-4.2405 -0.9915 -0.2083  0.1113  0.0836]]\n",
      "MSE loss: 163.7256\n",
      "Iteration: 333500\n",
      "Gradient: [[  0.2      1.3806 -14.4054 176.657  -74.137 ]]\n",
      "Weights: [[-4.2415 -0.9916 -0.2084  0.1113  0.0836]]\n",
      "MSE loss: 163.7102\n",
      "Iteration: 333600\n",
      "Gradient: [[  7.0679  -6.5247 -39.9813  93.1046  36.5804]]\n",
      "Weights: [[-4.242  -0.9916 -0.2085  0.1113  0.0836]]\n",
      "MSE loss: 163.6989\n",
      "Iteration: 333700\n",
      "Gradient: [[   4.7827    1.1922  -44.5968 -119.1347   88.7241]]\n",
      "Weights: [[-4.2404 -0.9913 -0.2086  0.1113  0.0836]]\n",
      "MSE loss: 163.6833\n",
      "Iteration: 333800\n",
      "Gradient: [[  -7.5661    7.8972    0.6055    4.0657 -113.2666]]\n",
      "Weights: [[-4.2404 -0.9911 -0.2087  0.1113  0.0836]]\n",
      "MSE loss: 163.6725\n",
      "Iteration: 333900\n",
      "Gradient: [[  -3.3386  -12.7678   31.8135  -11.3052 -504.8984]]\n",
      "Weights: [[-4.2415 -0.9911 -0.2088  0.1113  0.0836]]\n",
      "MSE loss: 163.6616\n",
      "Iteration: 334000\n",
      "Gradient: [[ -0.5368  -5.2407  14.231  -74.8291 144.3654]]\n",
      "Weights: [[-4.2411 -0.9908 -0.2089  0.1113  0.0836]]\n",
      "MSE loss: 163.6449\n",
      "Iteration: 334100\n",
      "Gradient: [[  -1.0233  -18.1736   11.2223   18.974  -164.1741]]\n",
      "Weights: [[-4.2413 -0.9907 -0.2089  0.1113  0.0836]]\n",
      "MSE loss: 163.6347\n",
      "Iteration: 334200\n",
      "Gradient: [[-10.4209  -9.2876  -3.4392 155.0493 -51.4957]]\n",
      "Weights: [[-4.2405 -0.9905 -0.209   0.1113  0.0837]]\n",
      "MSE loss: 163.6248\n",
      "Iteration: 334300\n",
      "Gradient: [[  -1.2226   14.9547   -9.9227   89.9578 -590.1102]]\n",
      "Weights: [[-4.2417 -0.9906 -0.2091  0.1112  0.0837]]\n",
      "MSE loss: 163.608\n",
      "Iteration: 334400\n",
      "Gradient: [[   5.61      7.2625    7.3954  -10.7363 -248.0519]]\n",
      "Weights: [[-4.2404 -0.9906 -0.2092  0.1112  0.0837]]\n",
      "MSE loss: 163.598\n",
      "Iteration: 334500\n",
      "Gradient: [[   4.624     0.3322   29.1462  -40.4154 -266.8367]]\n",
      "Weights: [[-4.241  -0.9905 -0.2093  0.1112  0.0837]]\n",
      "MSE loss: 163.5829\n",
      "Iteration: 334600\n",
      "Gradient: [[ 1.36    3.6601 45.8089 31.6106  7.3488]]\n",
      "Weights: [[-4.2415 -0.9902 -0.2093  0.1112  0.0837]]\n",
      "MSE loss: 163.5696\n",
      "Iteration: 334700\n",
      "Gradient: [[ 16.5854  -1.3401  54.8313  65.5064 -95.3673]]\n",
      "Weights: [[-4.2405 -0.9901 -0.2093  0.1112  0.0837]]\n",
      "MSE loss: 163.5619\n",
      "Iteration: 334800\n",
      "Gradient: [[10.0044  1.3205 10.8137 27.4905 85.7746]]\n",
      "Weights: [[-4.2424 -0.9899 -0.2094  0.1112  0.0837]]\n",
      "MSE loss: 163.5434\n",
      "Iteration: 334900\n",
      "Gradient: [[   3.7524  -17.4783    6.3008  -65.9859 -106.6853]]\n",
      "Weights: [[-4.2421 -0.9898 -0.2095  0.1112  0.0837]]\n",
      "MSE loss: 163.534\n",
      "Iteration: 335000\n",
      "Gradient: [[  6.7188   7.7746   2.8251 -11.2171 -56.0882]]\n",
      "Weights: [[-4.2416 -0.9897 -0.2095  0.1112  0.0837]]\n",
      "MSE loss: 163.5286\n",
      "Iteration: 335100\n",
      "Gradient: [[ -4.4048   1.1176  -0.37   -41.4965 255.2847]]\n",
      "Weights: [[-4.2421 -0.9894 -0.2096  0.1112  0.0837]]\n",
      "MSE loss: 163.5089\n",
      "Iteration: 335200\n",
      "Gradient: [[-2.401000e-01  1.610670e+01 -4.676220e+01  3.303020e+01 -2.992074e+02]]\n",
      "Weights: [[-4.2425 -0.9892 -0.2096  0.1112  0.0837]]\n",
      "MSE loss: 163.498\n",
      "Iteration: 335300\n",
      "Gradient: [[ -12.5485   -0.7155    9.4498 -112.9529 -432.3526]]\n",
      "Weights: [[-4.2421 -0.9892 -0.2097  0.1112  0.0837]]\n",
      "MSE loss: 163.4907\n",
      "Iteration: 335400\n",
      "Gradient: [[  6.1533  -9.9706  -2.7212 129.6444  92.8375]]\n",
      "Weights: [[-4.2415 -0.9891 -0.2098  0.1112  0.0837]]\n",
      "MSE loss: 163.4773\n",
      "Iteration: 335500\n",
      "Gradient: [[  1.3553 -18.0555  -5.5736 166.9979 221.9607]]\n",
      "Weights: [[-4.2423 -0.989  -0.2099  0.1112  0.0837]]\n",
      "MSE loss: 163.461\n",
      "Iteration: 335600\n",
      "Gradient: [[ -7.2715 -23.0636  24.3695  35.9956 107.3575]]\n",
      "Weights: [[-4.2422 -0.9889 -0.2099  0.1112  0.0838]]\n",
      "MSE loss: 163.4532\n",
      "Iteration: 335700\n",
      "Gradient: [[ -11.135   -10.3327    7.1114   60.5141 -261.0613]]\n",
      "Weights: [[-4.243  -0.9889 -0.21    0.1112  0.0838]]\n",
      "MSE loss: 163.4456\n",
      "Iteration: 335800\n",
      "Gradient: [[  -4.4149   -8.2336   19.3868  -65.8609 -147.7249]]\n",
      "Weights: [[-4.2427 -0.9888 -0.21    0.1112  0.0838]]\n",
      "MSE loss: 163.4378\n",
      "Iteration: 335900\n",
      "Gradient: [[ -9.905   -8.4623   8.06    -8.7151 -72.7004]]\n",
      "Weights: [[-4.2429 -0.9886 -0.2101  0.1112  0.0838]]\n",
      "MSE loss: 163.4238\n",
      "Iteration: 336000\n",
      "Gradient: [[  -7.2925  -12.4807    4.4741 -103.0904  -66.0981]]\n",
      "Weights: [[-4.2431 -0.9886 -0.2102  0.1112  0.0838]]\n",
      "MSE loss: 163.4108\n",
      "Iteration: 336100\n",
      "Gradient: [[  16.0144  -11.8488   -5.06     81.3224 -330.844 ]]\n",
      "Weights: [[-4.2433 -0.9884 -0.2103  0.1112  0.0838]]\n",
      "MSE loss: 163.3985\n",
      "Iteration: 336200\n",
      "Gradient: [[  -1.0826    4.2099  -26.0436   53.5173 -124.8271]]\n",
      "Weights: [[-4.2433 -0.9879 -0.2103  0.1112  0.0838]]\n",
      "MSE loss: 163.3819\n",
      "Iteration: 336300\n",
      "Gradient: [[  4.8544 -15.8988 -22.7246  59.2083  78.3897]]\n",
      "Weights: [[-4.2437 -0.9877 -0.2104  0.1111  0.0838]]\n",
      "MSE loss: 163.369\n",
      "Iteration: 336400\n",
      "Gradient: [[ 12.4812   1.9485  11.8833 -68.6017  88.2247]]\n",
      "Weights: [[-4.2439 -0.9875 -0.2105  0.1111  0.0838]]\n",
      "MSE loss: 163.3557\n",
      "Iteration: 336500\n",
      "Gradient: [[ 1.088860e+01  2.248000e-01  1.672910e+01 -3.316750e+01 -2.263029e+02]]\n",
      "Weights: [[-4.2453 -0.9873 -0.2105  0.1111  0.0838]]\n",
      "MSE loss: 163.3484\n",
      "Iteration: 336600\n",
      "Gradient: [[ -5.6748  11.6844  37.1005 -73.137  428.0115]]\n",
      "Weights: [[-4.2446 -0.9871 -0.2107  0.1112  0.0838]]\n",
      "MSE loss: 163.3248\n",
      "Iteration: 336700\n",
      "Gradient: [[ -4.8915  -2.8964 -21.1078 135.9827  29.1853]]\n",
      "Weights: [[-4.2444 -0.9872 -0.2107  0.1111  0.0838]]\n",
      "MSE loss: 163.3201\n",
      "Iteration: 336800\n",
      "Gradient: [[ -0.2756   3.6602  -3.4405 -65.7861 111.5396]]\n",
      "Weights: [[-4.2454 -0.9872 -0.2108  0.1111  0.0838]]\n",
      "MSE loss: 163.317\n",
      "Iteration: 336900\n",
      "Gradient: [[-13.3636  -8.3931  17.683   20.2275 -60.0036]]\n",
      "Weights: [[-4.2447 -0.987  -0.2108  0.1111  0.0838]]\n",
      "MSE loss: 163.2995\n",
      "Iteration: 337000\n",
      "Gradient: [[  -3.1905  -20.3999   36.1224 -126.7391 -341.1794]]\n",
      "Weights: [[-4.2443 -0.9868 -0.2109  0.1111  0.0838]]\n",
      "MSE loss: 163.2794\n",
      "Iteration: 337100\n",
      "Gradient: [[ -0.899    1.9724  27.6663  62.7758 -62.2287]]\n",
      "Weights: [[-4.2439 -0.9866 -0.211   0.1111  0.0839]]\n",
      "MSE loss: 163.2672\n",
      "Iteration: 337200\n",
      "Gradient: [[  -4.2792  -25.9134   27.4146   77.3374 -129.0638]]\n",
      "Weights: [[-4.2438 -0.9867 -0.2111  0.1111  0.0839]]\n",
      "MSE loss: 163.2563\n",
      "Iteration: 337300\n",
      "Gradient: [[  -3.0515    5.432   -23.1363   27.4199 -164.5694]]\n",
      "Weights: [[-4.2433 -0.9865 -0.2112  0.1111  0.0839]]\n",
      "MSE loss: 163.2414\n",
      "Iteration: 337400\n",
      "Gradient: [[   5.6287  -31.7497   67.5608  111.3578 -315.9483]]\n",
      "Weights: [[-4.244  -0.9864 -0.2113  0.1111  0.0839]]\n",
      "MSE loss: 163.2306\n",
      "Iteration: 337500\n",
      "Gradient: [[ -3.7531 -11.9419  -7.3812  -6.0756 126.4884]]\n",
      "Weights: [[-4.2445 -0.9861 -0.2113  0.1111  0.0839]]\n",
      "MSE loss: 163.2191\n",
      "Iteration: 337600\n",
      "Gradient: [[ -14.5745    4.0055    5.6189   54.7214 -205.0501]]\n",
      "Weights: [[-4.2446 -0.986  -0.2114  0.1111  0.0839]]\n",
      "MSE loss: 163.2083\n",
      "Iteration: 337700\n",
      "Gradient: [[   7.3647   -8.9097   11.7352  122.6773 -250.3568]]\n",
      "Weights: [[-4.244  -0.9858 -0.2115  0.1111  0.0839]]\n",
      "MSE loss: 163.1964\n",
      "Iteration: 337800\n",
      "Gradient: [[ -3.5325  -1.4953  18.5127  32.547  -18.7639]]\n",
      "Weights: [[-4.2437 -0.9858 -0.2115  0.1111  0.0839]]\n",
      "MSE loss: 163.1881\n",
      "Iteration: 337900\n",
      "Gradient: [[   8.8995  -10.316     8.3736   40.8078 -181.4461]]\n",
      "Weights: [[-4.2437 -0.9857 -0.2116  0.1111  0.0839]]\n",
      "MSE loss: 163.1765\n",
      "Iteration: 338000\n",
      "Gradient: [[  0.4932  16.1487  18.8834 -90.5539  55.2253]]\n",
      "Weights: [[-4.2439 -0.9856 -0.2116  0.1111  0.0839]]\n",
      "MSE loss: 163.1642\n",
      "Iteration: 338100\n",
      "Gradient: [[   2.076    -3.5526  -44.1008   12.1644 -312.3837]]\n",
      "Weights: [[-4.2432 -0.9856 -0.2117  0.111   0.0839]]\n",
      "MSE loss: 163.1539\n",
      "Iteration: 338200\n",
      "Gradient: [[  -2.9131   -9.627    27.7018 -157.2414   27.4236]]\n",
      "Weights: [[-4.2432 -0.9854 -0.2118  0.111   0.0839]]\n",
      "MSE loss: 163.145\n",
      "Iteration: 338300\n",
      "Gradient: [[  2.6512  -6.1911  17.6785 -63.1867  11.7656]]\n",
      "Weights: [[-4.2438 -0.9853 -0.2119  0.111   0.0839]]\n",
      "MSE loss: 163.1343\n",
      "Iteration: 338400\n",
      "Gradient: [[  -9.7372   -4.8891   15.0371  194.5025 -381.9807]]\n",
      "Weights: [[-4.2438 -0.9854 -0.2119  0.111   0.0839]]\n",
      "MSE loss: 163.1242\n",
      "Iteration: 338500\n",
      "Gradient: [[ -1.2333  20.2037   1.8948  -4.1406 183.1234]]\n",
      "Weights: [[-4.2433 -0.985  -0.212   0.111   0.0839]]\n",
      "MSE loss: 163.1119\n",
      "Iteration: 338600\n",
      "Gradient: [[  9.3644   3.083   63.976   54.6666 -49.3622]]\n",
      "Weights: [[-4.2435 -0.9849 -0.2121  0.111   0.0839]]\n",
      "MSE loss: 163.0986\n",
      "Iteration: 338700\n",
      "Gradient: [[   3.2932   19.555    -3.232     7.8088 -280.6236]]\n",
      "Weights: [[-4.244  -0.9847 -0.2121  0.111   0.0839]]\n",
      "MSE loss: 163.0856\n",
      "Iteration: 338800\n",
      "Gradient: [[  6.0735 -10.6797  30.3836  -2.4906 186.0682]]\n",
      "Weights: [[-4.2437 -0.9845 -0.2122  0.111   0.0839]]\n",
      "MSE loss: 163.0743\n",
      "Iteration: 338900\n",
      "Gradient: [[ -12.2451    2.5185    9.185  -154.1943  -47.777 ]]\n",
      "Weights: [[-4.2438 -0.9844 -0.2123  0.111   0.084 ]]\n",
      "MSE loss: 163.0619\n",
      "Iteration: 339000\n",
      "Gradient: [[ -8.7951  -1.5176  12.2935 169.7479  13.0636]]\n",
      "Weights: [[-4.2445 -0.9843 -0.2124  0.111   0.084 ]]\n",
      "MSE loss: 163.0506\n",
      "Iteration: 339100\n",
      "Gradient: [[   7.4919    4.0597   14.4659   50.0829 -173.534 ]]\n",
      "Weights: [[-4.244  -0.9842 -0.2125  0.111   0.084 ]]\n",
      "MSE loss: 163.0379\n",
      "Iteration: 339200\n",
      "Gradient: [[  -7.4327   -3.9258    7.5358   -1.1696 -343.4315]]\n",
      "Weights: [[-4.2432 -0.9841 -0.2125  0.111   0.084 ]]\n",
      "MSE loss: 163.0261\n",
      "Iteration: 339300\n",
      "Gradient: [[ -1.5236  -6.5639   2.1033 -11.5176 132.798 ]]\n",
      "Weights: [[-4.2438 -0.984  -0.2126  0.111   0.084 ]]\n",
      "MSE loss: 163.0127\n",
      "Iteration: 339400\n",
      "Gradient: [[  3.3804  -9.2577 -27.8016  12.3974 123.7701]]\n",
      "Weights: [[-4.2433 -0.9838 -0.2127  0.111   0.084 ]]\n",
      "MSE loss: 163.0044\n",
      "Iteration: 339500\n",
      "Gradient: [[ -16.8133    3.2508   48.8659 -119.5421 -396.326 ]]\n",
      "Weights: [[-4.2429 -0.9836 -0.2127  0.111   0.084 ]]\n",
      "MSE loss: 162.9952\n",
      "Iteration: 339600\n",
      "Gradient: [[ -6.0105   8.8382  23.5327 -20.3286  -6.0928]]\n",
      "Weights: [[-4.2431 -0.9835 -0.2128  0.111   0.084 ]]\n",
      "MSE loss: 162.9783\n",
      "Iteration: 339700\n",
      "Gradient: [[   2.5648  -28.9578   20.8516  155.1087 -591.8166]]\n",
      "Weights: [[-4.2434 -0.9834 -0.2129  0.1109  0.084 ]]\n",
      "MSE loss: 162.9652\n",
      "Iteration: 339800\n",
      "Gradient: [[-3.0759  2.5621 -1.1881 99.3784 11.2172]]\n",
      "Weights: [[-4.2447 -0.9832 -0.213   0.1109  0.084 ]]\n",
      "MSE loss: 162.9492\n",
      "Iteration: 339900\n",
      "Gradient: [[-0.6259  1.5805 38.5677 83.3736 -3.8313]]\n",
      "Weights: [[-4.2452 -0.983  -0.2131  0.1109  0.084 ]]\n",
      "MSE loss: 162.9373\n",
      "Iteration: 340000\n",
      "Gradient: [[   3.6971  -16.9527   11.3332   -0.795  -247.6764]]\n",
      "Weights: [[-4.2449 -0.9828 -0.2132  0.1109  0.084 ]]\n",
      "MSE loss: 162.9212\n",
      "Iteration: 340100\n",
      "Gradient: [[  1.97   -15.4591 -19.5749  92.8148  88.1015]]\n",
      "Weights: [[-4.2461 -0.9825 -0.2133  0.1109  0.084 ]]\n",
      "MSE loss: 162.907\n",
      "Iteration: 340200\n",
      "Gradient: [[ -2.9133  -1.4475  40.344  -21.1444  17.171 ]]\n",
      "Weights: [[-4.2453 -0.9821 -0.2133  0.1109  0.084 ]]\n",
      "MSE loss: 162.8873\n",
      "Iteration: 340300\n",
      "Gradient: [[   7.4836    4.0028   -9.253    71.0205 -362.4289]]\n",
      "Weights: [[-4.2447 -0.9819 -0.2135  0.1109  0.084 ]]\n",
      "MSE loss: 162.8723\n",
      "Iteration: 340400\n",
      "Gradient: [[ -3.9299 -17.6073  27.5215 -69.1641 136.6348]]\n",
      "Weights: [[-4.245  -0.982  -0.2136  0.1109  0.084 ]]\n",
      "MSE loss: 162.8566\n",
      "Iteration: 340500\n",
      "Gradient: [[   6.2379   17.3688   -5.7163 -111.4717  336.5079]]\n",
      "Weights: [[-4.2447 -0.9819 -0.2137  0.1109  0.0841]]\n",
      "MSE loss: 162.8454\n",
      "Iteration: 340600\n",
      "Gradient: [[ -10.0575  -16.1442  -38.5851   68.514  -123.4203]]\n",
      "Weights: [[-4.245  -0.9818 -0.2137  0.1109  0.0841]]\n",
      "MSE loss: 162.8358\n",
      "Iteration: 340700\n",
      "Gradient: [[ -9.9369   4.5179  22.8479  -5.0833 101.2902]]\n",
      "Weights: [[-4.2456 -0.9817 -0.2138  0.1109  0.0841]]\n",
      "MSE loss: 162.8226\n",
      "Iteration: 340800\n",
      "Gradient: [[   8.5366    8.1855  -14.5326  -69.4316 -304.548 ]]\n",
      "Weights: [[-4.2451 -0.9815 -0.2139  0.1109  0.0841]]\n",
      "MSE loss: 162.8097\n",
      "Iteration: 340900\n",
      "Gradient: [[ 12.5643  14.661    8.6416 -43.7399 111.1214]]\n",
      "Weights: [[-4.2452 -0.9815 -0.2139  0.1109  0.0841]]\n",
      "MSE loss: 162.8004\n",
      "Iteration: 341000\n",
      "Gradient: [[  9.1992 -11.6785  15.6232 111.1535 -53.4471]]\n",
      "Weights: [[-4.2457 -0.9812 -0.214   0.1109  0.0841]]\n",
      "MSE loss: 162.7895\n",
      "Iteration: 341100\n",
      "Gradient: [[ -2.0515  11.2066  25.2374   7.4309 179.5299]]\n",
      "Weights: [[-4.2458 -0.9811 -0.214   0.1109  0.0841]]\n",
      "MSE loss: 162.7799\n",
      "Iteration: 341200\n",
      "Gradient: [[ -5.1798 -12.3268  25.9921 103.0763 -96.7879]]\n",
      "Weights: [[-4.2453 -0.9812 -0.2141  0.1109  0.0841]]\n",
      "MSE loss: 162.7711\n",
      "Iteration: 341300\n",
      "Gradient: [[-10.9479 -17.5739  65.3771  88.8631 134.6331]]\n",
      "Weights: [[-4.2453 -0.9809 -0.2142  0.1109  0.0841]]\n",
      "MSE loss: 162.7567\n",
      "Iteration: 341400\n",
      "Gradient: [[   7.0687    3.5219   31.4214   62.3116 -176.9051]]\n",
      "Weights: [[-4.2448 -0.9807 -0.2143  0.1108  0.0841]]\n",
      "MSE loss: 162.7418\n",
      "Iteration: 341500\n",
      "Gradient: [[   8.0261    2.0943   -9.473   -53.7912 -152.3159]]\n",
      "Weights: [[-4.2449 -0.9805 -0.2144  0.1108  0.0841]]\n",
      "MSE loss: 162.7257\n",
      "Iteration: 341600\n",
      "Gradient: [[ -3.2566  -3.5666  38.6952 -96.925  -94.4559]]\n",
      "Weights: [[-4.2451 -0.9803 -0.2145  0.1108  0.0841]]\n",
      "MSE loss: 162.7146\n",
      "Iteration: 341700\n",
      "Gradient: [[  -5.0783    9.0163   13.9842    6.1855 -280.4949]]\n",
      "Weights: [[-4.2452 -0.9805 -0.2146  0.1108  0.0841]]\n",
      "MSE loss: 162.7039\n",
      "Iteration: 341800\n",
      "Gradient: [[  5.4368   9.4093  17.468  -24.9866 317.7668]]\n",
      "Weights: [[-4.2444 -0.9802 -0.2146  0.1108  0.0841]]\n",
      "MSE loss: 162.693\n",
      "Iteration: 341900\n",
      "Gradient: [[  4.6061   0.2514  18.5198  35.0354 186.3236]]\n",
      "Weights: [[-4.2447 -0.9802 -0.2147  0.1108  0.0841]]\n",
      "MSE loss: 162.6799\n",
      "Iteration: 342000\n",
      "Gradient: [[  17.5203   22.0766  -20.046   -66.9883 -133.6425]]\n",
      "Weights: [[-4.2447 -0.9801 -0.2148  0.1108  0.0841]]\n",
      "MSE loss: 162.6651\n",
      "Iteration: 342100\n",
      "Gradient: [[ -0.4836   2.2937  33.9582  50.2041 -35.8517]]\n",
      "Weights: [[-4.2443 -0.9798 -0.2149  0.1108  0.0842]]\n",
      "MSE loss: 162.6482\n",
      "Iteration: 342200\n",
      "Gradient: [[-12.146  -16.0735  12.4309  62.5509  36.9617]]\n",
      "Weights: [[-4.2439 -0.9798 -0.215   0.1108  0.0842]]\n",
      "MSE loss: 162.6391\n",
      "Iteration: 342300\n",
      "Gradient: [[ -0.4308  -5.1995  -8.0257 -40.5354 127.428 ]]\n",
      "Weights: [[-4.2442 -0.9797 -0.215   0.1108  0.0842]]\n",
      "MSE loss: 162.6322\n",
      "Iteration: 342400\n",
      "Gradient: [[ -5.84     9.1741  22.9894  25.1561 120.4894]]\n",
      "Weights: [[-4.2432 -0.9797 -0.2151  0.1108  0.0842]]\n",
      "MSE loss: 162.6263\n",
      "Iteration: 342500\n",
      "Gradient: [[  -4.3628    8.5245  -19.2354 -106.122    33.5257]]\n",
      "Weights: [[-4.2432 -0.9798 -0.2152  0.1108  0.0842]]\n",
      "MSE loss: 162.6121\n",
      "Iteration: 342600\n",
      "Gradient: [[ -8.0818  12.2576 -28.5977   5.1771 -16.7549]]\n",
      "Weights: [[-4.2435 -0.9797 -0.2153  0.1107  0.0842]]\n",
      "MSE loss: 162.5966\n",
      "Iteration: 342700\n",
      "Gradient: [[  -2.4712   -4.5089    9.7619  -21.9925 -349.4763]]\n",
      "Weights: [[-4.2432 -0.9794 -0.2154  0.1107  0.0842]]\n",
      "MSE loss: 162.5829\n",
      "Iteration: 342800\n",
      "Gradient: [[   7.8275  -20.1172  -75.7087  115.2703 -296.312 ]]\n",
      "Weights: [[-4.2445 -0.9792 -0.2155  0.1107  0.0842]]\n",
      "MSE loss: 162.5678\n",
      "Iteration: 342900\n",
      "Gradient: [[ -0.7421   6.5025 -13.6634 104.5356 107.2398]]\n",
      "Weights: [[-4.244  -0.9791 -0.2156  0.1107  0.0842]]\n",
      "MSE loss: 162.5537\n",
      "Iteration: 343000\n",
      "Gradient: [[  -1.0075   -4.0589   17.6136   27.9993 -178.3914]]\n",
      "Weights: [[-4.2438 -0.9789 -0.2156  0.1107  0.0842]]\n",
      "MSE loss: 162.5473\n",
      "Iteration: 343100\n",
      "Gradient: [[  0.8173  14.2488  27.9503 101.6019  82.4829]]\n",
      "Weights: [[-4.2433 -0.9787 -0.2156  0.1107  0.0842]]\n",
      "MSE loss: 162.5377\n",
      "Iteration: 343200\n",
      "Gradient: [[  -4.5407   14.9547   53.4307   23.6619 -134.4233]]\n",
      "Weights: [[-4.2432 -0.9787 -0.2158  0.1107  0.0842]]\n",
      "MSE loss: 162.5231\n",
      "Iteration: 343300\n",
      "Gradient: [[  -9.2667  -30.1106    6.917  -143.7796  409.2436]]\n",
      "Weights: [[-4.2435 -0.9787 -0.2159  0.1107  0.0842]]\n",
      "MSE loss: 162.5119\n",
      "Iteration: 343400\n",
      "Gradient: [[ 13.6198 -12.1518  37.4586 -26.1602 -56.7591]]\n",
      "Weights: [[-4.2439 -0.9786 -0.216   0.1107  0.0842]]\n",
      "MSE loss: 162.4975\n",
      "Iteration: 343500\n",
      "Gradient: [[ -4.5023  -1.9854  -5.6092 -39.9499  25.7014]]\n",
      "Weights: [[-4.2438 -0.9782 -0.216   0.1107  0.0842]]\n",
      "MSE loss: 162.4818\n",
      "Iteration: 343600\n",
      "Gradient: [[   5.3944   -5.0084   24.0786   15.195  -152.2124]]\n",
      "Weights: [[-4.2441 -0.9781 -0.2161  0.1107  0.0842]]\n",
      "MSE loss: 162.4699\n",
      "Iteration: 343700\n",
      "Gradient: [[  4.0166 -11.0211  10.7773  55.3758  91.2817]]\n",
      "Weights: [[-4.2442 -0.9779 -0.2162  0.1107  0.0843]]\n",
      "MSE loss: 162.4568\n",
      "Iteration: 343800\n",
      "Gradient: [[ -3.0365  -7.3386  26.3869 153.898   60.7934]]\n",
      "Weights: [[-4.2449 -0.9777 -0.2162  0.1107  0.0843]]\n",
      "MSE loss: 162.4438\n",
      "Iteration: 343900\n",
      "Gradient: [[ 15.3598 -10.7197  47.7782 121.8908  24.7423]]\n",
      "Weights: [[-4.2456 -0.9775 -0.2164  0.1107  0.0843]]\n",
      "MSE loss: 162.4293\n",
      "Iteration: 344000\n",
      "Gradient: [[   7.9763  -11.9718  -12.2721  -25.7074 -432.182 ]]\n",
      "Weights: [[-4.2444 -0.9774 -0.2164  0.1107  0.0843]]\n",
      "MSE loss: 162.422\n",
      "Iteration: 344100\n",
      "Gradient: [[ -4.091    0.8518  13.188  133.8696 270.8491]]\n",
      "Weights: [[-4.2461 -0.9772 -0.2165  0.1107  0.0843]]\n",
      "MSE loss: 162.4072\n",
      "Iteration: 344200\n",
      "Gradient: [[   4.874    23.7131    1.7701  -23.271  -104.1293]]\n",
      "Weights: [[-4.245  -0.9771 -0.2165  0.1107  0.0843]]\n",
      "MSE loss: 162.3928\n",
      "Iteration: 344300\n",
      "Gradient: [[  -1.3093   -3.0724  -17.6148 -125.522  -379.6795]]\n",
      "Weights: [[-4.2444 -0.9771 -0.2167  0.1107  0.0843]]\n",
      "MSE loss: 162.3791\n",
      "Iteration: 344400\n",
      "Gradient: [[  10.7483    6.6875   65.4868 -202.4739 -354.1456]]\n",
      "Weights: [[-4.2452 -0.9768 -0.2167  0.1107  0.0843]]\n",
      "MSE loss: 162.3606\n",
      "Iteration: 344500\n",
      "Gradient: [[  16.6038    8.0589   15.071    45.8978 -358.5386]]\n",
      "Weights: [[-4.2456 -0.9767 -0.2168  0.1107  0.0843]]\n",
      "MSE loss: 162.3481\n",
      "Iteration: 344600\n",
      "Gradient: [[  -8.9712   -1.1859   21.0191 -138.2416 -325.6239]]\n",
      "Weights: [[-4.2461 -0.9766 -0.2169  0.1107  0.0843]]\n",
      "MSE loss: 162.3352\n",
      "Iteration: 344700\n",
      "Gradient: [[  -4.339     0.9775   24.409     2.8448 -200.0915]]\n",
      "Weights: [[-4.2456 -0.9763 -0.217   0.1107  0.0843]]\n",
      "MSE loss: 162.3215\n",
      "Iteration: 344800\n",
      "Gradient: [[-11.3279  -9.2551  -2.7889  24.5586  74.0485]]\n",
      "Weights: [[-4.2454 -0.9763 -0.2171  0.1107  0.0843]]\n",
      "MSE loss: 162.3082\n",
      "Iteration: 344900\n",
      "Gradient: [[  5.2585  16.2866   5.76    84.0806 111.8484]]\n",
      "Weights: [[-4.2454 -0.9761 -0.2172  0.1107  0.0843]]\n",
      "MSE loss: 162.2927\n",
      "Iteration: 345000\n",
      "Gradient: [[   4.8298   24.4327   67.5053  -31.9366 -482.9834]]\n",
      "Weights: [[-4.2442 -0.9759 -0.2173  0.1106  0.0843]]\n",
      "MSE loss: 162.2819\n",
      "Iteration: 345100\n",
      "Gradient: [[  6.501    1.5867   2.9612 112.626  -37.1066]]\n",
      "Weights: [[-4.2447 -0.9758 -0.2173  0.1106  0.0844]]\n",
      "MSE loss: 162.2646\n",
      "Iteration: 345200\n",
      "Gradient: [[  -8.0959  -12.3191   34.4901   16.9971 -356.4299]]\n",
      "Weights: [[-4.2447 -0.9756 -0.2174  0.1106  0.0844]]\n",
      "MSE loss: 162.2508\n",
      "Iteration: 345300\n",
      "Gradient: [[  14.3934  -24.329    26.34   -139.0841   11.005 ]]\n",
      "Weights: [[-4.2465 -0.9755 -0.2175  0.1106  0.0844]]\n",
      "MSE loss: 162.2381\n",
      "Iteration: 345400\n",
      "Gradient: [[   8.556    -6.6684   42.0468   58.281  -204.7492]]\n",
      "Weights: [[-4.2466 -0.9753 -0.2176  0.1106  0.0844]]\n",
      "MSE loss: 162.2254\n",
      "Iteration: 345500\n",
      "Gradient: [[  4.8119  -4.3047  23.7382  99.1759 276.0419]]\n",
      "Weights: [[-4.2468 -0.9749 -0.2176  0.1106  0.0844]]\n",
      "MSE loss: 162.2072\n",
      "Iteration: 345600\n",
      "Gradient: [[  10.0202   17.7537   18.6088    6.7685 -116.9766]]\n",
      "Weights: [[-4.2465 -0.9748 -0.2177  0.1106  0.0844]]\n",
      "MSE loss: 162.1966\n",
      "Iteration: 345700\n",
      "Gradient: [[ 11.0801 -16.3092 -10.3787   4.5401  43.1859]]\n",
      "Weights: [[-4.246  -0.9748 -0.2177  0.1106  0.0844]]\n",
      "MSE loss: 162.191\n",
      "Iteration: 345800\n",
      "Gradient: [[  11.6253   18.4086   64.9554   58.3795 -232.3875]]\n",
      "Weights: [[-4.2464 -0.9746 -0.2178  0.1106  0.0844]]\n",
      "MSE loss: 162.1831\n",
      "Iteration: 345900\n",
      "Gradient: [[  12.5286    5.4901   22.7045  -48.6434 -212.6309]]\n",
      "Weights: [[-4.2463 -0.9745 -0.2178  0.1106  0.0844]]\n",
      "MSE loss: 162.1708\n",
      "Iteration: 346000\n",
      "Gradient: [[ 13.6133 -11.1301  23.9308 -17.0902  65.6493]]\n",
      "Weights: [[-4.246  -0.9745 -0.2178  0.1106  0.0844]]\n",
      "MSE loss: 162.166\n",
      "Iteration: 346100\n",
      "Gradient: [[ 12.0253 -20.6835  17.6505 -98.8399 219.1375]]\n",
      "Weights: [[-4.2464 -0.9743 -0.2179  0.1106  0.0844]]\n",
      "MSE loss: 162.1502\n",
      "Iteration: 346200\n",
      "Gradient: [[  8.5452 -20.283   38.1889   7.591    4.9435]]\n",
      "Weights: [[-4.2472 -0.9741 -0.218   0.1106  0.0844]]\n",
      "MSE loss: 162.1361\n",
      "Iteration: 346300\n",
      "Gradient: [[   1.3174  -25.5908  -34.5513   33.5316 -362.8613]]\n",
      "Weights: [[-4.2473 -0.9739 -0.2181  0.1105  0.0844]]\n",
      "MSE loss: 162.1183\n",
      "Iteration: 346400\n",
      "Gradient: [[ -13.876    -8.1279   10.3169   37.4463 -157.4634]]\n",
      "Weights: [[-4.2476 -0.9738 -0.2182  0.1105  0.0844]]\n",
      "MSE loss: 162.104\n",
      "Iteration: 346500\n",
      "Gradient: [[  -0.2793  -18.0609   -0.2018   47.7465 -113.1877]]\n",
      "Weights: [[-4.2474 -0.9735 -0.2183  0.1105  0.0844]]\n",
      "MSE loss: 162.0876\n",
      "Iteration: 346600\n",
      "Gradient: [[ -22.0347  -21.3177   15.5097   72.8033 -217.9401]]\n",
      "Weights: [[-4.2468 -0.9733 -0.2183  0.1105  0.0845]]\n",
      "MSE loss: 162.0777\n",
      "Iteration: 346700\n",
      "Gradient: [[  -7.9326    4.2331   -1.8962  -83.6409 -483.0579]]\n",
      "Weights: [[-4.2462 -0.9733 -0.2185  0.1105  0.0845]]\n",
      "MSE loss: 162.065\n",
      "Iteration: 346800\n",
      "Gradient: [[  -6.2227   13.0563   63.4308 -113.8664 -136.8034]]\n",
      "Weights: [[-4.2464 -0.9732 -0.2186  0.1105  0.0845]]\n",
      "MSE loss: 162.0519\n",
      "Iteration: 346900\n",
      "Gradient: [[-10.3267  27.2318  12.3045 -35.3477 -29.5684]]\n",
      "Weights: [[-4.2463 -0.973  -0.2186  0.1105  0.0845]]\n",
      "MSE loss: 162.0421\n",
      "Iteration: 347000\n",
      "Gradient: [[  -1.1528   -0.5829    8.4397   -7.7702 -282.0733]]\n",
      "Weights: [[-4.246  -0.9731 -0.2187  0.1105  0.0845]]\n",
      "MSE loss: 162.0341\n",
      "Iteration: 347100\n",
      "Gradient: [[  -6.952    14.2565    8.8917 -120.2787  -25.4002]]\n",
      "Weights: [[-4.2471 -0.9731 -0.2188  0.1105  0.0845]]\n",
      "MSE loss: 162.0206\n",
      "Iteration: 347200\n",
      "Gradient: [[  5.7511 -16.2393  42.4444 -20.4267  18.8714]]\n",
      "Weights: [[-4.2468 -0.973  -0.2188  0.1105  0.0845]]\n",
      "MSE loss: 162.0066\n",
      "Iteration: 347300\n",
      "Gradient: [[ -8.6179  11.7603  43.8014 -49.9127 -60.0808]]\n",
      "Weights: [[-4.2466 -0.9728 -0.2189  0.1105  0.0845]]\n",
      "MSE loss: 161.9943\n",
      "Iteration: 347400\n",
      "Gradient: [[-0.2099 11.3969  8.9308 15.2561 17.8484]]\n",
      "Weights: [[-4.2463 -0.9727 -0.219   0.1105  0.0845]]\n",
      "MSE loss: 161.9813\n",
      "Iteration: 347500\n",
      "Gradient: [[ -0.4618  -0.8538 -27.7461 108.8353  15.2295]]\n",
      "Weights: [[-4.2464 -0.9725 -0.2191  0.1105  0.0845]]\n",
      "MSE loss: 161.9666\n",
      "Iteration: 347600\n",
      "Gradient: [[  -7.2892  -13.3496  -37.6257  -84.0955 -411.1577]]\n",
      "Weights: [[-4.2465 -0.9724 -0.2192  0.1105  0.0845]]\n",
      "MSE loss: 161.953\n",
      "Iteration: 347700\n",
      "Gradient: [[ -11.9776   -0.7344   -7.0224 -204.4001 -207.2799]]\n",
      "Weights: [[-4.2469 -0.9721 -0.2193  0.1105  0.0845]]\n",
      "MSE loss: 161.9364\n",
      "Iteration: 347800\n",
      "Gradient: [[ 13.5122 -19.6758 -23.0074   3.74    75.7275]]\n",
      "Weights: [[-4.2467 -0.9721 -0.2194  0.1105  0.0845]]\n",
      "MSE loss: 161.9265\n",
      "Iteration: 347900\n",
      "Gradient: [[  0.6857  -8.5752 -10.1708 -54.6109 -29.6963]]\n",
      "Weights: [[-4.2462 -0.972  -0.2195  0.1105  0.0845]]\n",
      "MSE loss: 161.9176\n",
      "Iteration: 348000\n",
      "Gradient: [[ -13.9966    7.2145  -41.8855  -23.2411 -278.0417]]\n",
      "Weights: [[-4.2471 -0.972  -0.2196  0.1105  0.0845]]\n",
      "MSE loss: 161.9035\n",
      "Iteration: 348100\n",
      "Gradient: [[   8.5069    0.7288   19.6132   11.6949 -120.9483]]\n",
      "Weights: [[-4.2475 -0.9718 -0.2196  0.1105  0.0846]]\n",
      "MSE loss: 161.8908\n",
      "Iteration: 348200\n",
      "Gradient: [[  -3.2423  -18.1475  -15.9383   45.5099 -366.8074]]\n",
      "Weights: [[-4.2479 -0.9717 -0.2197  0.1105  0.0846]]\n",
      "MSE loss: 161.8823\n",
      "Iteration: 348300\n",
      "Gradient: [[ 4.686200e+00 -4.856600e+00 -4.230000e-02 -6.761270e+01 -1.802316e+02]]\n",
      "Weights: [[-4.2477 -0.9714 -0.2198  0.1105  0.0846]]\n",
      "MSE loss: 161.8626\n",
      "Iteration: 348400\n",
      "Gradient: [[  11.7194   -5.3078   28.9438  -45.0039 -182.4535]]\n",
      "Weights: [[-4.2473 -0.9712 -0.2198  0.1104  0.0846]]\n",
      "MSE loss: 161.8517\n",
      "Iteration: 348500\n",
      "Gradient: [[ 12.4579   9.3036   1.1589  30.403  -84.1335]]\n",
      "Weights: [[-4.2483 -0.9709 -0.2198  0.1104  0.0846]]\n",
      "MSE loss: 161.8388\n",
      "Iteration: 348600\n",
      "Gradient: [[ -5.5855   7.5075  57.4007 -22.7014 -78.454 ]]\n",
      "Weights: [[-4.249  -0.9708 -0.2199  0.1104  0.0846]]\n",
      "MSE loss: 161.8314\n",
      "Iteration: 348700\n",
      "Gradient: [[ -9.0093  13.2742   4.3078 -98.9121   0.9514]]\n",
      "Weights: [[-4.2491 -0.9706 -0.22    0.1104  0.0846]]\n",
      "MSE loss: 161.8156\n",
      "Iteration: 348800\n",
      "Gradient: [[ -3.3067 -21.6766 -25.1219  83.2636 -77.0837]]\n",
      "Weights: [[-4.2485 -0.9705 -0.2201  0.1104  0.0846]]\n",
      "MSE loss: 161.8\n",
      "Iteration: 348900\n",
      "Gradient: [[ 9.9127  8.4375 12.2135 95.6919 29.194 ]]\n",
      "Weights: [[-4.2484 -0.9702 -0.2202  0.1104  0.0846]]\n",
      "MSE loss: 161.7852\n",
      "Iteration: 349000\n",
      "Gradient: [[  4.0784   6.5798  -0.1581  63.6772 -21.0433]]\n",
      "Weights: [[-4.249  -0.9701 -0.2203  0.1104  0.0846]]\n",
      "MSE loss: 161.7726\n",
      "Iteration: 349100\n",
      "Gradient: [[  11.92     -2.8806   24.0294   58.0212 -180.7013]]\n",
      "Weights: [[-4.2492 -0.9698 -0.2204  0.1104  0.0846]]\n",
      "MSE loss: 161.7569\n",
      "Iteration: 349200\n",
      "Gradient: [[  3.7392  14.4962  -6.6744  -0.7537 -11.1864]]\n",
      "Weights: [[-4.2502 -0.9695 -0.2205  0.1104  0.0846]]\n",
      "MSE loss: 161.7432\n",
      "Iteration: 349300\n",
      "Gradient: [[  -4.4495  -16.4425  -12.0703  111.0116 -405.963 ]]\n",
      "Weights: [[-4.2502 -0.9692 -0.2205  0.1104  0.0846]]\n",
      "MSE loss: 161.7348\n",
      "Iteration: 349400\n",
      "Gradient: [[  7.727   -1.2308  -8.4455  61.7723 213.8144]]\n",
      "Weights: [[-4.2497 -0.969  -0.2205  0.1104  0.0846]]\n",
      "MSE loss: 161.721\n",
      "Iteration: 349500\n",
      "Gradient: [[  -4.1859  -12.6235   11.4328   74.2852 -128.2577]]\n",
      "Weights: [[-4.2496 -0.9691 -0.2206  0.1104  0.0846]]\n",
      "MSE loss: 161.7089\n",
      "Iteration: 349600\n",
      "Gradient: [[   9.9935  -20.0068   45.4139   24.0594 -152.9064]]\n",
      "Weights: [[-4.2498 -0.969  -0.2207  0.1104  0.0847]]\n",
      "MSE loss: 161.6967\n",
      "Iteration: 349700\n",
      "Gradient: [[ -9.6491  -7.1212 -43.996   59.2761 345.334 ]]\n",
      "Weights: [[-4.2499 -0.9688 -0.2208  0.1104  0.0847]]\n",
      "MSE loss: 161.6832\n",
      "Iteration: 349800\n",
      "Gradient: [[ 5.57600e+00  1.09315e+01  1.89425e+01 -3.80000e-02 -6.18645e+01]]\n",
      "Weights: [[-4.2494 -0.9686 -0.2209  0.1104  0.0847]]\n",
      "MSE loss: 161.6697\n",
      "Iteration: 349900\n",
      "Gradient: [[-8.229  -6.8599 -2.8223 44.7307 49.2824]]\n",
      "Weights: [[-4.2504 -0.9685 -0.221   0.1104  0.0847]]\n",
      "MSE loss: 161.6585\n",
      "Iteration: 350000\n",
      "Gradient: [[ -9.8008  -5.0023  -3.1025 -43.9221 -42.1174]]\n",
      "Weights: [[-4.2498 -0.9683 -0.2211  0.1104  0.0847]]\n",
      "MSE loss: 161.6406\n",
      "Iteration: 350100\n",
      "Gradient: [[ -0.4864 -30.795    0.4076  62.3581   5.2467]]\n",
      "Weights: [[-4.2492 -0.9682 -0.2212  0.1104  0.0847]]\n",
      "MSE loss: 161.6246\n",
      "Iteration: 350200\n",
      "Gradient: [[-4.6007 21.5069 42.9002 53.0232 96.545 ]]\n",
      "Weights: [[-4.2485 -0.9681 -0.2213  0.1103  0.0847]]\n",
      "MSE loss: 161.6101\n",
      "Iteration: 350300\n",
      "Gradient: [[   8.4685  -18.8982   -4.3451   24.7149 -132.9661]]\n",
      "Weights: [[-4.2492 -0.9679 -0.2214  0.1103  0.0847]]\n",
      "MSE loss: 161.5938\n",
      "Iteration: 350400\n",
      "Gradient: [[  14.3168  -17.664   -12.4853   27.8734 -317.9657]]\n",
      "Weights: [[-4.249  -0.9677 -0.2215  0.1103  0.0847]]\n",
      "MSE loss: 161.5802\n",
      "Iteration: 350500\n",
      "Gradient: [[ 11.8024   0.2448   2.8648 -10.1488 195.434 ]]\n",
      "Weights: [[-4.2494 -0.9675 -0.2215  0.1103  0.0847]]\n",
      "MSE loss: 161.5667\n",
      "Iteration: 350600\n",
      "Gradient: [[ -0.3307  -8.8888 -10.8965 -82.9942 -30.999 ]]\n",
      "Weights: [[-4.2484 -0.9674 -0.2216  0.1103  0.0847]]\n",
      "MSE loss: 161.5543\n",
      "Iteration: 350700\n",
      "Gradient: [[  9.7839   3.4077   5.4538 110.0423  22.4487]]\n",
      "Weights: [[-4.2484 -0.9673 -0.2217  0.1103  0.0847]]\n",
      "MSE loss: 161.5431\n",
      "Iteration: 350800\n",
      "Gradient: [[-1.41400e-01 -1.81910e+00 -1.27200e+00  1.66401e+01  3.41314e+02]]\n",
      "Weights: [[-4.2477 -0.9674 -0.2218  0.1103  0.0847]]\n",
      "MSE loss: 161.532\n",
      "Iteration: 350900\n",
      "Gradient: [[  13.7237   -5.7011   56.4608  -10.416  -174.4096]]\n",
      "Weights: [[-4.2485 -0.9675 -0.2219  0.1103  0.0847]]\n",
      "MSE loss: 161.5186\n",
      "Iteration: 351000\n",
      "Gradient: [[-2.08317e+01 -1.02369e+01 -2.72000e-02 -8.79841e+01 -1.36073e+01]]\n",
      "Weights: [[-4.2484 -0.9674 -0.222   0.1103  0.0847]]\n",
      "MSE loss: 161.5075\n",
      "Iteration: 351100\n",
      "Gradient: [[ -6.921   -5.9616 -21.6298  38.2125 288.6235]]\n",
      "Weights: [[-4.2483 -0.9672 -0.222   0.1103  0.0848]]\n",
      "MSE loss: 161.4958\n",
      "Iteration: 351200\n",
      "Gradient: [[  19.5788    1.7437  -39.4661   43.5916 -232.0445]]\n",
      "Weights: [[-4.2485 -0.9671 -0.2221  0.1103  0.0848]]\n",
      "MSE loss: 161.4831\n",
      "Iteration: 351300\n",
      "Gradient: [[ -7.2021   4.3194  55.2489 -40.1034 -82.1614]]\n",
      "Weights: [[-4.2484 -0.967  -0.2222  0.1103  0.0848]]\n",
      "MSE loss: 161.4675\n",
      "Iteration: 351400\n",
      "Gradient: [[  10.1635    0.8618  -30.5777  -60.112  -144.2887]]\n",
      "Weights: [[-4.2479 -0.9667 -0.2223  0.1102  0.0848]]\n",
      "MSE loss: 161.4545\n",
      "Iteration: 351500\n",
      "Gradient: [[ 9.1657 -4.8375 16.8201 91.1892 84.2015]]\n",
      "Weights: [[-4.2476 -0.9663 -0.2224  0.1102  0.0848]]\n",
      "MSE loss: 161.4411\n",
      "Iteration: 351600\n",
      "Gradient: [[ 20.4736   0.5346  30.2216  38.59   -60.2399]]\n",
      "Weights: [[-4.2485 -0.9662 -0.2225  0.1102  0.0848]]\n",
      "MSE loss: 161.425\n",
      "Iteration: 351700\n",
      "Gradient: [[  11.1379  -15.1005    6.7151  131.1318 -162.4846]]\n",
      "Weights: [[-4.2499 -0.9658 -0.2226  0.1102  0.0848]]\n",
      "MSE loss: 161.4079\n",
      "Iteration: 351800\n",
      "Gradient: [[   3.4605   -7.3179   -5.3778   20.5651 -321.2179]]\n",
      "Weights: [[-4.2498 -0.9656 -0.2226  0.1102  0.0848]]\n",
      "MSE loss: 161.3917\n",
      "Iteration: 351900\n",
      "Gradient: [[  -0.5335    4.4836   37.4281   33.3062 -243.7947]]\n",
      "Weights: [[-4.2509 -0.9653 -0.2227  0.1102  0.0848]]\n",
      "MSE loss: 161.3777\n",
      "Iteration: 352000\n",
      "Gradient: [[ -2.2071 -23.6432 -17.8611 -14.8969 -20.8307]]\n",
      "Weights: [[-4.2516 -0.9652 -0.2228  0.1102  0.0848]]\n",
      "MSE loss: 161.3701\n",
      "Iteration: 352100\n",
      "Gradient: [[ -1.8248 -16.9385  -3.2008  51.9536  85.9933]]\n",
      "Weights: [[-4.2506 -0.9652 -0.2228  0.1102  0.0848]]\n",
      "MSE loss: 161.3546\n",
      "Iteration: 352200\n",
      "Gradient: [[-12.7568   3.0005  -2.3603   5.41    76.8213]]\n",
      "Weights: [[-4.2502 -0.9648 -0.2229  0.1102  0.0848]]\n",
      "MSE loss: 161.3389\n",
      "Iteration: 352300\n",
      "Gradient: [[  -7.1945   10.5749  -23.5942  -53.5609 -149.4792]]\n",
      "Weights: [[-4.2502 -0.9648 -0.223   0.1102  0.0848]]\n",
      "MSE loss: 161.3294\n",
      "Iteration: 352400\n",
      "Gradient: [[  8.2968  -0.1245  36.5274  95.4006 102.2323]]\n",
      "Weights: [[-4.2492 -0.9647 -0.223   0.1102  0.0848]]\n",
      "MSE loss: 161.3223\n",
      "Iteration: 352500\n",
      "Gradient: [[ 13.9452   1.2895  28.7035 -64.8152  -3.8849]]\n",
      "Weights: [[-4.2497 -0.9645 -0.2231  0.1102  0.0848]]\n",
      "MSE loss: 161.3072\n",
      "Iteration: 352600\n",
      "Gradient: [[  14.5289   -9.85    -38.0018   51.1251 -271.952 ]]\n",
      "Weights: [[-4.2505 -0.9644 -0.2233  0.1102  0.0849]]\n",
      "MSE loss: 161.2925\n",
      "Iteration: 352700\n",
      "Gradient: [[  18.147    14.2276   24.9021   22.8894 -326.4291]]\n",
      "Weights: [[-4.2499 -0.9643 -0.2233  0.1102  0.0849]]\n",
      "MSE loss: 161.2812\n",
      "Iteration: 352800\n",
      "Gradient: [[  -8.074    12.1697   41.3629 -150.2444   -3.4089]]\n",
      "Weights: [[-4.2495 -0.9641 -0.2234  0.1102  0.0849]]\n",
      "MSE loss: 161.2688\n",
      "Iteration: 352900\n",
      "Gradient: [[  -1.0979   17.783    -4.0382  -73.838  -101.5825]]\n",
      "Weights: [[-4.2494 -0.9641 -0.2235  0.1101  0.0849]]\n",
      "MSE loss: 161.2576\n",
      "Iteration: 353000\n",
      "Gradient: [[   1.9153    5.6329   14.4628  -39.779  -114.6003]]\n",
      "Weights: [[-4.2497 -0.9642 -0.2236  0.1101  0.0849]]\n",
      "MSE loss: 161.2484\n",
      "Iteration: 353100\n",
      "Gradient: [[ -2.4107   2.9851 -34.5321  13.5998  28.7419]]\n",
      "Weights: [[-4.2502 -0.964  -0.2236  0.1101  0.0849]]\n",
      "MSE loss: 161.235\n",
      "Iteration: 353200\n",
      "Gradient: [[  1.4313 -33.9652 -14.4073  19.7651 -60.0637]]\n",
      "Weights: [[-4.2515 -0.9636 -0.2237  0.1101  0.0849]]\n",
      "MSE loss: 161.2192\n",
      "Iteration: 353300\n",
      "Gradient: [[ 11.4947 -14.49    -1.6519  -3.3985  46.4063]]\n",
      "Weights: [[-4.2503 -0.9635 -0.2238  0.1101  0.0849]]\n",
      "MSE loss: 161.2075\n",
      "Iteration: 353400\n",
      "Gradient: [[   2.5026   11.0892   26.8432  -66.4314 -321.3505]]\n",
      "Weights: [[-4.2497 -0.9633 -0.2238  0.1101  0.0849]]\n",
      "MSE loss: 161.1971\n",
      "Iteration: 353500\n",
      "Gradient: [[  -2.9402   -9.2578   22.0495   63.4384 -155.4235]]\n",
      "Weights: [[-4.2495 -0.9631 -0.2239  0.1101  0.0849]]\n",
      "MSE loss: 161.1884\n",
      "Iteration: 353600\n",
      "Gradient: [[ -5.0743  -8.6069   3.8183 -14.515  273.8281]]\n",
      "Weights: [[-4.2489 -0.9629 -0.224   0.1101  0.0849]]\n",
      "MSE loss: 161.1771\n",
      "Iteration: 353700\n",
      "Gradient: [[  -0.4512   12.4204   57.0873  -12.9073 -254.8316]]\n",
      "Weights: [[-4.248  -0.9632 -0.224   0.1101  0.0849]]\n",
      "MSE loss: 161.1752\n",
      "Iteration: 353800\n",
      "Gradient: [[16.0783  9.2756  7.3154 69.2676  9.0133]]\n",
      "Weights: [[-4.2487 -0.9632 -0.2242  0.11    0.0849]]\n",
      "MSE loss: 161.1553\n",
      "Iteration: 353900\n",
      "Gradient: [[-11.7647 -17.0713  15.239   68.351  147.6582]]\n",
      "Weights: [[-4.2486 -0.963  -0.2242  0.11    0.0849]]\n",
      "MSE loss: 161.1413\n",
      "Iteration: 354000\n",
      "Gradient: [[   5.4586   24.8568   13.9264  -26.7868 -100.7994]]\n",
      "Weights: [[-4.2489 -0.9628 -0.2243  0.11    0.0849]]\n",
      "MSE loss: 161.1245\n",
      "Iteration: 354100\n",
      "Gradient: [[ -1.9628  -7.5651 -29.6452  25.7626   5.7776]]\n",
      "Weights: [[-4.2502 -0.9627 -0.2243  0.11    0.0849]]\n",
      "MSE loss: 161.1174\n",
      "Iteration: 354200\n",
      "Gradient: [[  6.6973  -7.5613  -3.7948 -78.2106 -65.6047]]\n",
      "Weights: [[-4.2496 -0.9625 -0.2244  0.11    0.085 ]]\n",
      "MSE loss: 161.1041\n",
      "Iteration: 354300\n",
      "Gradient: [[  2.7029  17.2299 -32.4292   1.6196 480.7195]]\n",
      "Weights: [[-4.2492 -0.9623 -0.2244  0.11    0.085 ]]\n",
      "MSE loss: 161.0996\n",
      "Iteration: 354400\n",
      "Gradient: [[-10.8385 -18.0711  21.4826  12.6343 -97.8873]]\n",
      "Weights: [[-4.2501 -0.9623 -0.2245  0.11    0.085 ]]\n",
      "MSE loss: 161.0857\n",
      "Iteration: 354500\n",
      "Gradient: [[   2.8205    6.3005   -8.9141  145.3848 -331.3072]]\n",
      "Weights: [[-4.2502 -0.9623 -0.2246  0.11    0.085 ]]\n",
      "MSE loss: 161.0763\n",
      "Iteration: 354600\n",
      "Gradient: [[  4.2912  16.1643  12.7989 -10.0565 -77.4169]]\n",
      "Weights: [[-4.2506 -0.9622 -0.2246  0.11    0.085 ]]\n",
      "MSE loss: 161.0677\n",
      "Iteration: 354700\n",
      "Gradient: [[   7.1243   -1.9949  -52.4553    6.0393 -360.6648]]\n",
      "Weights: [[-4.2503 -0.9621 -0.2247  0.11    0.085 ]]\n",
      "MSE loss: 161.0547\n",
      "Iteration: 354800\n",
      "Gradient: [[  13.2405  -14.9648   -5.7664  -26.1021 -128.3083]]\n",
      "Weights: [[-4.251  -0.9619 -0.2248  0.11    0.085 ]]\n",
      "MSE loss: 161.0425\n",
      "Iteration: 354900\n",
      "Gradient: [[  -4.8114    9.4854  -31.1972 -134.3916  -70.0991]]\n",
      "Weights: [[-4.2514 -0.9616 -0.2248  0.11    0.085 ]]\n",
      "MSE loss: 161.029\n",
      "Iteration: 355000\n",
      "Gradient: [[   1.1171   -6.3054   14.8937   77.8031 -249.2472]]\n",
      "Weights: [[-4.2513 -0.9612 -0.2249  0.11    0.085 ]]\n",
      "MSE loss: 161.0143\n",
      "Iteration: 355100\n",
      "Gradient: [[   3.4848   12.6524    4.4973    5.0657 -297.6368]]\n",
      "Weights: [[-4.2509 -0.9609 -0.2249  0.11    0.085 ]]\n",
      "MSE loss: 161.0014\n",
      "Iteration: 355200\n",
      "Gradient: [[  1.1556  -1.0603  59.822  -14.7451  87.3022]]\n",
      "Weights: [[-4.251  -0.9607 -0.225   0.11    0.085 ]]\n",
      "MSE loss: 160.9888\n",
      "Iteration: 355300\n",
      "Gradient: [[  -3.4918   16.6945  -19.3391  -90.7887 -105.1497]]\n",
      "Weights: [[-4.2514 -0.9607 -0.2251  0.1099  0.085 ]]\n",
      "MSE loss: 160.9766\n",
      "Iteration: 355400\n",
      "Gradient: [[  -3.0447  -14.1009  -25.6971   20.7067 -106.5133]]\n",
      "Weights: [[-4.2499 -0.9607 -0.2251  0.1099  0.085 ]]\n",
      "MSE loss: 160.9721\n",
      "Iteration: 355500\n",
      "Gradient: [[   1.4641   10.5961   26.8763  -97.0897 -505.6265]]\n",
      "Weights: [[-4.2501 -0.9607 -0.2252  0.1099  0.085 ]]\n",
      "MSE loss: 160.9609\n",
      "Iteration: 355600\n",
      "Gradient: [[   6.5835   -5.082   -45.8836   39.0328 -100.1839]]\n",
      "Weights: [[-4.2498 -0.9606 -0.2252  0.1099  0.085 ]]\n",
      "MSE loss: 160.9533\n",
      "Iteration: 355700\n",
      "Gradient: [[  -6.896   -22.3031  -12.8473   61.5206 -358.1127]]\n",
      "Weights: [[-4.2501 -0.9604 -0.2253  0.1099  0.085 ]]\n",
      "MSE loss: 160.937\n",
      "Iteration: 355800\n",
      "Gradient: [[   0.6304    8.5677   12.1494  -85.5782 -130.5225]]\n",
      "Weights: [[-4.2509 -0.9604 -0.2254  0.1099  0.0851]]\n",
      "MSE loss: 160.9205\n",
      "Iteration: 355900\n",
      "Gradient: [[   9.31      9.8158   17.6683   48.9828 -124.1268]]\n",
      "Weights: [[-4.2499 -0.9604 -0.2255  0.1099  0.0851]]\n",
      "MSE loss: 160.9083\n",
      "Iteration: 356000\n",
      "Gradient: [[-18.1277 -15.5437  -2.5747  76.2854 352.1119]]\n",
      "Weights: [[-4.2497 -0.9603 -0.2256  0.1099  0.0851]]\n",
      "MSE loss: 160.8995\n",
      "Iteration: 356100\n",
      "Gradient: [[  -3.1771  -22.2093    5.1659 -136.9707   -4.9154]]\n",
      "Weights: [[-4.2484 -0.9601 -0.2257  0.1099  0.0851]]\n",
      "MSE loss: 160.8974\n",
      "Iteration: 356200\n",
      "Gradient: [[ -3.0226  16.0753  -0.7664  39.8799 192.137 ]]\n",
      "Weights: [[-4.2489 -0.9601 -0.2258  0.1099  0.0851]]\n",
      "MSE loss: 160.8783\n",
      "Iteration: 356300\n",
      "Gradient: [[ 19.1737   1.5773  17.3097 -36.5212 215.313 ]]\n",
      "Weights: [[-4.2485 -0.96   -0.2259  0.1098  0.0851]]\n",
      "MSE loss: 160.8672\n",
      "Iteration: 356400\n",
      "Gradient: [[-14.6606  11.7131  33.2175 -53.2621  12.0499]]\n",
      "Weights: [[-4.2476 -0.9599 -0.226   0.1098  0.0851]]\n",
      "MSE loss: 160.8622\n",
      "Iteration: 356500\n",
      "Gradient: [[  10.3258  -12.9278   44.1787  -28.819  -126.8361]]\n",
      "Weights: [[-4.2479 -0.9598 -0.2261  0.1098  0.0851]]\n",
      "MSE loss: 160.8428\n",
      "Iteration: 356600\n",
      "Gradient: [[  -6.0351    9.4265   76.4441   73.4031 -112.3035]]\n",
      "Weights: [[-4.2485 -0.9597 -0.2263  0.1098  0.0851]]\n",
      "MSE loss: 160.819\n",
      "Iteration: 356700\n",
      "Gradient: [[ -7.9839  28.6452 -23.5355 -27.3323  64.5635]]\n",
      "Weights: [[-4.2493 -0.9596 -0.2263  0.1098  0.0851]]\n",
      "MSE loss: 160.8075\n",
      "Iteration: 356800\n",
      "Gradient: [[  -1.1751    7.8037  -10.7984 -115.6259 -280.4585]]\n",
      "Weights: [[-4.2492 -0.9596 -0.2265  0.1098  0.0851]]\n",
      "MSE loss: 160.7922\n",
      "Iteration: 356900\n",
      "Gradient: [[ 10.1919  11.3604  24.1296 154.6861  21.0957]]\n",
      "Weights: [[-4.2496 -0.9595 -0.2265  0.1098  0.0851]]\n",
      "MSE loss: 160.7808\n",
      "Iteration: 357000\n",
      "Gradient: [[ 11.8867  11.7416  16.299  -13.2957  95.9179]]\n",
      "Weights: [[-4.2493 -0.9593 -0.2266  0.1098  0.0851]]\n",
      "MSE loss: 160.7684\n",
      "Iteration: 357100\n",
      "Gradient: [[  -3.8695   -4.6277   25.6808   21.6057 -145.7503]]\n",
      "Weights: [[-4.2499 -0.9593 -0.2266  0.1098  0.0851]]\n",
      "MSE loss: 160.7649\n",
      "Iteration: 357200\n",
      "Gradient: [[ 1.8257 -6.613  -5.7358 15.2383 -5.0531]]\n",
      "Weights: [[-4.2492 -0.9591 -0.2267  0.1098  0.0851]]\n",
      "MSE loss: 160.7521\n",
      "Iteration: 357300\n",
      "Gradient: [[   1.1057   19.9299   -0.3832 -130.8765 -129.8545]]\n",
      "Weights: [[-4.2485 -0.9591 -0.2268  0.1098  0.0851]]\n",
      "MSE loss: 160.7427\n",
      "Iteration: 357400\n",
      "Gradient: [[-14.3242   8.1719  41.2206 -69.0918 -38.9277]]\n",
      "Weights: [[-4.248  -0.9593 -0.2269  0.1098  0.0852]]\n",
      "MSE loss: 160.7344\n",
      "Iteration: 357500\n",
      "Gradient: [[-17.9178  14.355  -10.0115 209.1032 143.9237]]\n",
      "Weights: [[-4.2478 -0.959  -0.227   0.1098  0.0852]]\n",
      "MSE loss: 160.7171\n",
      "Iteration: 357600\n",
      "Gradient: [[  -1.1516   -8.5687   -3.6189 -105.1857 -170.3279]]\n",
      "Weights: [[-4.2488 -0.9591 -0.2272  0.1098  0.0852]]\n",
      "MSE loss: 160.7024\n",
      "Iteration: 357700\n",
      "Gradient: [[  1.3766  -1.5218 -18.3858 -72.3431 -47.1538]]\n",
      "Weights: [[-4.2494 -0.9587 -0.2272  0.1098  0.0852]]\n",
      "MSE loss: 160.6829\n",
      "Iteration: 357800\n",
      "Gradient: [[   0.6962   11.1611  -37.4337 -107.9723   62.8935]]\n",
      "Weights: [[-4.2497 -0.9584 -0.2273  0.1098  0.0852]]\n",
      "MSE loss: 160.6714\n",
      "Iteration: 357900\n",
      "Gradient: [[  -4.5418   -7.7032  -22.5385   34.6915 -178.1718]]\n",
      "Weights: [[-4.249  -0.9584 -0.2273  0.1098  0.0852]]\n",
      "MSE loss: 160.6683\n",
      "Iteration: 358000\n",
      "Gradient: [[ -4.9248  18.5261  20.587   -2.7574 177.2928]]\n",
      "Weights: [[-4.249  -0.9583 -0.2273  0.1098  0.0852]]\n",
      "MSE loss: 160.6608\n",
      "Iteration: 358100\n",
      "Gradient: [[  -1.0218   -2.4353  -43.9524   91.5944 -177.451 ]]\n",
      "Weights: [[-4.2497 -0.958  -0.2274  0.1098  0.0852]]\n",
      "MSE loss: 160.6416\n",
      "Iteration: 358200\n",
      "Gradient: [[ 15.5213 -18.501   42.6421  98.5956 -23.7003]]\n",
      "Weights: [[-4.2501 -0.9579 -0.2275  0.1098  0.0852]]\n",
      "MSE loss: 160.6253\n",
      "Iteration: 358300\n",
      "Gradient: [[  -6.432   -13.5302   33.4847  165.8494 -254.8296]]\n",
      "Weights: [[-4.2511 -0.9575 -0.2275  0.1098  0.0852]]\n",
      "MSE loss: 160.6122\n",
      "Iteration: 358400\n",
      "Gradient: [[ -8.9136  -6.9841  28.886  160.4094 126.9927]]\n",
      "Weights: [[-4.2511 -0.9572 -0.2276  0.1098  0.0852]]\n",
      "MSE loss: 160.5943\n",
      "Iteration: 358500\n",
      "Gradient: [[   2.81     -2.2289  -16.1334  170.2232 -523.0732]]\n",
      "Weights: [[-4.2513 -0.9571 -0.2277  0.1098  0.0852]]\n",
      "MSE loss: 160.5874\n",
      "Iteration: 358600\n",
      "Gradient: [[ -8.3066  -6.3903  16.5461 -48.9621 234.8161]]\n",
      "Weights: [[-4.2514 -0.957  -0.2278  0.1098  0.0852]]\n",
      "MSE loss: 160.57\n",
      "Iteration: 358700\n",
      "Gradient: [[   6.5336  -26.0135   61.3177   81.4279 -281.0552]]\n",
      "Weights: [[-4.2521 -0.9568 -0.2279  0.1098  0.0852]]\n",
      "MSE loss: 160.5537\n",
      "Iteration: 358800\n",
      "Gradient: [[-12.1333 -15.4031  15.3056  48.786  317.4887]]\n",
      "Weights: [[-4.2514 -0.9564 -0.228   0.1098  0.0853]]\n",
      "MSE loss: 160.5322\n",
      "Iteration: 358900\n",
      "Gradient: [[  -4.6159    5.7923   13.3305   36.9362 -348.6185]]\n",
      "Weights: [[-4.2504 -0.9564 -0.2281  0.1098  0.0853]]\n",
      "MSE loss: 160.5257\n",
      "Iteration: 359000\n",
      "Gradient: [[ 1.253000e-01  1.818530e+01  4.114380e+01 -4.016340e+01 -1.958364e+02]]\n",
      "Weights: [[-4.2496 -0.9565 -0.2282  0.1098  0.0853]]\n",
      "MSE loss: 160.5142\n",
      "Iteration: 359100\n",
      "Gradient: [[   6.3872    3.0509   32.5896   93.2282 -152.9052]]\n",
      "Weights: [[-4.2499 -0.9564 -0.2283  0.1098  0.0853]]\n",
      "MSE loss: 160.5007\n",
      "Iteration: 359200\n",
      "Gradient: [[   3.3606   -8.3662   19.2479   -2.7248 -148.7695]]\n",
      "Weights: [[-4.2501 -0.9563 -0.2284  0.1098  0.0853]]\n",
      "MSE loss: 160.4885\n",
      "Iteration: 359300\n",
      "Gradient: [[  9.4275   3.3502   4.8407  28.7761 -82.0172]]\n",
      "Weights: [[-4.2513 -0.9561 -0.2284  0.1098  0.0853]]\n",
      "MSE loss: 160.4746\n",
      "Iteration: 359400\n",
      "Gradient: [[ -7.0129   4.0945  35.8095  32.5388 -35.8499]]\n",
      "Weights: [[-4.2515 -0.956  -0.2284  0.1098  0.0853]]\n",
      "MSE loss: 160.4666\n",
      "Iteration: 359500\n",
      "Gradient: [[ 14.3698 -21.913   39.1317  21.8382 -14.891 ]]\n",
      "Weights: [[-4.2516 -0.9559 -0.2285  0.1097  0.0853]]\n",
      "MSE loss: 160.4568\n",
      "Iteration: 359600\n",
      "Gradient: [[  8.9425   3.8132 -54.0258 -65.5222 215.8027]]\n",
      "Weights: [[-4.2521 -0.9558 -0.2285  0.1097  0.0853]]\n",
      "MSE loss: 160.4491\n",
      "Iteration: 359700\n",
      "Gradient: [[  0.7018 -14.6794  -7.1697   5.6882 -34.2433]]\n",
      "Weights: [[-4.2522 -0.9557 -0.2286  0.1097  0.0853]]\n",
      "MSE loss: 160.4401\n",
      "Iteration: 359800\n",
      "Gradient: [[  5.2214  -5.2482  -0.5259  10.8764 136.8845]]\n",
      "Weights: [[-4.2524 -0.9555 -0.2287  0.1097  0.0853]]\n",
      "MSE loss: 160.4256\n",
      "Iteration: 359900\n",
      "Gradient: [[ -8.5514  -1.9876  83.8973  24.4662 -75.0386]]\n",
      "Weights: [[-4.2509 -0.9552 -0.2287  0.1097  0.0853]]\n",
      "MSE loss: 160.4145\n",
      "Iteration: 360000\n",
      "Gradient: [[-14.7935  14.4945  43.682  -39.2872 -62.1933]]\n",
      "Weights: [[-4.2513 -0.955  -0.2288  0.1097  0.0853]]\n",
      "MSE loss: 160.3985\n",
      "Iteration: 360100\n",
      "Gradient: [[ 2.40000e-03  3.46090e+00  5.47793e+01  6.82310e+00 -1.24886e+02]]\n",
      "Weights: [[-4.2514 -0.9551 -0.229   0.1097  0.0853]]\n",
      "MSE loss: 160.3867\n",
      "Iteration: 360200\n",
      "Gradient: [[  14.2744   -3.3898  -20.5929  -33.3576 -120.7895]]\n",
      "Weights: [[-4.2513 -0.9549 -0.2291  0.1097  0.0853]]\n",
      "MSE loss: 160.3729\n",
      "Iteration: 360300\n",
      "Gradient: [[ -4.5315  -9.0975   1.9968 171.4959  57.8595]]\n",
      "Weights: [[-4.2511 -0.9546 -0.2291  0.1097  0.0853]]\n",
      "MSE loss: 160.3604\n",
      "Iteration: 360400\n",
      "Gradient: [[  7.9082  11.5714  28.8505  27.2763 159.2297]]\n",
      "Weights: [[-4.2517 -0.9545 -0.2292  0.1097  0.0854]]\n",
      "MSE loss: 160.3461\n",
      "Iteration: 360500\n",
      "Gradient: [[  3.909   -2.9767 -35.3421 -34.057   -3.5503]]\n",
      "Weights: [[-4.2523 -0.9545 -0.2292  0.1097  0.0854]]\n",
      "MSE loss: 160.3373\n",
      "Iteration: 360600\n",
      "Gradient: [[ -1.8191  -7.679   -8.9906  92.7116 172.9667]]\n",
      "Weights: [[-4.2523 -0.9543 -0.2293  0.1097  0.0854]]\n",
      "MSE loss: 160.3228\n",
      "Iteration: 360700\n",
      "Gradient: [[  -2.379   -29.8271   -6.1963   19.6119 -475.782 ]]\n",
      "Weights: [[-4.252  -0.9539 -0.2294  0.1097  0.0854]]\n",
      "MSE loss: 160.3088\n",
      "Iteration: 360800\n",
      "Gradient: [[ -7.2663 -13.9113  10.23    55.3226 -12.7575]]\n",
      "Weights: [[-4.2527 -0.9537 -0.2294  0.1097  0.0854]]\n",
      "MSE loss: 160.2936\n",
      "Iteration: 360900\n",
      "Gradient: [[  4.44     3.4495  32.8228  -7.9476 157.5207]]\n",
      "Weights: [[-4.2532 -0.9537 -0.2295  0.1097  0.0854]]\n",
      "MSE loss: 160.2851\n",
      "Iteration: 361000\n",
      "Gradient: [[  15.6423   -2.7187   50.4151 -114.1954   61.0461]]\n",
      "Weights: [[-4.2528 -0.9537 -0.2296  0.1097  0.0854]]\n",
      "MSE loss: 160.2727\n",
      "Iteration: 361100\n",
      "Gradient: [[ -1.6812  -6.171    3.3202 -61.3985 -73.0077]]\n",
      "Weights: [[-4.2526 -0.9536 -0.2297  0.1097  0.0854]]\n",
      "MSE loss: 160.2612\n",
      "Iteration: 361200\n",
      "Gradient: [[-14.8749   0.7499   5.697  -99.0323 134.7334]]\n",
      "Weights: [[-4.2538 -0.9532 -0.2297  0.1097  0.0854]]\n",
      "MSE loss: 160.2512\n",
      "Iteration: 361300\n",
      "Gradient: [[  1.2787  13.5024 -20.1028  -4.6493 195.1733]]\n",
      "Weights: [[-4.2536 -0.953  -0.2298  0.1097  0.0854]]\n",
      "MSE loss: 160.2351\n",
      "Iteration: 361400\n",
      "Gradient: [[ -5.2494 -11.4075  24.1305  42.0122 -61.6938]]\n",
      "Weights: [[-4.2538 -0.9528 -0.2299  0.1096  0.0854]]\n",
      "MSE loss: 160.2219\n",
      "Iteration: 361500\n",
      "Gradient: [[ -5.0079 -11.471   22.7329   8.1869 180.4342]]\n",
      "Weights: [[-4.2533 -0.9528 -0.2299  0.1096  0.0854]]\n",
      "MSE loss: 160.2096\n",
      "Iteration: 361600\n",
      "Gradient: [[ -3.1845  10.2494  -7.4554 -29.7998 -97.6354]]\n",
      "Weights: [[-4.2537 -0.9525 -0.23    0.1096  0.0854]]\n",
      "MSE loss: 160.1975\n",
      "Iteration: 361700\n",
      "Gradient: [[ -4.5055  -3.5415   6.6755  14.1932 212.7249]]\n",
      "Weights: [[-4.2536 -0.9524 -0.2301  0.1096  0.0854]]\n",
      "MSE loss: 160.1842\n",
      "Iteration: 361800\n",
      "Gradient: [[  4.6426   5.0667  -0.3452 -36.2627  74.2488]]\n",
      "Weights: [[-4.2531 -0.9523 -0.2302  0.1096  0.0855]]\n",
      "MSE loss: 160.1728\n",
      "Iteration: 361900\n",
      "Gradient: [[ 14.9813  -6.3758  -7.9692   6.312  -79.5134]]\n",
      "Weights: [[-4.2538 -0.9522 -0.2302  0.1096  0.0855]]\n",
      "MSE loss: 160.1596\n",
      "Iteration: 362000\n",
      "Gradient: [[  6.7462  -6.4986   9.5722   3.3288 311.7409]]\n",
      "Weights: [[-4.2539 -0.9519 -0.2303  0.1096  0.0855]]\n",
      "MSE loss: 160.1463\n",
      "Iteration: 362100\n",
      "Gradient: [[   6.4865    3.0645   -2.6016  -82.0147 -323.4962]]\n",
      "Weights: [[-4.2546 -0.9516 -0.2304  0.1096  0.0855]]\n",
      "MSE loss: 160.1299\n",
      "Iteration: 362200\n",
      "Gradient: [[  -9.6912   -6.1793   15.8749 -166.693   -37.9279]]\n",
      "Weights: [[-4.2553 -0.9512 -0.2305  0.1096  0.0855]]\n",
      "MSE loss: 160.114\n",
      "Iteration: 362300\n",
      "Gradient: [[   5.4804  -22.8499   -5.0442  107.5517 -304.9845]]\n",
      "Weights: [[-4.2548 -0.9512 -0.2306  0.1096  0.0855]]\n",
      "MSE loss: 160.1053\n",
      "Iteration: 362400\n",
      "Gradient: [[  7.2504  -5.3706   9.5654 109.4908 -48.2384]]\n",
      "Weights: [[-4.2549 -0.9511 -0.2307  0.1096  0.0855]]\n",
      "MSE loss: 160.0937\n",
      "Iteration: 362500\n",
      "Gradient: [[  5.6071 -10.9007  11.6286  -1.1607 100.0689]]\n",
      "Weights: [[-4.254  -0.951  -0.2307  0.1096  0.0855]]\n",
      "MSE loss: 160.0823\n",
      "Iteration: 362600\n",
      "Gradient: [[ -7.1324 -11.9088  53.106  -47.4383  89.6471]]\n",
      "Weights: [[-4.2539 -0.951  -0.2308  0.1096  0.0855]]\n",
      "MSE loss: 160.0706\n",
      "Iteration: 362700\n",
      "Gradient: [[-2.686700e+00 -8.124600e+00  2.293210e+01 -2.587000e-01 -2.716746e+02]]\n",
      "Weights: [[-4.2537 -0.9507 -0.2309  0.1096  0.0855]]\n",
      "MSE loss: 160.0596\n",
      "Iteration: 362800\n",
      "Gradient: [[  -1.1155  -18.3497   20.1682  -75.6006 -102.234 ]]\n",
      "Weights: [[-4.2532 -0.9506 -0.231   0.1096  0.0855]]\n",
      "MSE loss: 160.0503\n",
      "Iteration: 362900\n",
      "Gradient: [[   7.3347  -16.7473   50.4895 -175.6311  -30.1742]]\n",
      "Weights: [[-4.2538 -0.9506 -0.231   0.1096  0.0855]]\n",
      "MSE loss: 160.0388\n",
      "Iteration: 363000\n",
      "Gradient: [[ -12.4886  -19.7174   20.7155   43.0432 -114.5281]]\n",
      "Weights: [[-4.2548 -0.9504 -0.2311  0.1096  0.0855]]\n",
      "MSE loss: 160.025\n",
      "Iteration: 363100\n",
      "Gradient: [[ 10.4544  19.223  -28.4418 159.5888  43.5694]]\n",
      "Weights: [[-4.2549 -0.9504 -0.2312  0.1096  0.0855]]\n",
      "MSE loss: 160.0154\n",
      "Iteration: 363200\n",
      "Gradient: [[ -3.3356  -5.6086  13.1399  18.4447 102.4182]]\n",
      "Weights: [[-4.2546 -0.9502 -0.2313  0.1096  0.0855]]\n",
      "MSE loss: 160.0025\n",
      "Iteration: 363300\n",
      "Gradient: [[-3.739900e+00 -1.573130e+01  8.750000e-02  4.042770e+01 -3.929668e+02]]\n",
      "Weights: [[-4.2538 -0.9501 -0.2313  0.1096  0.0855]]\n",
      "MSE loss: 159.9924\n",
      "Iteration: 363400\n",
      "Gradient: [[ -3.9629  -2.6011  64.5461 130.9846 -48.9871]]\n",
      "Weights: [[-4.2548 -0.9498 -0.2314  0.1096  0.0856]]\n",
      "MSE loss: 159.9771\n",
      "Iteration: 363500\n",
      "Gradient: [[ 21.742   -3.1134 -13.912   36.5908  54.2798]]\n",
      "Weights: [[-4.2546 -0.9497 -0.2315  0.1096  0.0856]]\n",
      "MSE loss: 159.9662\n",
      "Iteration: 363600\n",
      "Gradient: [[ -9.7233 -15.7314   0.32    58.8383 169.7267]]\n",
      "Weights: [[-4.2552 -0.9495 -0.2315  0.1095  0.0856]]\n",
      "MSE loss: 159.9535\n",
      "Iteration: 363700\n",
      "Gradient: [[-14.7239  22.54     4.4713 -33.      79.9581]]\n",
      "Weights: [[-4.2557 -0.9495 -0.2316  0.1095  0.0856]]\n",
      "MSE loss: 159.9443\n",
      "Iteration: 363800\n",
      "Gradient: [[-15.4345   3.835  -25.3313  20.7685 376.821 ]]\n",
      "Weights: [[-4.2548 -0.9494 -0.2316  0.1095  0.0856]]\n",
      "MSE loss: 159.9377\n",
      "Iteration: 363900\n",
      "Gradient: [[  9.804   11.5993  12.8081 -36.1261  38.1303]]\n",
      "Weights: [[-4.2544 -0.9495 -0.2316  0.1095  0.0856]]\n",
      "MSE loss: 159.9343\n",
      "Iteration: 364000\n",
      "Gradient: [[  -4.4677   -4.4063  -51.8432   36.0472 -114.5396]]\n",
      "Weights: [[-4.255  -0.9497 -0.2317  0.1095  0.0856]]\n",
      "MSE loss: 159.9333\n",
      "Iteration: 364100\n",
      "Gradient: [[ -17.6374  -15.563     1.0897   72.0314 -473.9431]]\n",
      "Weights: [[-4.2548 -0.9496 -0.2317  0.1095  0.0856]]\n",
      "MSE loss: 159.9211\n",
      "Iteration: 364200\n",
      "Gradient: [[ -18.6071  -10.3975   53.1486   11.6989 -339.8135]]\n",
      "Weights: [[-4.2536 -0.9493 -0.2318  0.1095  0.0856]]\n",
      "MSE loss: 159.9103\n",
      "Iteration: 364300\n",
      "Gradient: [[-14.0306   1.9115  40.5082 -84.9806 165.9385]]\n",
      "Weights: [[-4.2551 -0.9493 -0.2318  0.1095  0.0856]]\n",
      "MSE loss: 159.8996\n",
      "Iteration: 364400\n",
      "Gradient: [[ -2.0346 -14.8702 -21.2818 -17.6543 227.8157]]\n",
      "Weights: [[-4.2554 -0.9492 -0.2319  0.1095  0.0856]]\n",
      "MSE loss: 159.8899\n",
      "Iteration: 364500\n",
      "Gradient: [[ 13.9011  -2.7394  -1.9874 -83.8657  41.312 ]]\n",
      "Weights: [[-4.2554 -0.9488 -0.232   0.1095  0.0856]]\n",
      "MSE loss: 159.8755\n",
      "Iteration: 364600\n",
      "Gradient: [[16.3794 -5.2616 -9.4544 23.0368 82.5313]]\n",
      "Weights: [[-4.2553 -0.9485 -0.232   0.1095  0.0856]]\n",
      "MSE loss: 159.8617\n",
      "Iteration: 364700\n",
      "Gradient: [[ -3.0297   0.2558  29.8439  63.5425 -79.2519]]\n",
      "Weights: [[-4.255  -0.9484 -0.2322  0.1095  0.0856]]\n",
      "MSE loss: 159.8481\n",
      "Iteration: 364800\n",
      "Gradient: [[  10.1411   10.3381   23.3765  -18.0013 -202.8049]]\n",
      "Weights: [[-4.2541 -0.9485 -0.2323  0.1095  0.0856]]\n",
      "MSE loss: 159.8361\n",
      "Iteration: 364900\n",
      "Gradient: [[ -5.41     9.7055  10.282  -86.0332 134.8545]]\n",
      "Weights: [[-4.256  -0.9482 -0.2323  0.1095  0.0856]]\n",
      "MSE loss: 159.8229\n",
      "Iteration: 365000\n",
      "Gradient: [[  12.2201    5.2301  -19.2046   65.7957 -164.3584]]\n",
      "Weights: [[-4.2557 -0.9481 -0.2324  0.1095  0.0856]]\n",
      "MSE loss: 159.8081\n",
      "Iteration: 365100\n",
      "Gradient: [[ -4.739   13.2306 -38.0365 -46.4378 194.8117]]\n",
      "Weights: [[-4.2563 -0.9479 -0.2325  0.1095  0.0856]]\n",
      "MSE loss: 159.7981\n",
      "Iteration: 365200\n",
      "Gradient: [[   1.8449  -13.5221   15.7343   60.9569 -143.7564]]\n",
      "Weights: [[-4.2566 -0.9475 -0.2326  0.1095  0.0857]]\n",
      "MSE loss: 159.7837\n",
      "Iteration: 365300\n",
      "Gradient: [[ -5.3211  18.2949  17.5876  43.0335 123.7047]]\n",
      "Weights: [[-4.2562 -0.9471 -0.2326  0.1095  0.0857]]\n",
      "MSE loss: 159.7661\n",
      "Iteration: 365400\n",
      "Gradient: [[  -8.8841  -10.7731   14.9386   63.3034 -110.7368]]\n",
      "Weights: [[-4.2571 -0.947  -0.2327  0.1095  0.0857]]\n",
      "MSE loss: 159.7585\n",
      "Iteration: 365500\n",
      "Gradient: [[ 17.4145   7.1354  28.5414 -10.0507 -52.2588]]\n",
      "Weights: [[-4.2562 -0.9469 -0.2328  0.1095  0.0857]]\n",
      "MSE loss: 159.7467\n",
      "Iteration: 365600\n",
      "Gradient: [[ -1.5463   5.1865  68.5394 -44.3666  82.9538]]\n",
      "Weights: [[-4.2571 -0.9467 -0.2329  0.1095  0.0857]]\n",
      "MSE loss: 159.7338\n",
      "Iteration: 365700\n",
      "Gradient: [[   4.4935   21.3996  -35.0654 -106.2283   19.156 ]]\n",
      "Weights: [[-4.257  -0.9466 -0.2329  0.1095  0.0857]]\n",
      "MSE loss: 159.7232\n",
      "Iteration: 365800\n",
      "Gradient: [[ -13.9988    1.9495    7.8603  -58.774  -225.6648]]\n",
      "Weights: [[-4.2571 -0.9463 -0.2331  0.1095  0.0857]]\n",
      "MSE loss: 159.7074\n",
      "Iteration: 365900\n",
      "Gradient: [[  2.1524   7.6449  19.7324 -41.5959 315.8378]]\n",
      "Weights: [[-4.2583 -0.9462 -0.2331  0.1095  0.0857]]\n",
      "MSE loss: 159.7004\n",
      "Iteration: 366000\n",
      "Gradient: [[   7.9441    3.56     42.1572  -34.6769 -289.3027]]\n",
      "Weights: [[-4.257  -0.946  -0.2332  0.1095  0.0857]]\n",
      "MSE loss: 159.6833\n",
      "Iteration: 366100\n",
      "Gradient: [[-11.7295   4.7053  19.6728  31.6729  46.928 ]]\n",
      "Weights: [[-4.257  -0.9457 -0.2333  0.1095  0.0857]]\n",
      "MSE loss: 159.6657\n",
      "Iteration: 366200\n",
      "Gradient: [[  -5.6194  -10.841    24.4887 -170.3128 -159.4491]]\n",
      "Weights: [[-4.2569 -0.9455 -0.2334  0.1094  0.0857]]\n",
      "MSE loss: 159.6515\n",
      "Iteration: 366300\n",
      "Gradient: [[  17.4077   -3.1937   12.4331   42.3578 -210.6001]]\n",
      "Weights: [[-4.2565 -0.9456 -0.2335  0.1094  0.0857]]\n",
      "MSE loss: 159.6401\n",
      "Iteration: 366400\n",
      "Gradient: [[   7.8997   -8.6605  -14.7511  257.8957 -244.5526]]\n",
      "Weights: [[-4.2563 -0.9457 -0.2335  0.1094  0.0857]]\n",
      "MSE loss: 159.6359\n",
      "Iteration: 366500\n",
      "Gradient: [[ -12.7933  -18.6082  -22.6858   27.1483 -187.9969]]\n",
      "Weights: [[-4.2562 -0.9455 -0.2336  0.1094  0.0857]]\n",
      "MSE loss: 159.6236\n",
      "Iteration: 366600\n",
      "Gradient: [[ -10.7216  -15.5542  -14.3242   10.6451 -135.1144]]\n",
      "Weights: [[-4.2567 -0.9454 -0.2338  0.1094  0.0857]]\n",
      "MSE loss: 159.6068\n",
      "Iteration: 366700\n",
      "Gradient: [[ -1.6237   6.8665   1.136   76.0912 -58.8972]]\n",
      "Weights: [[-4.2569 -0.9451 -0.2339  0.1094  0.0857]]\n",
      "MSE loss: 159.5911\n",
      "Iteration: 366800\n",
      "Gradient: [[  14.8615    0.3031   62.6545  -46.6692 -183.6757]]\n",
      "Weights: [[-4.258  -0.9449 -0.2339  0.1094  0.0857]]\n",
      "MSE loss: 159.5812\n",
      "Iteration: 366900\n",
      "Gradient: [[  15.1201    2.9077   48.9127   32.0334 -298.1312]]\n",
      "Weights: [[-4.2564 -0.945  -0.234   0.1094  0.0858]]\n",
      "MSE loss: 159.5698\n",
      "Iteration: 367000\n",
      "Gradient: [[ -15.6067    4.9399   35.9353 -147.6576  -89.3547]]\n",
      "Weights: [[-4.257  -0.9449 -0.2341  0.1094  0.0858]]\n",
      "MSE loss: 159.5578\n",
      "Iteration: 367100\n",
      "Gradient: [[  -0.8959   -1.7956   25.1919   66.1338 -302.0932]]\n",
      "Weights: [[-4.2569 -0.9448 -0.2341  0.1094  0.0858]]\n",
      "MSE loss: 159.5493\n",
      "Iteration: 367200\n",
      "Gradient: [[  -4.9652   -8.8016   12.8634    8.834  -155.8422]]\n",
      "Weights: [[-4.2568 -0.9445 -0.2342  0.1094  0.0858]]\n",
      "MSE loss: 159.5346\n",
      "Iteration: 367300\n",
      "Gradient: [[-11.1422   4.0177 -19.4579  67.6773  -1.692 ]]\n",
      "Weights: [[-4.2561 -0.9443 -0.2343  0.1093  0.0858]]\n",
      "MSE loss: 159.5189\n",
      "Iteration: 367400\n",
      "Gradient: [[  -1.2586    6.1177   29.1363   46.2986 -447.2325]]\n",
      "Weights: [[-4.257  -0.9442 -0.2344  0.1093  0.0858]]\n",
      "MSE loss: 159.5063\n",
      "Iteration: 367500\n",
      "Gradient: [[  11.053    -0.665     4.5156   71.0895 -282.1122]]\n",
      "Weights: [[-4.2572 -0.944  -0.2345  0.1093  0.0858]]\n",
      "MSE loss: 159.4931\n",
      "Iteration: 367600\n",
      "Gradient: [[  -1.8911   -1.4821  -40.5336   58.9801 -266.8998]]\n",
      "Weights: [[-4.2575 -0.9437 -0.2346  0.1093  0.0858]]\n",
      "MSE loss: 159.4763\n",
      "Iteration: 367700\n",
      "Gradient: [[   4.6731   10.7971    3.8458  -56.1001 -223.039 ]]\n",
      "Weights: [[-4.2569 -0.9437 -0.2347  0.1093  0.0858]]\n",
      "MSE loss: 159.4663\n",
      "Iteration: 367800\n",
      "Gradient: [[  -6.7177   14.8258   14.9478   41.4784 -132.6243]]\n",
      "Weights: [[-4.258  -0.9436 -0.2347  0.1093  0.0858]]\n",
      "MSE loss: 159.4554\n",
      "Iteration: 367900\n",
      "Gradient: [[ -4.8191 -11.5052 -48.065  -94.6132 -22.94  ]]\n",
      "Weights: [[-4.2577 -0.9436 -0.2348  0.1093  0.0858]]\n",
      "MSE loss: 159.4465\n",
      "Iteration: 368000\n",
      "Gradient: [[ 0.3651  5.421  13.9564 35.359  23.223 ]]\n",
      "Weights: [[-4.2575 -0.9436 -0.2349  0.1093  0.0858]]\n",
      "MSE loss: 159.436\n",
      "Iteration: 368100\n",
      "Gradient: [[-16.0241  -8.9976  -7.0671  76.1073 -29.4358]]\n",
      "Weights: [[-4.2581 -0.9436 -0.235   0.1093  0.0858]]\n",
      "MSE loss: 159.4274\n",
      "Iteration: 368200\n",
      "Gradient: [[  -8.528     1.7773   65.9777  -33.3238 -241.2273]]\n",
      "Weights: [[-4.2564 -0.9436 -0.235   0.1093  0.0858]]\n",
      "MSE loss: 159.4116\n",
      "Iteration: 368300\n",
      "Gradient: [[  6.1678  16.2565  19.4212 103.0723 -90.9524]]\n",
      "Weights: [[-4.2554 -0.9437 -0.2351  0.1093  0.0859]]\n",
      "MSE loss: 159.4017\n",
      "Iteration: 368400\n",
      "Gradient: [[  -0.7323   -2.4203   -1.5951  126.2201 -197.326 ]]\n",
      "Weights: [[-4.2556 -0.9435 -0.2351  0.1093  0.0859]]\n",
      "MSE loss: 159.3915\n",
      "Iteration: 368500\n",
      "Gradient: [[  -3.9127   -7.3428    5.7249  167.8209 -499.0803]]\n",
      "Weights: [[-4.2552 -0.9436 -0.2352  0.1093  0.0859]]\n",
      "MSE loss: 159.3839\n",
      "Iteration: 368600\n",
      "Gradient: [[ 10.2685 -12.3926  -8.0704 -45.5914 254.843 ]]\n",
      "Weights: [[-4.2565 -0.9433 -0.2353  0.1093  0.0859]]\n",
      "MSE loss: 159.3657\n",
      "Iteration: 368700\n",
      "Gradient: [[   4.3107    0.6523  -38.5314 -198.1412 -290.769 ]]\n",
      "Weights: [[-4.2563 -0.9431 -0.2354  0.1093  0.0859]]\n",
      "MSE loss: 159.3504\n",
      "Iteration: 368800\n",
      "Gradient: [[   3.388    11.6327   26.8833   61.969  -137.1426]]\n",
      "Weights: [[-4.2559 -0.9431 -0.2354  0.1092  0.0859]]\n",
      "MSE loss: 159.3408\n",
      "Iteration: 368900\n",
      "Gradient: [[ -2.8271   2.0221  22.6413 -18.2599  99.6606]]\n",
      "Weights: [[-4.255  -0.9429 -0.2355  0.1092  0.0859]]\n",
      "MSE loss: 159.3329\n",
      "Iteration: 369000\n",
      "Gradient: [[ -4.0386  -2.1513  33.5986  45.2267 -96.1021]]\n",
      "Weights: [[-4.2551 -0.9429 -0.2356  0.1092  0.0859]]\n",
      "MSE loss: 159.3218\n",
      "Iteration: 369100\n",
      "Gradient: [[ -1.7346  -8.7957 -45.6983 -37.7253 246.513 ]]\n",
      "Weights: [[-4.2562 -0.943  -0.2357  0.1092  0.0859]]\n",
      "MSE loss: 159.3106\n",
      "Iteration: 369200\n",
      "Gradient: [[  4.1784 -10.0351  54.0243  68.1116   0.4149]]\n",
      "Weights: [[-4.2556 -0.9426 -0.2357  0.1092  0.0859]]\n",
      "MSE loss: 159.2959\n",
      "Iteration: 369300\n",
      "Gradient: [[ 1.393160e+01 -9.940000e-02  5.322860e+01  9.087420e+01  3.889873e+02]]\n",
      "Weights: [[-4.2563 -0.9423 -0.2358  0.1092  0.0859]]\n",
      "MSE loss: 159.2776\n",
      "Iteration: 369400\n",
      "Gradient: [[  -2.7178  -14.2861   73.4967  147.7338 -256.2323]]\n",
      "Weights: [[-4.2563 -0.9422 -0.2359  0.1092  0.0859]]\n",
      "MSE loss: 159.2675\n",
      "Iteration: 369500\n",
      "Gradient: [[  -7.0365   -1.4744    6.6912   71.289  -184.1845]]\n",
      "Weights: [[-4.2565 -0.942  -0.236   0.1092  0.0859]]\n",
      "MSE loss: 159.2527\n",
      "Iteration: 369600\n",
      "Gradient: [[  6.5529  -0.9261   5.1156  23.3925 278.1136]]\n",
      "Weights: [[-4.2574 -0.9418 -0.2361  0.1092  0.0859]]\n",
      "MSE loss: 159.2419\n",
      "Iteration: 369700\n",
      "Gradient: [[ -2.1515 -16.4538  17.5311  42.6407  80.7278]]\n",
      "Weights: [[-4.2575 -0.9416 -0.2362  0.1092  0.0859]]\n",
      "MSE loss: 159.2285\n",
      "Iteration: 369800\n",
      "Gradient: [[  1.8471 -12.9428 -31.0147  12.0555 -61.7244]]\n",
      "Weights: [[-4.2577 -0.9412 -0.2362  0.1092  0.0859]]\n",
      "MSE loss: 159.2156\n",
      "Iteration: 369900\n",
      "Gradient: [[  6.7983   1.7307 -29.4364   6.7566   5.0202]]\n",
      "Weights: [[-4.2575 -0.9411 -0.2362  0.1092  0.0859]]\n",
      "MSE loss: 159.2097\n",
      "Iteration: 370000\n",
      "Gradient: [[  3.3614  13.0686  24.4627  69.1517 315.9414]]\n",
      "Weights: [[-4.2586 -0.9408 -0.2363  0.1092  0.086 ]]\n",
      "MSE loss: 159.1996\n",
      "Iteration: 370100\n",
      "Gradient: [[  -4.6724    0.591   -17.1986   22.2918 -169.4567]]\n",
      "Weights: [[-4.2589 -0.9405 -0.2363  0.1092  0.086 ]]\n",
      "MSE loss: 159.1911\n",
      "Iteration: 370200\n",
      "Gradient: [[ -0.2403  -9.6209 -34.0562  53.7661 -59.4426]]\n",
      "Weights: [[-4.2594 -0.9403 -0.2364  0.1092  0.086 ]]\n",
      "MSE loss: 159.1791\n",
      "Iteration: 370300\n",
      "Gradient: [[  5.4688 -13.3474  21.4222  14.9617 -17.9743]]\n",
      "Weights: [[-4.2595 -0.94   -0.2365  0.1092  0.086 ]]\n",
      "MSE loss: 159.1631\n",
      "Iteration: 370400\n",
      "Gradient: [[-13.3619  24.3767   3.9064 -17.1672  26.3157]]\n",
      "Weights: [[-4.2595 -0.9399 -0.2366  0.1092  0.086 ]]\n",
      "MSE loss: 159.152\n",
      "Iteration: 370500\n",
      "Gradient: [[ -7.2482  -1.4664  39.823  114.9723  23.956 ]]\n",
      "Weights: [[-4.2594 -0.9398 -0.2366  0.1091  0.086 ]]\n",
      "MSE loss: 159.1448\n",
      "Iteration: 370600\n",
      "Gradient: [[  0.2914   8.6413   6.7903 -25.9539 -58.3603]]\n",
      "Weights: [[-4.2587 -0.9397 -0.2367  0.1091  0.086 ]]\n",
      "MSE loss: 159.1325\n",
      "Iteration: 370700\n",
      "Gradient: [[  7.5858   8.7142  30.5229 -42.4238  86.6454]]\n",
      "Weights: [[-4.2594 -0.9395 -0.2368  0.1091  0.086 ]]\n",
      "MSE loss: 159.1215\n",
      "Iteration: 370800\n",
      "Gradient: [[  -9.041   -16.6222  -19.3211  -33.5254 -253.5103]]\n",
      "Weights: [[-4.2584 -0.9392 -0.2368  0.1091  0.086 ]]\n",
      "MSE loss: 159.1081\n",
      "Iteration: 370900\n",
      "Gradient: [[ 0.1895 -1.6964 13.4936 17.2927 71.4522]]\n",
      "Weights: [[-4.2594 -0.9389 -0.2369  0.1091  0.086 ]]\n",
      "MSE loss: 159.098\n",
      "Iteration: 371000\n",
      "Gradient: [[ 1.660000e-01  1.223100e+01 -1.926950e+01 -4.168650e+01 -2.756537e+02]]\n",
      "Weights: [[-4.2596 -0.9389 -0.237   0.1091  0.086 ]]\n",
      "MSE loss: 159.0877\n",
      "Iteration: 371100\n",
      "Gradient: [[  7.0126 -10.4828 -13.2402  33.1131 -86.1311]]\n",
      "Weights: [[-4.2593 -0.9387 -0.237   0.1091  0.086 ]]\n",
      "MSE loss: 159.0713\n",
      "Iteration: 371200\n",
      "Gradient: [[   5.8725    5.7219  -18.2904  -46.699  -225.0648]]\n",
      "Weights: [[-4.2594 -0.9384 -0.2371  0.1091  0.086 ]]\n",
      "MSE loss: 159.0557\n",
      "Iteration: 371300\n",
      "Gradient: [[ 13.7188  13.6459   1.8857   8.9379 179.1566]]\n",
      "Weights: [[-4.2581 -0.9384 -0.2373  0.1091  0.086 ]]\n",
      "MSE loss: 159.0423\n",
      "Iteration: 371400\n",
      "Gradient: [[ -5.974   12.4788  25.2853  78.9969 160.0227]]\n",
      "Weights: [[-4.2588 -0.9382 -0.2374  0.1091  0.086 ]]\n",
      "MSE loss: 159.0281\n",
      "Iteration: 371500\n",
      "Gradient: [[ 12.3611  -3.8369  -4.035    4.8024 133.1437]]\n",
      "Weights: [[-4.2588 -0.9381 -0.2375  0.1091  0.086 ]]\n",
      "MSE loss: 159.0152\n",
      "Iteration: 371600\n",
      "Gradient: [[ -5.6338  31.5276   6.7878  38.3787 226.0194]]\n",
      "Weights: [[-4.2586 -0.938  -0.2375  0.1091  0.086 ]]\n",
      "MSE loss: 159.0094\n",
      "Iteration: 371700\n",
      "Gradient: [[  -1.7813    1.5344   14.8702   93.0361 -220.6026]]\n",
      "Weights: [[-4.258  -0.9381 -0.2375  0.1091  0.086 ]]\n",
      "MSE loss: 159.0026\n",
      "Iteration: 371800\n",
      "Gradient: [[   4.66      4.7234   42.9963  129.4772 -352.2516]]\n",
      "Weights: [[-4.2591 -0.9379 -0.2377  0.1091  0.086 ]]\n",
      "MSE loss: 158.9839\n",
      "Iteration: 371900\n",
      "Gradient: [[   6.0434   -4.1896   10.5542   72.7924 -106.0206]]\n",
      "Weights: [[-4.2592 -0.9377 -0.2378  0.1091  0.0861]]\n",
      "MSE loss: 158.9743\n",
      "Iteration: 372000\n",
      "Gradient: [[  3.4545  -5.6026  -6.7653 -57.3067  99.9549]]\n",
      "Weights: [[-4.259  -0.9377 -0.2379  0.1091  0.0861]]\n",
      "MSE loss: 158.9595\n",
      "Iteration: 372100\n",
      "Gradient: [[   2.2816   -4.797   -28.4935   15.9502 -197.6933]]\n",
      "Weights: [[-4.2587 -0.9376 -0.238   0.1091  0.0861]]\n",
      "MSE loss: 158.947\n",
      "Iteration: 372200\n",
      "Gradient: [[  -0.6301   11.0835  -18.5056  -74.1145 -215.4595]]\n",
      "Weights: [[-4.2604 -0.9374 -0.238   0.109   0.0861]]\n",
      "MSE loss: 158.9413\n",
      "Iteration: 372300\n",
      "Gradient: [[  7.552  -21.032    6.1765  37.3671 117.0035]]\n",
      "Weights: [[-4.2601 -0.9371 -0.2381  0.109   0.0861]]\n",
      "MSE loss: 158.9223\n",
      "Iteration: 372400\n",
      "Gradient: [[ -4.4549  -1.1074   2.6185 -67.6694  50.4492]]\n",
      "Weights: [[-4.2593 -0.937  -0.2382  0.109   0.0861]]\n",
      "MSE loss: 158.9091\n",
      "Iteration: 372500\n",
      "Gradient: [[  -5.8311  -16.1152  -11.615   -24.8131 -141.977 ]]\n",
      "Weights: [[-4.2583 -0.937  -0.2382  0.109   0.0861]]\n",
      "MSE loss: 158.9014\n",
      "Iteration: 372600\n",
      "Gradient: [[   2.6498   -7.787     3.5974 -151.7184  209.6055]]\n",
      "Weights: [[-4.2586 -0.9369 -0.2383  0.109   0.0861]]\n",
      "MSE loss: 158.8889\n",
      "Iteration: 372700\n",
      "Gradient: [[ 20.2365  -5.1168 -42.6855   2.0993 141.5888]]\n",
      "Weights: [[-4.2589 -0.9368 -0.2383  0.109   0.0861]]\n",
      "MSE loss: 158.8799\n",
      "Iteration: 372800\n",
      "Gradient: [[  -1.7947  -13.3252   58.7643  -46.6321 -389.7521]]\n",
      "Weights: [[-4.2591 -0.9366 -0.2384  0.109   0.0861]]\n",
      "MSE loss: 158.8668\n",
      "Iteration: 372900\n",
      "Gradient: [[ -4.0559   8.3269  13.9406  60.1817 -58.8323]]\n",
      "Weights: [[-4.2599 -0.9363 -0.2385  0.109   0.0861]]\n",
      "MSE loss: 158.8487\n",
      "Iteration: 373000\n",
      "Gradient: [[  9.5536  10.5226  -5.6744  34.2142 -23.7107]]\n",
      "Weights: [[-4.26   -0.9358 -0.2386  0.109   0.0861]]\n",
      "MSE loss: 158.8323\n",
      "Iteration: 373100\n",
      "Gradient: [[  -3.9215    3.568    16.9368  -22.2931 -428.1061]]\n",
      "Weights: [[-4.2607 -0.9357 -0.2387  0.109   0.0861]]\n",
      "MSE loss: 158.821\n",
      "Iteration: 373200\n",
      "Gradient: [[ -6.1408 -10.8039  -1.4596 -57.4854  26.5869]]\n",
      "Weights: [[-4.2594 -0.9355 -0.2388  0.109   0.0861]]\n",
      "MSE loss: 158.8075\n",
      "Iteration: 373300\n",
      "Gradient: [[  7.6647  19.4182  43.9851  60.4176 -99.5168]]\n",
      "Weights: [[-4.2592 -0.9356 -0.2388  0.109   0.0861]]\n",
      "MSE loss: 158.8021\n",
      "Iteration: 373400\n",
      "Gradient: [[-6.3193  1.5312 41.7146 80.7942 81.692 ]]\n",
      "Weights: [[-4.2587 -0.9355 -0.2389  0.109   0.0861]]\n",
      "MSE loss: 158.79\n",
      "Iteration: 373500\n",
      "Gradient: [[ -17.4545   -2.5719   19.622    34.4085 -322.4948]]\n",
      "Weights: [[-4.2583 -0.9355 -0.239   0.109   0.0862]]\n",
      "MSE loss: 158.7765\n",
      "Iteration: 373600\n",
      "Gradient: [[  -4.2689   12.7606   31.244   -29.1565 -381.0033]]\n",
      "Weights: [[-4.2581 -0.9355 -0.2391  0.109   0.0862]]\n",
      "MSE loss: 158.7684\n",
      "Iteration: 373700\n",
      "Gradient: [[ -5.9132   8.7271  -5.9541 -56.5925 -12.6538]]\n",
      "Weights: [[-4.257  -0.9353 -0.2392  0.109   0.0862]]\n",
      "MSE loss: 158.7594\n",
      "Iteration: 373800\n",
      "Gradient: [[-21.8213  -6.06   -16.9687  80.6104 194.7783]]\n",
      "Weights: [[-4.2568 -0.9353 -0.2393  0.109   0.0862]]\n",
      "MSE loss: 158.747\n",
      "Iteration: 373900\n",
      "Gradient: [[   8.1838   -4.4025   67.9825  -67.2275 -108.7459]]\n",
      "Weights: [[-4.2579 -0.9353 -0.2394  0.1089  0.0862]]\n",
      "MSE loss: 158.7276\n",
      "Iteration: 374000\n",
      "Gradient: [[   6.0747    4.7692   13.9926   -5.5059 -168.7022]]\n",
      "Weights: [[-4.258  -0.9351 -0.2395  0.1089  0.0862]]\n",
      "MSE loss: 158.7104\n",
      "Iteration: 374100\n",
      "Gradient: [[ -10.5281  -26.1874   19.8251   48.0505 -120.5792]]\n",
      "Weights: [[-4.259  -0.935  -0.2396  0.1089  0.0862]]\n",
      "MSE loss: 158.6987\n",
      "Iteration: 374200\n",
      "Gradient: [[  -1.9198    2.7068   52.7166  -79.2466 -473.1372]]\n",
      "Weights: [[-4.2586 -0.9348 -0.2396  0.1089  0.0862]]\n",
      "MSE loss: 158.6876\n",
      "Iteration: 374300\n",
      "Gradient: [[ -3.0068 -12.7419  46.2464  39.7906 -28.4766]]\n",
      "Weights: [[-4.2595 -0.9343 -0.2397  0.1089  0.0862]]\n",
      "MSE loss: 158.6663\n",
      "Iteration: 374400\n",
      "Gradient: [[ -3.028   -5.8028  36.5625  17.5297 279.0677]]\n",
      "Weights: [[-4.26   -0.9342 -0.2398  0.1089  0.0862]]\n",
      "MSE loss: 158.6546\n",
      "Iteration: 374500\n",
      "Gradient: [[ -1.4407   9.1793   9.4361 -18.108  -38.0256]]\n",
      "Weights: [[-4.2595 -0.9341 -0.2399  0.1089  0.0862]]\n",
      "MSE loss: 158.645\n",
      "Iteration: 374600\n",
      "Gradient: [[-6.2761  3.4409 80.8581 -0.9718 59.4887]]\n",
      "Weights: [[-4.2602 -0.9338 -0.24    0.109   0.0862]]\n",
      "MSE loss: 158.6303\n",
      "Iteration: 374700\n",
      "Gradient: [[  -1.7108   -3.3793   12.4059   94.3235 -363.9134]]\n",
      "Weights: [[-4.2603 -0.9336 -0.2401  0.109   0.0862]]\n",
      "MSE loss: 158.6202\n",
      "Iteration: 374800\n",
      "Gradient: [[   2.072   -12.1049   33.7576   67.6392 -461.7976]]\n",
      "Weights: [[-4.2604 -0.9335 -0.2401  0.1089  0.0862]]\n",
      "MSE loss: 158.6071\n",
      "Iteration: 374900\n",
      "Gradient: [[  -2.1067    2.822     1.4064  -30.5826 -187.664 ]]\n",
      "Weights: [[-4.2612 -0.9333 -0.2402  0.1089  0.0862]]\n",
      "MSE loss: 158.5964\n",
      "Iteration: 375000\n",
      "Gradient: [[ -3.9744  -7.9926  -8.4362  22.6782 256.5374]]\n",
      "Weights: [[-4.2596 -0.9333 -0.2403  0.1089  0.0863]]\n",
      "MSE loss: 158.5867\n",
      "Iteration: 375100\n",
      "Gradient: [[  1.6031   1.8762   0.9518  31.5707 191.7633]]\n",
      "Weights: [[-4.2585 -0.9331 -0.2404  0.1089  0.0863]]\n",
      "MSE loss: 158.5755\n",
      "Iteration: 375200\n",
      "Gradient: [[ 8.620000e-02 -5.507500e+00  5.084220e+01  5.087200e+01  2.120723e+02]]\n",
      "Weights: [[-4.2589 -0.9332 -0.2405  0.1089  0.0863]]\n",
      "MSE loss: 158.5608\n",
      "Iteration: 375300\n",
      "Gradient: [[   3.0731   11.1893   59.3498   -6.9466 -187.0135]]\n",
      "Weights: [[-4.2581 -0.933  -0.2406  0.1089  0.0863]]\n",
      "MSE loss: 158.551\n",
      "Iteration: 375400\n",
      "Gradient: [[ -4.714   -7.1616  -0.7336 -67.1937 217.24  ]]\n",
      "Weights: [[-4.2594 -0.9331 -0.2406  0.1089  0.0863]]\n",
      "MSE loss: 158.5383\n",
      "Iteration: 375500\n",
      "Gradient: [[  6.0017   9.3438  64.0241  27.4113 -51.4548]]\n",
      "Weights: [[-4.2588 -0.9329 -0.2407  0.1088  0.0863]]\n",
      "MSE loss: 158.5249\n",
      "Iteration: 375600\n",
      "Gradient: [[-7.1979 11.3135 -0.7038  9.9246  4.8004]]\n",
      "Weights: [[-4.26   -0.9326 -0.2408  0.1088  0.0863]]\n",
      "MSE loss: 158.5098\n",
      "Iteration: 375700\n",
      "Gradient: [[   1.8933  -11.2878  -20.1052 -162.2909   94.5979]]\n",
      "Weights: [[-4.2595 -0.9324 -0.2408  0.1088  0.0863]]\n",
      "MSE loss: 158.4998\n",
      "Iteration: 375800\n",
      "Gradient: [[   3.5516   15.0631  -16.1684   -5.3062 -197.3527]]\n",
      "Weights: [[-4.2592 -0.9323 -0.2409  0.1088  0.0863]]\n",
      "MSE loss: 158.4921\n",
      "Iteration: 375900\n",
      "Gradient: [[  -2.3688   -4.5148  -34.1877  -11.0698 -126.7872]]\n",
      "Weights: [[-4.2594 -0.9321 -0.2409  0.1088  0.0863]]\n",
      "MSE loss: 158.4807\n",
      "Iteration: 376000\n",
      "Gradient: [[ -5.4345 -12.5467 -36.7807 -16.9237 -73.4882]]\n",
      "Weights: [[-4.2594 -0.9319 -0.241   0.1088  0.0863]]\n",
      "MSE loss: 158.4703\n",
      "Iteration: 376100\n",
      "Gradient: [[  -7.7016   -8.9119    9.7032 -110.9315  149.2365]]\n",
      "Weights: [[-4.2584 -0.9317 -0.241   0.1088  0.0863]]\n",
      "MSE loss: 158.4688\n",
      "Iteration: 376200\n",
      "Gradient: [[  20.6844  -13.9772  -18.3049   17.5067 -236.6057]]\n",
      "Weights: [[-4.2589 -0.9316 -0.2411  0.1088  0.0863]]\n",
      "MSE loss: 158.4522\n",
      "Iteration: 376300\n",
      "Gradient: [[ -3.009   -5.3412  49.8987  52.9172 142.5314]]\n",
      "Weights: [[-4.2595 -0.9315 -0.2412  0.1088  0.0863]]\n",
      "MSE loss: 158.4365\n",
      "Iteration: 376400\n",
      "Gradient: [[ -9.0025  23.9366 -39.0785 -46.2011 192.6457]]\n",
      "Weights: [[-4.2607 -0.9313 -0.2412  0.1088  0.0863]]\n",
      "MSE loss: 158.4226\n",
      "Iteration: 376500\n",
      "Gradient: [[   5.6626    5.6147   17.8274    7.0089 -213.1026]]\n",
      "Weights: [[-4.2606 -0.9314 -0.2413  0.1088  0.0863]]\n",
      "MSE loss: 158.4168\n",
      "Iteration: 376600\n",
      "Gradient: [[  8.1155   4.7692 -77.8547 111.2141 -65.2605]]\n",
      "Weights: [[-4.2611 -0.9315 -0.2413  0.1088  0.0863]]\n",
      "MSE loss: 158.413\n",
      "Iteration: 376700\n",
      "Gradient: [[ -10.7961  -20.6721  -27.5408  -63.6963 -394.3304]]\n",
      "Weights: [[-4.2608 -0.9313 -0.2414  0.1088  0.0864]]\n",
      "MSE loss: 158.4017\n",
      "Iteration: 376800\n",
      "Gradient: [[ -5.9014  29.947   32.5233  59.5436 171.6497]]\n",
      "Weights: [[-4.2599 -0.9312 -0.2414  0.1088  0.0864]]\n",
      "MSE loss: 158.3938\n",
      "Iteration: 376900\n",
      "Gradient: [[ 10.3984  24.459   -4.0186  31.1339 279.5579]]\n",
      "Weights: [[-4.2605 -0.9311 -0.2415  0.1087  0.0864]]\n",
      "MSE loss: 158.3803\n",
      "Iteration: 377000\n",
      "Gradient: [[ 13.5974 -40.8983  -3.3247 160.9009 231.7021]]\n",
      "Weights: [[-4.2605 -0.931  -0.2415  0.1087  0.0864]]\n",
      "MSE loss: 158.3685\n",
      "Iteration: 377100\n",
      "Gradient: [[  -6.3324  -11.6041   -2.8457   97.5707 -156.6802]]\n",
      "Weights: [[-4.2606 -0.9309 -0.2416  0.1087  0.0864]]\n",
      "MSE loss: 158.3563\n",
      "Iteration: 377200\n",
      "Gradient: [[  -6.3527   -7.2706  -16.2485   97.5289 -170.4225]]\n",
      "Weights: [[-4.2599 -0.9309 -0.2417  0.1087  0.0864]]\n",
      "MSE loss: 158.3449\n",
      "Iteration: 377300\n",
      "Gradient: [[ 8.684100e+00  6.058900e+00 -2.100000e-03  1.266604e+02  2.812984e+02]]\n",
      "Weights: [[-4.2605 -0.9307 -0.2418  0.1087  0.0864]]\n",
      "MSE loss: 158.3309\n",
      "Iteration: 377400\n",
      "Gradient: [[-12.0096   7.4951  23.5226  95.935   24.5526]]\n",
      "Weights: [[-4.2621 -0.9304 -0.2418  0.1087  0.0864]]\n",
      "MSE loss: 158.3203\n",
      "Iteration: 377500\n",
      "Gradient: [[-10.2192 -14.8944  -9.7061  60.9211   6.1502]]\n",
      "Weights: [[-4.2623 -0.9305 -0.2419  0.1087  0.0864]]\n",
      "MSE loss: 158.3137\n",
      "Iteration: 377600\n",
      "Gradient: [[   9.8721    2.2696   17.2879   -3.2321 -215.0238]]\n",
      "Weights: [[-4.2619 -0.9304 -0.2419  0.1087  0.0864]]\n",
      "MSE loss: 158.3019\n",
      "Iteration: 377700\n",
      "Gradient: [[ -9.247   -0.3517  31.3257  32.7316 -65.1059]]\n",
      "Weights: [[-4.2616 -0.9299 -0.2419  0.1087  0.0864]]\n",
      "MSE loss: 158.2839\n",
      "Iteration: 377800\n",
      "Gradient: [[   1.6488   19.8814   42.5843   35.1164 -263.5012]]\n",
      "Weights: [[-4.2615 -0.9297 -0.242   0.1087  0.0864]]\n",
      "MSE loss: 158.2718\n",
      "Iteration: 377900\n",
      "Gradient: [[   3.3697  -28.7305  -12.4771  -91.0299 -432.0828]]\n",
      "Weights: [[-4.2618 -0.9296 -0.2421  0.1087  0.0864]]\n",
      "MSE loss: 158.2592\n",
      "Iteration: 378000\n",
      "Gradient: [[  9.3786  -5.8213   1.135  -21.5648 250.8128]]\n",
      "Weights: [[-4.262  -0.9295 -0.2422  0.1087  0.0865]]\n",
      "MSE loss: 158.2492\n",
      "Iteration: 378100\n",
      "Gradient: [[   8.5499   -6.2886    2.9563   42.1732 -357.7951]]\n",
      "Weights: [[-4.2615 -0.9294 -0.2423  0.1086  0.0865]]\n",
      "MSE loss: 158.2346\n",
      "Iteration: 378200\n",
      "Gradient: [[  -7.2305   10.9975   19.234   -72.6907 -137.1777]]\n",
      "Weights: [[-4.2613 -0.9292 -0.2423  0.1086  0.0865]]\n",
      "MSE loss: 158.2238\n",
      "Iteration: 378300\n",
      "Gradient: [[   3.584    -9.6365  -21.9671  -46.1441 -107.0615]]\n",
      "Weights: [[-4.2604 -0.9292 -0.2424  0.1086  0.0865]]\n",
      "MSE loss: 158.2102\n",
      "Iteration: 378400\n",
      "Gradient: [[  2.706    1.8469  -2.7265  11.9353 265.6066]]\n",
      "Weights: [[-4.261  -0.9292 -0.2425  0.1086  0.0865]]\n",
      "MSE loss: 158.1972\n",
      "Iteration: 378500\n",
      "Gradient: [[  -8.3295  -14.7771   13.945   -31.1684 -228.5733]]\n",
      "Weights: [[-4.2612 -0.9289 -0.2426  0.1086  0.0865]]\n",
      "MSE loss: 158.1852\n",
      "Iteration: 378600\n",
      "Gradient: [[  -5.5987  -24.1575   -6.7705  -60.6964 -336.0958]]\n",
      "Weights: [[-4.2615 -0.9289 -0.2427  0.1086  0.0865]]\n",
      "MSE loss: 158.1745\n",
      "Iteration: 378700\n",
      "Gradient: [[  -5.9089  -11.356    36.5483 -112.6754 -217.092 ]]\n",
      "Weights: [[-4.261  -0.9289 -0.2427  0.1086  0.0865]]\n",
      "MSE loss: 158.1629\n",
      "Iteration: 378800\n",
      "Gradient: [[ -5.468    6.5593  26.1311  72.5816 205.3534]]\n",
      "Weights: [[-4.26   -0.9288 -0.2429  0.1086  0.0865]]\n",
      "MSE loss: 158.1531\n",
      "Iteration: 378900\n",
      "Gradient: [[ -1.6646   5.3661 -29.5576  61.729  133.5717]]\n",
      "Weights: [[-4.2595 -0.9288 -0.2429  0.1086  0.0865]]\n",
      "MSE loss: 158.145\n",
      "Iteration: 379000\n",
      "Gradient: [[ -0.975    6.9171  49.5784  49.2327 121.1983]]\n",
      "Weights: [[-4.2599 -0.9286 -0.243   0.1086  0.0865]]\n",
      "MSE loss: 158.1312\n",
      "Iteration: 379100\n",
      "Gradient: [[ 6.16200e-01 -2.94940e+00  2.38000e-02  2.60387e+01 -3.69485e+01]]\n",
      "Weights: [[-4.261  -0.9283 -0.2431  0.1086  0.0865]]\n",
      "MSE loss: 158.1114\n",
      "Iteration: 379200\n",
      "Gradient: [[-7.4081  3.4046  7.9967 11.2091 29.4247]]\n",
      "Weights: [[-4.2613 -0.9282 -0.2432  0.1086  0.0865]]\n",
      "MSE loss: 158.1003\n",
      "Iteration: 379300\n",
      "Gradient: [[   4.2451    3.5272  -47.2347   39.2878 -119.0893]]\n",
      "Weights: [[-4.2608 -0.9279 -0.2432  0.1086  0.0865]]\n",
      "MSE loss: 158.0874\n",
      "Iteration: 379400\n",
      "Gradient: [[  9.0194   1.353    2.9019  15.0887 -99.0903]]\n",
      "Weights: [[-4.2605 -0.9277 -0.2433  0.1086  0.0865]]\n",
      "MSE loss: 158.0765\n",
      "Iteration: 379500\n",
      "Gradient: [[ -17.8276   -1.0325   36.5277   44.0736 -288.7312]]\n",
      "Weights: [[-4.2604 -0.9278 -0.2434  0.1085  0.0865]]\n",
      "MSE loss: 158.0676\n",
      "Iteration: 379600\n",
      "Gradient: [[  -5.0454  -14.7659   25.1442   79.6647 -410.6417]]\n",
      "Weights: [[-4.261  -0.9276 -0.2434  0.1085  0.0866]]\n",
      "MSE loss: 158.0547\n",
      "Iteration: 379700\n",
      "Gradient: [[  -1.1945   -2.8725   22.1767   -7.7125 -278.1286]]\n",
      "Weights: [[-4.2606 -0.9275 -0.2435  0.1085  0.0866]]\n",
      "MSE loss: 158.045\n",
      "Iteration: 379800\n",
      "Gradient: [[  13.5333  -10.964   -12.8474 -100.0837   76.5207]]\n",
      "Weights: [[-4.261  -0.9274 -0.2436  0.1085  0.0866]]\n",
      "MSE loss: 158.0344\n",
      "Iteration: 379900\n",
      "Gradient: [[  3.9594   0.6149  12.8176 157.8529  71.0538]]\n",
      "Weights: [[-4.2607 -0.9273 -0.2436  0.1085  0.0866]]\n",
      "MSE loss: 158.0251\n",
      "Iteration: 380000\n",
      "Gradient: [[-5.086000e-01 -2.716300e+00  1.625330e+01 -2.997670e+01 -5.799287e+02]]\n",
      "Weights: [[-4.2608 -0.9272 -0.2437  0.1085  0.0866]]\n",
      "MSE loss: 158.0096\n",
      "Iteration: 380100\n",
      "Gradient: [[   2.1952    0.1832   -2.1486 -124.3174   38.8196]]\n",
      "Weights: [[-4.2611 -0.927  -0.2438  0.1085  0.0866]]\n",
      "MSE loss: 157.9946\n",
      "Iteration: 380200\n",
      "Gradient: [[  -2.0702   -1.5551  -15.7184  -43.8537 -151.963 ]]\n",
      "Weights: [[-4.2615 -0.9267 -0.2439  0.1085  0.0866]]\n",
      "MSE loss: 157.9831\n",
      "Iteration: 380300\n",
      "Gradient: [[   5.9076  -15.1096   42.3648   27.914  -103.4159]]\n",
      "Weights: [[-4.2626 -0.9265 -0.244   0.1085  0.0866]]\n",
      "MSE loss: 157.9673\n",
      "Iteration: 380400\n",
      "Gradient: [[ -2.4578   2.429   34.8508   3.022  147.4301]]\n",
      "Weights: [[-4.2626 -0.9263 -0.2441  0.1085  0.0866]]\n",
      "MSE loss: 157.949\n",
      "Iteration: 380500\n",
      "Gradient: [[  1.7941   1.988   26.9896   7.7115 164.6734]]\n",
      "Weights: [[-4.2619 -0.9261 -0.2442  0.1085  0.0866]]\n",
      "MSE loss: 157.933\n",
      "Iteration: 380600\n",
      "Gradient: [[   6.8918   14.0365   -1.8218  -28.5052 -114.8328]]\n",
      "Weights: [[-4.2611 -0.926  -0.2443  0.1085  0.0866]]\n",
      "MSE loss: 157.9236\n",
      "Iteration: 380700\n",
      "Gradient: [[-11.7254  14.0017  -1.8375  -3.5599 243.1447]]\n",
      "Weights: [[-4.261  -0.926  -0.2444  0.1085  0.0866]]\n",
      "MSE loss: 157.9133\n",
      "Iteration: 380800\n",
      "Gradient: [[   1.4399   -4.1108   36.6701   63.596  -208.3416]]\n",
      "Weights: [[-4.2615 -0.9259 -0.2445  0.1085  0.0866]]\n",
      "MSE loss: 157.8977\n",
      "Iteration: 380900\n",
      "Gradient: [[  4.2502   0.4874   2.5672   6.0453 221.7699]]\n",
      "Weights: [[-4.2624 -0.9257 -0.2446  0.1085  0.0866]]\n",
      "MSE loss: 157.8814\n",
      "Iteration: 381000\n",
      "Gradient: [[-15.1722  -7.6269  -8.1243 -55.3763  97.3   ]]\n",
      "Weights: [[-4.2623 -0.9253 -0.2447  0.1085  0.0866]]\n",
      "MSE loss: 157.866\n",
      "Iteration: 381100\n",
      "Gradient: [[ -4.2323  -3.8859   9.227   24.9798 -67.627 ]]\n",
      "Weights: [[-4.2612 -0.9249 -0.2448  0.1085  0.0866]]\n",
      "MSE loss: 157.8568\n",
      "Iteration: 381200\n",
      "Gradient: [[  -3.3562   -2.6012   -6.5636   76.6038 -166.1374]]\n",
      "Weights: [[-4.2617 -0.9251 -0.2449  0.1084  0.0866]]\n",
      "MSE loss: 157.849\n",
      "Iteration: 381300\n",
      "Gradient: [[   5.7604   -8.7136  -11.4285   68.8682 -204.12  ]]\n",
      "Weights: [[-4.2617 -0.9251 -0.2449  0.1084  0.0867]]\n",
      "MSE loss: 157.8403\n",
      "Iteration: 381400\n",
      "Gradient: [[  1.2099  -1.1893  18.709  -24.7653 108.7099]]\n",
      "Weights: [[-4.2619 -0.9249 -0.245   0.1084  0.0867]]\n",
      "MSE loss: 157.8268\n",
      "Iteration: 381500\n",
      "Gradient: [[  -4.6204   -7.7363   20.2884  -46.1873 -349.6611]]\n",
      "Weights: [[-4.2618 -0.9248 -0.245   0.1084  0.0867]]\n",
      "MSE loss: 157.8174\n",
      "Iteration: 381600\n",
      "Gradient: [[ 1.7162  4.0415 26.0135 10.3434 57.9463]]\n",
      "Weights: [[-4.2622 -0.9245 -0.2451  0.1084  0.0867]]\n",
      "MSE loss: 157.8027\n",
      "Iteration: 381700\n",
      "Gradient: [[  10.2088  -19.8691  -24.0012  103.6806 -736.2049]]\n",
      "Weights: [[-4.2628 -0.9244 -0.2451  0.1084  0.0867]]\n",
      "MSE loss: 157.7974\n",
      "Iteration: 381800\n",
      "Gradient: [[  -2.5085    8.9821  -27.2131  -58.6903 -380.6126]]\n",
      "Weights: [[-4.2624 -0.9245 -0.2452  0.1084  0.0867]]\n",
      "MSE loss: 157.7874\n",
      "Iteration: 381900\n",
      "Gradient: [[ -3.4634  -4.4368  35.2066   6.7894 -20.0186]]\n",
      "Weights: [[-4.263  -0.9243 -0.2453  0.1084  0.0867]]\n",
      "MSE loss: 157.7794\n",
      "Iteration: 382000\n",
      "Gradient: [[ -7.5152  -8.6622  67.8416  30.7886 -82.8056]]\n",
      "Weights: [[-4.2622 -0.9243 -0.2454  0.1084  0.0867]]\n",
      "MSE loss: 157.7646\n",
      "Iteration: 382100\n",
      "Gradient: [[  2.7452  -1.8072   9.2376  67.5212 105.5234]]\n",
      "Weights: [[-4.2615 -0.9239 -0.2455  0.1084  0.0867]]\n",
      "MSE loss: 157.748\n",
      "Iteration: 382200\n",
      "Gradient: [[-11.1329 -28.3281  16.6658 -75.1963 101.9301]]\n",
      "Weights: [[-4.2619 -0.9239 -0.2456  0.1084  0.0867]]\n",
      "MSE loss: 157.7373\n",
      "Iteration: 382300\n",
      "Gradient: [[  2.6364 -12.5071 -47.3715 -55.7138 193.0942]]\n",
      "Weights: [[-4.2624 -0.9237 -0.2456  0.1084  0.0867]]\n",
      "MSE loss: 157.7246\n",
      "Iteration: 382400\n",
      "Gradient: [[ -3.7343  13.2786  57.1145 128.006  165.9955]]\n",
      "Weights: [[-4.2614 -0.9233 -0.2457  0.1084  0.0867]]\n",
      "MSE loss: 157.7153\n",
      "Iteration: 382500\n",
      "Gradient: [[  -8.6374  -16.2799  -10.4607  -62.9745 -154.1788]]\n",
      "Weights: [[-4.2621 -0.9232 -0.2458  0.1084  0.0867]]\n",
      "MSE loss: 157.6988\n",
      "Iteration: 382600\n",
      "Gradient: [[   2.7008   -6.7659   24.0481   49.2234 -122.6988]]\n",
      "Weights: [[-4.2628 -0.923  -0.2459  0.1084  0.0867]]\n",
      "MSE loss: 157.6862\n",
      "Iteration: 382700\n",
      "Gradient: [[ -1.233  -14.9478  24.5654 -32.5643  -6.1604]]\n",
      "Weights: [[-4.263  -0.9229 -0.2459  0.1084  0.0867]]\n",
      "MSE loss: 157.6769\n",
      "Iteration: 382800\n",
      "Gradient: [[  7.5016   3.9427 -34.5299   7.6332  55.1703]]\n",
      "Weights: [[-4.2634 -0.9228 -0.2459  0.1084  0.0867]]\n",
      "MSE loss: 157.6682\n",
      "Iteration: 382900\n",
      "Gradient: [[  -7.4423  -12.4367   19.4212  128.4944 -294.6141]]\n",
      "Weights: [[-4.2638 -0.9226 -0.246   0.1084  0.0868]]\n",
      "MSE loss: 157.6535\n",
      "Iteration: 383000\n",
      "Gradient: [[ -5.1591 -12.5134 -24.7335 -56.3856 132.5686]]\n",
      "Weights: [[-4.2636 -0.9223 -0.2461  0.1084  0.0868]]\n",
      "MSE loss: 157.6416\n",
      "Iteration: 383100\n",
      "Gradient: [[   6.3958    3.673   -57.5277 -102.7083   16.8209]]\n",
      "Weights: [[-4.264  -0.922  -0.2462  0.1084  0.0868]]\n",
      "MSE loss: 157.6268\n",
      "Iteration: 383200\n",
      "Gradient: [[  2.4154   8.6466  -8.4466  42.0518 188.841 ]]\n",
      "Weights: [[-4.2632 -0.9218 -0.2463  0.1083  0.0868]]\n",
      "MSE loss: 157.6141\n",
      "Iteration: 383300\n",
      "Gradient: [[   6.701     9.593    41.8864  122.0829 -143.1984]]\n",
      "Weights: [[-4.2634 -0.9216 -0.2463  0.1083  0.0868]]\n",
      "MSE loss: 157.6032\n",
      "Iteration: 383400\n",
      "Gradient: [[-10.4158   7.3401  11.4331 -57.2763 -52.2723]]\n",
      "Weights: [[-4.2628 -0.9215 -0.2464  0.1083  0.0868]]\n",
      "MSE loss: 157.5953\n",
      "Iteration: 383500\n",
      "Gradient: [[ -2.4899 -20.5347 -12.905   22.8179 147.2446]]\n",
      "Weights: [[-4.263  -0.9216 -0.2464  0.1083  0.0868]]\n",
      "MSE loss: 157.5861\n",
      "Iteration: 383600\n",
      "Gradient: [[ -3.336   -5.3107  53.9883 -42.2871 225.9834]]\n",
      "Weights: [[-4.2627 -0.9214 -0.2465  0.1083  0.0868]]\n",
      "MSE loss: 157.5786\n",
      "Iteration: 383700\n",
      "Gradient: [[  -3.6968  -15.5209   55.0791 -100.7959   30.5743]]\n",
      "Weights: [[-4.2633 -0.9213 -0.2466  0.1083  0.0868]]\n",
      "MSE loss: 157.5583\n",
      "Iteration: 383800\n",
      "Gradient: [[ -8.3789  12.2362  38.7058  33.7052 -64.8064]]\n",
      "Weights: [[-4.2631 -0.9212 -0.2467  0.1083  0.0868]]\n",
      "MSE loss: 157.545\n",
      "Iteration: 383900\n",
      "Gradient: [[   4.5275   -8.2579   34.6501  -38.3763 -276.0724]]\n",
      "Weights: [[-4.2637 -0.921  -0.2468  0.1083  0.0868]]\n",
      "MSE loss: 157.5336\n",
      "Iteration: 384000\n",
      "Gradient: [[ -7.3764 -11.6786  -8.0448  64.3848 -20.3233]]\n",
      "Weights: [[-4.2644 -0.9211 -0.2469  0.1083  0.0868]]\n",
      "MSE loss: 157.5252\n",
      "Iteration: 384100\n",
      "Gradient: [[   4.2151    6.8266   43.6537   62.4675 -473.3877]]\n",
      "Weights: [[-4.2638 -0.9209 -0.2469  0.1083  0.0868]]\n",
      "MSE loss: 157.511\n",
      "Iteration: 384200\n",
      "Gradient: [[   0.8682    5.122    -1.9893   -4.5122 -270.1218]]\n",
      "Weights: [[-4.2647 -0.9207 -0.247   0.1083  0.0868]]\n",
      "MSE loss: 157.5005\n",
      "Iteration: 384300\n",
      "Gradient: [[   7.5153   -6.2024    8.5075    5.0901 -129.9223]]\n",
      "Weights: [[-4.2648 -0.9205 -0.247   0.1083  0.0868]]\n",
      "MSE loss: 157.4919\n",
      "Iteration: 384400\n",
      "Gradient: [[  -3.5864  -22.2115  -21.5925 -116.3051  -65.5696]]\n",
      "Weights: [[-4.2647 -0.9203 -0.2471  0.1083  0.0868]]\n",
      "MSE loss: 157.4785\n",
      "Iteration: 384500\n",
      "Gradient: [[ -4.2595  -0.3303  27.287   19.8459 114.437 ]]\n",
      "Weights: [[-4.2643 -0.9202 -0.2472  0.1083  0.0869]]\n",
      "MSE loss: 157.4696\n",
      "Iteration: 384600\n",
      "Gradient: [[ 11.3834 -23.7686  17.2993  26.0446 -93.2676]]\n",
      "Weights: [[-4.2641 -0.9202 -0.2472  0.1083  0.0869]]\n",
      "MSE loss: 157.4621\n",
      "Iteration: 384700\n",
      "Gradient: [[   3.9139   25.0527    0.793    72.9704 -204.1344]]\n",
      "Weights: [[-4.2637 -0.9201 -0.2473  0.1083  0.0869]]\n",
      "MSE loss: 157.4539\n",
      "Iteration: 384800\n",
      "Gradient: [[ -3.1201   3.8147  23.1293 -41.8711 -51.9627]]\n",
      "Weights: [[-4.2636 -0.9201 -0.2474  0.1083  0.0869]]\n",
      "MSE loss: 157.4414\n",
      "Iteration: 384900\n",
      "Gradient: [[ -2.1342  -3.3358  31.1199 -38.2787  65.8256]]\n",
      "Weights: [[-4.2633 -0.9201 -0.2475  0.1083  0.0869]]\n",
      "MSE loss: 157.4274\n",
      "Iteration: 385000\n",
      "Gradient: [[ -2.7458 -11.3787  -7.8017   0.7934  98.992 ]]\n",
      "Weights: [[-4.2629 -0.9201 -0.2475  0.1083  0.0869]]\n",
      "MSE loss: 157.4223\n",
      "Iteration: 385100\n",
      "Gradient: [[   1.3249   -6.7968  -17.7615   40.8535 -172.1465]]\n",
      "Weights: [[-4.2636 -0.9199 -0.2476  0.1082  0.0869]]\n",
      "MSE loss: 157.4067\n",
      "Iteration: 385200\n",
      "Gradient: [[   5.4312  -12.7959  -57.7175   92.4885 -174.3751]]\n",
      "Weights: [[-4.2622 -0.9198 -0.2477  0.1082  0.0869]]\n",
      "MSE loss: 157.3997\n",
      "Iteration: 385300\n",
      "Gradient: [[ 0.398  -0.2128 32.1781 -6.7133 -6.5533]]\n",
      "Weights: [[-4.2628 -0.9197 -0.2478  0.1082  0.0869]]\n",
      "MSE loss: 157.3857\n",
      "Iteration: 385400\n",
      "Gradient: [[ -7.6102 -26.327   53.0485   3.1226 -43.7515]]\n",
      "Weights: [[-4.263  -0.9195 -0.2479  0.1082  0.0869]]\n",
      "MSE loss: 157.3712\n",
      "Iteration: 385500\n",
      "Gradient: [[  11.5403   13.4836    5.641   -15.1395 -327.4834]]\n",
      "Weights: [[-4.2636 -0.9194 -0.248   0.1082  0.0869]]\n",
      "MSE loss: 157.356\n",
      "Iteration: 385600\n",
      "Gradient: [[   8.1628  -30.2727  -12.9177   95.4466 -266.6745]]\n",
      "Weights: [[-4.2641 -0.9191 -0.248   0.1082  0.0869]]\n",
      "MSE loss: 157.3456\n",
      "Iteration: 385700\n",
      "Gradient: [[  1.2749  21.178   25.9644 -32.92   263.7905]]\n",
      "Weights: [[-4.2631 -0.919  -0.2481  0.1082  0.0869]]\n",
      "MSE loss: 157.3368\n",
      "Iteration: 385800\n",
      "Gradient: [[ -14.6559   -5.7327   -6.8997   33.4882 -166.4772]]\n",
      "Weights: [[-4.2633 -0.919  -0.2483  0.1082  0.0869]]\n",
      "MSE loss: 157.3211\n",
      "Iteration: 385900\n",
      "Gradient: [[   5.1535    1.3654   13.7545  138.7115 -101.7531]]\n",
      "Weights: [[-4.2629 -0.9188 -0.2484  0.1082  0.0869]]\n",
      "MSE loss: 157.3092\n",
      "Iteration: 386000\n",
      "Gradient: [[  8.6523  -1.5418 -16.6504 -13.673    8.957 ]]\n",
      "Weights: [[-4.2634 -0.9186 -0.2484  0.1082  0.0869]]\n",
      "MSE loss: 157.2955\n",
      "Iteration: 386100\n",
      "Gradient: [[-12.6936   1.466  -22.6545 -81.6708 -17.4476]]\n",
      "Weights: [[-4.2643 -0.9184 -0.2485  0.1082  0.0869]]\n",
      "MSE loss: 157.2792\n",
      "Iteration: 386200\n",
      "Gradient: [[  8.7549  -2.6136  59.4321 -32.9401 124.1719]]\n",
      "Weights: [[-4.2644 -0.9181 -0.2486  0.1082  0.087 ]]\n",
      "MSE loss: 157.2621\n",
      "Iteration: 386300\n",
      "Gradient: [[  -7.8972    1.1293   -0.4015   -3.1621 -167.8959]]\n",
      "Weights: [[-4.2637 -0.9178 -0.2487  0.1082  0.087 ]]\n",
      "MSE loss: 157.2507\n",
      "Iteration: 386400\n",
      "Gradient: [[ 3.1373  8.9568 -3.4476 35.1376 39.225 ]]\n",
      "Weights: [[-4.2644 -0.9177 -0.2487  0.1082  0.087 ]]\n",
      "MSE loss: 157.241\n",
      "Iteration: 386500\n",
      "Gradient: [[ -5.8814  20.7639 -23.7295 -66.6303 234.489 ]]\n",
      "Weights: [[-4.2649 -0.9174 -0.2488  0.1082  0.087 ]]\n",
      "MSE loss: 157.2307\n",
      "Iteration: 386600\n",
      "Gradient: [[-11.448   -6.5423  20.9022 -70.1649  44.8324]]\n",
      "Weights: [[-4.2646 -0.9172 -0.2488  0.1082  0.087 ]]\n",
      "MSE loss: 157.2208\n",
      "Iteration: 386700\n",
      "Gradient: [[ -11.3481   -6.3517  -43.1464   13.4151 -324.9515]]\n",
      "Weights: [[-4.2633 -0.9172 -0.2489  0.1081  0.087 ]]\n",
      "MSE loss: 157.2161\n",
      "Iteration: 386800\n",
      "Gradient: [[   8.2762   -3.7856    8.9376  -69.0186 -167.0922]]\n",
      "Weights: [[-4.2654 -0.9171 -0.249   0.1081  0.087 ]]\n",
      "MSE loss: 157.2027\n",
      "Iteration: 386900\n",
      "Gradient: [[ -2.0745   4.8318  12.6432 -95.8639 124.8939]]\n",
      "Weights: [[-4.2661 -0.9169 -0.249   0.1081  0.087 ]]\n",
      "MSE loss: 157.1904\n",
      "Iteration: 387000\n",
      "Gradient: [[  5.386   12.6784  -2.4371 -24.0717 138.3886]]\n",
      "Weights: [[-4.2662 -0.9167 -0.2491  0.1081  0.087 ]]\n",
      "MSE loss: 157.1798\n",
      "Iteration: 387100\n",
      "Gradient: [[   2.4432  -27.2422    2.1281  129.1991 -113.7915]]\n",
      "Weights: [[-4.2656 -0.9166 -0.2492  0.1081  0.087 ]]\n",
      "MSE loss: 157.1704\n",
      "Iteration: 387200\n",
      "Gradient: [[  -9.6731  -10.4594  -37.9409   63.3465 -132.4029]]\n",
      "Weights: [[-4.2656 -0.9163 -0.2492  0.1081  0.087 ]]\n",
      "MSE loss: 157.1561\n",
      "Iteration: 387300\n",
      "Gradient: [[  -7.4727    2.1033   31.6317   64.9058 -365.3793]]\n",
      "Weights: [[-4.2658 -0.9162 -0.2493  0.1081  0.087 ]]\n",
      "MSE loss: 157.1455\n",
      "Iteration: 387400\n",
      "Gradient: [[ 12.1635 -10.6878  24.1036 -53.7823 -41.2143]]\n",
      "Weights: [[-4.2661 -0.9161 -0.2493  0.1081  0.087 ]]\n",
      "MSE loss: 157.1353\n",
      "Iteration: 387500\n",
      "Gradient: [[-6.5117 11.0439 42.9629 -5.5785 16.1034]]\n",
      "Weights: [[-4.2662 -0.9159 -0.2494  0.1081  0.087 ]]\n",
      "MSE loss: 157.1241\n",
      "Iteration: 387600\n",
      "Gradient: [[ -13.4413   -4.9682   20.1706  -15.0905 -161.998 ]]\n",
      "Weights: [[-4.2665 -0.9159 -0.2494  0.1081  0.087 ]]\n",
      "MSE loss: 157.1179\n",
      "Iteration: 387700\n",
      "Gradient: [[ 2.65560e+00 -2.64000e-02  6.25230e+00  3.37940e+00 -4.80743e+01]]\n",
      "Weights: [[-4.2664 -0.9157 -0.2495  0.1081  0.087 ]]\n",
      "MSE loss: 157.1031\n",
      "Iteration: 387800\n",
      "Gradient: [[ -2.1165  -7.7811  33.632  160.8666  97.1263]]\n",
      "Weights: [[-4.2657 -0.9153 -0.2496  0.1081  0.087 ]]\n",
      "MSE loss: 157.0881\n",
      "Iteration: 387900\n",
      "Gradient: [[   8.5285   -8.6997  -24.7696   52.8445 -194.7534]]\n",
      "Weights: [[-4.2654 -0.9154 -0.2496  0.1081  0.0871]]\n",
      "MSE loss: 157.0788\n",
      "Iteration: 388000\n",
      "Gradient: [[  -6.9123   -2.0012   96.2481    7.9225 -157.7769]]\n",
      "Weights: [[-4.2659 -0.9153 -0.2497  0.108   0.0871]]\n",
      "MSE loss: 157.0656\n",
      "Iteration: 388100\n",
      "Gradient: [[   3.5311   -1.5531  -18.5178   30.8866 -159.4915]]\n",
      "Weights: [[-4.2668 -0.915  -0.2498  0.108   0.0871]]\n",
      "MSE loss: 157.0482\n",
      "Iteration: 388200\n",
      "Gradient: [[ -8.2584   9.6584 -11.6946 -98.516   70.2856]]\n",
      "Weights: [[-4.2671 -0.9148 -0.2499  0.108   0.0871]]\n",
      "MSE loss: 157.0368\n",
      "Iteration: 388300\n",
      "Gradient: [[ -5.5304  -4.6545 -54.0679  17.3787 -39.4509]]\n",
      "Weights: [[-4.2675 -0.9144 -0.2499  0.108   0.0871]]\n",
      "MSE loss: 157.0244\n",
      "Iteration: 388400\n",
      "Gradient: [[ 9.360000e-02  1.738580e+01  3.855740e+01 -7.095130e+01  2.923659e+02]]\n",
      "Weights: [[-4.2677 -0.9141 -0.25    0.108   0.0871]]\n",
      "MSE loss: 157.0143\n",
      "Iteration: 388500\n",
      "Gradient: [[  -5.3162   -0.3843  -21.4216  -11.3908 -129.9011]]\n",
      "Weights: [[-4.268  -0.914  -0.2501  0.108   0.0871]]\n",
      "MSE loss: 156.9967\n",
      "Iteration: 388600\n",
      "Gradient: [[   8.2073   -7.4291   32.0982 -114.9943   80.7904]]\n",
      "Weights: [[-4.2678 -0.9139 -0.2502  0.108   0.0871]]\n",
      "MSE loss: 156.9835\n",
      "Iteration: 388700\n",
      "Gradient: [[  -8.7586   -5.5634   -5.0475   71.2785 -193.128 ]]\n",
      "Weights: [[-4.2677 -0.9135 -0.2503  0.108   0.0871]]\n",
      "MSE loss: 156.9637\n",
      "Iteration: 388800\n",
      "Gradient: [[   7.6758  -12.2355   45.8535  133.4901 -179.9336]]\n",
      "Weights: [[-4.2675 -0.9131 -0.2504  0.108   0.0871]]\n",
      "MSE loss: 156.9495\n",
      "Iteration: 388900\n",
      "Gradient: [[  -7.5879   -8.9805   45.8467   -1.3361 -353.8253]]\n",
      "Weights: [[-4.2675 -0.9129 -0.2505  0.108   0.0871]]\n",
      "MSE loss: 156.935\n",
      "Iteration: 389000\n",
      "Gradient: [[  -8.4069    2.9681    3.2931  104.9721 -229.1696]]\n",
      "Weights: [[-4.2667 -0.9128 -0.2506  0.108   0.0871]]\n",
      "MSE loss: 156.9257\n",
      "Iteration: 389100\n",
      "Gradient: [[-13.2405   8.1164 -35.4533 169.9458  90.3601]]\n",
      "Weights: [[-4.2662 -0.9129 -0.2507  0.108   0.0871]]\n",
      "MSE loss: 156.9138\n",
      "Iteration: 389200\n",
      "Gradient: [[ 1.6331  1.9377 -6.5738 58.2212 -9.5319]]\n",
      "Weights: [[-4.2652 -0.9129 -0.2508  0.108   0.0871]]\n",
      "MSE loss: 156.9074\n",
      "Iteration: 389300\n",
      "Gradient: [[  -2.1368   13.5045  -29.0151 -102.2814  -55.0719]]\n",
      "Weights: [[-4.2676 -0.9128 -0.2509  0.108   0.0871]]\n",
      "MSE loss: 156.8876\n",
      "Iteration: 389400\n",
      "Gradient: [[ -7.3265   1.8245  58.2872  10.6028 -27.102 ]]\n",
      "Weights: [[-4.2674 -0.9127 -0.2509  0.108   0.0872]]\n",
      "MSE loss: 156.8775\n",
      "Iteration: 389500\n",
      "Gradient: [[-16.6443 -11.3319  10.3268  44.0895  43.4054]]\n",
      "Weights: [[-4.2676 -0.9124 -0.251   0.108   0.0872]]\n",
      "MSE loss: 156.8661\n",
      "Iteration: 389600\n",
      "Gradient: [[  3.9775   2.3636  -5.0667  20.9882 174.4885]]\n",
      "Weights: [[-4.2673 -0.9122 -0.251   0.108   0.0872]]\n",
      "MSE loss: 156.8568\n",
      "Iteration: 389700\n",
      "Gradient: [[  -3.4053   -9.9471   15.5608   85.6906 -187.81  ]]\n",
      "Weights: [[-4.2671 -0.9118 -0.251   0.1079  0.0872]]\n",
      "MSE loss: 156.8506\n",
      "Iteration: 389800\n",
      "Gradient: [[  9.4785  14.2433 -40.2982  12.9474 110.6334]]\n",
      "Weights: [[-4.268  -0.9117 -0.2511  0.1079  0.0872]]\n",
      "MSE loss: 156.8371\n",
      "Iteration: 389900\n",
      "Gradient: [[-9.3788  5.259  27.6813 36.6412 44.8433]]\n",
      "Weights: [[-4.2695 -0.9117 -0.2512  0.1079  0.0872]]\n",
      "MSE loss: 156.8278\n",
      "Iteration: 390000\n",
      "Gradient: [[  12.6541  -18.4073   11.7605   26.8751 -174.8502]]\n",
      "Weights: [[-4.2696 -0.9114 -0.2512  0.1079  0.0872]]\n",
      "MSE loss: 156.8165\n",
      "Iteration: 390100\n",
      "Gradient: [[ 3.5308  8.2251  7.3312 10.7386 36.6729]]\n",
      "Weights: [[-4.2688 -0.9112 -0.2512  0.1079  0.0872]]\n",
      "MSE loss: 156.8043\n",
      "Iteration: 390200\n",
      "Gradient: [[  11.7506   17.9027   74.5424   45.3011 -237.2343]]\n",
      "Weights: [[-4.268  -0.9114 -0.2513  0.1079  0.0872]]\n",
      "MSE loss: 156.7961\n",
      "Iteration: 390300\n",
      "Gradient: [[  7.9355 -16.2779  -6.0553 139.714   25.6407]]\n",
      "Weights: [[-4.2689 -0.9112 -0.2514  0.1079  0.0872]]\n",
      "MSE loss: 156.7807\n",
      "Iteration: 390400\n",
      "Gradient: [[   1.5741  -25.1352  -34.7533 -102.6622  325.8823]]\n",
      "Weights: [[-4.2688 -0.911  -0.2515  0.1079  0.0872]]\n",
      "MSE loss: 156.7666\n",
      "Iteration: 390500\n",
      "Gradient: [[  -7.1725   10.6487   58.5037  -59.7147 -272.3712]]\n",
      "Weights: [[-4.2704 -0.9109 -0.2516  0.1079  0.0872]]\n",
      "MSE loss: 156.7616\n",
      "Iteration: 390600\n",
      "Gradient: [[ -2.9554 -11.0902 -16.1463 117.5684 384.5373]]\n",
      "Weights: [[-4.2716 -0.9107 -0.2516  0.1079  0.0872]]\n",
      "MSE loss: 156.7603\n",
      "Iteration: 390700\n",
      "Gradient: [[  -9.8007    5.0862  -14.9747 -125.632   -98.5607]]\n",
      "Weights: [[-4.2713 -0.9106 -0.2517  0.1079  0.0872]]\n",
      "MSE loss: 156.7426\n",
      "Iteration: 390800\n",
      "Gradient: [[ -5.3986  11.9441  26.292  141.4881  62.4644]]\n",
      "Weights: [[-4.2712 -0.9104 -0.2517  0.1079  0.0872]]\n",
      "MSE loss: 156.7329\n",
      "Iteration: 390900\n",
      "Gradient: [[   1.4805  -16.1131  -24.7813  -42.7363 -304.9414]]\n",
      "Weights: [[-4.2704 -0.9104 -0.2519  0.1079  0.0873]]\n",
      "MSE loss: 156.716\n",
      "Iteration: 391000\n",
      "Gradient: [[   0.9498   14.633     2.2063   22.3108 -687.1582]]\n",
      "Weights: [[-4.2696 -0.9101 -0.2519  0.1079  0.0873]]\n",
      "MSE loss: 156.704\n",
      "Iteration: 391100\n",
      "Gradient: [[  4.9768   9.7862   8.5464  84.2166 226.6478]]\n",
      "Weights: [[-4.2697 -0.9099 -0.252   0.1079  0.0873]]\n",
      "MSE loss: 156.6901\n",
      "Iteration: 391200\n",
      "Gradient: [[   8.146     6.6376   -5.3562   31.2906 -268.1651]]\n",
      "Weights: [[-4.2702 -0.9101 -0.252   0.1079  0.0873]]\n",
      "MSE loss: 156.6883\n",
      "Iteration: 391300\n",
      "Gradient: [[ 10.6557  -4.8096 -12.482  -47.9312  94.9731]]\n",
      "Weights: [[-4.2703 -0.91   -0.2521  0.1079  0.0873]]\n",
      "MSE loss: 156.6785\n",
      "Iteration: 391400\n",
      "Gradient: [[-3.7043  5.0927 14.2122 -6.5469 30.194 ]]\n",
      "Weights: [[-4.2699 -0.9098 -0.2522  0.1079  0.0873]]\n",
      "MSE loss: 156.662\n",
      "Iteration: 391500\n",
      "Gradient: [[  -3.4065    0.7084    5.5359  -26.7013 -224.072 ]]\n",
      "Weights: [[-4.2694 -0.9097 -0.2522  0.1079  0.0873]]\n",
      "MSE loss: 156.6507\n",
      "Iteration: 391600\n",
      "Gradient: [[  8.6197   6.5147 -22.0942  -6.5242 -24.8513]]\n",
      "Weights: [[-4.2694 -0.9094 -0.2523  0.1079  0.0873]]\n",
      "MSE loss: 156.6358\n",
      "Iteration: 391700\n",
      "Gradient: [[  -1.0633    6.7872   44.4875    8.2338 -154.0201]]\n",
      "Weights: [[-4.2693 -0.9093 -0.2524  0.1078  0.0873]]\n",
      "MSE loss: 156.624\n",
      "Iteration: 391800\n",
      "Gradient: [[  -0.3288   -4.3648   16.755   -11.0059 -151.8201]]\n",
      "Weights: [[-4.2693 -0.9092 -0.2525  0.1078  0.0873]]\n",
      "MSE loss: 156.612\n",
      "Iteration: 391900\n",
      "Gradient: [[ 3.3414 -0.4902 53.4294 81.6992 25.0362]]\n",
      "Weights: [[-4.2681 -0.9091 -0.2526  0.1078  0.0873]]\n",
      "MSE loss: 156.6047\n",
      "Iteration: 392000\n",
      "Gradient: [[-15.7464 -22.4786  51.6101  50.2825 444.3445]]\n",
      "Weights: [[-4.2674 -0.9092 -0.2526  0.1078  0.0873]]\n",
      "MSE loss: 156.5993\n",
      "Iteration: 392100\n",
      "Gradient: [[ 10.9271 -13.7159  27.9772 -45.6066 -68.474 ]]\n",
      "Weights: [[-4.267  -0.9091 -0.2528  0.1078  0.0873]]\n",
      "MSE loss: 156.5852\n",
      "Iteration: 392200\n",
      "Gradient: [[   3.7977   -3.5951   -1.1165  130.1352 -122.7663]]\n",
      "Weights: [[-4.2675 -0.909  -0.2528  0.1078  0.0873]]\n",
      "MSE loss: 156.5721\n",
      "Iteration: 392300\n",
      "Gradient: [[  -9.5891    6.5481  -18.7918   56.8695 -183.036 ]]\n",
      "Weights: [[-4.2682 -0.9088 -0.2529  0.1078  0.0873]]\n",
      "MSE loss: 156.5578\n",
      "Iteration: 392400\n",
      "Gradient: [[  0.4118  27.6708  12.8721 167.7292 -44.3518]]\n",
      "Weights: [[-4.2677 -0.9087 -0.253   0.1078  0.0873]]\n",
      "MSE loss: 156.5435\n",
      "Iteration: 392500\n",
      "Gradient: [[  8.0273  -3.1384  59.1686 -34.9976 237.7619]]\n",
      "Weights: [[-4.2676 -0.9087 -0.2531  0.1078  0.0873]]\n",
      "MSE loss: 156.5322\n",
      "Iteration: 392600\n",
      "Gradient: [[  12.6708   17.7856   -7.0215  -32.6896 -231.9191]]\n",
      "Weights: [[-4.2684 -0.9085 -0.2532  0.1078  0.0873]]\n",
      "MSE loss: 156.521\n",
      "Iteration: 392700\n",
      "Gradient: [[  -2.3867   -8.2456    7.1051  -28.082  -153.1974]]\n",
      "Weights: [[-4.2689 -0.9085 -0.2533  0.1077  0.0874]]\n",
      "MSE loss: 156.5093\n",
      "Iteration: 392800\n",
      "Gradient: [[ -4.1436 -19.179   24.2406 -49.0823 -89.9808]]\n",
      "Weights: [[-4.269  -0.9082 -0.2533  0.1078  0.0874]]\n",
      "MSE loss: 156.4958\n",
      "Iteration: 392900\n",
      "Gradient: [[ -2.0495 -10.7546 -28.384   10.223  218.8442]]\n",
      "Weights: [[-4.2679 -0.9081 -0.2534  0.1077  0.0874]]\n",
      "MSE loss: 156.4847\n",
      "Iteration: 393000\n",
      "Gradient: [[   5.9836   -3.7749   18.0336   86.0744 -236.978 ]]\n",
      "Weights: [[-4.2686 -0.9081 -0.2535  0.1077  0.0874]]\n",
      "MSE loss: 156.4743\n",
      "Iteration: 393100\n",
      "Gradient: [[  6.0532  23.4792  14.0392 -64.1869 -79.1312]]\n",
      "Weights: [[-4.2679 -0.908  -0.2536  0.1077  0.0874]]\n",
      "MSE loss: 156.4658\n",
      "Iteration: 393200\n",
      "Gradient: [[ -0.824  -13.7386  47.2646  42.4523 -39.2783]]\n",
      "Weights: [[-4.2679 -0.9079 -0.2537  0.1077  0.0874]]\n",
      "MSE loss: 156.4494\n",
      "Iteration: 393300\n",
      "Gradient: [[  -1.652    -6.7142    4.3387   29.5995 -125.9566]]\n",
      "Weights: [[-4.2678 -0.9078 -0.2538  0.1078  0.0874]]\n",
      "MSE loss: 156.4388\n",
      "Iteration: 393400\n",
      "Gradient: [[ -4.7292  17.4735  67.8584 113.6249  39.6409]]\n",
      "Weights: [[-4.2677 -0.9078 -0.2539  0.1078  0.0874]]\n",
      "MSE loss: 156.4257\n",
      "Iteration: 393500\n",
      "Gradient: [[  -4.8633   15.4666   24.8386  -56.3823 -156.5851]]\n",
      "Weights: [[-4.2673 -0.9078 -0.254   0.1077  0.0874]]\n",
      "MSE loss: 156.4156\n",
      "Iteration: 393600\n",
      "Gradient: [[ -3.3393 -10.4718 -27.1468 -10.1229 128.5468]]\n",
      "Weights: [[-4.2677 -0.9076 -0.2541  0.1077  0.0874]]\n",
      "MSE loss: 156.4041\n",
      "Iteration: 393700\n",
      "Gradient: [[  -2.9157    0.6822   14.4965   91.6555 -115.6656]]\n",
      "Weights: [[-4.2674 -0.9074 -0.2542  0.1077  0.0874]]\n",
      "MSE loss: 156.3922\n",
      "Iteration: 393800\n",
      "Gradient: [[  -6.5781   -8.5342  -25.975  -126.3708 -135.3764]]\n",
      "Weights: [[-4.2679 -0.9072 -0.2542  0.1077  0.0874]]\n",
      "MSE loss: 156.3796\n",
      "Iteration: 393900\n",
      "Gradient: [[-8.520000e-02 -4.329200e+00  7.843100e+00 -1.363606e+02 -1.685875e+02]]\n",
      "Weights: [[-4.2683 -0.9073 -0.2542  0.1077  0.0874]]\n",
      "MSE loss: 156.3755\n",
      "Iteration: 394000\n",
      "Gradient: [[  12.4422  -15.3003   31.9784   63.7322 -235.3704]]\n",
      "Weights: [[-4.2679 -0.9071 -0.2543  0.1077  0.0874]]\n",
      "MSE loss: 156.365\n",
      "Iteration: 394100\n",
      "Gradient: [[  3.0966   8.1325 -14.007   94.6161 163.4263]]\n",
      "Weights: [[-4.2677 -0.9071 -0.2544  0.1077  0.0874]]\n",
      "MSE loss: 156.3528\n",
      "Iteration: 394200\n",
      "Gradient: [[ -10.3652  -19.7641   -2.0124 -176.1885 -257.6627]]\n",
      "Weights: [[-4.2676 -0.9072 -0.2545  0.1077  0.0874]]\n",
      "MSE loss: 156.345\n",
      "Iteration: 394300\n",
      "Gradient: [[  19.909    12.2469   -1.1023  126.3274 -266.3357]]\n",
      "Weights: [[-4.2679 -0.9071 -0.2546  0.1077  0.0874]]\n",
      "MSE loss: 156.3323\n",
      "Iteration: 394400\n",
      "Gradient: [[ 10.2226 -24.7038 -24.8622 106.336  393.2354]]\n",
      "Weights: [[-4.269  -0.9071 -0.2546  0.1077  0.0875]]\n",
      "MSE loss: 156.326\n",
      "Iteration: 394500\n",
      "Gradient: [[  -2.8444  -10.6421   -2.0745   53.2988 -634.7815]]\n",
      "Weights: [[-4.2698 -0.9068 -0.2547  0.1077  0.0875]]\n",
      "MSE loss: 156.3147\n",
      "Iteration: 394600\n",
      "Gradient: [[ -12.2567   20.8688  -22.1349  -62.1067 -118.2384]]\n",
      "Weights: [[-4.2696 -0.9065 -0.2547  0.1077  0.0875]]\n",
      "MSE loss: 156.2974\n",
      "Iteration: 394700\n",
      "Gradient: [[   3.1257   17.0956   23.5642  -36.6398 -226.5155]]\n",
      "Weights: [[-4.269  -0.9062 -0.2548  0.1077  0.0875]]\n",
      "MSE loss: 156.2804\n",
      "Iteration: 394800\n",
      "Gradient: [[  2.5472 -26.6713  25.6943  24.1188 320.7265]]\n",
      "Weights: [[-4.2698 -0.906  -0.2549  0.1077  0.0875]]\n",
      "MSE loss: 156.2685\n",
      "Iteration: 394900\n",
      "Gradient: [[ -17.9093   -9.5007   21.9187 -124.1412 -153.2656]]\n",
      "Weights: [[-4.2703 -0.906  -0.255   0.1077  0.0875]]\n",
      "MSE loss: 156.2622\n",
      "Iteration: 395000\n",
      "Gradient: [[   2.6448    1.5093  -15.9521   72.2509 -279.9002]]\n",
      "Weights: [[-4.2699 -0.9058 -0.2551  0.1077  0.0875]]\n",
      "MSE loss: 156.2458\n",
      "Iteration: 395100\n",
      "Gradient: [[ 6.7324 13.3239 -8.2024 25.0448 88.8129]]\n",
      "Weights: [[-4.2698 -0.9056 -0.2551  0.1077  0.0875]]\n",
      "MSE loss: 156.2359\n",
      "Iteration: 395200\n",
      "Gradient: [[  -7.2963   -1.1466   11.3812 -111.366    94.3564]]\n",
      "Weights: [[-4.2694 -0.9055 -0.2553  0.1077  0.0875]]\n",
      "MSE loss: 156.2209\n",
      "Iteration: 395300\n",
      "Gradient: [[  -8.9628   -6.8145    6.8434  -98.1408 -215.205 ]]\n",
      "Weights: [[-4.2694 -0.9053 -0.2553  0.1077  0.0875]]\n",
      "MSE loss: 156.2112\n",
      "Iteration: 395400\n",
      "Gradient: [[  -4.4634   14.5603  -44.7482 -143.7181  239.963 ]]\n",
      "Weights: [[-4.2694 -0.9051 -0.2554  0.1077  0.0875]]\n",
      "MSE loss: 156.1996\n",
      "Iteration: 395500\n",
      "Gradient: [[ -6.8792   9.0816 -15.2073 133.1564 -37.8222]]\n",
      "Weights: [[-4.2695 -0.9049 -0.2555  0.1077  0.0875]]\n",
      "MSE loss: 156.1845\n",
      "Iteration: 395600\n",
      "Gradient: [[   6.5619  -17.8411  -24.6889   37.6547 -174.6154]]\n",
      "Weights: [[-4.2701 -0.9046 -0.2556  0.1077  0.0875]]\n",
      "MSE loss: 156.172\n",
      "Iteration: 395700\n",
      "Gradient: [[ -8.0873  27.6975  21.4355 -28.8721 254.1535]]\n",
      "Weights: [[-4.2698 -0.9045 -0.2557  0.1077  0.0875]]\n",
      "MSE loss: 156.158\n",
      "Iteration: 395800\n",
      "Gradient: [[  -0.3469    2.8368    3.221  -100.5299  -12.9827]]\n",
      "Weights: [[-4.2697 -0.9044 -0.2557  0.1077  0.0875]]\n",
      "MSE loss: 156.15\n",
      "Iteration: 395900\n",
      "Gradient: [[  -8.8292  -17.4415   51.4921  -74.1052 -216.255 ]]\n",
      "Weights: [[-4.2697 -0.9043 -0.2558  0.1077  0.0875]]\n",
      "MSE loss: 156.1393\n",
      "Iteration: 396000\n",
      "Gradient: [[  -1.6888   -5.7344   56.6431   42.6966 -360.8068]]\n",
      "Weights: [[-4.2702 -0.9041 -0.2558  0.1077  0.0876]]\n",
      "MSE loss: 156.1306\n",
      "Iteration: 396100\n",
      "Gradient: [[  8.9392  19.8157  -2.0035 -52.2534 293.0184]]\n",
      "Weights: [[-4.2692 -0.9039 -0.2559  0.1077  0.0876]]\n",
      "MSE loss: 156.1229\n",
      "Iteration: 396200\n",
      "Gradient: [[ -6.5873  19.8623  17.0518  75.4142 -84.8881]]\n",
      "Weights: [[-4.2704 -0.9038 -0.256   0.1077  0.0876]]\n",
      "MSE loss: 156.1067\n",
      "Iteration: 396300\n",
      "Gradient: [[-15.532  -10.6818  65.5559  66.5457 -77.4348]]\n",
      "Weights: [[-4.2703 -0.9038 -0.256   0.1077  0.0876]]\n",
      "MSE loss: 156.1027\n",
      "Iteration: 396400\n",
      "Gradient: [[-13.1599  -2.9797  14.7755 -24.143   59.9351]]\n",
      "Weights: [[-4.2703 -0.9038 -0.256   0.1076  0.0876]]\n",
      "MSE loss: 156.0952\n",
      "Iteration: 396500\n",
      "Gradient: [[ 16.3176  -5.8554 -37.3082  23.245   15.2376]]\n",
      "Weights: [[-4.2706 -0.9036 -0.2561  0.1076  0.0876]]\n",
      "MSE loss: 156.0865\n",
      "Iteration: 396600\n",
      "Gradient: [[  7.2708  23.3235  39.8063 -42.5518  71.5526]]\n",
      "Weights: [[-4.27   -0.9033 -0.2561  0.1076  0.0876]]\n",
      "MSE loss: 156.0749\n",
      "Iteration: 396700\n",
      "Gradient: [[  -4.55     22.2396   34.4892   34.1111 -216.6656]]\n",
      "Weights: [[-4.2704 -0.9032 -0.2562  0.1076  0.0876]]\n",
      "MSE loss: 156.0646\n",
      "Iteration: 396800\n",
      "Gradient: [[ -6.471    7.4513   2.4323  61.5125 -97.6609]]\n",
      "Weights: [[-4.2708 -0.903  -0.2563  0.1076  0.0876]]\n",
      "MSE loss: 156.0533\n",
      "Iteration: 396900\n",
      "Gradient: [[  7.2421  -1.5044 -59.8768  24.0613  20.5171]]\n",
      "Weights: [[-4.2711 -0.903  -0.2564  0.1076  0.0876]]\n",
      "MSE loss: 156.0455\n",
      "Iteration: 397000\n",
      "Gradient: [[  -5.2421   -7.602    -5.2671   75.1517 -329.5911]]\n",
      "Weights: [[-4.2716 -0.9029 -0.2564  0.1076  0.0876]]\n",
      "MSE loss: 156.0401\n",
      "Iteration: 397100\n",
      "Gradient: [[ -11.0976  -22.3199  -26.5764  -17.5293 -104.5929]]\n",
      "Weights: [[-4.2709 -0.9026 -0.2565  0.1076  0.0876]]\n",
      "MSE loss: 156.0235\n",
      "Iteration: 397200\n",
      "Gradient: [[  6.6321 -11.3166 -36.4975  99.2054 309.7746]]\n",
      "Weights: [[-4.2701 -0.9024 -0.2565  0.1076  0.0876]]\n",
      "MSE loss: 156.013\n",
      "Iteration: 397300\n",
      "Gradient: [[-10.1273 -19.963   19.6737  70.4749  78.163 ]]\n",
      "Weights: [[-4.2711 -0.9022 -0.2566  0.1076  0.0876]]\n",
      "MSE loss: 155.9975\n",
      "Iteration: 397400\n",
      "Gradient: [[  3.1756  19.1561 -18.2334  36.0647 166.7097]]\n",
      "Weights: [[-4.2697 -0.9021 -0.2567  0.1076  0.0876]]\n",
      "MSE loss: 155.9904\n",
      "Iteration: 397500\n",
      "Gradient: [[  -1.1132   -5.0689   24.1747  -31.8287 -201.4296]]\n",
      "Weights: [[-4.2703 -0.9021 -0.2567  0.1075  0.0876]]\n",
      "MSE loss: 155.9782\n",
      "Iteration: 397600\n",
      "Gradient: [[ -5.9956  -6.8672  36.1742  -3.0031 -64.089 ]]\n",
      "Weights: [[-4.2694 -0.902  -0.2569  0.1075  0.0876]]\n",
      "MSE loss: 155.9674\n",
      "Iteration: 397700\n",
      "Gradient: [[ 10.7547 -13.7429  13.2565   1.5892 251.366 ]]\n",
      "Weights: [[-4.2696 -0.9019 -0.257   0.1075  0.0876]]\n",
      "MSE loss: 155.9511\n",
      "Iteration: 397800\n",
      "Gradient: [[  3.6358  26.4159 -11.6576  60.787   36.4128]]\n",
      "Weights: [[-4.27   -0.9016 -0.2571  0.1075  0.0876]]\n",
      "MSE loss: 155.9374\n",
      "Iteration: 397900\n",
      "Gradient: [[ -5.2276  -2.7644   3.19    55.8211 -91.0313]]\n",
      "Weights: [[-4.2708 -0.9014 -0.2572  0.1075  0.0877]]\n",
      "MSE loss: 155.9229\n",
      "Iteration: 398000\n",
      "Gradient: [[  -1.3293   -0.6403   37.2169 -121.0722 -157.7108]]\n",
      "Weights: [[-4.2708 -0.9014 -0.2573  0.1075  0.0877]]\n",
      "MSE loss: 155.9101\n",
      "Iteration: 398100\n",
      "Gradient: [[  1.9166 -12.0772   2.8016 -60.0438  19.9302]]\n",
      "Weights: [[-4.2712 -0.9012 -0.2574  0.1075  0.0877]]\n",
      "MSE loss: 155.8968\n",
      "Iteration: 398200\n",
      "Gradient: [[   0.6841   -5.3864   17.2129  -11.3295 -301.3436]]\n",
      "Weights: [[-4.2711 -0.901  -0.2574  0.1075  0.0877]]\n",
      "MSE loss: 155.8831\n",
      "Iteration: 398300\n",
      "Gradient: [[15.7328  0.7063 12.3857 12.6969 48.6668]]\n",
      "Weights: [[-4.2713 -0.901  -0.2575  0.1075  0.0877]]\n",
      "MSE loss: 155.8705\n",
      "Iteration: 398400\n",
      "Gradient: [[  -6.2065  -13.5039   49.731   -24.4062 -305.8116]]\n",
      "Weights: [[-4.2719 -0.9006 -0.2576  0.1075  0.0877]]\n",
      "MSE loss: 155.8569\n",
      "Iteration: 398500\n",
      "Gradient: [[ -1.9134  -8.0767  -2.9311  39.4992 -62.4153]]\n",
      "Weights: [[-4.2711 -0.9003 -0.2577  0.1075  0.0877]]\n",
      "MSE loss: 155.8433\n",
      "Iteration: 398600\n",
      "Gradient: [[ 13.8586   9.0835  -8.3506  -2.3282 -62.643 ]]\n",
      "Weights: [[-4.2711 -0.9004 -0.2578  0.1075  0.0877]]\n",
      "MSE loss: 155.8326\n",
      "Iteration: 398700\n",
      "Gradient: [[ -1.7989  27.3827  -2.7824 107.0603 149.1928]]\n",
      "Weights: [[-4.2715 -0.9002 -0.2578  0.1075  0.0877]]\n",
      "MSE loss: 155.8234\n",
      "Iteration: 398800\n",
      "Gradient: [[ -11.078     3.416    76.6044  -96.6464 -185.2932]]\n",
      "Weights: [[-4.2709 -0.9001 -0.2579  0.1075  0.0877]]\n",
      "MSE loss: 155.8125\n",
      "Iteration: 398900\n",
      "Gradient: [[  -9.7733   22.5225   28.726  -140.7345  148.1113]]\n",
      "Weights: [[-4.2715 -0.9    -0.2579  0.1075  0.0877]]\n",
      "MSE loss: 155.8056\n",
      "Iteration: 399000\n",
      "Gradient: [[  9.7176  -9.0413 -27.2192   8.0743  52.0924]]\n",
      "Weights: [[-4.2715 -0.9    -0.258   0.1075  0.0877]]\n",
      "MSE loss: 155.7964\n",
      "Iteration: 399100\n",
      "Gradient: [[ -16.8134   -6.6603   12.0162   50.3294 -190.2819]]\n",
      "Weights: [[-4.2706 -0.8999 -0.2581  0.1075  0.0877]]\n",
      "MSE loss: 155.7824\n",
      "Iteration: 399200\n",
      "Gradient: [[  -1.3664   -5.6912   -9.1421 -171.6357   55.0024]]\n",
      "Weights: [[-4.2698 -0.9    -0.2581  0.1075  0.0877]]\n",
      "MSE loss: 155.7765\n",
      "Iteration: 399300\n",
      "Gradient: [[  -4.7049   11.0638   22.0971 -108.0821  -45.8169]]\n",
      "Weights: [[-4.2693 -0.8998 -0.2582  0.1075  0.0877]]\n",
      "MSE loss: 155.766\n",
      "Iteration: 399400\n",
      "Gradient: [[  2.5697  -0.7269   0.8646 106.2071  17.3171]]\n",
      "Weights: [[-4.2703 -0.8995 -0.2583  0.1074  0.0878]]\n",
      "MSE loss: 155.7459\n",
      "Iteration: 399500\n",
      "Gradient: [[ -3.4561  -8.6844  34.9127  -9.869  108.3167]]\n",
      "Weights: [[-4.2709 -0.8994 -0.2584  0.1074  0.0878]]\n",
      "MSE loss: 155.7345\n",
      "Iteration: 399600\n",
      "Gradient: [[ -2.2416  -2.1769  -3.6856 153.0952 280.4833]]\n",
      "Weights: [[-4.2704 -0.8994 -0.2585  0.1074  0.0878]]\n",
      "MSE loss: 155.7245\n",
      "Iteration: 399700\n",
      "Gradient: [[  -4.6916    9.7561  -19.9077    6.8316 -201.5972]]\n",
      "Weights: [[-4.2712 -0.8992 -0.2585  0.1074  0.0878]]\n",
      "MSE loss: 155.7135\n",
      "Iteration: 399800\n",
      "Gradient: [[  -1.695    13.7021   29.8292   24.3474 -154.6401]]\n",
      "Weights: [[-4.27   -0.8991 -0.2586  0.1074  0.0878]]\n",
      "MSE loss: 155.7045\n",
      "Iteration: 399900\n",
      "Gradient: [[ -2.3278  23.6495 -10.2466  -3.2366  29.0612]]\n",
      "Weights: [[-4.2694 -0.8991 -0.2587  0.1074  0.0878]]\n",
      "MSE loss: 155.6939\n",
      "Iteration: 400000\n",
      "Gradient: [[ -5.5384 -30.9281  -2.9179 133.0749  13.9873]]\n",
      "Weights: [[-4.2695 -0.899  -0.2589  0.1074  0.0878]]\n",
      "MSE loss: 155.6756\n",
      "Iteration: 400100\n",
      "Gradient: [[  -4.3977  -10.2677   14.121    12.8335 -309.4789]]\n",
      "Weights: [[-4.2694 -0.8987 -0.2589  0.1074  0.0878]]\n",
      "MSE loss: 155.6642\n",
      "Iteration: 400200\n",
      "Gradient: [[ -9.442   10.6332 -35.6013   7.5041 282.3533]]\n",
      "Weights: [[-4.2695 -0.8987 -0.259   0.1074  0.0878]]\n",
      "MSE loss: 155.6545\n",
      "Iteration: 400300\n",
      "Gradient: [[   0.9748   12.688   -13.2026  -87.5599 -165.585 ]]\n",
      "Weights: [[-4.2694 -0.8987 -0.2591  0.1074  0.0878]]\n",
      "MSE loss: 155.641\n",
      "Iteration: 400400\n",
      "Gradient: [[ -4.4231   1.6737  -0.7895 131.285  268.1241]]\n",
      "Weights: [[-4.2709 -0.8986 -0.2592  0.1074  0.0878]]\n",
      "MSE loss: 155.6285\n",
      "Iteration: 400500\n",
      "Gradient: [[ -3.7227  -4.6731 -15.6687  13.1499 232.1805]]\n",
      "Weights: [[-4.2716 -0.8985 -0.2592  0.1074  0.0878]]\n",
      "MSE loss: 155.6197\n",
      "Iteration: 400600\n",
      "Gradient: [[  4.3575 -10.0597  10.2143 -44.4768 -43.5103]]\n",
      "Weights: [[-4.2722 -0.8983 -0.2593  0.1074  0.0878]]\n",
      "MSE loss: 155.6083\n",
      "Iteration: 400700\n",
      "Gradient: [[  2.4055  18.8816  -9.0878 -18.3418  82.1027]]\n",
      "Weights: [[-4.2718 -0.898  -0.2593  0.1074  0.0878]]\n",
      "MSE loss: 155.5935\n",
      "Iteration: 400800\n",
      "Gradient: [[  16.0553   14.8406   19.4515   79.4325 -182.1871]]\n",
      "Weights: [[-4.2722 -0.8978 -0.2594  0.1074  0.0878]]\n",
      "MSE loss: 155.5851\n",
      "Iteration: 400900\n",
      "Gradient: [[ 17.4335  -9.9042 -37.3772 -25.8416 129.5562]]\n",
      "Weights: [[-4.2728 -0.8975 -0.2594  0.1074  0.0879]]\n",
      "MSE loss: 155.5715\n",
      "Iteration: 401000\n",
      "Gradient: [[ -2.1664 -12.3828  14.5841 -55.1224  36.3993]]\n",
      "Weights: [[-4.2719 -0.8972 -0.2595  0.1074  0.0879]]\n",
      "MSE loss: 155.5593\n",
      "Iteration: 401100\n",
      "Gradient: [[  -1.3628    2.6434   -1.4568 -170.6972 -314.7769]]\n",
      "Weights: [[-4.2713 -0.897  -0.2596  0.1074  0.0879]]\n",
      "MSE loss: 155.5486\n",
      "Iteration: 401200\n",
      "Gradient: [[ 6.10000e-03 -6.46350e+00  2.33557e+01  2.42036e+01  9.84700e+00]]\n",
      "Weights: [[-4.272  -0.897  -0.2597  0.1074  0.0879]]\n",
      "MSE loss: 155.5324\n",
      "Iteration: 401300\n",
      "Gradient: [[  7.674   -5.1641  24.1869  56.7154 219.7931]]\n",
      "Weights: [[-4.2733 -0.8966 -0.2597  0.1073  0.0879]]\n",
      "MSE loss: 155.52\n",
      "Iteration: 401400\n",
      "Gradient: [[ -4.965   -8.383   -3.9708 -20.511  -45.4706]]\n",
      "Weights: [[-4.2733 -0.8964 -0.2597  0.1073  0.0879]]\n",
      "MSE loss: 155.5157\n",
      "Iteration: 401500\n",
      "Gradient: [[ -4.9624   8.7268 -49.5609 -55.318   68.4532]]\n",
      "Weights: [[-4.2734 -0.8963 -0.2598  0.1073  0.0879]]\n",
      "MSE loss: 155.5048\n",
      "Iteration: 401600\n",
      "Gradient: [[ -3.7678   6.6026  28.5429 -97.1147  66.0202]]\n",
      "Weights: [[-4.2722 -0.8962 -0.2599  0.1073  0.0879]]\n",
      "MSE loss: 155.491\n",
      "Iteration: 401700\n",
      "Gradient: [[ 9.9387 20.0692 39.5025 17.9995 69.7929]]\n",
      "Weights: [[-4.2725 -0.8959 -0.26    0.1073  0.0879]]\n",
      "MSE loss: 155.4792\n",
      "Iteration: 401800\n",
      "Gradient: [[   0.345    12.0566   13.3814   76.4278 -180.0915]]\n",
      "Weights: [[-4.2738 -0.8957 -0.26    0.1073  0.0879]]\n",
      "MSE loss: 155.4725\n",
      "Iteration: 401900\n",
      "Gradient: [[  10.7636   -9.8508   14.3809   -8.2834 -472.1506]]\n",
      "Weights: [[-4.2732 -0.8957 -0.2601  0.1073  0.0879]]\n",
      "MSE loss: 155.4585\n",
      "Iteration: 402000\n",
      "Gradient: [[ -6.2821  -8.5703 -38.0156   3.3586 -47.1053]]\n",
      "Weights: [[-4.2727 -0.8955 -0.2602  0.1073  0.0879]]\n",
      "MSE loss: 155.4473\n",
      "Iteration: 402100\n",
      "Gradient: [[ -5.2989  12.0007  12.2587  17.6987 -37.8619]]\n",
      "Weights: [[-4.2732 -0.8952 -0.2603  0.1073  0.0879]]\n",
      "MSE loss: 155.4307\n",
      "Iteration: 402200\n",
      "Gradient: [[-13.9197   2.6266  35.5025  72.9489  39.664 ]]\n",
      "Weights: [[-4.2725 -0.8951 -0.2604  0.1073  0.0879]]\n",
      "MSE loss: 155.4169\n",
      "Iteration: 402300\n",
      "Gradient: [[-5.250000e-02  9.292400e+00 -5.014400e+01 -4.086520e+01  1.065162e+02]]\n",
      "Weights: [[-4.2719 -0.8951 -0.2605  0.1073  0.0879]]\n",
      "MSE loss: 155.4104\n",
      "Iteration: 402400\n",
      "Gradient: [[  10.1629   -6.4821   30.558   222.7677 -276.2546]]\n",
      "Weights: [[-4.2721 -0.8949 -0.2606  0.1073  0.0879]]\n",
      "MSE loss: 155.4003\n",
      "Iteration: 402500\n",
      "Gradient: [[  13.7448   20.4766   30.4743 -101.8423  -38.0998]]\n",
      "Weights: [[-4.2715 -0.8947 -0.2607  0.1073  0.0879]]\n",
      "MSE loss: 155.3894\n",
      "Iteration: 402600\n",
      "Gradient: [[  0.498   -5.692  -21.0164 -15.7858 179.7294]]\n",
      "Weights: [[-4.271  -0.8948 -0.2608  0.1073  0.0879]]\n",
      "MSE loss: 155.3782\n",
      "Iteration: 402700\n",
      "Gradient: [[ -4.317   -2.4095 -39.8343  11.1769 -61.4507]]\n",
      "Weights: [[-4.2704 -0.8945 -0.2609  0.1073  0.0879]]\n",
      "MSE loss: 155.3721\n",
      "Iteration: 402800\n",
      "Gradient: [[ 17.2884  -8.6562   4.1187  22.032  183.1384]]\n",
      "Weights: [[-4.2704 -0.8945 -0.261   0.1072  0.0879]]\n",
      "MSE loss: 155.3598\n",
      "Iteration: 402900\n",
      "Gradient: [[   5.6523   -5.0696   36.9187 -108.9535  -29.0122]]\n",
      "Weights: [[-4.2725 -0.8942 -0.261   0.1072  0.088 ]]\n",
      "MSE loss: 155.3383\n",
      "Iteration: 403000\n",
      "Gradient: [[  -3.9668   -3.7153    7.154    93.9235 -172.0606]]\n",
      "Weights: [[-4.2717 -0.8941 -0.2611  0.1072  0.088 ]]\n",
      "MSE loss: 155.3276\n",
      "Iteration: 403100\n",
      "Gradient: [[ -0.6579 -16.9649  36.1246 -34.9865 127.1149]]\n",
      "Weights: [[-4.2721 -0.8939 -0.2612  0.1072  0.088 ]]\n",
      "MSE loss: 155.3122\n",
      "Iteration: 403200\n",
      "Gradient: [[ 10.6591  -9.7663   2.7395  27.9385 317.342 ]]\n",
      "Weights: [[-4.272  -0.8936 -0.2613  0.1072  0.088 ]]\n",
      "MSE loss: 155.2991\n",
      "Iteration: 403300\n",
      "Gradient: [[ -14.0862   -6.2521    4.181     4.7508 -131.3963]]\n",
      "Weights: [[-4.2725 -0.8937 -0.2614  0.1072  0.088 ]]\n",
      "MSE loss: 155.2899\n",
      "Iteration: 403400\n",
      "Gradient: [[ 4.2301 -5.8303  6.846  65.9089 63.0891]]\n",
      "Weights: [[-4.2721 -0.8936 -0.2615  0.1072  0.088 ]]\n",
      "MSE loss: 155.2779\n",
      "Iteration: 403500\n",
      "Gradient: [[ -2.6237 -22.9827  -1.4673 -20.5261   2.1913]]\n",
      "Weights: [[-4.2706 -0.8934 -0.2616  0.1072  0.088 ]]\n",
      "MSE loss: 155.2682\n",
      "Iteration: 403600\n",
      "Gradient: [[  -7.0537   13.2383   27.184    56.6171 -263.8172]]\n",
      "Weights: [[-4.2713 -0.8934 -0.2617  0.1072  0.088 ]]\n",
      "MSE loss: 155.2569\n",
      "Iteration: 403700\n",
      "Gradient: [[  -1.7007  -23.1299   15.5797   80.5218 -103.5836]]\n",
      "Weights: [[-4.2717 -0.8933 -0.2617  0.1072  0.088 ]]\n",
      "MSE loss: 155.245\n",
      "Iteration: 403800\n",
      "Gradient: [[  6.6232  -4.9612  34.3825 -84.3093   7.4959]]\n",
      "Weights: [[-4.2723 -0.8932 -0.2618  0.1072  0.088 ]]\n",
      "MSE loss: 155.2347\n",
      "Iteration: 403900\n",
      "Gradient: [[ 1.8345  0.9811  5.4951 29.7989 47.3382]]\n",
      "Weights: [[-4.2729 -0.8931 -0.2619  0.1072  0.088 ]]\n",
      "MSE loss: 155.2202\n",
      "Iteration: 404000\n",
      "Gradient: [[ 3.055000e+00 -2.599000e-01  2.357800e+00 -1.756810e+01 -4.081588e+02]]\n",
      "Weights: [[-4.2731 -0.8927 -0.262   0.1072  0.088 ]]\n",
      "MSE loss: 155.1999\n",
      "Iteration: 404100\n",
      "Gradient: [[  7.1555   1.9182  -0.5393 104.4874  32.5545]]\n",
      "Weights: [[-4.2728 -0.8923 -0.2621  0.1072  0.088 ]]\n",
      "MSE loss: 155.1859\n",
      "Iteration: 404200\n",
      "Gradient: [[  -7.6835   -3.0729   -0.9548   49.5414 -132.5269]]\n",
      "Weights: [[-4.273  -0.8923 -0.2621  0.1071  0.088 ]]\n",
      "MSE loss: 155.1749\n",
      "Iteration: 404300\n",
      "Gradient: [[  -8.2846  -19.0874   20.9151 -100.9455 -156.4293]]\n",
      "Weights: [[-4.2725 -0.8922 -0.2623  0.1071  0.088 ]]\n",
      "MSE loss: 155.16\n",
      "Iteration: 404400\n",
      "Gradient: [[ 11.1715 -20.8239   2.0201  71.7307 181.7303]]\n",
      "Weights: [[-4.273  -0.8919 -0.2623  0.1071  0.088 ]]\n",
      "MSE loss: 155.1479\n",
      "Iteration: 404500\n",
      "Gradient: [[ -14.4584   -9.2113   16.5004   23.0699 -108.8502]]\n",
      "Weights: [[-4.2728 -0.8917 -0.2624  0.1071  0.0881]]\n",
      "MSE loss: 155.1396\n",
      "Iteration: 404600\n",
      "Gradient: [[  -1.2578   -4.511    -6.9717  -22.3597 -426.7189]]\n",
      "Weights: [[-4.2734 -0.8915 -0.2624  0.1071  0.0881]]\n",
      "MSE loss: 155.1297\n",
      "Iteration: 404700\n",
      "Gradient: [[ -11.2767    6.3487    1.8076  -57.0971 -334.0009]]\n",
      "Weights: [[-4.2725 -0.8914 -0.2624  0.1071  0.0881]]\n",
      "MSE loss: 155.1237\n",
      "Iteration: 404800\n",
      "Gradient: [[ -5.9558  -2.3285 -44.9475 -96.3239   0.8444]]\n",
      "Weights: [[-4.2726 -0.8915 -0.2625  0.1071  0.0881]]\n",
      "MSE loss: 155.1176\n",
      "Iteration: 404900\n",
      "Gradient: [[  0.6474  -0.5412  15.5945  60.9633 181.5772]]\n",
      "Weights: [[-4.2729 -0.8914 -0.2626  0.1071  0.0881]]\n",
      "MSE loss: 155.104\n",
      "Iteration: 405000\n",
      "Gradient: [[  13.8683    3.7375   54.0933    8.8731 -309.2419]]\n",
      "Weights: [[-4.2731 -0.8912 -0.2627  0.1071  0.0881]]\n",
      "MSE loss: 155.0898\n",
      "Iteration: 405100\n",
      "Gradient: [[  1.6605   6.6055  42.0112 -81.4055 283.2776]]\n",
      "Weights: [[-4.2733 -0.8912 -0.2627  0.1071  0.0881]]\n",
      "MSE loss: 155.0814\n",
      "Iteration: 405200\n",
      "Gradient: [[   2.4442   -8.8809   -2.5598 -137.6233 -451.139 ]]\n",
      "Weights: [[-4.2739 -0.891  -0.2628  0.1071  0.0881]]\n",
      "MSE loss: 155.0679\n",
      "Iteration: 405300\n",
      "Gradient: [[ 5.4582 -2.4452  7.7645 66.4081 -2.5822]]\n",
      "Weights: [[-4.275  -0.8908 -0.2629  0.1071  0.0881]]\n",
      "MSE loss: 155.057\n",
      "Iteration: 405400\n",
      "Gradient: [[  10.2795    5.8375   -7.4373   60.2277 -415.3315]]\n",
      "Weights: [[-4.2743 -0.8907 -0.2629  0.1071  0.0881]]\n",
      "MSE loss: 155.0471\n",
      "Iteration: 405500\n",
      "Gradient: [[  11.3669  -21.6114  -22.927   -30.8777 -111.3457]]\n",
      "Weights: [[-4.2759 -0.8904 -0.263   0.1071  0.0881]]\n",
      "MSE loss: 155.0364\n",
      "Iteration: 405600\n",
      "Gradient: [[   6.5642    2.8368   12.5798  -35.6803 -322.657 ]]\n",
      "Weights: [[-4.2756 -0.8902 -0.2631  0.1071  0.0881]]\n",
      "MSE loss: 155.021\n",
      "Iteration: 405700\n",
      "Gradient: [[ -0.8188  10.6254   1.6069  83.4989 -40.8572]]\n",
      "Weights: [[-4.2757 -0.8899 -0.2632  0.1071  0.0881]]\n",
      "MSE loss: 155.007\n",
      "Iteration: 405800\n",
      "Gradient: [[   2.9229    6.6584    8.2197 -139.2583 -195.6181]]\n",
      "Weights: [[-4.2762 -0.8898 -0.2632  0.1071  0.0881]]\n",
      "MSE loss: 154.9999\n",
      "Iteration: 405900\n",
      "Gradient: [[  7.3857  -6.9406 -41.1006 -51.3857  52.1233]]\n",
      "Weights: [[-4.2763 -0.8896 -0.2633  0.1071  0.0881]]\n",
      "MSE loss: 154.9906\n",
      "Iteration: 406000\n",
      "Gradient: [[  1.1535  -3.0572 -21.6304  26.1347  80.8359]]\n",
      "Weights: [[-4.2749 -0.8895 -0.2633  0.1071  0.0881]]\n",
      "MSE loss: 154.9792\n",
      "Iteration: 406100\n",
      "Gradient: [[-1.1875  1.8388 19.9689 55.4076  1.8934]]\n",
      "Weights: [[-4.2755 -0.8893 -0.2634  0.1071  0.0881]]\n",
      "MSE loss: 154.9666\n",
      "Iteration: 406200\n",
      "Gradient: [[  -4.9407    5.3015  -32.926  -122.9016  -42.9039]]\n",
      "Weights: [[-4.2754 -0.8893 -0.2635  0.1071  0.0882]]\n",
      "MSE loss: 154.9569\n",
      "Iteration: 406300\n",
      "Gradient: [[  7.8393  22.6172  13.1882  22.9384 -61.1759]]\n",
      "Weights: [[-4.2753 -0.8891 -0.2635  0.1071  0.0882]]\n",
      "MSE loss: 154.9482\n",
      "Iteration: 406400\n",
      "Gradient: [[ 2.5048 11.9358 12.3762 60.6674 18.867 ]]\n",
      "Weights: [[-4.2744 -0.8892 -0.2636  0.1071  0.0882]]\n",
      "MSE loss: 154.9389\n",
      "Iteration: 406500\n",
      "Gradient: [[ -3.1625  12.5952   6.3205  31.0653 -47.0588]]\n",
      "Weights: [[-4.2751 -0.889  -0.2637  0.1071  0.0882]]\n",
      "MSE loss: 154.9245\n",
      "Iteration: 406600\n",
      "Gradient: [[  -1.6484    4.6535   30.2443  -33.695  -164.0071]]\n",
      "Weights: [[-4.2751 -0.889  -0.2638  0.1071  0.0882]]\n",
      "MSE loss: 154.9139\n",
      "Iteration: 406700\n",
      "Gradient: [[  -1.2976  -19.5817  -13.32    115.8118 -158.2308]]\n",
      "Weights: [[-4.2746 -0.8889 -0.2639  0.1071  0.0882]]\n",
      "MSE loss: 154.9027\n",
      "Iteration: 406800\n",
      "Gradient: [[  -1.2607   -9.8141   -7.8054  -28.4493 -167.8278]]\n",
      "Weights: [[-4.2742 -0.8887 -0.2639  0.1071  0.0882]]\n",
      "MSE loss: 154.8927\n",
      "Iteration: 406900\n",
      "Gradient: [[  8.3316   1.6087 -42.1316  61.5446  -3.2164]]\n",
      "Weights: [[-4.2756 -0.8886 -0.264   0.1071  0.0882]]\n",
      "MSE loss: 154.8837\n",
      "Iteration: 407000\n",
      "Gradient: [[  -9.2525  -14.8061    1.7855   90.7637 -283.651 ]]\n",
      "Weights: [[-4.2746 -0.8885 -0.2641  0.107   0.0882]]\n",
      "MSE loss: 154.8702\n",
      "Iteration: 407100\n",
      "Gradient: [[ -7.6869  -4.6355  10.6053  12.8883 131.1484]]\n",
      "Weights: [[-4.2731 -0.8882 -0.2642  0.107   0.0882]]\n",
      "MSE loss: 154.8605\n",
      "Iteration: 407200\n",
      "Gradient: [[ -7.6459   1.5484  13.2746 -56.3757 -10.3266]]\n",
      "Weights: [[-4.2741 -0.8882 -0.2643  0.107   0.0882]]\n",
      "MSE loss: 154.8444\n",
      "Iteration: 407300\n",
      "Gradient: [[  3.2146  15.8678   7.3081  12.9102 303.1213]]\n",
      "Weights: [[-4.2737 -0.8882 -0.2643  0.107   0.0882]]\n",
      "MSE loss: 154.8357\n",
      "Iteration: 407400\n",
      "Gradient: [[  -2.0329   17.9364  -28.5942 -104.4877 -374.8789]]\n",
      "Weights: [[-4.2751 -0.8882 -0.2644  0.107   0.0882]]\n",
      "MSE loss: 154.8268\n",
      "Iteration: 407500\n",
      "Gradient: [[ -8.4434 -24.9823  21.21   -17.1743 273.1486]]\n",
      "Weights: [[-4.2746 -0.8879 -0.2645  0.107   0.0882]]\n",
      "MSE loss: 154.8117\n",
      "Iteration: 407600\n",
      "Gradient: [[   1.1325    3.5958   17.1299  -42.2072 -203.5313]]\n",
      "Weights: [[-4.2748 -0.8877 -0.2645  0.107   0.0882]]\n",
      "MSE loss: 154.8025\n",
      "Iteration: 407700\n",
      "Gradient: [[  3.4729  11.4403   3.4512  39.1499 -53.5438]]\n",
      "Weights: [[-4.276  -0.8874 -0.2646  0.107   0.0882]]\n",
      "MSE loss: 154.7892\n",
      "Iteration: 407800\n",
      "Gradient: [[-1.308   3.942   9.1306 27.9207 19.5266]]\n",
      "Weights: [[-4.2752 -0.8871 -0.2647  0.107   0.0882]]\n",
      "MSE loss: 154.7789\n",
      "Iteration: 407900\n",
      "Gradient: [[ 6.1363  4.8712 22.6685 57.1049 11.1568]]\n",
      "Weights: [[-4.2744 -0.8871 -0.2647  0.107   0.0883]]\n",
      "MSE loss: 154.7706\n",
      "Iteration: 408000\n",
      "Gradient: [[   3.48     29.5441  -24.8238  -34.7242 -315.0366]]\n",
      "Weights: [[-4.2744 -0.8871 -0.2648  0.107   0.0883]]\n",
      "MSE loss: 154.7619\n",
      "Iteration: 408100\n",
      "Gradient: [[  3.749  -27.5879  23.7741 145.3502  40.3796]]\n",
      "Weights: [[-4.2741 -0.887  -0.2648  0.107   0.0883]]\n",
      "MSE loss: 154.7559\n",
      "Iteration: 408200\n",
      "Gradient: [[   4.8315  -11.0507   34.2304  -13.1587 -137.9735]]\n",
      "Weights: [[-4.2744 -0.8869 -0.2649  0.107   0.0883]]\n",
      "MSE loss: 154.7439\n",
      "Iteration: 408300\n",
      "Gradient: [[ -1.3349 -15.9025  31.76   122.4876  93.7871]]\n",
      "Weights: [[-4.274  -0.8868 -0.265   0.107   0.0883]]\n",
      "MSE loss: 154.7385\n",
      "Iteration: 408400\n",
      "Gradient: [[  -2.8481   -1.5598   48.158   107.692  -130.1313]]\n",
      "Weights: [[-4.2746 -0.8866 -0.265   0.1069  0.0883]]\n",
      "MSE loss: 154.7269\n",
      "Iteration: 408500\n",
      "Gradient: [[  -2.5649   -4.6339    9.3231  -78.7536 -109.0942]]\n",
      "Weights: [[-4.2756 -0.8866 -0.2651  0.1069  0.0883]]\n",
      "MSE loss: 154.7142\n",
      "Iteration: 408600\n",
      "Gradient: [[ -1.6466  -4.4776  16.5706 -55.6881 -96.8217]]\n",
      "Weights: [[-4.2757 -0.8865 -0.2651  0.1069  0.0883]]\n",
      "MSE loss: 154.7073\n",
      "Iteration: 408700\n",
      "Gradient: [[  5.8916  -7.408   39.9926  27.2246 128.5252]]\n",
      "Weights: [[-4.2753 -0.8864 -0.2652  0.1069  0.0883]]\n",
      "MSE loss: 154.6963\n",
      "Iteration: 408800\n",
      "Gradient: [[ -9.9814  10.1598 -24.2127  27.7855 -69.9313]]\n",
      "Weights: [[-4.2742 -0.8862 -0.2652  0.1069  0.0883]]\n",
      "MSE loss: 154.6909\n",
      "Iteration: 408900\n",
      "Gradient: [[ -1.7795  11.287   12.8486 -42.8142 338.3964]]\n",
      "Weights: [[-4.2738 -0.8859 -0.2653  0.1069  0.0883]]\n",
      "MSE loss: 154.6763\n",
      "Iteration: 409000\n",
      "Gradient: [[ -7.8413  -6.9596  10.2777 -93.0413 -53.1463]]\n",
      "Weights: [[-4.274  -0.886  -0.2654  0.1069  0.0883]]\n",
      "MSE loss: 154.6656\n",
      "Iteration: 409100\n",
      "Gradient: [[  -9.2201  -11.2103   25.9597  -78.0698 -111.4077]]\n",
      "Weights: [[-4.2746 -0.8859 -0.2655  0.1069  0.0883]]\n",
      "MSE loss: 154.6525\n",
      "Iteration: 409200\n",
      "Gradient: [[   5.8091  -37.6172  -49.9129  -18.4125 -163.0536]]\n",
      "Weights: [[-4.2744 -0.8858 -0.2656  0.1069  0.0883]]\n",
      "MSE loss: 154.6388\n",
      "Iteration: 409300\n",
      "Gradient: [[   2.3985   10.7404   -9.0838   47.3888 -182.2764]]\n",
      "Weights: [[-4.2751 -0.8855 -0.2657  0.1069  0.0883]]\n",
      "MSE loss: 154.6227\n",
      "Iteration: 409400\n",
      "Gradient: [[ -0.3351  19.3855 -10.6993 122.9531 -34.3728]]\n",
      "Weights: [[-4.2746 -0.8855 -0.2658  0.1069  0.0883]]\n",
      "MSE loss: 154.6107\n",
      "Iteration: 409500\n",
      "Gradient: [[  -3.4263    3.0793   31.0603  -78.33   -124.3243]]\n",
      "Weights: [[-4.2744 -0.8855 -0.2659  0.1069  0.0883]]\n",
      "MSE loss: 154.6038\n",
      "Iteration: 409600\n",
      "Gradient: [[  10.8062   16.4308   44.063   -32.7177 -174.2367]]\n",
      "Weights: [[-4.2743 -0.8853 -0.266   0.1069  0.0884]]\n",
      "MSE loss: 154.592\n",
      "Iteration: 409700\n",
      "Gradient: [[  -4.9333   -3.8703  -10.1227   31.9713 -231.4103]]\n",
      "Weights: [[-4.2749 -0.8853 -0.266   0.1069  0.0884]]\n",
      "MSE loss: 154.5783\n",
      "Iteration: 409800\n",
      "Gradient: [[ 10.881  -10.168  -31.2646 127.7899  31.5261]]\n",
      "Weights: [[-4.2735 -0.8851 -0.266   0.1068  0.0884]]\n",
      "MSE loss: 154.576\n",
      "Iteration: 409900\n",
      "Gradient: [[ -5.9403 -12.0225  12.5465  50.0583 176.9113]]\n",
      "Weights: [[-4.2735 -0.8852 -0.2661  0.1068  0.0884]]\n",
      "MSE loss: 154.5677\n",
      "Iteration: 410000\n",
      "Gradient: [[  -6.0877  -11.1789   22.0386  -73.3161 -127.0767]]\n",
      "Weights: [[-4.2738 -0.8853 -0.2662  0.1068  0.0884]]\n",
      "MSE loss: 154.5568\n",
      "Iteration: 410100\n",
      "Gradient: [[  9.5196 -19.2709  27.3994  -7.3979 189.7918]]\n",
      "Weights: [[-4.2746 -0.8853 -0.2663  0.1068  0.0884]]\n",
      "MSE loss: 154.5441\n",
      "Iteration: 410200\n",
      "Gradient: [[  -4.9466    5.6997   16.8094  -32.0012 -116.9834]]\n",
      "Weights: [[-4.2744 -0.8852 -0.2663  0.1068  0.0884]]\n",
      "MSE loss: 154.5334\n",
      "Iteration: 410300\n",
      "Gradient: [[  5.0613   4.6529  33.5566  26.3133 150.2491]]\n",
      "Weights: [[-4.2748 -0.8849 -0.2664  0.1068  0.0884]]\n",
      "MSE loss: 154.5151\n",
      "Iteration: 410400\n",
      "Gradient: [[ -6.6979 -16.4581  -1.4781  14.6094 -64.7026]]\n",
      "Weights: [[-4.2746 -0.8847 -0.2665  0.1068  0.0884]]\n",
      "MSE loss: 154.5039\n",
      "Iteration: 410500\n",
      "Gradient: [[  -0.7519    4.1273  -10.3771  -28.8696 -208.4872]]\n",
      "Weights: [[-4.2742 -0.8846 -0.2666  0.1068  0.0884]]\n",
      "MSE loss: 154.4942\n",
      "Iteration: 410600\n",
      "Gradient: [[  7.0418   9.5544  38.9161  58.1907 -41.1351]]\n",
      "Weights: [[-4.2746 -0.8845 -0.2666  0.1068  0.0884]]\n",
      "MSE loss: 154.4819\n",
      "Iteration: 410700\n",
      "Gradient: [[ -5.7834  11.7442  21.6683 -13.63   -99.1734]]\n",
      "Weights: [[-4.2746 -0.8844 -0.2667  0.1068  0.0884]]\n",
      "MSE loss: 154.4714\n",
      "Iteration: 410800\n",
      "Gradient: [[ -1.2559  13.1162   5.2606 -16.4448  74.845 ]]\n",
      "Weights: [[-4.2747 -0.8842 -0.2668  0.1068  0.0884]]\n",
      "MSE loss: 154.4569\n",
      "Iteration: 410900\n",
      "Gradient: [[  7.3874   8.5015 -12.7675 -76.7855  58.3761]]\n",
      "Weights: [[-4.2736 -0.884  -0.2668  0.1068  0.0884]]\n",
      "MSE loss: 154.4509\n",
      "Iteration: 411000\n",
      "Gradient: [[  -5.2259   17.5182   11.8479   68.1098 -214.5287]]\n",
      "Weights: [[-4.2736 -0.8839 -0.2669  0.1067  0.0885]]\n",
      "MSE loss: 154.4406\n",
      "Iteration: 411100\n",
      "Gradient: [[ 5.8797 -7.3307 44.9808 -0.227   2.2123]]\n",
      "Weights: [[-4.2745 -0.8838 -0.267   0.1067  0.0885]]\n",
      "MSE loss: 154.425\n",
      "Iteration: 411200\n",
      "Gradient: [[  4.9406 -24.7604   2.8257  21.9418 102.6613]]\n",
      "Weights: [[-4.2759 -0.8836 -0.267   0.1067  0.0885]]\n",
      "MSE loss: 154.4132\n",
      "Iteration: 411300\n",
      "Gradient: [[  -3.4881  -13.2476  -14.2163 -114.7649  -33.708 ]]\n",
      "Weights: [[-4.276  -0.8834 -0.2671  0.1067  0.0885]]\n",
      "MSE loss: 154.3967\n",
      "Iteration: 411400\n",
      "Gradient: [[  -4.5103   -0.7221   10.2475   16.6239 -242.2935]]\n",
      "Weights: [[-4.2762 -0.883  -0.2672  0.1067  0.0885]]\n",
      "MSE loss: 154.3845\n",
      "Iteration: 411500\n",
      "Gradient: [[ -5.777    4.8367 -23.6782  14.4593 153.4447]]\n",
      "Weights: [[-4.2768 -0.8827 -0.2673  0.1067  0.0885]]\n",
      "MSE loss: 154.371\n",
      "Iteration: 411600\n",
      "Gradient: [[  3.3665   1.8437  37.7918  -0.9342 224.9023]]\n",
      "Weights: [[-4.2769 -0.8824 -0.2673  0.1067  0.0885]]\n",
      "MSE loss: 154.3618\n",
      "Iteration: 411700\n",
      "Gradient: [[  6.9218  -4.5199 -58.3355 166.2351 125.3584]]\n",
      "Weights: [[-4.277  -0.8824 -0.2674  0.1067  0.0885]]\n",
      "MSE loss: 154.3517\n",
      "Iteration: 411800\n",
      "Gradient: [[ -10.4025   -9.459   -42.2381 -106.4898 -345.0878]]\n",
      "Weights: [[-4.2762 -0.8825 -0.2674  0.1067  0.0885]]\n",
      "MSE loss: 154.3464\n",
      "Iteration: 411900\n",
      "Gradient: [[ -3.1509  17.8774   9.9576 -43.8837  55.062 ]]\n",
      "Weights: [[-4.2756 -0.8824 -0.2675  0.1067  0.0885]]\n",
      "MSE loss: 154.3382\n",
      "Iteration: 412000\n",
      "Gradient: [[   2.6149  -12.8899    5.3201   36.8701 -109.2378]]\n",
      "Weights: [[-4.276  -0.8823 -0.2676  0.1067  0.0885]]\n",
      "MSE loss: 154.3238\n",
      "Iteration: 412100\n",
      "Gradient: [[ -8.3627  -2.6583  46.801   29.8963 -12.3984]]\n",
      "Weights: [[-4.2758 -0.8822 -0.2676  0.1067  0.0885]]\n",
      "MSE loss: 154.3126\n",
      "Iteration: 412200\n",
      "Gradient: [[ -17.7501    6.281   -26.8133 -132.6574 -149.7521]]\n",
      "Weights: [[-4.2759 -0.8821 -0.2677  0.1067  0.0885]]\n",
      "MSE loss: 154.3031\n",
      "Iteration: 412300\n",
      "Gradient: [[ -12.4149    8.7251  -11.7523   52.7574 -414.097 ]]\n",
      "Weights: [[-4.2757 -0.8819 -0.2678  0.1067  0.0885]]\n",
      "MSE loss: 154.295\n",
      "Iteration: 412400\n",
      "Gradient: [[  -8.6577  -35.1576    9.2465   27.9862 -118.9418]]\n",
      "Weights: [[-4.2764 -0.8817 -0.2678  0.1067  0.0885]]\n",
      "MSE loss: 154.2823\n",
      "Iteration: 412500\n",
      "Gradient: [[  8.1195   5.6931  45.5077  22.4117 -63.311 ]]\n",
      "Weights: [[-4.2755 -0.8815 -0.2679  0.1067  0.0885]]\n",
      "MSE loss: 154.2756\n",
      "Iteration: 412600\n",
      "Gradient: [[  15.9808   -8.7541   33.6066 -116.6731   88.0007]]\n",
      "Weights: [[-4.276  -0.8816 -0.268   0.1067  0.0885]]\n",
      "MSE loss: 154.2602\n",
      "Iteration: 412700\n",
      "Gradient: [[ -1.5721  12.9568 -13.2878  53.1847  92.2827]]\n",
      "Weights: [[-4.2766 -0.8815 -0.2681  0.1067  0.0886]]\n",
      "MSE loss: 154.2475\n",
      "Iteration: 412800\n",
      "Gradient: [[  1.1761   1.6633 -31.0717  -7.0778 116.3062]]\n",
      "Weights: [[-4.2769 -0.8812 -0.2682  0.1067  0.0886]]\n",
      "MSE loss: 154.2314\n",
      "Iteration: 412900\n",
      "Gradient: [[  -3.0135    0.2205   66.4985  -56.6    -150.2931]]\n",
      "Weights: [[-4.2769 -0.8809 -0.2683  0.1067  0.0886]]\n",
      "MSE loss: 154.22\n",
      "Iteration: 413000\n",
      "Gradient: [[ -9.4784 -12.5109  13.1894 -59.3496 -20.4187]]\n",
      "Weights: [[-4.2768 -0.8807 -0.2683  0.1066  0.0886]]\n",
      "MSE loss: 154.2076\n",
      "Iteration: 413100\n",
      "Gradient: [[  -1.1638   16.7192   13.6663  157.5803 -180.2793]]\n",
      "Weights: [[-4.2777 -0.8805 -0.2683  0.1066  0.0886]]\n",
      "MSE loss: 154.2002\n",
      "Iteration: 413200\n",
      "Gradient: [[  4.1293  -0.4482 -28.4894  66.9889 -39.9551]]\n",
      "Weights: [[-4.2773 -0.8803 -0.2684  0.1066  0.0886]]\n",
      "MSE loss: 154.1887\n",
      "Iteration: 413300\n",
      "Gradient: [[ -7.5589 -12.3902  33.5295 -52.2042 -41.0666]]\n",
      "Weights: [[-4.2764 -0.8802 -0.2685  0.1066  0.0886]]\n",
      "MSE loss: 154.1758\n",
      "Iteration: 413400\n",
      "Gradient: [[  6.2844  -3.7055 -10.5872  61.5167 -84.1226]]\n",
      "Weights: [[-4.2774 -0.8801 -0.2686  0.1066  0.0886]]\n",
      "MSE loss: 154.162\n",
      "Iteration: 413500\n",
      "Gradient: [[   1.729     1.2373   19.4946  175.7303 -151.1926]]\n",
      "Weights: [[-4.2776 -0.8798 -0.2686  0.1066  0.0886]]\n",
      "MSE loss: 154.1495\n",
      "Iteration: 413600\n",
      "Gradient: [[-10.9568   4.5554 -21.146   29.7046 -87.1617]]\n",
      "Weights: [[-4.2772 -0.8796 -0.2687  0.1066  0.0886]]\n",
      "MSE loss: 154.1361\n",
      "Iteration: 413700\n",
      "Gradient: [[ 20.6576   5.4378  -3.9953 107.6808 -59.284 ]]\n",
      "Weights: [[-4.2769 -0.8795 -0.2688  0.1066  0.0886]]\n",
      "MSE loss: 154.1292\n",
      "Iteration: 413800\n",
      "Gradient: [[   2.9373   -4.6917  -10.9255   82.8082 -214.0013]]\n",
      "Weights: [[-4.2771 -0.8796 -0.2688  0.1066  0.0886]]\n",
      "MSE loss: 154.1231\n",
      "Iteration: 413900\n",
      "Gradient: [[  8.4882  -3.0701 -37.7306   5.1547 186.2138]]\n",
      "Weights: [[-4.2776 -0.8796 -0.2689  0.1066  0.0886]]\n",
      "MSE loss: 154.1119\n",
      "Iteration: 414000\n",
      "Gradient: [[-19.769   13.5479 -23.282   27.6443 -67.7579]]\n",
      "Weights: [[-4.2781 -0.8794 -0.269   0.1066  0.0886]]\n",
      "MSE loss: 154.1\n",
      "Iteration: 414100\n",
      "Gradient: [[ -9.1129  -3.0586 -50.9301 -77.2937 204.0207]]\n",
      "Weights: [[-4.2786 -0.8792 -0.269   0.1065  0.0886]]\n",
      "MSE loss: 154.0883\n",
      "Iteration: 414200\n",
      "Gradient: [[-19.6408 -11.1914  13.6937   9.4054 -12.8178]]\n",
      "Weights: [[-4.2789 -0.879  -0.2691  0.1065  0.0886]]\n",
      "MSE loss: 154.0771\n",
      "Iteration: 414300\n",
      "Gradient: [[   1.2426   -2.9258   10.8741  166.9723 -290.944 ]]\n",
      "Weights: [[-4.2795 -0.8789 -0.2692  0.1065  0.0887]]\n",
      "MSE loss: 154.0701\n",
      "Iteration: 414400\n",
      "Gradient: [[   1.5143   -3.1358   47.3026   10.547  -117.7581]]\n",
      "Weights: [[-4.2793 -0.8787 -0.2692  0.1065  0.0887]]\n",
      "MSE loss: 154.0593\n",
      "Iteration: 414500\n",
      "Gradient: [[-1.754000e-01  1.422660e+01  5.069910e+01  3.392140e+01 -2.678172e+02]]\n",
      "Weights: [[-4.2785 -0.8787 -0.2692  0.1065  0.0887]]\n",
      "MSE loss: 154.0502\n",
      "Iteration: 414600\n",
      "Gradient: [[  2.0737  10.1994  39.5525  42.5382 -62.953 ]]\n",
      "Weights: [[-4.2787 -0.8785 -0.2693  0.1065  0.0887]]\n",
      "MSE loss: 154.0404\n",
      "Iteration: 414700\n",
      "Gradient: [[ -8.3755   7.9261  47.3404 -10.6716 158.2313]]\n",
      "Weights: [[-4.2784 -0.8785 -0.2693  0.1065  0.0887]]\n",
      "MSE loss: 154.0294\n",
      "Iteration: 414800\n",
      "Gradient: [[-11.5605  22.8434  -6.8712 -21.0747 264.712 ]]\n",
      "Weights: [[-4.2782 -0.8784 -0.2694  0.1065  0.0887]]\n",
      "MSE loss: 154.0199\n",
      "Iteration: 414900\n",
      "Gradient: [[   3.021    -6.0664  -21.083    35.2526 -128.7169]]\n",
      "Weights: [[-4.2795 -0.8782 -0.2695  0.1065  0.0887]]\n",
      "MSE loss: 154.0119\n",
      "Iteration: 415000\n",
      "Gradient: [[ 9.3382 -3.9493  5.2513 39.9009 -4.168 ]]\n",
      "Weights: [[-4.2804 -0.8781 -0.2695  0.1065  0.0887]]\n",
      "MSE loss: 154.0063\n",
      "Iteration: 415100\n",
      "Gradient: [[  15.1102  -19.6698   42.8211   -7.3837 -131.6564]]\n",
      "Weights: [[-4.2796 -0.8778 -0.2696  0.1065  0.0887]]\n",
      "MSE loss: 153.9874\n",
      "Iteration: 415200\n",
      "Gradient: [[   4.0473    9.2459   26.7751  -68.8926 -219.0014]]\n",
      "Weights: [[-4.2795 -0.8774 -0.2697  0.1065  0.0887]]\n",
      "MSE loss: 153.9726\n",
      "Iteration: 415300\n",
      "Gradient: [[ -1.4085 -20.7727   0.8345  90.1513  73.9315]]\n",
      "Weights: [[-4.2793 -0.877  -0.2698  0.1065  0.0887]]\n",
      "MSE loss: 153.9591\n",
      "Iteration: 415400\n",
      "Gradient: [[-17.0984 -16.4199 -13.3517 -24.1426 -43.1728]]\n",
      "Weights: [[-4.2795 -0.877  -0.2698  0.1065  0.0887]]\n",
      "MSE loss: 153.9486\n",
      "Iteration: 415500\n",
      "Gradient: [[ -3.8341 -10.0199 -44.6767 -50.2112 -66.9898]]\n",
      "Weights: [[-4.2791 -0.8769 -0.2699  0.1065  0.0887]]\n",
      "MSE loss: 153.9385\n",
      "Iteration: 415600\n",
      "Gradient: [[   4.0872    8.1628  -27.0613  -60.8891 -168.5205]]\n",
      "Weights: [[-4.2788 -0.877  -0.27    0.1065  0.0887]]\n",
      "MSE loss: 153.9278\n",
      "Iteration: 415700\n",
      "Gradient: [[ 0.4155 -1.2667 63.7051 49.0904 30.6014]]\n",
      "Weights: [[-4.2794 -0.8769 -0.2701  0.1065  0.0887]]\n",
      "MSE loss: 153.918\n",
      "Iteration: 415800\n",
      "Gradient: [[  -0.8973  -14.8227   14.7765   92.1325 -258.79  ]]\n",
      "Weights: [[-4.2787 -0.8769 -0.2702  0.1065  0.0887]]\n",
      "MSE loss: 153.9031\n",
      "Iteration: 415900\n",
      "Gradient: [[ -6.2084 -11.773   16.3497 157.4539 -64.8875]]\n",
      "Weights: [[-4.2787 -0.8766 -0.2703  0.1065  0.0887]]\n",
      "MSE loss: 153.8887\n",
      "Iteration: 416000\n",
      "Gradient: [[ -1.3207  -4.7418 -10.1004   6.3802 -46.5135]]\n",
      "Weights: [[-4.2788 -0.8764 -0.2704  0.1064  0.0888]]\n",
      "MSE loss: 153.8727\n",
      "Iteration: 416100\n",
      "Gradient: [[ -0.3959   2.5385  43.5882 -40.5362  68.0084]]\n",
      "Weights: [[-4.2785 -0.8763 -0.2705  0.1064  0.0888]]\n",
      "MSE loss: 153.8573\n",
      "Iteration: 416200\n",
      "Gradient: [[   4.5433   24.3572  -11.1598   80.9575 -338.908 ]]\n",
      "Weights: [[-4.2783 -0.8761 -0.2706  0.1064  0.0888]]\n",
      "MSE loss: 153.8469\n",
      "Iteration: 416300\n",
      "Gradient: [[ 13.5355  10.8136 -17.4789 142.449   98.7121]]\n",
      "Weights: [[-4.2781 -0.8761 -0.2707  0.1064  0.0888]]\n",
      "MSE loss: 153.8362\n",
      "Iteration: 416400\n",
      "Gradient: [[  -7.2425   -2.589    42.4562  -39.0337 -144.125 ]]\n",
      "Weights: [[-4.2797 -0.8758 -0.2708  0.1064  0.0888]]\n",
      "MSE loss: 153.8246\n",
      "Iteration: 416500\n",
      "Gradient: [[   0.391     5.504    11.3499   91.4802 -152.5658]]\n",
      "Weights: [[-4.2795 -0.8757 -0.2709  0.1064  0.0888]]\n",
      "MSE loss: 153.8161\n",
      "Iteration: 416600\n",
      "Gradient: [[   1.0411    2.6947  -14.2468   18.9455 -159.1173]]\n",
      "Weights: [[-4.2798 -0.8754 -0.2709  0.1064  0.0888]]\n",
      "MSE loss: 153.8019\n",
      "Iteration: 416700\n",
      "Gradient: [[   5.0988   13.5134   14.1529  -61.4201 -202.187 ]]\n",
      "Weights: [[-4.2796 -0.8751 -0.271   0.1064  0.0888]]\n",
      "MSE loss: 153.7899\n",
      "Iteration: 416800\n",
      "Gradient: [[ -6.0915  10.1456 -19.2756  -8.1393 167.6087]]\n",
      "Weights: [[-4.2806 -0.8749 -0.271   0.1064  0.0888]]\n",
      "MSE loss: 153.7777\n",
      "Iteration: 416900\n",
      "Gradient: [[ -13.281     8.388   -33.3406  -48.9624 -140.4565]]\n",
      "Weights: [[-4.2809 -0.8749 -0.2711  0.1064  0.0888]]\n",
      "MSE loss: 153.7718\n",
      "Iteration: 417000\n",
      "Gradient: [[   8.702    -4.1312   36.4506  -19.3493 -224.633 ]]\n",
      "Weights: [[-4.2805 -0.8747 -0.2711  0.1064  0.0888]]\n",
      "MSE loss: 153.7588\n",
      "Iteration: 417100\n",
      "Gradient: [[   0.8577    5.614    22.2838  -19.1211 -174.9471]]\n",
      "Weights: [[-4.2799 -0.8745 -0.2712  0.1064  0.0888]]\n",
      "MSE loss: 153.7452\n",
      "Iteration: 417200\n",
      "Gradient: [[-10.6396   2.0122   3.3922  -1.0689  52.9042]]\n",
      "Weights: [[-4.2803 -0.8744 -0.2713  0.1064  0.0888]]\n",
      "MSE loss: 153.733\n",
      "Iteration: 417300\n",
      "Gradient: [[ -16.7589  -20.0495   25.037     6.3474 -153.0345]]\n",
      "Weights: [[-4.2802 -0.8741 -0.2714  0.1064  0.0888]]\n",
      "MSE loss: 153.7239\n",
      "Iteration: 417400\n",
      "Gradient: [[  -7.5743  -26.382   -29.2746  -67.6171 -248.3602]]\n",
      "Weights: [[-4.281  -0.8739 -0.2715  0.1064  0.0888]]\n",
      "MSE loss: 153.7139\n",
      "Iteration: 417500\n",
      "Gradient: [[  -1.083   -10.7594   43.5006  116.6355 -144.8154]]\n",
      "Weights: [[-4.2797 -0.8739 -0.2715  0.1064  0.0888]]\n",
      "MSE loss: 153.7054\n",
      "Iteration: 417600\n",
      "Gradient: [[ 6.3501  1.519  47.6053 21.254  48.6001]]\n",
      "Weights: [[-4.2791 -0.8738 -0.2716  0.1064  0.0889]]\n",
      "MSE loss: 153.6915\n",
      "Iteration: 417700\n",
      "Gradient: [[ -9.7812 -14.9046  50.7811 -26.2437 -17.7949]]\n",
      "Weights: [[-4.2802 -0.8737 -0.2717  0.1063  0.0889]]\n",
      "MSE loss: 153.6793\n",
      "Iteration: 417800\n",
      "Gradient: [[   9.8545  -20.3912   40.5088  128.0903 -126.8687]]\n",
      "Weights: [[-4.281  -0.8737 -0.2717  0.1063  0.0889]]\n",
      "MSE loss: 153.6724\n",
      "Iteration: 417900\n",
      "Gradient: [[ -0.5872 -16.4075  35.1902  37.0994  37.4554]]\n",
      "Weights: [[-4.2811 -0.8734 -0.2717  0.1063  0.0889]]\n",
      "MSE loss: 153.6635\n",
      "Iteration: 418000\n",
      "Gradient: [[ -1.6763  -8.7198  -4.932  -43.0633 -86.3347]]\n",
      "Weights: [[-4.2813 -0.8732 -0.2718  0.1063  0.0889]]\n",
      "MSE loss: 153.65\n",
      "Iteration: 418100\n",
      "Gradient: [[  -2.0782   18.2487  -98.0803  -15.1824 -128.9025]]\n",
      "Weights: [[-4.2812 -0.8731 -0.2719  0.1063  0.0889]]\n",
      "MSE loss: 153.6397\n",
      "Iteration: 418200\n",
      "Gradient: [[   1.172   -12.2694   19.2087  -80.7661 -187.6208]]\n",
      "Weights: [[-4.281  -0.8731 -0.272   0.1063  0.0889]]\n",
      "MSE loss: 153.6291\n",
      "Iteration: 418300\n",
      "Gradient: [[ 10.1688  -7.9851  45.9115  19.872  266.7713]]\n",
      "Weights: [[-4.2808 -0.8729 -0.2721  0.1063  0.0889]]\n",
      "MSE loss: 153.6164\n",
      "Iteration: 418400\n",
      "Gradient: [[-13.7225   1.2349  57.3718  19.2959   0.2045]]\n",
      "Weights: [[-4.2801 -0.8729 -0.2721  0.1063  0.0889]]\n",
      "MSE loss: 153.6065\n",
      "Iteration: 418500\n",
      "Gradient: [[  -4.1581  -11.7213  -33.7188   44.1791 -392.7401]]\n",
      "Weights: [[-4.2796 -0.8727 -0.2722  0.1063  0.0889]]\n",
      "MSE loss: 153.5951\n",
      "Iteration: 418600\n",
      "Gradient: [[ -1.4701 -24.8761   0.2691  -9.6063 -15.1947]]\n",
      "Weights: [[-4.2802 -0.8728 -0.2723  0.1063  0.0889]]\n",
      "MSE loss: 153.5863\n",
      "Iteration: 418700\n",
      "Gradient: [[-11.2002  -8.8685 -26.7015  93.2527  56.1981]]\n",
      "Weights: [[-4.281  -0.8727 -0.2724  0.1063  0.0889]]\n",
      "MSE loss: 153.5757\n",
      "Iteration: 418800\n",
      "Gradient: [[ -5.5278 -14.3075  25.5118   7.3957 111.463 ]]\n",
      "Weights: [[-4.2805 -0.8726 -0.2725  0.1063  0.0889]]\n",
      "MSE loss: 153.5583\n",
      "Iteration: 418900\n",
      "Gradient: [[   9.5708  -12.9978   10.4211   11.0904 -372.883 ]]\n",
      "Weights: [[-4.2806 -0.8722 -0.2726  0.1063  0.0889]]\n",
      "MSE loss: 153.542\n",
      "Iteration: 419000\n",
      "Gradient: [[ 15.8582   0.6746 -21.1141  15.9941 341.6486]]\n",
      "Weights: [[-4.2805 -0.8719 -0.2726  0.1063  0.0889]]\n",
      "MSE loss: 153.5258\n",
      "Iteration: 419100\n",
      "Gradient: [[   0.5129    7.9444  -13.8042    8.861  -118.1809]]\n",
      "Weights: [[-4.2808 -0.8717 -0.2727  0.1063  0.0889]]\n",
      "MSE loss: 153.5165\n",
      "Iteration: 419200\n",
      "Gradient: [[  -5.8403    1.6109    0.7695   51.8733 -130.8316]]\n",
      "Weights: [[-4.2812 -0.8715 -0.2728  0.1063  0.089 ]]\n",
      "MSE loss: 153.5029\n",
      "Iteration: 419300\n",
      "Gradient: [[  5.2673 -19.0775  -3.3155 -41.6879 102.2701]]\n",
      "Weights: [[-4.2807 -0.8716 -0.2729  0.1062  0.089 ]]\n",
      "MSE loss: 153.4933\n",
      "Iteration: 419400\n",
      "Gradient: [[  1.7669 -10.0581  -8.6874 129.7719  -8.3117]]\n",
      "Weights: [[-4.2804 -0.8716 -0.273   0.1062  0.089 ]]\n",
      "MSE loss: 153.4819\n",
      "Iteration: 419500\n",
      "Gradient: [[ 17.3811   8.8287 -14.5723  24.2518 -96.0393]]\n",
      "Weights: [[-4.2805 -0.8715 -0.2731  0.1062  0.089 ]]\n",
      "MSE loss: 153.4692\n",
      "Iteration: 419600\n",
      "Gradient: [[  -4.3061   -4.0504  -26.0683   91.6052 -285.9186]]\n",
      "Weights: [[-4.2801 -0.8713 -0.2731  0.1062  0.089 ]]\n",
      "MSE loss: 153.4604\n",
      "Iteration: 419700\n",
      "Gradient: [[ 13.3715  -8.0318  44.1703  70.0026 -14.6776]]\n",
      "Weights: [[-4.28   -0.871  -0.2732  0.1062  0.089 ]]\n",
      "MSE loss: 153.4486\n",
      "Iteration: 419800\n",
      "Gradient: [[ -12.2415   -3.7702   53.9113   82.017  -165.4786]]\n",
      "Weights: [[-4.2801 -0.8709 -0.2733  0.1062  0.089 ]]\n",
      "MSE loss: 153.4354\n",
      "Iteration: 419900\n",
      "Gradient: [[-17.0595 -22.0204   2.631  -82.3313 449.2285]]\n",
      "Weights: [[-4.2801 -0.8708 -0.2734  0.1062  0.089 ]]\n",
      "MSE loss: 153.4234\n",
      "Iteration: 420000\n",
      "Gradient: [[-5.5342  8.8593 41.8676 -3.3966 36.5687]]\n",
      "Weights: [[-4.2816 -0.8705 -0.2735  0.1062  0.089 ]]\n",
      "MSE loss: 153.4115\n",
      "Iteration: 420100\n",
      "Gradient: [[  2.9386  -7.2713  38.5718 133.3979 125.1743]]\n",
      "Weights: [[-4.2816 -0.8704 -0.2736  0.1062  0.089 ]]\n",
      "MSE loss: 153.3994\n",
      "Iteration: 420200\n",
      "Gradient: [[-15.7543   5.6955  -6.0528 -53.4888 -49.4968]]\n",
      "Weights: [[-4.2821 -0.8701 -0.2736  0.1062  0.089 ]]\n",
      "MSE loss: 153.389\n",
      "Iteration: 420300\n",
      "Gradient: [[  -1.6274  -12.7699   28.6358  -33.3604 -425.4071]]\n",
      "Weights: [[-4.2838 -0.87   -0.2737  0.1062  0.089 ]]\n",
      "MSE loss: 153.3871\n",
      "Iteration: 420400\n",
      "Gradient: [[ -8.7698 -29.4203 -37.3608 161.0262 121.2817]]\n",
      "Weights: [[-4.283  -0.8699 -0.2738  0.1062  0.089 ]]\n",
      "MSE loss: 153.3724\n",
      "Iteration: 420500\n",
      "Gradient: [[  3.0242   4.4973  43.6971 -11.4293 258.9199]]\n",
      "Weights: [[-4.2822 -0.8696 -0.2738  0.1062  0.089 ]]\n",
      "MSE loss: 153.3593\n",
      "Iteration: 420600\n",
      "Gradient: [[   7.0764   -5.5066   -1.0447  -37.8772 -154.4547]]\n",
      "Weights: [[-4.2825 -0.8694 -0.2739  0.1062  0.089 ]]\n",
      "MSE loss: 153.3488\n",
      "Iteration: 420700\n",
      "Gradient: [[  -1.8478   -8.1896   35.616    -0.9188 -103.4823]]\n",
      "Weights: [[-4.2812 -0.8692 -0.274   0.1062  0.089 ]]\n",
      "MSE loss: 153.3359\n",
      "Iteration: 420800\n",
      "Gradient: [[   0.8795    4.6733  -17.9811   76.0196 -213.4418]]\n",
      "Weights: [[-4.2805 -0.8689 -0.2741  0.1062  0.089 ]]\n",
      "MSE loss: 153.325\n",
      "Iteration: 420900\n",
      "Gradient: [[  13.0786   -0.2548   70.477    58.7984 -185.6858]]\n",
      "Weights: [[-4.2814 -0.8691 -0.2742  0.1062  0.0891]]\n",
      "MSE loss: 153.3081\n",
      "Iteration: 421000\n",
      "Gradient: [[  16.9356    4.5275   22.3914 -146.9269  217.9293]]\n",
      "Weights: [[-4.2821 -0.8688 -0.2742  0.1062  0.0891]]\n",
      "MSE loss: 153.2943\n",
      "Iteration: 421100\n",
      "Gradient: [[-13.1655  17.4026  24.5549 -50.2645   5.7857]]\n",
      "Weights: [[-4.2828 -0.8685 -0.2743  0.1062  0.0891]]\n",
      "MSE loss: 153.2802\n",
      "Iteration: 421200\n",
      "Gradient: [[  -6.798   -17.0525  -21.6392   -0.8648 -127.843 ]]\n",
      "Weights: [[-4.2828 -0.8684 -0.2744  0.1062  0.0891]]\n",
      "MSE loss: 153.268\n",
      "Iteration: 421300\n",
      "Gradient: [[  -8.8187   -1.9187  -33.2218   69.8539 -190.8281]]\n",
      "Weights: [[-4.2838 -0.868  -0.2745  0.1062  0.0891]]\n",
      "MSE loss: 153.2551\n",
      "Iteration: 421400\n",
      "Gradient: [[   2.6475   31.6107  -17.598  -115.3804 -599.2407]]\n",
      "Weights: [[-4.2828 -0.8678 -0.2745  0.1062  0.0891]]\n",
      "MSE loss: 153.245\n",
      "Iteration: 421500\n",
      "Gradient: [[  -8.2665    2.4991   54.5336  -33.8642 -292.7724]]\n",
      "Weights: [[-4.283  -0.8678 -0.2746  0.1062  0.0891]]\n",
      "MSE loss: 153.2343\n",
      "Iteration: 421600\n",
      "Gradient: [[  -9.286    -0.6278   12.6422  -14.6972 -129.4092]]\n",
      "Weights: [[-4.2825 -0.8676 -0.2747  0.1062  0.0891]]\n",
      "MSE loss: 153.2254\n",
      "Iteration: 421700\n",
      "Gradient: [[ -7.4563  14.896  -33.0272 -96.3656 -34.4309]]\n",
      "Weights: [[-4.2824 -0.8675 -0.2747  0.1061  0.0891]]\n",
      "MSE loss: 153.2148\n",
      "Iteration: 421800\n",
      "Gradient: [[  -9.7212  -10.95     38.3735    1.3439 -202.0281]]\n",
      "Weights: [[-4.2822 -0.8674 -0.2749  0.1061  0.0891]]\n",
      "MSE loss: 153.1992\n",
      "Iteration: 421900\n",
      "Gradient: [[   3.1898    6.6248   22.76     87.1173 -104.3474]]\n",
      "Weights: [[-4.2824 -0.8673 -0.275   0.1061  0.0891]]\n",
      "MSE loss: 153.1844\n",
      "Iteration: 422000\n",
      "Gradient: [[ 3.5984 12.6385 -6.6686 30.8279 61.2776]]\n",
      "Weights: [[-4.2832 -0.8671 -0.2751  0.1061  0.0891]]\n",
      "MSE loss: 153.1711\n",
      "Iteration: 422100\n",
      "Gradient: [[   8.8778    2.166     0.8391   69.9522 -194.8263]]\n",
      "Weights: [[-4.2818 -0.8668 -0.2751  0.1061  0.0891]]\n",
      "MSE loss: 153.1639\n",
      "Iteration: 422200\n",
      "Gradient: [[ -0.7497  -1.1321  27.8276  -5.892  317.392 ]]\n",
      "Weights: [[-4.2823 -0.8668 -0.2753  0.1061  0.0891]]\n",
      "MSE loss: 153.1484\n",
      "Iteration: 422300\n",
      "Gradient: [[  13.7114   -4.8168  -12.9891   42.9442 -569.2143]]\n",
      "Weights: [[-4.2819 -0.8667 -0.2754  0.1061  0.0891]]\n",
      "MSE loss: 153.1348\n",
      "Iteration: 422400\n",
      "Gradient: [[-13.7527 -17.2076  16.0536 -87.1041  43.6425]]\n",
      "Weights: [[-4.2829 -0.8667 -0.2755  0.1061  0.0891]]\n",
      "MSE loss: 153.1237\n",
      "Iteration: 422500\n",
      "Gradient: [[  10.7668   -5.7294  -40.4619   52.9377 -187.1693]]\n",
      "Weights: [[-4.2813 -0.8665 -0.2755  0.1061  0.0891]]\n",
      "MSE loss: 153.1164\n",
      "Iteration: 422600\n",
      "Gradient: [[  -3.1376    3.6951  -27.1262   -5.5002 -360.4701]]\n",
      "Weights: [[-4.2815 -0.8664 -0.2756  0.1061  0.0891]]\n",
      "MSE loss: 153.1058\n",
      "Iteration: 422700\n",
      "Gradient: [[  5.4988   0.7267  49.0715  24.8871 186.6018]]\n",
      "Weights: [[-4.2821 -0.8663 -0.2756  0.1061  0.0892]]\n",
      "MSE loss: 153.0959\n",
      "Iteration: 422800\n",
      "Gradient: [[ -6.3393  -7.5099  50.351  -32.291  -93.4216]]\n",
      "Weights: [[-4.2825 -0.8662 -0.2757  0.1061  0.0892]]\n",
      "MSE loss: 153.0849\n",
      "Iteration: 422900\n",
      "Gradient: [[  3.8335 -15.9921   4.023   -0.5226  30.5728]]\n",
      "Weights: [[-4.2832 -0.8661 -0.2758  0.106   0.0892]]\n",
      "MSE loss: 153.0746\n",
      "Iteration: 423000\n",
      "Gradient: [[ -6.7937 -15.2122  35.4552 162.4981 -53.4227]]\n",
      "Weights: [[-4.2829 -0.8659 -0.2759  0.1061  0.0892]]\n",
      "MSE loss: 153.0625\n",
      "Iteration: 423100\n",
      "Gradient: [[  -7.6703    8.6285   21.0483   20.7429 -292.439 ]]\n",
      "Weights: [[-4.2829 -0.8657 -0.276   0.106   0.0892]]\n",
      "MSE loss: 153.0468\n",
      "Iteration: 423200\n",
      "Gradient: [[ 11.0648   1.6228  -3.3452 137.7795 244.9492]]\n",
      "Weights: [[-4.2831 -0.8657 -0.276   0.106   0.0892]]\n",
      "MSE loss: 153.0401\n",
      "Iteration: 423300\n",
      "Gradient: [[  -7.358     0.829    21.085   -87.0652 -234.6382]]\n",
      "Weights: [[-4.2829 -0.8656 -0.2761  0.106   0.0892]]\n",
      "MSE loss: 153.0313\n",
      "Iteration: 423400\n",
      "Gradient: [[  -8.6171   -3.4377   -7.2521  -17.0037 -177.0045]]\n",
      "Weights: [[-4.2841 -0.8654 -0.2762  0.106   0.0892]]\n",
      "MSE loss: 153.0225\n",
      "Iteration: 423500\n",
      "Gradient: [[ -6.1538 -20.9943 -73.4534  11.2868  -4.845 ]]\n",
      "Weights: [[-4.2834 -0.8652 -0.2762  0.106   0.0892]]\n",
      "MSE loss: 153.0087\n",
      "Iteration: 423600\n",
      "Gradient: [[ -8.9402   5.0785  10.5428  25.2568 -15.7843]]\n",
      "Weights: [[-4.2829 -0.8652 -0.2762  0.106   0.0892]]\n",
      "MSE loss: 153.0008\n",
      "Iteration: 423700\n",
      "Gradient: [[ -4.5029  23.0752  19.2861  75.3326 242.1338]]\n",
      "Weights: [[-4.2826 -0.8652 -0.2763  0.106   0.0892]]\n",
      "MSE loss: 152.9907\n",
      "Iteration: 423800\n",
      "Gradient: [[ -0.26    -2.0224  29.4939 -65.8755  42.8213]]\n",
      "Weights: [[-4.2825 -0.8651 -0.2764  0.106   0.0892]]\n",
      "MSE loss: 152.9803\n",
      "Iteration: 423900\n",
      "Gradient: [[   4.3988   -6.0989   27.1338  -75.0471 -201.0254]]\n",
      "Weights: [[-4.2832 -0.8651 -0.2765  0.106   0.0892]]\n",
      "MSE loss: 152.9718\n",
      "Iteration: 424000\n",
      "Gradient: [[  -2.1071    5.3387   43.8821   49.6245 -176.0729]]\n",
      "Weights: [[-4.2837 -0.8648 -0.2765  0.106   0.0892]]\n",
      "MSE loss: 152.9598\n",
      "Iteration: 424100\n",
      "Gradient: [[-6.0346 -8.6691 27.9779 57.3486 56.6712]]\n",
      "Weights: [[-4.2835 -0.8646 -0.2766  0.106   0.0892]]\n",
      "MSE loss: 152.9454\n",
      "Iteration: 424200\n",
      "Gradient: [[ -17.4493    8.1179   -9.2611  -45.086  -442.4337]]\n",
      "Weights: [[-4.2832 -0.8644 -0.2766  0.106   0.0892]]\n",
      "MSE loss: 152.9373\n",
      "Iteration: 424300\n",
      "Gradient: [[-7.16    1.6316 39.9155 22.5957 88.3127]]\n",
      "Weights: [[-4.2838 -0.8644 -0.2767  0.106   0.0893]]\n",
      "MSE loss: 152.9305\n",
      "Iteration: 424400\n",
      "Gradient: [[ -4.2267  16.4493  15.3284 115.9906  -5.9736]]\n",
      "Weights: [[-4.2823 -0.8643 -0.2768  0.106   0.0893]]\n",
      "MSE loss: 152.9197\n",
      "Iteration: 424500\n",
      "Gradient: [[ -15.9208   -3.4178   -4.4173    6.5565 -161.1528]]\n",
      "Weights: [[-4.2831 -0.8642 -0.2768  0.106   0.0893]]\n",
      "MSE loss: 152.9083\n",
      "Iteration: 424600\n",
      "Gradient: [[  -0.3776  -12.975    32.0096 -110.4105   -1.0057]]\n",
      "Weights: [[-4.2835 -0.8639 -0.2769  0.106   0.0893]]\n",
      "MSE loss: 152.8949\n",
      "Iteration: 424700\n",
      "Gradient: [[   5.4302    4.986    -6.8681   12.2109 -347.1248]]\n",
      "Weights: [[-4.282  -0.8639 -0.277   0.106   0.0893]]\n",
      "MSE loss: 152.8884\n",
      "Iteration: 424800\n",
      "Gradient: [[   2.4023  -24.8816  -37.8949  -29.2086 -129.9181]]\n",
      "Weights: [[-4.2819 -0.8637 -0.277   0.106   0.0893]]\n",
      "MSE loss: 152.8801\n",
      "Iteration: 424900\n",
      "Gradient: [[  9.2498   4.897   28.0306 -55.4898 -81.9279]]\n",
      "Weights: [[-4.2817 -0.8636 -0.2771  0.106   0.0893]]\n",
      "MSE loss: 152.8694\n",
      "Iteration: 425000\n",
      "Gradient: [[ -2.953   -4.6261 -23.7662 -63.0779   3.4939]]\n",
      "Weights: [[-4.2825 -0.8635 -0.2772  0.106   0.0893]]\n",
      "MSE loss: 152.8554\n",
      "Iteration: 425100\n",
      "Gradient: [[  10.7615    0.8016  -43.2593  -36.1364 -138.9803]]\n",
      "Weights: [[-4.2838 -0.8633 -0.2772  0.1059  0.0893]]\n",
      "MSE loss: 152.8441\n",
      "Iteration: 425200\n",
      "Gradient: [[   3.9233    6.2697  -33.0166   19.9922 -345.7984]]\n",
      "Weights: [[-4.2837 -0.8634 -0.2773  0.1059  0.0893]]\n",
      "MSE loss: 152.8375\n",
      "Iteration: 425300\n",
      "Gradient: [[   0.5526    3.868    22.4519  -53.3144 -157.4947]]\n",
      "Weights: [[-4.2835 -0.8632 -0.2774  0.1059  0.0893]]\n",
      "MSE loss: 152.8256\n",
      "Iteration: 425400\n",
      "Gradient: [[ -4.8115 -13.9578  71.4391  97.1241 162.7341]]\n",
      "Weights: [[-4.2841 -0.863  -0.2775  0.106   0.0893]]\n",
      "MSE loss: 152.8139\n",
      "Iteration: 425500\n",
      "Gradient: [[ 4.0584  2.4259  4.96   60.4441 -8.4036]]\n",
      "Weights: [[-4.2828 -0.8629 -0.2775  0.1059  0.0893]]\n",
      "MSE loss: 152.8049\n",
      "Iteration: 425600\n",
      "Gradient: [[   6.3231   -7.0257   10.5294  -28.4666 -165.8594]]\n",
      "Weights: [[-4.283  -0.8628 -0.2776  0.1059  0.0893]]\n",
      "MSE loss: 152.7939\n",
      "Iteration: 425700\n",
      "Gradient: [[  9.6226 -12.583   22.7827   1.5375 331.0572]]\n",
      "Weights: [[-4.2839 -0.8626 -0.2777  0.1059  0.0893]]\n",
      "MSE loss: 152.783\n",
      "Iteration: 425800\n",
      "Gradient: [[  -3.5777  -10.1137   26.2702   -0.4668 -213.8976]]\n",
      "Weights: [[-4.2849 -0.8624 -0.2778  0.1059  0.0893]]\n",
      "MSE loss: 152.7725\n",
      "Iteration: 425900\n",
      "Gradient: [[  -3.5369   -7.3383   -7.479    77.5982 -206.489 ]]\n",
      "Weights: [[-4.2844 -0.8622 -0.2778  0.1059  0.0893]]\n",
      "MSE loss: 152.7595\n",
      "Iteration: 426000\n",
      "Gradient: [[ -7.2726   8.0696 -17.3274  16.7186 -99.0203]]\n",
      "Weights: [[-4.2851 -0.8619 -0.2779  0.1059  0.0893]]\n",
      "MSE loss: 152.7493\n",
      "Iteration: 426100\n",
      "Gradient: [[  6.2463 -22.531   -5.8368  77.7558 -22.5131]]\n",
      "Weights: [[-4.2837 -0.8619 -0.278   0.1059  0.0894]]\n",
      "MSE loss: 152.7386\n",
      "Iteration: 426200\n",
      "Gradient: [[   4.3862  -11.606    -6.8765   17.7873 -140.2009]]\n",
      "Weights: [[-4.2834 -0.862  -0.2781  0.1059  0.0894]]\n",
      "MSE loss: 152.7283\n",
      "Iteration: 426300\n",
      "Gradient: [[  11.8077   22.071     6.7292   43.984  -214.5671]]\n",
      "Weights: [[-4.2833 -0.8619 -0.2782  0.1059  0.0894]]\n",
      "MSE loss: 152.72\n",
      "Iteration: 426400\n",
      "Gradient: [[ -1.2173  -5.6644 -36.8508  88.4727   9.6224]]\n",
      "Weights: [[-4.2831 -0.8621 -0.2782  0.1059  0.0894]]\n",
      "MSE loss: 152.7129\n",
      "Iteration: 426500\n",
      "Gradient: [[  -8.8585   -3.8766   -8.5502 -101.4277  108.5792]]\n",
      "Weights: [[-4.2822 -0.862  -0.2783  0.1059  0.0894]]\n",
      "MSE loss: 152.7031\n",
      "Iteration: 426600\n",
      "Gradient: [[ 11.9601 -16.2049 -70.7032   4.3858   1.1065]]\n",
      "Weights: [[-4.2826 -0.8619 -0.2784  0.1059  0.0894]]\n",
      "MSE loss: 152.688\n",
      "Iteration: 426700\n",
      "Gradient: [[  -2.1962  -10.2508   17.9849   33.8019 -198.9801]]\n",
      "Weights: [[-4.2834 -0.8617 -0.2785  0.1059  0.0894]]\n",
      "MSE loss: 152.6738\n",
      "Iteration: 426800\n",
      "Gradient: [[  -7.6928    1.0224   -0.4332  -61.0935 -175.5198]]\n",
      "Weights: [[-4.2829 -0.8614 -0.2786  0.1059  0.0894]]\n",
      "MSE loss: 152.6613\n",
      "Iteration: 426900\n",
      "Gradient: [[ 9.920000e-02  7.467400e+00  4.458800e+00 -4.588980e+01 -1.376199e+02]]\n",
      "Weights: [[-4.2831 -0.8613 -0.2786  0.1059  0.0894]]\n",
      "MSE loss: 152.653\n",
      "Iteration: 427000\n",
      "Gradient: [[  -4.135     1.3465  -22.1583   33.5907 -369.4035]]\n",
      "Weights: [[-4.2839 -0.8613 -0.2787  0.1059  0.0894]]\n",
      "MSE loss: 152.6455\n",
      "Iteration: 427100\n",
      "Gradient: [[  -7.6284   -9.3026   16.3078  150.2906 -119.5833]]\n",
      "Weights: [[-4.284  -0.8611 -0.2787  0.1059  0.0894]]\n",
      "MSE loss: 152.6339\n",
      "Iteration: 427200\n",
      "Gradient: [[  6.7839 -11.5105  32.5794 -30.0806  47.8684]]\n",
      "Weights: [[-4.2833 -0.8609 -0.2787  0.1059  0.0894]]\n",
      "MSE loss: 152.6255\n",
      "Iteration: 427300\n",
      "Gradient: [[  0.7912   5.6353  -3.3971 -58.5128   1.0481]]\n",
      "Weights: [[-4.2837 -0.8606 -0.2789  0.1059  0.0894]]\n",
      "MSE loss: 152.6097\n",
      "Iteration: 427400\n",
      "Gradient: [[  2.9654  -6.8485  -2.477   11.8768 -34.7962]]\n",
      "Weights: [[-4.2837 -0.8605 -0.2789  0.1058  0.0894]]\n",
      "MSE loss: 152.6009\n",
      "Iteration: 427500\n",
      "Gradient: [[   3.4974   18.2411   59.161   -54.4536 -494.784 ]]\n",
      "Weights: [[-4.2842 -0.8603 -0.279   0.1058  0.0894]]\n",
      "MSE loss: 152.5907\n",
      "Iteration: 427600\n",
      "Gradient: [[-7.5324 12.8286 38.6013 18.068  26.4709]]\n",
      "Weights: [[-4.2841 -0.8603 -0.279   0.1058  0.0894]]\n",
      "MSE loss: 152.5837\n",
      "Iteration: 427700\n",
      "Gradient: [[  5.9145   4.3114  -0.2715 -62.9892  24.4379]]\n",
      "Weights: [[-4.2842 -0.8602 -0.2791  0.1058  0.0895]]\n",
      "MSE loss: 152.5687\n",
      "Iteration: 427800\n",
      "Gradient: [[   9.0275   -6.235    26.2946  -26.4355 -321.3256]]\n",
      "Weights: [[-4.2835 -0.8602 -0.2792  0.1058  0.0895]]\n",
      "MSE loss: 152.5583\n",
      "Iteration: 427900\n",
      "Gradient: [[  -0.7133  -23.2444   33.1706   53.9044 -362.5207]]\n",
      "Weights: [[-4.2827 -0.8602 -0.2793  0.1058  0.0895]]\n",
      "MSE loss: 152.5506\n",
      "Iteration: 428000\n",
      "Gradient: [[  -0.9662   18.3555    6.8843  -21.6253 -466.3804]]\n",
      "Weights: [[-4.2826 -0.8602 -0.2793  0.1058  0.0895]]\n",
      "MSE loss: 152.5421\n",
      "Iteration: 428100\n",
      "Gradient: [[   0.2839  -25.8594  -23.9846 -143.4174  144.2413]]\n",
      "Weights: [[-4.2833 -0.8602 -0.2794  0.1058  0.0895]]\n",
      "MSE loss: 152.5323\n",
      "Iteration: 428200\n",
      "Gradient: [[ -13.5333    8.5757  -37.0825   60.2652 -271.0045]]\n",
      "Weights: [[-4.2844 -0.86   -0.2795  0.1058  0.0895]]\n",
      "MSE loss: 152.522\n",
      "Iteration: 428300\n",
      "Gradient: [[   5.2205  -22.1504  -19.2982  -44.6579 -307.5168]]\n",
      "Weights: [[-4.2834 -0.8599 -0.2796  0.1058  0.0895]]\n",
      "MSE loss: 152.5053\n",
      "Iteration: 428400\n",
      "Gradient: [[  -0.9833  -29.659    31.641  -137.922   205.2354]]\n",
      "Weights: [[-4.2836 -0.8596 -0.2796  0.1058  0.0895]]\n",
      "MSE loss: 152.4932\n",
      "Iteration: 428500\n",
      "Gradient: [[  -3.0243    9.758    35.0145   55.6486 -195.3791]]\n",
      "Weights: [[-4.2837 -0.8594 -0.2797  0.1057  0.0895]]\n",
      "MSE loss: 152.4843\n",
      "Iteration: 428600\n",
      "Gradient: [[ 10.8518  31.5249 -43.3206 -84.8782  70.5653]]\n",
      "Weights: [[-4.2848 -0.8591 -0.2798  0.1057  0.0895]]\n",
      "MSE loss: 152.4686\n",
      "Iteration: 428700\n",
      "Gradient: [[  0.7341   2.8172  12.4454 -81.3647 169.4191]]\n",
      "Weights: [[-4.2849 -0.859  -0.2798  0.1057  0.0895]]\n",
      "MSE loss: 152.4643\n",
      "Iteration: 428800\n",
      "Gradient: [[   5.294     5.2895  -14.0034 -105.4697  161.4101]]\n",
      "Weights: [[-4.2859 -0.8588 -0.2798  0.1057  0.0895]]\n",
      "MSE loss: 152.458\n",
      "Iteration: 428900\n",
      "Gradient: [[  2.4767 -12.9757  -8.0596 -48.5116 102.7791]]\n",
      "Weights: [[-4.2864 -0.8585 -0.2799  0.1057  0.0895]]\n",
      "MSE loss: 152.4455\n",
      "Iteration: 429000\n",
      "Gradient: [[  2.7356 -10.3994 -18.5881 -66.3671 -26.4097]]\n",
      "Weights: [[-4.2856 -0.8583 -0.2799  0.1057  0.0895]]\n",
      "MSE loss: 152.4293\n",
      "Iteration: 429100\n",
      "Gradient: [[  3.7126  21.2452  -8.1442 -88.4873 -39.2027]]\n",
      "Weights: [[-4.2851 -0.858  -0.28    0.1057  0.0895]]\n",
      "MSE loss: 152.4159\n",
      "Iteration: 429200\n",
      "Gradient: [[ 10.8273  -2.7165  11.194   -0.1852 -30.2469]]\n",
      "Weights: [[-4.2857 -0.8579 -0.2801  0.1057  0.0895]]\n",
      "MSE loss: 152.405\n",
      "Iteration: 429300\n",
      "Gradient: [[  6.1331  -9.8371  20.8817 179.1237  87.7688]]\n",
      "Weights: [[-4.2853 -0.8577 -0.2801  0.1057  0.0896]]\n",
      "MSE loss: 152.3966\n",
      "Iteration: 429400\n",
      "Gradient: [[   1.0179    8.2146    0.2859  118.9931 -261.6982]]\n",
      "Weights: [[-4.2863 -0.8574 -0.2802  0.1057  0.0896]]\n",
      "MSE loss: 152.3839\n",
      "Iteration: 429500\n",
      "Gradient: [[   8.8753   -3.2465   26.767  -112.1692  304.6448]]\n",
      "Weights: [[-4.2868 -0.8572 -0.2803  0.1057  0.0896]]\n",
      "MSE loss: 152.3734\n",
      "Iteration: 429600\n",
      "Gradient: [[  6.5457 -15.5969  25.1676 -14.7044  -3.9607]]\n",
      "Weights: [[-4.2872 -0.8571 -0.2803  0.1057  0.0896]]\n",
      "MSE loss: 152.3681\n",
      "Iteration: 429700\n",
      "Gradient: [[-12.5477   1.4533   4.534   22.9891 -79.8741]]\n",
      "Weights: [[-4.2874 -0.8568 -0.2803  0.1057  0.0896]]\n",
      "MSE loss: 152.3569\n",
      "Iteration: 429800\n",
      "Gradient: [[  5.1248 -22.7253 -29.5664 126.2587 230.859 ]]\n",
      "Weights: [[-4.286  -0.8566 -0.2804  0.1057  0.0896]]\n",
      "MSE loss: 152.3451\n",
      "Iteration: 429900\n",
      "Gradient: [[  7.8432  11.6029  41.966  -41.2126 299.6449]]\n",
      "Weights: [[-4.2862 -0.8566 -0.2805  0.1056  0.0896]]\n",
      "MSE loss: 152.3338\n",
      "Iteration: 430000\n",
      "Gradient: [[-15.1731   7.6598  -6.3416 126.4743 -30.4972]]\n",
      "Weights: [[-4.2869 -0.8564 -0.2805  0.1056  0.0896]]\n",
      "MSE loss: 152.3249\n",
      "Iteration: 430100\n",
      "Gradient: [[ 4.5865  5.6021  6.4139 95.0414 81.3097]]\n",
      "Weights: [[-4.2867 -0.8563 -0.2806  0.1056  0.0896]]\n",
      "MSE loss: 152.3163\n",
      "Iteration: 430200\n",
      "Gradient: [[ -22.5053    5.5719   40.7708  -56.7984 -159.1488]]\n",
      "Weights: [[-4.2873 -0.8562 -0.2807  0.1056  0.0896]]\n",
      "MSE loss: 152.3066\n",
      "Iteration: 430300\n",
      "Gradient: [[ 12.4793  12.0949  26.0611  88.3597 119.7405]]\n",
      "Weights: [[-4.2865 -0.8559 -0.2807  0.1056  0.0896]]\n",
      "MSE loss: 152.297\n",
      "Iteration: 430400\n",
      "Gradient: [[  -2.5089    5.4617   12.167   100.3995 -194.3003]]\n",
      "Weights: [[-4.2868 -0.8557 -0.2808  0.1056  0.0896]]\n",
      "MSE loss: 152.2864\n",
      "Iteration: 430500\n",
      "Gradient: [[   4.1023    5.8033   27.2318  102.1755 -126.5954]]\n",
      "Weights: [[-4.288  -0.8556 -0.2808  0.1056  0.0896]]\n",
      "MSE loss: 152.2802\n",
      "Iteration: 430600\n",
      "Gradient: [[   9.295   -22.2219   16.2286   15.3823 -135.4944]]\n",
      "Weights: [[-4.2879 -0.8555 -0.2809  0.1056  0.0896]]\n",
      "MSE loss: 152.2732\n",
      "Iteration: 430700\n",
      "Gradient: [[  5.8325  -4.4389  34.707   12.7625 221.8242]]\n",
      "Weights: [[-4.2885 -0.8552 -0.281   0.1056  0.0896]]\n",
      "MSE loss: 152.2615\n",
      "Iteration: 430800\n",
      "Gradient: [[ -3.1712  -9.9723   4.3565 -92.0721 124.6031]]\n",
      "Weights: [[-4.2881 -0.8549 -0.281   0.1056  0.0896]]\n",
      "MSE loss: 152.248\n",
      "Iteration: 430900\n",
      "Gradient: [[ -2.9662  15.2755  21.5874 -54.8636 390.2232]]\n",
      "Weights: [[-4.288  -0.8547 -0.2811  0.1056  0.0896]]\n",
      "MSE loss: 152.2382\n",
      "Iteration: 431000\n",
      "Gradient: [[  -5.4038  -26.6439    4.9205   64.5844 -258.6881]]\n",
      "Weights: [[-4.2877 -0.8545 -0.2812  0.1056  0.0896]]\n",
      "MSE loss: 152.2272\n",
      "Iteration: 431100\n",
      "Gradient: [[   6.7138   16.4889  -22.823  -102.3745   16.3215]]\n",
      "Weights: [[-4.2883 -0.8543 -0.2812  0.1056  0.0896]]\n",
      "MSE loss: 152.2176\n",
      "Iteration: 431200\n",
      "Gradient: [[ 12.1833  -7.1544  31.2668 -23.9095 268.4389]]\n",
      "Weights: [[-4.288  -0.8544 -0.2813  0.1055  0.0896]]\n",
      "MSE loss: 152.2124\n",
      "Iteration: 431300\n",
      "Gradient: [[ -1.8384   1.5774  25.1304   8.9447 -93.7812]]\n",
      "Weights: [[-4.2876 -0.8542 -0.2814  0.1055  0.0896]]\n",
      "MSE loss: 152.1953\n",
      "Iteration: 431400\n",
      "Gradient: [[-8.500000e-02 -1.521090e+01  6.170640e+01 -9.850760e+01 -3.707551e+02]]\n",
      "Weights: [[-4.2874 -0.854  -0.2815  0.1055  0.0897]]\n",
      "MSE loss: 152.1798\n",
      "Iteration: 431500\n",
      "Gradient: [[ -1.2552   6.8607  33.5598 -83.2554 -71.7959]]\n",
      "Weights: [[-4.2885 -0.8542 -0.2816  0.1055  0.0897]]\n",
      "MSE loss: 152.1736\n",
      "Iteration: 431600\n",
      "Gradient: [[  -6.6167  -24.3935   31.1295    3.3202 -157.7056]]\n",
      "Weights: [[-4.2877 -0.8539 -0.2817  0.1055  0.0897]]\n",
      "MSE loss: 152.1572\n",
      "Iteration: 431700\n",
      "Gradient: [[   4.3796    5.796     6.862   -90.5236 -137.3268]]\n",
      "Weights: [[-4.2872 -0.8537 -0.2818  0.1055  0.0897]]\n",
      "MSE loss: 152.1452\n",
      "Iteration: 431800\n",
      "Gradient: [[  11.4181   -2.0725   23.753    73.5378 -298.5164]]\n",
      "Weights: [[-4.2888 -0.8536 -0.2819  0.1055  0.0897]]\n",
      "MSE loss: 152.134\n",
      "Iteration: 431900\n",
      "Gradient: [[  5.28   -11.0678  -1.7904  44.0532 -50.8868]]\n",
      "Weights: [[-4.2872 -0.8534 -0.282   0.1055  0.0897]]\n",
      "MSE loss: 152.1204\n",
      "Iteration: 432000\n",
      "Gradient: [[  -1.8992   -4.5226   40.0769  -41.3333 -178.149 ]]\n",
      "Weights: [[-4.2881 -0.8533 -0.282   0.1055  0.0897]]\n",
      "MSE loss: 152.1094\n",
      "Iteration: 432100\n",
      "Gradient: [[ -0.7933 -13.43    24.6948  47.0854  77.4256]]\n",
      "Weights: [[-4.2874 -0.8532 -0.2821  0.1055  0.0897]]\n",
      "MSE loss: 152.0948\n",
      "Iteration: 432200\n",
      "Gradient: [[ -12.3575  -12.9575   57.5264  -67.9503 -262.9184]]\n",
      "Weights: [[-4.2872 -0.853  -0.2822  0.1055  0.0897]]\n",
      "MSE loss: 152.0852\n",
      "Iteration: 432300\n",
      "Gradient: [[  4.9707  -5.7794   9.815  115.8845 152.7284]]\n",
      "Weights: [[-4.2889 -0.8529 -0.2823  0.1055  0.0897]]\n",
      "MSE loss: 152.0734\n",
      "Iteration: 432400\n",
      "Gradient: [[  3.1113 -18.5061 -19.1348  16.259  -38.3319]]\n",
      "Weights: [[-4.2901 -0.8526 -0.2824  0.1055  0.0897]]\n",
      "MSE loss: 152.0625\n",
      "Iteration: 432500\n",
      "Gradient: [[ -0.6011  19.2464  30.7812 -21.929   -6.2219]]\n",
      "Weights: [[-4.2884 -0.8523 -0.2825  0.1055  0.0897]]\n",
      "MSE loss: 152.0393\n",
      "Iteration: 432600\n",
      "Gradient: [[  -8.0831   15.7885    0.9726 -102.2536  -25.6907]]\n",
      "Weights: [[-4.2888 -0.8524 -0.2825  0.1055  0.0897]]\n",
      "MSE loss: 152.0347\n",
      "Iteration: 432700\n",
      "Gradient: [[   7.9517  -12.0393  -18.7823  -31.8287 -542.2617]]\n",
      "Weights: [[-4.288  -0.8522 -0.2826  0.1055  0.0897]]\n",
      "MSE loss: 152.0211\n",
      "Iteration: 432800\n",
      "Gradient: [[  -0.135    -3.1591  -31.9868  -19.8686 -107.9738]]\n",
      "Weights: [[-4.2892 -0.8521 -0.2827  0.1055  0.0897]]\n",
      "MSE loss: 152.0122\n",
      "Iteration: 432900\n",
      "Gradient: [[  10.8674   -6.069    -6.8097   42.0333 -156.3538]]\n",
      "Weights: [[-4.2896 -0.8519 -0.2828  0.1055  0.0898]]\n",
      "MSE loss: 151.9994\n",
      "Iteration: 433000\n",
      "Gradient: [[  1.1616 -14.4831  -6.3651 -52.616  -13.0525]]\n",
      "Weights: [[-4.2891 -0.8518 -0.2828  0.1055  0.0898]]\n",
      "MSE loss: 151.9864\n",
      "Iteration: 433100\n",
      "Gradient: [[  5.3718   9.8124  59.0997 -74.3088  97.5386]]\n",
      "Weights: [[-4.2893 -0.8518 -0.2829  0.1055  0.0898]]\n",
      "MSE loss: 151.9789\n",
      "Iteration: 433200\n",
      "Gradient: [[ -1.3034  11.1052  30.5835 -42.9995  92.24  ]]\n",
      "Weights: [[-4.2894 -0.8515 -0.283   0.1055  0.0898]]\n",
      "MSE loss: 151.9655\n",
      "Iteration: 433300\n",
      "Gradient: [[ -7.1795 -14.3529 -15.13   -68.6433 112.8111]]\n",
      "Weights: [[-4.2907 -0.8513 -0.2831  0.1054  0.0898]]\n",
      "MSE loss: 151.9599\n",
      "Iteration: 433400\n",
      "Gradient: [[  -6.1352    1.4424   23.5927  -11.1028 -344.    ]]\n",
      "Weights: [[-4.2894 -0.8511 -0.2831  0.1054  0.0898]]\n",
      "MSE loss: 151.9415\n",
      "Iteration: 433500\n",
      "Gradient: [[  6.2185  23.2029   6.4982 148.4244  21.822 ]]\n",
      "Weights: [[-4.2886 -0.851  -0.2832  0.1054  0.0898]]\n",
      "MSE loss: 151.9286\n",
      "Iteration: 433600\n",
      "Gradient: [[  -4.0635   13.1898   32.0028   91.747  -522.1251]]\n",
      "Weights: [[-4.2896 -0.851  -0.2833  0.1054  0.0898]]\n",
      "MSE loss: 151.9239\n",
      "Iteration: 433700\n",
      "Gradient: [[ -4.5988 -12.9603 -44.8496  69.6398 185.0029]]\n",
      "Weights: [[-4.2895 -0.8509 -0.2833  0.1054  0.0898]]\n",
      "MSE loss: 151.9171\n",
      "Iteration: 433800\n",
      "Gradient: [[-10.9287  -8.2517  -4.9838  65.748  -38.4217]]\n",
      "Weights: [[-4.2892 -0.8507 -0.2834  0.1054  0.0898]]\n",
      "MSE loss: 151.9043\n",
      "Iteration: 433900\n",
      "Gradient: [[12.0886 11.2389 35.9631 14.13   59.4036]]\n",
      "Weights: [[-4.2889 -0.8506 -0.2835  0.1054  0.0898]]\n",
      "MSE loss: 151.8906\n",
      "Iteration: 434000\n",
      "Gradient: [[   8.5184  -10.4337  -39.6859   51.747  -152.9298]]\n",
      "Weights: [[-4.2883 -0.8507 -0.2836  0.1054  0.0898]]\n",
      "MSE loss: 151.8805\n",
      "Iteration: 434100\n",
      "Gradient: [[   1.4567   -5.7958   20.4541  -45.9518 -136.319 ]]\n",
      "Weights: [[-4.2893 -0.8504 -0.2836  0.1054  0.0898]]\n",
      "MSE loss: 151.8708\n",
      "Iteration: 434200\n",
      "Gradient: [[-12.361   -4.9996  62.7812  27.4033  85.4282]]\n",
      "Weights: [[-4.2878 -0.8503 -0.2837  0.1054  0.0898]]\n",
      "MSE loss: 151.8601\n",
      "Iteration: 434300\n",
      "Gradient: [[ -4.7887   6.8383  21.7052 -71.964    4.6008]]\n",
      "Weights: [[-4.2867 -0.8503 -0.2838  0.1054  0.0898]]\n",
      "MSE loss: 151.8568\n",
      "Iteration: 434400\n",
      "Gradient: [[   7.9352   10.0782   45.4126  -39.7623 -140.8918]]\n",
      "Weights: [[-4.2872 -0.8503 -0.2839  0.1054  0.0898]]\n",
      "MSE loss: 151.8406\n",
      "Iteration: 434500\n",
      "Gradient: [[-11.9349  -3.3412 -22.5406 -22.392  -62.3559]]\n",
      "Weights: [[-4.287  -0.8503 -0.284   0.1054  0.0898]]\n",
      "MSE loss: 151.8308\n",
      "Iteration: 434600\n",
      "Gradient: [[ -3.5212 -12.9861  -7.8239 -33.1838  10.7258]]\n",
      "Weights: [[-4.2877 -0.8502 -0.2841  0.1054  0.0898]]\n",
      "MSE loss: 151.8208\n",
      "Iteration: 434700\n",
      "Gradient: [[   5.8599    8.4676   -3.5723  -16.084  -193.0287]]\n",
      "Weights: [[-4.2882 -0.85   -0.2842  0.1054  0.0898]]\n",
      "MSE loss: 151.8073\n",
      "Iteration: 434800\n",
      "Gradient: [[  8.7131   3.0482 -65.1308  84.7711  19.7152]]\n",
      "Weights: [[-4.2883 -0.8499 -0.2843  0.1054  0.0899]]\n",
      "MSE loss: 151.7941\n",
      "Iteration: 434900\n",
      "Gradient: [[ -3.3084  -4.5363 -35.885    4.3256  87.4653]]\n",
      "Weights: [[-4.2896 -0.8498 -0.2843  0.1054  0.0899]]\n",
      "MSE loss: 151.7872\n",
      "Iteration: 435000\n",
      "Gradient: [[   2.0963   -7.1241   12.7489  -71.5477 -339.3014]]\n",
      "Weights: [[-4.2896 -0.8494 -0.2843  0.1054  0.0899]]\n",
      "MSE loss: 151.7739\n",
      "Iteration: 435100\n",
      "Gradient: [[  7.4761   6.5565  -1.7857 -56.2745 -74.9182]]\n",
      "Weights: [[-4.2888 -0.8491 -0.2844  0.1054  0.0899]]\n",
      "MSE loss: 151.76\n",
      "Iteration: 435200\n",
      "Gradient: [[   1.7421   -5.0713   28.9421  -91.6568 -162.5916]]\n",
      "Weights: [[-4.2882 -0.8492 -0.2845  0.1053  0.0899]]\n",
      "MSE loss: 151.7529\n",
      "Iteration: 435300\n",
      "Gradient: [[  -5.8875  -10.8098  -30.3925   31.8015 -320.4698]]\n",
      "Weights: [[-4.2891 -0.8489 -0.2845  0.1053  0.0899]]\n",
      "MSE loss: 151.7409\n",
      "Iteration: 435400\n",
      "Gradient: [[  -1.1521    6.0196   32.0797   -2.641  -185.4933]]\n",
      "Weights: [[-4.29   -0.8488 -0.2846  0.1053  0.0899]]\n",
      "MSE loss: 151.7325\n",
      "Iteration: 435500\n",
      "Gradient: [[   4.4898   15.0306   20.6522   -0.4679 -319.2931]]\n",
      "Weights: [[-4.2902 -0.8485 -0.2847  0.1053  0.0899]]\n",
      "MSE loss: 151.7192\n",
      "Iteration: 435600\n",
      "Gradient: [[-12.9883  -5.4848   3.6259 121.3507   4.6096]]\n",
      "Weights: [[-4.2902 -0.8484 -0.2847  0.1053  0.0899]]\n",
      "MSE loss: 151.7084\n",
      "Iteration: 435700\n",
      "Gradient: [[  1.2223  -6.7341  40.8117  69.7036 -50.3855]]\n",
      "Weights: [[-4.29   -0.8482 -0.2848  0.1053  0.0899]]\n",
      "MSE loss: 151.6899\n",
      "Iteration: 435800\n",
      "Gradient: [[-2.7019  7.0093 59.939  -6.8939 20.457 ]]\n",
      "Weights: [[-4.2903 -0.8479 -0.2849  0.1053  0.0899]]\n",
      "MSE loss: 151.6765\n",
      "Iteration: 435900\n",
      "Gradient: [[ 1.0199 15.286  46.1259 99.0698 17.3285]]\n",
      "Weights: [[-4.2897 -0.8477 -0.2851  0.1053  0.0899]]\n",
      "MSE loss: 151.6587\n",
      "Iteration: 436000\n",
      "Gradient: [[   1.1458    5.3678  -45.0625   -7.6322 -157.6955]]\n",
      "Weights: [[-4.2893 -0.8475 -0.2851  0.1053  0.0899]]\n",
      "MSE loss: 151.6481\n",
      "Iteration: 436100\n",
      "Gradient: [[  3.1727  -6.3601 -14.5163 -35.9362  -0.2483]]\n",
      "Weights: [[-4.2898 -0.8473 -0.2852  0.1053  0.0899]]\n",
      "MSE loss: 151.6332\n",
      "Iteration: 436200\n",
      "Gradient: [[   6.1421    5.2327   38.1805  -29.3734 -219.7026]]\n",
      "Weights: [[-4.2904 -0.8471 -0.2853  0.1053  0.0899]]\n",
      "MSE loss: 151.6206\n",
      "Iteration: 436300\n",
      "Gradient: [[  -3.6469   -5.6971  -27.5459   -2.3469 -274.9653]]\n",
      "Weights: [[-4.2903 -0.8466 -0.2854  0.1052  0.09  ]]\n",
      "MSE loss: 151.6056\n",
      "Iteration: 436400\n",
      "Gradient: [[-7.861   8.0305 -6.9089 29.1825 31.2363]]\n",
      "Weights: [[-4.2895 -0.8467 -0.2855  0.1052  0.09  ]]\n",
      "MSE loss: 151.5947\n",
      "Iteration: 436500\n",
      "Gradient: [[  13.2384    5.8638   -4.0003  -69.942  -209.2438]]\n",
      "Weights: [[-4.2893 -0.8464 -0.2856  0.1052  0.09  ]]\n",
      "MSE loss: 151.5813\n",
      "Iteration: 436600\n",
      "Gradient: [[  -5.2761    6.9385  -24.9569   63.062  -366.2801]]\n",
      "Weights: [[-4.2904 -0.8462 -0.2856  0.1052  0.09  ]]\n",
      "MSE loss: 151.57\n",
      "Iteration: 436700\n",
      "Gradient: [[  -6.0532   -7.1024  -22.6485  -30.5702 -265.0312]]\n",
      "Weights: [[-4.2896 -0.8461 -0.2857  0.1052  0.09  ]]\n",
      "MSE loss: 151.5606\n",
      "Iteration: 436800\n",
      "Gradient: [[   7.4356  -21.4433    6.3354   15.661  -284.2726]]\n",
      "Weights: [[-4.2889 -0.846  -0.2858  0.1052  0.09  ]]\n",
      "MSE loss: 151.5523\n",
      "Iteration: 436900\n",
      "Gradient: [[ 5.840000e-02  1.834320e+01  3.665170e+01  5.391740e+01 -3.608039e+02]]\n",
      "Weights: [[-4.2885 -0.8457 -0.2859  0.1052  0.09  ]]\n",
      "MSE loss: 151.5381\n",
      "Iteration: 437000\n",
      "Gradient: [[ 2.0405 -6.0603 -7.365  40.36   85.4108]]\n",
      "Weights: [[-4.2894 -0.8457 -0.286   0.1052  0.09  ]]\n",
      "MSE loss: 151.5248\n",
      "Iteration: 437100\n",
      "Gradient: [[11.8787  7.4211 -5.5535 52.7377 37.0126]]\n",
      "Weights: [[-4.2889 -0.8455 -0.2861  0.1052  0.09  ]]\n",
      "MSE loss: 151.5136\n",
      "Iteration: 437200\n",
      "Gradient: [[  -2.7967    7.8877    9.3199   10.7519 -182.1878]]\n",
      "Weights: [[-4.2895 -0.8455 -0.2862  0.1052  0.09  ]]\n",
      "MSE loss: 151.5013\n",
      "Iteration: 437300\n",
      "Gradient: [[  2.6539 -11.8292   2.7238  47.1188 170.4806]]\n",
      "Weights: [[-4.2902 -0.8453 -0.2863  0.1052  0.09  ]]\n",
      "MSE loss: 151.4874\n",
      "Iteration: 437400\n",
      "Gradient: [[  -1.3535  -17.538    25.6954 -101.5561 -102.8914]]\n",
      "Weights: [[-4.289  -0.8452 -0.2864  0.1052  0.09  ]]\n",
      "MSE loss: 151.4795\n",
      "Iteration: 437500\n",
      "Gradient: [[  -4.5339    8.3475  -10.3423   14.6035 -284.9557]]\n",
      "Weights: [[-4.2895 -0.8448 -0.2864  0.1052  0.09  ]]\n",
      "MSE loss: 151.4673\n",
      "Iteration: 437600\n",
      "Gradient: [[-17.5774  -4.0192  17.424  -65.9124 -95.5928]]\n",
      "Weights: [[-4.2906 -0.8446 -0.2865  0.1052  0.09  ]]\n",
      "MSE loss: 151.4573\n",
      "Iteration: 437700\n",
      "Gradient: [[ 10.0239 -16.7497  30.5678 107.2356  40.2297]]\n",
      "Weights: [[-4.2911 -0.8446 -0.2866  0.1052  0.09  ]]\n",
      "MSE loss: 151.4478\n",
      "Iteration: 437800\n",
      "Gradient: [[   1.1144    9.9854  -23.0723   31.757  -192.742 ]]\n",
      "Weights: [[-4.2915 -0.8444 -0.2866  0.1052  0.09  ]]\n",
      "MSE loss: 151.4395\n",
      "Iteration: 437900\n",
      "Gradient: [[  -8.6774   13.4276   20.0957   33.062  -131.1041]]\n",
      "Weights: [[-4.291  -0.8444 -0.2866  0.1052  0.09  ]]\n",
      "MSE loss: 151.4331\n",
      "Iteration: 438000\n",
      "Gradient: [[ -9.9399   0.6273 -40.6391  34.0729  65.5968]]\n",
      "Weights: [[-4.2912 -0.8441 -0.2867  0.1052  0.09  ]]\n",
      "MSE loss: 151.418\n",
      "Iteration: 438100\n",
      "Gradient: [[  -5.3833  -23.5483  -12.6827  -23.5708 -240.3538]]\n",
      "Weights: [[-4.2905 -0.844  -0.2868  0.1052  0.0901]]\n",
      "MSE loss: 151.4099\n",
      "Iteration: 438200\n",
      "Gradient: [[  1.3636  -3.3     78.0645 -55.5546  95.9301]]\n",
      "Weights: [[-4.2907 -0.8439 -0.2868  0.1051  0.0901]]\n",
      "MSE loss: 151.4005\n",
      "Iteration: 438300\n",
      "Gradient: [[  -0.8635    6.2267   19.0727   41.924  -547.367 ]]\n",
      "Weights: [[-4.29   -0.8438 -0.2869  0.1051  0.0901]]\n",
      "MSE loss: 151.3903\n",
      "Iteration: 438400\n",
      "Gradient: [[  -0.4804    9.7009   -3.4443  -59.4503 -219.7312]]\n",
      "Weights: [[-4.2907 -0.8439 -0.287   0.1051  0.0901]]\n",
      "MSE loss: 151.3789\n",
      "Iteration: 438500\n",
      "Gradient: [[ -1.7812 -10.7663   5.8386 102.7223 -35.322 ]]\n",
      "Weights: [[-4.291  -0.8438 -0.2871  0.1051  0.0901]]\n",
      "MSE loss: 151.3668\n",
      "Iteration: 438600\n",
      "Gradient: [[   3.3803    1.7141   29.29     46.4558 -119.8467]]\n",
      "Weights: [[-4.2906 -0.8436 -0.2872  0.1051  0.0901]]\n",
      "MSE loss: 151.3527\n",
      "Iteration: 438700\n",
      "Gradient: [[ 1.409000e-01  3.333600e+00  3.932760e+01 -1.557573e+02 -1.369994e+02]]\n",
      "Weights: [[-4.2909 -0.8434 -0.2872  0.1051  0.0901]]\n",
      "MSE loss: 151.3417\n",
      "Iteration: 438800\n",
      "Gradient: [[-11.8237   0.902   31.8363 -33.2991  42.5631]]\n",
      "Weights: [[-4.291  -0.8432 -0.2873  0.1051  0.0901]]\n",
      "MSE loss: 151.3301\n",
      "Iteration: 438900\n",
      "Gradient: [[  -3.4517    8.4477  -10.1558   10.9675 -100.1649]]\n",
      "Weights: [[-4.2915 -0.843  -0.2874  0.1051  0.0901]]\n",
      "MSE loss: 151.3187\n",
      "Iteration: 439000\n",
      "Gradient: [[ 6.4093 -2.4363 27.8725 49.8053  3.0381]]\n",
      "Weights: [[-4.2922 -0.843  -0.2875  0.1051  0.0901]]\n",
      "MSE loss: 151.3065\n",
      "Iteration: 439100\n",
      "Gradient: [[ -9.208  -17.3452   7.7952  74.0881  43.718 ]]\n",
      "Weights: [[-4.291  -0.8427 -0.2876  0.1051  0.0901]]\n",
      "MSE loss: 151.2892\n",
      "Iteration: 439200\n",
      "Gradient: [[  7.2237 -28.6838  29.5357  76.1041  54.1271]]\n",
      "Weights: [[-4.2916 -0.8425 -0.2877  0.1051  0.0901]]\n",
      "MSE loss: 151.2792\n",
      "Iteration: 439300\n",
      "Gradient: [[  -5.1708  -23.9575    2.8848 -186.4198 -134.0941]]\n",
      "Weights: [[-4.2913 -0.8425 -0.2877  0.1051  0.0901]]\n",
      "MSE loss: 151.2677\n",
      "Iteration: 439400\n",
      "Gradient: [[ -7.1012 -12.0946 -14.9993  -7.3948 222.2315]]\n",
      "Weights: [[-4.2923 -0.8423 -0.2878  0.1051  0.0901]]\n",
      "MSE loss: 151.262\n",
      "Iteration: 439500\n",
      "Gradient: [[  0.7523  -3.7436  -3.5627 -52.4203 110.3801]]\n",
      "Weights: [[-4.2914 -0.8421 -0.2878  0.1051  0.0901]]\n",
      "MSE loss: 151.2488\n",
      "Iteration: 439600\n",
      "Gradient: [[  -1.5105   -2.5672  -11.5532   29.6545 -217.4744]]\n",
      "Weights: [[-4.2916 -0.8421 -0.2879  0.1051  0.0902]]\n",
      "MSE loss: 151.236\n",
      "Iteration: 439700\n",
      "Gradient: [[  7.7805  -2.738    9.3797   1.0057 -69.4177]]\n",
      "Weights: [[-4.2919 -0.8421 -0.288   0.1051  0.0902]]\n",
      "MSE loss: 151.2253\n",
      "Iteration: 439800\n",
      "Gradient: [[ 7.210000e-02  1.275770e+01  7.527800e+00  7.197400e+00 -1.710595e+02]]\n",
      "Weights: [[-4.2916 -0.842  -0.2881  0.1051  0.0902]]\n",
      "MSE loss: 151.2177\n",
      "Iteration: 439900\n",
      "Gradient: [[   7.3874    6.6247  -23.9226   84.9912 -138.6113]]\n",
      "Weights: [[-4.2908 -0.842  -0.2882  0.1051  0.0902]]\n",
      "MSE loss: 151.2037\n",
      "Iteration: 440000\n",
      "Gradient: [[  -0.388     5.2695  -50.913    14.5335 -291.9882]]\n",
      "Weights: [[-4.2905 -0.842  -0.2883  0.1051  0.0902]]\n",
      "MSE loss: 151.1941\n",
      "Iteration: 440100\n",
      "Gradient: [[  -1.3284   -8.2113   17.664   -24.3991 -185.6249]]\n",
      "Weights: [[-4.2886 -0.8418 -0.2884  0.1051  0.0902]]\n",
      "MSE loss: 151.1889\n",
      "Iteration: 440200\n",
      "Gradient: [[   3.3126  -19.8518   34.7816  137.2857 -187.2401]]\n",
      "Weights: [[-4.2899 -0.8418 -0.2885  0.105   0.0902]]\n",
      "MSE loss: 151.1734\n",
      "Iteration: 440300\n",
      "Gradient: [[  1.1112  -0.2154  -8.2013 -75.9672 120.2184]]\n",
      "Weights: [[-4.2904 -0.8415 -0.2886  0.105   0.0902]]\n",
      "MSE loss: 151.1572\n",
      "Iteration: 440400\n",
      "Gradient: [[ -4.6164   6.5812  29.6185  13.0043 234.2088]]\n",
      "Weights: [[-4.2898 -0.8414 -0.2887  0.105   0.0902]]\n",
      "MSE loss: 151.1466\n",
      "Iteration: 440500\n",
      "Gradient: [[   5.4457   -3.3902   29.3575 -163.0257 -745.6081]]\n",
      "Weights: [[-4.2912 -0.8413 -0.2887  0.105   0.0902]]\n",
      "MSE loss: 151.1376\n",
      "Iteration: 440600\n",
      "Gradient: [[   0.3735   -0.3698   19.4571  -93.3053 -232.4663]]\n",
      "Weights: [[-4.2902 -0.8412 -0.2888  0.105   0.0902]]\n",
      "MSE loss: 151.1256\n",
      "Iteration: 440700\n",
      "Gradient: [[ 3.4241 24.7515 -8.6931  9.8576 64.5964]]\n",
      "Weights: [[-4.2906 -0.841  -0.2889  0.105   0.0902]]\n",
      "MSE loss: 151.1123\n",
      "Iteration: 440800\n",
      "Gradient: [[ -4.7452   2.1447  23.2729 -28.6597 292.7272]]\n",
      "Weights: [[-4.29   -0.8407 -0.289   0.105   0.0902]]\n",
      "MSE loss: 151.0989\n",
      "Iteration: 440900\n",
      "Gradient: [[  -1.8916    2.047     8.9701 -126.6738 -210.8597]]\n",
      "Weights: [[-4.2908 -0.8406 -0.289   0.105   0.0902]]\n",
      "MSE loss: 151.0895\n",
      "Iteration: 441000\n",
      "Gradient: [[  3.9805  20.835   10.0732  73.0455 -25.4569]]\n",
      "Weights: [[-4.292  -0.8403 -0.2891  0.105   0.0902]]\n",
      "MSE loss: 151.0786\n",
      "Iteration: 441100\n",
      "Gradient: [[  3.5342 -23.063  -21.0542  28.7193   4.029 ]]\n",
      "Weights: [[-4.2928 -0.8402 -0.2892  0.105   0.0902]]\n",
      "MSE loss: 151.07\n",
      "Iteration: 441200\n",
      "Gradient: [[-11.6622   4.1524  45.6852  -0.9669   7.7228]]\n",
      "Weights: [[-4.2924 -0.8398 -0.2892  0.105   0.0902]]\n",
      "MSE loss: 151.0538\n",
      "Iteration: 441300\n",
      "Gradient: [[   1.6542   -2.3146   49.2409   36.7827 -110.0477]]\n",
      "Weights: [[-4.2922 -0.8398 -0.2893  0.105   0.0903]]\n",
      "MSE loss: 151.0449\n",
      "Iteration: 441400\n",
      "Gradient: [[  -2.3358   -0.6296    8.3954   48.6959 -147.7491]]\n",
      "Weights: [[-4.2924 -0.8397 -0.2894  0.105   0.0903]]\n",
      "MSE loss: 151.0337\n",
      "Iteration: 441500\n",
      "Gradient: [[ -4.84   -21.495   11.5913  74.3381 -21.781 ]]\n",
      "Weights: [[-4.2919 -0.8394 -0.2894  0.105   0.0903]]\n",
      "MSE loss: 151.0227\n",
      "Iteration: 441600\n",
      "Gradient: [[ -4.1767 -19.696  -13.7649 -60.7907 100.5528]]\n",
      "Weights: [[-4.2913 -0.8394 -0.2895  0.105   0.0903]]\n",
      "MSE loss: 151.0154\n",
      "Iteration: 441700\n",
      "Gradient: [[  3.2657  15.9425  22.7622 -37.4495  67.8426]]\n",
      "Weights: [[-4.292  -0.8391 -0.2896  0.105   0.0903]]\n",
      "MSE loss: 151.0032\n",
      "Iteration: 441800\n",
      "Gradient: [[  -9.1869  -10.1615   -2.5171 -141.856    33.1264]]\n",
      "Weights: [[-4.2927 -0.8389 -0.2896  0.105   0.0903]]\n",
      "MSE loss: 150.9918\n",
      "Iteration: 441900\n",
      "Gradient: [[ -2.6277   7.6257  24.0594 -18.2298 -16.2438]]\n",
      "Weights: [[-4.2933 -0.8387 -0.2897  0.105   0.0903]]\n",
      "MSE loss: 150.9825\n",
      "Iteration: 442000\n",
      "Gradient: [[  3.5132  13.8586  22.5047  47.5744 157.4779]]\n",
      "Weights: [[-4.2928 -0.8386 -0.2898  0.105   0.0903]]\n",
      "MSE loss: 150.9725\n",
      "Iteration: 442100\n",
      "Gradient: [[  -9.686    -2.812    39.6265  -68.5171 -313.2154]]\n",
      "Weights: [[-4.2926 -0.8386 -0.2898  0.105   0.0903]]\n",
      "MSE loss: 150.9631\n",
      "Iteration: 442200\n",
      "Gradient: [[  3.2834 -12.3179  -4.2111  26.7922 -63.9761]]\n",
      "Weights: [[-4.2927 -0.8385 -0.2899  0.105   0.0903]]\n",
      "MSE loss: 150.9511\n",
      "Iteration: 442300\n",
      "Gradient: [[  8.1004 -11.8953  43.7213  63.5297 282.7972]]\n",
      "Weights: [[-4.2923 -0.8383 -0.29    0.1049  0.0903]]\n",
      "MSE loss: 150.9408\n",
      "Iteration: 442400\n",
      "Gradient: [[  -4.3312   -7.1346  -11.547  -109.7988  214.7042]]\n",
      "Weights: [[-4.2933 -0.8381 -0.2901  0.1049  0.0903]]\n",
      "MSE loss: 150.9273\n",
      "Iteration: 442500\n",
      "Gradient: [[  -1.8145    6.245     1.1775   76.0495 -231.5065]]\n",
      "Weights: [[-4.2928 -0.838  -0.2901  0.1049  0.0903]]\n",
      "MSE loss: 150.9174\n",
      "Iteration: 442600\n",
      "Gradient: [[ -12.3102  -16.7767  -10.6908    5.0537 -305.026 ]]\n",
      "Weights: [[-4.2917 -0.8379 -0.2902  0.1049  0.0903]]\n",
      "MSE loss: 150.9055\n",
      "Iteration: 442700\n",
      "Gradient: [[-3.1229 10.4517 11.8174 46.1446 63.6452]]\n",
      "Weights: [[-4.2929 -0.8376 -0.2903  0.1049  0.0903]]\n",
      "MSE loss: 150.8912\n",
      "Iteration: 442800\n",
      "Gradient: [[ -7.1736   0.1608  11.2861  20.8347 -67.4504]]\n",
      "Weights: [[-4.2928 -0.8375 -0.2904  0.1049  0.0903]]\n",
      "MSE loss: 150.8768\n",
      "Iteration: 442900\n",
      "Gradient: [[  8.9468 -25.7042  -3.4329  32.3581 213.7079]]\n",
      "Weights: [[-4.2926 -0.8375 -0.2905  0.1049  0.0903]]\n",
      "MSE loss: 150.8665\n",
      "Iteration: 443000\n",
      "Gradient: [[  7.4273   9.7028  69.4832 -40.1201 107.1687]]\n",
      "Weights: [[-4.2926 -0.8374 -0.2906  0.1049  0.0904]]\n",
      "MSE loss: 150.8558\n",
      "Iteration: 443100\n",
      "Gradient: [[ -15.5065  -22.3825   -9.1011  -29.0631 -114.1737]]\n",
      "Weights: [[-4.293  -0.8373 -0.2906  0.1049  0.0904]]\n",
      "MSE loss: 150.8493\n",
      "Iteration: 443200\n",
      "Gradient: [[ -15.0661   -1.0699    1.5005  -67.4525 -202.7862]]\n",
      "Weights: [[-4.2925 -0.8374 -0.2907  0.1049  0.0904]]\n",
      "MSE loss: 150.8425\n",
      "Iteration: 443300\n",
      "Gradient: [[   0.5713   10.5739    2.7317  -10.524  -302.8901]]\n",
      "Weights: [[-4.2921 -0.8374 -0.2908  0.1049  0.0904]]\n",
      "MSE loss: 150.8307\n",
      "Iteration: 443400\n",
      "Gradient: [[  3.6417  -6.4921  34.5041   6.8714 -54.3021]]\n",
      "Weights: [[-4.2921 -0.8375 -0.2909  0.1049  0.0904]]\n",
      "MSE loss: 150.8212\n",
      "Iteration: 443500\n",
      "Gradient: [[ -14.623     3.9238  -47.9365   77.1533 -179.2161]]\n",
      "Weights: [[-4.2922 -0.8373 -0.2909  0.1049  0.0904]]\n",
      "MSE loss: 150.8106\n",
      "Iteration: 443600\n",
      "Gradient: [[  8.3216 -18.3607  37.9224  -4.8599  -9.8676]]\n",
      "Weights: [[-4.2924 -0.837  -0.291   0.1049  0.0904]]\n",
      "MSE loss: 150.7998\n",
      "Iteration: 443700\n",
      "Gradient: [[  6.8463   1.2065  68.9563 -40.1689 -52.6172]]\n",
      "Weights: [[-4.2924 -0.8369 -0.291   0.1049  0.0904]]\n",
      "MSE loss: 150.7929\n",
      "Iteration: 443800\n",
      "Gradient: [[-11.5594  -8.4395  -9.1081  48.081   85.5419]]\n",
      "Weights: [[-4.2928 -0.8366 -0.2911  0.1049  0.0904]]\n",
      "MSE loss: 150.7758\n",
      "Iteration: 443900\n",
      "Gradient: [[ 5.0606 -4.9571 16.808  55.2322 -1.6282]]\n",
      "Weights: [[-4.293  -0.8365 -0.2912  0.1049  0.0904]]\n",
      "MSE loss: 150.7688\n",
      "Iteration: 444000\n",
      "Gradient: [[  -6.6996    1.8198   -9.911     1.9143 -242.5634]]\n",
      "Weights: [[-4.2927 -0.8363 -0.2913  0.1049  0.0904]]\n",
      "MSE loss: 150.7538\n",
      "Iteration: 444100\n",
      "Gradient: [[   9.232     0.409    27.53     81.9811 -266.4556]]\n",
      "Weights: [[-4.2922 -0.8361 -0.2913  0.1049  0.0904]]\n",
      "MSE loss: 150.7446\n",
      "Iteration: 444200\n",
      "Gradient: [[  -9.9866   -8.3496  -11.6227 -147.9904  106.2259]]\n",
      "Weights: [[-4.2935 -0.8358 -0.2914  0.1048  0.0904]]\n",
      "MSE loss: 150.7319\n",
      "Iteration: 444300\n",
      "Gradient: [[ 2.639000e-01 -4.232500e+00 -1.234610e+01 -1.146580e+02 -2.882918e+02]]\n",
      "Weights: [[-4.2948 -0.8355 -0.2915  0.1048  0.0904]]\n",
      "MSE loss: 150.7234\n",
      "Iteration: 444400\n",
      "Gradient: [[-11.6753 -21.3012   8.3296  -9.3206 110.1459]]\n",
      "Weights: [[-4.2944 -0.8353 -0.2915  0.1048  0.0904]]\n",
      "MSE loss: 150.7092\n",
      "Iteration: 444500\n",
      "Gradient: [[ -1.98     0.5397  21.874  -31.8669 101.8591]]\n",
      "Weights: [[-4.294  -0.8351 -0.2916  0.1048  0.0904]]\n",
      "MSE loss: 150.6957\n",
      "Iteration: 444600\n",
      "Gradient: [[ -9.5951   7.4413  -6.1187 -50.1679 -86.8561]]\n",
      "Weights: [[-4.2934 -0.835  -0.2917  0.1048  0.0904]]\n",
      "MSE loss: 150.6837\n",
      "Iteration: 444700\n",
      "Gradient: [[-10.4057 -10.436   -9.8603 -61.7295  81.0956]]\n",
      "Weights: [[-4.2929 -0.835  -0.2918  0.1048  0.0905]]\n",
      "MSE loss: 150.6739\n",
      "Iteration: 444800\n",
      "Gradient: [[ 12.3836  -4.1952 -18.6549  -5.5408  16.5109]]\n",
      "Weights: [[-4.2926 -0.8351 -0.2919  0.1048  0.0905]]\n",
      "MSE loss: 150.6652\n",
      "Iteration: 444900\n",
      "Gradient: [[-13.4834  -5.4894  24.9041 -36.4492 -10.285 ]]\n",
      "Weights: [[-4.2921 -0.835  -0.292   0.1048  0.0905]]\n",
      "MSE loss: 150.6538\n",
      "Iteration: 445000\n",
      "Gradient: [[ -1.2133   4.7925 -50.4668  74.2357  37.3549]]\n",
      "Weights: [[-4.2924 -0.835  -0.2921  0.1048  0.0905]]\n",
      "MSE loss: 150.6454\n",
      "Iteration: 445100\n",
      "Gradient: [[   4.4769   -9.0879   -7.6709  -27.8688 -274.7699]]\n",
      "Weights: [[-4.2929 -0.8348 -0.2921  0.1048  0.0905]]\n",
      "MSE loss: 150.6345\n",
      "Iteration: 445200\n",
      "Gradient: [[ 22.8204  15.7494  20.5571  34.4031 205.3824]]\n",
      "Weights: [[-4.2931 -0.8343 -0.2922  0.1048  0.0905]]\n",
      "MSE loss: 150.6198\n",
      "Iteration: 445300\n",
      "Gradient: [[ -13.7507   -8.4222    4.6838   61.6468 -273.1471]]\n",
      "Weights: [[-4.2933 -0.834  -0.2923  0.1048  0.0905]]\n",
      "MSE loss: 150.6059\n",
      "Iteration: 445400\n",
      "Gradient: [[  1.7355   5.1218  48.3873  -4.7601 -19.9091]]\n",
      "Weights: [[-4.2934 -0.8337 -0.2924  0.1047  0.0905]]\n",
      "MSE loss: 150.5922\n",
      "Iteration: 445500\n",
      "Gradient: [[ -15.7899   -5.0687  -41.8378 -134.1995  122.3323]]\n",
      "Weights: [[-4.2922 -0.8338 -0.2924  0.1047  0.0905]]\n",
      "MSE loss: 150.5873\n",
      "Iteration: 445600\n",
      "Gradient: [[  10.9319  -12.7748   19.1314 -123.312  -360.4418]]\n",
      "Weights: [[-4.2925 -0.8339 -0.2925  0.1047  0.0905]]\n",
      "MSE loss: 150.5751\n",
      "Iteration: 445700\n",
      "Gradient: [[-4.191900e+00  2.524900e+00  2.480000e-02  3.312680e+01 -1.059895e+02]]\n",
      "Weights: [[-4.2932 -0.8336 -0.2926  0.1047  0.0905]]\n",
      "MSE loss: 150.5578\n",
      "Iteration: 445800\n",
      "Gradient: [[ -2.6205 -14.4267  43.8424  85.5825  -7.4674]]\n",
      "Weights: [[-4.2931 -0.8334 -0.2928  0.1047  0.0905]]\n",
      "MSE loss: 150.544\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      2\u001b[39m learning_rate = [\u001b[32m1e-5\u001b[39m, \u001b[32m1e-6\u001b[39m, \u001b[32m1e-7\u001b[39m, \u001b[32m1e-8\u001b[39m, \u001b[32m1e-9\u001b[39m]\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#learning_rate = [1e-6, 1e-6, 1e-6, 1e-6, 1e-6]\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Включен Momentum.\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Обучение на батчах.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m weights_1, losses_1 = \u001b[43mmodel_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mliveplot\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mmodel_fit\u001b[39m\u001b[34m(X_train, y_train, learning_rate, tolerance, beta, batch_ratio, liveplot)\u001b[39m\n\u001b[32m     50\u001b[39m w_coeff = w_coeff_buff.copy()\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Вычисление ошибки на полном датасете, при обновленных коэффициентах.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m loss = \u001b[43mloss_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_coeff\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# Накопление ошибки в отдельный массив для дальнейшей визуализации.\u001b[39;00m\n\u001b[32m     54\u001b[39m losses.append(loss)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mloss_func\u001b[39m\u001b[34m(X, y, w_coeff)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mloss_func\u001b[39m(X, y, w_coeff):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.sum((y - \u001b[43mf_poly\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_coeff\u001b[49m\u001b[43m)\u001b[49m)**\u001b[32m2\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mf_poly\u001b[39m\u001b[34m(X, w_coeff)\u001b[39m\n\u001b[32m      2\u001b[39m Y = np.zeros_like(X)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m n, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(X):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m idx, w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(w_coeff.tolist()[\u001b[32m0\u001b[39m]):\n\u001b[32m      5\u001b[39m         Y[n] = Y[n] + w * x**idx\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Y\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Массив с коэффициентами при шагах обучения.\n",
    "learning_rate = [1e-5, 1e-6, 1e-7, 1e-8, 1e-9]\n",
    "#learning_rate = [1e-6, 1e-6, 1e-6, 1e-6, 1e-6]\n",
    "\n",
    "# Включен Momentum.\n",
    "# Обучение на батчах.\n",
    "weights_1, losses_1 = model_fit(X_train, y_train, learning_rate, tolerance=1e-2, beta=0.9, batch_ratio=0.1, liveplot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ccc912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Отключен Momentum.\n",
    "# Обучение на батчах.\n",
    "weights_2, losses_2 = model_fit(X_train, y_train, learning_rate, tolerance=1e-2, beta=0, batch_ratio=0.1, liveplot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baf34b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Отключен Momentum.\n",
    "# Обучение на полном датасете.\n",
    "weights_3, losses_3 = model_fit(X_train, y_train, learning_rate, tolerance=1e-2, beta=0, batch_ratio=1.0, liveplot=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

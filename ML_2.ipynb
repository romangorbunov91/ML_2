{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "453d2819",
   "metadata": {},
   "source": [
    "# Градиентный спуск\n",
    "## Задача поиска оптимальных коэффициентов полиномиальной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3d6c553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a770ef8",
   "metadata": {},
   "source": [
    "Формируем синтетический датасет на основе полиномиальной функции с добавлением случайного шума ${\\epsilon}$:\n",
    "\n",
    "${f(x) = \\sum_{k=0}^{K-1}{w_k \\cdot x^k} + \\epsilon}$,\n",
    "\n",
    "где ${w}$ - массив весов размера ${K}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05952c8",
   "metadata": {},
   "source": [
    "Определим полиномиальную функцию, в частном и в общем виде:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee933d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_trend(x):\n",
    "    return x**3 - 3*x**2 + 2*x - 5\n",
    "\n",
    "def f_poly(X, w_coeff):\n",
    "    Y = np.zeros_like(X)\n",
    "    for n, x in enumerate(X):\n",
    "        for idx, w in enumerate(w_coeff.tolist()[0]):\n",
    "            Y[n] = Y[n] + w * x**idx\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaf5f32",
   "metadata": {},
   "source": [
    "Сгенерируем синтетический датасет и построим графики."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d9e5cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiIAAAGdCAYAAAAvwBgXAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAaflJREFUeJzt3Qd4VGXWB/B/eq8ESICEJIQWIITepIMoWBArVuyNteuCZRVdRfezi7q6rmXtDbDRBUF6CT2UEEIIkBBIJb3N95wLE5Mw5c5k+vx/zzPMZOa2DJO5577vec/rodFoNCAiIiKyA0977JSIiIhIMBAhIiIiu2EgQkRERHbDQISIiIjshoEIERER2Q0DESIiIrIbBiJERERkNwxEiIiIyG684cAaGhpw4sQJhISEwMPDw96HQ0RERCpIrdQzZ86gQ4cO8PT0dN5ARIKQ2NhYex8GERERmSEnJwedOnVy3kBEWkK0v0hoaKi9D4eIiIhUKC0tVRoStOdxpw1EtN0xEoQwECEiInIuatIqmKxKREREdsNAhIiIiOyGgQgRERHZjUPniKgdIlRXV4f6+np7Hwo5KC8vL3h7e3MIOBGRA3LqQKSmpga5ubmoqKiw96GQgwsMDERMTAx8fX3tfShEROQKgYgUO8vKylKudqVgipxgeMVLulrMJGA9deqU8nnp2rWr0eI6RERkO04biMjJRYIRGacsV7tE+gQEBMDHxwfZ2dnK58bf39/eh0REROc4/aUhr25JDX5OiIgck9O2iBAREZH56hs02JxViPwzVWgX4o/BCZHw8rR9igMDESIiIjezaFcunv5pDwrLaxqfiw71x3OXJeOi3jE2PRa2V7uA+Ph4vPnmm06zXSIisp+5i9Jx31dpzYIQkVdahXu+SMOSPbk2PR4GInYwZswYPPTQQxbb3pYtW3DXXXfB3j799FOEh4fbfL8zZszA1KlTbb5fIiJn8+uO4/hgTZbBZWbP361029gKA5Fz/WQbMgvw047jyr0t/wOMFWpTo23bthw5RERERrtjZn6zw/BCAIoqarHxcAFsxe0DEWmCuuCVlZj+n4148Jsdyr38bK2mKbl6X716Nd566y2l7oncjhw5gj/++EN5vHjxYgwYMAB+fn5Yu3YtMjMzcfnll6N9+/YIDg7GoEGDsGLFCoNdKLKdjz76CFdccYUSoEjtjJ9//tngceXn5+PSSy9VhromJCTgyy+/PG+Z119/HX369EFQUJAybPq+++5DWVmZ8poc/6233oqSkpLG3+u5555TXvv8888xcOBAZTro6OhoXH/99cr+tIqKinDDDTcoAZXsX473k08+aXw9JycH11xzjdLaEhkZqbwf8p4J2cdnn32Gn376qXG/cixERPQXOadJd4xaclFuK57u/h9z7xdpyC2pavZ8XkmV8rw1ghEJQIYNG4Y777xTqQorNzmpa82aNQsvv/wy9u3bh5SUFOVEP3nyZPz+++/Yvn07LrroIiVgOHr0qMH9zJkzRzl579q1S1lfTvSFhYUGAyQ54a9atQo//PAD3nvvvWbBgnYI7Ntvv429e/cqJ/+VK1fiiSeeUF4bPny4EgyFhoY2/l6PPfaY8lptbS1eeOEF7Ny5EwsXLlSCCNmf1jPPPIP09HQlCJPf+/3330dUVFTjupMmTVKCmD///BPr1q1TAjJ5H6QmiOxDfk/5WbtfORYiIjpLWvmf+zkdprFdz4C3O//HzPklXedbLc/JACZ5fWJytEWHM4WFhSlVYKWlQloHWnr++ecxceLExp+lBaBv376NP8sJfcGCBUoLx8yZM/XuR07006dPVx6/9NJLSgCxefNm5YTd0sGDB5UgQF6XFhfx3//+Fz179my2XNO8FmmF+ec//4l77rlHCVrkd5LfTVokWv5et912W+PjxMRE5VhkPxJkSVAhQVW/fv2UVhPttrW+/fZbpXCdtPBoK+dKa4m0jkjLx4UXXqi0olRXV+t8P4mI3N3mrEIlEdUUwxLPXgw6fYvImjVrlKt3KcEuJxG5Gnak/5iWLSEtgxF5XZazJe3JWEtO1nLVL0GBnHzlxC2tBsZaRKQ1RUu6UqSlomULh5ZsTyaFky4hrR49epyXeCpdQuPHj0fHjh2VFoqbbroJBQUFRuf62bZtm/I5iIuLU9YbPXq08rz2d7j33nvxzTffIDU1VWlhWb9+feO60opy6NAhZT353eUmwVlVVZXSbUVERIZJnRBThAd4Y2iXNnCJQKS8vFy5mn/33XfhrP8xpv4HtpYEDU1JECItINKqIV0TO3bsUPI0pFvCEClp3pQEgtKyYC7pTrnkkkuUAOfHH39Uggvt/6uhY5HPgHStSCAkeScywkd+n6brXXzxxUr59YcffhgnTpxQgh1tt44EYhIgye/d9CatOJJrQkREhgdiZJw8A1O8fGWKTQubWbVrRk4wcnNEUkXOksuZQrox6uvrVS0rORHSzSKJp9oTszZR01Kk9UNG6Ehwoe2aOXDgAIqLixuXkdckkHnttdcay6V/9913Rn+v/fv3K60mkveizYXZunXreccgiaq33HKLchs5ciQef/xxvPrqq+jfv7/SPdOuXTslmGnt+0lE5A6W7MlV0gsMtfzrMnNsF/cuaCb9/KWlpc1u1iKlbGPC/JVcEF3keXldlrM0yYHYtGmTElCcPn3aYEuFjCCZP3++0gog3RTSCtCalg1dunfvruSO3H333cpxSdBxxx13KLkXWklJSUri6DvvvIPDhw8rI2H+/e9/n/d7SaAkibXye0mXjXTHSKCgXU9yWyTPpal//OMfyqgX6YKRRNhff/21MT9FkmwlcVVGykiLkMygK7khDzzwAI4dO9a4X0nKleBJ9ivHSUTkrpboGYjRUtuyQsxe9TGGHt0Fe3KoQGTu3LlKwqP21nQ0iaVJs9OzlyYrj1sGI9qf5XVrNE9Jt4OXlxeSk5OVlgBD+R4yZDYiIkIZCSJ5FtLNIa0EliYJoJLLI/kb06ZNUwqkSSuElnSxybG88sor6N27t9LNIv9fTckxSvLqtddeq/xe//rXv5R7KXT2/fffK7+vtIxIS0dTEqjMnj1b6fYZNWqU8t5IzoiQpF7JNZKARo5LApTbb79dyRHRtpDICCQJpiS/RvYnrUhERO6o3sBAjJYG7VuD5M3zccOKD5s8a/u5Zjw0UjnLFjvy8FByAwxVwJQWEblpSYuIBCNSm6Jls7yciOTqWGpetGZad13NV9ISIkGIrZunyHos9XkhInJkGzILlHpYary34CVMPrgerw27Fu+Mukl57oFxXfDIhT1afRxy/pYGBV3nb4cevitFvORmSxJsyBBdR5iBkIiIqDXUDrDwrq/DBUfOVlldkzS48fn/rs3CgxO6u06yqrOQN3yYDYcqERERWUM7lQMsemVugaamAqcDw7Arpmvj8+U1DZi3MgMPTugGl8gRkcRF7XBLIU3j8thYDQwiIiIyfyCGMZUbvoNcfj8QHAmNR/NQ4JN1R1xn0jsZpikVM+UmHnnkEeWxjJIgIiIi6wzEMNaxknXqCKToQUFs7/NeK66stWkxT29rT3dvo1xYIiIiwtncx/dv7I9Hv9uJ8przayxFHd2NbfW18AJwbNBUuxfzdKjhu0RERNR6MgjDx0t3u0jbtF+V+xS/QJSHtbdZMU99GIgQERG5mHkrD6G4sk7nawVHdyv37TueraXVkrWKeerDQISIiMiF1Ddo8Mm6LJ2v+ZcVYmfl2arlRf0m61zGWsU89WEg4qaee+45ZbZbfaSMuhSh0843I9VRW87GS0REjmdzVqGScKpL3LZfINONxnl6ITex+WzvEnq8d30/955rxl1IEu9DDz0EZyJl22XGWyIicmyGEk2nleTjfQBjO/WCx7kJTLXent4Pk1M6wNZY0MxByWgjmVHW29sx/otkArymk+AREZFjaqcv0VSjwRU5uyHpqeuGXtXspTtHJuDSvrYPQgRbRGxsxowZWL16Nd566y2l60NuMguvtitk8eLFGDBggFLqfu3atcpMuzK5nMyRIoGATD73ww8/NG5Pu57MeCuTvskkcTL5nMxE25RMNte+fXuEhIQ0ThpnipZdM9quHZmFV2a/lTkFrrvuOpw5c6ZxGWPHTkREljegcwT8vc8/vfc6mYn2ZYUo9/HHptg+zV4b10P36BlbcIzLbUuRmiUVFfbZd2CgzOxndDEJQKSLQ2awff7555XnZMZYCUbErFmzlNlpExMTlVl35UT+xRdf4N///je6du2qzER74403KuvITLlaTz31FF577TXleZkB97bbbmuchfa7775TAod3330XF1xwgRI8vP3228o+WiMzMxMLFy7Er7/+iqKiIlxzzTVKwPPiiy8qr6s9diIisgyZyPW5n9NRVddw3msRG7/HewDCOyWjxtvHbnVDXDsQkSAkONg++y4rA4KCjC4mLQcy7b20XERHR5/3ugQnEydOVB7LTMQvvfQSVqxYgWHDhinPSfAgLSUffPBBs5O5nPy1P0swM2XKFKXVQ2aaffPNN5VWELmJf/7zn8o2TW0VaUlaPKSlRFpZxE033aS0zMixmHLsRERkmSDk3i/SoK+M6M7D2/AlgEmB4XatG+LagYgLkO4VrUOHDqGioqIxMNGqqalpLJuvlZKS0vg4JuZsxnN+fj7i4uKwb98+pZWkKQkOVq1a1apjlS4ZbRCi3a/s09RjJyKi1g/ZnfNLut4gJOzUEeyqPXvxmTvo8mavhQd427RuiGsHItI9Ii0T9tq3BQQ1aVWRSQPFb7/9ho4dOzZbTnJImvLx+auZTXJGtC0W1tR0n9r9avdpyrETEVHrh+zmluhv5Y7Z+hN2AUj29kNJ+y7NXpPCZ8vT82w+bNc1AxE5AavoHrE36ZqRETHGJCcnKydtma24NV0ZPXv2xKZNm3DzzTc3Prdx40ZYk6WOnYiIjMsrqTT4+pnDacp9bHRX7NfxurSmSFl4WxYyc81AxElIl4YEBpKgGhwcjMhI3U1i0u3x2GOP4eGHH1ZaGiTRtKSkRElCDQ0NxS233KJqfw8++KAyWke6fUaMGIEvv/wSe/fubXWyqiGWOnYiIjIu/0y13te8ayqwu6xAeVyRMkHnMtKaIq0qw7q0ga0xELEDOUHLiVhaDSorK5GVpbsUr3jhhReUUSYyAuXw4cPKENr+/fvjySefNKkYmYxweeKJJ5QE1SuvvBL33nsvli5daqHfyHrHTkRExpNU3/5df8HJuO2LkSkjND08cLTXGL3L2WvkjIdGKmc5qNLSUmWUiVxJy1V0U3JClRO41KiQkSFEhvDzQkTuOFJGjPh6Nr47uhsXRMQg667/QJ+v7xxqsRYRQ+fvlljQjIiIyAVHyig0GvzzzGmcBtBp8JXQJzzAx24jZ9g1Q0RE5IIjZURSQQ4SinJR7eWDbcn6Bw5c0DXKLomqgi0iRERETihfRU7HuANnK2yv69wXFb765wv7bVeu0s1jDwxEiIiInFA7FdVQv930I4YA+KK98VGS0s0j3T22xkCEiIjICUlOR3hg88KSTYWfzMLu2ipsBpDWbbjBbWmaDOG1NQYiRERETqpWx+R2WtGb5ysBRm8ffxRHJ6nanj2G8DIQISIickLzVmagvEZ/le7iI2erqXbs2FP1Nu0x+R1HzRARETmZ+gYNPll3RO/r/uVF2FFRojwu7TfZ6PZkvEx0mL9dhvCyRYSIiMjJbM4qRHFlrd7X4zYvhBR9j/P0wvEkSVfVTzto99lLk+0yhJeBiJP69NNPlZLprZ3z5s0332w2e+7ChQstcHRERGRN+UZyOWoOrlfuu7WNh4en4VO9tIS8f2N/zr5L9pebm4uIiAh7HwYREbUil8OroR63VRSjLYDCPronuZs5tgu6tg9RtiPdMfYqZiYYiFCj6Ohoex8CERGpIMFDTJg/8kqqzivxPuD4PtxcU4lL/UMwUE9+yIiktnaZaVcXds3YwZgxYzBz5kzlJpMCRUVF4ZlnnkHT+QeLiopw8803Ky0UgYGBuPjii5GRkaFze0eOHIGnpye2bt3a7HnpduncuTMaGvQP72qqadeMbFN+nj9/PsaOHascQ9++fbFhw4Zm66xduxYjR45EQEAAYmNj8cADD6C8vNyMd4WIiNSSFgzJ6dBVfmxixkblfmWXgaj39NK5flF5DRyFSwYiciLUd5NZWNUuW1lZqWpZc3z22Wfw9vbG5s2b8dZbb+H111/HRx991Pj6jBkzlMDi559/Vk7+EqRMnjwZtbW1OnM9JkyYgE8++aTZ8/KzbEeCFHM99dRTeOyxx7Bjxw5069YN06dPR11dnfJaZmYmLrroIlx55ZXYtWsXvv32WyUwkQCLiIis66LeMbhtRHyz5zQNDcjdsxL7ACwzkKT6wm/2qaLqNoFIcHCw3pucNJtq166d3mWlFaLlCV/XcuaQ1oM33ngD3bt3xw033IC//e1vys9CWj4kAJHARFobpCXiyy+/xPHjx/Umk95xxx34+uuvUV0tedJAWloadu/ejVtvvRWtIUHIlClTlCBkzpw5yM7OxqFDh5TX5s6dqxz7Qw89hK5du2L48OF4++238b///e+8gI+IiCyjvkGDDZkF+GnHcZRWnr0w1IrJ3IJnK0vRD8Dq2F56t2GvKqpuE4g4g6FDhypdH1rDhg1TApD6+nrs27dPaS0ZMuSvaLZNmzZK0CKv6TJ16lR4eXlhwYIFjaNqpEtFgqfWSElJaXwcE3M2ozo/P1+537lzp7KfpkHZpEmTlK6grKysVu2XiIjOJxPTXfDKSkz/z0Y8+M0O/JB2rNnrEWm/KvepAaGoCopwuCqqbpOsWlZWpvc1OVk3pT2p6tKyS0PyJhyVr6+vklMi3THTpk3DV199pXT5tJaPz1/zGGgDJ23OibzPd999t5IX0lJcXFyr901ERM2DkHu/SNOZF6J17PjZi9WIhH7Ig+NVUXWbQCQoKMjuyxqzadOmZj9v3LhR6d6QQKlnz55KHoYsI90doqCgAAcOHEBycrLebUr3TO/evfHee+8p60tAYk39+/dHeno6kpLUzWFARETmd8fM+SXdYBASdvIwdtWebeXIHXSFwe1FBvnapYqqLuyasZOjR4/ikUceUYILye1455138OCDDyqvSUBy+eWX484771SSP6UL5MYbb0THjh2V5/WRAEa6fP7+978rSaUyksWaZD/r169XklMlmVW6ln766ScmqxIRWdjmrEIlr8OQDpvnK/dqJrkbkhBh19ohTTEQsRPpRpFROYMHD8b999+vBCF33XVX4+vSxTJgwABccsklSv6IjJpZtGhRs64SXW6//XbU1NTgtttus/rvIPkjq1evxsGDB5Wk2n79+uEf//gHOnToYPV9ExG5EzX5HIVZ5ya569DD6LJd2obAUbhk14wzkIBC6ny8//77Ol+X+iEy+kQfGZYrt5ZkZE2fPn0waNAgo8fQMuelaR0TSXJt+rOQkvItn5P9LFu2zOi+iIjIfO2M5HMElRfjUGWp8rh4kP6Wcy1HKWZmsxaRd999Vzmx+fv7KyNBpHYGWZYkju7Zswfz5s1ThgITEZHrVVLVZ3z2Dsj4mY9D2iKvi+EL0SA/LwxNdKNARIpcSS7Es88+q9S2kJoYMsTT0GgVMp3kZUhXjlRttUW3DBER2b6Sqoee1y8+sB6SFVjWe5zRbZVX12N5urExNbbjoWnZ1m5h0gIizfdypa4d+inFvOSqfdasWQbXLS0tVUqgl5SUIDQ0tNlrUjBLalUkJCQoLS1EhvDzQkSu4M7/bcHy9OYX8n41ldjxzo0IqKvGlFvexF4jiaoe52bcXfv3cVZLWDV0/rZpi4gkTW7btk0pP964Q09P5eeWc5YQERGR/uG793+57bwgRMSv/wb966rxnH8I9rbvAmM0DlZZ1arJqqdPn1YqhbZv377Z8/Lz/v37z1teypNrS5RrIyoiIiJ3L2T29x92oaSqeTl3rar9ayFn1PVh7aTypOrtOkplVYcavitzl0hTjvYmXTjGWLlniVwEPydE5KxByD1fpOkNQrxrKrCz5KTyuCZlkknbdpTKqlYNRGR6e6kUevLk2TdJS36Ojo4+b/nZs2cr/UnaW05Ojt5ta+tpVFRUWOHIydVoPyfG6rAQETlSd8xzP+81uEz81l9QIkGFhweOpF6oarvSZiIjcBylsqq3tec/kZEcv//+uzIpmzZZVX7WVX3Tz89PuakhAY7UtdCOvgkMDGw2iRyRtiVEghD5nMjnpeVcQ0REjmpzViHySv9KV9BFs3eVct87shMyPdWf0mUEjqNUVrV6QTMZunvLLbdg4MCBShVRKeJVXl7e6unphbZVhUOByRgJQnS1whEROaoVRobYetTVYHfh8bM/9DI+bFeEB/jg5Sv74KLeZ2dTd4tA5Nprr8WpU6eU0t95eXlITU3FkiVLzktgNYe0gMjU9O3atUNtba1Fjpdcj3THsCWEiJwtN+S/6wzP+B6/cwn+gAbhUil7wKWqtvvuDf0xIikKjsQmJd6lG8aaE6HJSYYnGiIicqWZdo0Ze+IgEiXvsk0s9vgaTzyVvBBHqqjqkKNmiIiI3N1mFTPtQqPB7Tl78V9JUxh9/rxjjp4X0hQDESIiIgeSr6K+R0peBjqeOYVyH3/8GZ9qdPmHJ3RzqLyQpjj7LhERkQNRU98jbvN8bAGQlzAA1T6GR5u2D/HFzHGGy77bE1tEiIiInGimXU1DAxYd3IjBAN4LMZ7zMefy3g7ZJaPFQISIiMgBZ9rVp2PGBmQ11MFXRssMngZDHhyf5LBdMloMRIiIiBzMRb1jcHFv3bWPQrb8pNwPCIpAZaj+objhgT54YHw3ODoGIkRERA4or6RCZ7fModyDyuOQrsMMrn/r8ASH7pLRYiBCRETkYF78bS+255w/A32ng+txuKEOkp56dNg1BrcRHxUIZ8BAhIiIyIHU1DXgoz91V1UN3vqzqm4ZR5pd1xgGIkRERA7ks/VHoNH1gkaDwpOHlIfB3YYb3EZ0qJ/DzK5rDAMRIiIiB5pj5o3lZ3NAWup5Kgsb62qwxNML2Ua6ZaYPjnOK/BDBgmZEREQOEoTc+0Wa7tYQAFP2r4XMqqZJGowqI/VD4qOC4CzYIkJEROQgE91p9Lwuo2Uu3rdGefxb9wuMbs9Z8kMEW0SIiIgcfKK72P1rMao4D9M9PPF7ktRU1U+qsjpLfohgiwgREZGdLU/PM/h60LafcQLAhqBwVPgGOOUsu/owECEiIrJzt8zCHRJm6O+WOZCXoTwO7j7C4LZuGxHv8CXdW2IgQkREZOdumcLyGr2vx+5bg+yGekjWR/bQqw1ua2Ky7rLwjoyBCBERkR3ln9GfGyKC0n5R7gcER6IqWH/uh/TGDOgcAWfDQISIiMhOXTIbMgtw8OQZg90y+/POFjELMjJapkEDbMsugrPhqBkiIiI71Ax57ud05JUabg2JS/8DaxvqIempR4wUMVPTuuKIGIgQERHZOAi554s0VctOztmNgQAOhcdgd1C4S9UP0WLXDBERkQ27Y2bN361qWQ9NA2YcTsMbUhtk7G1Glw8P8HGq+iFaDESIiIhsZOPhAhRX1KpadnDOXsSUFaDULwh/JEq7iGG3joh3qvohWuyaISIishFJTlWrzbqv8bvkfXQdhhpvH4PL+nl7Yua4rnBGbBEhIiKykcxT+kfINOVdU4Fvju7CBAAfRcXCmC5tg5yyNUQwECEiIrJRkuriPSdVLRu/eQFkIG5bDw8cHnCp0eVPFFcp+SfOiIEIERGRjWbXVat2j3TKAH3bxkPj7Wt0+eLKWqVCqzNiIEJERGTn2XWb8q0owbaSfOVxTb9LoJYz1hARDESIiIiszJQgIWH9NyiXOWY8vHAkZaLq9ZyxhohgIEJERGRlpgQJZ/avVe57xnSFh6e603Sov7dT1hARDESIiIisTIKEmDB/GBvXElJRgpLys/PFnBkyTfX2X5rah6NmiIiISDcJEp69NNnochdnbMIeAAvCY3Ci23BV256Y3A6XpHaAs2IgQkREZAMX9Y7BXaMSDC5z2b7VSqvJQZW5IXeOTMB/bh4EZ8bKqkRERDYawvvzzly9r0cW5WJA9k7l8S89RxnNCdn05AQE+HrB2TEQISIicoAhvO1X/RcxAG4JiUJOeLTOZbRZIP+6KsUlghDBQISIiMjKLSEShCzeo781RGRnpaFEumU6dIc+EUG+eOmK3ko3j6tgIEJERGTFsu5SUdVYMbOYgxuwsa5GOSkfG3mT3uXq6xvgaqyWrPriiy9i+PDhCAwMRHh4uLV2Q0RE5LBByL1fpKmqqBq68QflflBQOM606aR3uZKqOmWbsm1XYbVApKamBldffTXuvfdea+2CiIjIoeeWUTUNXUMddudlKA+Dk8eo2r5s21knubNZ18ycOXOU+08//dRauyAiInL6uWUSt/6MVZoGhAHIGn6d0eUl/JBtyz6GdWkDZ+dQOSLV1dXKTau0tNSux0NERGTtuWU02xcr9wMiOyHTP9gq+3BkDlXQbO7cuQgLC2u8xcbG2vuQiIiITBYZ6KtqOf/aKrxRXohnJCAZPNWkfTjrJHetCkRmzZoFDw8Pg7f9+/ebfTCzZ89GSUlJ4y0nJ8fsbREREdmDJJI++O0OVctemLERqbXVuCOsPQ6nTFK1jtQSkXlrnHWSu1Z1zTz66KOYMWOGwWUSExPNPhg/Pz/lRkRE5MwjZdSmkU7bs0q5X9BrHOBhfNI67RIyb42zTnLXqkCkbdu2yo2IiIhaMVIGQPjJw5iXtQ2SDTk/ebTOZSICfVBUUdv4c3SYvxKEsKCZCkePHkVhYaFyX19fjx07zjZTJSUlIThYfTIOERGRq42UEdF/foEfpZKqjz9KddQOCfDxVOaT2ZZdpCSmSk6IdMe4SkuI1QORf/zjH/jss88af+7Xr59yv2rVKowZo26cNBERkbMwdRRLdvbZC/SOSYOVVpGWBsVHwtfb0yWG6Npl1IzUD9FoNOfdGIQQEZErMmUUS4cD67C/rgY+AHIuuEHnMlNTO8IdONTwXSIiImcl3SYymkVNx0lwY0n3CJRF6g44YsID4A4YiBAREVmA5G5IIqmxZFWPuhrsPnlIeRzUe5zOZUL9vFxmeK4xDESIiIgsZGJyNAJ9vQwu02Xjj8jTaBAB4PDQa3Uuc2GvaJdLSnWKEu9ERETOPnKmoqbe4DIjs3egAECbtgk45B+oc5kRXd2nVAZbRIiIiCxkeXqewdfDK0sxK/cAtktXzkUz9S63LuOUy8yuawwDESIiIgtVVf143RGDy1yevhp+9XXY074LDnTorne5H9KOY8TLvyvbdHUMRIiIiFpJWi9mzd9tdLmwzQuUmiHf95lgdNm80mqlXLyrByMMRIiIiFpp3spDKG5Sil2X2PTVeLQ0H/Eyt0y3Yaq3PeeXdJfupmEgQkRE1AoSJHyyLsvocv6bztYOSQ6JQmlIlKptawClbLwkwboqBiJEREStIEFCcaXh1hDvqgpsyz8brPinXmT18vHOhIEIERGRmS0hGzILsFhFDkeX9V+hSKqlengic/A0q5aPdzasI0JERGQiSSCV3A21s+0W71mp3Pfp0B0HvH1V78dDZukNOzvrrqtiIEJERGRiECKjWdSmj0bkZmBr5dn5dYsuuF71fjzO3UvZeFeussquGSIiIhO6Y6QlxJQxLD03z1eW7+sbiPz4fqrXiw7zx/s39sdFvWPgytgiQkREZEJiqtruGIVGgxdPHsK/ADw5+AqsUbHK/WO74IKktkp3jCu3hGgxECEiIrLS6JVhR3cjoSgXZ3wDsHXQVFXreMADw7q0gbtg1wwREZGVRq9cmvaLcv9T8hhU+AaoXEsDd8IWESIiIpWkuyQmzB95JVVGw4WQgmO44+AGfCG5JcljVO9jWKK6Ymeugi0iREREKknOhoxiUaPjqo9RCeCItx8OxPZStY4HgEEuPFRXFwYiREREJhQwq65rwIPjuzYOr9VF09CAjKw05XGcCfPKaABsy5bSZ+6DXTNEREQWLmCWuP03/NFQhyBpERk9w6R95btwOXddGIgQERFZsICZaNiyULkfHNkRh0NNy/lo58Ll3HVh1wwREZEFC5gFFeViU8lJ5XHd0KtN2l+Mi5dz14WBCBERkaUKmElOyB8foxpAd29fZPcaZ9K6z7p4OXddGIgQERFZKl9DKqkW5OA/AHonj4WHp7rTbICPJ/7tBuXcdWGOCBERkYXyNfof34/+BcfQ09sPb427TfV6H90yCCOS3Kt+iBZbRIiIiIwUMFPr+p1LlPtfeo7EGT8ZM2NcTJg/hia6T0n3lhiIEBERWaCAWVBxHl7ZsxLzAHzZZ6LqfTzrhnkhTTEQISIiMkDyNm4d3tnocrErPsRWaDDPywc7OvY0unxEoI/b5oU0xRwRIiIiIzpFBBpeoKEO+7O2KQ8Tuo/APgNJqv7eHvjPzYMwPCnKrVtCtNgiQkREZERkkK/B15M2/ojshnqEAsgad7vBZavqNPD28mQQcg5bRIiIiPQUM5M6IjKEt7C8xuCy5dt+Ue4Ht0tARlCE0W27Wxl3QxiIEBERtWJumaicvdhUUaw8PjPqFlXbd7cy7oYwECEiImrF3DKRf3ysLDvQPxh5XQYaXz7Ix+3KuBvCQISIiNyethsmr7QKL/y6V3UQ4ldXg5mncyCprJV9L8IpFetc3rcD80OaYCBCRERuzZRumJYuPrAOl9VUYGBIFEaOuskyI3DcDAMRIiJyW6Z2w7R0c9qvyv1XqReh3tNL1TqRwX5m7s01WW347pEjR3D77bcjISEBAQEB6NKlC5599lnU1BjOPCYiIrJVd4y0hJgbhMSm/4FvTxzAAQ8vfNN3kur1okOZqGqTFpH9+/ejoaEBH3zwAZKSkrBnzx7ceeedKC8vx6uvvmqt3RIREakiOSHmdMdo+az9Gv8CsCEkEqdVDNkVTFS1YSBy0UUXKTetxMREHDhwAO+//z4DESIisrvW1PKQeWU2Fh1XHnsOuVL1elekdmSiqj1zREpKShAZqT8SrK6uVm5apaWlNjoyIiJyN62p5SHzyqQD6O7lg6zUyVAbWkxIjjZ7n67KZiXeDx06hHfeeQd333233mXmzp2LsLCwxltsbKytDo+IiNyMdJHEhPmrDiK0POpqkH74r3llPAzMK9NUeCC7ZSwSiMyaNQseHh4Gb5If0tTx48eVbpqrr75ayRPRZ/bs2UqrifaWk5Nj6uERERGpIl0kz16abHKyate1XyFHUw/JCskcr/+c1tKtwxPYLWOJrplHH30UM2bMMLiM5INonThxAmPHjsXw4cPx4YcfGlzPz89PuRERETmqgh2LlftBHbrjQGCYqnX8vD0xc1ySlY/MTQKRtm3bKjc1pCVEgpABAwbgk08+gafK5isiIiJbDd81RY/cDPSsLscRSXYdrz/VoKV7RyeyNcTWyaoShIwZMwadO3dWRsmcOvVX4dvoaCbrEBGR8w3fvT3tN1wNYGS3EXikQzdV6wT5eeFv49Ut646sFogsX75cSVCVW6dOnZq9ptGYWz6GiIjIMvJKKk1avk15MS7b94fy+PPBV6he77Wr+7I1xACr9ZVIHokEHLpuRERE9lZYblql794rPsCm+jpsj+6K7R17GF0+OtQP/76xPy7qHdOKo3R9nGuGiIjckilzvnhXVWDJgbX4H4BLYnsbXf7hCd2U5FS2hBjH7FEiInJLpsz5kvTHx8jXaBDt4YH0C24wuOxtI+Lx4ISuDEJUYiBCRERuXdDMGE1DA3L2rlIe9+2cinpfw+tMZPVUkzAQISIity5oJu0WhtouErf9hPS6akj4cXyC4SG7rJ5qOgYiRETktiSR9P0b+yPaQMtI5YYflPvhUXE406b5KNCWiitqsTw9z+LH6coYiBAREdw9GFn793G4sn/H816LObgBmypLlBaTM+PvMrotWU6KpEmxNFKHo2aIiMitSJCw8XABNmQWSAYIhiVGoaSyFj+mHT9v2Yk7FuM0gPbBkTgWn2p02xJ+SJE0KZY2rEsbK/0GroWBCBERuXzgIYFB/pkqHDldjo/XZaGksq7x9XmrMnXmiLQ7U4Cns3fhWQCXXfwQjpmwT+meYSCiDgMRIiJyWUv25CpdJcZKuevqSJmR9gt8G+qwuVMy9ib2N2m/H687oiStspiZccwRISIilw1C7v0izeT5ZIT/mQJotv4MWfM/g6eZvD5zRdRjIEJERC47s665YUDC8vcxs64Go719sSJpsMnrN80VIcMYiBARkcsxZ2ZdLa+aKuw+tFl5HNFtBDQe5p8qJS+FDGMgQkRELqc1AUDSH5/guKYBUfBA5kTDBcyMaReivoy8u2IgQkRELsfcAEAp5757ufJ4QOcU1PoHm7UdyRGR8vGssmocAxEiInLZeWRMnXYuadOP2FdXg0AAxy68z6x9a/cp5eM58Z1xDESIiMhl55ExVdGmc+Xc2yWgLPL8SqtqSLl4KRvPobvqsI4IERG55KiZsABfpMaGYXtOiap1Uo7sRF11OXwBnJ70N5P3Ob5HW9wxsovSGsOWEPUYiBARkVsWMWvpoS0LME4qrfYYhVc7dDNp3QfHJ+Hhid1NPFISDESIiMjlipiZWj8k+eRhjDu8FfUenvh+1I0mrRse4IMHxpsWuNBfGIgQERHcvYhZjyXv4ASALT1GIjuig0nr3joinl0xrcBkVSIicusiZu0zt+LNvAx0AfBG6iST1g3288bMcV1N3if9hYEIERG5BJnx1hyBKz5QWlH6BUXgSFyKSev+68oUtoa0EgMRIiJyidwQmfHWVG2OpWNtca7y2HvUzSate/eoBExO4RDd1mKOCBERuURuiDkilr2LOgAD/YNxNGWiqnWC/Lzwf1emYHKKabkkpBsDESIicuog5NN1WWblhoSfzMLaU9nK45Bh1+GUinWm9InG29P7szvGghiIEBGRW9UL0Wq3+C3slEJmPgHIHHiZqnLwaUeLzdoX6cccESIictp6IeYGIW3LitA/PwsB8njwNHh4qjsdyv5kdA5ZDgMRIiJym3ohWndv+gEva+rxa7suyBh+rUnrrjBzdA7pxkCEiIjcol6IVtuyQty4Y7Hy+NPRN6tuDdFasOO4EgyRZTAQISIip5J/xvwgRCT/8irS6mqwrUMPrEnob/L6heW17J6xICarEhGRU2kX4m/2uuF5h/Dt0V34H4CLkkcDHh52CYboL2wRISIipzI4IRIxYeYFI1GL3kK1jJTxDUB6vyl2CYaoOQYiRETkVKSGx7OXJpu8XkRuBtaeylIetxl2rcm5IULaTyQIkmCILIOBCBEROZ2Lesfgvev7wZS6Ym0Wv6m0hvT1DUDm4Gkm71O7KwmCWNDMchiIEBGR05DRKhsyC/DTjuOICPLDW9f1U7VexImD+PNcFdXIEdPNag2JDvPH+zf2V4IgshwmqxIRkdNWUg0P9FG1buSiN1AjM+z6BSJz4FRVVVS1bhsRj4nJ0Up3DFtCLI+BCBEROU0l1ZbVO4orao2uG194HDcWHEMGgNBRN6NQZWuIxBzzpvfnDLvO3DVz2WWXIS4uDv7+/oiJicFNN92EEydOWHOXRETkYl0w6w6dxnM/7zW7kuoja7/ErdDgo4T+ONL/EtXrzZvej0GIs7eIjB07Fk8++aQShBw/fhyPPfYYrrrqKqxfv96auyUiIjeezK6p5JOHcdm+NcrjN0fPULWOdPe8PK0Pc0FcIRB5+OGHGx937twZs2bNwtSpU1FbWwsfH3X9ekRE5D70dcGYy+vH5/GJBBc9RiK9faLR5R8c3xUPjO/KXBBXzBEpLCzEl19+ieHDh+sNQqqrq5WbVmlpqa0Oj4iIXGAyu6bit/2CX8+cxhJJUk2dZHT5W4Z1xsMTu1lo7+Qww3f//ve/IygoCG3atMHRo0fx008/6V127ty5CAsLa7zFxsZa+/CIiMhFJrNrStPQgOI1UsgdGNUmFvmdU42uU15db5F9k5UDEele8fDwMHjbv39/4/KPP/44tm/fjmXLlsHLyws333wzNBrd8e7s2bNRUlLSeMvJyTH18IiIyElZcv6Wruu/xs6aSkgh9sJLHlG1zvJ9eZxV1xm6Zh599FHMmGE44Scx8a9+uKioKOXWrVs39OzZU2nl2LhxI4YNG3been5+fsqNiIjcj8Xmb2mow7GNPygPR8V0w4HorqpWK6msU1plhnVpY5njIOsEIm3btlVu5mhoaFDum+aBEBERNZ3MLq+kqlV5Ij2W/RtL62sRCuDYpY+btC5n1XWhHJFNmzZh3rx52LFjB7Kzs7Fy5UpMnz4dXbp00dkaQkRE7q3pZHbmjlnxqyrD7l3LlMfDEwagPMK0IbicVdeFApHAwEDMnz8f48ePR/fu3XH77bcjJSUFq1evZvcLERHpJLU7ZD4XmdfFHLdvX4QfNA24xNsPhy5T3xrCWXVdcPhunz59lFYQIiIiU4MRmdtF8jXySirxwm/7UFReY7S7Jqq8CPdt/B7BABIu/ht2+8sj4zirrn1xrhkiIrIrGakiQYfkZ0jXiHZyOW3SaICvl1LkzJi7V3+G4JpK7Izuil96jlK9f2l9kSCElVTtg4EIERE5VDn3mBaBgdxPSYnBr7ty9W4nOmMTHtm9AlkAMsbeCo2H+syDV6/qixFdo1r5m5DDFjQjIiIyVM69ZREzGTUjz8vr2uUMBSFnF3obZQBWB0VgS1yKScdxupwjOe2JgQgREdmlO2bW/N068z60z0lLSU1dg3JvSMKm+dhUUaI08ddN/muOM7U4Usa+GIgQEZHNzVuZgeKKWr2vSzAiLSWfbzhiuOx7Qx3y1n6hPBzdNgEnE/ubdBwcKWN/DESIiMjmrSGfrDuiatkftx8z+HqPxe9gf10NwgCcvPzvJh8LR8rYHwMRIiKyKRkhU1ypvzWkqfQTZ/S+FlRyEtv2nC0TMazLIJxp08mk47h9RDxHyjgABiJERGRTliqjfuHvH6EcGiR6eiPDhOJlWhOSoy1yHNQ6DESIiMimLJEcmnT6KP6VuRky13v3cXegzjdQ9bqsoupYGIgQEZFdJrczm0aDZ1d8CJ+GeuzvOhTpAy5RvSqrqDoeBiJERGSXye3MDQN6rfsayN6Bai8fvDDuDpPWDQ3wVuayYW6I42AgQkREdpvcLjzQx6T1fCpKsXv9N5AC7vckDkROuGl5HiWVdSYeKVkbAxEiIrJbMLLt6Ym4NEV9MJH488vI0TQgxsMT6yfdb9Z+pUCaDCEmx8BAhIiI7D6cV402x9LxZ/Yu5XGffpNRHRRu1v6kQJrafZL1MRAhIiK7mbfyEE6eqTG6nKahAb4LXkQlgH5+gdg//i6HGEJMrcfZd4mIyGqkC0RaH+TEL8N2ZcSMdrSKTGb3xoqDqrbTfc1nWF5RAsko8b3kMXh4tu46mvPLOA4GIkREZBUSaEg+RtO5YmTYroyYGdejPZ5csEfVdgLKCrFn8wLl8diOyTiQNNjsY5IQKJo1RBwKAxEiIrJKEHLvF2nnza6bV1KFe75IQ7CfF8qq61Vt65GNPyBY04DXvHxweNrTZh8Ta4g4JuaIEBGRxbtjpCVE17gU7XNqg5BeJzNxW9qvmA4g+YqnUBsYavZxyVBh1hBxPAxEiIjIoiQnpGl3jLk86mowa/Hb8NI04NceI7Gmy8BWbc/P2xMTOb+Mw2EgQkREFmWpESk9Fr2Jq05m4n/evnjexAqquuSVVnPYrgNijggREVm0W+b0mepWbyc87xA27FuDEgBfdxmE/JA2Fjk+Dtt1PAxEiIjIaqNkzCE1QwJ/mKMEIb18/LD/kkctdowctut4GIgQEZHVRsmYo8fKj7CsvEipGRI65RGUefu2epsctuu4mCNCRERWGyXTUoCPl8HXQwqOIW3bL8rjcbG9caL7iFYfH4ftOjYGIkREZLNRMpW1hoftRn7/D5yGBl29fHDoqn9Y5PikJYTDdh0Xu2aIiKhV8kpkBpjWG52xCd4l+VgnwcOF9+Gob2Crt/nMlJ6YMSKBLSEOjIEIERG1SmG58UnrjAmpLsfLy96DtFkk9JmIj1ImWiQnhEGI42MgQkRErZrELjLYr9XbfWb5h4gpK8CR8Bh8MfHuVm2LOSHOhYEIERG1ahK76wbFtmq7Xf/4BB/t/R1DATw65WFU+RgfYhvq7405l/fG0YJyfL35qFKsTEtaQiQIYU6Ic2AgQkREZg/PlaDkjRUZCPL1QnmNuvljmgo9lY0dm+bjNIB7OvZEeqdkVeuVVtUhOtQfV/TriJnjup7XSsOWEOfBQMRBmzr5R0REzjQ815wgRAqXBX/7tDJKpruXDw5eNcesKqnyfTmsi2Uqr5LtMRBx0KZONisSkatNYtdSz6XvYOm5wmVtLnkUx/1NGyXDKqmugXVEHKCps+UfeF5JlfK8vE5EZG/WmJ+lzbF0rN+1XHk8PmEAjve4wKT1wwN9WCXVRTAQccCmTu1z8rosR0RkTxZveWioA36YgzMAUnwDsH/aUyZv4tbhHJbrKhiIOGhTp4Qf8jqnrCYie5OWB+kyttRp/7r136KyuhxBAHynPQ2NiXPJSGvIzHFJFjoacotApLq6GqmpqfDw8MCOHTtssUuXaerklNVEZG/S8iB5a6K1wUif3Aw8v+F7bAFw+aArcKpzX5O38fK0PmwNcSE2CUSeeOIJdOjQwRa7crmmTiZjEZEjkOR5ma9FanSYK6iqHO/8/C/4NtRhdbfhWDf2NpO38fCErkzkdzFWHzWzePFiLFu2DD/++KPymJo3dUpiqsaKU1ZzaDARWer7Y2JytHKT55an5+HjdUdMGqrb/tMH8V1JHq4LicKsix8APEz/LoqPkg4dciVWDUROnjyJO++8EwsXLkRgYKCqLhy5aZWWlsLVmzpldIz8KWqsUJ6YQ4OJyJLfH1JAbPrgOMRHBSoBycDOkXj8x50orzZeQ6Tn0nlYWpKHNQCWjrwJpf7BZh0XW4ldj9UCEY1GgxkzZuCee+7BwIEDceSI8ch57ty5mDPHtII2rtDUed4fuwWCBX1VELVDgzklNhGZ/P1RKlVUDzb+HB3qhwYVI/vaH07D2l3LlMcTOvfF/j7jTT4mS7USk+Px0EjEYIJZs2bhlVdeMbjMvn37lO6Y7777DqtXr4aXl5cSiCQkJGD79u1K4qraFpHY2FiUlJQgNDQUrsrS3SeyvQteWal3VI72D3rt38c12w+7cYjI2PeHqXyqyuDz7s3YV1eD/n5BKJz5ucmjZLTfQryAch5y/g4LC1N1/ja5ReTRRx9VWjoMSUxMxMqVK7Fhwwb4+TWflVFaR2644QZ89tln560ny7Zc3h1YujyxKUODtfttbTcOgxgi12DpKqqxnz+GVXU1iIQHGq79p8lBiOAkdq7N5ECkbdu2ys2Yt99+G//85z8bfz5x4gQmTZqEb7/9FkOGDDH9SMlqQ4Nb243DXBQi17EiPc9i2+qx6E0sLTymtGikjroZmTFdTVo/PMAH797QH0MT2/DCxoVZLUckLi6u2c/BwWcTk7p06YJOnTpZa7duwVjrgylDg41VeJWtyuuSmKbri4C5KESuQ/6e/2vCSBhDkk8expS9q7ASwLi4FOwfdrXJ27h2UCeMSIqyyPGQ4+Kkd1ZgzW4KNa0PxoYGi8ggX9TVN+CN5QdN7sZp+nu2JoghIseh/Xu2hNCqMry3cC7iG+oR2CkZc6593qzt/LwzF09c1JPfHy7OZoFIfHy8MpLG1Vmzm0Jf64Ps654v0vDe9f0wOaWDwaHBWoXlNbjp482t6u4xJxeFiFw8N6ShDi/88irii3NxLLQd3pz2DOBp3qmG3x/ugXPNWOhKYkNmAZ7/Za8SEFhjNl1DrQ9aM7/ejkW7ci1WBbEpXd09LFNP5Dos9Xfa4/s5ePTwVmz29MK9U2ejJCDEIY6LHBe7ZlrZpaKrBcQa3RRqrlZkOP99X6Xh4fxuSsEh+R1WPjoGI175HYXltTCHobH71ihTb87/AUfsEJlP+/eTcbKs1dvqtvozLD2yXXn8jx6jsN/E5FRdWMDM9TEQaUWXir6uEmt0U5hyVdC04JDkgrQmCDFU4dXSZerN/T/giB0i86i5kFKrw4F12LDxe+Xx+Ladsf/SR1u9TRk1wwJmro9dM00CClO6VNR0lViymdHcqwLJBTGXBBHvXt8PYQG++GnHcazLOI11h04rj6UrSuibkbNlEKPtvtKuKz+39v/AnHWIyPDfjzlCCo7h2E+vQNpUpGhZ1o2vWeQYbx0Rz9ZNN+D2LSLmjvwwN7HL3IBC2/pgyUJDhkhLytUDOuHJBXtQXKm7RSUswAe3jUhQgpUXftvX7Nhk/ctTOyhBzKJdJ857vWmrhTn/BxyxQ2S+mroGPLlgt8kXUrp411TA5/NHkKNpQGdPL1Tf/DrqfVvfnRIR6IOZ41rftUOOz+0DEXNHfpjastHaeRK0I2EkGdYWpCXl7ZWHDC5TUlmrdAOFB/rgpal9EBHkqxRDWrDjOArKa5SZOfXNztm0zogEK2r/D+T9k3tpmTF1HeaQEJ1tCZELDHO7bJvRaNDnq9lYWF0BSUmNnjobeZEdLXGYmDutD/9O3YTbByLmjvwwpWXD1Nl09SVfytX9g+OT8PbvhyxyJWMpxRW1SpLs3aMSlMBDbc6MttXiiUndVe1Hph1/5LsdJrUKrdCxDnNIyF2Zktemxk3bf8PDJzOVLhnN8Ok41HWoRVpCJAjh36f7cPtAxNyRH2qKhhmaJ0FfsKEv+fKyvjFKcR9bdc2Y48M1WSZ9wWlbLdTmsehrXTFEV5VIVn0ld2RuXps+4w5txnMrPoQXgLGjb8GHQ02vnNrU1NQOuHpgLMu5uyG3D0TMHfmhpmjY7SPiMSE5GgM6R2BbdpGSqClBR1F5DV74TXewoetkLst9sCYLjs7cL7jIYD+jQZ2Hh9IKbBHMISF3ZMnJ7OJ2/44+S96Bl6YB3/WZgA+HXNXqbY7t0Y7l3N2U2wcihgIKY10q2qJhhoaPSgvH6P9bZfQLwFmCDWtIyy7ERb3a45P12XqXsXRRXlZ9JXdjqcJgbY/uxr5Fb+JPaJAbFYf5k2aevVJoJdYLcV9uH4gYCijUTD0tr8lVtb5uFkv2x7qqzzcetdu+WbWR3MWR0+Wt3kZw4XEUfPsMTkODHt6++PmaF1Dn1frTiFy8sV6I+2IgoiKgMFa9U+6bjsyQe+mOsWR/LFkHr8LIXYbrfrq+dS2uPhWl8P7sIWQ11KGjhyf8bvgXikMs05qoNpGfXBMDkSbkD0FXM72x6p0yv8vTP+1plnR5tqKp+cXEyLrkK699qB8aNJrG3B0O6yVnYcq0BtrhukUVdWbvz6umCm3+ey821VQiDEDs1NnIjU5Ca8khz5vOpHF356Fx4ClxS0tLERYWhpKSEoSGhtr0DzyvtAqFZdU4VlShN3dB/uwnJLfD8vR8mxwbWZbUP5Ghx1oc1kvOQN+F0TNTkpVaPk2DExny3truYY/6OsR9eBfWlOZD2g+HTLgbRwZcapHfRTtjOLkeU87fDERa/IE/93O6EoSQ+9FeT6od1svJ9sjWTMk7iw71R1VdfbNg22QaDf657D0U7FiMJwCMHHE9Dl1wvcmbCfT1REVNQ+PPDPpdX6kJ5292zTT5A7dV1VJyTNov96cW7MG4Hu3h661/KiZOtkeOXgfEEhdUj6/5H27csRgN8MC2CXdhjYktIdLq+PK0Pgbz74g46d25P/BZ83fb+zDIQUh5+qFzV+idNI+T7ZGz1wFRY/DCl3H9udl0n5p0v8lBiJCpHyQw1+bfXZ7aUblnEEJNMRABsPFwQeuaL8nlyDwcuoIKY5PtCXm95ezCRM401Lzn98/i+wNrMQbA8xfcgK9TLzJ5GxJqSOFGmbVb36zbRIJdM0DjlPZELbWsvmruJIlEzjLUvOf3z2HJ4W3K45i4FHw8YrpZ29H+Ldzw302Nz7H7knRhiwiAQ/ln7H0I5ICaBhWtnSTR0ciVqQTgvFJ1nvdcOx2FNTs1en4/B0sOb1UeT4pLwf7pL1l0++y+JF3cvkVE/iCW7D1p78MgB/b5hqzG4dxqa8M4cqE0Jto653surXIyRFdmurZeELLFakGI4DxPpItbD9+VK5ILXlnp0DPamkvT0AD/ylL4lxXCr7IUgQEh8PH0hremHnXlJaguK0CDly/qfPzQ4OOHerl5+6MqMBSVQRHw8GRjWWumMd/69ESH/JLVN/zT1KHLZPv3XLYjSfUWz2fTaND/++ewIOtsd8yk2D7Yf/1cWNvXdw5l96ULK+XwXcfMQrcEKbMcdWwvgnMzkBIQgrjqCrQrK8CO3Az8UnQcZ+rrcEbTAOls+mvUPvAHgNHnHs8D8DcD+/gWHpgQEIJSvyAs8/DAh5WlCPDxh39AKLyC20AT1g61ETEoaxOHkugkVAfapticsyiqqFUKSTnaCd1Yoi2vVC1PTXKzDBevrKlHdFhA40zduuatskZ5AQ9NA55e+V+MzdoGyeTo07kv9l/3ImzB0bsvyXbcOhBx5D+EwJpKdMzcAq+DG1F7KgvFZYXIq6lEtqYBh84tMwfA8HOPTwPYp2M7cjoJAlDmG4DT3r6o9/RCQ10tOlaXox4a1GqAWmggHQ7V574c20CDyMpS5SY/K9dJVWXAmdNA/uFm2/8GwLjAcGRHxGC1fzCW11ZD06YTKtsnoSC+L8rD2sPdaE/oIX4+OF1erapuQsviaPpOSK3BRFvHvNiR4eIPf7dTeSz/xU1TR85WTO2JF37T9dfdOl4N9Zi75B1cs3uF8vMNo2fgu6FXwVYcufuSbMutAxFH+UPwrKtBhwPrEHJwA66tr8PIouNIKDyBt6DBIzqWl7keOnr7YXN0Eg5HxSE/OBIHPbwwob4G1aHtUBMcgaqgCFQFR6IyMFzpZrlfx3+83PxadOf4VZ3B32uqEF5XjdCqcqDgKC48mQnPskI0lBWiqqIYZVXlKK6tRn5DHaQ4c9uKYuUmV1RLZUNHdzVuM8bDE7H+wQgLa4+O3UcgPz4Vh9rEotqn6Z5di6mjBXTlD+g6IbU2h8NVEm2dialFxVrmr8pn4r6vtlv2oORvv6oCHT++DwFnTqPewxOPT34Q83uPt2j3pHQh6WoJ8jg3szln2yW4eyAiV6Ay4Zmftweq62ycJlNfi9j0PxC0ZxUK8g9jX1UZtPNi3gCgy7nHXQNCMASeCIuIgUfbeFRGd0FhbB+URnRAuacn3lSxK1OuoSVgqQkMw2m5aZ/s1FPv8nLNfG9VGTqXnETnolzUHt6KscfSUVxRjNyaSuRpNMjVNCC3shSoLMW6vAwMXw3li+/DkDb4ztMb/tFdUB7fH7ndh6PWPxiuSjtaoGU+gL78gZYnJH3rWyPwtkSAzvL3Z0mCs6MJKjkJ/08ewJrqckg7zNTJD+GP3uMstn3tvDf3f5WmfP80/ShrPwGcbZfg7oGIritQa2tTXoxRWWlou3clXjuyE2tbnHqklaN7QChWJQ7Ekl5jsa9dAgqCwpXX8lpsy5H+fMv8g7FXbu27AD0uaHxe2jt6lp5C28ytCDy2F3WnjqDcLxCFBceULp99paeUvBUU5wL718J7ydvo6u2LmNB28OmUjNL+l+BEuwTAw5F+W/PpysEwpWS3oRwOtSd97fBPCWpMuVI1NagwZ4SIqwYuMgu3I2lzLB2l3zyJ9Po6SNg/YNQtFg1ChPb/+X3P/ud9DuTzxdFZBHcfNWPKpFGtFXbyMDps/A5jTh3F4wU58IQGMk+vZE3Ib9MnOBIhsb1R0n0ETnQdAni6QVyo0aBdWaESkNVlbkVxQQ4yK8+goMX/yDFJzA2KwJZOyfgptC32te+C491HoMHbsb7YWzNaYN2h07jho01mr69vokaZ7Oy5y/R3A8nnH3quVHW12JgSVJgzQsQew4ltFfhIvZDp/9kIRxC7dxUO/vY68jUapcs04bLHcbzHSIttX96+edP7Y3JKjMsHmGQcZ9+143BdafaMW/0Z8jK3YmdNhfKFPArAagB72nfBH4kDsDCsHbJ6jkG9r2PkqNib5KZE5R5AmwPrgZw9qC/Ow+KaSvg21CmvTwaw+FzSbU//YERFd0V5z1HISR4NjRMGJm9dlwo/b0/M+nE3iitrzVpf5uwwNpLithHxSutJyy9/tSd+U4MKY39f2haXtX8f13g89hhObMvAx1FKBHT98wtsWP8NyqXl1csHAde/gqIO3Sy6j/eu74fJKZI1RgQGIra+OpEhcImbF6Au7TdsLs1H06+cFB9/dOzcF8Xj78KJcPcbQWIuv9pqpORlYNCxdCxL+xVbywpR0mIZ6c7qF9wGI4dcifXnkmCdoSvnqv4d8UPacbPXnzm2C4YlRuH+r9NU1ZTQdZI1dqVqTlCh9u9L26Jjzj5ay16Bj61aYc+j0eC2rT+jYOV/8DaAgf4hKLv1HVSGRllsFyyGR7qwjogeK9JbZlu0TkRFCabvXIrrdyzBHaX5WHnu+W5ePkiI74f84degsEOP806gZJyMqtkS21u5Ydg1CG+oQ/KhzQhLX43SY+nYW16kvK+eZQV4/vcPlXVOBkfiXr8gVHTsidyBl6GkbTwcUWuCEDFvVaZyU0tXoqt2NlRdJED4dF2WyUN9TR2VY+vhxPaqo6LkS9zY3zrFyIwE8y8texdX7lmJegBnOvTA6mv/aZGW2DZBvnh6Sk+l9gm7W6i13CYQkS+hBTtadwLQkqG2AWv+h/+W5CO+/uwXyy3efqgLj0b1kCtxNHkM9rMyqWV5euNEt+HKTUTU1SBl3xp0P3EAfxblYtDxdHiUFWJBWSFQkAPsWoae3r6IbZ+E6uQxOJoywSXyS8xhyknW1ETupsGHqaNybD2cWG3gI0FYVIifRXMa5H2XXB6p2mMLkSf2I+mHF3BpZQnqPDzx4rjbsXLAZRZrMXzxit5sASGLcZtARL6EZGr31uQxJG36ESWbf8QGKe4F4DMAV0Qn4dMBl+K37hc01sbgtYH1SW7I0T4TlNsiufqrq0Fy5hZcuGUhjp08jPS6auyrq8G+4+nA8XSELX8P17RLRNiAS5U8nVPB7lXDQE3rgjldCE2DD2OjcrRX0lKsreW6avfRGmoDmqbFwyKDfHBFakeM69Fe+cM+XaauQF1L8r6bWlPEXPHbfsHeFR9iOzRo5+2Lwiv/oXRdWoL8xu9e349BCFmU2wQi5l5VSQDSdf3XOLl5AVbWnt2GF4ChwW3wx4jp+KTvJKfIS3B11d6+2N59BCA3yc05dQQx235BRVYa9pSeQhGAkfmHcdPit5TXf2nbGe8GhuNM6sU41m2428yts2xvrs5AxJShxPqG+sqJWXIFJJhpWT+iaRXR0f+3SllOWgnMGU5sLnMCGrl4+e+6I8qtNXk3lu4W1qmhTpm4bvmR7UpXjCSlbrzmeZyW7k0LeXB8VyakksW5TSBi8peQRoPxGRuw9dfX8Hvt2aJEsoUL2iWi8MJ7cayj/kJfZH+SH1Jy0dkZdcLqatBnz0ocK8rFjpxdSMk9hH2nsrEU2UD2TnT08ESPqDhoeo5G9oApqPMNhKv6dH02BsW3aTbEUmzMLFDdHWOoKJU2H8JQ944EHjLa5+EJXTG5d/R5J3ktCU6kvLml8g/UtNiYm3djaCSOBFzzt1umW1if0NNH4f/VLCyR4oEyUi+sPfJueg2nz9UislS11L+N72qx7RG53agZU4bRDTy2F7NXfYIBJ/bjbgBfAhgR0w15Fz/gsAmQpF7bsiLEbfgOuQf+xI7yYlQ2eS0EQN+QKHQfciW2JI9BSYA843r+3SRxVZnV1YShxGpGSdTUNWDo3BWqu0NblrRXuy9ziq3pqqNiDm2LjQRL938lHSHnvy7PTUpuj6XpJ2EtSdsXYe/y95VKxgEShPQej30XP2jRVj75XTgzM5mCw3f1ePG3vfjPn7qvvkR0xiZ4Ln0HH5QXow+ACh8/vNV3Er7sNwVlkR1bvX9yPL4VJei89WfUH1iLPUUncEqjgbSHFEhzoYcntnZKxn+iOmN3t2HKPDmuIubcsFiZJdiUvBA56d40LF7nhHxNg4LTZ6otMlGboZol81Zm4JN1R5oFUGqCJEtXVpY8ktbkn5nLp74WD6/9EhM3/oiB0KCtlzfaXfZEY0K3pXB4Ljl1IBIfH4/s7Oxmz82dOxezZs2yS4vIoBeX6/zCCCrKRfT8F/DH6aNoAHAJgOtSL8KbI653u6RGt9ZQh7jdK9H50CY8WXISPU+dDVqTz81snCTDsqO7oCplEo72Huv0lXC/vGMIHvt+p+oTsnYOkRd+O78L4rK+Mfh5Z65VCne1rCeitOAYGQrbtMWn6XeAdEFtOHwaGfllWJ9ZgDNVZ4vmOZv47J14b+VHSM4/O0vVS4mD8NnkB1Ftga6YJy/ugV4dwlTPHE3k8IHI7bffjjvvvLPxuZCQEAQFSY1M2wYiOostNdShx6K3sXXvqsYS48ODI1F/0d+Q12VQq/ZHzq9TcR5GHViHZRu/x86qMjQ9ZbX38ECvyE7w7j0eWQMvR423D5zNzLFJmLfqkOrl7xwZj4/+PGKfwlznCqGVVNaoasEJD/TBtqcnNqvgaus6HtbgUVeDHgtexKrD2/CTdCMHhOLJSTOxtPtwi+WBbG3yvhG5REEzCTyio6PhaKNmZN6F4qXvYum5kTBdvXzQafQtODxoKtyJfN2EB3qjqMI5rwyt6Vh4NL4aciUw5Ep0K85Dp03zUZ65GTvPnMZJjQYnC3Jw6+pP8cOGb7E6YQCWJQ3G752SUR5u/8+7GseKKkxa/se043YLQsSJ4kq8uuyAqmOQgGPeykN4cEJXo6XwW0Nfbos1SP2ist/ewJJz31lvh7ZF/k2v41Tw2eHQljB3Wh8GIWRzVm8RqaqqQm1tLeLi4nD99dfj4Ycfhre37vinurpauTWNqGJjYy3eInLNzmVou+RtPHEuOXF4l8HIuOwJt5v7Rft1I3UBnv5pj136uZ2Rd1UF4tN+hiZ9Ne4qL8a1VWeU57fIZ0mGDvsHo11cX+QPnooCjq6ymBB/L5ypkoGp6kiryOYnJ2DUv1Yir/Sv7xVL8/fxRFWtdOpah09FKRJ+fB6rTuxXWuWU7ywLJ6TKLMFTUzvonJuIyKm7Zl5//XX0798fkZGRWL9+PWbPno1bb71VeV6X5557DnPmzDnvecvliEgWfw1iSk9h0X/uwW1h7XD40sdR0j4RzkK+XF+a2hthgb6478s0lBgZ6aCvnkPLJLRFu07gvq+2G953gA/uH5uEFxe1PgnRVcg8Qym5GZh4aBOO71qGl8uLm73ew9sXnWO6oyz1YuT0uMBt6pU4CkmutUTSrL0kbZ6PQ6s/Q3ZDfWPXccW0Z1AUY7lhtEF+Xiiv/ivAY3IqOXwgIommr7zyisFl9u3bhx49epz3/Mcff4y7774bZWVl8PM7W4XUVi0iYtGuXNz31dkm2nZnCpAf0vr5K6xp3nWpiAj2U1pzJJyQic6GdmnTrN/bWJOzzIgZEeSndE1FBfspUYm+JLS5i9LxwZqzyW/6hu/JFZMMg7ZELQZX1C57B9ps+Rl5OXuwq6ZCSX7W+jEwHCU9R2J50hBsju2NOi/nTnZ1BjcOjcMXG4/CGfOTnlr1MaoOrsc0CQ48PNF78DQcHDPDYvuQfJAiHTkz1pwAkNxHqTUDkVOnTqGgQE6M+iUmJsLX9/x5Pfbu3YvevXtj//796N69u82H74o7/7cFy9Pz4ejuHJmAp6bIeA3D9CXhScvJy9P6mPxFIsHa2W6aGr1XSJasxeDKQgpy0HHzfJzJ3Iqj5UWQEE8bejzu5YNNQREI6DYc2YOvQJWDB8XOanS3KKw+eNqm+wz08USFmV01fmWFmLL6U8zd9yf86mtR7+GJ22J7Y/OURywyY+59YxLRPTpUuSh59LsderusrDHzMbmXUkfpmmnpyy+/xM0334zTp08jIiLC5oGIKUXN7B2EzLq4p+pCTcqwxMMFeltOTKWmSJSuWgzRoX6YPjgOpZW1ygSDzDn5i191OUYe3YOJGRsx7tAmDKgsRc651yRkTw0IRURCP+QOnoaS9l3sfLSuJTzAG8WVtk3G7tk+GPtOnp2TSg2vmip0WzoP2/etRo1GgwwA6Z1T8cL4O3DQgkUUZeSRlPjXOYrQwPJEThmIbNiwAZs2bcLYsWOVkTPysySqXnzxxfjsM5kuDjYPRNT+8VmalLFetCdPVTLeK9NS4OnpobdctCM1lRoKWLSvScGshTtONGthcXce9XXovHMpfHevQEb+YRw51/+vNd4/GBcOnoZlXYfiUJtYzmXk4MmkrR2O2+33j3Bg19LGPJDOnl5IHXMrdgy83KL/96H+3tj+jwuVv9GfdhzHg9/sMLrOW9el4vJUFnMkJw1E0tLScN999yndMJL3kZCQgJtuugmPPPKIzvwQWwQiav/4LElqL8y6OBkD/rncYA0DmZV0w+zxWLn/pM46Cc7cb9s0YMk4WWZS7QpXJ5Mqxhzeiohtv+DY8X3YU1uFB+QEcO71jPBoPBAQispeY5Hdd5Iy6zCZzttTJuXzRHWdgwQkDXXotuZzHNn6MzLqz34vRMEDA7sNQ8bFD6HO3/LzHfXuEIpfHxipPF6XcRo3/HeT0XXYIkJOXUdERsts3Gj71gdbTCeu1iUpMXhqSi/lseRr6Eos1QYYL17RW7lS0TcDqubcsvK6JIw6U7+tHKv2y+ytFdLorI6hET+uQkbR5CUNVm4i5eRh1GWlYWXOHozI3oFjxXlYUpwH5B5E1IoP0TsiGj7dL8CRQVNRExhm78N3GhJ/1DXYPwjxrq/DZftW4/J132J88Qllllz5XxwW3w+HL3kU+yw4SV1Le06UKjlgMnDruZ/TDS5r6ZmPiQxxq7lmZCKu7k8vttnJrWWzpqEZOqWVw9X7baVlZMTLUtNBfUlxeW+kNeVjPTO0urKg6gokb1+E/B2LsavkJJoODJbJzVKDwtE9ZRIy+l6IY2Ht7XikZIx/WSH6/fkFXs3ajo5nTinP3eXpheyYbjh26WMot9H/X7CfN8qr6wx+Bzpz6ys5DodoEXFEW7IKbXqF3bIFRv6opTVDX15Fy+qv+qhdztHI760mCLm4d3vcPCyh8b0JC/B1y0Ck3C8QW4ZeBQy9CpE1Vei3YxG89q7C/lPZOKZpwIbyYry74Vv02/AtDkd0wKftErEhsgNy+k3hKBwHEZ25FSFrPsPm/CwckLomkpwcFI6PB16OjakXo9Q/2KbHU1ZtPGm3fagfnrusF4MQshm3CkRksitbMNSs2bSbwtyuI1t3MVmK2gDqwl4xzd4jeR+ldcTRRztZk1T9PTx4GjB4GjwbGjD84DqE71yK6tpq1J04gMSiEzhadAIr5I96w3fo7ReI9jHdUZ48Bjm9Rjv9BH3OxLumAonrvkHhrmXYVPXXyBlJQn2x/yXYOvoWVDtwrs9r16RiRFLrhwoTqeVm306WyasI8vVSCiV9eK74l0bHHqRLwdQ8Du0JV1+xMGfvt1UbQL3w614E+Hg2XpHJ+yjvpynT1bt6XsnxHiOV23Uy2qq6HMOyd6F4w7eIy8/C0YZ67KiuAI5sV24Ri95Az6AITBg0DVvi+2Jfu3hoPFjh1aI0GvTNPYiBWxbijf1/IrPJ3+yQwHAEDbgUh4ZeiXVOEBCeLrNeOXwiuHuOyLpDp3HDR8YzxQ3x8/bEG9f0xeSUDkZzPsyhr1iYK/Tbauu4qKnK6qHjd9X1fkt5ai8PoNSEOUhcXdvsnYjauRSlR3djd3kRZCacPgB2nXu9xC8I9waGoTCyE8p6jsTx7iM4GsfMEU8dMzagT8YmPJF7EF0Kj0FO4fKJ9fXwQN/obigYextOx55NWHcWzpqDRo7FIYbvWoI1CpqlPLcU5TXmn7RaBgRqin+ZyhoBjqNQW5VVX2VHXe+3kJlW31hx0OrH72ykUFbHfWvQ9fh+3FhegEHH0uFdUwkpJ6j9dMmojW7+IYiM6oy6zn2R12s0yiI62PnI4bB1P+L2/A6/nctwMP+wUvtDZqqSAelV3n5Y0m0YPonrg929xpod3Mmn3cvz7EgfS5OKyyUVtQZbXFlNlSyBgYgechIzVs9DDVv8wVojwHGkYOTJBc3LyFvi6syU7borr4Z6dD+2F5pN81GUl4F9FSUobbHMVMkTiOiA7R26Iy2mO/70C0Ru0mDU2jix0lHEFuchauMPOJW5BellBShq8ppUROoXGI6BI6ZjVa+xKPOzfP0PS9BeyAhXbXElx8JRM3rIib21QYj2D1haK2R71mrCNJTU6uzki66yph4Pf7fToiOETNmuu6r39EJ6XAogNymkV1eDPgfWIfjQZlTmHkTOmQIMa6hTkl/lNmDvKrwkn0eZQ8rLBzFBEQiMikNNp2ScShqC0rad4WotHh0ObUZoxkZM9/DAmGPpiCs5iZkAFp5bRr5S+4a2Q0B3mSdoGnKDI/ELHJO0gLw7vX+zKR8k2DhvegYXaXEl5+RWgYilh7066zBaRxAdJpUwLD9CSO126awGb18c6zUWkNs5X1aUID03A/1OHAAytyDyZCZk4LtUAM0ozQfkdngrsOZ/eNrHH5dEJyEzsiP2hkRhc0M9SiVIie2tbNuR+ZcXo13mFgTm7EH9qSMoLM7DgepyaAeKzwYQJ/WHPL3RJyoWk/xDUNlzJHJ6j8dRB//dtC0cUkhxRNcok8oIENmaWwUilh726qzDaB2BtUYIGduurv3IVaMkIeubidTdlASG4Y8uA5UbRt6A4IYGxJ3MRJvMzfA+vh9lBTk4UV6MIw116FlbhSE5e5TbEkBpPdF+sXT09EIbH3+EBITALygSHeL6oKFTMvJC2uC0bxCKA0KUYcnWHEYbXHgCYXmZ8D+VBc+iExjnG4gh1eWILzqBRSUnca+O9UKUnJlgzO8yGB/3HInNsb1R4etcAa6xFg5XbnEl5+NWgYil6lE4+zBaR9B0SK6HBYdAG9puS9otz53Wp/EKMa+kEs/8tFdV4Sd3Gi5cFNNVuTXVrawQXxSdwKbSU0gsPI6io7vRPS8DR+tqUAkoiZzZ1eWA3Irz8PnxdNx4bt2fAVx+LlE23NMLoV4+CPD2hY+XN7y8fDC6bTziwqNR6eOPY9Xl2Hn6aPOD0jSgobYaDfU1GBPaDqk+/giorcKBolz8qzgXxQ31KNHxu1wFYNS5x73lYsLDA3F+QQgPbQevdgko7ToEuYkDcdrbt7ErxllEBvngmUt6ITqULRzkXNwqWVWb0Khrzhe1mNRlWdYaIaRru/K93NDk065vP639jLi9hjq0yc1ASN4h+J3OgWdxLmrKCnCdfwguqC5H+7JCzK8sxZ0GNvEVgOnnHi8AMM3Ash/K5JLnHktBt4lNXpNqKdEenmjn44+wwFD06tATgZ1TkBPeHgfaxKHIinO72JKu4e5E9sRRM0Ys2nUCM7/e3uykpE9YgDdKKutcbhitI7HWCKGW2x3QOQLbsotU7UdfgHRZ3xh8u/WYRZKe3ZkkhQaVnkJw0Qn4l+TDt/QUPCtL4FFbDdRUIiWsPeJ9/OFfV40TZwqwsiDnvG14efvC09sPSe0SEBXZEZU+fjjp5YMjNRWoCotGWUQMysLbu3xV2WiWZCcHxEBEBZmF8r6vDF/13j0qAU9c1JNJXW5KX4Akz89bmYFP1h1BcSUDErKfhyd0w8xxSfxOIofDQEQlueqdNX/3eVe3MkPlv65MweQUXmGQukAlKshPaR/fkHka81ZpC3wTWUebIF9cntpByW3ixRE5IgYiJp5MNh4uwIbMAiW1cVhiVLMx90SmBCUyu7DMlVNYzpYSsqyHJ3RFaWUtFuw43uzzxe5ickQMRIhsSFc+CZGlRAT6KCO7hK6JH5lAT46IlVWJbDx3jsNG8+R0nprcEyWVMk3B2VofQxPP1vuQCSN1fc4054IRCYalq4atueRsGIgQtaI7Rr789QUhcjqIDPLF01N64mhhJSflI1XahfrhzlEyld5fpOvYUIubLaadILIWGWZPRGaQL31jJ4eC8hq0C/XHN1taFOTSMwxz3nWpSlM8uS9dFZvVTifBaSfIGbFFhMhMar/0jV3Nar12TSpGJEXB29tT5wyp5NoMVWxWO50Ep50gZ8QWESIzqf/SVxdOnC47O9eNJBxK4qGclMg9GJvWQDs9hb7sD3leXue0E+SMGIgQmUntyUGGhJsa2Egwsvbv4/D1nUNx87DOFjpiclQSdBoa9aKdQ0m0/Ly1Zm4mIkfAQITITGpPDlKXxpyrWe0MqRdzSKZdyKzMD4xLsuo+JMiUYFOCTmNDb/W1lBkLYogcHeuIENlg4j7tMF/omWnY0IlERufI0M28kiqzc0YCfTxRUdtg5truS0Y9FZbLUFrrkCDE1FEu1pqbiciSWNCMyMbUnBxaM9OwvkDGmPAAH7x7Q3/kn6nGw9/uMPG3ImsnpkpLCIMIckUsaEZkY9puFEMk2JCCU+ZczWqb5VsGMtJ9IHMleehpaXn5yj7KSJyzUxiQrbT8/2j5mmBOB9FZDESIHCxgMTWQWZ6ed16AEt2ipUWbWNua7h1S18rxzJSeeOG3fXqHbLf8vyFyd+yaIXKjriHWJ7GOlrk+umZmluHZzOkgd1HKHBEi0kVfnsolKTH4z59ZcFQDO4dja3axxbd796gE/Lwzt9n7EeLvhTNV9SZthzPgEjXHHBEiUt29M6BzBEb/3yo4Kmk8uG1EAiKCjmN5ev55r4/v0Q4pncJNmstHcmtentZHeT/6dorA0z/taRwdI0GI7LNB5SWadMXMGJHAVg4iMzEQIXLzPBW1JejtRQKC+7/arnR7vH1df7y0KB1HCioQ3yYQT05ORoCvl7Jc9+jg81p72gT54tK+MegYHojiyhqlC0UKzEltF3kfpIXo/q/Onz1ZbRAiokL8GIQQtQIDESI3Z++J0uQUHhbgjdKqOoMBgAQZa/8ejRem9rHIqCRjsyerxfldiFqHlVWJ3JwpJ1Jj1/3SAmEK7fZuHZFgMAhpOs29mtaey1M7KveGWiqMzZ5sDOd3IbIMtogQuTljQ3v/GpaajBd+Oz/R9bpBcYiPClQCmtTYcPR6donqrg3tUNbqugabt96Ysi19dVpYC4So9RiIELk57Zw5MrTX0AlXuj4m9Tbc9SH5JmqCkJlju2BEUtvG9dUWXLNkN4jabT08oSu+2ZJjsE4LEZmPgQgR6a3c2vKEa6wgm9pWhq7tQ5ptR22rjCW7QdTuc+a4rsqN87sQOWEg8ttvv+H555/Hrl274O/vj9GjR2PhwoXW3CURmak1JehNbWVouZzaVhlLnvxN3ae5FXGJyE7Jqj/++CNuuukm3Hrrrdi5cyfWrVuH66+/3lq7IyILMCXZ01Arg4cZCZ72mObeHvskIhtUVq2rq0N8fDzmzJmD22+/3eztsLIqkfPRV0q+ZRl0R5rm3h77JHJlpfaurJqWlobjx4/D09MT/fr1Q15eHlJTU/F///d/6N27t971qqurlVvTX4SIXDPfxBoTA5rLHvskIisGIocPH1bun3vuObz++utK68hrr72GMWPG4ODBg4iM1J1wNnfuXKUVhYicmyXyTYjIPZiUIzJr1ix4eHgYvO3fvx8NDWdrAjz11FO48sorMWDAAHzyySfK699//73e7c+ePVtpxtHecnJyWv8bEpFT5psQkXswqUXk0UcfxYwZMwwuk5iYiNzcXOVxcnJy4/N+fn7Ka0ePHtW7riwjNyIiInIPJgUibdu2VW7GSAuIBBQHDhzABRdcoDxXW1uLI0eOoHPnzuYfLREREbkUq+SISIbsPffcg2effRaxsbFK8CGJquLqq6+2xi6JiIjICVmtoJkEHt7e3kotkcrKSgwZMgQrV65ERESEtXZJRERETsYqdUQshXVEiIiInI8p52+rVVYlIiIiMoaBCBEREdkNAxEiIiKyGwYiRERE5HqjZixBm0fLOWeIiIich/a8rWY8jEMHImfOnFHupRYJERERORc5j8voGacdvitz1pw4cQIhISHKPDWWjNQkuJG5bDgs2Di+X+rxvVKP75V6fK9Mw/fL/u+VhBYShHTo0AGenp7O2yIiB9+pUyerbV/edH5I1eP7pR7fK/X4XqnH98o0fL/s+14ZawnRYrIqERER2Q0DESIiIrIbtwxEZGZgmZBP7sk4vl/q8b1Sj++VenyvTMP3y7neK4dOViUiIiLX5pYtIkREROQYGIgQERGR3TAQISIiIrthIEJERER247KByLvvvov4+Hj4+/tjyJAh2Lx5s8Hlv//+e/To0UNZvk+fPli0aBHchSnv1aeffqpUuW16k/XcwZo1a3DppZcqlQLl9164cKHRdf744w/0799fyUhPSkpS3j93Yer7Je9Vy8+W3PLy8uDK5s6di0GDBikVpNu1a4epU6fiwIEDRtdz1+8sc94vd/3eev/995GSktJYrGzYsGFYvHixw32uXDIQ+fbbb/HII48oQ5LS0tLQt29fTJo0Cfn5+TqXX79+PaZPn47bb78d27dvVz7YctuzZw9cnanvlZAPdG5ubuMtOzsb7qC8vFx5fyRwUyMrKwtTpkzB2LFjsWPHDjz00EO44447sHTpUrgDU98vLTmpNP18ycnGla1evRr3338/Nm7ciOXLl6O2thYXXnih8v7p487fWea8X+76vdWpUye8/PLL2LZtG7Zu3Ypx48bh8ssvx969ex3rc6VxQYMHD9bcf//9jT/X19drOnTooJk7d67O5a+55hrNlClTmj03ZMgQzd13361xdaa+V5988okmLCxM4+7kT2fBggUGl3niiSc0vXr1avbctddeq5k0aZLG3ah5v1atWqUsV1RUpHFn+fn5yvuwevVqvcu483eWOe8Xv7f+EhERofnoo480jvS5crkWkZqaGiX6mzBhQrM5a+TnDRs26FxHnm+6vJBWAX3Lu/N7JcrKytC5c2dloiRD0bW7c9fPVWulpqYiJiYGEydOxLp16+BuSkpKlPvIyEi9y/CzZdr7Jdz9e6u+vh7ffPON0nIkXTSO9LlyuUDk9OnTyhvevn37Zs/Lz/r6muV5U5Z35/eqe/fu+Pjjj/HTTz/hiy++UGZIHj58OI4dO2ajo3Ye+j5XMttlZWWl3Y7LUUnw8e9//xs//vijcpMTxpgxY5QuQ3chf0/ShTdixAj07t1b73Lu+p1l7vvlzt9bu3fvRnBwsJKnds8992DBggVITk52qM+VQ8++S45HIumm0bT8Mffs2RMffPABXnjhBbseGzk3OVnIrelnKzMzE2+88QY+//xzuAPJfZD++LVr19r7UFzq/XLn763u3bsrOWrScvTDDz/glltuUfJs9AUj9uByLSJRUVHw8vLCyZMnmz0vP0dHR+tcR543ZXl3fq9a8vHxQb9+/XDo0CErHaXz0ve5kqS5gIAAux2XMxk8eLDbfLZmzpyJX3/9FatWrVKSDA1x1+8sc98vd/7e8vX1VUbsDRgwQBlxJAnkb731lkN9rjxd8U2XN/z3339vfE6a4eRnff1i8nzT5YVkY+tb3p3fq5aka0ea/qRZnZpz18+VJcmVnKt/tiSXV06q0mS+cuVKJCQkGF3HnT9b5rxfLbnz91ZDQwOqq6sd63OlcUHffPONxs/PT/Ppp59q0tPTNXfddZcmPDxck5eXp7x+0003aWbNmtW4/Lp16zTe3t6aV199VbNv3z7Ns88+q/Hx8dHs3r1b4+pMfa/mzJmjWbp0qSYzM1Ozbds2zXXXXafx9/fX7N27V+Pqzpw5o9m+fbtykz+d119/XXmcnZ2tvC7vk7xfWocPH9YEBgZqHn/8ceVz9e6772q8vLw0S5Ys0bgDU9+vN954Q7Nw4UJNRkaG8rf34IMPajw9PTUrVqzQuLJ7771XGdHxxx9/aHJzcxtvFRUVjcvwO6t175e7fm/NmjVLGU2UlZWl2bVrl/Kzh4eHZtmyZQ71uXLJQES88847mri4OI2vr68yRHXjxo2Nr40ePVpzyy23NFv+u+++03Tr1k1ZXoZc/vbbbxp3Ycp79dBDDzUu2759e83kyZM1aWlpGnegHV7a8qZ9f+Re3q+W66SmpirvV2JiojKM0F2Y+n698sormi5duigniMjISM2YMWM0K1eu1Lg6Xe+R3Jp+Vvid1br3y12/t2677TZN586dld+7bdu2mvHjxzcGIY70ufKQf6zb5kJERETkJjkiRERE5DwYiBAREZHdMBAhIiIiu2EgQkRERHbDQISIiIjshoEIERER2Q0DESIiIrIbBiJERERkNwxEiIiIyG4YiBAREZHdMBAhIiIiu2EgQkRERLCX/wdq84eSa2SaGwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_min = 0\n",
    "x_max = 3\n",
    "N_points = 2000\n",
    "# Случайный набор X-ов.\n",
    "X_train = np.random.uniform(low=x_min, high=x_max, size=(N_points,))\n",
    "# Отклики с добавлением шума.\n",
    "y_train = f_trend(X_train) + np.random.normal(0,0.2,N_points)\n",
    "\n",
    "plt.scatter(X_train, y_train)\n",
    "plt.plot(np.sort(X_train), f_trend(np.sort(X_train)), color='red')\n",
    "plt.plot(np.sort(X_train), f_poly(np.sort(X_train), np.array([[-5, 2, -3, 1]])), '--', color='black')\n",
    "plt.legend(['train dataset', 'trend line', 'poly line'])\n",
    "plt.savefig('plots/dataset_plot.png')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a4f4da",
   "metadata": {},
   "source": [
    "Функция ошибки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "858c2928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(X, y, w_coeff):\n",
    "    return np.sum((y - f_poly(X, w_coeff))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca17f1a9",
   "metadata": {},
   "source": [
    "Функция поиска параметров методом градиентного спуска."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "551da401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit(X_train, y_train, learning_rate, tolerance, beta, batch_ratio):\n",
    "    t_init = time.time()\n",
    "\n",
    "    batch_size = int(batch_ratio * len(X_train))\n",
    "    \n",
    "    iteration_max = 200000\n",
    "    # Коэффициенты (веса) инициилизируются нулями.\n",
    "    w_coeff = np.zeros((1,len(learning_rate)))\n",
    "    # Буфер коэффициентов.\n",
    "    w_coeff_buff = w_coeff.copy()\n",
    "    \n",
    "    # Градиенты инициализируются ненулевыми значениями.\n",
    "    grad = np.ones_like(w_coeff)\n",
    "    \n",
    "    # Аккумулятор для фильтра градиента.\n",
    "    grad_filter = np.zeros_like(grad)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for i in range(iteration_max):\n",
    "        for k, w in enumerate(w_coeff.tolist()[0]):\n",
    "            # Накопление \"момента\".\n",
    "            grad_filter[0,k] = beta * grad_filter[0,k] + (1 - beta) * grad[0,k]\n",
    "            # Шаг градиентного спуска.\n",
    "            w_step = - learning_rate[k] * grad_filter[0,k]\n",
    "            # Сбрасываем коэффициенты до необновленных значений.\n",
    "            w_coeff_1 = w_coeff.copy()\n",
    "            w_coeff_2 = w_coeff.copy()\n",
    "            # Модификация только k-го коэффициента:\n",
    "            w_coeff_1[0,k] = w + w_step\n",
    "            w_coeff_2[0,k] = w + 2*w_step\n",
    "\n",
    "            # Выборка случайных элементов (по индексам).\n",
    "            batch_indices = np.random.choice(len(X_train), size=batch_size, replace=True)\n",
    "            X_train_batch = [X_train[idx] for idx in batch_indices]\n",
    "            y_train_batch = [y_train[idx] for idx in batch_indices]\n",
    "            \n",
    "            # Формируем массив значений функции потерь, для вычисления градиента. Массив состоит из 3х элементов.\n",
    "            loss_func_grad = [loss_func(X_train_batch, y_train_batch, w_coeff  ),\n",
    "                              loss_func(X_train_batch, y_train_batch, w_coeff_1),\n",
    "                              loss_func(X_train_batch, y_train_batch, w_coeff_2)]\n",
    "            # Массив градиента состоит из 3х чисел с индексами [0, 1, 2]. Берем предпоследнее число.\n",
    "            grad[0,k] = np.gradient(loss_func_grad, w_step)[1]\n",
    "            \n",
    "            # Обновление оного коэффициента в итоговом массиве.\n",
    "            w_coeff_buff[0,k] = w_coeff_1[0,k]\n",
    "        # Обновление всех коэффициентов.\n",
    "        w_coeff = w_coeff_buff.copy()\n",
    "        # Вычисление ошибки на полном датасете, при обновленных коэффициентах.\n",
    "        loss = loss_func(X_train, y_train, w_coeff)\n",
    "        # Накопление ошибки в отдельный массив для дальнейшей визуализации.\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print('Iteration:', i)\n",
    "            print('Gradient:', np.round(grad[0],4))\n",
    "            print('Weights:', np.round(w_coeff[0],4))\n",
    "            print('MSE loss:', np.round(loss,4))\n",
    "        if (loss < tolerance):\n",
    "            break\n",
    "    iter_final = i\n",
    "    fit_time = time.time() - t_init\n",
    "    return w_coeff, losses, iter_final, fit_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec579397",
   "metadata": {},
   "source": [
    "В ходе обучения заложим среднеквадратическое отклонение до 0.2. Соответствующая сумма квадратов отклонений: (0.2**2 * N_points)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b052a6",
   "metadata": {},
   "source": [
    "Сначала попробуем обучение на полном датасете, причем learning rate одинакова для каждого параметра. Выбрано наибольшее значение learning rate, при котором сходимость устойчива (функция ошибки не улетает в бесконечность)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "add33c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Gradient: [ 16679.6419  21847.6737  38673.1021  76327.7321 160746.5519]\n",
      "Weights: [-0. -0. -0. -0. -0.]\n",
      "MSE loss: 39331.2203\n",
      "Iteration: 100\n",
      "Gradient: [11810.6595 11213.8093 12224.5503  5366.5725  7195.4937]\n",
      "Weights: [-0.1232 -0.1219 -0.1401 -0.1316  0.0279]\n",
      "MSE loss: 28378.6177\n",
      "Iteration: 200\n",
      "Gradient: [ 10448.5983  10062.8216  10287.9368   8921.9877 -11249.0956]\n",
      "Weights: [-0.2341 -0.2257 -0.2552 -0.2252  0.087 ]\n",
      "MSE loss: 23367.3334\n",
      "Iteration: 300\n",
      "Gradient: [  9591.0676   8746.7259   7430.7431   5605.247  -12423.3357]\n",
      "Weights: [-0.3351 -0.3172 -0.3552 -0.3058  0.1379]\n",
      "MSE loss: 19499.0244\n",
      "Iteration: 400\n",
      "Gradient: [ 8567.5884  7643.7405  7910.8877  6768.7965 -6117.6992]\n",
      "Weights: [-0.4275 -0.3979 -0.4408 -0.3711  0.1806]\n",
      "MSE loss: 16552.298\n",
      "Iteration: 500\n",
      "Gradient: [ 8182.7437  6814.4186  7539.0731  5061.8238 -2243.0619]\n",
      "Weights: [-0.5125 -0.4692 -0.5151 -0.4266  0.2181]\n",
      "MSE loss: 14253.4047\n",
      "Iteration: 600\n",
      "Gradient: [ 7483.4197  5871.8188  6150.9166  5043.2291 -1115.1463]\n",
      "Weights: [-0.5909 -0.5326 -0.578  -0.4729  0.2496]\n",
      "MSE loss: 12474.7285\n",
      "Iteration: 700\n",
      "Gradient: [ 7027.6792  5107.4968  5303.4072  3152.8553 -3294.2689]\n",
      "Weights: [-0.6631 -0.5886 -0.6327 -0.5125  0.2767]\n",
      "MSE loss: 11075.3488\n",
      "Iteration: 800\n",
      "Gradient: [ 6477.9789  4647.5387  4790.0447  2339.3144 -3400.1642]\n",
      "Weights: [-0.7304 -0.6386 -0.6796 -0.5442  0.2995]\n",
      "MSE loss: 9974.9339\n",
      "Iteration: 900\n",
      "Gradient: [ 6090.206   4127.7018  3603.9854  2962.887  -2217.2786]\n",
      "Weights: [-0.7928 -0.6831 -0.7197 -0.57    0.3187]\n",
      "MSE loss: 9103.358\n",
      "Iteration: 1000\n",
      "Gradient: [ 5875.0681  3730.6753  3495.3932  2111.6895 -1576.1751]\n",
      "Weights: [-0.8517 -0.7227 -0.7538 -0.5912  0.335 ]\n",
      "MSE loss: 8401.8032\n",
      "Iteration: 1100\n",
      "Gradient: [ 5398.726   3499.1754  2729.2339   996.1191 -1263.8663]\n",
      "Weights: [-0.9069 -0.7582 -0.7826 -0.6085  0.3487]\n",
      "MSE loss: 7833.0811\n",
      "Iteration: 1200\n",
      "Gradient: [4887.0794 2971.6654 2233.3443   36.514   558.0882]\n",
      "Weights: [-0.9589 -0.79   -0.807  -0.6218  0.3601]\n",
      "MSE loss: 7368.036\n",
      "Iteration: 1300\n",
      "Gradient: [ 4684.3419  2687.2185  1968.864   1242.2351 -2411.3039]\n",
      "Weights: [-1.008  -0.8184 -0.8277 -0.632   0.3694]\n",
      "MSE loss: 6982.624\n",
      "Iteration: 1400\n",
      "Gradient: [ 4566.9355  2435.0912  1532.6186  1437.8832 -1171.797 ]\n",
      "Weights: [-1.0547 -0.844  -0.8448 -0.6393  0.3769]\n",
      "MSE loss: 6658.038\n",
      "Iteration: 1500\n",
      "Gradient: [ 4240.6957  2257.8817  1272.2292   909.5291 -1976.4497]\n",
      "Weights: [-1.0992 -0.8674 -0.8591 -0.644   0.3828]\n",
      "MSE loss: 6380.5984\n",
      "Iteration: 1600\n",
      "Gradient: [4022.6365 2176.7577  975.9124  -95.0168  552.2235]\n",
      "Weights: [-1.1416 -0.8881 -0.8701 -0.6455  0.3873]\n",
      "MSE loss: 6142.6547\n",
      "Iteration: 1700\n",
      "Gradient: [ 3978.4716  1641.2125   278.7991  -961.1412 -2267.5782]\n",
      "Weights: [-1.1823 -0.9072 -0.8797 -0.6458  0.3905]\n",
      "MSE loss: 5930.6432\n",
      "Iteration: 1800\n",
      "Gradient: [ 3819.2449  1669.3609   238.0639   437.1881 -1144.3755]\n",
      "Weights: [-1.2212 -0.9245 -0.8866 -0.644   0.3924]\n",
      "MSE loss: 5743.1034\n",
      "Iteration: 1900\n",
      "Gradient: [3737.986  1630.7898  726.5654  479.4238 2114.6438]\n",
      "Weights: [-1.2588 -0.9404 -0.8918 -0.6417  0.3942]\n",
      "MSE loss: 5572.6621\n",
      "Iteration: 2000\n",
      "Gradient: [ 3620.719   1284.5249   330.6105 -1024.6003 -4960.0146]\n",
      "Weights: [-1.295  -0.9552 -0.8957 -0.6376  0.3948]\n",
      "MSE loss: 5416.1499\n",
      "Iteration: 2100\n",
      "Gradient: [3500.3343 1314.6086 -113.2218 -507.7577 1604.4877]\n",
      "Weights: [-1.33   -0.9685 -0.8979 -0.6313  0.3945]\n",
      "MSE loss: 5270.7824\n",
      "Iteration: 2200\n",
      "Gradient: [3320.2885 1147.4455  385.1809 -475.2416 4227.3311]\n",
      "Weights: [-1.364  -0.9806 -0.8995 -0.6254  0.3937]\n",
      "MSE loss: 5135.3115\n",
      "Iteration: 2300\n",
      "Gradient: [3345.7792 1234.9796  294.0677 -287.0311 1453.2125]\n",
      "Weights: [-1.3972 -0.9922 -0.9004 -0.6169  0.3925]\n",
      "MSE loss: 5005.3074\n",
      "Iteration: 2400\n",
      "Gradient: [3297.0211  910.5879  -88.5766 -938.8099 -958.4936]\n",
      "Weights: [-1.4296 -1.0032 -0.9005 -0.6087  0.3905]\n",
      "MSE loss: 4881.2013\n",
      "Iteration: 2500\n",
      "Gradient: [2945.3347 1156.1164  307.0714 -363.8383  767.8217]\n",
      "Weights: [-1.4611 -1.0136 -0.9004 -0.6008  0.3887]\n",
      "MSE loss: 4764.1054\n",
      "Iteration: 2600\n",
      "Gradient: [ 2774.6862   879.8088   -73.751   -775.2492 -2376.722 ]\n",
      "Weights: [-1.4919 -1.0231 -0.8992 -0.5928  0.3865]\n",
      "MSE loss: 4652.1289\n",
      "Iteration: 2700\n",
      "Gradient: [ 3133.3092  1044.8144  -220.3211  -372.6724 -1520.4031]\n",
      "Weights: [-1.5219 -1.0319 -0.8966 -0.583   0.3835]\n",
      "MSE loss: 4543.0305\n",
      "Iteration: 2800\n",
      "Gradient: [ 2879.7789   873.2029  -147.5658 -1479.4256  2238.1247]\n",
      "Weights: [-1.5513 -1.0404 -0.894  -0.5743  0.3811]\n",
      "MSE loss: 4439.0777\n",
      "Iteration: 2900\n",
      "Gradient: [ 2723.6249   722.0479   100.5691 -1489.3938 -4996.3354]\n",
      "Weights: [-1.5801 -1.0483 -0.8913 -0.5637  0.3777]\n",
      "MSE loss: 4336.8038\n",
      "Iteration: 3000\n",
      "Gradient: [2904.2256  675.2049 -410.5668 -470.996  2815.9069]\n",
      "Weights: [-1.6086 -1.0561 -0.8888 -0.5529  0.3747]\n",
      "MSE loss: 4236.6928\n",
      "Iteration: 3100\n",
      "Gradient: [2616.2318  694.5946   48.6718  -36.5518 2614.0382]\n",
      "Weights: [-1.6365 -1.0636 -0.8856 -0.5433  0.3715]\n",
      "MSE loss: 4141.0661\n",
      "Iteration: 3200\n",
      "Gradient: [2918.7693  545.1266 -415.1281 -995.2676  144.6398]\n",
      "Weights: [-1.6638 -1.0705 -0.8819 -0.5336  0.3683]\n",
      "MSE loss: 4048.68\n",
      "Iteration: 3300\n",
      "Gradient: [ 2630.8328   546.1595  -377.3534 -1463.7882 -3086.4195]\n",
      "Weights: [-1.6907 -1.0773 -0.8776 -0.5228  0.3645]\n",
      "MSE loss: 3956.9264\n",
      "Iteration: 3400\n",
      "Gradient: [2644.7815  575.3359 -525.22   -307.8862 -572.7816]\n",
      "Weights: [-1.7172 -1.0836 -0.8738 -0.5126  0.3612]\n",
      "MSE loss: 3868.8442\n",
      "Iteration: 3500\n",
      "Gradient: [2699.7752  372.9293  162.229  -241.5002 -319.0024]\n",
      "Weights: [-1.7432 -1.0899 -0.87   -0.5033  0.358 ]\n",
      "MSE loss: 3783.9569\n",
      "Iteration: 3600\n",
      "Gradient: [2515.2224  326.9235 -767.4583 -491.3942  526.6051]\n",
      "Weights: [-1.7689 -1.096  -0.8657 -0.4931  0.3547]\n",
      "MSE loss: 3700.0745\n",
      "Iteration: 3700\n",
      "Gradient: [2607.494   503.7167  -75.7582  272.4436 1225.5247]\n",
      "Weights: [-1.7943 -1.1018 -0.8609 -0.4822  0.3513]\n",
      "MSE loss: 3617.6453\n",
      "Iteration: 3800\n",
      "Gradient: [ 2554.4846   567.7108 -1030.2685  -157.4399  1734.0689]\n",
      "Weights: [-1.8192 -1.1077 -0.8566 -0.4715  0.347 ]\n",
      "MSE loss: 3536.6552\n",
      "Iteration: 3900\n",
      "Gradient: [ 2658.9301   530.6792  -656.9948  -689.9049 -2506.9541]\n",
      "Weights: [-1.8438 -1.1132 -0.8523 -0.4616  0.3435]\n",
      "MSE loss: 3459.2009\n",
      "Iteration: 4000\n",
      "Gradient: [2300.9336  482.7292 -420.2942  880.5005 1655.7811]\n",
      "Weights: [-1.8679 -1.1185 -0.8483 -0.4509  0.3401]\n",
      "MSE loss: 3383.1261\n",
      "Iteration: 4100\n",
      "Gradient: [ 2371.2812   835.4612  -162.3177 -1254.1635 -2152.8229]\n",
      "Weights: [-1.8918 -1.1236 -0.8439 -0.4404  0.3365]\n",
      "MSE loss: 3308.8647\n",
      "Iteration: 4200\n",
      "Gradient: [ 2127.0812   501.2887  -451.175   -948.9073 -2503.203 ]\n",
      "Weights: [-1.9155 -1.1286 -0.8396 -0.4306  0.3327]\n",
      "MSE loss: 3236.211\n",
      "Iteration: 4300\n",
      "Gradient: [ 2187.0137   630.3787  -322.8723 -1162.3255 -2717.0325]\n",
      "Weights: [-1.9389 -1.1335 -0.8345 -0.4209  0.3291]\n",
      "MSE loss: 3165.4216\n",
      "Iteration: 4400\n",
      "Gradient: [ 2177.0279   536.3429  -355.9432 -1397.8316  4722.0172]\n",
      "Weights: [-1.962  -1.1382 -0.8301 -0.4106  0.3256]\n",
      "MSE loss: 3095.8796\n",
      "Iteration: 4500\n",
      "Gradient: [2126.7967  425.8907 -878.0501 -803.489  1962.5812]\n",
      "Weights: [-1.9846 -1.1428 -0.8253 -0.3999  0.3218]\n",
      "MSE loss: 3027.6965\n",
      "Iteration: 4600\n",
      "Gradient: [ 2157.8136   510.6158  -243.7839 -1136.0223  2086.9167]\n",
      "Weights: [-2.0071 -1.1477 -0.8214 -0.3914  0.3188]\n",
      "MSE loss: 2963.573\n",
      "Iteration: 4700\n",
      "Gradient: [2342.0124  438.0718  133.0499 -589.8694  316.2193]\n",
      "Weights: [-2.0291 -1.1522 -0.8172 -0.3815  0.3152]\n",
      "MSE loss: 2899.2625\n",
      "Iteration: 4800\n",
      "Gradient: [ 2294.2086   588.3234  -582.1197 -2540.4928  -739.3771]\n",
      "Weights: [-2.0509 -1.1568 -0.813  -0.3714  0.3115]\n",
      "MSE loss: 2836.2149\n",
      "Iteration: 4900\n",
      "Gradient: [2097.8132  566.0322   72.4449 -285.8516 2685.2587]\n",
      "Weights: [-2.0724 -1.1611 -0.8082 -0.3619  0.3081]\n",
      "MSE loss: 2775.1709\n",
      "Iteration: 5000\n",
      "Gradient: [2080.0774  462.0229 -161.5882 -563.2735 1052.4012]\n",
      "Weights: [-2.0936 -1.1655 -0.8031 -0.3516  0.3038]\n",
      "MSE loss: 2713.8247\n",
      "Iteration: 5100\n",
      "Gradient: [ 2082.5116   380.1519  -745.195  -1065.5164   456.539 ]\n",
      "Weights: [-2.1147 -1.1697 -0.7986 -0.3424  0.3008]\n",
      "MSE loss: 2655.7503\n",
      "Iteration: 5200\n",
      "Gradient: [ 2137.6245   637.5648  -117.3427  -730.1712 -3208.081 ]\n",
      "Weights: [-2.1354 -1.1737 -0.7941 -0.3335  0.297 ]\n",
      "MSE loss: 2598.9324\n",
      "Iteration: 5300\n",
      "Gradient: [ 2089.0936   510.8123  -401.1737 -1489.0778  2781.0393]\n",
      "Weights: [-2.1561 -1.1781 -0.7902 -0.326   0.2949]\n",
      "MSE loss: 2545.0409\n",
      "Iteration: 5400\n",
      "Gradient: [ 1959.4076   320.0233  -230.4218 -1421.1495  2029.6626]\n",
      "Weights: [-2.1764 -1.1822 -0.7852 -0.3161  0.2909]\n",
      "MSE loss: 2489.1023\n",
      "Iteration: 5500\n",
      "Gradient: [1994.3277  343.5535 -296.6816 -950.0354 3380.7792]\n",
      "Weights: [-2.1968 -1.1861 -0.7814 -0.3067  0.2875]\n",
      "MSE loss: 2435.2479\n",
      "Iteration: 5600\n",
      "Gradient: [1888.664   435.4527 -260.4795 -144.7331 2136.8015]\n",
      "Weights: [-2.2167 -1.1901 -0.7775 -0.2989  0.2848]\n",
      "MSE loss: 2384.3765\n",
      "Iteration: 5700\n",
      "Gradient: [ 1805.2613   305.4737  -574.7935 -1375.3385  1189.2794]\n",
      "Weights: [-2.2362 -1.1941 -0.7734 -0.2903  0.2812]\n",
      "MSE loss: 2333.8881\n",
      "Iteration: 5800\n",
      "Gradient: [1982.0619  336.3659 -210.5927  274.8011 2941.2846]\n",
      "Weights: [-2.2557 -1.1978 -0.7691 -0.2815  0.2784]\n",
      "MSE loss: 2284.2055\n",
      "Iteration: 5900\n",
      "Gradient: [1833.4733  255.2725 -632.7676   -9.245  1164.7923]\n",
      "Weights: [-2.2749 -1.2014 -0.7659 -0.2732  0.2752]\n",
      "MSE loss: 2236.3142\n",
      "Iteration: 6000\n",
      "Gradient: [ 1859.6567   331.497   -327.6961 -1478.7682  -593.0525]\n",
      "Weights: [-2.2937 -1.2049 -0.7617 -0.2647  0.2719]\n",
      "MSE loss: 2189.3509\n",
      "Iteration: 6100\n",
      "Gradient: [1633.7551  383.8573 -326.7973 -921.2332 1210.4409]\n",
      "Weights: [-2.3123 -1.2086 -0.7581 -0.2573  0.2695]\n",
      "MSE loss: 2144.5813\n",
      "Iteration: 6200\n",
      "Gradient: [1811.4254  217.4182 -556.0526   11.5267 1242.4257]\n",
      "Weights: [-2.3306 -1.212  -0.7541 -0.2498  0.2666]\n",
      "MSE loss: 2100.6385\n",
      "Iteration: 6300\n",
      "Gradient: [1737.8719  583.1    -537.2671 -253.0308 1545.6948]\n",
      "Weights: [-2.3489 -1.2152 -0.7502 -0.2408  0.2634]\n",
      "MSE loss: 2056.0917\n",
      "Iteration: 6400\n",
      "Gradient: [1910.6793  343.2126 -564.6563 -541.6073 1762.5138]\n",
      "Weights: [-2.3669 -1.2185 -0.7465 -0.233   0.2605]\n",
      "MSE loss: 2013.6697\n",
      "Iteration: 6500\n",
      "Gradient: [ 1781.3729   303.7823  -294.2829 -1445.8814 -1088.4285]\n",
      "Weights: [-2.3848 -1.2217 -0.743  -0.226   0.2578]\n",
      "MSE loss: 1972.6585\n",
      "Iteration: 6600\n",
      "Gradient: [ 1939.1154   234.9785  -399.8406 -1405.3816  1204.2693]\n",
      "Weights: [-2.4023 -1.2248 -0.7389 -0.218   0.2548]\n",
      "MSE loss: 1931.9001\n",
      "Iteration: 6700\n",
      "Gradient: [1886.787   219.7181 -392.8058 -548.0692  909.3119]\n",
      "Weights: [-2.4196 -1.228  -0.7356 -0.2093  0.2516]\n",
      "MSE loss: 1891.4864\n",
      "Iteration: 6800\n",
      "Gradient: [ 1820.1486   230.983   -425.8513  -969.7706 -2408.0866]\n",
      "Weights: [-2.4369 -1.2313 -0.7315 -0.2032  0.2491]\n",
      "MSE loss: 1853.5731\n",
      "Iteration: 6900\n",
      "Gradient: [1533.4238  244.088  -246.3608 -982.0496  900.5006]\n",
      "Weights: [-2.454  -1.2343 -0.7279 -0.1961  0.2467]\n",
      "MSE loss: 1815.9067\n",
      "Iteration: 7000\n",
      "Gradient: [1575.3637  390.1424 -165.6968 -870.8745 2653.7474]\n",
      "Weights: [-2.4706 -1.2374 -0.7238 -0.1891  0.244 ]\n",
      "MSE loss: 1779.2762\n",
      "Iteration: 7100\n",
      "Gradient: [1597.7918  290.2636 -315.1197 -879.9626 -697.9564]\n",
      "Weights: [-2.4874 -1.2404 -0.7199 -0.1809  0.2411]\n",
      "MSE loss: 1742.006\n",
      "Iteration: 7200\n",
      "Gradient: [ 1508.3406   428.6017  -167.6073 -1335.5997   849.996 ]\n",
      "Weights: [-2.5038 -1.2434 -0.7163 -0.1738  0.2385]\n",
      "MSE loss: 1706.7028\n",
      "Iteration: 7300\n",
      "Gradient: [1566.0185  250.1797 -331.9105 -298.6995 -617.8378]\n",
      "Weights: [-2.5199 -1.2463 -0.7127 -0.1669  0.236 ]\n",
      "MSE loss: 1672.6923\n",
      "Iteration: 7400\n",
      "Gradient: [1606.5623  450.43   -403.4142 -994.0344 -370.4824]\n",
      "Weights: [-2.5361 -1.2492 -0.7092 -0.1603  0.2335]\n",
      "MSE loss: 1639.1742\n",
      "Iteration: 7500\n",
      "Gradient: [1568.6047  443.4829 -280.9528 -285.1255  355.5945]\n",
      "Weights: [-2.5519 -1.2519 -0.7056 -0.153   0.2308]\n",
      "MSE loss: 1606.0533\n",
      "Iteration: 7600\n",
      "Gradient: [ 1594.639    331.6752  -309.0281  -347.1278 -1280.532 ]\n",
      "Weights: [-2.5675 -1.2548 -0.7023 -0.1469  0.2282]\n",
      "MSE loss: 1574.5796\n",
      "Iteration: 7700\n",
      "Gradient: [1667.4068  232.1019 -495.4491  -47.3173   57.8123]\n",
      "Weights: [-2.5831 -1.2574 -0.6989 -0.1398  0.2257]\n",
      "MSE loss: 1542.9319\n",
      "Iteration: 7800\n",
      "Gradient: [ 1420.3736   211.8779  -252.6326 -1225.1256  -543.583 ]\n",
      "Weights: [-2.5983 -1.2601 -0.6953 -0.1335  0.2234]\n",
      "MSE loss: 1512.5861\n",
      "Iteration: 7900\n",
      "Gradient: [ 1630.6015   128.0302   114.3917 -1387.8057   325.2712]\n",
      "Weights: [-2.6135 -1.2628 -0.6923 -0.1271  0.2211]\n",
      "MSE loss: 1482.9385\n",
      "Iteration: 8000\n",
      "Gradient: [ 1479.3755   176.4799  -102.6261 -1079.5058 -2965.2373]\n",
      "Weights: [-2.6285 -1.2653 -0.6892 -0.1211  0.2188]\n",
      "MSE loss: 1454.0402\n",
      "Iteration: 8100\n",
      "Gradient: [1434.446   190.8947 -242.3096 -588.6612  782.1511]\n",
      "Weights: [-2.6433 -1.268  -0.6866 -0.1154  0.2168]\n",
      "MSE loss: 1426.3147\n",
      "Iteration: 8200\n",
      "Gradient: [1369.0192  424.6361 -177.8046 -275.2915  955.7707]\n",
      "Weights: [-2.6578 -1.2703 -0.6832 -0.109   0.2144]\n",
      "MSE loss: 1398.6463\n",
      "Iteration: 8300\n",
      "Gradient: [1620.3675  269.7246  196.0887  466.6937  547.101 ]\n",
      "Weights: [-2.6722 -1.2726 -0.6802 -0.1028  0.2121]\n",
      "MSE loss: 1371.7021\n",
      "Iteration: 8400\n",
      "Gradient: [1359.0229  258.7464 -349.4941 -749.3121 2711.9618]\n",
      "Weights: [-2.6864 -1.2749 -0.6772 -0.0964  0.2097]\n",
      "MSE loss: 1345.0921\n",
      "Iteration: 8500\n",
      "Gradient: [1382.7688  304.7508 -243.6479 -641.5299 -807.932 ]\n",
      "Weights: [-2.7006 -1.2771 -0.6741 -0.0903  0.2075]\n",
      "MSE loss: 1319.1232\n",
      "Iteration: 8600\n",
      "Gradient: [1404.4305  345.0949 -330.1902 -518.886  1017.6891]\n",
      "Weights: [-2.7146 -1.2794 -0.671  -0.0846  0.2053]\n",
      "MSE loss: 1294.0523\n",
      "Iteration: 8700\n",
      "Gradient: [ 1397.0024   229.9737  -316.2327   539.0721 -2073.0873]\n",
      "Weights: [-2.7284 -1.2816 -0.6681 -0.0797  0.2034]\n",
      "MSE loss: 1270.0016\n",
      "Iteration: 8800\n",
      "Gradient: [ 1380.8872   107.7307  -406.6737 -1003.8004   180.3865]\n",
      "Weights: [-2.742  -1.2838 -0.6649 -0.0742  0.2011]\n",
      "MSE loss: 1246.2195\n",
      "Iteration: 8900\n",
      "Gradient: [ 1427.7591    60.0639  -371.2585 -1487.7678   305.1723]\n",
      "Weights: [-2.7554 -1.2858 -0.6619 -0.0679  0.1987]\n",
      "MSE loss: 1222.3938\n",
      "Iteration: 9000\n",
      "Gradient: [1275.7963  221.4964 -481.4902 -513.1932  789.3348]\n",
      "Weights: [-2.7688 -1.2879 -0.6587 -0.0626  0.1969]\n",
      "MSE loss: 1199.4717\n",
      "Iteration: 9100\n",
      "Gradient: [ 1352.182    219.4529  -407.0227 -1042.4412  1648.0773]\n",
      "Weights: [-2.782  -1.2899 -0.6558 -0.0571  0.195 ]\n",
      "MSE loss: 1177.2511\n",
      "Iteration: 9200\n",
      "Gradient: [1231.1731  262.5035 -321.6083 -406.8921 -810.1137]\n",
      "Weights: [-2.7949 -1.2919 -0.6525 -0.0512  0.1923]\n",
      "MSE loss: 1154.9428\n",
      "Iteration: 9300\n",
      "Gradient: [1270.6808  169.7043 -156.4114 -455.6157 -233.8559]\n",
      "Weights: [-2.8078 -1.2937 -0.6495 -0.0457  0.1906]\n",
      "MSE loss: 1133.6007\n",
      "Iteration: 9400\n",
      "Gradient: [1322.2434  297.1974  -78.2753 -393.4275 1024.7216]\n",
      "Weights: [-2.8206 -1.2959 -0.6469 -0.0415  0.189 ]\n",
      "MSE loss: 1113.3227\n",
      "Iteration: 9500\n",
      "Gradient: [1310.8249  234.2639 -333.9321 -169.5854 -232.3623]\n",
      "Weights: [-2.8332 -1.2978 -0.6438 -0.0368  0.1874]\n",
      "MSE loss: 1093.4553\n",
      "Iteration: 9600\n",
      "Gradient: [1195.3616  259.13   -153.1503  450.2227  887.4952]\n",
      "Weights: [-2.8456 -1.2997 -0.6411 -0.0323  0.1855]\n",
      "MSE loss: 1073.7956\n",
      "Iteration: 9700\n",
      "Gradient: [1286.1601  224.3088 -242.4087  -97.5448  950.1832]\n",
      "Weights: [-2.8579 -1.3015 -0.6385 -0.0271  0.1835]\n",
      "MSE loss: 1054.4277\n",
      "Iteration: 9800\n",
      "Gradient: [1195.958   236.706  -608.83    -48.1445  622.0883]\n",
      "Weights: [-2.87   -1.3033 -0.6357 -0.0225  0.1818]\n",
      "MSE loss: 1035.7603\n",
      "Iteration: 9900\n",
      "Gradient: [ 1198.0136   136.4486  -427.1634 -1178.7101  -433.3521]\n",
      "Weights: [-2.882  -1.3051 -0.6328 -0.018   0.1799]\n",
      "MSE loss: 1017.473\n",
      "Iteration: 10000\n",
      "Gradient: [1232.8923  231.2805 -362.4332   26.345    45.9343]\n",
      "Weights: [-2.8938 -1.3068 -0.6299 -0.0135  0.1783]\n",
      "MSE loss: 999.5877\n",
      "Iteration: 10100\n",
      "Gradient: [1209.132   188.1991 -337.1993 -805.0278  -92.657 ]\n",
      "Weights: [-2.9055 -1.3083 -0.6271 -0.0082  0.176 ]\n",
      "MSE loss: 981.6223\n",
      "Iteration: 10200\n",
      "Gradient: [1153.8188  191.4305 -357.1974 -563.9441 1811.773 ]\n",
      "Weights: [-2.9172 -1.3101 -0.6245 -0.0036  0.1745]\n",
      "MSE loss: 964.4214\n",
      "Iteration: 10300\n",
      "Gradient: [1216.433   206.1038 -110.6445 -333.6317 1116.7463]\n",
      "Weights: [-2.9287e+00 -1.3116e+00 -6.2170e-01  9.0000e-04  1.7260e-01]\n",
      "MSE loss: 947.6129\n",
      "Iteration: 10400\n",
      "Gradient: [ 1127.6121   163.5225  -124.1925  -388.0783 -1528.7444]\n",
      "Weights: [-2.94   -1.3131 -0.6188  0.0057  0.1709]\n",
      "MSE loss: 931.0247\n",
      "Iteration: 10500\n",
      "Gradient: [1133.2372  127.2044 -326.1093 -603.4527  699.9372]\n",
      "Weights: [-2.9512 -1.3147 -0.6162  0.0104  0.1689]\n",
      "MSE loss: 914.8991\n",
      "Iteration: 10600\n",
      "Gradient: [1120.6916  261.118  -256.8923 -395.3446  -55.9505]\n",
      "Weights: [-2.9624 -1.316  -0.6135  0.0151  0.1672]\n",
      "MSE loss: 898.8949\n",
      "Iteration: 10700\n",
      "Gradient: [1181.0633  153.7649 -160.2416 -103.236  2236.0936]\n",
      "Weights: [-2.9735 -1.3175 -0.6111  0.0198  0.1657]\n",
      "MSE loss: 883.4968\n",
      "Iteration: 10800\n",
      "Gradient: [ 1051.3391    97.5052  -397.9323  -825.5824 -2159.8059]\n",
      "Weights: [-2.9844 -1.319  -0.6086  0.024   0.1637]\n",
      "MSE loss: 868.3596\n",
      "Iteration: 10900\n",
      "Gradient: [1129.1719  189.4945 -291.238  -731.7575  -34.6787]\n",
      "Weights: [-2.9952 -1.3204 -0.6062  0.0284  0.1623]\n",
      "MSE loss: 853.6981\n",
      "Iteration: 11000\n",
      "Gradient: [1087.1255  199.9556 -249.1088 -295.4428 1337.6556]\n",
      "Weights: [-3.0058 -1.3219 -0.6038  0.0324  0.1608]\n",
      "MSE loss: 839.4791\n",
      "Iteration: 11100\n",
      "Gradient: [1000.9052  194.6973 -200.3667 -227.642   295.1701]\n",
      "Weights: [-3.0163 -1.3233 -0.6015  0.0366  0.1591]\n",
      "MSE loss: 825.5758\n",
      "Iteration: 11200\n",
      "Gradient: [ 961.7062  139.0269 -281.387  -229.4024 1142.2364]\n",
      "Weights: [-3.0267 -1.3247 -0.5992  0.0404  0.1577]\n",
      "MSE loss: 812.1222\n",
      "Iteration: 11300\n",
      "Gradient: [ 994.8657  236.838  -229.9915 -796.0376  649.3646]\n",
      "Weights: [-3.037  -1.3261 -0.5969  0.0444  0.1562]\n",
      "MSE loss: 798.8283\n",
      "Iteration: 11400\n",
      "Gradient: [1069.6217  195.0176 -230.5056 -930.1008  510.888 ]\n",
      "Weights: [-3.0472 -1.3274 -0.5946  0.0479  0.1548]\n",
      "MSE loss: 786.0008\n",
      "Iteration: 11500\n",
      "Gradient: [ 965.4016  214.9165 -255.47   -912.5766  713.0192]\n",
      "Weights: [-3.0572 -1.3286 -0.5925  0.0521  0.1532]\n",
      "MSE loss: 773.2103\n",
      "Iteration: 11600\n",
      "Gradient: [ 938.6168  176.3594 -170.0383 -577.6693   38.1706]\n",
      "Weights: [-3.0671 -1.3299 -0.5902  0.0556  0.1519]\n",
      "MSE loss: 760.9928\n",
      "Iteration: 11700\n",
      "Gradient: [ 971.29     90.2726 -197.6484 -925.2597 -369.4582]\n",
      "Weights: [-3.0769 -1.331  -0.5875  0.0597  0.15  ]\n",
      "MSE loss: 748.7708\n",
      "Iteration: 11800\n",
      "Gradient: [ 916.6667  184.3038 -149.2512 -230.674  -447.646 ]\n",
      "Weights: [-3.0867 -1.3321 -0.5852  0.0635  0.1488]\n",
      "MSE loss: 736.8312\n",
      "Iteration: 11900\n",
      "Gradient: [ 974.3193  130.6697  -63.3046  -47.7692 1174.5142]\n",
      "Weights: [-3.0963 -1.3333 -0.583   0.0671  0.1475]\n",
      "MSE loss: 725.311\n",
      "Iteration: 12000\n",
      "Gradient: [ 932.7768  116.2428  -86.8205 -573.3855 -959.8169]\n",
      "Weights: [-3.1058 -1.3345 -0.5811  0.071   0.1457]\n",
      "MSE loss: 713.9254\n",
      "Iteration: 12100\n",
      "Gradient: [ 951.7648   79.2589   68.3514 -173.262  1250.5012]\n",
      "Weights: [-3.1153 -1.3356 -0.579   0.0747  0.1445]\n",
      "MSE loss: 702.7973\n",
      "Iteration: 12200\n",
      "Gradient: [ 949.2294   63.3352 -277.0303 -465.5029 1196.0745]\n",
      "Weights: [-3.1247 -1.3366 -0.577   0.0788  0.1429]\n",
      "MSE loss: 691.7431\n",
      "Iteration: 12300\n",
      "Gradient: [ 922.4604   81.0096 -290.2691 -481.9748  216.8857]\n",
      "Weights: [-3.1339 -1.3377 -0.5749  0.0821  0.1417]\n",
      "MSE loss: 681.284\n",
      "Iteration: 12400\n",
      "Gradient: [ 920.1529  129.8276 -231.8877 -240.4762  305.0038]\n",
      "Weights: [-3.143  -1.3387 -0.5728  0.0854  0.1403]\n",
      "MSE loss: 671.0692\n",
      "Iteration: 12500\n",
      "Gradient: [ 947.986    74.606  -217.0138 -556.6541 -750.5473]\n",
      "Weights: [-3.152  -1.3397 -0.5705  0.0891  0.1388]\n",
      "MSE loss: 660.8422\n",
      "Iteration: 12600\n",
      "Gradient: [ 889.5292   70.4639 -294.3876 -299.152  -543.022 ]\n",
      "Weights: [-3.1609 -1.3407 -0.5686  0.0919  0.1377]\n",
      "MSE loss: 651.262\n",
      "Iteration: 12700\n",
      "Gradient: [ 798.7384  112.0171 -133.6622   66.8181 -522.6506]\n",
      "Weights: [-3.1696 -1.3416 -0.5667  0.095   0.1367]\n",
      "MSE loss: 641.8275\n",
      "Iteration: 12800\n",
      "Gradient: [925.7204 104.9979 -47.7621 -60.0669 215.3004]\n",
      "Weights: [-3.1784 -1.3425 -0.5647  0.0983  0.1354]\n",
      "MSE loss: 632.4431\n",
      "Iteration: 12900\n",
      "Gradient: [ 837.4903   70.2733 -130.272  -628.8896  350.2644]\n",
      "Weights: [-3.187  -1.3434 -0.5628  0.1014  0.1343]\n",
      "MSE loss: 623.3644\n",
      "Iteration: 13000\n",
      "Gradient: [ 844.0555   72.4127 -106.3816  -49.8136  305.7457]\n",
      "Weights: [-3.1955 -1.3443 -0.5609  0.1042  0.1332]\n",
      "MSE loss: 614.5838\n",
      "Iteration: 13100\n",
      "Gradient: [ 802.9404  109.8548 -161.1904  435.8969   44.5653]\n",
      "Weights: [-3.2038 -1.3451 -0.559   0.1071  0.132 ]\n",
      "MSE loss: 605.9566\n",
      "Iteration: 13200\n",
      "Gradient: [ 850.3526  158.3948  -37.4104 -438.3496 1261.2864]\n",
      "Weights: [-3.2122 -1.3458 -0.5569  0.1099  0.1311]\n",
      "MSE loss: 597.6075\n",
      "Iteration: 13300\n",
      "Gradient: [ 762.5253   43.531  -364.1249 -439.7048 -278.6986]\n",
      "Weights: [-3.2203 -1.3467 -0.5551  0.1124  0.1298]\n",
      "MSE loss: 589.4116\n",
      "Iteration: 13400\n",
      "Gradient: [ 812.6094   35.9066 -217.0848 -573.9646   37.1915]\n",
      "Weights: [-3.2284 -1.3474 -0.5532  0.1154  0.1287]\n",
      "MSE loss: 581.2812\n",
      "Iteration: 13500\n",
      "Gradient: [ 7.9225030e+02 -5.3440000e-01 -2.8865990e+02 -4.7007120e+02\n",
      " -1.1104381e+03]\n",
      "Weights: [-3.2363 -1.3481 -0.5512  0.1184  0.1274]\n",
      "MSE loss: 573.3796\n",
      "Iteration: 13600\n",
      "Gradient: [ 818.8357   61.3595 -221.0764 -434.3222    1.6695]\n",
      "Weights: [-3.2442 -1.3487 -0.5491  0.1215  0.1263]\n",
      "MSE loss: 565.5441\n",
      "Iteration: 13700\n",
      "Gradient: [ 882.5137   34.3033 -354.0626 -522.358  1798.5379]\n",
      "Weights: [-3.2521 -1.3493 -0.547   0.1245  0.1252]\n",
      "MSE loss: 557.8914\n",
      "Iteration: 13800\n",
      "Gradient: [ 756.5591   55.4529 -217.262  -147.523  2375.1816]\n",
      "Weights: [-3.2599 -1.35   -0.5449  0.1278  0.1239]\n",
      "MSE loss: 550.2417\n",
      "Iteration: 13900\n",
      "Gradient: [ 779.2118  108.937   -54.8159  -70.1737 -229.3327]\n",
      "Weights: [-3.2676 -1.3507 -0.5431  0.1304  0.1229]\n",
      "MSE loss: 543.0302\n",
      "Iteration: 14000\n",
      "Gradient: [ 752.1113   55.8808 -144.8608   -4.7373  628.398 ]\n",
      "Weights: [-3.2752 -1.3514 -0.5415  0.1328  0.1219]\n",
      "MSE loss: 536.03\n",
      "Iteration: 14100\n",
      "Gradient: [ 722.287    21.4274 -185.907   -52.0595  880.9301]\n",
      "Weights: [-3.2828 -1.3519 -0.5396  0.1351  0.1209]\n",
      "MSE loss: 529.18\n",
      "Iteration: 14200\n",
      "Gradient: [ 781.7616   57.228   -74.2096 -489.4092  679.0241]\n",
      "Weights: [-3.2903 -1.3524 -0.5378  0.1381  0.1197]\n",
      "MSE loss: 522.2306\n",
      "Iteration: 14300\n",
      "Gradient: [ 682.8635   85.2125 -111.727  -328.1612  631.6406]\n",
      "Weights: [-3.2977 -1.353  -0.5359  0.1406  0.1187]\n",
      "MSE loss: 515.5707\n",
      "Iteration: 14400\n",
      "Gradient: [ 710.7204  126.3908  -59.3992 -379.843   201.4826]\n",
      "Weights: [-3.3051 -1.3536 -0.5342  0.1434  0.1175]\n",
      "MSE loss: 509.0183\n",
      "Iteration: 14500\n",
      "Gradient: [804.2478  50.4956 -90.5268 374.4488 631.5702]\n",
      "Weights: [-3.3123 -1.3542 -0.5324  0.146   0.1165]\n",
      "MSE loss: 502.6065\n",
      "Iteration: 14600\n",
      "Gradient: [ 696.9347   88.7237 -152.2467   19.6583  523.4325]\n",
      "Weights: [-3.3195 -1.3547 -0.5307  0.1486  0.1155]\n",
      "MSE loss: 496.4247\n",
      "Iteration: 14700\n",
      "Gradient: [  707.0319    55.5968   -68.5033   -44.1909 -1035.1739]\n",
      "Weights: [-3.3265 -1.3552 -0.5289  0.1512  0.1144]\n",
      "MSE loss: 490.3303\n",
      "Iteration: 14800\n",
      "Gradient: [ 622.3531  -40.6579 -225.5848 -256.3178 -367.8487]\n",
      "Weights: [-3.3336 -1.3557 -0.5274  0.1538  0.1135]\n",
      "MSE loss: 484.3774\n",
      "Iteration: 14900\n",
      "Gradient: [ 600.7081  133.7507 -153.7375 -483.3775  432.4228]\n",
      "Weights: [-3.3405 -1.3562 -0.5258  0.1557  0.1127]\n",
      "MSE loss: 478.7142\n",
      "Iteration: 15000\n",
      "Gradient: [ 751.8307  115.2417 -188.5066 -115.1981  851.2815]\n",
      "Weights: [-3.3474 -1.3567 -0.5242  0.1583  0.1118]\n",
      "MSE loss: 473.024\n",
      "Iteration: 15100\n",
      "Gradient: [ 680.4678    9.2464 -147.734  -516.0454 -434.8628]\n",
      "Weights: [-3.3541 -1.3572 -0.5226  0.1609  0.1107]\n",
      "MSE loss: 467.4216\n",
      "Iteration: 15200\n",
      "Gradient: [  637.43      19.5896  -262.9362  -215.9613 -1112.3212]\n",
      "Weights: [-3.3608 -1.3576 -0.5212  0.1628  0.1099]\n",
      "MSE loss: 462.1441\n",
      "Iteration: 15300\n",
      "Gradient: [ 609.3326   45.226  -404.6211 -593.0258   -1.2959]\n",
      "Weights: [-3.3674 -1.358  -0.5195  0.165   0.109 ]\n",
      "MSE loss: 456.8901\n",
      "Iteration: 15400\n",
      "Gradient: [ 630.3818   83.5575 -311.7192  428.0358   82.651 ]\n",
      "Weights: [-3.374  -1.3584 -0.518   0.1675  0.108 ]\n",
      "MSE loss: 451.6529\n",
      "Iteration: 15500\n",
      "Gradient: [ 645.6773   91.6853  -67.6247 -137.9597   75.1967]\n",
      "Weights: [-3.3805 -1.3588 -0.5164  0.1693  0.1074]\n",
      "MSE loss: 446.7442\n",
      "Iteration: 15600\n",
      "Gradient: [ 631.4624    1.8481 -115.0666 -335.4714 -321.0088]\n",
      "Weights: [-3.3869 -1.3592 -0.5149  0.1716  0.1063]\n",
      "MSE loss: 441.7932\n",
      "Iteration: 15700\n",
      "Gradient: [ 621.1279   71.9769  137.601  -196.8289 -845.5895]\n",
      "Weights: [-3.3932 -1.3595 -0.5133  0.1737  0.1055]\n",
      "MSE loss: 437.0084\n",
      "Iteration: 15800\n",
      "Gradient: [ 555.8373   87.161  -156.8121 -263.5375 1495.2928]\n",
      "Weights: [-3.3995 -1.3599 -0.5119  0.1754  0.105 ]\n",
      "MSE loss: 432.4169\n",
      "Iteration: 15900\n",
      "Gradient: [ 656.9521   11.9831 -104.9932 -530.0323 -459.7399]\n",
      "Weights: [-3.4057 -1.3602 -0.5104  0.1775  0.1041]\n",
      "MSE loss: 427.8268\n",
      "Iteration: 16000\n",
      "Gradient: [ 577.435    28.9183  -61.9281 -317.0834  229.2619]\n",
      "Weights: [-3.4119 -1.3605 -0.509   0.1794  0.1033]\n",
      "MSE loss: 423.3334\n",
      "Iteration: 16100\n",
      "Gradient: [ 614.2524   46.286  -213.2475   67.2941  725.483 ]\n",
      "Weights: [-3.4179 -1.3607 -0.5077  0.1812  0.1026]\n",
      "MSE loss: 419.0124\n",
      "Iteration: 16200\n",
      "Gradient: [ 6.086997e+02 -2.310000e-02 -1.277303e+02 -1.004367e+02  2.997862e+02]\n",
      "Weights: [-3.4239 -1.361  -0.5062  0.1827  0.102 ]\n",
      "MSE loss: 414.8161\n",
      "Iteration: 16300\n",
      "Gradient: [ 571.2152  117.9759  -35.8635 -470.0085  328.664 ]\n",
      "Weights: [-3.4299 -1.3612 -0.5047  0.1845  0.1013]\n",
      "MSE loss: 410.6034\n",
      "Iteration: 16400\n",
      "Gradient: [ 596.7373   24.6956 -115.9763   52.5847 -113.8169]\n",
      "Weights: [-3.4358 -1.3614 -0.5033  0.1862  0.1006]\n",
      "MSE loss: 406.5923\n",
      "Iteration: 16500\n",
      "Gradient: [ 532.1166  -23.7317 -232.5679  -68.6352 -998.1412]\n",
      "Weights: [-3.4416 -1.3615 -0.5019  0.1879  0.0998]\n",
      "MSE loss: 402.6265\n",
      "Iteration: 16600\n",
      "Gradient: [ 558.8743  -19.8544 -153.8302 -298.0521 -122.3093]\n",
      "Weights: [-3.4473 -1.3617 -0.5006  0.1896  0.0991]\n",
      "MSE loss: 398.7379\n",
      "Iteration: 16700\n",
      "Gradient: [ 540.0905    6.8548 -204.1778 -311.8992 -743.4178]\n",
      "Weights: [-3.453  -1.3618 -0.499   0.1916  0.0983]\n",
      "MSE loss: 394.8792\n",
      "Iteration: 16800\n",
      "Gradient: [601.7456  13.6925 -90.4004 -90.8706   4.0607]\n",
      "Weights: [-3.4586 -1.362  -0.4976  0.1933  0.0975]\n",
      "MSE loss: 391.1182\n",
      "Iteration: 16900\n",
      "Gradient: [ 522.9334   48.3901 -118.237  -221.38    238.4692]\n",
      "Weights: [-3.4642 -1.362  -0.4963  0.1949  0.0969]\n",
      "MSE loss: 387.4992\n",
      "Iteration: 17000\n",
      "Gradient: [ 500.2359  -68.415  -235.1884  287.5914 -384.7701]\n",
      "Weights: [-3.4697 -1.3621 -0.4949  0.1963  0.0963]\n",
      "MSE loss: 384.0014\n",
      "Iteration: 17100\n",
      "Gradient: [ 5.130009e+02  4.667000e-01 -1.509032e+02 -1.244395e+02 -8.302366e+02]\n",
      "Weights: [-3.4751 -1.3622 -0.4936  0.1979  0.0956]\n",
      "MSE loss: 380.5238\n",
      "Iteration: 17200\n",
      "Gradient: [ 499.8318   10.2367  -30.2741 -225.2383  128.7779]\n",
      "Weights: [-3.4805 -1.3622 -0.4921  0.1996  0.0951]\n",
      "MSE loss: 377.0817\n",
      "Iteration: 17300\n",
      "Gradient: [542.828  -15.3646  98.0789  80.3329  39.206 ]\n",
      "Weights: [-3.4857 -1.3623 -0.4907  0.2012  0.0942]\n",
      "MSE loss: 373.7461\n",
      "Iteration: 17400\n",
      "Gradient: [ 491.0008  -13.7511  -78.5155  -83.2815 -770.1751]\n",
      "Weights: [-3.491  -1.3623 -0.4895  0.2029  0.0935]\n",
      "MSE loss: 370.4606\n",
      "Iteration: 17500\n",
      "Gradient: [ 489.3697  -18.1266 -139.8073 -196.1745    4.0864]\n",
      "Weights: [-3.4962 -1.3622 -0.4882  0.2046  0.0929]\n",
      "MSE loss: 367.2745\n",
      "Iteration: 17600\n",
      "Gradient: [ 545.1863   -1.2007  -86.1277  229.4503 -830.3804]\n",
      "Weights: [-3.5013 -1.3623 -0.487   0.2056  0.0924]\n",
      "MSE loss: 364.2584\n",
      "Iteration: 17700\n",
      "Gradient: [ 594.0233   -7.5872 -107.7636 -172.9859 -611.862 ]\n",
      "Weights: [-3.5064 -1.3621 -0.4856  0.2075  0.0916]\n",
      "MSE loss: 361.1425\n",
      "Iteration: 17800\n",
      "Gradient: [ 545.0542  -39.6056  -70.4106 -327.1559  734.5849]\n",
      "Weights: [-3.5115 -1.3622 -0.4845  0.2086  0.0913]\n",
      "MSE loss: 358.2189\n",
      "Iteration: 17900\n",
      "Gradient: [565.0491  -3.0807 -99.6264  62.2656 324.1267]\n",
      "Weights: [-3.5165 -1.3621 -0.4834  0.2096  0.0907]\n",
      "MSE loss: 355.3587\n",
      "Iteration: 18000\n",
      "Gradient: [ 461.8757   -5.2462 -116.5639  -19.0868 -189.3441]\n",
      "Weights: [-3.5215 -1.3621 -0.482   0.2111  0.0902]\n",
      "MSE loss: 352.4566\n",
      "Iteration: 18100\n",
      "Gradient: [ 506.6617  -16.2816  -61.7447  213.2637 -529.2322]\n",
      "Weights: [-3.5265 -1.362  -0.4807  0.2123  0.0896]\n",
      "MSE loss: 349.6375\n",
      "Iteration: 18200\n",
      "Gradient: [ 545.4704   11.1807 -161.6303 -403.6167  363.9776]\n",
      "Weights: [-3.5313 -1.3619 -0.4794  0.2133  0.0891]\n",
      "MSE loss: 346.9426\n",
      "Iteration: 18300\n",
      "Gradient: [454.6168  77.1372 -69.3319 318.6001 150.3468]\n",
      "Weights: [-3.5361 -1.3618 -0.4779  0.215   0.0884]\n",
      "MSE loss: 344.1927\n",
      "Iteration: 18400\n",
      "Gradient: [ 530.047   -54.4564 -113.0648 -187.7759  231.3314]\n",
      "Weights: [-3.5408 -1.3618 -0.4769  0.216   0.088 ]\n",
      "MSE loss: 341.6449\n",
      "Iteration: 18500\n",
      "Gradient: [ 465.1708  -25.3618  -68.7869 -423.6581 1014.7993]\n",
      "Weights: [-3.5455 -1.3617 -0.4755  0.2174  0.0874]\n",
      "MSE loss: 339.0485\n",
      "Iteration: 18600\n",
      "Gradient: [412.4809 -16.9439 -61.4422 -64.5446 155.2086]\n",
      "Weights: [-3.5501 -1.3616 -0.4744  0.2185  0.0869]\n",
      "MSE loss: 336.5899\n",
      "Iteration: 18700\n",
      "Gradient: [ 440.289   -55.1701 -158.5827 -555.2565 -399.8491]\n",
      "Weights: [-3.5547 -1.3615 -0.4735  0.2193  0.0865]\n",
      "MSE loss: 334.2166\n",
      "Iteration: 18800\n",
      "Gradient: [ 475.8606  -30.3951  -81.7848 -171.2725 -835.2585]\n",
      "Weights: [-3.5593 -1.3614 -0.4722  0.2207  0.086 ]\n",
      "MSE loss: 331.7695\n",
      "Iteration: 18900\n",
      "Gradient: [ 427.2073   23.7214 -140.996  -380.995  -691.6371]\n",
      "Weights: [-3.5638 -1.3612 -0.4709  0.2221  0.0853]\n",
      "MSE loss: 329.3353\n",
      "Iteration: 19000\n",
      "Gradient: [ 510.4347   11.7002 -190.2712  260.141  -126.3667]\n",
      "Weights: [-3.5682 -1.3611 -0.4697  0.2229  0.0849]\n",
      "MSE loss: 327.0951\n",
      "Iteration: 19100\n",
      "Gradient: [ 444.1884  -18.798  -101.548   -47.5162  -92.0054]\n",
      "Weights: [-3.5727 -1.3609 -0.4687  0.224   0.0844]\n",
      "MSE loss: 324.8332\n",
      "Iteration: 19200\n",
      "Gradient: [ 415.1484   -4.7403 -118.7984  176.4171  232.1315]\n",
      "Weights: [-3.577  -1.3608 -0.4676  0.2248  0.0842]\n",
      "MSE loss: 322.7203\n",
      "Iteration: 19300\n",
      "Gradient: [ 434.7094  -89.9067 -155.4656  476.181   185.2642]\n",
      "Weights: [-3.5813 -1.3606 -0.4665  0.2261  0.0835]\n",
      "MSE loss: 320.5103\n",
      "Iteration: 19400\n",
      "Gradient: [393.3682 -10.3825 -51.3153 191.8559 -67.0823]\n",
      "Weights: [-3.5856 -1.3603 -0.4652  0.2273  0.083 ]\n",
      "MSE loss: 318.3693\n",
      "Iteration: 19500\n",
      "Gradient: [ 401.5909   -9.6386  -73.4936 -225.6838 -143.2463]\n",
      "Weights: [-3.5899 -1.36   -0.4638  0.2288  0.0823]\n",
      "MSE loss: 316.1708\n",
      "Iteration: 19600\n",
      "Gradient: [ 416.3423  -26.2181    6.3242 -131.486    -4.442 ]\n",
      "Weights: [-3.5941 -1.3599 -0.4628  0.2294  0.082 ]\n",
      "MSE loss: 314.2236\n",
      "Iteration: 19700\n",
      "Gradient: [ 469.4724  -57.466  -152.9851 -527.67    475.2156]\n",
      "Weights: [-3.5982 -1.3596 -0.4616  0.2302  0.0817]\n",
      "MSE loss: 312.2591\n",
      "Iteration: 19800\n",
      "Gradient: [ 362.3347   11.69    -85.6782 -388.6537  377.6754]\n",
      "Weights: [-3.6023 -1.3595 -0.4605  0.231   0.0813]\n",
      "MSE loss: 310.3377\n",
      "Iteration: 19900\n",
      "Gradient: [ 426.738  -116.5964  -91.8751 -219.9788 -169.2881]\n",
      "Weights: [-3.6064 -1.3593 -0.4596  0.2319  0.0808]\n",
      "MSE loss: 308.4307\n",
      "Iteration: 20000\n",
      "Gradient: [ 403.4966    7.1662 -143.3163  -45.1636 -184.3736]\n",
      "Weights: [-3.6105 -1.359  -0.4583  0.2329  0.0804]\n",
      "MSE loss: 306.5386\n",
      "Iteration: 20100\n",
      "Gradient: [ 349.9702  -11.031  -251.3227 -129.5021   17.7305]\n",
      "Weights: [-3.6145 -1.3588 -0.4575  0.2333  0.0802]\n",
      "MSE loss: 304.7854\n",
      "Iteration: 20200\n",
      "Gradient: [ 421.966    11.5148  -28.2054  -57.1435 -335.8502]\n",
      "Weights: [-3.6184 -1.3584 -0.4561  0.234   0.0799]\n",
      "MSE loss: 302.9929\n",
      "Iteration: 20300\n",
      "Gradient: [ 356.9703  -55.973  -242.9195  334.3467  160.8186]\n",
      "Weights: [-3.6223 -1.3581 -0.455   0.2347  0.0795]\n",
      "MSE loss: 301.2607\n",
      "Iteration: 20400\n",
      "Gradient: [ 387.8595  -65.8855  -48.1712 -347.5221 -592.0282]\n",
      "Weights: [-3.6262 -1.3579 -0.454   0.2357  0.0789]\n",
      "MSE loss: 299.5376\n",
      "Iteration: 20500\n",
      "Gradient: [ 421.972    33.1188 -147.0317  169.067   305.8638]\n",
      "Weights: [-3.6301 -1.3575 -0.4528  0.2364  0.0787]\n",
      "MSE loss: 297.8268\n",
      "Iteration: 20600\n",
      "Gradient: [ 327.6608  -85.2212 -240.8794 -422.1705  218.4436]\n",
      "Weights: [-3.6338 -1.3572 -0.4517  0.2371  0.0783]\n",
      "MSE loss: 296.1953\n",
      "Iteration: 20700\n",
      "Gradient: [ 388.469   -62.6819  -98.6432 -133.1505  448.8997]\n",
      "Weights: [-3.6375 -1.3567 -0.4505  0.2382  0.0777]\n",
      "MSE loss: 294.5558\n",
      "Iteration: 20800\n",
      "Gradient: [ 352.6646  -18.4487 -281.8349 -136.509  -380.9595]\n",
      "Weights: [-3.6413 -1.3565 -0.4495  0.2388  0.0775]\n",
      "MSE loss: 292.9573\n",
      "Iteration: 20900\n",
      "Gradient: [ 369.8237   17.1564  -38.3816 -242.0831 -209.3534]\n",
      "Weights: [-3.645  -1.3562 -0.4484  0.2393  0.0771]\n",
      "MSE loss: 291.4228\n",
      "Iteration: 21000\n",
      "Gradient: [ 396.9071   -6.4502   -9.2205 -215.8777  852.5381]\n",
      "Weights: [-3.6486 -1.3558 -0.4475  0.2401  0.077 ]\n",
      "MSE loss: 289.9875\n",
      "Iteration: 21100\n",
      "Gradient: [ 347.0065  -33.9075 -267.9525 -173.2409  815.7726]\n",
      "Weights: [-3.6522 -1.3554 -0.4466  0.2408  0.0764]\n",
      "MSE loss: 288.4647\n",
      "Iteration: 21200\n",
      "Gradient: [  414.8421   -72.6945  -122.7897   -43.7082 -1655.8776]\n",
      "Weights: [-3.6558 -1.355  -0.4456  0.2421  0.0759]\n",
      "MSE loss: 286.9177\n",
      "Iteration: 21300\n",
      "Gradient: [276.0095 -15.9469 -21.5757 461.5487 705.9831]\n",
      "Weights: [-3.6593 -1.3545 -0.4447  0.2431  0.0755]\n",
      "MSE loss: 285.4695\n",
      "Iteration: 21400\n",
      "Gradient: [ 342.2467  -44.0871 -129.8364    3.5257  975.3048]\n",
      "Weights: [-3.6628 -1.3541 -0.4438  0.2434  0.0754]\n",
      "MSE loss: 284.1427\n",
      "Iteration: 21500\n",
      "Gradient: [ 296.7987  -85.0236 -277.4999   15.5127  247.6445]\n",
      "Weights: [-3.6662 -1.3537 -0.4426  0.2442  0.0749]\n",
      "MSE loss: 282.7093\n",
      "Iteration: 21600\n",
      "Gradient: [  316.7712   -36.9411   -44.4262    12.7318 -1819.6631]\n",
      "Weights: [-3.6697 -1.3533 -0.4417  0.2448  0.0746]\n",
      "MSE loss: 281.3543\n",
      "Iteration: 21700\n",
      "Gradient: [ 370.6209  -57.528  -205.2241 -287.7368  683.9278]\n",
      "Weights: [-3.6731 -1.3529 -0.441   0.2452  0.0745]\n",
      "MSE loss: 280.112\n",
      "Iteration: 21800\n",
      "Gradient: [ 403.6672   26.8589  -56.2541   99.6163 -118.239 ]\n",
      "Weights: [-3.6765 -1.3526 -0.4401  0.2454  0.0743]\n",
      "MSE loss: 278.8488\n",
      "Iteration: 21900\n",
      "Gradient: [ 348.6289  -19.7208  -57.992  -106.5595  -41.3708]\n",
      "Weights: [-3.6797 -1.3521 -0.4392  0.2457  0.0741]\n",
      "MSE loss: 277.6114\n",
      "Iteration: 22000\n",
      "Gradient: [ 345.4298  -45.6876  -41.0412 -200.2654 1077.1644]\n",
      "Weights: [-3.683  -1.3517 -0.4382  0.2464  0.0737]\n",
      "MSE loss: 276.3563\n",
      "Iteration: 22100\n",
      "Gradient: [ 334.165  -106.2344  -93.7828 -163.8052 1377.517 ]\n",
      "Weights: [-3.6863 -1.3512 -0.4372  0.247   0.0734]\n",
      "MSE loss: 275.1334\n",
      "Iteration: 22200\n",
      "Gradient: [ 303.5505   -4.4133   15.6355  153.255  -205.4563]\n",
      "Weights: [-3.6895 -1.3508 -0.4363  0.2477  0.073 ]\n",
      "MSE loss: 273.9071\n",
      "Iteration: 22300\n",
      "Gradient: [ 382.9842  -85.5927  -63.1502 -272.6402  453.5469]\n",
      "Weights: [-3.6927 -1.3503 -0.4354  0.2487  0.0725]\n",
      "MSE loss: 272.6976\n",
      "Iteration: 22400\n",
      "Gradient: [  282.5232   -38.3766  -215.0287  -214.3731 -1315.0235]\n",
      "Weights: [-3.696  -1.3499 -0.4345  0.2495  0.0722]\n",
      "MSE loss: 271.5117\n",
      "Iteration: 22500\n",
      "Gradient: [ 355.3141  -10.3397  -81.6837 -487.622   540.0776]\n",
      "Weights: [-3.6991 -1.3495 -0.4336  0.2502  0.0718]\n",
      "MSE loss: 270.3725\n",
      "Iteration: 22600\n",
      "Gradient: [ 338.8602  -66.4004  -14.1679 -324.9227 -405.1525]\n",
      "Weights: [-3.7022 -1.349  -0.4329  0.2501  0.0717]\n",
      "MSE loss: 269.314\n",
      "Iteration: 22700\n",
      "Gradient: [  287.9459   -97.2542  -257.2059  -359.5076 -1144.7576]\n",
      "Weights: [-3.7053 -1.3485 -0.4322  0.2506  0.0713]\n",
      "MSE loss: 268.3187\n",
      "Iteration: 22800\n",
      "Gradient: [ 302.6317  -86.0261 -245.6724   91.7628 -906.8846]\n",
      "Weights: [-3.7084 -1.348  -0.4313  0.2511  0.0712]\n",
      "MSE loss: 267.1716\n",
      "Iteration: 22900\n",
      "Gradient: [ 289.3704  -63.6256   -2.4902  -78.0189 -302.7839]\n",
      "Weights: [-3.7115 -1.3475 -0.4304  0.2516  0.071 ]\n",
      "MSE loss: 266.0929\n",
      "Iteration: 23000\n",
      "Gradient: [ 281.8781  -10.9359 -113.0016   91.3123  220.3807]\n",
      "Weights: [-3.7145 -1.3471 -0.4295  0.2518  0.0709]\n",
      "MSE loss: 265.0814\n",
      "Iteration: 23100\n",
      "Gradient: [ 257.5193  -63.7916 -210.7901 -153.5241 -164.494 ]\n",
      "Weights: [-3.7174 -1.3466 -0.4287  0.252   0.0707]\n",
      "MSE loss: 264.0995\n",
      "Iteration: 23200\n",
      "Gradient: [ 245.092   -77.9668   65.621  -446.2969  611.3403]\n",
      "Weights: [-3.7204 -1.3462 -0.4279  0.2522  0.0704]\n",
      "MSE loss: 263.1346\n",
      "Iteration: 23300\n",
      "Gradient: [ 276.6398  -34.9118  -53.6051 -322.3403 -286.9883]\n",
      "Weights: [-3.7233 -1.3457 -0.427   0.2524  0.0704]\n",
      "MSE loss: 262.1647\n",
      "Iteration: 23400\n",
      "Gradient: [ 231.0719   -2.9507 -220.7347   42.384   451.7946]\n",
      "Weights: [-3.7261 -1.3451 -0.4261  0.2532  0.07  ]\n",
      "MSE loss: 261.1851\n",
      "Iteration: 23500\n",
      "Gradient: [ 270.4386  -71.473   -32.5796 -510.9818 -371.652 ]\n",
      "Weights: [-3.729  -1.3445 -0.4253  0.2535  0.0697]\n",
      "MSE loss: 260.2709\n",
      "Iteration: 23600\n",
      "Gradient: [ 318.198   -49.1503  -37.9822  -53.4008 -832.4653]\n",
      "Weights: [-3.7319 -1.344  -0.4245  0.2539  0.0695]\n",
      "MSE loss: 259.3236\n",
      "Iteration: 23700\n",
      "Gradient: [ 290.3192  -66.2543  -48.7851  117.614  1224.675 ]\n",
      "Weights: [-3.7347 -1.3435 -0.4236  0.2542  0.0695]\n",
      "MSE loss: 258.4223\n",
      "Iteration: 23800\n",
      "Gradient: [ 236.8698    8.9486  118.113   337.6007 1030.2914]\n",
      "Weights: [-3.7374 -1.343  -0.4227  0.254   0.0693]\n",
      "MSE loss: 257.5468\n",
      "Iteration: 23900\n",
      "Gradient: [ 283.7236  -53.0595 -151.0506 -373.6958 -289.2358]\n",
      "Weights: [-3.7402 -1.3425 -0.4219  0.2543  0.0693]\n",
      "MSE loss: 256.701\n",
      "Iteration: 24000\n",
      "Gradient: [ 259.8536 -103.154   -32.649    71.7091  233.4823]\n",
      "Weights: [-3.7429 -1.3419 -0.4212  0.2545  0.0691]\n",
      "MSE loss: 255.8476\n",
      "Iteration: 24100\n",
      "Gradient: [ 303.0293  -19.0409 -103.9737 -281.0546 -216.4625]\n",
      "Weights: [-3.7456 -1.3413 -0.4203  0.2548  0.0688]\n",
      "MSE loss: 254.9889\n",
      "Iteration: 24200\n",
      "Gradient: [ 252.2666  -67.8647 -270.8673  -86.1154  197.885 ]\n",
      "Weights: [-3.7482 -1.3407 -0.4195  0.2553  0.0685]\n",
      "MSE loss: 254.148\n",
      "Iteration: 24300\n",
      "Gradient: [304.6452 -34.8835 -24.1274  71.7635  48.5464]\n",
      "Weights: [-3.7509 -1.3402 -0.4187  0.2555  0.0684]\n",
      "MSE loss: 253.3536\n",
      "Iteration: 24400\n",
      "Gradient: [  208.3942   -15.5973    17.2092   132.1478 -1262.5729]\n",
      "Weights: [-3.7535 -1.3396 -0.4179  0.2558  0.0681]\n",
      "MSE loss: 252.547\n",
      "Iteration: 24500\n",
      "Gradient: [ 202.588   -94.3705   30.6727  -99.4511 -329.2371]\n",
      "Weights: [-3.7561 -1.339  -0.4171  0.2562  0.0681]\n",
      "MSE loss: 251.7622\n",
      "Iteration: 24600\n",
      "Gradient: [ 242.955   -93.8022 -138.4723 -382.396  -491.258 ]\n",
      "Weights: [-3.7587 -1.3384 -0.4163  0.2565  0.0676]\n",
      "MSE loss: 250.9998\n",
      "Iteration: 24700\n",
      "Gradient: [ 250.0548  -60.289    10.2812 -168.0189  -44.6553]\n",
      "Weights: [-3.7613 -1.3377 -0.4155  0.2568  0.0676]\n",
      "MSE loss: 250.2091\n",
      "Iteration: 24800\n",
      "Gradient: [ 273.2599  -50.9912  220.2924 -294.5519  564.53  ]\n",
      "Weights: [-3.7638 -1.3371 -0.4148  0.2569  0.0675]\n",
      "MSE loss: 249.4625\n",
      "Iteration: 24900\n",
      "Gradient: [ 248.8588  -67.4647   68.2404 -410.5574   70.9279]\n",
      "Weights: [-3.7663 -1.3365 -0.4142  0.2566  0.0673]\n",
      "MSE loss: 248.761\n",
      "Iteration: 25000\n",
      "Gradient: [ 2.154205e+02 -4.663790e+01  1.624200e+00 -3.630000e-02 -3.398857e+02]\n",
      "Weights: [-3.7688 -1.336  -0.4135  0.2567  0.0673]\n",
      "MSE loss: 248.0353\n",
      "Iteration: 25100\n",
      "Gradient: [ 276.8515  -98.1858 -122.8751 -117.1403 -695.9539]\n",
      "Weights: [-3.7713 -1.3353 -0.4128  0.2571  0.067 ]\n",
      "MSE loss: 247.3459\n",
      "Iteration: 25200\n",
      "Gradient: [ 260.652   -49.494  -230.8429 -199.194  -448.1173]\n",
      "Weights: [-3.7737 -1.3347 -0.412   0.2577  0.0668]\n",
      "MSE loss: 246.5982\n",
      "Iteration: 25300\n",
      "Gradient: [289.2213 -86.9992 -68.6426 -53.6456 284.1089]\n",
      "Weights: [-3.7761 -1.334  -0.4112  0.2577  0.0668]\n",
      "MSE loss: 245.9248\n",
      "Iteration: 25400\n",
      "Gradient: [266.3522 -46.185  -61.3138 142.8658 592.7581]\n",
      "Weights: [-3.7786 -1.3334 -0.4104  0.2576  0.0666]\n",
      "MSE loss: 245.2342\n",
      "Iteration: 25500\n",
      "Gradient: [ 273.389   -53.4932 -100.3542 -168.221   132.2006]\n",
      "Weights: [-3.781  -1.3327 -0.4095  0.2577  0.0664]\n",
      "MSE loss: 244.5491\n",
      "Iteration: 25600\n",
      "Gradient: [ 209.6388  -85.9832 -256.9801 -486.2003 -412.8   ]\n",
      "Weights: [-3.7833 -1.332  -0.4088  0.258   0.0662]\n",
      "MSE loss: 243.9099\n",
      "Iteration: 25700\n",
      "Gradient: [ 245.1027 -109.4401  -73.184   122.3721  886.3479]\n",
      "Weights: [-3.7857 -1.3314 -0.408   0.2586  0.0661]\n",
      "MSE loss: 243.2446\n",
      "Iteration: 25800\n",
      "Gradient: [ 247.7649    2.8633  -94.3679  -87.6849 -119.9944]\n",
      "Weights: [-3.788  -1.3308 -0.4074  0.2584  0.066 ]\n",
      "MSE loss: 242.6055\n",
      "Iteration: 25900\n",
      "Gradient: [ 207.7395  -77.2122 -182.5241 -452.107  -979.6295]\n",
      "Weights: [-3.7903 -1.3302 -0.4068  0.258   0.0659]\n",
      "MSE loss: 242.0149\n",
      "Iteration: 26000\n",
      "Gradient: [ 237.7387  -78.8544 -139.2534  243.8443  -39.6886]\n",
      "Weights: [-3.7926 -1.3295 -0.4061  0.2579  0.0661]\n",
      "MSE loss: 241.3836\n",
      "Iteration: 26100\n",
      "Gradient: [247.2215  13.5801 -33.1668 278.952  841.902 ]\n",
      "Weights: [-3.7949 -1.3288 -0.4053  0.2581  0.066 ]\n",
      "MSE loss: 240.8353\n",
      "Iteration: 26200\n",
      "Gradient: [ 203.5853 -128.9532  -90.5015 -218.2861  413.0342]\n",
      "Weights: [-3.7971 -1.3281 -0.4044  0.2586  0.0654]\n",
      "MSE loss: 240.1589\n",
      "Iteration: 26300\n",
      "Gradient: [ 117.0982  -22.2626 -185.7156 -419.9886   92.3529]\n",
      "Weights: [-3.7994 -1.3274 -0.4037  0.2586  0.0654]\n",
      "MSE loss: 239.5471\n",
      "Iteration: 26400\n",
      "Gradient: [ 185.4292  -51.9674  -68.753  -132.7663 -879.087 ]\n",
      "Weights: [-3.8015 -1.3267 -0.4029  0.2586  0.0653]\n",
      "MSE loss: 238.9671\n",
      "Iteration: 26500\n",
      "Gradient: [245.3247 -32.0891 -14.707  -49.1477 354.77  ]\n",
      "Weights: [-3.8037 -1.326  -0.4023  0.2585  0.0653]\n",
      "MSE loss: 238.3997\n",
      "Iteration: 26600\n",
      "Gradient: [ 214.6053   -4.0708  -81.3064  -73.5316 1026.6646]\n",
      "Weights: [-3.8059 -1.3254 -0.4016  0.2583  0.0655]\n",
      "MSE loss: 237.8884\n",
      "Iteration: 26700\n",
      "Gradient: [  245.6741   -10.4399   -22.1584   203.3057 -1000.6008]\n",
      "Weights: [-3.8081 -1.3246 -0.4008  0.2586  0.065 ]\n",
      "MSE loss: 237.2591\n",
      "Iteration: 26800\n",
      "Gradient: [ 170.1887  -84.3161 -144.3496  164.8046  311.2228]\n",
      "Weights: [-3.8103 -1.324  -0.4002  0.2587  0.0649]\n",
      "MSE loss: 236.714\n",
      "Iteration: 26900\n",
      "Gradient: [245.2841 -24.5459  77.0784  51.2282 776.0633]\n",
      "Weights: [-3.8124 -1.3233 -0.3996  0.2588  0.0649]\n",
      "MSE loss: 236.1931\n",
      "Iteration: 27000\n",
      "Gradient: [215.5629 -52.4533 -32.7947  75.3805 770.5634]\n",
      "Weights: [-3.8145 -1.3227 -0.399   0.2588  0.0647]\n",
      "MSE loss: 235.6491\n",
      "Iteration: 27100\n",
      "Gradient: [ 187.1134  -97.8419 -117.525   -43.3071 -279.3299]\n",
      "Weights: [-3.8166 -1.3219 -0.3983  0.2592  0.0645]\n",
      "MSE loss: 235.1156\n",
      "Iteration: 27200\n",
      "Gradient: [ 185.2096  -98.9126   35.8851 -108.607  -152.2313]\n",
      "Weights: [-3.8186 -1.3212 -0.3976  0.2589  0.0645]\n",
      "MSE loss: 234.6071\n",
      "Iteration: 27300\n",
      "Gradient: [ 179.1064  -79.0856   27.1267 -109.6799  631.4963]\n",
      "Weights: [-3.8207 -1.3205 -0.3972  0.2589  0.0646]\n",
      "MSE loss: 234.1371\n",
      "Iteration: 27400\n",
      "Gradient: [ 205.2316 -109.0462  -31.4401  167.8305 -584.0456]\n",
      "Weights: [-3.8227 -1.3198 -0.3965  0.2588  0.0643]\n",
      "MSE loss: 233.6467\n",
      "Iteration: 27500\n",
      "Gradient: [ 270.521   -82.6662  -38.7679   63.4687 -189.9071]\n",
      "Weights: [-3.8246 -1.3191 -0.3959  0.259   0.0641]\n",
      "MSE loss: 233.1684\n",
      "Iteration: 27600\n",
      "Gradient: [ 2.206495e+02 -5.482960e+01 -5.873000e-01 -7.348570e+01  6.272730e+02]\n",
      "Weights: [-3.8266 -1.3184 -0.3954  0.2592  0.0641]\n",
      "MSE loss: 232.6566\n",
      "Iteration: 27700\n",
      "Gradient: [186.1931 -97.1393 -39.0397   6.4455 -50.2436]\n",
      "Weights: [-3.8286 -1.3178 -0.3949  0.2594  0.0639]\n",
      "MSE loss: 232.1938\n",
      "Iteration: 27800\n",
      "Gradient: [ 211.0531  -63.9819 -244.0309 -235.0106  100.2689]\n",
      "Weights: [-3.8306 -1.3171 -0.3943  0.2596  0.0638]\n",
      "MSE loss: 231.7137\n",
      "Iteration: 27900\n",
      "Gradient: [ 138.7282  -10.3156 -186.2386 -109.5456  407.4517]\n",
      "Weights: [-3.8326 -1.3164 -0.3936  0.2601  0.0634]\n",
      "MSE loss: 231.2708\n",
      "Iteration: 28000\n",
      "Gradient: [ 211.2543 -106.658   -15.5434 -312.2703  283.9375]\n",
      "Weights: [-3.8346 -1.3157 -0.393   0.2602  0.0635]\n",
      "MSE loss: 230.7786\n",
      "Iteration: 28100\n",
      "Gradient: [ 218.0172 -112.3027  -29.3958 -122.7959 -773.1627]\n",
      "Weights: [-3.8365 -1.3151 -0.3924  0.2598  0.0636]\n",
      "MSE loss: 230.3135\n",
      "Iteration: 28200\n",
      "Gradient: [ 166.2527 -110.3016   68.2458 -179.1601 -771.7291]\n",
      "Weights: [-3.8385 -1.3144 -0.3918  0.26    0.0632]\n",
      "MSE loss: 229.8801\n",
      "Iteration: 28300\n",
      "Gradient: [ 202.6365  -68.9574 -182.5365  375.5828  987.6355]\n",
      "Weights: [-3.8404 -1.3137 -0.3913  0.2599  0.0635]\n",
      "MSE loss: 229.4407\n",
      "Iteration: 28400\n",
      "Gradient: [ 159.5828  -96.7002   47.8979  190.7022 -736.8811]\n",
      "Weights: [-3.8422 -1.313  -0.3908  0.2597  0.0633]\n",
      "MSE loss: 228.9995\n",
      "Iteration: 28500\n",
      "Gradient: [ 155.311  -100.4562 -108.8202  451.3255   55.7932]\n",
      "Weights: [-3.8441 -1.3123 -0.39    0.2594  0.0634]\n",
      "MSE loss: 228.552\n",
      "Iteration: 28600\n",
      "Gradient: [ 145.0706  -69.8773  -20.4656 -281.727   -10.8366]\n",
      "Weights: [-3.8459 -1.3116 -0.3894  0.2592  0.0634]\n",
      "MSE loss: 228.1354\n",
      "Iteration: 28700\n",
      "Gradient: [ 183.6999  -80.7666 -222.7773 -170.2127 -820.3604]\n",
      "Weights: [-3.8477 -1.3109 -0.3887  0.2593  0.0632]\n",
      "MSE loss: 227.7114\n",
      "Iteration: 28800\n",
      "Gradient: [ 196.8706 -112.105    20.2167   42.9982  459.5036]\n",
      "Weights: [-3.8496 -1.3103 -0.3882  0.2593  0.0631]\n",
      "MSE loss: 227.3222\n",
      "Iteration: 28900\n",
      "Gradient: [ 143.657   -60.8093  -75.5622  277.1382 -767.5416]\n",
      "Weights: [-3.8514 -1.3096 -0.3876  0.2595  0.0631]\n",
      "MSE loss: 226.9387\n",
      "Iteration: 29000\n",
      "Gradient: [ 193.1315  -76.9699  -37.4658   85.6714 -464.8554]\n",
      "Weights: [-3.8532 -1.3089 -0.3871  0.2589  0.063 ]\n",
      "MSE loss: 226.5202\n",
      "Iteration: 29100\n",
      "Gradient: [  160.4636   -58.9729  -286.3761  -348.1295 -1120.2273]\n",
      "Weights: [-3.8549 -1.3081 -0.3867  0.2592  0.0629]\n",
      "MSE loss: 226.1199\n",
      "Iteration: 29200\n",
      "Gradient: [142.6169 -37.5246 -47.7539 526.3206 879.9686]\n",
      "Weights: [-3.8567 -1.3075 -0.3863  0.2593  0.063 ]\n",
      "MSE loss: 225.7472\n",
      "Iteration: 29300\n",
      "Gradient: [ 204.4226  -68.5701 -184.7887  138.7838  674.4994]\n",
      "Weights: [-3.8585 -1.3067 -0.3858  0.2595  0.0627]\n",
      "MSE loss: 225.3487\n",
      "Iteration: 29400\n",
      "Gradient: [ 127.2398 -113.6495  -87.5941  398.2488  412.9241]\n",
      "Weights: [-3.8602 -1.306  -0.3855  0.259   0.0628]\n",
      "MSE loss: 224.9832\n",
      "Iteration: 29500\n",
      "Gradient: [ 189.0945 -130.8871    1.3693  252.013   990.6486]\n",
      "Weights: [-3.862  -1.3052 -0.3848  0.259   0.0628]\n",
      "MSE loss: 224.5807\n",
      "Iteration: 29600\n",
      "Gradient: [ 184.3983  -78.9773   47.2284  -55.0905 -174.7347]\n",
      "Weights: [-3.8637 -1.3045 -0.3844  0.2588  0.0628]\n",
      "MSE loss: 224.2052\n",
      "Iteration: 29700\n",
      "Gradient: [ 182.3976 -104.0544    9.5322  122.4402 -626.4277]\n",
      "Weights: [-3.8653 -1.3038 -0.384   0.2586  0.0626]\n",
      "MSE loss: 223.8808\n",
      "Iteration: 29800\n",
      "Gradient: [ 122.0979 -105.7539 -149.3551 -101.1336 -122.8095]\n",
      "Weights: [-3.8669 -1.303  -0.3836  0.2582  0.0628]\n",
      "MSE loss: 223.4922\n",
      "Iteration: 29900\n",
      "Gradient: [ 202.5831 -115.9338  -75.362    58.3272 -173.8685]\n",
      "Weights: [-3.8686 -1.3022 -0.3829  0.2583  0.0626]\n",
      "MSE loss: 223.1411\n",
      "Iteration: 30000\n",
      "Gradient: [ 200.4808  -97.0015  -26.5188  -78.7187 -442.4536]\n",
      "Weights: [-3.8703 -1.3015 -0.3824  0.2578  0.0627]\n",
      "MSE loss: 222.7995\n",
      "Iteration: 30100\n",
      "Gradient: [ 135.5743 -113.6596 -201.293  -260.6574  322.6571]\n",
      "Weights: [-3.8719 -1.3008 -0.3822  0.2573  0.063 ]\n",
      "MSE loss: 222.4267\n",
      "Iteration: 30200\n",
      "Gradient: [ 170.27    -47.4746  -53.0481 -154.987   468.3024]\n",
      "Weights: [-3.8735 -1.3    -0.3816  0.2573  0.0629]\n",
      "MSE loss: 222.0768\n",
      "Iteration: 30300\n",
      "Gradient: [ 214.3879 -118.3112 -120.8307  308.4011 -108.2025]\n",
      "Weights: [-3.8751 -1.2992 -0.3811  0.257   0.063 ]\n",
      "MSE loss: 221.7389\n",
      "Iteration: 30400\n",
      "Gradient: [  194.2896  -108.7655   -70.7643    44.4008 -1094.6347]\n",
      "Weights: [-3.8767 -1.2984 -0.3805  0.2564  0.0629]\n",
      "MSE loss: 221.4002\n",
      "Iteration: 30500\n",
      "Gradient: [ 141.2393 -103.2957  123.5445   32.3645  250.6223]\n",
      "Weights: [-3.8782 -1.2976 -0.3799  0.2569  0.0628]\n",
      "MSE loss: 221.039\n",
      "Iteration: 30600\n",
      "Gradient: [ 137.1126  -65.9619  -26.2335  418.6688 -957.2226]\n",
      "Weights: [-3.8798 -1.2968 -0.3793  0.2567  0.0628]\n",
      "MSE loss: 220.7033\n",
      "Iteration: 30700\n",
      "Gradient: [ 201.0048 -172.7382 -140.5222 -251.5198 -737.2831]\n",
      "Weights: [-3.8813 -1.296  -0.3789  0.2566  0.0625]\n",
      "MSE loss: 220.4516\n",
      "Iteration: 30800\n",
      "Gradient: [ 141.4548  -47.2979   57.7554  -97.1212 1227.5585]\n",
      "Weights: [-3.8829 -1.2952 -0.3785  0.2562  0.063 ]\n",
      "MSE loss: 220.1431\n",
      "Iteration: 30900\n",
      "Gradient: [ 127.5017 -148.2793  -12.6541 -252.7359 -110.0913]\n",
      "Weights: [-3.8844 -1.2945 -0.3781  0.2557  0.0627]\n",
      "MSE loss: 219.8627\n",
      "Iteration: 31000\n",
      "Gradient: [143.8202 -89.4629 101.1423 -57.1084 364.5638]\n",
      "Weights: [-3.8858 -1.2936 -0.3774  0.2557  0.0628]\n",
      "MSE loss: 219.4047\n",
      "Iteration: 31100\n",
      "Gradient: [ 182.1823 -143.095  -187.5287 -268.8934  164.2081]\n",
      "Weights: [-3.8873 -1.2927 -0.377   0.2554  0.063 ]\n",
      "MSE loss: 219.0942\n",
      "Iteration: 31200\n",
      "Gradient: [224.2169 -35.1013  47.033  -19.9215 130.925 ]\n",
      "Weights: [-3.8888 -1.2919 -0.3765  0.2553  0.0628]\n",
      "MSE loss: 218.7635\n",
      "Iteration: 31300\n",
      "Gradient: [  69.9103 -121.2644  -97.5315 -347.8631 -230.3861]\n",
      "Weights: [-3.8902 -1.2911 -0.376   0.2546  0.0628]\n",
      "MSE loss: 218.5077\n",
      "Iteration: 31400\n",
      "Gradient: [  149.465    -72.3247    -1.8179   148.5748 -1057.2684]\n",
      "Weights: [-3.8916 -1.2904 -0.3753  0.2547  0.0627]\n",
      "MSE loss: 218.1803\n",
      "Iteration: 31500\n",
      "Gradient: [130.3641 -30.5236  69.7737 429.855   68.9471]\n",
      "Weights: [-3.8931 -1.2895 -0.3748  0.2545  0.0629]\n",
      "MSE loss: 217.8397\n",
      "Iteration: 31600\n",
      "Gradient: [ 157.0415  -10.6138   82.2594  180.9846 -547.9726]\n",
      "Weights: [-3.8946 -1.2887 -0.3744  0.2547  0.0628]\n",
      "MSE loss: 217.5556\n",
      "Iteration: 31700\n",
      "Gradient: [ 102.3694  -42.869  -180.8632 -156.33     -2.0953]\n",
      "Weights: [-3.896  -1.288  -0.3741  0.2542  0.0629]\n",
      "MSE loss: 217.2498\n",
      "Iteration: 31800\n",
      "Gradient: [ 155.007   -18.725   -57.0113 -182.1314  285.5781]\n",
      "Weights: [-3.8974 -1.2871 -0.3735  0.2537  0.0631]\n",
      "MSE loss: 216.9648\n",
      "Iteration: 31900\n",
      "Gradient: [ 169.1499  -69.2517 -172.6242 -205.3616  182.4981]\n",
      "Weights: [-3.8988 -1.2863 -0.373   0.2535  0.0629]\n",
      "MSE loss: 216.645\n",
      "Iteration: 32000\n",
      "Gradient: [  124.6945   -89.0022  -134.3982  -159.9638 -1313.4832]\n",
      "Weights: [-3.9003 -1.2855 -0.3724  0.2529  0.0629]\n",
      "MSE loss: 216.3533\n",
      "Iteration: 32100\n",
      "Gradient: [ 172.2474 -123.7004 -151.1723 -158.46   -146.2511]\n",
      "Weights: [-3.9016 -1.2846 -0.3718  0.2524  0.0631]\n",
      "MSE loss: 216.0246\n",
      "Iteration: 32200\n",
      "Gradient: [ 128.2407 -144.1392 -186.1351  279.838   715.6909]\n",
      "Weights: [-3.903  -1.2838 -0.3712  0.2522  0.0631]\n",
      "MSE loss: 215.7371\n",
      "Iteration: 32300\n",
      "Gradient: [ 121.8254 -112.7431 -133.1848 -274.888    24.9597]\n",
      "Weights: [-3.9043 -1.2829 -0.3706  0.2519  0.0632]\n",
      "MSE loss: 215.4516\n",
      "Iteration: 32400\n",
      "Gradient: [ 139.8528 -105.0398  -91.1818 -136.3103 -200.9716]\n",
      "Weights: [-3.9057 -1.2822 -0.3701  0.2516  0.0632]\n",
      "MSE loss: 215.1663\n",
      "Iteration: 32500\n",
      "Gradient: [  74.2001 -143.6302  -85.9543  -39.8507 -365.8333]\n",
      "Weights: [-3.907  -1.2814 -0.3694  0.2513  0.0631]\n",
      "MSE loss: 214.8858\n",
      "Iteration: 32600\n",
      "Gradient: [172.9474 -28.1807   4.6954 346.5937 183.9502]\n",
      "Weights: [-3.9083 -1.2804 -0.3688  0.2513  0.063 ]\n",
      "MSE loss: 214.5992\n",
      "Iteration: 32700\n",
      "Gradient: [ 109.4107  -65.5676  -47.958  -120.2243  471.2034]\n",
      "Weights: [-3.9096 -1.2797 -0.3684  0.2511  0.0632]\n",
      "MSE loss: 214.3789\n",
      "Iteration: 32800\n",
      "Gradient: [ 150.3772  -29.8972  -65.0185 -454.9672 -654.4475]\n",
      "Weights: [-3.911  -1.2789 -0.3681  0.2508  0.0631]\n",
      "MSE loss: 214.0682\n",
      "Iteration: 32900\n",
      "Gradient: [ 123.5644  -17.5171  -21.176  -164.1712 -482.9765]\n",
      "Weights: [-3.9123 -1.2781 -0.3678  0.2503  0.0632]\n",
      "MSE loss: 213.8017\n",
      "Iteration: 33000\n",
      "Gradient: [185.6863 -44.8049  26.4605 303.926  131.4647]\n",
      "Weights: [-3.9136 -1.2773 -0.3671  0.2503  0.0632]\n",
      "MSE loss: 213.5375\n",
      "Iteration: 33100\n",
      "Gradient: [156.2687 -72.0309 159.3357  31.1015 590.8117]\n",
      "Weights: [-3.915  -1.2765 -0.3667  0.2496  0.0633]\n",
      "MSE loss: 213.2459\n",
      "Iteration: 33200\n",
      "Gradient: [ 144.2731 -134.5037  -62.412  -106.5087 -122.8557]\n",
      "Weights: [-3.9163 -1.2757 -0.3663  0.249   0.0633]\n",
      "MSE loss: 212.9941\n",
      "Iteration: 33300\n",
      "Gradient: [ 137.0653  -64.128  -154.6118  134.3989   20.0174]\n",
      "Weights: [-3.9175 -1.2749 -0.3659  0.2489  0.0634]\n",
      "MSE loss: 212.7192\n",
      "Iteration: 33400\n",
      "Gradient: [124.9301 -56.6423 -55.0073 -62.2552 358.1402]\n",
      "Weights: [-3.9188 -1.274  -0.3651  0.2488  0.0632]\n",
      "MSE loss: 212.4633\n",
      "Iteration: 33500\n",
      "Gradient: [ 155.3142  -25.5583   14.5303  160.1721 -302.1218]\n",
      "Weights: [-3.92   -1.2732 -0.3647  0.2485  0.0634]\n",
      "MSE loss: 212.1963\n",
      "Iteration: 33600\n",
      "Gradient: [  111.9837   -93.1595   -94.3128   -53.8978 -1486.0774]\n",
      "Weights: [-3.9212 -1.2724 -0.3644  0.2485  0.0632]\n",
      "MSE loss: 211.9616\n",
      "Iteration: 33700\n",
      "Gradient: [154.1567 -89.833  -52.0514 -48.7536 345.9191]\n",
      "Weights: [-3.9225 -1.2716 -0.3639  0.2478  0.0634]\n",
      "MSE loss: 211.6853\n",
      "Iteration: 33800\n",
      "Gradient: [ 156.9764  -77.9225  -57.8301 -460.4548 -297.8458]\n",
      "Weights: [-3.9237 -1.2707 -0.3636  0.2471  0.0638]\n",
      "MSE loss: 211.4312\n",
      "Iteration: 33900\n",
      "Gradient: [  76.7514 -128.3707 -154.2747 -278.5639 -298.1722]\n",
      "Weights: [-3.925  -1.2699 -0.3634  0.2472  0.0635]\n",
      "MSE loss: 211.209\n",
      "Iteration: 34000\n",
      "Gradient: [ 186.0821 -121.8999   21.033   188.0452  473.9336]\n",
      "Weights: [-3.9261 -1.269  -0.363   0.247   0.0636]\n",
      "MSE loss: 210.9576\n",
      "Iteration: 34100\n",
      "Gradient: [  56.5046    2.1321 -121.3197  368.0433  -36.0218]\n",
      "Weights: [-3.9273 -1.2682 -0.3626  0.2469  0.0636]\n",
      "MSE loss: 210.7282\n",
      "Iteration: 34200\n",
      "Gradient: [  88.7272 -107.6514 -163.0498 -128.5165 -374.3484]\n",
      "Weights: [-3.9285 -1.2674 -0.362   0.2469  0.0634]\n",
      "MSE loss: 210.4893\n",
      "Iteration: 34300\n",
      "Gradient: [ 118.6991 -128.0649   49.4212  -56.7771   87.8855]\n",
      "Weights: [-3.9298 -1.2667 -0.3617  0.2469  0.0634]\n",
      "MSE loss: 210.2647\n",
      "Iteration: 34400\n",
      "Gradient: [125.4067 -76.9491 104.8416 168.0528 362.3807]\n",
      "Weights: [-3.931  -1.2659 -0.3614  0.2463  0.0636]\n",
      "MSE loss: 210.0236\n",
      "Iteration: 34500\n",
      "Gradient: [  149.0043     6.0811   -77.647   -290.1194 -1028.4539]\n",
      "Weights: [-3.9322 -1.2652 -0.361   0.2455  0.0638]\n",
      "MSE loss: 209.7515\n",
      "Iteration: 34600\n",
      "Gradient: [ 192.0985  -73.6659 -162.9229  359.8105  711.9014]\n",
      "Weights: [-3.9334 -1.2643 -0.3606  0.245   0.064 ]\n",
      "MSE loss: 209.5055\n",
      "Iteration: 34700\n",
      "Gradient: [ 185.9721 -153.7603  -90.4678  381.1465 -176.1002]\n",
      "Weights: [-3.9346 -1.2635 -0.3601  0.2448  0.0639]\n",
      "MSE loss: 209.2773\n",
      "Iteration: 34800\n",
      "Gradient: [ 151.88    -24.5833  106.9075  117.4725 -404.0651]\n",
      "Weights: [-3.9357 -1.2627 -0.3597  0.2447  0.0639]\n",
      "MSE loss: 209.0683\n",
      "Iteration: 34900\n",
      "Gradient: [  75.8812 -129.8218 -113.1227 -313.0534  -16.1972]\n",
      "Weights: [-3.9369 -1.2619 -0.3595  0.2435  0.0642]\n",
      "MSE loss: 208.7915\n",
      "Iteration: 35000\n",
      "Gradient: [ 116.6468 -123.8441  -90.6928  -83.9923 -910.787 ]\n",
      "Weights: [-3.9381 -1.261  -0.359   0.2433  0.0642]\n",
      "MSE loss: 208.5511\n",
      "Iteration: 35100\n",
      "Gradient: [  88.4394  -31.5659  -87.1154 -182.7213  845.8583]\n",
      "Weights: [-3.9392 -1.2601 -0.3586  0.2431  0.0642]\n",
      "MSE loss: 208.3163\n",
      "Iteration: 35200\n",
      "Gradient: [131.8281 -81.0518 -60.4202 -48.5774 -13.2177]\n",
      "Weights: [-3.9404 -1.2593 -0.358   0.243   0.0641]\n",
      "MSE loss: 208.093\n",
      "Iteration: 35300\n",
      "Gradient: [ 118.9569 -132.1637   19.6201  -16.9488 -177.6343]\n",
      "Weights: [-3.9415 -1.2585 -0.3576  0.2427  0.0642]\n",
      "MSE loss: 207.8577\n",
      "Iteration: 35400\n",
      "Gradient: [  130.56    -121.8247   -78.0482  -323.0195 -1155.525 ]\n",
      "Weights: [-3.9426 -1.2576 -0.3571  0.2421  0.0643]\n",
      "MSE loss: 207.6166\n",
      "Iteration: 35500\n",
      "Gradient: [ 160.548   -79.4155   32.7812  260.9867 -646.8498]\n",
      "Weights: [-3.9437 -1.2568 -0.3566  0.2416  0.0644]\n",
      "MSE loss: 207.3845\n",
      "Iteration: 35600\n",
      "Gradient: [ 77.7293  12.4939 112.4355 367.4618 461.8469]\n",
      "Weights: [-3.9448 -1.2559 -0.3561  0.2414  0.0644]\n",
      "MSE loss: 207.1628\n",
      "Iteration: 35700\n",
      "Gradient: [ 107.2731  -56.0848 -184.0451  163.5997 -474.7853]\n",
      "Weights: [-3.9459 -1.2552 -0.3559  0.241   0.0645]\n",
      "MSE loss: 206.9458\n",
      "Iteration: 35800\n",
      "Gradient: [159.9328 -89.4958 -52.679  165.8934 500.5678]\n",
      "Weights: [-3.947  -1.2543 -0.3554  0.2405  0.0646]\n",
      "MSE loss: 206.7111\n",
      "Iteration: 35900\n",
      "Gradient: [ 100.1068  -94.3879   30.8807   73.8424 -245.4005]\n",
      "Weights: [-3.9481 -1.2535 -0.3551  0.24    0.0648]\n",
      "MSE loss: 206.504\n",
      "Iteration: 36000\n",
      "Gradient: [ 119.919   -36.6435   15.4671   84.3414 1138.8236]\n",
      "Weights: [-3.9492 -1.2526 -0.3548  0.2392  0.0651]\n",
      "MSE loss: 206.3207\n",
      "Iteration: 36100\n",
      "Gradient: [  94.4445 -123.2332  -80.2113  -52.456   -36.7261]\n",
      "Weights: [-3.9502 -1.2517 -0.3543  0.2389  0.065 ]\n",
      "MSE loss: 206.0306\n",
      "Iteration: 36200\n",
      "Gradient: [ 107.5198   11.7036  -85.3911  191.0111 -183.9512]\n",
      "Weights: [-3.9512 -1.2508 -0.354   0.2386  0.0651]\n",
      "MSE loss: 205.8404\n",
      "Iteration: 36300\n",
      "Gradient: [  89.8709 -102.7796 -289.2249   50.0507  255.8551]\n",
      "Weights: [-3.9522 -1.25   -0.3535  0.2381  0.0651]\n",
      "MSE loss: 205.599\n",
      "Iteration: 36400\n",
      "Gradient: [ 100.5889  -28.8304 -289.5866  187.9583  -18.1164]\n",
      "Weights: [-3.9532 -1.2491 -0.3531  0.2377  0.0652]\n",
      "MSE loss: 205.3788\n",
      "Iteration: 36500\n",
      "Gradient: [ 99.551  -85.0384  84.9103 -34.911  144.2937]\n",
      "Weights: [-3.9543 -1.2483 -0.3526  0.2373  0.0652]\n",
      "MSE loss: 205.1631\n",
      "Iteration: 36600\n",
      "Gradient: [  91.9319 -142.2051 -226.3634 -283.3304 -623.7314]\n",
      "Weights: [-3.9553 -1.2474 -0.3523  0.2368  0.0653]\n",
      "MSE loss: 204.9477\n",
      "Iteration: 36700\n",
      "Gradient: [119.3845 -73.8019 -39.2094 313.0501 192.2906]\n",
      "Weights: [-3.9563 -1.2465 -0.3518  0.2364  0.0653]\n",
      "MSE loss: 204.7331\n",
      "Iteration: 36800\n",
      "Gradient: [ 106.3409  -77.5994   25.8876   29.1006 -190.3998]\n",
      "Weights: [-3.9573 -1.2456 -0.3515  0.2363  0.0655]\n",
      "MSE loss: 204.5554\n",
      "Iteration: 36900\n",
      "Gradient: [132.3659 -80.9548   9.1576  -0.4001 107.5528]\n",
      "Weights: [-3.9583 -1.2446 -0.351   0.2363  0.0652]\n",
      "MSE loss: 204.3281\n",
      "Iteration: 37000\n",
      "Gradient: [ 139.2601 -143.9413 -175.3153   78.8593 -971.89  ]\n",
      "Weights: [-3.9594 -1.2438 -0.3506  0.2357  0.0654]\n",
      "MSE loss: 204.1012\n",
      "Iteration: 37100\n",
      "Gradient: [  62.8103  -52.7506 -143.2688  -36.8449 -321.0634]\n",
      "Weights: [-3.9604 -1.2429 -0.3501  0.2357  0.0652]\n",
      "MSE loss: 203.9081\n",
      "Iteration: 37200\n",
      "Gradient: [ 86.0982 -77.351   22.2961   3.3786 722.6849]\n",
      "Weights: [-3.9615 -1.2422 -0.3499  0.2349  0.0656]\n",
      "MSE loss: 203.6962\n",
      "Iteration: 37300\n",
      "Gradient: [  76.867  -150.2541 -175.2089  -64.6364 -409.0489]\n",
      "Weights: [-3.9625 -1.2413 -0.3494  0.2349  0.0654]\n",
      "MSE loss: 203.4999\n",
      "Iteration: 37400\n",
      "Gradient: [  54.8607 -137.4615  113.8399  169.6577 -892.6325]\n",
      "Weights: [-3.9636 -1.2404 -0.3491  0.2345  0.0656]\n",
      "MSE loss: 203.2802\n",
      "Iteration: 37500\n",
      "Gradient: [  94.571   -35.742   122.4148  375.3135 -707.6748]\n",
      "Weights: [-3.9646 -1.2395 -0.3489  0.2344  0.0655]\n",
      "MSE loss: 203.0862\n",
      "Iteration: 37600\n",
      "Gradient: [  78.3313 -102.0863   -2.3761  267.3667  -80.0307]\n",
      "Weights: [-3.9656 -1.2388 -0.3486  0.2338  0.0656]\n",
      "MSE loss: 202.8925\n",
      "Iteration: 37700\n",
      "Gradient: [  54.5491  -85.8255   -8.8834 -322.2154 -273.6996]\n",
      "Weights: [-3.9666 -1.238  -0.3483  0.2333  0.0658]\n",
      "MSE loss: 202.6821\n",
      "Iteration: 37800\n",
      "Gradient: [  87.4809  -80.4128  -27.3749 -391.5199 -239.888 ]\n",
      "Weights: [-3.9675 -1.2372 -0.3479  0.2333  0.0657]\n",
      "MSE loss: 202.5067\n",
      "Iteration: 37900\n",
      "Gradient: [103.1433 -33.5905  52.3369 -19.481  -47.0705]\n",
      "Weights: [-3.9685 -1.2364 -0.3474  0.2328  0.0658]\n",
      "MSE loss: 202.3043\n",
      "Iteration: 38000\n",
      "Gradient: [  49.4363 -172.202  -170.2996  178.9902   -3.4526]\n",
      "Weights: [-3.9694 -1.2355 -0.3471  0.2323  0.0659]\n",
      "MSE loss: 202.1045\n",
      "Iteration: 38100\n",
      "Gradient: [ 158.7718  -60.791  -228.4496  353.3587 -765.6203]\n",
      "Weights: [-3.9704 -1.2347 -0.3466  0.2322  0.0659]\n",
      "MSE loss: 201.9221\n",
      "Iteration: 38200\n",
      "Gradient: [  113.7146  -104.5667   -20.621   -198.1926 -1213.6616]\n",
      "Weights: [-3.9714 -1.2339 -0.3464  0.2317  0.0659]\n",
      "MSE loss: 201.7284\n",
      "Iteration: 38300\n",
      "Gradient: [  77.6028 -157.4898    0.7158  201.4807   81.2793]\n",
      "Weights: [-3.9723 -1.233  -0.3462  0.2312  0.0661]\n",
      "MSE loss: 201.5208\n",
      "Iteration: 38400\n",
      "Gradient: [ 92.3568 -41.7907 -37.8737 133.3442 247.5143]\n",
      "Weights: [-3.9733 -1.2322 -0.346   0.23    0.0665]\n",
      "MSE loss: 201.289\n",
      "Iteration: 38500\n",
      "Gradient: [ 135.9941  -77.9979  223.0531  -31.9287 1163.7366]\n",
      "Weights: [-3.9742 -1.2313 -0.3457  0.2298  0.0667]\n",
      "MSE loss: 201.1311\n",
      "Iteration: 38600\n",
      "Gradient: [  56.74   -143.5435  -75.9933  477.7709  124.7669]\n",
      "Weights: [-3.9751 -1.2305 -0.3452  0.2294  0.0665]\n",
      "MSE loss: 200.9119\n",
      "Iteration: 38700\n",
      "Gradient: [  97.6368  -77.3941   77.444   355.7166 -166.7889]\n",
      "Weights: [-3.9761 -1.2296 -0.3448  0.2288  0.0667]\n",
      "MSE loss: 200.6989\n",
      "Iteration: 38800\n",
      "Gradient: [  45.3869  -93.4742  110.2855 -526.4429 1222.8662]\n",
      "Weights: [-3.977  -1.2288 -0.3444  0.2282  0.0669]\n",
      "MSE loss: 200.5009\n",
      "Iteration: 38900\n",
      "Gradient: [ 111.2897  -79.978  -127.6138  -37.1235 -255.495 ]\n",
      "Weights: [-3.9779 -1.2279 -0.3442  0.2275  0.067 ]\n",
      "MSE loss: 200.3036\n",
      "Iteration: 39000\n",
      "Gradient: [ 77.6389 -17.8671  -9.9509 361.4495 -11.4065]\n",
      "Weights: [-3.9788 -1.227  -0.3437  0.2267  0.0672]\n",
      "MSE loss: 200.0771\n",
      "Iteration: 39100\n",
      "Gradient: [ 108.5345  -91.9667  -72.0909  129.0156 -776.2967]\n",
      "Weights: [-3.9797 -1.2261 -0.3432  0.2262  0.0673]\n",
      "MSE loss: 199.8833\n",
      "Iteration: 39200\n",
      "Gradient: [  33.2618 -108.3038  -61.3091   -4.3691  160.6067]\n",
      "Weights: [-3.9806 -1.2252 -0.3427  0.2258  0.0675]\n",
      "MSE loss: 199.6763\n",
      "Iteration: 39300\n",
      "Gradient: [ 124.5188 -110.4635  -74.5391  199.5409   23.4741]\n",
      "Weights: [-3.9815 -1.2243 -0.3424  0.2251  0.0676]\n",
      "MSE loss: 199.4629\n",
      "Iteration: 39400\n",
      "Gradient: [  99.8306   -4.2902   76.0727  498.0091 -135.4811]\n",
      "Weights: [-3.9823 -1.2234 -0.3418  0.2247  0.0678]\n",
      "MSE loss: 199.2878\n",
      "Iteration: 39500\n",
      "Gradient: [  61.2138  -90.3409    0.994   125.6287 -278.4021]\n",
      "Weights: [-3.9832 -1.2225 -0.3415  0.2243  0.0677]\n",
      "MSE loss: 199.0815\n",
      "Iteration: 39600\n",
      "Gradient: [ 110.3258  -51.2817  113.4901    6.873  -578.75  ]\n",
      "Weights: [-3.9841 -1.2216 -0.3413  0.2236  0.0679]\n",
      "MSE loss: 198.8783\n",
      "Iteration: 39700\n",
      "Gradient: [  54.9632  -59.6602 -133.8046 -284.6775  555.9544]\n",
      "Weights: [-3.9849 -1.2207 -0.3409  0.223   0.0682]\n",
      "MSE loss: 198.7217\n",
      "Iteration: 39800\n",
      "Gradient: [  85.3325  -15.4716   61.7431  391.7234 -340.4453]\n",
      "Weights: [-3.9858 -1.2198 -0.3406  0.2226  0.0682]\n",
      "MSE loss: 198.5054\n",
      "Iteration: 39900\n",
      "Gradient: [ 74.1294   1.6018 -60.3394 471.4165 722.0474]\n",
      "Weights: [-3.9867 -1.2189 -0.3403  0.222   0.0684]\n",
      "MSE loss: 198.3114\n",
      "Iteration: 40000\n",
      "Gradient: [  51.1598 -134.3029  -54.5757  -44.3361   84.4703]\n",
      "Weights: [-3.9875 -1.2181 -0.3401  0.2218  0.0682]\n",
      "MSE loss: 198.1513\n",
      "Iteration: 40100\n",
      "Gradient: [109.6974 -85.8307  17.5715  92.7905 525.1278]\n",
      "Weights: [-3.9884 -1.2172 -0.3397  0.2216  0.0682]\n",
      "MSE loss: 197.9621\n",
      "Iteration: 40200\n",
      "Gradient: [102.5708 -77.5033  76.7303 569.5609 932.6153]\n",
      "Weights: [-3.9892 -1.2163 -0.3394  0.2213  0.0684]\n",
      "MSE loss: 197.7845\n",
      "Iteration: 40300\n",
      "Gradient: [  91.8313 -136.5321  -69.9583 -201.1434  308.9333]\n",
      "Weights: [-3.9901 -1.2154 -0.3393  0.2205  0.0685]\n",
      "MSE loss: 197.5824\n",
      "Iteration: 40400\n",
      "Gradient: [109.3773 -37.8449  61.2468 503.9263 200.7117]\n",
      "Weights: [-3.9909 -1.2144 -0.339   0.22    0.0688]\n",
      "MSE loss: 197.3999\n",
      "Iteration: 40500\n",
      "Gradient: [ 111.9722  -74.9037   70.6025 -106.4712 -968.8815]\n",
      "Weights: [-3.9917 -1.2134 -0.3386  0.2197  0.0686]\n",
      "MSE loss: 197.219\n",
      "Iteration: 40600\n",
      "Gradient: [  82.2789  -92.0515  -93.2281   68.1435 -620.3407]\n",
      "Weights: [-3.9925 -1.2125 -0.3383  0.2194  0.0687]\n",
      "MSE loss: 197.0382\n",
      "Iteration: 40700\n",
      "Gradient: [ 75.928  -69.4707  10.7347 -99.5999 322.2179]\n",
      "Weights: [-3.9934 -1.2116 -0.3378  0.2191  0.0687]\n",
      "MSE loss: 196.8563\n",
      "Iteration: 40800\n",
      "Gradient: [   16.0652  -140.8348  -136.2876  -281.805  -1339.5392]\n",
      "Weights: [-3.9942 -1.2107 -0.3374  0.2188  0.0688]\n",
      "MSE loss: 196.6695\n",
      "Iteration: 40900\n",
      "Gradient: [ 105.7873 -106.8178  174.8509    4.9302 1414.0857]\n",
      "Weights: [-3.995  -1.2098 -0.3371  0.2184  0.069 ]\n",
      "MSE loss: 196.5323\n",
      "Iteration: 41000\n",
      "Gradient: [ 116.9204 -119.0483 -236.8582 -125.2797  459.0697]\n",
      "Weights: [-3.9959 -1.209  -0.3368  0.2176  0.069 ]\n",
      "MSE loss: 196.2968\n",
      "Iteration: 41100\n",
      "Gradient: [ 122.6181 -131.2363  -62.7625  -98.562   769.8323]\n",
      "Weights: [-3.9967 -1.2082 -0.3363  0.2172  0.0691]\n",
      "MSE loss: 196.1127\n",
      "Iteration: 41200\n",
      "Gradient: [  83.1237  -77.9657  -35.1544 -189.5343 -129.2161]\n",
      "Weights: [-3.9975 -1.2072 -0.336   0.2167  0.0693]\n",
      "MSE loss: 195.935\n",
      "Iteration: 41300\n",
      "Gradient: [  44.1788 -110.4834   53.2048  204.1817   97.3491]\n",
      "Weights: [-3.9983 -1.2063 -0.3356  0.2165  0.0692]\n",
      "MSE loss: 195.7553\n",
      "Iteration: 41400\n",
      "Gradient: [ 141.7183 -109.8006  -45.4493 -228.6096  148.8923]\n",
      "Weights: [-3.9992 -1.2054 -0.3353  0.2161  0.0694]\n",
      "MSE loss: 195.5912\n",
      "Iteration: 41500\n",
      "Gradient: [ 46.4627 -48.5884   3.7825 115.8743 530.8918]\n",
      "Weights: [-4.     -1.2046 -0.3351  0.2156  0.0694]\n",
      "MSE loss: 195.4085\n",
      "Iteration: 41600\n",
      "Gradient: [ 63.5207 -77.0678 -62.8819 162.0256 707.2952]\n",
      "Weights: [-4.0007 -1.2038 -0.3348  0.2151  0.0696]\n",
      "MSE loss: 195.2461\n",
      "Iteration: 41700\n",
      "Gradient: [  58.2853 -147.476   -85.8547  -48.4464 -860.437 ]\n",
      "Weights: [-4.0015 -1.203  -0.3345  0.2146  0.0696]\n",
      "MSE loss: 195.07\n",
      "Iteration: 41800\n",
      "Gradient: [  49.9981 -172.4492  -26.954   112.5005 -468.7644]\n",
      "Weights: [-4.0023 -1.2021 -0.3339  0.2135  0.0698]\n",
      "MSE loss: 194.8654\n",
      "Iteration: 41900\n",
      "Gradient: [  82.9284 -106.1252   25.8902   -0.8008 -664.3677]\n",
      "Weights: [-4.0031 -1.2013 -0.3336  0.213   0.07  ]\n",
      "MSE loss: 194.6792\n",
      "Iteration: 42000\n",
      "Gradient: [  77.2292  -83.0119 -103.0064  191.6951  969.6059]\n",
      "Weights: [-4.0039 -1.2004 -0.3333  0.2129  0.0701]\n",
      "MSE loss: 194.5198\n",
      "Iteration: 42100\n",
      "Gradient: [   40.0948    -7.1534   -45.4917   -63.8308 -1395.3579]\n",
      "Weights: [-4.0047 -1.1996 -0.333   0.2125  0.0701]\n",
      "MSE loss: 194.3567\n",
      "Iteration: 42200\n",
      "Gradient: [  72.7009 -118.9847 -104.3395  250.3711 -453.7471]\n",
      "Weights: [-4.0055 -1.1987 -0.3326  0.2117  0.0703]\n",
      "MSE loss: 194.1572\n",
      "Iteration: 42300\n",
      "Gradient: [ 33.5596   8.8247 -61.4662   1.7219 349.6951]\n",
      "Weights: [-4.0063 -1.1978 -0.3323  0.2111  0.0705]\n",
      "MSE loss: 193.9734\n",
      "Iteration: 42400\n",
      "Gradient: [  80.2051  -76.0215 -126.0622 -141.2263 -157.1285]\n",
      "Weights: [-4.0071 -1.197  -0.3321  0.2105  0.0706]\n",
      "MSE loss: 193.8098\n",
      "Iteration: 42500\n",
      "Gradient: [  96.2706 -114.6      -7.147  -257.8845  654.6721]\n",
      "Weights: [-4.0078 -1.1961 -0.3318  0.2098  0.0709]\n",
      "MSE loss: 193.6196\n",
      "Iteration: 42600\n",
      "Gradient: [  95.5942 -145.6378  -49.3062   -7.3978  134.0727]\n",
      "Weights: [-4.0086 -1.1952 -0.3316  0.2089  0.0711]\n",
      "MSE loss: 193.4164\n",
      "Iteration: 42700\n",
      "Gradient: [ 63.0609 -85.2786 -73.7119  88.4596 322.2967]\n",
      "Weights: [-4.0094 -1.1944 -0.3311  0.2084  0.0712]\n",
      "MSE loss: 193.2349\n",
      "Iteration: 42800\n",
      "Gradient: [ 41.9605 -63.8209  45.2944 247.5756 425.4543]\n",
      "Weights: [-4.0101 -1.1934 -0.3308  0.2082  0.0712]\n",
      "MSE loss: 193.0733\n",
      "Iteration: 42900\n",
      "Gradient: [  75.9563  -93.2702  -47.0044  274.1283 -766.399 ]\n",
      "Weights: [-4.0108 -1.1926 -0.3305  0.2079  0.0712]\n",
      "MSE loss: 192.9318\n",
      "Iteration: 43000\n",
      "Gradient: [ 49.8907 -56.6045  65.9585 139.2542 427.3086]\n",
      "Weights: [-4.0115 -1.1917 -0.3301  0.2072  0.0716]\n",
      "MSE loss: 192.7961\n",
      "Iteration: 43100\n",
      "Gradient: [  55.5862 -120.9393  -96.4518  222.4691  -53.6681]\n",
      "Weights: [-4.0123 -1.1908 -0.3297  0.2064  0.0716]\n",
      "MSE loss: 192.5488\n",
      "Iteration: 43200\n",
      "Gradient: [ 79.7514 -48.8439 133.9448 263.4643 573.8494]\n",
      "Weights: [-4.013  -1.1899 -0.3295  0.206   0.0717]\n",
      "MSE loss: 192.3879\n",
      "Iteration: 43300\n",
      "Gradient: [ 91.9601 -55.2425 -28.7142 276.1449  75.7092]\n",
      "Weights: [-4.0137 -1.189  -0.3291  0.2058  0.0716]\n",
      "MSE loss: 192.2434\n",
      "Iteration: 43400\n",
      "Gradient: [  44.3698 -133.3615 -203.9258 -264.8559 1087.0995]\n",
      "Weights: [-4.0144 -1.1881 -0.3288  0.2052  0.0719]\n",
      "MSE loss: 192.0533\n",
      "Iteration: 43500\n",
      "Gradient: [  71.9536  -64.3178  118.1772  323.7899 -981.561 ]\n",
      "Weights: [-4.0152 -1.1873 -0.3286  0.2045  0.072 ]\n",
      "MSE loss: 191.8782\n",
      "Iteration: 43600\n",
      "Gradient: [   48.9722   -68.3572    82.9786   269.5135 -1413.073 ]\n",
      "Weights: [-4.0159 -1.1864 -0.3283  0.2041  0.0722]\n",
      "MSE loss: 191.7116\n",
      "Iteration: 43700\n",
      "Gradient: [ 144.4665  -97.0862   63.7898 -213.2631   39.3696]\n",
      "Weights: [-4.0165 -1.1855 -0.328   0.2036  0.0722]\n",
      "MSE loss: 191.5511\n",
      "Iteration: 43800\n",
      "Gradient: [ 111.0005  -46.5997 -111.707  -210.2085 -543.8053]\n",
      "Weights: [-4.0173 -1.1847 -0.3278  0.2029  0.0724]\n",
      "MSE loss: 191.3767\n",
      "Iteration: 43900\n",
      "Gradient: [  25.6216  -83.9775  -60.3295   60.5454 -372.1311]\n",
      "Weights: [-4.018  -1.1838 -0.3273  0.2028  0.0723]\n",
      "MSE loss: 191.2364\n",
      "Iteration: 44000\n",
      "Gradient: [  30.338   -92.0772 -100.3191 -320.2402  185.4106]\n",
      "Weights: [-4.0188 -1.1829 -0.3269  0.2024  0.0723]\n",
      "MSE loss: 191.0833\n",
      "Iteration: 44100\n",
      "Gradient: [ 86.7353 -64.4193  -1.545  -48.7665 460.9051]\n",
      "Weights: [-4.0195 -1.182  -0.3264  0.2023  0.0724]\n",
      "MSE loss: 190.9005\n",
      "Iteration: 44200\n",
      "Gradient: [  76.5721 -165.7983 -170.6279  157.3424 -339.9392]\n",
      "Weights: [-4.0202 -1.1811 -0.3262  0.2022  0.0723]\n",
      "MSE loss: 190.772\n",
      "Iteration: 44300\n",
      "Gradient: [   48.2498  -103.3883  -106.5475  -215.37   -1009.2131]\n",
      "Weights: [-4.0209 -1.1803 -0.326   0.2013  0.0725]\n",
      "MSE loss: 190.6151\n",
      "Iteration: 44400\n",
      "Gradient: [  44.2661 -119.2379 -287.7616  251.2023 -292.3897]\n",
      "Weights: [-4.0216 -1.1795 -0.3259  0.2007  0.0727]\n",
      "MSE loss: 190.4204\n",
      "Iteration: 44500\n",
      "Gradient: [ 104.3844  -69.1654  -21.4253  -72.0286 -654.5461]\n",
      "Weights: [-4.0223 -1.1786 -0.3255  0.2002  0.0728]\n",
      "MSE loss: 190.2607\n",
      "Iteration: 44600\n",
      "Gradient: [   20.3515  -119.8076   -82.5195  -249.1321 -1322.1336]\n",
      "Weights: [-4.0231 -1.1777 -0.3253  0.2     0.0728]\n",
      "MSE loss: 190.1211\n",
      "Iteration: 44700\n",
      "Gradient: [ 81.6552 -65.7572 213.559  156.3713 802.2392]\n",
      "Weights: [-4.0238 -1.1769 -0.3251  0.2     0.0729]\n",
      "MSE loss: 189.9965\n",
      "Iteration: 44800\n",
      "Gradient: [  61.2067 -106.5417  -17.8248  218.3098  634.4323]\n",
      "Weights: [-4.0245 -1.176  -0.3249  0.1994  0.073 ]\n",
      "MSE loss: 189.8037\n",
      "Iteration: 44900\n",
      "Gradient: [  71.8687 -105.4558 -156.2083 -559.2411 -677.3005]\n",
      "Weights: [-4.0252 -1.1751 -0.3246  0.1991  0.073 ]\n",
      "MSE loss: 189.6554\n",
      "Iteration: 45000\n",
      "Gradient: [ 55.6018 -90.8557 -15.3231 409.5582 263.4663]\n",
      "Weights: [-4.026  -1.1743 -0.3243  0.1987  0.0731]\n",
      "MSE loss: 189.4957\n",
      "Iteration: 45100\n",
      "Gradient: [ 102.6663  -53.5523    3.2716 -281.3039 -232.17  ]\n",
      "Weights: [-4.0267 -1.1735 -0.324   0.1983  0.0732]\n",
      "MSE loss: 189.3485\n",
      "Iteration: 45200\n",
      "Gradient: [ 23.3332 -43.9661  75.3027 170.2966 471.3947]\n",
      "Weights: [-4.0275 -1.1726 -0.3237  0.198   0.0732]\n",
      "MSE loss: 189.1858\n",
      "Iteration: 45300\n",
      "Gradient: [ 104.5966  -88.5524  -78.1208  231.8687 -119.8317]\n",
      "Weights: [-4.0282 -1.1717 -0.3235  0.1974  0.0734]\n",
      "MSE loss: 189.0199\n",
      "Iteration: 45400\n",
      "Gradient: [ 102.7608 -117.7394  -47.4023  -61.6614 -112.6429]\n",
      "Weights: [-4.0289 -1.1709 -0.3234  0.1969  0.0735]\n",
      "MSE loss: 188.8614\n",
      "Iteration: 45500\n",
      "Gradient: [ 100.2639 -108.892    22.976   238.1405 -360.583 ]\n",
      "Weights: [-4.0296 -1.17   -0.3232  0.1962  0.0738]\n",
      "MSE loss: 188.7116\n",
      "Iteration: 45600\n",
      "Gradient: [ 40.6346 -93.8282  32.8935 138.0672 504.4444]\n",
      "Weights: [-4.0303 -1.1692 -0.323   0.1955  0.0739]\n",
      "MSE loss: 188.5205\n",
      "Iteration: 45700\n",
      "Gradient: [   60.8908   -56.2196    47.6878   -83.8812 -1246.0118]\n",
      "Weights: [-4.031  -1.1684 -0.3227  0.195   0.074 ]\n",
      "MSE loss: 188.3706\n",
      "Iteration: 45800\n",
      "Gradient: [ 74.4172 -39.0858  61.0696 163.7234 850.5776]\n",
      "Weights: [-4.0317 -1.1675 -0.3223  0.1946  0.0741]\n",
      "MSE loss: 188.2146\n",
      "Iteration: 45900\n",
      "Gradient: [  82.8335  -36.9183  -73.79     55.9059 -802.9861]\n",
      "Weights: [-4.0324 -1.1667 -0.3221  0.1939  0.0743]\n",
      "MSE loss: 188.0468\n",
      "Iteration: 46000\n",
      "Gradient: [  78.0693 -122.0525 -103.8914   66.1797  715.7356]\n",
      "Weights: [-4.033  -1.1659 -0.3217  0.1934  0.0745]\n",
      "MSE loss: 187.8868\n",
      "Iteration: 46100\n",
      "Gradient: [  83.9596  -36.2977   88.6013  427.8101 -886.7784]\n",
      "Weights: [-4.0337 -1.165  -0.3212  0.1934  0.0743]\n",
      "MSE loss: 187.7599\n",
      "Iteration: 46200\n",
      "Gradient: [  45.6947  -91.6509  -99.8409 -316.9808  365.0961]\n",
      "Weights: [-4.0344 -1.1642 -0.321   0.1926  0.0746]\n",
      "MSE loss: 187.5822\n",
      "Iteration: 46300\n",
      "Gradient: [  73.7861  -57.9113   74.8369   33.1374 -720.8117]\n",
      "Weights: [-4.0351 -1.1633 -0.3205  0.1918  0.0749]\n",
      "MSE loss: 187.4069\n",
      "Iteration: 46400\n",
      "Gradient: [ 8.069230e+01 -7.500390e+01 -4.258000e-01  1.179372e+02 -5.469696e+02]\n",
      "Weights: [-4.0358 -1.1625 -0.3202  0.1909  0.075 ]\n",
      "MSE loss: 187.2186\n",
      "Iteration: 46500\n",
      "Gradient: [  93.6424 -121.6135   -1.9539  175.7629  974.3845]\n",
      "Weights: [-4.0364 -1.1615 -0.3198  0.1902  0.0752]\n",
      "MSE loss: 187.0336\n",
      "Iteration: 46600\n",
      "Gradient: [   9.1984  -31.5227 -124.3864  -85.2423  673.3129]\n",
      "Weights: [-4.0371 -1.1607 -0.3195  0.1897  0.0753]\n",
      "MSE loss: 186.8787\n",
      "Iteration: 46700\n",
      "Gradient: [  59.8101 -196.02    -44.1866  258.8775  273.9204]\n",
      "Weights: [-4.0377 -1.1598 -0.3192  0.1897  0.0751]\n",
      "MSE loss: 186.7755\n",
      "Iteration: 46800\n",
      "Gradient: [116.5776 -79.1364  16.3147 324.3957 443.0097]\n",
      "Weights: [-4.0383 -1.159  -0.319   0.1894  0.0753]\n",
      "MSE loss: 186.621\n",
      "Iteration: 46900\n",
      "Gradient: [ 65.0098 -50.8741 -57.9581 294.5954 414.61  ]\n",
      "Weights: [-4.039  -1.1582 -0.3186  0.1891  0.0754]\n",
      "MSE loss: 186.4828\n",
      "Iteration: 47000\n",
      "Gradient: [ 95.8484 -85.4516 170.3353 158.607  556.7235]\n",
      "Weights: [-4.0397 -1.1574 -0.3184  0.1887  0.0756]\n",
      "MSE loss: 186.3689\n",
      "Iteration: 47100\n",
      "Gradient: [  39.5097  -92.2616 -210.6976   93.9303  136.1534]\n",
      "Weights: [-4.0404 -1.1565 -0.3183  0.188   0.0756]\n",
      "MSE loss: 186.1774\n",
      "Iteration: 47200\n",
      "Gradient: [  71.0709  -84.1158   99.4492  -15.9037 -765.6778]\n",
      "Weights: [-4.041  -1.1557 -0.318   0.1874  0.0758]\n",
      "MSE loss: 186.0169\n",
      "Iteration: 47300\n",
      "Gradient: [  40.5112  -61.2558  -90.3673  -59.4515 -672.8726]\n",
      "Weights: [-4.0417 -1.1549 -0.3179  0.1872  0.0758]\n",
      "MSE loss: 185.8814\n",
      "Iteration: 47400\n",
      "Gradient: [108.9098 -53.3525  74.4519 185.9964 224.6705]\n",
      "Weights: [-4.0424 -1.154  -0.3177  0.1865  0.076 ]\n",
      "MSE loss: 185.7191\n",
      "Iteration: 47500\n",
      "Gradient: [  35.1337  -88.9466  -18.3143 -245.8001 -651.1308]\n",
      "Weights: [-4.043  -1.1532 -0.3173  0.1856  0.0762]\n",
      "MSE loss: 185.5359\n",
      "Iteration: 47600\n",
      "Gradient: [  42.8253  -66.3455   97.6828 -258.9657  999.9875]\n",
      "Weights: [-4.0436 -1.1523 -0.317   0.1854  0.0763]\n",
      "MSE loss: 185.4045\n",
      "Iteration: 47700\n",
      "Gradient: [111.9326 -99.7123  75.6732  54.1729  22.5148]\n",
      "Weights: [-4.0443 -1.1514 -0.3167  0.1852  0.0763]\n",
      "MSE loss: 185.2787\n",
      "Iteration: 47800\n",
      "Gradient: [  123.6153   -50.2661    64.2469  -147.6651 -1058.4779]\n",
      "Weights: [-4.0449 -1.1505 -0.3164  0.1844  0.0764]\n",
      "MSE loss: 185.1011\n",
      "Iteration: 47900\n",
      "Gradient: [ 110.0199  -40.5829    9.9652  180.9186 1017.5596]\n",
      "Weights: [-4.0456 -1.1496 -0.3159  0.1842  0.0765]\n",
      "MSE loss: 184.9459\n",
      "Iteration: 48000\n",
      "Gradient: [ 100.4236  -68.2177  -66.2749  -81.427  -131.7706]\n",
      "Weights: [-4.0462 -1.1487 -0.3157  0.1838  0.0766]\n",
      "MSE loss: 184.8053\n",
      "Iteration: 48100\n",
      "Gradient: [ 111.3975 -105.2271    1.9764   24.9948  961.3936]\n",
      "Weights: [-4.0469 -1.1479 -0.3154  0.183   0.0768]\n",
      "MSE loss: 184.6324\n",
      "Iteration: 48200\n",
      "Gradient: [ 106.4183  -48.014  -114.7373  110.2362  346.7003]\n",
      "Weights: [-4.0476 -1.147  -0.3152  0.1825  0.0768]\n",
      "MSE loss: 184.4838\n",
      "Iteration: 48300\n",
      "Gradient: [  61.7355  -58.6333  -11.1309 -274.1776 -519.827 ]\n",
      "Weights: [-4.0482 -1.1462 -0.3148  0.1816  0.077 ]\n",
      "MSE loss: 184.3076\n",
      "Iteration: 48400\n",
      "Gradient: [ 113.1313 -124.2865   70.8913  360.238  -674.8661]\n",
      "Weights: [-4.0489 -1.1453 -0.3146  0.1818  0.077 ]\n",
      "MSE loss: 184.1877\n",
      "Iteration: 48500\n",
      "Gradient: [  55.6639 -167.2948 -380.9537   67.8821 -498.5856]\n",
      "Weights: [-4.0495 -1.1445 -0.3142  0.1812  0.0769]\n",
      "MSE loss: 184.0938\n",
      "Iteration: 48600\n",
      "Gradient: [ 96.4375 -54.5205   6.8717 266.7293 -42.1719]\n",
      "Weights: [-4.0501 -1.1436 -0.3139  0.1808  0.0772]\n",
      "MSE loss: 183.887\n",
      "Iteration: 48700\n",
      "Gradient: [  41.5133  -70.0847 -215.3776  -12.4425 -746.4773]\n",
      "Weights: [-4.0508 -1.1429 -0.3137  0.1803  0.0773]\n",
      "MSE loss: 183.7473\n",
      "Iteration: 48800\n",
      "Gradient: [   2.565  -104.0446   48.7547  206.5934  153.9558]\n",
      "Weights: [-4.0514 -1.1421 -0.3134  0.1796  0.0775]\n",
      "MSE loss: 183.5877\n",
      "Iteration: 48900\n",
      "Gradient: [ 106.3376  -79.2046   47.2868  302.5863 1220.1062]\n",
      "Weights: [-4.052  -1.1412 -0.3132  0.1795  0.0775]\n",
      "MSE loss: 183.465\n",
      "Iteration: 49000\n",
      "Gradient: [  80.7211 -106.3208    6.4623  119.0728  224.3396]\n",
      "Weights: [-4.0527 -1.1404 -0.3129  0.179   0.0776]\n",
      "MSE loss: 183.3148\n",
      "Iteration: 49100\n",
      "Gradient: [ 37.6406 -97.3231  33.3567  -2.5647  97.5604]\n",
      "Weights: [-4.0533 -1.1396 -0.3126  0.1788  0.0776]\n",
      "MSE loss: 183.1909\n",
      "Iteration: 49200\n",
      "Gradient: [  76.1675  -49.3578  -58.9493 -168.863  -496.5465]\n",
      "Weights: [-4.0539 -1.1387 -0.3123  0.1785  0.0777]\n",
      "MSE loss: 183.0561\n",
      "Iteration: 49300\n",
      "Gradient: [ 76.3714 -58.0861 137.7837 474.1533 257.2318]\n",
      "Weights: [-4.0546 -1.138  -0.3122  0.1772  0.0782]\n",
      "MSE loss: 182.8991\n",
      "Iteration: 49400\n",
      "Gradient: [  46.07    -79.8566   63.5898 -481.9545 -931.8943]\n",
      "Weights: [-4.0552 -1.1372 -0.3121  0.1762  0.0783]\n",
      "MSE loss: 182.7036\n",
      "Iteration: 49500\n",
      "Gradient: [  25.9208  -40.837  -161.5986  111.6747  -47.4709]\n",
      "Weights: [-4.0558 -1.1364 -0.3121  0.1759  0.0784]\n",
      "MSE loss: 182.5778\n",
      "Iteration: 49600\n",
      "Gradient: [  61.0751  -85.0236    2.7188 -216.1836  111.2273]\n",
      "Weights: [-4.0564 -1.1356 -0.3119  0.1755  0.0785]\n",
      "MSE loss: 182.4421\n",
      "Iteration: 49700\n",
      "Gradient: [  11.2746 -132.7395   61.9959  151.9953 -162.596 ]\n",
      "Weights: [-4.057  -1.1347 -0.3118  0.1747  0.0787]\n",
      "MSE loss: 182.2899\n",
      "Iteration: 49800\n",
      "Gradient: [  97.1912 -118.4342  104.3633  181.4737 -859.2794]\n",
      "Weights: [-4.0576 -1.1339 -0.3114  0.1737  0.0791]\n",
      "MSE loss: 182.0959\n",
      "Iteration: 49900\n",
      "Gradient: [   40.6835    10.6822   -37.9201   -96.595  -1030.1317]\n",
      "Weights: [-4.0581 -1.1329 -0.3111  0.1732  0.0793]\n",
      "MSE loss: 181.9553\n",
      "Iteration: 50000\n",
      "Gradient: [  59.1854  -32.9098 -132.3382  -29.4981 -282.2742]\n",
      "Weights: [-4.0587 -1.132  -0.311   0.1722  0.0795]\n",
      "MSE loss: 181.7802\n",
      "Iteration: 50100\n",
      "Gradient: [ 5.3386600e+01 -1.2652180e+02 -1.3666000e+00 -5.6408100e+01\n",
      " -1.6501525e+03]\n",
      "Weights: [-4.0592 -1.1311 -0.3107  0.1718  0.0796]\n",
      "MSE loss: 181.633\n",
      "Iteration: 50200\n",
      "Gradient: [ 81.9767 -28.9298  47.2214  96.6364 -18.929 ]\n",
      "Weights: [-4.0598 -1.1302 -0.3104  0.1714  0.0797]\n",
      "MSE loss: 181.4908\n",
      "Iteration: 50300\n",
      "Gradient: [ 89.6093 -67.8108 -35.3103 429.3786 349.8631]\n",
      "Weights: [-4.0603 -1.1293 -0.3102  0.1708  0.0798]\n",
      "MSE loss: 181.3435\n",
      "Iteration: 50400\n",
      "Gradient: [  46.4332  -87.0652  -83.7389   -5.9537 -720.8615]\n",
      "Weights: [-4.0609 -1.1284 -0.3097  0.1705  0.0798]\n",
      "MSE loss: 181.2049\n",
      "Iteration: 50500\n",
      "Gradient: [  74.266   -82.9096   21.3798  -54.1553 -124.5443]\n",
      "Weights: [-4.0614 -1.1276 -0.3095  0.1698  0.0801]\n",
      "MSE loss: 181.0507\n",
      "Iteration: 50600\n",
      "Gradient: [  11.132  -102.9575  -84.9576   12.5666 -107.6951]\n",
      "Weights: [-4.062  -1.1268 -0.3093  0.1695  0.08  ]\n",
      "MSE loss: 180.9242\n",
      "Iteration: 50700\n",
      "Gradient: [  27.6189  -96.9442   53.6393 -140.9353  891.2926]\n",
      "Weights: [-4.0626 -1.1259 -0.3089  0.1691  0.0803]\n",
      "MSE loss: 180.8023\n",
      "Iteration: 50800\n",
      "Gradient: [   2.0925 -101.6615  -27.9649   46.1564   65.4984]\n",
      "Weights: [-4.0631 -1.1251 -0.3086  0.1683  0.0804]\n",
      "MSE loss: 180.6206\n",
      "Iteration: 50900\n",
      "Gradient: [  91.5024 -105.8199 -226.1553   91.5315  -75.8388]\n",
      "Weights: [-4.0637 -1.1242 -0.3083  0.1679  0.0806]\n",
      "MSE loss: 180.5107\n",
      "Iteration: 51000\n",
      "Gradient: [  53.0627  -59.9788  -45.5132   80.5471 -873.1488]\n",
      "Weights: [-4.0643 -1.1234 -0.3081  0.1673  0.0806]\n",
      "MSE loss: 180.3374\n",
      "Iteration: 51100\n",
      "Gradient: [115.0335 -71.7686 -18.6668 291.9397 383.5232]\n",
      "Weights: [-4.0648 -1.1225 -0.3077  0.167   0.0807]\n",
      "MSE loss: 180.2093\n",
      "Iteration: 51200\n",
      "Gradient: [ 15.562  -89.4859 -38.7765  75.8164  40.3542]\n",
      "Weights: [-4.0653 -1.1216 -0.3075  0.1663  0.0807]\n",
      "MSE loss: 180.0624\n",
      "Iteration: 51300\n",
      "Gradient: [ 72.522  -48.7339  71.9886 -30.287  426.5015]\n",
      "Weights: [-4.0659 -1.1207 -0.3071  0.1657  0.081 ]\n",
      "MSE loss: 179.9016\n",
      "Iteration: 51400\n",
      "Gradient: [ 41.0092 -87.8524 -64.7526 -62.5129 101.0059]\n",
      "Weights: [-4.0664 -1.1198 -0.3069  0.1653  0.081 ]\n",
      "MSE loss: 179.7543\n",
      "Iteration: 51500\n",
      "Gradient: [  51.0749  -47.6119   10.8996 -259.5562  649.7525]\n",
      "Weights: [-4.067  -1.1189 -0.3067  0.1647  0.0812]\n",
      "MSE loss: 179.6039\n",
      "Iteration: 51600\n",
      "Gradient: [ 100.1539  -71.4635  -22.0768  294.8932 -154.6479]\n",
      "Weights: [-4.0676 -1.1181 -0.3063  0.1645  0.0812]\n",
      "MSE loss: 179.4814\n",
      "Iteration: 51700\n",
      "Gradient: [  64.5782 -134.3764 -114.96   -236.1277 -290.7923]\n",
      "Weights: [-4.0681 -1.1172 -0.3061  0.164   0.0813]\n",
      "MSE loss: 179.3477\n",
      "Iteration: 51800\n",
      "Gradient: [  41.748    16.5364 -219.7193  157.6853  -96.0401]\n",
      "Weights: [-4.0687 -1.1165 -0.3061  0.164   0.0812]\n",
      "MSE loss: 179.2509\n",
      "Iteration: 51900\n",
      "Gradient: [ -7.9939 -58.6164  -7.7453 -84.6425 -72.7274]\n",
      "Weights: [-4.0692 -1.1156 -0.3058  0.1634  0.0814]\n",
      "MSE loss: 179.1019\n",
      "Iteration: 52000\n",
      "Gradient: [  64.1174 -114.4785  -24.8505  582.2853  726.7228]\n",
      "Weights: [-4.0698 -1.1148 -0.3056  0.1628  0.0816]\n",
      "MSE loss: 178.9572\n",
      "Iteration: 52100\n",
      "Gradient: [  45.376   -21.1732   93.7453 -445.2677  -10.1941]\n",
      "Weights: [-4.0704 -1.114  -0.3052  0.1626  0.0815]\n",
      "MSE loss: 178.845\n",
      "Iteration: 52200\n",
      "Gradient: [  62.6879  -65.7279 -171.3857 -163.9198 1121.5068]\n",
      "Weights: [-4.0709 -1.1131 -0.3049  0.162   0.0819]\n",
      "MSE loss: 178.7109\n",
      "Iteration: 52300\n",
      "Gradient: [  65.0673  -96.4334 -230.5937 -235.8152 -788.9657]\n",
      "Weights: [-4.0715 -1.1123 -0.3047  0.1615  0.0818]\n",
      "MSE loss: 178.5468\n",
      "Iteration: 52400\n",
      "Gradient: [  74.6885  -59.1168 -101.2959 -161.5789  602.3532]\n",
      "Weights: [-4.072  -1.1114 -0.3044  0.1608  0.082 ]\n",
      "MSE loss: 178.4\n",
      "Iteration: 52500\n",
      "Gradient: [  83.0352 -168.4258   18.0953   69.9495   -7.5637]\n",
      "Weights: [-4.0726 -1.1106 -0.3041  0.1601  0.0821]\n",
      "MSE loss: 178.2466\n",
      "Iteration: 52600\n",
      "Gradient: [  82.497   -64.6325    1.0728   83.6914 -541.0436]\n",
      "Weights: [-4.0732 -1.1097 -0.3038  0.1593  0.0822]\n",
      "MSE loss: 178.1305\n",
      "Iteration: 52700\n",
      "Gradient: [  87.5786 -128.1739   17.3894 -291.073   152.7483]\n",
      "Weights: [-4.0737 -1.1088 -0.3035  0.1588  0.0825]\n",
      "MSE loss: 177.9366\n",
      "Iteration: 52800\n",
      "Gradient: [  42.255   -87.047   -40.1993 -140.4759    5.1139]\n",
      "Weights: [-4.0743 -1.108  -0.3033  0.1586  0.0825]\n",
      "MSE loss: 177.8092\n",
      "Iteration: 52900\n",
      "Gradient: [  56.8314 -137.013  -118.7425  246.5255 -850.301 ]\n",
      "Weights: [-4.0749 -1.1071 -0.3029  0.1582  0.0826]\n",
      "MSE loss: 177.6816\n",
      "Iteration: 53000\n",
      "Gradient: [  33.3622  -82.1144  -41.8535  -34.8524 -727.8946]\n",
      "Weights: [-4.0755 -1.1063 -0.3027  0.1575  0.0828]\n",
      "MSE loss: 177.5259\n",
      "Iteration: 53100\n",
      "Gradient: [ 91.91   -15.0092  50.3477 251.8581 533.1413]\n",
      "Weights: [-4.0761 -1.1055 -0.3025  0.1567  0.0832]\n",
      "MSE loss: 177.3853\n",
      "Iteration: 53200\n",
      "Gradient: [  42.2044 -137.6685   44.4151  332.7768  374.1736]\n",
      "Weights: [-4.0767 -1.1047 -0.3023  0.1562  0.0833]\n",
      "MSE loss: 177.2426\n",
      "Iteration: 53300\n",
      "Gradient: [  21.6011  -90.8683  -90.7809 -264.0822  560.9289]\n",
      "Weights: [-4.0772 -1.1038 -0.302   0.1558  0.0833]\n",
      "MSE loss: 177.1023\n",
      "Iteration: 53400\n",
      "Gradient: [ 87.142  -60.1571 -46.8654 251.0289 964.3906]\n",
      "Weights: [-4.0778 -1.103  -0.3018  0.1553  0.0836]\n",
      "MSE loss: 177.032\n",
      "Iteration: 53500\n",
      "Gradient: [  92.0619 -137.773    18.0013  200.7364  453.394 ]\n",
      "Weights: [-4.0783 -1.1021 -0.3016  0.1551  0.0834]\n",
      "MSE loss: 176.8504\n",
      "Iteration: 53600\n",
      "Gradient: [  64.8533  -92.5997 -162.6848 -101.513    80.2486]\n",
      "Weights: [-4.0788 -1.1012 -0.3012  0.1547  0.0834]\n",
      "MSE loss: 176.7151\n",
      "Iteration: 53700\n",
      "Gradient: [  64.0999 -122.3317  164.0021 -195.5327  265.8304]\n",
      "Weights: [-4.0794 -1.1004 -0.301   0.1537  0.0836]\n",
      "MSE loss: 176.5998\n",
      "Iteration: 53800\n",
      "Gradient: [ 81.044  -84.071   99.8653  98.4317 481.0872]\n",
      "Weights: [-4.08   -1.0996 -0.3008  0.1533  0.0839]\n",
      "MSE loss: 176.4302\n",
      "Iteration: 53900\n",
      "Gradient: [ 27.1953 -48.6489 -15.2705 376.3996 649.6424]\n",
      "Weights: [-4.0806 -1.0987 -0.3006  0.1525  0.0842]\n",
      "MSE loss: 176.2927\n",
      "Iteration: 54000\n",
      "Gradient: [ 62.8067  19.9036 -21.0551 610.3383 485.1698]\n",
      "Weights: [-4.0811 -1.0979 -0.3004  0.1517  0.0844]\n",
      "MSE loss: 176.1373\n",
      "Iteration: 54100\n",
      "Gradient: [ 72.325  -60.4702 -15.0772 380.4015  40.6489]\n",
      "Weights: [-4.0816 -1.0971 -0.3001  0.1514  0.0843]\n",
      "MSE loss: 175.9918\n",
      "Iteration: 54200\n",
      "Gradient: [  76.0531  -92.6351 -289.5751   31.4667   33.595 ]\n",
      "Weights: [-4.0822 -1.0963 -0.3001  0.1506  0.0846]\n",
      "MSE loss: 175.841\n",
      "Iteration: 54300\n",
      "Gradient: [   42.5763   -88.029    -87.5876   105.9093 -1135.0726]\n",
      "Weights: [-4.0828 -1.0954 -0.2997  0.1499  0.0847]\n",
      "MSE loss: 175.6957\n",
      "Iteration: 54400\n",
      "Gradient: [  51.6472 -123.5602   -8.5977 -161.0107 1001.7862]\n",
      "Weights: [-4.0833 -1.0946 -0.2995  0.1493  0.0851]\n",
      "MSE loss: 175.5784\n",
      "Iteration: 54500\n",
      "Gradient: [  14.4067  -95.3624   18.7402   91.4665 -252.0016]\n",
      "Weights: [-4.0838 -1.0938 -0.2993  0.1489  0.0851]\n",
      "MSE loss: 175.4246\n",
      "Iteration: 54600\n",
      "Gradient: [ 4.5112500e+01 -1.3459200e+02 -7.3585500e+01 -1.3320000e+00\n",
      " -1.5474536e+03]\n",
      "Weights: [-4.0843 -1.0929 -0.2989  0.1485  0.085 ]\n",
      "MSE loss: 175.306\n",
      "Iteration: 54700\n",
      "Gradient: [  53.3697  -61.544    45.5413 -185.2427  147.461 ]\n",
      "Weights: [-4.0848 -1.0921 -0.2988  0.1477  0.0854]\n",
      "MSE loss: 175.1517\n",
      "Iteration: 54800\n",
      "Gradient: [  59.1753  -97.0693  -86.9271 -125.3469  596.0645]\n",
      "Weights: [-4.0853 -1.0912 -0.2983  0.1472  0.0854]\n",
      "MSE loss: 175.0078\n",
      "Iteration: 54900\n",
      "Gradient: [  43.8437  -53.5893 -185.852   332.2793  273.6369]\n",
      "Weights: [-4.0859 -1.0905 -0.2979  0.1463  0.0856]\n",
      "MSE loss: 174.8658\n",
      "Iteration: 55000\n",
      "Gradient: [  87.4372  -25.0879 -162.5115  598.2405  439.8365]\n",
      "Weights: [-4.0864 -1.0896 -0.2975  0.1459  0.0857]\n",
      "MSE loss: 174.7211\n",
      "Iteration: 55100\n",
      "Gradient: [    2.05     -94.9196  -112.2223   198.1603 -1125.6415]\n",
      "Weights: [-4.0869 -1.0887 -0.2973  0.1451  0.0859]\n",
      "MSE loss: 174.5666\n",
      "Iteration: 55200\n",
      "Gradient: [ 55.5745 -72.6084 -40.3784 237.48   182.8889]\n",
      "Weights: [-4.0874 -1.0879 -0.2969  0.1445  0.0861]\n",
      "MSE loss: 174.4273\n",
      "Iteration: 55300\n",
      "Gradient: [  30.1051  -85.0666   61.7531 -153.9638 -123.1482]\n",
      "Weights: [-4.0879 -1.087  -0.2966  0.1436  0.0864]\n",
      "MSE loss: 174.2698\n",
      "Iteration: 55400\n",
      "Gradient: [   11.2853   -75.0035  -110.4296   -56.3916 -1133.3626]\n",
      "Weights: [-4.0884 -1.0862 -0.2963  0.1429  0.0865]\n",
      "MSE loss: 174.1312\n",
      "Iteration: 55500\n",
      "Gradient: [  41.6353  -70.2236 -117.5093  -80.8989 -828.5432]\n",
      "Weights: [-4.0889 -1.0853 -0.2959  0.1425  0.0866]\n",
      "MSE loss: 173.9948\n",
      "Iteration: 55600\n",
      "Gradient: [  43.6856  -88.5875  -47.8245  422.2519 -569.9541]\n",
      "Weights: [-4.0895 -1.0845 -0.2956  0.142   0.0867]\n",
      "MSE loss: 173.8559\n",
      "Iteration: 55700\n",
      "Gradient: [ -3.6177 -60.3866 -49.4632  70.1433 -39.9281]\n",
      "Weights: [-4.09   -1.0837 -0.2955  0.1412  0.087 ]\n",
      "MSE loss: 173.7275\n",
      "Iteration: 55800\n",
      "Gradient: [ 38.8082 -62.6504 190.2154 239.4576 437.7216]\n",
      "Weights: [-4.0905 -1.0828 -0.2952  0.141   0.0871]\n",
      "MSE loss: 173.6163\n",
      "Iteration: 55900\n",
      "Gradient: [  73.1197  -83.5871 -215.3763   65.1501  972.6846]\n",
      "Weights: [-4.091  -1.0819 -0.2951  0.1404  0.0871]\n",
      "MSE loss: 173.4649\n",
      "Iteration: 56000\n",
      "Gradient: [  39.2362  -66.8392  -80.1813 -480.1369  838.778 ]\n",
      "Weights: [-4.0915 -1.0811 -0.2946  0.14    0.0872]\n",
      "MSE loss: 173.3275\n",
      "Iteration: 56100\n",
      "Gradient: [ 76.9812  -3.9339 -21.6463 229.6548 322.3965]\n",
      "Weights: [-4.092  -1.0803 -0.2944  0.1393  0.0875]\n",
      "MSE loss: 173.2284\n",
      "Iteration: 56200\n",
      "Gradient: [  26.082  -130.0707 -174.2745  130.4738 -888.6584]\n",
      "Weights: [-4.0926 -1.0796 -0.2941  0.1387  0.0874]\n",
      "MSE loss: 173.0735\n",
      "Iteration: 56300\n",
      "Gradient: [  68.5589  -74.8565  -63.885  -188.6275 -897.7372]\n",
      "Weights: [-4.0931 -1.0787 -0.2937  0.1382  0.0875]\n",
      "MSE loss: 172.9321\n",
      "Iteration: 56400\n",
      "Gradient: [   78.2728   -64.7134    -8.036    -94.4408 -1261.3389]\n",
      "Weights: [-4.0936 -1.0778 -0.2932  0.1377  0.0877]\n",
      "MSE loss: 172.7765\n",
      "Iteration: 56500\n",
      "Gradient: [  7.951   -4.4753 -50.4203 277.0857 711.0418]\n",
      "Weights: [-4.0942 -1.077  -0.2929  0.1373  0.0879]\n",
      "MSE loss: 172.6685\n",
      "Iteration: 56600\n",
      "Gradient: [  32.6928 -116.216   152.4725   44.4873  662.6169]\n",
      "Weights: [-4.0947 -1.0761 -0.2925  0.1371  0.0879]\n",
      "MSE loss: 172.5675\n",
      "Iteration: 56700\n",
      "Gradient: [  53.3457  -76.438   108.3089 -191.7995  570.0655]\n",
      "Weights: [-4.0953 -1.0753 -0.2921  0.1365  0.0879]\n",
      "MSE loss: 172.3953\n",
      "Iteration: 56800\n",
      "Gradient: [ 8.544000e+00  4.734000e-01 -1.273715e+02  5.272467e+02 -3.523747e+02]\n",
      "Weights: [-4.0958 -1.0744 -0.2915  0.1365  0.0879]\n",
      "MSE loss: 172.2856\n",
      "Iteration: 56900\n",
      "Gradient: [  49.3869 -106.4521  227.4557  234.5121 1611.0301]\n",
      "Weights: [-4.0963 -1.0736 -0.2912  0.1358  0.0881]\n",
      "MSE loss: 172.1838\n",
      "Iteration: 57000\n",
      "Gradient: [  77.2757  -65.4134  102.6386   44.6224 -643.3477]\n",
      "Weights: [-4.0969 -1.0729 -0.2912  0.1351  0.0882]\n",
      "MSE loss: 172.0052\n",
      "Iteration: 57100\n",
      "Gradient: [ 100.8699  -36.4563  -71.7073 -322.388   391.0709]\n",
      "Weights: [-4.0974 -1.0721 -0.2909  0.1348  0.0882]\n",
      "MSE loss: 171.8877\n",
      "Iteration: 57200\n",
      "Gradient: [ 78.5727  22.5113   9.351  121.7309 229.7948]\n",
      "Weights: [-4.098  -1.0713 -0.2907  0.1344  0.0885]\n",
      "MSE loss: 171.7977\n",
      "Iteration: 57300\n",
      "Gradient: [  38.7703  -71.3408 -128.3671  -19.3053  273.8798]\n",
      "Weights: [-4.0985 -1.0705 -0.2904  0.1337  0.0885]\n",
      "MSE loss: 171.6272\n",
      "Iteration: 57400\n",
      "Gradient: [121.7353 -93.6686  -3.178  -68.4235 141.2585]\n",
      "Weights: [-4.099  -1.0697 -0.2901  0.1334  0.0886]\n",
      "MSE loss: 171.5287\n",
      "Iteration: 57500\n",
      "Gradient: [  89.8619  -65.3866  -83.9494 -134.5612 -632.8509]\n",
      "Weights: [-4.0996 -1.0689 -0.29    0.1325  0.0888]\n",
      "MSE loss: 171.3638\n",
      "Iteration: 57600\n",
      "Gradient: [ 78.4373 -41.3245 136.7611 380.9404 247.1618]\n",
      "Weights: [-4.1001 -1.0681 -0.2898  0.1319  0.0892]\n",
      "MSE loss: 171.2854\n",
      "Iteration: 57700\n",
      "Gradient: [  61.5121 -166.507   -80.8451    4.4967 -516.8598]\n",
      "Weights: [-4.1006 -1.0674 -0.2896  0.1316  0.089 ]\n",
      "MSE loss: 171.1299\n",
      "Iteration: 57800\n",
      "Gradient: [ -18.3622 -101.7208   67.4959 -106.7582  544.961 ]\n",
      "Weights: [-4.1011 -1.0666 -0.2893  0.1306  0.0894]\n",
      "MSE loss: 170.9712\n",
      "Iteration: 57900\n",
      "Gradient: [118.0357 -23.6965  58.4116 387.5672 756.7982]\n",
      "Weights: [-4.1016 -1.0657 -0.289   0.1304  0.0894]\n",
      "MSE loss: 170.8625\n",
      "Iteration: 58000\n",
      "Gradient: [  34.9888  -96.0754 -185.6106  202.8149 -434.2153]\n",
      "Weights: [-4.1021 -1.0649 -0.2888  0.1296  0.0896]\n",
      "MSE loss: 170.7178\n",
      "Iteration: 58100\n",
      "Gradient: [  48.8411  -63.6686  225.7914  284.4666 -349.005 ]\n",
      "Weights: [-4.1026 -1.0641 -0.2883  0.1294  0.0896]\n",
      "MSE loss: 170.6021\n",
      "Iteration: 58200\n",
      "Gradient: [  46.0965  -86.7839   11.8461  -49.7543 -166.2719]\n",
      "Weights: [-4.1032 -1.0634 -0.2881  0.1288  0.0897]\n",
      "MSE loss: 170.4782\n",
      "Iteration: 58300\n",
      "Gradient: [  88.0577 -114.261  -136.0636 -705.4989 -771.7214]\n",
      "Weights: [-4.1036 -1.0626 -0.2879  0.1288  0.0894]\n",
      "MSE loss: 170.4594\n",
      "Iteration: 58400\n",
      "Gradient: [ 47.1867 -25.6302  36.867  142.49   593.9796]\n",
      "Weights: [-4.1041 -1.0618 -0.2876  0.1289  0.0895]\n",
      "MSE loss: 170.2905\n",
      "Iteration: 58500\n",
      "Gradient: [  41.6253 -117.3435   42.3082 -164.4834  -62.9873]\n",
      "Weights: [-4.1047 -1.061  -0.2874  0.1285  0.0894]\n",
      "MSE loss: 170.2559\n",
      "Iteration: 58600\n",
      "Gradient: [  55.7173 -109.0374  -45.2067 -106.2521  360.1709]\n",
      "Weights: [-4.1053 -1.0603 -0.2872  0.1278  0.0899]\n",
      "MSE loss: 170.0446\n",
      "Iteration: 58700\n",
      "Gradient: [  46.5004  -63.8301  -94.6037 -312.3944  193.0031]\n",
      "Weights: [-4.1059 -1.0596 -0.2868  0.1271  0.09  ]\n",
      "MSE loss: 169.9046\n",
      "Iteration: 58800\n",
      "Gradient: [  53.9611  -29.8794 -139.529    38.5108  308.5727]\n",
      "Weights: [-4.1064 -1.0588 -0.2865  0.1267  0.0902]\n",
      "MSE loss: 169.8225\n",
      "Iteration: 58900\n",
      "Gradient: [ 52.5004 -78.5437  90.8749 599.7568 198.1152]\n",
      "Weights: [-4.1069 -1.058  -0.2862  0.1266  0.0901]\n",
      "MSE loss: 169.6884\n",
      "Iteration: 59000\n",
      "Gradient: [  30.2832  -60.8345  -89.8219 -107.6152  750.6738]\n",
      "Weights: [-4.1074 -1.0572 -0.2861  0.1258  0.0903]\n",
      "MSE loss: 169.5485\n",
      "Iteration: 59100\n",
      "Gradient: [  85.812   -48.0798  -70.8346  412.6481 -205.3258]\n",
      "Weights: [-4.1079 -1.0564 -0.2859  0.1253  0.0903]\n",
      "MSE loss: 169.4501\n",
      "Iteration: 59200\n",
      "Gradient: [  91.8616  -76.1718 -114.9613  -45.5542  405.7988]\n",
      "Weights: [-4.1084 -1.0557 -0.2856  0.1249  0.0905]\n",
      "MSE loss: 169.3123\n",
      "Iteration: 59300\n",
      "Gradient: [   1.7322 -148.8599   45.2352 -106.1302 -477.2668]\n",
      "Weights: [-4.1089 -1.0549 -0.2853  0.1247  0.0905]\n",
      "MSE loss: 169.2105\n",
      "Iteration: 59400\n",
      "Gradient: [ 97.3745  -1.1072 110.6138 221.3343 430.4201]\n",
      "Weights: [-4.1095 -1.0542 -0.2851  0.1237  0.0909]\n",
      "MSE loss: 169.0692\n",
      "Iteration: 59500\n",
      "Gradient: [  41.6821   -6.6896   13.2889  367.0123 -244.6779]\n",
      "Weights: [-4.1099 -1.0535 -0.2851  0.1229  0.0911]\n",
      "MSE loss: 168.9338\n",
      "Iteration: 59600\n",
      "Gradient: [  49.8233  -89.6538    3.5243 -273.6947 -159.3644]\n",
      "Weights: [-4.1104 -1.0527 -0.2848  0.1226  0.0912]\n",
      "MSE loss: 168.8191\n",
      "Iteration: 59700\n",
      "Gradient: [  29.5984 -121.4692  100.4589   64.1583  118.8749]\n",
      "Weights: [-4.1109 -1.0519 -0.2845  0.1226  0.0911]\n",
      "MSE loss: 168.737\n",
      "Iteration: 59800\n",
      "Gradient: [  38.0637  -62.728   -24.7384 -310.3536  205.6574]\n",
      "Weights: [-4.1115 -1.0512 -0.2845  0.1216  0.0913]\n",
      "MSE loss: 168.5979\n",
      "Iteration: 59900\n",
      "Gradient: [  9.3006 -27.7805  37.369  511.5063 742.1297]\n",
      "Weights: [-4.1119 -1.0504 -0.2845  0.1214  0.0914]\n",
      "MSE loss: 168.4853\n",
      "Iteration: 60000\n",
      "Gradient: [  78.3311  -78.8023  119.6825 -147.6386 -825.0745]\n",
      "Weights: [-4.1124 -1.0497 -0.2843  0.1204  0.0917]\n",
      "MSE loss: 168.3549\n",
      "Iteration: 60100\n",
      "Gradient: [ 51.6217 -63.7372 -40.7066 383.6517 681.7688]\n",
      "Weights: [-4.1129 -1.0488 -0.2841  0.1198  0.092 ]\n",
      "MSE loss: 168.2126\n",
      "Iteration: 60200\n",
      "Gradient: [  88.8885  -71.2176   50.6741 -159.5725  413.5933]\n",
      "Weights: [-4.1134 -1.048  -0.2839  0.1197  0.0918]\n",
      "MSE loss: 168.1393\n",
      "Iteration: 60300\n",
      "Gradient: [ 21.9138 -74.8213 -20.8794 110.1123 319.6738]\n",
      "Weights: [-4.1139 -1.0472 -0.2837  0.119   0.0921]\n",
      "MSE loss: 167.9767\n",
      "Iteration: 60400\n",
      "Gradient: [  57.5038 -103.041   -97.3448  287.1689 -735.9495]\n",
      "Weights: [-4.1143 -1.0463 -0.2832  0.1189  0.0921]\n",
      "MSE loss: 167.8686\n",
      "Iteration: 60500\n",
      "Gradient: [ 71.9792 -61.0334  11.0098 189.5124 492.4749]\n",
      "Weights: [-4.1148 -1.0456 -0.2832  0.1186  0.0922]\n",
      "MSE loss: 167.7836\n",
      "Iteration: 60600\n",
      "Gradient: [ 46.5257 -94.5339 -87.2523  58.5714 347.764 ]\n",
      "Weights: [-4.1154 -1.0448 -0.2831  0.1177  0.0923]\n",
      "MSE loss: 167.6349\n",
      "Iteration: 60700\n",
      "Gradient: [  24.4622 -108.5569  -45.0434 -160.0196   53.2609]\n",
      "Weights: [-4.1159 -1.0441 -0.2829  0.1171  0.0926]\n",
      "MSE loss: 167.5136\n",
      "Iteration: 60800\n",
      "Gradient: [  62.2635 -130.6973  -21.024  -108.1187   65.2547]\n",
      "Weights: [-4.1163 -1.0433 -0.2826  0.1166  0.0926]\n",
      "MSE loss: 167.3983\n",
      "Iteration: 60900\n",
      "Gradient: [  82.6581  -48.7495   32.2704   79.6439 -584.5493]\n",
      "Weights: [-4.1168 -1.0425 -0.2823  0.1162  0.0928]\n",
      "MSE loss: 167.2663\n",
      "Iteration: 61000\n",
      "Gradient: [   16.1946  -116.547   -124.4541    28.3738 -1008.2782]\n",
      "Weights: [-4.1173 -1.0418 -0.282   0.1158  0.0928]\n",
      "MSE loss: 167.1594\n",
      "Iteration: 61100\n",
      "Gradient: [    7.2826   -53.7338   -88.2895   373.8505 -1848.3122]\n",
      "Weights: [-4.1177 -1.041  -0.282   0.1151  0.0929]\n",
      "MSE loss: 167.0811\n",
      "Iteration: 61200\n",
      "Gradient: [   37.9411   -93.8538  -298.0927    96.2489 -1285.8914]\n",
      "Weights: [-4.1182 -1.0401 -0.2816  0.1147  0.093 ]\n",
      "MSE loss: 166.9246\n",
      "Iteration: 61300\n",
      "Gradient: [  75.6515 -147.7194  -23.9022   68.3051 -485.9626]\n",
      "Weights: [-4.1187 -1.0394 -0.2814  0.1141  0.0933]\n",
      "MSE loss: 166.7923\n",
      "Iteration: 61400\n",
      "Gradient: [  -0.6358 -118.1257 -311.8342 -504.2743 -534.5856]\n",
      "Weights: [-4.1191 -1.0385 -0.2814  0.1135  0.0934]\n",
      "MSE loss: 166.6877\n",
      "Iteration: 61500\n",
      "Gradient: [ 53.8611 -58.6846  77.8108 514.7615 645.8013]\n",
      "Weights: [-4.1196 -1.0377 -0.2811  0.1132  0.0936]\n",
      "MSE loss: 166.572\n",
      "Iteration: 61600\n",
      "Gradient: [  74.5926 -123.7901   68.6134  454.5378 -301.0555]\n",
      "Weights: [-4.1201 -1.0369 -0.2809  0.1125  0.0937]\n",
      "MSE loss: 166.4309\n",
      "Iteration: 61700\n",
      "Gradient: [  39.1439  -35.1987   25.1728  121.3985 -372.4246]\n",
      "Weights: [-4.1206 -1.0361 -0.2806  0.1121  0.0937]\n",
      "MSE loss: 166.3127\n",
      "Iteration: 61800\n",
      "Gradient: [ 29.7245 -20.0367 -73.112  404.1439 948.05  ]\n",
      "Weights: [-4.1211 -1.0354 -0.2805  0.1116  0.094 ]\n",
      "MSE loss: 166.2069\n",
      "Iteration: 61900\n",
      "Gradient: [ 25.0371 -48.6991 123.0901 301.9622 354.0127]\n",
      "Weights: [-4.1215 -1.0345 -0.2803  0.1109  0.0942]\n",
      "MSE loss: 166.0932\n",
      "Iteration: 62000\n",
      "Gradient: [   77.6104  -152.0802    38.8794  -428.1857 -1502.1357]\n",
      "Weights: [-4.1219 -1.0336 -0.28    0.1106  0.094 ]\n",
      "MSE loss: 165.9939\n",
      "Iteration: 62100\n",
      "Gradient: [ 43.8229 -44.5531  47.3287 239.4425 514.951 ]\n",
      "Weights: [-4.1224 -1.0329 -0.2798  0.1102  0.0943]\n",
      "MSE loss: 165.8458\n",
      "Iteration: 62200\n",
      "Gradient: [  68.159   -59.3908  -10.2754 -408.3564   63.9427]\n",
      "Weights: [-4.1229 -1.032  -0.2796  0.1096  0.0944]\n",
      "MSE loss: 165.7208\n",
      "Iteration: 62300\n",
      "Gradient: [  67.7241  -46.296   -91.497   235.0962 1217.087 ]\n",
      "Weights: [-4.1233 -1.0312 -0.2793  0.1095  0.0945]\n",
      "MSE loss: 165.6496\n",
      "Iteration: 62400\n",
      "Gradient: [  80.0505  -81.6554   10.4954 -239.622    11.7493]\n",
      "Weights: [-4.1238 -1.0304 -0.2788  0.1093  0.0944]\n",
      "MSE loss: 165.5267\n",
      "Iteration: 62500\n",
      "Gradient: [ 53.1079 -17.5745 -23.9963 429.9783 330.0289]\n",
      "Weights: [-4.1243 -1.0297 -0.2787  0.1088  0.0945]\n",
      "MSE loss: 165.4122\n",
      "Iteration: 62600\n",
      "Gradient: [  85.8976  -58.7265  -30.8084 -154.4636  479.8081]\n",
      "Weights: [-4.1248 -1.0289 -0.2786  0.1081  0.0947]\n",
      "MSE loss: 165.2846\n",
      "Iteration: 62700\n",
      "Gradient: [ 47.9117 -68.1212  52.2384 600.9748 789.4828]\n",
      "Weights: [-4.1252 -1.0282 -0.2784  0.1072  0.095 ]\n",
      "MSE loss: 165.143\n",
      "Iteration: 62800\n",
      "Gradient: [  66.327  -102.525    56.9443  557.2738  446.766 ]\n",
      "Weights: [-4.1257 -1.0274 -0.2782  0.1064  0.0952]\n",
      "MSE loss: 165.0138\n",
      "Iteration: 62900\n",
      "Gradient: [   72.8556   -80.5597    64.4721   103.5602 -1383.7022]\n",
      "Weights: [-4.1261 -1.0266 -0.2782  0.1054  0.0954]\n",
      "MSE loss: 164.9339\n",
      "Iteration: 63000\n",
      "Gradient: [  84.1186 -150.6057 -128.9203   96.6074  779.8444]\n",
      "Weights: [-4.1266 -1.0258 -0.2779  0.1052  0.0955]\n",
      "MSE loss: 164.7728\n",
      "Iteration: 63100\n",
      "Gradient: [  68.4001 -147.3624  -15.8879 -368.2903  378.9974]\n",
      "Weights: [-4.127  -1.0249 -0.2776  0.1048  0.0957]\n",
      "MSE loss: 164.6619\n",
      "Iteration: 63200\n",
      "Gradient: [  -4.7809  -98.2048 -177.5867   34.0895 -911.2662]\n",
      "Weights: [-4.1275 -1.0242 -0.2774  0.1042  0.0957]\n",
      "MSE loss: 164.5542\n",
      "Iteration: 63300\n",
      "Gradient: [   24.5022   -13.743   -180.8636   431.9479 -1052.3804]\n",
      "Weights: [-4.128  -1.0234 -0.277   0.1038  0.0958]\n",
      "MSE loss: 164.4345\n",
      "Iteration: 63400\n",
      "Gradient: [ 31.2583 -85.8502 -64.9231   3.8712 400.7679]\n",
      "Weights: [-4.1284 -1.0226 -0.2766  0.1035  0.0958]\n",
      "MSE loss: 164.3204\n",
      "Iteration: 63500\n",
      "Gradient: [  17.787   -63.2832  -75.6377 -135.8861 -310.9674]\n",
      "Weights: [-4.1289 -1.0219 -0.2765  0.1029  0.096 ]\n",
      "MSE loss: 164.2154\n",
      "Iteration: 63600\n",
      "Gradient: [  73.8228  -85.131  -162.203  -667.172   356.2001]\n",
      "Weights: [-4.1294 -1.0212 -0.2763  0.1025  0.096 ]\n",
      "MSE loss: 164.1338\n",
      "Iteration: 63700\n",
      "Gradient: [ 81.2738 -60.3457  61.6744 325.7582 543.2322]\n",
      "Weights: [-4.1298 -1.0203 -0.2761  0.102   0.0964]\n",
      "MSE loss: 164.0069\n",
      "Iteration: 63800\n",
      "Gradient: [ 21.2303 -60.5     93.7233  32.0083 861.231 ]\n",
      "Weights: [-4.1303 -1.0195 -0.2758  0.1012  0.0964]\n",
      "MSE loss: 163.8396\n",
      "Iteration: 63900\n",
      "Gradient: [   23.6676  -119.2654   -62.2647  -130.1413 -1004.535 ]\n",
      "Weights: [-4.1307 -1.0186 -0.2755  0.1009  0.0964]\n",
      "MSE loss: 163.7536\n",
      "Iteration: 64000\n",
      "Gradient: [  71.758    -2.3046  -39.2584 -216.2769  294.4529]\n",
      "Weights: [-4.1312 -1.0178 -0.2751  0.1004  0.0966]\n",
      "MSE loss: 163.6103\n",
      "Iteration: 64100\n",
      "Gradient: [  54.7684  -72.6489 -164.5103  515.3298 -793.8484]\n",
      "Weights: [-4.1317 -1.0171 -0.275   0.0996  0.0968]\n",
      "MSE loss: 163.4946\n",
      "Iteration: 64200\n",
      "Gradient: [  97.0253  -47.9545   23.7899  374.0077 -173.9144]\n",
      "Weights: [-4.1321 -1.0164 -0.2749  0.0989  0.097 ]\n",
      "MSE loss: 163.3785\n",
      "Iteration: 64300\n",
      "Gradient: [  45.504   -35.7201 -101.2992  127.6589  572.0202]\n",
      "Weights: [-4.1326 -1.0156 -0.2745  0.099   0.097 ]\n",
      "MSE loss: 163.2823\n",
      "Iteration: 64400\n",
      "Gradient: [  14.419  -114.2189   27.287   553.8125 -749.0337]\n",
      "Weights: [-4.133  -1.0148 -0.2743  0.0983  0.0971]\n",
      "MSE loss: 163.1636\n",
      "Iteration: 64500\n",
      "Gradient: [  33.9462  -92.9417 -149.4229  187.3019 -283.4646]\n",
      "Weights: [-4.1335 -1.014  -0.274   0.0979  0.0971]\n",
      "MSE loss: 163.0611\n",
      "Iteration: 64600\n",
      "Gradient: [  49.6352  -89.9168   85.5695   35.9253 1045.5211]\n",
      "Weights: [-4.134  -1.0132 -0.2738  0.0975  0.0974]\n",
      "MSE loss: 162.9472\n",
      "Iteration: 64700\n",
      "Gradient: [ 47.1826 -54.0305 -32.0509 162.7231   4.052 ]\n",
      "Weights: [-4.1345 -1.0125 -0.2735  0.0972  0.0974]\n",
      "MSE loss: 162.8338\n",
      "Iteration: 64800\n",
      "Gradient: [ 10.3562 -39.5504 -33.6435 197.4392 214.3182]\n",
      "Weights: [-4.135  -1.0117 -0.2733  0.0962  0.0977]\n",
      "MSE loss: 162.7028\n",
      "Iteration: 64900\n",
      "Gradient: [ 22.9777 -79.8813  95.359  356.3473 814.139 ]\n",
      "Weights: [-4.1354 -1.011  -0.2731  0.0958  0.0977]\n",
      "MSE loss: 162.5963\n",
      "Iteration: 65000\n",
      "Gradient: [ 65.6069 -39.9685   7.8782  48.8439 187.8538]\n",
      "Weights: [-4.1359 -1.0102 -0.2729  0.0958  0.0978]\n",
      "MSE loss: 162.5212\n",
      "Iteration: 65100\n",
      "Gradient: [   41.1213  -172.7689  -150.722    -30.962  -1148.2031]\n",
      "Weights: [-4.1363 -1.0095 -0.2727  0.0953  0.0977]\n",
      "MSE loss: 162.4123\n",
      "Iteration: 65200\n",
      "Gradient: [  54.8523  -99.8232  -39.4031 -292.0034 -584.7893]\n",
      "Weights: [-4.1368 -1.0087 -0.2725  0.0952  0.0978]\n",
      "MSE loss: 162.309\n",
      "Iteration: 65300\n",
      "Gradient: [  15.9627  -56.2668  110.6335 -364.5993 -573.1187]\n",
      "Weights: [-4.1373 -1.008  -0.2723  0.095   0.0978]\n",
      "MSE loss: 162.2168\n",
      "Iteration: 65400\n",
      "Gradient: [  37.2423  -86.3089  -45.3957  352.1525 -182.4587]\n",
      "Weights: [-4.1377 -1.0073 -0.2722  0.0946  0.0979]\n",
      "MSE loss: 162.1218\n",
      "Iteration: 65500\n",
      "Gradient: [ 56.2276 -45.6573 -75.049  246.69   -32.7093]\n",
      "Weights: [-4.1383 -1.0066 -0.2721  0.0944  0.098 ]\n",
      "MSE loss: 162.0349\n",
      "Iteration: 65600\n",
      "Gradient: [  41.8381 -130.4898   77.8566  378.6306  147.3114]\n",
      "Weights: [-4.1387 -1.0059 -0.272   0.0939  0.0981]\n",
      "MSE loss: 161.922\n",
      "Iteration: 65700\n",
      "Gradient: [  68.7471   -4.4876 -123.328   471.0842  204.9096]\n",
      "Weights: [-4.1392 -1.0051 -0.2717  0.0934  0.0984]\n",
      "MSE loss: 161.8495\n",
      "Iteration: 65800\n",
      "Gradient: [  56.3546 -144.0194   75.9129 -263.4599  487.8997]\n",
      "Weights: [-4.1397 -1.0044 -0.2716  0.0929  0.0983]\n",
      "MSE loss: 161.7021\n",
      "Iteration: 65900\n",
      "Gradient: [  16.2251 -107.9071   36.1247  150.3731   41.2021]\n",
      "Weights: [-4.1401 -1.0037 -0.2715  0.0924  0.0985]\n",
      "MSE loss: 161.6024\n",
      "Iteration: 66000\n",
      "Gradient: [  66.8647  -77.9678   84.6044 -160.2633  649.9543]\n",
      "Weights: [-4.1406 -1.0029 -0.2713  0.0921  0.0985]\n",
      "MSE loss: 161.5036\n",
      "Iteration: 66100\n",
      "Gradient: [  35.4478 -122.2263 -122.0797  -21.7544  -86.3878]\n",
      "Weights: [-4.1411 -1.0021 -0.271   0.092   0.0986]\n",
      "MSE loss: 161.4276\n",
      "Iteration: 66200\n",
      "Gradient: [  45.2822  -20.492  -134.9562  284.7994 -106.4386]\n",
      "Weights: [-4.1415 -1.0014 -0.2709  0.0918  0.0986]\n",
      "MSE loss: 161.3271\n",
      "Iteration: 66300\n",
      "Gradient: [  50.9115   -1.0753  -45.8985  494.9996 -129.1771]\n",
      "Weights: [-4.142  -1.0006 -0.2707  0.0914  0.0987]\n",
      "MSE loss: 161.2151\n",
      "Iteration: 66400\n",
      "Gradient: [   80.001    -25.505     55.0478  -161.3932 -1635.7202]\n",
      "Weights: [-4.1425 -1.     -0.2705  0.0909  0.0987]\n",
      "MSE loss: 161.1275\n",
      "Iteration: 66500\n",
      "Gradient: [  14.3321  -58.5756   89.4326 -198.366   497.2391]\n",
      "Weights: [-4.143  -0.9993 -0.2705  0.0905  0.0989]\n",
      "MSE loss: 161.0206\n",
      "Iteration: 66600\n",
      "Gradient: [  10.4633  -25.969   -31.725    44.7459 -494.9155]\n",
      "Weights: [-4.1434 -0.9985 -0.2704  0.0903  0.0989]\n",
      "MSE loss: 160.9244\n",
      "Iteration: 66700\n",
      "Gradient: [ 18.7094 -98.5489  93.3993  93.3694 -20.612 ]\n",
      "Weights: [-4.1439 -0.9978 -0.2701  0.0898  0.099 ]\n",
      "MSE loss: 160.8205\n",
      "Iteration: 66800\n",
      "Gradient: [ 17.0298 -96.6691   4.0363  10.331  868.5696]\n",
      "Weights: [-4.1444 -0.9971 -0.2698  0.0893  0.0992]\n",
      "MSE loss: 160.7176\n",
      "Iteration: 66900\n",
      "Gradient: [  33.4164  -39.354     3.5599 -231.9775  170.5671]\n",
      "Weights: [-4.1448 -0.9964 -0.2698  0.0887  0.0993]\n",
      "MSE loss: 160.5954\n",
      "Iteration: 67000\n",
      "Gradient: [ 56.4535 -61.3744  23.0401 -16.1694 161.6001]\n",
      "Weights: [-4.1453 -0.9957 -0.2696  0.0887  0.0992]\n",
      "MSE loss: 160.5443\n",
      "Iteration: 67100\n",
      "Gradient: [  68.5799  -95.1988  170.662   327.0742 -195.7675]\n",
      "Weights: [-4.1457 -0.995  -0.2694  0.0886  0.0993]\n",
      "MSE loss: 160.4513\n",
      "Iteration: 67200\n",
      "Gradient: [  38.8047 -102.7958  -32.5999  315.7271   66.7783]\n",
      "Weights: [-4.1462 -0.9943 -0.2692  0.0885  0.0993]\n",
      "MSE loss: 160.3872\n",
      "Iteration: 67300\n",
      "Gradient: [  8.9631 -47.6418  12.3544 151.8251 562.4261]\n",
      "Weights: [-4.1466 -0.9935 -0.269   0.0881  0.0993]\n",
      "MSE loss: 160.2714\n",
      "Iteration: 67400\n",
      "Gradient: [  13.673  -107.819   -98.4948  129.3827  -25.4025]\n",
      "Weights: [-4.1471 -0.9928 -0.2689  0.0876  0.0994]\n",
      "MSE loss: 160.1842\n",
      "Iteration: 67500\n",
      "Gradient: [ 67.1763 -98.2974 -69.3103 -12.2303 431.0493]\n",
      "Weights: [-4.1475 -0.9921 -0.2688  0.0868  0.0997]\n",
      "MSE loss: 160.0392\n",
      "Iteration: 67600\n",
      "Gradient: [ 75.1024 -59.4701  35.9724 -96.1455 -37.8596]\n",
      "Weights: [-4.148  -0.9914 -0.2686  0.0861  0.1   ]\n",
      "MSE loss: 159.9179\n",
      "Iteration: 67700\n",
      "Gradient: [  19.031  -108.7594   20.3772   -2.0911 -495.4328]\n",
      "Weights: [-4.1485 -0.9907 -0.2682  0.0855  0.1001]\n",
      "MSE loss: 159.8025\n",
      "Iteration: 67800\n",
      "Gradient: [  59.7869 -154.1999 -159.1252 -221.1561  -50.54  ]\n",
      "Weights: [-4.149  -0.9899 -0.268   0.0851  0.1   ]\n",
      "MSE loss: 159.7343\n",
      "Iteration: 67900\n",
      "Gradient: [ 64.6263 -95.6975 -33.8789  31.746  264.5548]\n",
      "Weights: [-4.1495 -0.9892 -0.2678  0.0847  0.1002]\n",
      "MSE loss: 159.6013\n",
      "Iteration: 68000\n",
      "Gradient: [ 37.1633 -66.3349  37.2823  54.4371 217.0892]\n",
      "Weights: [-4.15   -0.9885 -0.2676  0.0845  0.1002]\n",
      "MSE loss: 159.5181\n",
      "Iteration: 68100\n",
      "Gradient: [ 81.4378 -69.6053 122.8853  25.7717 -13.5745]\n",
      "Weights: [-4.1504 -0.9878 -0.2675  0.0841  0.1003]\n",
      "MSE loss: 159.4198\n",
      "Iteration: 68200\n",
      "Gradient: [   0.476   -64.8625   29.3751 -101.8999 -430.0636]\n",
      "Weights: [-4.1509 -0.9871 -0.2673  0.0835  0.1005]\n",
      "MSE loss: 159.3161\n",
      "Iteration: 68300\n",
      "Gradient: [  15.4928 -130.4201  -10.1586 -118.7218  487.6372]\n",
      "Weights: [-4.1513 -0.9863 -0.2673  0.0829  0.1007]\n",
      "MSE loss: 159.1993\n",
      "Iteration: 68400\n",
      "Gradient: [  53.6839  -86.2815  -20.3562  469.8483 -405.7085]\n",
      "Weights: [-4.1518 -0.9856 -0.267   0.0824  0.1008]\n",
      "MSE loss: 159.0928\n",
      "Iteration: 68500\n",
      "Gradient: [   38.9056   -42.8646   -21.9973   128.046  -1140.4314]\n",
      "Weights: [-4.1522 -0.9848 -0.2667  0.0822  0.1009]\n",
      "MSE loss: 159.0016\n",
      "Iteration: 68600\n",
      "Gradient: [  60.0492 -126.8268 -132.2826  -15.1044  639.7635]\n",
      "Weights: [-4.1527 -0.9842 -0.2666  0.0812  0.1012]\n",
      "MSE loss: 158.8644\n",
      "Iteration: 68700\n",
      "Gradient: [  36.7594  -58.8583   30.1842   14.3595 -278.4897]\n",
      "Weights: [-4.1531 -0.9834 -0.2663  0.0806  0.1013]\n",
      "MSE loss: 158.752\n",
      "Iteration: 68800\n",
      "Gradient: [  58.2877  -89.4186  167.5402  -75.1995 1413.9537]\n",
      "Weights: [-4.1536 -0.9827 -0.2662  0.0803  0.1015]\n",
      "MSE loss: 158.692\n",
      "Iteration: 68900\n",
      "Gradient: [  59.5884  -83.0211  -83.5434 -321.3946 -454.0839]\n",
      "Weights: [-4.154  -0.9819 -0.266   0.0794  0.1016]\n",
      "MSE loss: 158.5332\n",
      "Iteration: 69000\n",
      "Gradient: [  54.4745  -97.0677   -9.2788  194.4593 1007.2343]\n",
      "Weights: [-4.1544 -0.9811 -0.2658  0.0792  0.1017]\n",
      "MSE loss: 158.4377\n",
      "Iteration: 69100\n",
      "Gradient: [ 52.407  -76.3451  15.2032 148.8984 246.426 ]\n",
      "Weights: [-4.1549 -0.9804 -0.2656  0.0786  0.1018]\n",
      "MSE loss: 158.3435\n",
      "Iteration: 69200\n",
      "Gradient: [  46.0222    8.1082   32.493    67.8416 -136.3851]\n",
      "Weights: [-4.1553 -0.9797 -0.2654  0.0782  0.1021]\n",
      "MSE loss: 158.2513\n",
      "Iteration: 69300\n",
      "Gradient: [  84.8579  -38.4915  -17.3466 -189.2947 -506.7119]\n",
      "Weights: [-4.1557 -0.9789 -0.2653  0.0781  0.1019]\n",
      "MSE loss: 158.1564\n",
      "Iteration: 69400\n",
      "Gradient: [ 43.5464 -46.9379 -95.1516 391.7249 661.8534]\n",
      "Weights: [-4.1562 -0.9782 -0.2653  0.0777  0.1022]\n",
      "MSE loss: 158.084\n",
      "Iteration: 69500\n",
      "Gradient: [  36.2842 -125.905    60.718   268.8408 -554.4435]\n",
      "Weights: [-4.1566 -0.9775 -0.265   0.0773  0.1022]\n",
      "MSE loss: 157.9593\n",
      "Iteration: 69600\n",
      "Gradient: [ 48.1727 -16.5934 -62.5037  45.9586 511.4746]\n",
      "Weights: [-4.1571 -0.9768 -0.2648  0.077   0.1023]\n",
      "MSE loss: 157.8905\n",
      "Iteration: 69700\n",
      "Gradient: [  22.3234  -45.3713  -40.6905  105.548  1261.894 ]\n",
      "Weights: [-4.1575 -0.9761 -0.2649  0.0761  0.1026]\n",
      "MSE loss: 157.7554\n",
      "Iteration: 69800\n",
      "Gradient: [  12.0802 -101.158   -96.3391  265.5304  907.9512]\n",
      "Weights: [-4.158  -0.9754 -0.2646  0.0756  0.1027]\n",
      "MSE loss: 157.6534\n",
      "Iteration: 69900\n",
      "Gradient: [  25.9469  -29.3686   66.0916  227.2796 -240.4048]\n",
      "Weights: [-4.1584 -0.9747 -0.2644  0.075   0.1027]\n",
      "MSE loss: 157.5452\n",
      "Iteration: 70000\n",
      "Gradient: [115.4204 -83.5124 -77.4711   4.0701 951.359 ]\n",
      "Weights: [-4.1588 -0.974  -0.2642  0.0744  0.1029]\n",
      "MSE loss: 157.4367\n",
      "Iteration: 70100\n",
      "Gradient: [  57.3578  -82.1792   21.0545  243.2055 -120.4759]\n",
      "Weights: [-4.1592 -0.9732 -0.2639  0.0741  0.1029]\n",
      "MSE loss: 157.3429\n",
      "Iteration: 70200\n",
      "Gradient: [  51.2245   -2.0562   49.251     5.3766 -273.3261]\n",
      "Weights: [-4.1596 -0.9724 -0.2637  0.0737  0.103 ]\n",
      "MSE loss: 157.2493\n",
      "Iteration: 70300\n",
      "Gradient: [ 28.8853 -70.8172  98.6115 125.9937 504.6186]\n",
      "Weights: [-4.16   -0.9717 -0.2636  0.0733  0.1032]\n",
      "MSE loss: 157.1469\n",
      "Iteration: 70400\n",
      "Gradient: [  17.2286  -45.4707  206.271   473.5447 -129.3037]\n",
      "Weights: [-4.1605 -0.971  -0.2635  0.0728  0.1034]\n",
      "MSE loss: 157.052\n",
      "Iteration: 70500\n",
      "Gradient: [   6.5007  -76.014    23.6725 -154.7498  962.7727]\n",
      "Weights: [-4.1609 -0.9703 -0.2633  0.072   0.1036]\n",
      "MSE loss: 156.9297\n",
      "Iteration: 70600\n",
      "Gradient: [   85.1083  -124.6665   -29.0789  -145.5324 -1748.0173]\n",
      "Weights: [-4.1613 -0.9695 -0.263   0.0714  0.1037]\n",
      "MSE loss: 156.8223\n",
      "Iteration: 70700\n",
      "Gradient: [  30.5824  -40.2677  -19.8315  -30.8762 1259.4057]\n",
      "Weights: [-4.1616 -0.9688 -0.2627  0.0709  0.1038]\n",
      "MSE loss: 156.7102\n",
      "Iteration: 70800\n",
      "Gradient: [  87.668   -68.9952  -47.1324 -375.8999  130.1944]\n",
      "Weights: [-4.162  -0.9681 -0.2624  0.0705  0.1039]\n",
      "MSE loss: 156.6188\n",
      "Iteration: 70900\n",
      "Gradient: [  61.6167  -89.9729  115.5275  154.3995 1129.0782]\n",
      "Weights: [-4.1624 -0.9674 -0.2621  0.0697  0.1043]\n",
      "MSE loss: 156.5201\n",
      "Iteration: 71000\n",
      "Gradient: [  49.323   -26.8225 -182.0026  -65.4319 -232.7558]\n",
      "Weights: [-4.1628 -0.9666 -0.2619  0.0689  0.1044]\n",
      "MSE loss: 156.3876\n",
      "Iteration: 71100\n",
      "Gradient: [  42.5554  -70.1147 -159.1882 -354.2878 -862.3635]\n",
      "Weights: [-4.1633 -0.9659 -0.2617  0.0685  0.1043]\n",
      "MSE loss: 156.328\n",
      "Iteration: 71200\n",
      "Gradient: [  46.9321  -91.5421 -136.2805 -121.8386 -452.3848]\n",
      "Weights: [-4.1636 -0.9651 -0.2614  0.0684  0.1043]\n",
      "MSE loss: 156.2152\n",
      "Iteration: 71300\n",
      "Gradient: [   54.979    -91.2742  -178.8086  -121.6545 -1038.3428]\n",
      "Weights: [-4.1641 -0.9644 -0.2612  0.0675  0.1047]\n",
      "MSE loss: 156.0782\n",
      "Iteration: 71400\n",
      "Gradient: [   14.3543   -49.5609   -99.221    201.8427 -1519.6358]\n",
      "Weights: [-4.1645 -0.9636 -0.2609  0.0669  0.1048]\n",
      "MSE loss: 155.9874\n",
      "Iteration: 71500\n",
      "Gradient: [   7.886   -66.8463   84.9883   -4.8601 1287.9128]\n",
      "Weights: [-4.1649 -0.963  -0.2608  0.0668  0.1049]\n",
      "MSE loss: 155.8949\n",
      "Iteration: 71600\n",
      "Gradient: [  83.163   -85.6363 -190.2517   30.9277  355.4258]\n",
      "Weights: [-4.1653 -0.9622 -0.2604  0.0664  0.1049]\n",
      "MSE loss: 155.8\n",
      "Iteration: 71700\n",
      "Gradient: [  -1.3982 -118.1132  -13.1535 -171.705   236.8407]\n",
      "Weights: [-4.1658 -0.9615 -0.2602  0.0657  0.1051]\n",
      "MSE loss: 155.6878\n",
      "Iteration: 71800\n",
      "Gradient: [  34.083  -127.5101   -7.691  -503.4627  662.6252]\n",
      "Weights: [-4.1662 -0.9608 -0.2599  0.0652  0.1051]\n",
      "MSE loss: 155.5943\n",
      "Iteration: 71900\n",
      "Gradient: [ 18.3949 -72.0659  17.7296 -90.4861 674.7967]\n",
      "Weights: [-4.1666 -0.9601 -0.2597  0.0651  0.1053]\n",
      "MSE loss: 155.5076\n",
      "Iteration: 72000\n",
      "Gradient: [  30.4321 -109.9752   89.7849   22.0996  -97.3301]\n",
      "Weights: [-4.167  -0.9594 -0.2594  0.0646  0.1053]\n",
      "MSE loss: 155.401\n",
      "Iteration: 72100\n",
      "Gradient: [  71.1929 -127.5609   24.5144  245.3386 -254.4071]\n",
      "Weights: [-4.1675 -0.9586 -0.2591  0.0642  0.1054]\n",
      "MSE loss: 155.3066\n",
      "Iteration: 72200\n",
      "Gradient: [ 51.1358 -56.612  -27.3899 -82.4947 119.1257]\n",
      "Weights: [-4.1679 -0.9579 -0.259   0.0635  0.1056]\n",
      "MSE loss: 155.1989\n",
      "Iteration: 72300\n",
      "Gradient: [  51.7478 -108.6486   -5.6664  441.826   846.7321]\n",
      "Weights: [-4.1683 -0.9572 -0.2589  0.0632  0.1057]\n",
      "MSE loss: 155.1119\n",
      "Iteration: 72400\n",
      "Gradient: [  30.0765  -23.9236  -22.2784  232.1239 -621.7609]\n",
      "Weights: [-4.1687 -0.9565 -0.2587  0.0624  0.106 ]\n",
      "MSE loss: 154.9988\n",
      "Iteration: 72500\n",
      "Gradient: [   27.8208   -89.9156   -75.6458   113.2123 -1041.7268]\n",
      "Weights: [-4.1692 -0.9557 -0.2582  0.062   0.1059]\n",
      "MSE loss: 154.9009\n",
      "Iteration: 72600\n",
      "Gradient: [  68.1519 -112.6327   54.5315   61.2582 -928.0579]\n",
      "Weights: [-4.1695 -0.955  -0.2579  0.0618  0.106 ]\n",
      "MSE loss: 154.8074\n",
      "Iteration: 72700\n",
      "Gradient: [  37.8061 -140.5914   14.7835  530.2216 -422.1318]\n",
      "Weights: [-4.17   -0.9542 -0.2578  0.0615  0.1061]\n",
      "MSE loss: 154.7265\n",
      "Iteration: 72800\n",
      "Gradient: [   79.2115   -32.0866    -3.1869   187.2826 -1090.7826]\n",
      "Weights: [-4.1704 -0.9536 -0.2575  0.0611  0.1062]\n",
      "MSE loss: 154.6436\n",
      "Iteration: 72900\n",
      "Gradient: [   6.6249 -114.5126  232.4929  259.0064 -724.4158]\n",
      "Weights: [-4.1709 -0.9528 -0.2573  0.0605  0.1062]\n",
      "MSE loss: 154.5386\n",
      "Iteration: 73000\n",
      "Gradient: [ 83.0475 -50.6418  90.4717 129.3493 642.0673]\n",
      "Weights: [-4.1713 -0.9521 -0.2571  0.0595  0.1066]\n",
      "MSE loss: 154.3917\n",
      "Iteration: 73100\n",
      "Gradient: [ 40.7963 -69.8819 -64.4525 162.4737 258.3343]\n",
      "Weights: [-4.1718 -0.9514 -0.257   0.0591  0.1067]\n",
      "MSE loss: 154.3076\n",
      "Iteration: 73200\n",
      "Gradient: [ 33.4048 -64.4811 109.3031 200.3464 179.1055]\n",
      "Weights: [-4.1722 -0.9506 -0.2568  0.0587  0.1069]\n",
      "MSE loss: 154.214\n",
      "Iteration: 73300\n",
      "Gradient: [  10.9915  -28.957   -66.3238 -351.3641 -860.9548]\n",
      "Weights: [-4.1726 -0.9499 -0.2566  0.0578  0.1069]\n",
      "MSE loss: 154.1222\n",
      "Iteration: 73400\n",
      "Gradient: [  24.5784 -105.9991   -6.243   -44.7331 -231.2198]\n",
      "Weights: [-4.173  -0.9491 -0.2564  0.0574  0.1071]\n",
      "MSE loss: 154.0005\n",
      "Iteration: 73500\n",
      "Gradient: [ 8.6083300e+01 -7.6925800e+01 -8.7600000e-01 -2.6304280e+02\n",
      "  1.1835757e+03]\n",
      "Weights: [-4.1734 -0.9484 -0.2561  0.0573  0.1072]\n",
      "MSE loss: 153.9046\n",
      "Iteration: 73600\n",
      "Gradient: [   21.0344   -76.8763   -50.0272   163.351  -1081.0793]\n",
      "Weights: [-4.1738 -0.9476 -0.256   0.0564  0.1074]\n",
      "MSE loss: 153.7894\n",
      "Iteration: 73700\n",
      "Gradient: [  23.7213  -74.8307 -123.7641 -327.7523 -344.1351]\n",
      "Weights: [-4.1742 -0.9469 -0.2555  0.0561  0.1074]\n",
      "MSE loss: 153.6967\n",
      "Iteration: 73800\n",
      "Gradient: [  69.7387   38.083  -148.3273   18.0993  139.0156]\n",
      "Weights: [-4.1746 -0.9462 -0.2553  0.0558  0.1076]\n",
      "MSE loss: 153.6312\n",
      "Iteration: 73900\n",
      "Gradient: [  27.9956  -84.5006 -147.2616   62.8923 -524.2374]\n",
      "Weights: [-4.175  -0.9455 -0.2551  0.0554  0.1076]\n",
      "MSE loss: 153.518\n",
      "Iteration: 74000\n",
      "Gradient: [ 11.518  -82.6108 -75.151   95.554  503.2556]\n",
      "Weights: [-4.1754 -0.9448 -0.255   0.0548  0.1078]\n",
      "MSE loss: 153.4185\n",
      "Iteration: 74100\n",
      "Gradient: [  86.6538 -137.6312  -36.0106   74.8267 -321.4325]\n",
      "Weights: [-4.1757 -0.9441 -0.2549  0.0538  0.1079]\n",
      "MSE loss: 153.3391\n",
      "Iteration: 74200\n",
      "Gradient: [  58.958   -27.7097  -80.0014  245.0369 -459.288 ]\n",
      "Weights: [-4.1762 -0.9433 -0.2546  0.0535  0.1081]\n",
      "MSE loss: 153.2082\n",
      "Iteration: 74300\n",
      "Gradient: [ 57.5493 -79.6204  52.8779  47.3111 815.2981]\n",
      "Weights: [-4.1765 -0.9426 -0.2544  0.0533  0.1081]\n",
      "MSE loss: 153.1331\n",
      "Iteration: 74400\n",
      "Gradient: [  59.9398  -20.2045  130.5069  145.0276 1069.3892]\n",
      "Weights: [-4.177  -0.9419 -0.2544  0.0529  0.1083]\n",
      "MSE loss: 153.0463\n",
      "Iteration: 74500\n",
      "Gradient: [  68.0924 -151.8708  -72.7705 -221.1675 -656.6415]\n",
      "Weights: [-4.1774 -0.9411 -0.2544  0.0525  0.1083]\n",
      "MSE loss: 152.9556\n",
      "Iteration: 74600\n",
      "Gradient: [  47.7849  -84.7572 -146.5415  -86.2587  447.0741]\n",
      "Weights: [-4.1778 -0.9404 -0.2542  0.0518  0.1086]\n",
      "MSE loss: 152.8572\n",
      "Iteration: 74700\n",
      "Gradient: [   62.4686  -136.8448    32.8266    37.3749 -1041.8338]\n",
      "Weights: [-4.1782 -0.9397 -0.2539  0.0514  0.1087]\n",
      "MSE loss: 152.7564\n",
      "Iteration: 74800\n",
      "Gradient: [  16.1552 -104.9291   78.1389  -68.1067  -78.8858]\n",
      "Weights: [-4.1786 -0.9391 -0.2539  0.0503  0.109 ]\n",
      "MSE loss: 152.6368\n",
      "Iteration: 74900\n",
      "Gradient: [  33.7891  -87.2296  -63.211  -114.4518 -299.2497]\n",
      "Weights: [-4.179  -0.9383 -0.2536  0.05    0.109 ]\n",
      "MSE loss: 152.5507\n",
      "Iteration: 75000\n",
      "Gradient: [ 81.2886 -34.539  -28.3122 178.1783 322.5403]\n",
      "Weights: [-4.1794 -0.9375 -0.2533  0.0497  0.1092]\n",
      "MSE loss: 152.4607\n",
      "Iteration: 75100\n",
      "Gradient: [   47.7143   -64.7956  -249.3232   -35.0019 -1182.3243]\n",
      "Weights: [-4.1799 -0.9369 -0.2531  0.0489  0.1093]\n",
      "MSE loss: 152.3457\n",
      "Iteration: 75200\n",
      "Gradient: [ 46.3438   1.5584   2.7931 147.7985 -83.2467]\n",
      "Weights: [-4.1803 -0.9362 -0.2529  0.0485  0.1094]\n",
      "MSE loss: 152.2557\n",
      "Iteration: 75300\n",
      "Gradient: [ 35.6171 -56.6001  -5.1456  17.4842 527.528 ]\n",
      "Weights: [-4.1807 -0.9354 -0.2528  0.0483  0.1095]\n",
      "MSE loss: 152.179\n",
      "Iteration: 75400\n",
      "Gradient: [ 74.1053 -38.71    42.0888 398.6031 598.0626]\n",
      "Weights: [-4.181  -0.9347 -0.2525  0.0481  0.1096]\n",
      "MSE loss: 152.1044\n",
      "Iteration: 75500\n",
      "Gradient: [  45.0098 -116.2926   16.4669  124.4894 1014.2932]\n",
      "Weights: [-4.1814 -0.934  -0.2523  0.0475  0.1096]\n",
      "MSE loss: 151.9967\n",
      "Iteration: 75600\n",
      "Gradient: [ 66.6379 -70.8961  57.444  194.7417 227.8003]\n",
      "Weights: [-4.1818 -0.9334 -0.252   0.0469  0.1098]\n",
      "MSE loss: 151.8967\n",
      "Iteration: 75700\n",
      "Gradient: [  57.2828  -42.6145  -83.4689  267.96   -529.9563]\n",
      "Weights: [-4.1822 -0.9326 -0.2519  0.0466  0.1098]\n",
      "MSE loss: 151.8103\n",
      "Iteration: 75800\n",
      "Gradient: [  60.4104  -16.4343 -173.8423  141.8826 -449.6009]\n",
      "Weights: [-4.1826 -0.9319 -0.2517  0.0464  0.1098]\n",
      "MSE loss: 151.7362\n",
      "Iteration: 75900\n",
      "Gradient: [  45.3812  -86.6633  -36.833   182.7491 -148.7962]\n",
      "Weights: [-4.183  -0.9312 -0.2515  0.0458  0.1101]\n",
      "MSE loss: 151.6423\n",
      "Iteration: 76000\n",
      "Gradient: [ 27.348  -32.4156 -54.2275 160.6482 480.4878]\n",
      "Weights: [-4.1834 -0.9305 -0.2514  0.0455  0.1101]\n",
      "MSE loss: 151.5465\n",
      "Iteration: 76100\n",
      "Gradient: [  73.2441  -82.9399 -145.2823 -150.5135 -817.8466]\n",
      "Weights: [-4.1838 -0.9297 -0.2511  0.0452  0.1101]\n",
      "MSE loss: 151.4565\n",
      "Iteration: 76200\n",
      "Gradient: [  84.5184  -95.2517  -49.5386 -393.2181 -885.758 ]\n",
      "Weights: [-4.1842 -0.929  -0.251   0.0444  0.1105]\n",
      "MSE loss: 151.3667\n",
      "Iteration: 76300\n",
      "Gradient: [  81.6087  -73.2242   10.6566  194.0047 -301.2994]\n",
      "Weights: [-4.1846 -0.9283 -0.2508  0.0441  0.1106]\n",
      "MSE loss: 151.3111\n",
      "Iteration: 76400\n",
      "Gradient: [  53.947   -64.2557 -114.7151  -41.64   -428.1018]\n",
      "Weights: [-4.185  -0.9276 -0.2507  0.0436  0.1106]\n",
      "MSE loss: 151.1772\n",
      "Iteration: 76500\n",
      "Gradient: [  68.5601 -103.375    77.0305  -75.2786  450.492 ]\n",
      "Weights: [-4.1854 -0.9269 -0.2504  0.0432  0.1108]\n",
      "MSE loss: 151.1089\n",
      "Iteration: 76600\n",
      "Gradient: [ 43.2536 -59.1766  76.0169 -19.9074  36.2892]\n",
      "Weights: [-4.1858 -0.9262 -0.2503  0.043   0.1107]\n",
      "MSE loss: 151.0077\n",
      "Iteration: 76700\n",
      "Gradient: [  31.4302  -80.6515  131.8724  257.1194 -566.7523]\n",
      "Weights: [-4.1862 -0.9256 -0.2502  0.0426  0.1108]\n",
      "MSE loss: 150.9236\n",
      "Iteration: 76800\n",
      "Gradient: [   26.3679   -49.8564    66.217   -131.7932 -1262.0307]\n",
      "Weights: [-4.1866 -0.9249 -0.25    0.0418  0.1111]\n",
      "MSE loss: 150.8231\n",
      "Iteration: 76900\n",
      "Gradient: [  37.4919  -79.2966 -156.1824 -725.0775  168.7438]\n",
      "Weights: [-4.187  -0.9242 -0.2497  0.0413  0.1112]\n",
      "MSE loss: 150.7247\n",
      "Iteration: 77000\n",
      "Gradient: [  37.4554  -85.7016   11.8576 -228.7114  -84.5332]\n",
      "Weights: [-4.1874 -0.9235 -0.2494  0.0405  0.1114]\n",
      "MSE loss: 150.6145\n",
      "Iteration: 77100\n",
      "Gradient: [  -0.8253  -56.0421   82.2184  -91.7111 -529.7611]\n",
      "Weights: [-4.1878 -0.9228 -0.2492  0.0398  0.1116]\n",
      "MSE loss: 150.5167\n",
      "Iteration: 77200\n",
      "Gradient: [ 34.2334 -48.6211 -68.1401  -2.8731 979.5311]\n",
      "Weights: [-4.1881 -0.9221 -0.2489  0.0401  0.1114]\n",
      "MSE loss: 150.4608\n",
      "Iteration: 77300\n",
      "Gradient: [  61.7859 -102.2694   17.9533   11.4827  -75.0123]\n",
      "Weights: [-4.1886 -0.9214 -0.2487  0.0396  0.1116]\n",
      "MSE loss: 150.3672\n",
      "Iteration: 77400\n",
      "Gradient: [  42.6406 -106.0041  -58.406  -171.6465  -97.7277]\n",
      "Weights: [-4.1889 -0.9206 -0.2484  0.0393  0.1116]\n",
      "MSE loss: 150.279\n",
      "Iteration: 77500\n",
      "Gradient: [ 27.1442 -62.5257  40.4933  65.825   -3.7901]\n",
      "Weights: [-4.1893 -0.9199 -0.2481  0.0391  0.1116]\n",
      "MSE loss: 150.2003\n",
      "Iteration: 77600\n",
      "Gradient: [  54.8504  -95.6624  109.7339   58.7967 1207.0393]\n",
      "Weights: [-4.1897 -0.9192 -0.2478  0.0388  0.1116]\n",
      "MSE loss: 150.1189\n",
      "Iteration: 77700\n",
      "Gradient: [ 93.1098 -29.7327 -48.2629 146.349  413.4417]\n",
      "Weights: [-4.1902 -0.9186 -0.2477  0.0387  0.1116]\n",
      "MSE loss: 150.0506\n",
      "Iteration: 77800\n",
      "Gradient: [ 33.5982 -95.6418  26.5791 292.4431 547.5467]\n",
      "Weights: [-4.1906 -0.9179 -0.2475  0.0385  0.1117]\n",
      "MSE loss: 149.9846\n",
      "Iteration: 77900\n",
      "Gradient: [  93.2194 -106.8063  -44.709  -213.79    372.534 ]\n",
      "Weights: [-4.1911 -0.9173 -0.2474  0.038   0.1118]\n",
      "MSE loss: 149.8813\n",
      "Iteration: 78000\n",
      "Gradient: [  19.4736  -34.1032  -31.7003 -109.9597 -350.432 ]\n",
      "Weights: [-4.1915 -0.9166 -0.2472  0.0376  0.1118]\n",
      "MSE loss: 149.8098\n",
      "Iteration: 78100\n",
      "Gradient: [ -15.7225 -142.4071  -82.3996  -40.7178 -604.4379]\n",
      "Weights: [-4.1919 -0.9161 -0.247   0.0374  0.1117]\n",
      "MSE loss: 149.7666\n",
      "Iteration: 78200\n",
      "Gradient: [  40.7742  -28.5411   46.7065  118.9297 -604.1464]\n",
      "Weights: [-4.1923 -0.9154 -0.247   0.037   0.112 ]\n",
      "MSE loss: 149.6445\n",
      "Iteration: 78300\n",
      "Gradient: [ 35.0626 -47.3575  52.7497 296.1693 889.4595]\n",
      "Weights: [-4.1927 -0.9147 -0.2467  0.0366  0.1122]\n",
      "MSE loss: 149.5722\n",
      "Iteration: 78400\n",
      "Gradient: [   5.0822 -110.8541  -22.5895   29.7919 -944.4312]\n",
      "Weights: [-4.1931 -0.914  -0.2466  0.0357  0.1124]\n",
      "MSE loss: 149.4497\n",
      "Iteration: 78500\n",
      "Gradient: [  73.0283  -54.2346   67.5484 -179.9137 -149.0684]\n",
      "Weights: [-4.1936 -0.9133 -0.2465  0.0351  0.1125]\n",
      "MSE loss: 149.3518\n",
      "Iteration: 78600\n",
      "Gradient: [ 115.3371   -6.4299 -188.7237   87.8395 -657.4614]\n",
      "Weights: [-4.194  -0.9126 -0.2463  0.0349  0.1125]\n",
      "MSE loss: 149.2906\n",
      "Iteration: 78700\n",
      "Gradient: [  45.536   -30.5864 -142.879   174.4135  -19.4536]\n",
      "Weights: [-4.1944 -0.9119 -0.246   0.0346  0.1126]\n",
      "MSE loss: 149.1961\n",
      "Iteration: 78800\n",
      "Gradient: [  49.8716  -76.8535 -163.6516  192.5951 -457.5562]\n",
      "Weights: [-4.1948 -0.9113 -0.2456  0.0344  0.1127]\n",
      "MSE loss: 149.1179\n",
      "Iteration: 78900\n",
      "Gradient: [ 42.3515 -77.8455  84.0259 107.0548 355.2494]\n",
      "Weights: [-4.1953 -0.9106 -0.2455  0.0339  0.1129]\n",
      "MSE loss: 149.0428\n",
      "Iteration: 79000\n",
      "Gradient: [  65.4697  -85.122   121.2449 -538.5057 -594.3265]\n",
      "Weights: [-4.1957 -0.91   -0.2455  0.0332  0.113 ]\n",
      "MSE loss: 148.9338\n",
      "Iteration: 79100\n",
      "Gradient: [ 31.2842 -31.1804 -18.5372 -59.249  826.4287]\n",
      "Weights: [-4.1961 -0.9093 -0.2454  0.0323  0.1134]\n",
      "MSE loss: 148.8367\n",
      "Iteration: 79200\n",
      "Gradient: [ 51.6498 -95.2002 -15.546  -60.4369 712.2661]\n",
      "Weights: [-4.1965 -0.9086 -0.2451  0.0316  0.1134]\n",
      "MSE loss: 148.7277\n",
      "Iteration: 79300\n",
      "Gradient: [  61.9464  -38.1335  -16.0525  302.5772 1135.7157]\n",
      "Weights: [-4.1969 -0.9079 -0.2448  0.031   0.1136]\n",
      "MSE loss: 148.6261\n",
      "Iteration: 79400\n",
      "Gradient: [  69.1268  -57.2301  107.1939 -352.8242 -313.5343]\n",
      "Weights: [-4.1973 -0.9072 -0.2448  0.0301  0.1139]\n",
      "MSE loss: 148.521\n",
      "Iteration: 79500\n",
      "Gradient: [  75.212  -100.4853  107.7748  387.0436  -55.2205]\n",
      "Weights: [-4.1977 -0.9065 -0.2447  0.0298  0.1141]\n",
      "MSE loss: 148.449\n",
      "Iteration: 79600\n",
      "Gradient: [   6.4443  -29.1175  -35.1737  339.5251 -913.9018]\n",
      "Weights: [-4.198  -0.9057 -0.2445  0.0292  0.1141]\n",
      "MSE loss: 148.347\n",
      "Iteration: 79700\n",
      "Gradient: [  70.2743  -19.6013 -100.6857  -59.5444 -680.6644]\n",
      "Weights: [-4.1983 -0.905  -0.2442  0.0289  0.1142]\n",
      "MSE loss: 148.2615\n",
      "Iteration: 79800\n",
      "Gradient: [  -8.1759  -79.0288  -84.4699  197.3127 -662.6106]\n",
      "Weights: [-4.1987 -0.9044 -0.2442  0.0282  0.1143]\n",
      "MSE loss: 148.2093\n",
      "Iteration: 79900\n",
      "Gradient: [ 74.2145 -91.1098  48.4773 350.3956  -2.4078]\n",
      "Weights: [-4.1991 -0.9037 -0.244   0.0278  0.1146]\n",
      "MSE loss: 148.0839\n",
      "Iteration: 80000\n",
      "Gradient: [  33.4223  -88.1055   13.9049 -278.6195  465.879 ]\n",
      "Weights: [-4.1995 -0.903  -0.2437  0.0277  0.1145]\n",
      "MSE loss: 148.0114\n",
      "Iteration: 80100\n",
      "Gradient: [ 39.5654 -76.589  -73.0257 116.5593 508.8983]\n",
      "Weights: [-4.1998 -0.9023 -0.2434  0.0275  0.1145]\n",
      "MSE loss: 147.9336\n",
      "Iteration: 80200\n",
      "Gradient: [   3.9702  -51.8639  -21.2361  -92.7718 -233.6651]\n",
      "Weights: [-4.2003 -0.9016 -0.2432  0.027   0.1147]\n",
      "MSE loss: 147.8501\n",
      "Iteration: 80300\n",
      "Gradient: [ 91.1603 -35.1784  87.7664 123.0398  30.5658]\n",
      "Weights: [-4.2006 -0.901  -0.2429  0.0265  0.1149]\n",
      "MSE loss: 147.8149\n",
      "Iteration: 80400\n",
      "Gradient: [  22.6848 -109.3172  121.0812  150.9702  740.3979]\n",
      "Weights: [-4.201  -0.9002 -0.2428  0.0265  0.1147]\n",
      "MSE loss: 147.6967\n",
      "Iteration: 80500\n",
      "Gradient: [ 26.1169 -11.0656  21.536  106.1254 614.3748]\n",
      "Weights: [-4.2014 -0.8996 -0.2426  0.0259  0.1149]\n",
      "MSE loss: 147.6101\n",
      "Iteration: 80600\n",
      "Gradient: [  17.1523 -101.8264   44.7239 -304.6154  467.2714]\n",
      "Weights: [-4.2018 -0.899  -0.2424  0.0254  0.115 ]\n",
      "MSE loss: 147.5201\n",
      "Iteration: 80700\n",
      "Gradient: [  54.5345 -135.9461  -68.2453  439.4019  236.933 ]\n",
      "Weights: [-4.2022 -0.8983 -0.2421  0.0245  0.1153]\n",
      "MSE loss: 147.4116\n",
      "Iteration: 80800\n",
      "Gradient: [ -15.2376 -129.3186 -125.1774  493.376  -241.7553]\n",
      "Weights: [-4.2025 -0.8975 -0.2419  0.0243  0.1152]\n",
      "MSE loss: 147.3377\n",
      "Iteration: 80900\n",
      "Gradient: [  -3.9752  -39.5423  -70.848   521.4125 -749.7563]\n",
      "Weights: [-4.2029 -0.8969 -0.2417  0.0241  0.1152]\n",
      "MSE loss: 147.2704\n",
      "Iteration: 81000\n",
      "Gradient: [  -2.4811  -96.8053 -229.1433 -394.7706 -187.5372]\n",
      "Weights: [-4.2033 -0.8963 -0.2415  0.0235  0.1154]\n",
      "MSE loss: 147.1821\n",
      "Iteration: 81100\n",
      "Gradient: [  52.1512  -68.9914  -82.772    82.0407 -176.2079]\n",
      "Weights: [-4.2037 -0.8956 -0.2414  0.023   0.1155]\n",
      "MSE loss: 147.1142\n",
      "Iteration: 81200\n",
      "Gradient: [  61.2358  -60.4012  153.8674  139.0422 -835.2192]\n",
      "Weights: [-4.2041 -0.8949 -0.2412  0.0225  0.1158]\n",
      "MSE loss: 147.0034\n",
      "Iteration: 81300\n",
      "Gradient: [ 5.5122500e+01 -5.4137300e+01  2.4570000e-01 -8.6498300e+01\n",
      " -1.3163133e+03]\n",
      "Weights: [-4.2044 -0.8943 -0.241   0.0222  0.1157]\n",
      "MSE loss: 146.9397\n",
      "Iteration: 81400\n",
      "Gradient: [  63.2961 -141.3106  -62.3848  118.9292 -330.1754]\n",
      "Weights: [-4.2048 -0.8936 -0.2408  0.0218  0.1158]\n",
      "MSE loss: 146.8513\n",
      "Iteration: 81500\n",
      "Gradient: [   1.4283 -116.0411   26.278  -146.5711  830.2491]\n",
      "Weights: [-4.2051 -0.8929 -0.2404  0.0211  0.1162]\n",
      "MSE loss: 146.7553\n",
      "Iteration: 81600\n",
      "Gradient: [  38.2207 -175.1579  -57.3876 -140.0996 -612.8907]\n",
      "Weights: [-4.2055 -0.8922 -0.2401  0.0202  0.1162]\n",
      "MSE loss: 146.6557\n",
      "Iteration: 81700\n",
      "Gradient: [ 42.4043  30.7808  77.5122 185.6797 540.4619]\n",
      "Weights: [-4.2059 -0.8915 -0.2398  0.0199  0.1164]\n",
      "MSE loss: 146.5595\n",
      "Iteration: 81800\n",
      "Gradient: [  53.0527  -42.3866  -80.8634 -272.2327  724.0825]\n",
      "Weights: [-4.2063 -0.8909 -0.2396  0.0194  0.1164]\n",
      "MSE loss: 146.4826\n",
      "Iteration: 81900\n",
      "Gradient: [  31.1644  -86.1763  -64.1873  -99.1966 -568.5259]\n",
      "Weights: [-4.2067 -0.8902 -0.2395  0.0189  0.1167]\n",
      "MSE loss: 146.3916\n",
      "Iteration: 82000\n",
      "Gradient: [  29.3093  -73.5041 -177.1331  282.5872  564.1825]\n",
      "Weights: [-4.2071 -0.8894 -0.2392  0.0184  0.1168]\n",
      "MSE loss: 146.2969\n",
      "Iteration: 82100\n",
      "Gradient: [  30.6176  -91.7615   21.1594  140.2651 -551.8015]\n",
      "Weights: [-4.2074 -0.8887 -0.239   0.0185  0.1167]\n",
      "MSE loss: 146.2428\n",
      "Iteration: 82200\n",
      "Gradient: [ -21.417  -118.8302   -6.135  -157.1445  279.628 ]\n",
      "Weights: [-4.2078 -0.8881 -0.2388  0.0179  0.1168]\n",
      "MSE loss: 146.1459\n",
      "Iteration: 82300\n",
      "Gradient: [  13.2458  -43.3456  -10.6441  -79.7436 -921.6269]\n",
      "Weights: [-4.2082 -0.8873 -0.2384  0.0174  0.1169]\n",
      "MSE loss: 146.0623\n",
      "Iteration: 82400\n",
      "Gradient: [ 34.1943 -68.7765  57.6477   8.4878 734.5435]\n",
      "Weights: [-4.2086 -0.8866 -0.2382  0.017   0.1171]\n",
      "MSE loss: 145.9713\n",
      "Iteration: 82500\n",
      "Gradient: [  22.7273 -108.3535  -27.125    41.9716  719.7606]\n",
      "Weights: [-4.209  -0.8859 -0.2382  0.017   0.117 ]\n",
      "MSE loss: 145.9022\n",
      "Iteration: 82600\n",
      "Gradient: [  64.1828  -93.7147 -143.4765  -63.1997   98.3714]\n",
      "Weights: [-4.2094 -0.8853 -0.2378  0.0167  0.117 ]\n",
      "MSE loss: 145.8296\n",
      "Iteration: 82700\n",
      "Gradient: [ -20.8516  -87.8775 -206.1648 -362.9445  219.4192]\n",
      "Weights: [-4.2098 -0.8846 -0.2377  0.0158  0.1172]\n",
      "MSE loss: 145.7618\n",
      "Iteration: 82800\n",
      "Gradient: [   6.696   -96.362   -67.8038  424.7977 -161.9251]\n",
      "Weights: [-4.2102 -0.884  -0.2375  0.0155  0.1172]\n",
      "MSE loss: 145.6913\n",
      "Iteration: 82900\n",
      "Gradient: [ 56.1793 -80.779   -2.6478 309.3056 -14.5856]\n",
      "Weights: [-4.2105 -0.8833 -0.2372  0.0151  0.1175]\n",
      "MSE loss: 145.5772\n",
      "Iteration: 83000\n",
      "Gradient: [  43.9747  -93.3797   98.016    25.3904 -980.4366]\n",
      "Weights: [-4.2109 -0.8827 -0.2372  0.0143  0.1176]\n",
      "MSE loss: 145.5241\n",
      "Iteration: 83100\n",
      "Gradient: [  55.4365  -63.6455 -136.4205 -361.6371  757.2392]\n",
      "Weights: [-4.2113 -0.882  -0.2369  0.0141  0.1179]\n",
      "MSE loss: 145.4311\n",
      "Iteration: 83200\n",
      "Gradient: [ 42.5429 -45.5596 -16.6977 111.7135  82.016 ]\n",
      "Weights: [-4.2117 -0.8814 -0.2368  0.0135  0.1179]\n",
      "MSE loss: 145.3251\n",
      "Iteration: 83300\n",
      "Gradient: [  71.3062  -27.7906 -153.8694 -124.8701  284.2081]\n",
      "Weights: [-4.2121 -0.8807 -0.2367  0.0132  0.1181]\n",
      "MSE loss: 145.2497\n",
      "Iteration: 83400\n",
      "Gradient: [ 46.48   -85.7968 -57.473   81.4529 -86.1497]\n",
      "Weights: [-4.2125 -0.8801 -0.2364  0.0131  0.1179]\n",
      "MSE loss: 145.1938\n",
      "Iteration: 83500\n",
      "Gradient: [  55.2139   25.0822  -26.3174  -68.2301 -192.7878]\n",
      "Weights: [-4.2129 -0.8794 -0.2361  0.0127  0.1181]\n",
      "MSE loss: 145.0945\n",
      "Iteration: 83600\n",
      "Gradient: [  25.1547  -56.0509 -204.8553 -197.1679 -193.54  ]\n",
      "Weights: [-4.2133 -0.8787 -0.236   0.0121  0.1182]\n",
      "MSE loss: 145.0075\n",
      "Iteration: 83700\n",
      "Gradient: [   9.6739  -57.2937   22.2847 -337.1372   -6.3168]\n",
      "Weights: [-4.2138 -0.8781 -0.236   0.0116  0.1182]\n",
      "MSE loss: 144.9552\n",
      "Iteration: 83800\n",
      "Gradient: [   38.6591   -59.9403   -10.3325  -230.2024 -1074.9917]\n",
      "Weights: [-4.2141 -0.8775 -0.2355  0.0114  0.1183]\n",
      "MSE loss: 144.8606\n",
      "Iteration: 83900\n",
      "Gradient: [  95.3416  -76.2596  -22.4207 -214.2469 -446.802 ]\n",
      "Weights: [-4.2146 -0.8768 -0.2353  0.0108  0.1185]\n",
      "MSE loss: 144.7698\n",
      "Iteration: 84000\n",
      "Gradient: [ 65.6325 -31.2113  56.1168 -75.8978  44.2259]\n",
      "Weights: [-4.215  -0.8762 -0.2352  0.0102  0.1188]\n",
      "MSE loss: 144.6928\n",
      "Iteration: 84100\n",
      "Gradient: [  70.3229  -11.8748  113.396    21.8106 -778.7177]\n",
      "Weights: [-4.2154 -0.8757 -0.2351  0.0098  0.1189]\n",
      "MSE loss: 144.6062\n",
      "Iteration: 84200\n",
      "Gradient: [  22.9197  -63.4043 -136.7552 -451.3508  334.6355]\n",
      "Weights: [-4.2158 -0.875  -0.235   0.0092  0.1191]\n",
      "MSE loss: 144.5243\n",
      "Iteration: 84300\n",
      "Gradient: [  65.9255  -55.8663  -32.4824  -30.4476 -963.4768]\n",
      "Weights: [-4.2162 -0.8743 -0.2349  0.0091  0.1189]\n",
      "MSE loss: 144.4615\n",
      "Iteration: 84400\n",
      "Gradient: [ 44.9208 -83.9942  34.5798 265.0418 616.5356]\n",
      "Weights: [-4.2166 -0.8737 -0.2346  0.0087  0.1192]\n",
      "MSE loss: 144.3924\n",
      "Iteration: 84500\n",
      "Gradient: [ 61.8856  -5.41   -64.5591  37.7071 462.2496]\n",
      "Weights: [-4.217  -0.873  -0.2345  0.0081  0.1194]\n",
      "MSE loss: 144.3141\n",
      "Iteration: 84600\n",
      "Gradient: [  21.5997 -135.4699  -31.7519  240.4492   14.0204]\n",
      "Weights: [-4.2174 -0.8723 -0.2342  0.0075  0.1194]\n",
      "MSE loss: 144.1948\n",
      "Iteration: 84700\n",
      "Gradient: [  28.1547 -106.9305   11.2016  -64.3073 -539.7071]\n",
      "Weights: [-4.2178 -0.8717 -0.234   0.0074  0.1193]\n",
      "MSE loss: 144.1375\n",
      "Iteration: 84800\n",
      "Gradient: [  -6.4456  -36.7457  -23.3448   29.5195 -244.006 ]\n",
      "Weights: [-4.2181 -0.871  -0.2337  0.0071  0.1194]\n",
      "MSE loss: 144.0598\n",
      "Iteration: 84900\n",
      "Gradient: [  -3.6899 -114.1908 -103.7328 -217.1604 -280.6352]\n",
      "Weights: [-4.2185 -0.8705 -0.2334  0.0066  0.1195]\n",
      "MSE loss: 143.9892\n",
      "Iteration: 85000\n",
      "Gradient: [  59.9599  -91.7572  -56.879    56.2109 -896.6597]\n",
      "Weights: [-4.2189 -0.8698 -0.2332  0.006   0.1196]\n",
      "MSE loss: 143.9059\n",
      "Iteration: 85100\n",
      "Gradient: [  10.6687  -51.7875  -58.0679 -222.9879 -260.7423]\n",
      "Weights: [-4.2193 -0.8692 -0.233   0.006   0.1196]\n",
      "MSE loss: 143.8456\n",
      "Iteration: 85200\n",
      "Gradient: [   30.0324   -92.636    111.7209   274.8617 -1001.4624]\n",
      "Weights: [-4.2196 -0.8686 -0.2328  0.0056  0.1196]\n",
      "MSE loss: 143.7856\n",
      "Iteration: 85300\n",
      "Gradient: [ 29.6181 -67.7866  79.8835 196.467  821.3769]\n",
      "Weights: [-4.22   -0.868  -0.2326  0.005   0.12  ]\n",
      "MSE loss: 143.6808\n",
      "Iteration: 85400\n",
      "Gradient: [ 10.1401 -99.6459 145.8974 179.3932 467.5218]\n",
      "Weights: [-4.2204 -0.8673 -0.2325  0.0047  0.1201]\n",
      "MSE loss: 143.6105\n",
      "Iteration: 85500\n",
      "Gradient: [  64.0041  -79.6413  129.3648  200.9431 -448.848 ]\n",
      "Weights: [-4.2208e+00 -8.6670e-01 -2.3240e-01  3.9000e-03  1.2020e-01]\n",
      "MSE loss: 143.5145\n",
      "Iteration: 85600\n",
      "Gradient: [ 45.4193  -9.3808 -75.8984  77.1248 267.9276]\n",
      "Weights: [-4.2212e+00 -8.6600e-01 -2.3220e-01  3.4000e-03  1.2030e-01]\n",
      "MSE loss: 143.4362\n",
      "Iteration: 85700\n",
      "Gradient: [  -0.3563  -75.2547 -189.3522  166.1688   66.9202]\n",
      "Weights: [-4.2215e+00 -8.6540e-01 -2.3190e-01  3.4000e-03  1.2040e-01]\n",
      "MSE loss: 143.3716\n",
      "Iteration: 85800\n",
      "Gradient: [ 19.7478 -51.027  -59.23    28.6066 638.868 ]\n",
      "Weights: [-4.2219e+00 -8.6480e-01 -2.3160e-01  3.2000e-03  1.2040e-01]\n",
      "MSE loss: 143.3075\n",
      "Iteration: 85900\n",
      "Gradient: [ 31.6247 -32.8615  51.5974 144.124  153.0466]\n",
      "Weights: [-4.2223e+00 -8.6420e-01 -2.3140e-01  2.9000e-03  1.2040e-01]\n",
      "MSE loss: 143.2393\n",
      "Iteration: 86000\n",
      "Gradient: [  10.4689 -129.66   -184.204  -152.5848   11.2707]\n",
      "Weights: [-4.2227e+00 -8.6360e-01 -2.3110e-01  2.9000e-03  1.2020e-01]\n",
      "MSE loss: 143.2294\n",
      "Iteration: 86100\n",
      "Gradient: [  99.0312  -96.691   118.9245 -151.9401 -681.9953]\n",
      "Weights: [-4.2231e+00 -8.6300e-01 -2.3090e-01  2.6000e-03  1.2030e-01]\n",
      "MSE loss: 143.1194\n",
      "Iteration: 86200\n",
      "Gradient: [ 12.3776 -51.6575 -64.2076 392.6896  67.2095]\n",
      "Weights: [-4.2235e+00 -8.6240e-01 -2.3090e-01  1.9000e-03  1.2070e-01]\n",
      "MSE loss: 143.0447\n",
      "Iteration: 86300\n",
      "Gradient: [   30.7812   -49.65      23.5059   150.6815 -1530.3516]\n",
      "Weights: [-4.2239e+00 -8.6180e-01 -2.3070e-01  1.5000e-03  1.2070e-01]\n",
      "MSE loss: 142.9586\n",
      "Iteration: 86400\n",
      "Gradient: [  33.8026  -37.3698   26.6297   84.2061 -560.4594]\n",
      "Weights: [-4.2242e+00 -8.6120e-01 -2.3050e-01  7.0000e-04  1.2100e-01]\n",
      "MSE loss: 142.8586\n",
      "Iteration: 86500\n",
      "Gradient: [  48.8604 -110.6982 -257.3001  -89.0868 -132.2202]\n",
      "Weights: [-4.2246e+00 -8.6060e-01 -2.3030e-01  4.0000e-04  1.2100e-01]\n",
      "MSE loss: 142.7987\n",
      "Iteration: 86600\n",
      "Gradient: [  34.7179  -84.3958  -85.3824 -279.3839 -961.7513]\n",
      "Weights: [-4.2249e+00 -8.6000e-01 -2.3000e-01  3.0000e-04  1.2080e-01]\n",
      "MSE loss: 142.7811\n",
      "Iteration: 86700\n",
      "Gradient: [  38.9647  -99.4222  -34.5437   71.6801 -257.0871]\n",
      "Weights: [-4.2253e+00 -8.5940e-01 -2.2990e-01  1.0000e-04  1.2090e-01]\n",
      "MSE loss: 142.7003\n",
      "Iteration: 86800\n",
      "Gradient: [ 104.9107 -100.1574  -91.7704 -154.4603  644.1956]\n",
      "Weights: [-4.2257e+00 -8.5870e-01 -2.2980e-01 -4.0000e-04  1.2120e-01]\n",
      "MSE loss: 142.5906\n",
      "Iteration: 86900\n",
      "Gradient: [  80.4246  -41.2806  123.8437  222.3938 -185.8095]\n",
      "Weights: [-4.2261e+00 -8.5810e-01 -2.2950e-01 -9.0000e-04  1.2140e-01]\n",
      "MSE loss: 142.5261\n",
      "Iteration: 87000\n",
      "Gradient: [ 42.9725 -20.4468  72.432   55.6524 380.4986]\n",
      "Weights: [-4.2265e+00 -8.5750e-01 -2.2940e-01 -1.2000e-03  1.2120e-01]\n",
      "MSE loss: 142.4749\n",
      "Iteration: 87100\n",
      "Gradient: [  21.0411  -64.8882 -102.1461 -296.1179   93.6171]\n",
      "Weights: [-4.2268e+00 -8.5690e-01 -2.2920e-01 -1.8000e-03  1.2140e-01]\n",
      "MSE loss: 142.407\n",
      "Iteration: 87200\n",
      "Gradient: [  37.895   -88.8597  -92.2208  -22.9053 1082.1052]\n",
      "Weights: [-4.2272e+00 -8.5620e-01 -2.2880e-01 -2.3000e-03  1.2170e-01]\n",
      "MSE loss: 142.2823\n",
      "Iteration: 87300\n",
      "Gradient: [ 56.5125 -12.5775  58.4584 198.4257  40.8086]\n",
      "Weights: [-4.2276e+00 -8.5570e-01 -2.2870e-01 -2.9000e-03  1.2180e-01]\n",
      "MSE loss: 142.1953\n",
      "Iteration: 87400\n",
      "Gradient: [  63.8801  -28.9814  -39.794  -346.9908 -223.2843]\n",
      "Weights: [-4.228e+00 -8.550e-01 -2.283e-01 -3.600e-03  1.221e-01]\n",
      "MSE loss: 142.1251\n",
      "Iteration: 87500\n",
      "Gradient: [  50.2976  -49.0201   50.8216  407.1372 -164.4033]\n",
      "Weights: [-4.2284e+00 -8.5450e-01 -2.2810e-01 -4.2000e-03  1.2240e-01]\n",
      "MSE loss: 142.0699\n",
      "Iteration: 87600\n",
      "Gradient: [  24.3151  -81.4146  -13.1065 -145.0532 1580.4721]\n",
      "Weights: [-4.2288 -0.8539 -0.2279 -0.0046  0.1222]\n",
      "MSE loss: 141.9594\n",
      "Iteration: 87700\n",
      "Gradient: [ 69.0309 -27.794   35.8224 -11.0015 261.8039]\n",
      "Weights: [-4.2291 -0.8532 -0.228  -0.005   0.1225]\n",
      "MSE loss: 141.9022\n",
      "Iteration: 87800\n",
      "Gradient: [  42.3811  -85.239   193.6422  204.076  -172.2276]\n",
      "Weights: [-4.2295 -0.8526 -0.2279 -0.0056  0.1226]\n",
      "MSE loss: 141.8049\n",
      "Iteration: 87900\n",
      "Gradient: [ 16.4192 -40.2346 144.7868  73.8441 835.3934]\n",
      "Weights: [-4.2299 -0.852  -0.2278 -0.0064  0.1229]\n",
      "MSE loss: 141.7254\n",
      "Iteration: 88000\n",
      "Gradient: [   35.5704  -157.9263   131.1958  -177.0261 -1553.4787]\n",
      "Weights: [-4.2302 -0.8513 -0.2276 -0.0066  0.1227]\n",
      "MSE loss: 141.6734\n",
      "Iteration: 88100\n",
      "Gradient: [  47.9874  -12.1933 -130.6771 -459.7181 -204.5345]\n",
      "Weights: [-4.2305 -0.8506 -0.2274 -0.0067  0.1229]\n",
      "MSE loss: 141.5889\n",
      "Iteration: 88200\n",
      "Gradient: [  78.1836  -39.1663  -56.9332 -297.6403 -292.6737]\n",
      "Weights: [-4.2309 -0.8499 -0.2272 -0.0071  0.123 ]\n",
      "MSE loss: 141.5061\n",
      "Iteration: 88300\n",
      "Gradient: [  42.178   -59.3154  -19.154   249.2289 -794.5601]\n",
      "Weights: [-4.2313 -0.8493 -0.227  -0.0077  0.1231]\n",
      "MSE loss: 141.4276\n",
      "Iteration: 88400\n",
      "Gradient: [  31.2484 -146.3956  123.1731    4.7833  682.3236]\n",
      "Weights: [-4.2316 -0.8486 -0.2267 -0.008   0.1232]\n",
      "MSE loss: 141.354\n",
      "Iteration: 88500\n",
      "Gradient: [  11.3754  -27.6789 -200.3317  223.9661 -580.8716]\n",
      "Weights: [-4.232  -0.848  -0.2265 -0.0083  0.1233]\n",
      "MSE loss: 141.2844\n",
      "Iteration: 88600\n",
      "Gradient: [ 43.3651 -77.1218 111.9989 228.5541 351.8049]\n",
      "Weights: [-4.2323 -0.8473 -0.2263 -0.0084  0.1232]\n",
      "MSE loss: 141.2211\n",
      "Iteration: 88700\n",
      "Gradient: [   -2.603   -121.1616   -68.9526  -215.5685 -1511.8765]\n",
      "Weights: [-4.2327 -0.8467 -0.2263 -0.0089  0.1233]\n",
      "MSE loss: 141.1477\n",
      "Iteration: 88800\n",
      "Gradient: [  27.0371  -55.9927 -101.422    94.0257  900.1651]\n",
      "Weights: [-4.2331 -0.846  -0.2263 -0.0095  0.1236]\n",
      "MSE loss: 141.0772\n",
      "Iteration: 88900\n",
      "Gradient: [  27.5981  -94.4594   -1.3273   91.5818 -315.9657]\n",
      "Weights: [-4.2335 -0.8454 -0.2261 -0.0102  0.1238]\n",
      "MSE loss: 140.9801\n",
      "Iteration: 89000\n",
      "Gradient: [  43.6455 -103.5008    1.8008   88.9178  276.9037]\n",
      "Weights: [-4.2338 -0.8447 -0.2257 -0.0101  0.1236]\n",
      "MSE loss: 140.9224\n",
      "Iteration: 89100\n",
      "Gradient: [  28.2369  -71.9061 -126.8553  282.0496 -195.5929]\n",
      "Weights: [-4.2342 -0.8441 -0.2255 -0.0106  0.1238]\n",
      "MSE loss: 140.849\n",
      "Iteration: 89200\n",
      "Gradient: [ 11.0647 -79.2804 190.4535 349.7348 198.4628]\n",
      "Weights: [-4.2346 -0.8434 -0.2253 -0.0109  0.1239]\n",
      "MSE loss: 140.7944\n",
      "Iteration: 89300\n",
      "Gradient: [  52.2234 -112.9724  -13.5634   -0.9644 -753.0661]\n",
      "Weights: [-4.2349 -0.8429 -0.2253 -0.0112  0.1239]\n",
      "MSE loss: 140.7164\n",
      "Iteration: 89400\n",
      "Gradient: [  23.8379  -78.4595  118.8685 -251.3928 1031.253 ]\n",
      "Weights: [-4.2353 -0.8422 -0.2252 -0.0113  0.124 ]\n",
      "MSE loss: 140.6737\n",
      "Iteration: 89500\n",
      "Gradient: [ 10.7743 -34.8595  -4.8454 209.8828 122.1298]\n",
      "Weights: [-4.2356 -0.8417 -0.2251 -0.0118  0.1243]\n",
      "MSE loss: 140.6476\n",
      "Iteration: 89600\n",
      "Gradient: [  30.4022 -101.3899   14.766  -183.3752  -57.1352]\n",
      "Weights: [-4.2359 -0.841  -0.2248 -0.012   0.124 ]\n",
      "MSE loss: 140.529\n",
      "Iteration: 89700\n",
      "Gradient: [ 81.0341 -38.4782  73.8255 -67.1845 530.3443]\n",
      "Weights: [-4.2363 -0.8403 -0.2247 -0.0123  0.1241]\n",
      "MSE loss: 140.4527\n",
      "Iteration: 89800\n",
      "Gradient: [ 25.3038 -57.2635  76.2045 -14.5791  66.5136]\n",
      "Weights: [-4.2366 -0.8397 -0.2244 -0.0129  0.1243]\n",
      "MSE loss: 140.3691\n",
      "Iteration: 89900\n",
      "Gradient: [  42.3219    7.6879  -10.5578  250.0396 1200.6625]\n",
      "Weights: [-4.237  -0.8391 -0.2243 -0.0133  0.1245]\n",
      "MSE loss: 140.3071\n",
      "Iteration: 90000\n",
      "Gradient: [  37.1253  -60.6165 -124.4218  163.0451  633.0451]\n",
      "Weights: [-4.2373 -0.8385 -0.224  -0.0133  0.1244]\n",
      "MSE loss: 140.2545\n",
      "Iteration: 90100\n",
      "Gradient: [  33.3429  -16.1347   63.2999  -82.1951 1147.4013]\n",
      "Weights: [-4.2377 -0.838  -0.2239 -0.0131  0.1244]\n",
      "MSE loss: 140.2497\n",
      "Iteration: 90200\n",
      "Gradient: [  57.3433  -59.1048   43.5063 -102.0609  433.2535]\n",
      "Weights: [-4.2381 -0.8374 -0.2238 -0.0139  0.1245]\n",
      "MSE loss: 140.124\n",
      "Iteration: 90300\n",
      "Gradient: [  57.0023 -104.8765  -81.7409   -1.684   717.0684]\n",
      "Weights: [-4.2384 -0.8368 -0.2237 -0.015   0.1248]\n",
      "MSE loss: 140.0341\n",
      "Iteration: 90400\n",
      "Gradient: [  -5.4865  -81.2517   88.6127  385.3165 -326.5933]\n",
      "Weights: [-4.2388 -0.8362 -0.2237 -0.0152  0.1249]\n",
      "MSE loss: 139.9696\n",
      "Iteration: 90500\n",
      "Gradient: [   7.8595  -94.7139  -95.3527 -162.9714  259.8039]\n",
      "Weights: [-4.2392 -0.8355 -0.2237 -0.0154  0.125 ]\n",
      "MSE loss: 139.9024\n",
      "Iteration: 90600\n",
      "Gradient: [  75.3263  -63.6359   -0.996  -199.5888   13.6787]\n",
      "Weights: [-4.2395 -0.8349 -0.2234 -0.0157  0.125 ]\n",
      "MSE loss: 139.834\n",
      "Iteration: 90700\n",
      "Gradient: [ -6.8135 -29.3115 169.754  254.5133 515.6557]\n",
      "Weights: [-4.2399 -0.8343 -0.2233 -0.016   0.1251]\n",
      "MSE loss: 139.7723\n",
      "Iteration: 90800\n",
      "Gradient: [ 75.536  -44.6694 -68.7704 143.3694 190.4838]\n",
      "Weights: [-4.2402 -0.8338 -0.2231 -0.0161  0.1251]\n",
      "MSE loss: 139.7197\n",
      "Iteration: 90900\n",
      "Gradient: [ 16.8865 -13.5443 -18.8731 328.8316  40.1445]\n",
      "Weights: [-4.2406 -0.8332 -0.2231 -0.0166  0.1253]\n",
      "MSE loss: 139.6713\n",
      "Iteration: 91000\n",
      "Gradient: [  12.9806    9.2709   97.3868 -185.9199   22.2224]\n",
      "Weights: [-4.2409 -0.8326 -0.2229 -0.0171  0.1253]\n",
      "MSE loss: 139.5882\n",
      "Iteration: 91100\n",
      "Gradient: [-3.471000e-01 -1.275791e+02  9.190000e-02  9.109010e+01 -5.397064e+02]\n",
      "Weights: [-4.2412 -0.832  -0.2229 -0.0175  0.1254]\n",
      "MSE loss: 139.5135\n",
      "Iteration: 91200\n",
      "Gradient: [  29.7417 -107.4753  -17.639   -45.4528 -113.792 ]\n",
      "Weights: [-4.2416 -0.8314 -0.2228 -0.0177  0.1254]\n",
      "MSE loss: 139.4583\n",
      "Iteration: 91300\n",
      "Gradient: [  46.2654  -42.1595  -71.2993   85.8307 -857.65  ]\n",
      "Weights: [-4.242  -0.8308 -0.2227 -0.0182  0.1256]\n",
      "MSE loss: 139.3768\n",
      "Iteration: 91400\n",
      "Gradient: [ 60.9945 -68.4675  66.5762 430.7437 168.906 ]\n",
      "Weights: [-4.2423 -0.8302 -0.2225 -0.0185  0.1258]\n",
      "MSE loss: 139.3263\n",
      "Iteration: 91500\n",
      "Gradient: [  24.6221  -85.3245 -121.5939 -148.0544   -8.8716]\n",
      "Weights: [-4.2427 -0.8295 -0.2223 -0.0185  0.1257]\n",
      "MSE loss: 139.2583\n",
      "Iteration: 91600\n",
      "Gradient: [  -22.9989   -88.3327   -53.0776   224.4038 -1349.3856]\n",
      "Weights: [-4.243  -0.8288 -0.2223 -0.019   0.1256]\n",
      "MSE loss: 139.2266\n",
      "Iteration: 91700\n",
      "Gradient: [ 87.4328 -53.4767  33.6807 163.1463 120.7945]\n",
      "Weights: [-4.2433 -0.8282 -0.2219 -0.0191  0.1258]\n",
      "MSE loss: 139.1304\n",
      "Iteration: 91800\n",
      "Gradient: [  11.9471   -9.8813   26.3362   66.2613 -359.9065]\n",
      "Weights: [-4.2437 -0.8277 -0.2217 -0.0198  0.1261]\n",
      "MSE loss: 139.0707\n",
      "Iteration: 91900\n",
      "Gradient: [  38.8696  -53.147    10.3653 -201.5056 -495.845 ]\n",
      "Weights: [-4.244  -0.8271 -0.2217 -0.0199  0.1259]\n",
      "MSE loss: 139.0025\n",
      "Iteration: 92000\n",
      "Gradient: [   7.5737  -84.8463   19.5414 -183.0757   70.2948]\n",
      "Weights: [-4.2444 -0.8265 -0.2213 -0.02    0.126 ]\n",
      "MSE loss: 138.9435\n",
      "Iteration: 92100\n",
      "Gradient: [  40.7002  -14.7408    6.7136 -202.9933  451.3009]\n",
      "Weights: [-4.2447 -0.8259 -0.2213 -0.0202  0.1259]\n",
      "MSE loss: 138.8968\n",
      "Iteration: 92200\n",
      "Gradient: [  58.2347   11.0926  -15.9637  404.8066 -129.7763]\n",
      "Weights: [-4.2452 -0.8254 -0.2212 -0.021   0.1264]\n",
      "MSE loss: 138.8338\n",
      "Iteration: 92300\n",
      "Gradient: [ 41.0474 -63.4183  59.2066 217.4094  17.4304]\n",
      "Weights: [-4.2455 -0.8248 -0.2212 -0.0216  0.1265]\n",
      "MSE loss: 138.7231\n",
      "Iteration: 92400\n",
      "Gradient: [ -1.8321 -57.0366 -57.3056 -42.6861 446.4114]\n",
      "Weights: [-4.2459 -0.8242 -0.2211 -0.022   0.1265]\n",
      "MSE loss: 138.6606\n",
      "Iteration: 92500\n",
      "Gradient: [-12.0791 -77.8646 104.7709 266.9508 -62.2298]\n",
      "Weights: [-4.2462 -0.8236 -0.2211 -0.0224  0.1267]\n",
      "MSE loss: 138.595\n",
      "Iteration: 92600\n",
      "Gradient: [   38.7333   -78.2858   -39.2339  -246.1553 -1135.9351]\n",
      "Weights: [-4.2465 -0.823  -0.221  -0.0231  0.1268]\n",
      "MSE loss: 138.5213\n",
      "Iteration: 92700\n",
      "Gradient: [  28.0884  -37.0827   33.3174  233.3656 -265.5905]\n",
      "Weights: [-4.2468 -0.8224 -0.2208 -0.0237  0.127 ]\n",
      "MSE loss: 138.4412\n",
      "Iteration: 92800\n",
      "Gradient: [  -4.0772  -37.5476  146.5986  413.4525 -631.5471]\n",
      "Weights: [-4.2472 -0.8218 -0.2207 -0.0237  0.1271]\n",
      "MSE loss: 138.3918\n",
      "Iteration: 92900\n",
      "Gradient: [  -3.4158    6.3096   53.3533  -11.3828 -377.7886]\n",
      "Weights: [-4.2475 -0.8212 -0.2206 -0.0242  0.1272]\n",
      "MSE loss: 138.322\n",
      "Iteration: 93000\n",
      "Gradient: [ 51.3883 -93.3833   4.739  339.6512 820.7112]\n",
      "Weights: [-4.2479 -0.8206 -0.2204 -0.0247  0.1273]\n",
      "MSE loss: 138.2497\n",
      "Iteration: 93100\n",
      "Gradient: [ 42.5499 -42.0751  96.3487 384.0246 566.5097]\n",
      "Weights: [-4.2482 -0.82   -0.2203 -0.0254  0.1277]\n",
      "MSE loss: 138.2206\n",
      "Iteration: 93200\n",
      "Gradient: [  24.007   -85.6065 -155.1297  266.6256  863.9982]\n",
      "Weights: [-4.2486 -0.8194 -0.2201 -0.0257  0.1277]\n",
      "MSE loss: 138.1038\n",
      "Iteration: 93300\n",
      "Gradient: [  31.7259  -99.241   -32.1012 -219.862  -345.8523]\n",
      "Weights: [-4.2489 -0.8187 -0.2199 -0.0261  0.1277]\n",
      "MSE loss: 138.0339\n",
      "Iteration: 93400\n",
      "Gradient: [  32.4274  -30.8751   49.2746  -85.5617 -130.1627]\n",
      "Weights: [-4.2493 -0.8181 -0.2198 -0.0265  0.1278]\n",
      "MSE loss: 137.9712\n",
      "Iteration: 93500\n",
      "Gradient: [ 46.073  -67.8085  12.2642  18.1153 463.0507]\n",
      "Weights: [-4.2496 -0.8175 -0.2196 -0.0268  0.1279]\n",
      "MSE loss: 137.8991\n",
      "Iteration: 93600\n",
      "Gradient: [  15.556   -75.9649 -142.5513 -261.1102 -282.5044]\n",
      "Weights: [-4.25   -0.8169 -0.2194 -0.0273  0.128 ]\n",
      "MSE loss: 137.8292\n",
      "Iteration: 93700\n",
      "Gradient: [ 33.6834 -65.198   72.1073 114.2997 698.761 ]\n",
      "Weights: [-4.2503 -0.8162 -0.219  -0.0272  0.128 ]\n",
      "MSE loss: 137.8045\n",
      "Iteration: 93800\n",
      "Gradient: [ 89.2028 -24.2168 127.8655 -28.0921 686.233 ]\n",
      "Weights: [-4.2506 -0.8155 -0.2187 -0.0279  0.1281]\n",
      "MSE loss: 137.6924\n",
      "Iteration: 93900\n",
      "Gradient: [  17.5268 -108.5271  -32.6466  162.314   366.4064]\n",
      "Weights: [-4.251  -0.815  -0.2185 -0.0283  0.1281]\n",
      "MSE loss: 137.6271\n",
      "Iteration: 94000\n",
      "Gradient: [ -10.0944  -55.6258 -165.0686   50.3052  333.5214]\n",
      "Weights: [-4.2513 -0.8143 -0.2183 -0.0288  0.1283]\n",
      "MSE loss: 137.553\n",
      "Iteration: 94100\n",
      "Gradient: [ 51.0632 -19.4801   1.8075 504.1815 891.0881]\n",
      "Weights: [-4.2516 -0.8138 -0.2182 -0.0293  0.1285]\n",
      "MSE loss: 137.4965\n",
      "Iteration: 94200\n",
      "Gradient: [   9.2116  -80.9063   39.5206  250.149  -891.912 ]\n",
      "Weights: [-4.252  -0.8131 -0.2179 -0.0295  0.1285]\n",
      "MSE loss: 137.4192\n",
      "Iteration: 94300\n",
      "Gradient: [  29.9232  -85.8626 -116.6278  -67.3108   55.5546]\n",
      "Weights: [-4.2523 -0.8125 -0.2176 -0.0296  0.1283]\n",
      "MSE loss: 137.3749\n",
      "Iteration: 94400\n",
      "Gradient: [  13.1724  -79.3485  -76.912  -109.4539  820.664 ]\n",
      "Weights: [-4.2527 -0.8118 -0.2173 -0.0299  0.1285]\n",
      "MSE loss: 137.299\n",
      "Iteration: 94500\n",
      "Gradient: [  43.2742  -22.302    62.6374 -271.3798 -668.6799]\n",
      "Weights: [-4.253  -0.8112 -0.2171 -0.0304  0.1286]\n",
      "MSE loss: 137.2293\n",
      "Iteration: 94600\n",
      "Gradient: [  69.4993  -82.5602 -191.0233  -20.1577  194.5362]\n",
      "Weights: [-4.2534 -0.8107 -0.2171 -0.0308  0.1287]\n",
      "MSE loss: 137.1696\n",
      "Iteration: 94700\n",
      "Gradient: [ 71.8516 -61.8524  78.4885 217.3085 328.3175]\n",
      "Weights: [-4.2537 -0.8101 -0.2169 -0.031   0.1289]\n",
      "MSE loss: 137.1438\n",
      "Iteration: 94800\n",
      "Gradient: [  33.5963  -28.7981 -116.6138 -321.9735  339.7318]\n",
      "Weights: [-4.2541 -0.8095 -0.2168 -0.0314  0.1289]\n",
      "MSE loss: 137.0419\n",
      "Iteration: 94900\n",
      "Gradient: [  81.9916  -59.6358 -105.2216  -76.927    12.0293]\n",
      "Weights: [-4.2544 -0.8089 -0.2166 -0.0316  0.1288]\n",
      "MSE loss: 136.9991\n",
      "Iteration: 95000\n",
      "Gradient: [  29.0923    4.6878   27.9542  188.547  -357.0581]\n",
      "Weights: [-4.2548 -0.8084 -0.2166 -0.0319  0.1289]\n",
      "MSE loss: 136.929\n",
      "Iteration: 95100\n",
      "Gradient: [  31.0909  -66.8847   68.0114  249.267  -872.378 ]\n",
      "Weights: [-4.2551 -0.8078 -0.2164 -0.0321  0.1289]\n",
      "MSE loss: 136.8899\n",
      "Iteration: 95200\n",
      "Gradient: [  55.5972  -73.1383  -37.1288  293.8899 -180.9722]\n",
      "Weights: [-4.2555 -0.8073 -0.2163 -0.0321  0.129 ]\n",
      "MSE loss: 136.832\n",
      "Iteration: 95300\n",
      "Gradient: [ 17.1379 -43.4297  13.5404 220.5309 456.1556]\n",
      "Weights: [-4.2558 -0.8068 -0.2161 -0.0324  0.129 ]\n",
      "MSE loss: 136.7738\n",
      "Iteration: 95400\n",
      "Gradient: [ 42.0564 -78.3133 -75.0262 -31.1316 747.9705]\n",
      "Weights: [-4.2561 -0.8063 -0.2161 -0.0331  0.1294]\n",
      "MSE loss: 136.7369\n",
      "Iteration: 95500\n",
      "Gradient: [ 32.3323 -75.9972 -62.0796  45.0202 539.0924]\n",
      "Weights: [-4.2565 -0.8056 -0.2158 -0.0333  0.1292]\n",
      "MSE loss: 136.6442\n",
      "Iteration: 95600\n",
      "Gradient: [  24.3465  -74.6215 -194.4378  268.7626 -578.367 ]\n",
      "Weights: [-4.2568 -0.805  -0.2155 -0.0333  0.1292]\n",
      "MSE loss: 136.5922\n",
      "Iteration: 95700\n",
      "Gradient: [  54.0032   13.1136  123.1839   73.097  -297.521 ]\n",
      "Weights: [-4.2572 -0.8045 -0.2154 -0.0337  0.1294]\n",
      "MSE loss: 136.5371\n",
      "Iteration: 95800\n",
      "Gradient: [  36.583  -110.4949  -28.1599  252.5657 -197.9009]\n",
      "Weights: [-4.2576 -0.804  -0.2154 -0.0346  0.1297]\n",
      "MSE loss: 136.4508\n",
      "Iteration: 95900\n",
      "Gradient: [  8.0595 -19.7018  62.1146 110.535   -1.806 ]\n",
      "Weights: [-4.2579 -0.8035 -0.2153 -0.035   0.1297]\n",
      "MSE loss: 136.3892\n",
      "Iteration: 96000\n",
      "Gradient: [  56.8958  -21.8666   94.8363  364.4504 -397.4423]\n",
      "Weights: [-4.2583 -0.803  -0.2152 -0.0354  0.1299]\n",
      "MSE loss: 136.3332\n",
      "Iteration: 96100\n",
      "Gradient: [   8.7705  -36.8219   88.0241 -194.9383  975.8145]\n",
      "Weights: [-4.2586 -0.8024 -0.2153 -0.036   0.1301]\n",
      "MSE loss: 136.2592\n",
      "Iteration: 96200\n",
      "Gradient: [  31.7583  -36.9754 -147.3095  370.8883   20.8737]\n",
      "Weights: [-4.2589 -0.8019 -0.2152 -0.0363  0.1302]\n",
      "MSE loss: 136.2077\n",
      "Iteration: 96300\n",
      "Gradient: [  23.2531  -47.7003 -101.7384   80.3749  -51.84  ]\n",
      "Weights: [-4.2593 -0.8013 -0.2152 -0.037   0.1303]\n",
      "MSE loss: 136.1329\n",
      "Iteration: 96400\n",
      "Gradient: [  25.3218    9.0377  -67.0258 -159.1532   52.7203]\n",
      "Weights: [-4.2596 -0.8007 -0.2149 -0.0375  0.1304]\n",
      "MSE loss: 136.0701\n",
      "Iteration: 96500\n",
      "Gradient: [  50.2733  -67.931   -58.1074  -27.9607 -151.0735]\n",
      "Weights: [-4.2598 -0.8001 -0.2146 -0.038   0.1306]\n",
      "MSE loss: 135.9984\n",
      "Iteration: 96600\n",
      "Gradient: [ 22.2628 -43.2666 -60.4167 253.0427 759.4006]\n",
      "Weights: [-4.2602 -0.7995 -0.2146 -0.0382  0.1307]\n",
      "MSE loss: 135.9482\n",
      "Iteration: 96700\n",
      "Gradient: [  24.5464  -83.6377  176.8161   28.4331 -712.7489]\n",
      "Weights: [-4.2605 -0.7989 -0.2145 -0.0385  0.1306]\n",
      "MSE loss: 135.903\n",
      "Iteration: 96800\n",
      "Gradient: [  45.9939 -111.5926 -197.7822 -218.5921   69.3256]\n",
      "Weights: [-4.2608 -0.7983 -0.2145 -0.0389  0.1307]\n",
      "MSE loss: 135.8378\n",
      "Iteration: 96900\n",
      "Gradient: [  67.278   -21.4371  140.08   -286.7329  -65.1873]\n",
      "Weights: [-4.2611 -0.7977 -0.2143 -0.039   0.1308]\n",
      "MSE loss: 135.7704\n",
      "Iteration: 97000\n",
      "Gradient: [  14.0413 -121.49    -75.3136  -50.8678 -107.2555]\n",
      "Weights: [-4.2614 -0.7972 -0.2143 -0.0396  0.131 ]\n",
      "MSE loss: 135.7102\n",
      "Iteration: 97100\n",
      "Gradient: [   28.166    -79.7044  -136.4275   117.9692 -1277.72  ]\n",
      "Weights: [-4.2617 -0.7965 -0.214  -0.0403  0.1312]\n",
      "MSE loss: 135.6323\n",
      "Iteration: 97200\n",
      "Gradient: [ 55.4707 -64.7641 -33.5555  92.6527 259.6336]\n",
      "Weights: [-4.262  -0.7959 -0.2136 -0.0411  0.1315]\n",
      "MSE loss: 135.5538\n",
      "Iteration: 97300\n",
      "Gradient: [ 32.9786 -42.6634 126.8802 517.4189 803.5225]\n",
      "Weights: [-4.2623 -0.7953 -0.2134 -0.0414  0.1316]\n",
      "MSE loss: 135.5058\n",
      "Iteration: 97400\n",
      "Gradient: [ -5.9679 -77.2993 -26.5609 480.7579 168.581 ]\n",
      "Weights: [-4.2626 -0.7947 -0.2134 -0.0423  0.1318]\n",
      "MSE loss: 135.4114\n",
      "Iteration: 97500\n",
      "Gradient: [  14.3298  -74.249  -120.6929 -490.4322  170.3794]\n",
      "Weights: [-4.2628 -0.794  -0.2131 -0.0422  0.1317]\n",
      "MSE loss: 135.3539\n",
      "Iteration: 97600\n",
      "Gradient: [ 61.1444 -44.5355  -2.0985 263.5761 230.7656]\n",
      "Weights: [-4.2632 -0.7934 -0.2127 -0.0426  0.1317]\n",
      "MSE loss: 135.2858\n",
      "Iteration: 97700\n",
      "Gradient: [   1.0921  -68.8428   91.5113 -147.3301  612.8874]\n",
      "Weights: [-4.2635 -0.7928 -0.2127 -0.0431  0.1319]\n",
      "MSE loss: 135.2231\n",
      "Iteration: 97800\n",
      "Gradient: [  76.263   -24.5753   61.9421 -252.7964 1447.2092]\n",
      "Weights: [-4.2638 -0.7922 -0.2124 -0.0434  0.1319]\n",
      "MSE loss: 135.1655\n",
      "Iteration: 97900\n",
      "Gradient: [  74.8067 -106.7604   15.7653  -48.0885  760.0168]\n",
      "Weights: [-4.2642 -0.7916 -0.2124 -0.0439  0.1321]\n",
      "MSE loss: 135.0887\n",
      "Iteration: 98000\n",
      "Gradient: [  16.5036  -45.0406 -147.1081 -196.2245 -359.5521]\n",
      "Weights: [-4.2646 -0.7909 -0.2121 -0.0442  0.1322]\n",
      "MSE loss: 135.0238\n",
      "Iteration: 98100\n",
      "Gradient: [ -13.6599  -99.1802  -18.3237 -217.2926  597.9089]\n",
      "Weights: [-4.2649 -0.7904 -0.2118 -0.0444  0.1321]\n",
      "MSE loss: 134.9885\n",
      "Iteration: 98200\n",
      "Gradient: [ 46.3844   7.2074  -6.6341 -77.9175 432.1173]\n",
      "Weights: [-4.2652 -0.7898 -0.2115 -0.0449  0.1324]\n",
      "MSE loss: 134.9148\n",
      "Iteration: 98300\n",
      "Gradient: [  22.7105  -56.9235   30.8887  -96.9946 -403.3816]\n",
      "Weights: [-4.2655 -0.7892 -0.2113 -0.0451  0.1324]\n",
      "MSE loss: 134.8523\n",
      "Iteration: 98400\n",
      "Gradient: [  13.7456  -40.2587 -134.1502 -237.5689 -547.5019]\n",
      "Weights: [-4.2658 -0.7887 -0.2112 -0.0455  0.1324]\n",
      "MSE loss: 134.788\n",
      "Iteration: 98500\n",
      "Gradient: [ 36.862  -76.3794  46.6085 305.2903 439.186 ]\n",
      "Weights: [-4.2662 -0.7881 -0.211  -0.0456  0.1326]\n",
      "MSE loss: 134.7735\n",
      "Iteration: 98600\n",
      "Gradient: [  52.1274  -63.3797    1.9421   73.9243 -433.0501]\n",
      "Weights: [-4.2665 -0.7875 -0.2109 -0.0466  0.1326]\n",
      "MSE loss: 134.6821\n",
      "Iteration: 98700\n",
      "Gradient: [ -40.4632  -41.1018 -166.9036 -193.4064  114.7256]\n",
      "Weights: [-4.2668 -0.7869 -0.2105 -0.0466  0.1326]\n",
      "MSE loss: 134.6001\n",
      "Iteration: 98800\n",
      "Gradient: [ 49.6014 -63.8371 -70.5502  65.7142 -28.576 ]\n",
      "Weights: [-4.2671 -0.7862 -0.2103 -0.0471  0.1327]\n",
      "MSE loss: 134.5361\n",
      "Iteration: 98900\n",
      "Gradient: [  32.0874  -61.0502   14.5712 -308.8533  162.5282]\n",
      "Weights: [-4.2675 -0.7857 -0.2102 -0.0474  0.1329]\n",
      "MSE loss: 134.4726\n",
      "Iteration: 99000\n",
      "Gradient: [  35.9537  -71.0222  137.764  -191.9627  290.1146]\n",
      "Weights: [-4.2678 -0.7851 -0.2099 -0.0474  0.1329]\n",
      "MSE loss: 134.439\n",
      "Iteration: 99100\n",
      "Gradient: [   -9.7895   -89.7784   -29.5437   297.0167 -1240.1396]\n",
      "Weights: [-4.2681 -0.7846 -0.21   -0.0476  0.1328]\n",
      "MSE loss: 134.3856\n",
      "Iteration: 99200\n",
      "Gradient: [  17.377   -58.0072 -104.9535   21.0833  -62.1319]\n",
      "Weights: [-4.2684 -0.784  -0.2098 -0.0477  0.1328]\n",
      "MSE loss: 134.3323\n",
      "Iteration: 99300\n",
      "Gradient: [ 20.206  -53.0884 -29.9481 127.788   -5.4127]\n",
      "Weights: [-4.2688 -0.7835 -0.2097 -0.0483  0.133 ]\n",
      "MSE loss: 134.2657\n",
      "Iteration: 99400\n",
      "Gradient: [  31.413   -57.459   -43.705  -146.8448   49.5048]\n",
      "Weights: [-4.2691 -0.783  -0.2095 -0.0485  0.133 ]\n",
      "MSE loss: 134.2141\n",
      "Iteration: 99500\n",
      "Gradient: [  16.018    -1.4618  171.783    -3.1452 -115.1596]\n",
      "Weights: [-4.2695 -0.7824 -0.2094 -0.0486  0.133 ]\n",
      "MSE loss: 134.1672\n",
      "Iteration: 99600\n",
      "Gradient: [ 71.187  -83.2614 -16.3174 -74.6373 176.5948]\n",
      "Weights: [-4.2698 -0.7819 -0.2091 -0.0488  0.1331]\n",
      "MSE loss: 134.1165\n",
      "Iteration: 99700\n",
      "Gradient: [  19.8489  -47.8415   49.2228   31.7326 -481.4455]\n",
      "Weights: [-4.2702 -0.7813 -0.209  -0.0491  0.1332]\n",
      "MSE loss: 134.0584\n",
      "Iteration: 99800\n",
      "Gradient: [ 36.4482 -51.9135 -24.6989 374.7633   8.3747]\n",
      "Weights: [-4.2705 -0.7808 -0.2088 -0.0498  0.1334]\n",
      "MSE loss: 133.9893\n",
      "Iteration: 99900\n",
      "Gradient: [  14.4775  -55.5928 -187.7558  -78.6321  109.2117]\n",
      "Weights: [-4.2709 -0.7802 -0.2087 -0.0504  0.1335]\n",
      "MSE loss: 133.9166\n",
      "Iteration: 100000\n",
      "Gradient: [ 91.4911  -2.2922 -42.619  236.8383 215.9336]\n",
      "Weights: [-4.2712 -0.7797 -0.2087 -0.0509  0.1337]\n",
      "MSE loss: 133.8586\n",
      "Iteration: 100100\n",
      "Gradient: [  19.0724  -63.274   -15.347  -313.7043 -460.2281]\n",
      "Weights: [-4.2716 -0.7791 -0.2087 -0.0516  0.1339]\n",
      "MSE loss: 133.7863\n",
      "Iteration: 100200\n",
      "Gradient: [   67.5124  -118.8275  -104.8601  -253.4727 -1261.8471]\n",
      "Weights: [-4.2719 -0.7786 -0.2085 -0.052   0.134 ]\n",
      "MSE loss: 133.7282\n",
      "Iteration: 100300\n",
      "Gradient: [  30.3128 -129.4228   11.0971 -629.1071  -63.1748]\n",
      "Weights: [-4.2723 -0.7779 -0.2082 -0.0521  0.1339]\n",
      "MSE loss: 133.7147\n",
      "Iteration: 100400\n",
      "Gradient: [  10.3559  -97.0401 -181.318   -17.3883 -565.3212]\n",
      "Weights: [-4.2726 -0.7773 -0.2079 -0.0522  0.1338]\n",
      "MSE loss: 133.6704\n",
      "Iteration: 100500\n",
      "Gradient: [  41.8439  -76.4016 -115.3267 -380.5211  414.1272]\n",
      "Weights: [-4.2729 -0.7767 -0.2075 -0.0521  0.1338]\n",
      "MSE loss: 133.5814\n",
      "Iteration: 100600\n",
      "Gradient: [ 29.7956 -49.4732  81.7962  15.2956   4.5652]\n",
      "Weights: [-4.2733 -0.7762 -0.2074 -0.0525  0.134 ]\n",
      "MSE loss: 133.5121\n",
      "Iteration: 100700\n",
      "Gradient: [  55.3751  -42.8371  -58.4184 -494.0035 -170.6796]\n",
      "Weights: [-4.2736 -0.7756 -0.2072 -0.0531  0.1343]\n",
      "MSE loss: 133.4627\n",
      "Iteration: 100800\n",
      "Gradient: [  78.2585  -85.5429  -73.2133   -2.7328 1228.8978]\n",
      "Weights: [-4.2739 -0.775  -0.207  -0.0532  0.1342]\n",
      "MSE loss: 133.3945\n",
      "Iteration: 100900\n",
      "Gradient: [  22.2155  -15.8688    1.3319 -270.3309 -234.232 ]\n",
      "Weights: [-4.2743 -0.7745 -0.207  -0.0538  0.1343]\n",
      "MSE loss: 133.3301\n",
      "Iteration: 101000\n",
      "Gradient: [  88.149   -60.4078   64.8276 -245.6025 -444.0894]\n",
      "Weights: [-4.2746 -0.774  -0.2069 -0.0541  0.1344]\n",
      "MSE loss: 133.2769\n",
      "Iteration: 101100\n",
      "Gradient: [  52.8583  -85.5331  -13.6404 -104.8084 1000.0144]\n",
      "Weights: [-4.275  -0.7735 -0.2068 -0.0544  0.1345]\n",
      "MSE loss: 133.223\n",
      "Iteration: 101200\n",
      "Gradient: [ 24.3432 -31.7932  12.4611 552.2699  40.4243]\n",
      "Weights: [-4.2753 -0.7729 -0.2067 -0.0549  0.1347]\n",
      "MSE loss: 133.1604\n",
      "Iteration: 101300\n",
      "Gradient: [  21.6553  -58.1185  -13.381   -22.6082 -436.0507]\n",
      "Weights: [-4.2757 -0.7724 -0.2066 -0.0549  0.1346]\n",
      "MSE loss: 133.1223\n",
      "Iteration: 101400\n",
      "Gradient: [  17.3393  -91.7501 -144.8987  143.8033 -986.1167]\n",
      "Weights: [-4.276  -0.7719 -0.2065 -0.0557  0.1347]\n",
      "MSE loss: 133.0729\n",
      "Iteration: 101500\n",
      "Gradient: [  9.1966 -56.9635 132.4339 214.935  985.4509]\n",
      "Weights: [-4.2763 -0.7713 -0.2064 -0.0559  0.1348]\n",
      "MSE loss: 132.999\n",
      "Iteration: 101600\n",
      "Gradient: [  15.216   -84.7573   48.5414   76.4374 -668.6529]\n",
      "Weights: [-4.2767 -0.7707 -0.2062 -0.056   0.1349]\n",
      "MSE loss: 132.9464\n",
      "Iteration: 101700\n",
      "Gradient: [ 72.7744 -37.1325 -18.3652 112.1741 462.5623]\n",
      "Weights: [-4.2769 -0.7701 -0.206  -0.0563  0.135 ]\n",
      "MSE loss: 132.888\n",
      "Iteration: 101800\n",
      "Gradient: [  27.8227   36.0157  -36.072  -313.9364 -656.1065]\n",
      "Weights: [-4.2773 -0.7697 -0.2058 -0.0567  0.1351]\n",
      "MSE loss: 132.8443\n",
      "Iteration: 101900\n",
      "Gradient: [   6.5645 -121.5874  -87.2552   65.4348 -311.986 ]\n",
      "Weights: [-4.2776 -0.7691 -0.2057 -0.0572  0.1351]\n",
      "MSE loss: 132.781\n",
      "Iteration: 102000\n",
      "Gradient: [  38.9016  -47.2313   41.9164  248.846  -308.549 ]\n",
      "Weights: [-4.278  -0.7685 -0.2055 -0.0576  0.1353]\n",
      "MSE loss: 132.7125\n",
      "Iteration: 102100\n",
      "Gradient: [  38.8001  -75.0547  -23.4645  316.5375 -444.0362]\n",
      "Weights: [-4.2783 -0.768  -0.2055 -0.0582  0.1356]\n",
      "MSE loss: 132.6685\n",
      "Iteration: 102200\n",
      "Gradient: [-18.498  -89.0757   7.2837 -23.7894 361.3916]\n",
      "Weights: [-4.2786 -0.7675 -0.2054 -0.0583  0.1355]\n",
      "MSE loss: 132.6052\n",
      "Iteration: 102300\n",
      "Gradient: [  53.8735  -92.6304 -334.6701 -147.8646 -185.5045]\n",
      "Weights: [-4.279  -0.7669 -0.2053 -0.0586  0.1356]\n",
      "MSE loss: 132.5506\n",
      "Iteration: 102400\n",
      "Gradient: [   1.4716  -43.3625   56.5669 -335.8385  619.5243]\n",
      "Weights: [-4.2793 -0.7665 -0.2053 -0.0592  0.1358]\n",
      "MSE loss: 132.4974\n",
      "Iteration: 102500\n",
      "Gradient: [  59.8121  -70.203  -117.9998  328.5462  272.4829]\n",
      "Weights: [-4.2796 -0.7658 -0.2051 -0.0595  0.1358]\n",
      "MSE loss: 132.4383\n",
      "Iteration: 102600\n",
      "Gradient: [  19.502  -136.0499  127.4061   68.128  -990.2448]\n",
      "Weights: [-4.2799 -0.7653 -0.2048 -0.0597  0.1357]\n",
      "MSE loss: 132.3964\n",
      "Iteration: 102700\n",
      "Gradient: [   43.1433    38.5334    83.3573   354.5201 -1158.0212]\n",
      "Weights: [-4.2802 -0.7647 -0.2048 -0.0601  0.136 ]\n",
      "MSE loss: 132.3256\n",
      "Iteration: 102800\n",
      "Gradient: [  28.9594 -171.9179 -117.5479  294.4106  176.6688]\n",
      "Weights: [-4.2805 -0.7642 -0.2047 -0.0603  0.1362]\n",
      "MSE loss: 132.3137\n",
      "Iteration: 102900\n",
      "Gradient: [  51.9189  -43.9353  -31.5287 -101.8666 -371.0877]\n",
      "Weights: [-4.2808 -0.7636 -0.2047 -0.0606  0.1362]\n",
      "MSE loss: 132.2316\n",
      "Iteration: 103000\n",
      "Gradient: [  50.792   -25.6192   84.2461 -256.3572  439.0965]\n",
      "Weights: [-4.2811 -0.7631 -0.2045 -0.0611  0.1363]\n",
      "MSE loss: 132.1719\n",
      "Iteration: 103100\n",
      "Gradient: [  20.9736  -58.6027   91.3663  173.5549 -455.5266]\n",
      "Weights: [-4.2814 -0.7625 -0.2044 -0.0612  0.1362]\n",
      "MSE loss: 132.13\n",
      "Iteration: 103200\n",
      "Gradient: [  13.8554  -57.6999   39.7364   87.7246 -729.2312]\n",
      "Weights: [-4.2817 -0.762  -0.2042 -0.0616  0.1363]\n",
      "MSE loss: 132.0733\n",
      "Iteration: 103300\n",
      "Gradient: [  -1.2334 -102.6374  -13.0247 -340.7885 -287.5294]\n",
      "Weights: [-4.282  -0.7615 -0.204  -0.0622  0.1363]\n",
      "MSE loss: 132.0564\n",
      "Iteration: 103400\n",
      "Gradient: [  44.3788   -3.2923  -83.2457  231.917  -436.7188]\n",
      "Weights: [-4.2823 -0.7609 -0.2038 -0.0622  0.1364]\n",
      "MSE loss: 131.9542\n",
      "Iteration: 103500\n",
      "Gradient: [   0.7943  -26.4504  101.3198   95.8413 -408.8072]\n",
      "Weights: [-4.2827 -0.7603 -0.2037 -0.0627  0.1366]\n",
      "MSE loss: 131.8963\n",
      "Iteration: 103600\n",
      "Gradient: [  32.634   -27.4382 -197.9251 -305.6468  407.7891]\n",
      "Weights: [-4.283  -0.7597 -0.2036 -0.0629  0.1365]\n",
      "MSE loss: 131.8573\n",
      "Iteration: 103700\n",
      "Gradient: [  35.1833  -35.3427 -149.0803  230.2055 -647.326 ]\n",
      "Weights: [-4.2833 -0.7592 -0.2035 -0.0631  0.1367]\n",
      "MSE loss: 131.7972\n",
      "Iteration: 103800\n",
      "Gradient: [-2.650000e-01 -6.198740e+01 -4.343810e+01  1.907167e+02  6.416144e+02]\n",
      "Weights: [-4.2836 -0.7586 -0.2034 -0.0635  0.1367]\n",
      "MSE loss: 131.7461\n",
      "Iteration: 103900\n",
      "Gradient: [  20.65      9.5616  -95.7717 -115.4145  524.6984]\n",
      "Weights: [-4.2839 -0.7581 -0.2032 -0.0641  0.1369]\n",
      "MSE loss: 131.6779\n",
      "Iteration: 104000\n",
      "Gradient: [ 29.0531 -41.7516  36.5001  -6.5583 325.2177]\n",
      "Weights: [-4.2842 -0.7575 -0.2029 -0.0641  0.1368]\n",
      "MSE loss: 131.634\n",
      "Iteration: 104100\n",
      "Gradient: [ 53.9058 -55.3887 -30.8965 134.5053 -72.5762]\n",
      "Weights: [-4.2845 -0.757  -0.2028 -0.0646  0.1371]\n",
      "MSE loss: 131.5751\n",
      "Iteration: 104200\n",
      "Gradient: [  20.0462   -0.7471   35.8505 -227.505   194.1728]\n",
      "Weights: [-4.2847 -0.7564 -0.2026 -0.0647  0.1371]\n",
      "MSE loss: 131.5299\n",
      "Iteration: 104300\n",
      "Gradient: [   39.9141   -83.1003  -131.402   -115.9835 -1045.4698]\n",
      "Weights: [-4.2851 -0.7559 -0.2023 -0.065   0.137 ]\n",
      "MSE loss: 131.4791\n",
      "Iteration: 104400\n",
      "Gradient: [  19.2712  -16.038   -55.0446 -119.9886 -468.9479]\n",
      "Weights: [-4.2854 -0.7553 -0.2021 -0.0653  0.1371]\n",
      "MSE loss: 131.4217\n",
      "Iteration: 104500\n",
      "Gradient: [  3.3334 -57.1714 127.9627 461.2283 792.027 ]\n",
      "Weights: [-4.2857 -0.7549 -0.202  -0.0658  0.1374]\n",
      "MSE loss: 131.3799\n",
      "Iteration: 104600\n",
      "Gradient: [   5.1383  -22.0425   36.0008  -98.6872 1276.5516]\n",
      "Weights: [-4.286  -0.7543 -0.2019 -0.0657  0.1372]\n",
      "MSE loss: 131.3302\n",
      "Iteration: 104700\n",
      "Gradient: [   30.802    -64.1328   -30.1052   -42.86   -1255.4098]\n",
      "Weights: [-4.2864 -0.7538 -0.2018 -0.0659  0.1372]\n",
      "MSE loss: 131.2882\n",
      "Iteration: 104800\n",
      "Gradient: [ 46.1338 -52.6418 137.1926 210.561  498.5704]\n",
      "Weights: [-4.2867 -0.7533 -0.2017 -0.0663  0.1373]\n",
      "MSE loss: 131.2376\n",
      "Iteration: 104900\n",
      "Gradient: [ 41.1978  -6.6379  63.4729 115.7338 251.2882]\n",
      "Weights: [-4.287  -0.7527 -0.2015 -0.0663  0.1373]\n",
      "MSE loss: 131.1873\n",
      "Iteration: 105000\n",
      "Gradient: [ 44.1299 -32.1214  86.5911 383.4207 198.844 ]\n",
      "Weights: [-4.2874 -0.7522 -0.2013 -0.0667  0.1375]\n",
      "MSE loss: 131.1367\n",
      "Iteration: 105100\n",
      "Gradient: [  10.4245  -79.058   -19.7227 -119.2485 -201.5946]\n",
      "Weights: [-4.2877 -0.7517 -0.2013 -0.0673  0.1376]\n",
      "MSE loss: 131.0755\n",
      "Iteration: 105200\n",
      "Gradient: [ 33.1909 -79.7075 -21.5647 315.4896 225.3491]\n",
      "Weights: [-4.288  -0.7511 -0.2012 -0.0676  0.1376]\n",
      "MSE loss: 131.0371\n",
      "Iteration: 105300\n",
      "Gradient: [  45.8793 -135.7024   22.8642  110.8969  830.1382]\n",
      "Weights: [-4.2883 -0.7506 -0.2012 -0.0681  0.1379]\n",
      "MSE loss: 130.9566\n",
      "Iteration: 105400\n",
      "Gradient: [  70.5543 -169.9652  -61.8866  -80.8279  506.6088]\n",
      "Weights: [-4.2886 -0.7502 -0.2012 -0.0688  0.138 ]\n",
      "MSE loss: 130.9189\n",
      "Iteration: 105500\n",
      "Gradient: [  47.2129 -148.2279 -276.397   -53.47    210.4555]\n",
      "Weights: [-4.2889 -0.7496 -0.2009 -0.0687  0.138 ]\n",
      "MSE loss: 130.8609\n",
      "Iteration: 105600\n",
      "Gradient: [ 49.5482 -84.6468  57.1575  73.6533 -98.2266]\n",
      "Weights: [-4.2893 -0.7491 -0.2008 -0.0688  0.138 ]\n",
      "MSE loss: 130.8164\n",
      "Iteration: 105700\n",
      "Gradient: [   14.5779   -50.6967   -49.6098   408.1476 -1124.163 ]\n",
      "Weights: [-4.2897 -0.7486 -0.2007 -0.0691  0.138 ]\n",
      "MSE loss: 130.7724\n",
      "Iteration: 105800\n",
      "Gradient: [  17.2636  -14.7724   13.4951  -26.6719 -567.9859]\n",
      "Weights: [-4.29   -0.7481 -0.2007 -0.0693  0.1382]\n",
      "MSE loss: 130.7241\n",
      "Iteration: 105900\n",
      "Gradient: [  53.6936  -26.3887   66.96     82.4306 -102.2463]\n",
      "Weights: [-4.2903 -0.7475 -0.2004 -0.0696  0.1381]\n",
      "MSE loss: 130.6713\n",
      "Iteration: 106000\n",
      "Gradient: [ 62.3331 -66.9532 125.2433  92.3091 573.2345]\n",
      "Weights: [-4.2906 -0.747  -0.2002 -0.07    0.1383]\n",
      "MSE loss: 130.6181\n",
      "Iteration: 106100\n",
      "Gradient: [   5.6874  -94.5016  -86.2959 -150.532   -13.105 ]\n",
      "Weights: [-4.2909 -0.7466 -0.2002 -0.0707  0.1386]\n",
      "MSE loss: 130.5537\n",
      "Iteration: 106200\n",
      "Gradient: [  16.4259  -63.2704   14.3194  164.7405 1066.0252]\n",
      "Weights: [-4.2912 -0.746  -0.2001 -0.0706  0.1385]\n",
      "MSE loss: 130.5193\n",
      "Iteration: 106300\n",
      "Gradient: [  32.5217  -28.9115  131.0081 -255.7865 -316.2968]\n",
      "Weights: [-4.2915 -0.7455 -0.2    -0.0712  0.1386]\n",
      "MSE loss: 130.466\n",
      "Iteration: 106400\n",
      "Gradient: [  25.0139  -36.4391  149.6801  -32.5094 -787.3541]\n",
      "Weights: [-4.2919 -0.745  -0.1998 -0.0712  0.1385]\n",
      "MSE loss: 130.4292\n",
      "Iteration: 106500\n",
      "Gradient: [  32.6121  -74.139    39.5285  308.4839 -118.0935]\n",
      "Weights: [-4.2921 -0.7446 -0.1998 -0.0718  0.1389]\n",
      "MSE loss: 130.3887\n",
      "Iteration: 106600\n",
      "Gradient: [  27.5163  -49.3253    2.0139  179.1303 -110.119 ]\n",
      "Weights: [-4.2924 -0.744  -0.1997 -0.0718  0.1387]\n",
      "MSE loss: 130.3321\n",
      "Iteration: 106700\n",
      "Gradient: [  75.8225 -137.2375   79.1611  221.534    60.5915]\n",
      "Weights: [-4.2927 -0.7435 -0.1996 -0.0719  0.1387]\n",
      "MSE loss: 130.2807\n",
      "Iteration: 106800\n",
      "Gradient: [ 15.1191 -43.8654  48.8439  78.3817  14.5964]\n",
      "Weights: [-4.293  -0.743  -0.1996 -0.0722  0.1389]\n",
      "MSE loss: 130.2364\n",
      "Iteration: 106900\n",
      "Gradient: [  15.1374 -100.3435  -50.774   114.0456  750.7856]\n",
      "Weights: [-4.2933 -0.7425 -0.1995 -0.0726  0.139 ]\n",
      "MSE loss: 130.1822\n",
      "Iteration: 107000\n",
      "Gradient: [ -4.4772 -79.359  -96.725  137.2795 348.2491]\n",
      "Weights: [-4.2936 -0.7419 -0.1994 -0.073   0.1391]\n",
      "MSE loss: 130.1267\n",
      "Iteration: 107100\n",
      "Gradient: [  32.5223  -76.2539  151.2384 -274.099    25.36  ]\n",
      "Weights: [-4.2939 -0.7414 -0.1991 -0.0733  0.1391]\n",
      "MSE loss: 130.0789\n",
      "Iteration: 107200\n",
      "Gradient: [  63.7705 -115.6117   43.3165  -28.258  -847.844 ]\n",
      "Weights: [-4.2943 -0.7409 -0.199  -0.0736  0.1391]\n",
      "MSE loss: 130.0485\n",
      "Iteration: 107300\n",
      "Gradient: [ 39.2911 -37.1744 -49.4568  93.0646 462.9872]\n",
      "Weights: [-4.2946 -0.7403 -0.199  -0.0741  0.1393]\n",
      "MSE loss: 129.9752\n",
      "Iteration: 107400\n",
      "Gradient: [  39.5242  -37.4541 -169.6222 -122.6954  446.5015]\n",
      "Weights: [-4.2949 -0.7397 -0.1988 -0.0741  0.1393]\n",
      "MSE loss: 129.922\n",
      "Iteration: 107500\n",
      "Gradient: [  60.0155   -1.7749    9.0308   18.7535 -655.5599]\n",
      "Weights: [-4.2953 -0.7392 -0.1989 -0.0745  0.1394]\n",
      "MSE loss: 129.8849\n",
      "Iteration: 107600\n",
      "Gradient: [  45.8287  -37.0629   79.1941   12.503  -622.2711]\n",
      "Weights: [-4.2956 -0.7387 -0.1986 -0.0746  0.1394]\n",
      "MSE loss: 129.8315\n",
      "Iteration: 107700\n",
      "Gradient: [  39.1072  -27.0276  -79.8185  -73.6166 -820.2304]\n",
      "Weights: [-4.2959 -0.7381 -0.1984 -0.0751  0.1396]\n",
      "MSE loss: 129.7699\n",
      "Iteration: 107800\n",
      "Gradient: [  43.0174  -24.4091  -88.8014   56.216  -169.6731]\n",
      "Weights: [-4.2962 -0.7376 -0.1984 -0.0757  0.1398]\n",
      "MSE loss: 129.7113\n",
      "Iteration: 107900\n",
      "Gradient: [  13.2056  -67.0303 -133.9332 -488.7841 -499.7851]\n",
      "Weights: [-4.2966 -0.737  -0.1985 -0.0756  0.1397]\n",
      "MSE loss: 129.6807\n",
      "Iteration: 108000\n",
      "Gradient: [  22.7337  -32.7912   40.8149  -42.0638 -209.0367]\n",
      "Weights: [-4.2969 -0.7365 -0.1983 -0.0759  0.1397]\n",
      "MSE loss: 129.6288\n",
      "Iteration: 108100\n",
      "Gradient: [ 21.4815 -25.8041 107.0032  78.2154 463.6383]\n",
      "Weights: [-4.2972 -0.736  -0.1981 -0.076   0.1397]\n",
      "MSE loss: 129.6014\n",
      "Iteration: 108200\n",
      "Gradient: [  25.3439  -63.1044   46.2309  181.688  -189.3993]\n",
      "Weights: [-4.2975 -0.7356 -0.1982 -0.0763  0.1398]\n",
      "MSE loss: 129.5485\n",
      "Iteration: 108300\n",
      "Gradient: [  15.4856  -41.1016    2.3141  176.0571 -104.2216]\n",
      "Weights: [-4.2978 -0.735  -0.198  -0.0766  0.1399]\n",
      "MSE loss: 129.496\n",
      "Iteration: 108400\n",
      "Gradient: [ 26.9247 -21.314   24.2257  26.7562 424.5943]\n",
      "Weights: [-4.2981 -0.7345 -0.1979 -0.0767  0.14  ]\n",
      "MSE loss: 129.4541\n",
      "Iteration: 108500\n",
      "Gradient: [  41.1507 -100.0413  -27.9052 -214.882   401.5753]\n",
      "Weights: [-4.2984 -0.734  -0.1977 -0.077   0.14  ]\n",
      "MSE loss: 129.4014\n",
      "Iteration: 108600\n",
      "Gradient: [-10.6219 -72.6482 135.2258 424.6731 284.5531]\n",
      "Weights: [-4.2987 -0.7335 -0.1977 -0.0774  0.1401]\n",
      "MSE loss: 129.3514\n",
      "Iteration: 108700\n",
      "Gradient: [ 55.4486 -64.4234 -55.0058 -90.8719 368.5174]\n",
      "Weights: [-4.2991 -0.733  -0.1979 -0.0778  0.1402]\n",
      "MSE loss: 129.3034\n",
      "Iteration: 108800\n",
      "Gradient: [  25.2061  -69.4288   80.8058 -183.1417   40.3399]\n",
      "Weights: [-4.2994 -0.7325 -0.1976 -0.078   0.1403]\n",
      "MSE loss: 129.2561\n",
      "Iteration: 108900\n",
      "Gradient: [   9.2226 -115.149   -42.478   -41.5742  479.3138]\n",
      "Weights: [-4.2997 -0.7321 -0.1976 -0.0783  0.1404]\n",
      "MSE loss: 129.2128\n",
      "Iteration: 109000\n",
      "Gradient: [  26.3463 -133.4682   88.9543  169.7257    1.1518]\n",
      "Weights: [-4.3    -0.7315 -0.1975 -0.0788  0.1405]\n",
      "MSE loss: 129.1569\n",
      "Iteration: 109100\n",
      "Gradient: [  -1.643  -104.5414   43.9992  172.1168  -39.0981]\n",
      "Weights: [-4.3003 -0.731  -0.1974 -0.0793  0.1407]\n",
      "MSE loss: 129.0985\n",
      "Iteration: 109200\n",
      "Gradient: [ 31.0205  16.3564 223.2346 141.3703 382.5536]\n",
      "Weights: [-4.3006 -0.7305 -0.1972 -0.0799  0.1408]\n",
      "MSE loss: 129.0415\n",
      "Iteration: 109300\n",
      "Gradient: [ 18.8414  11.1249 129.1187 113.7966 -38.0339]\n",
      "Weights: [-4.3009 -0.73   -0.1972 -0.0803  0.141 ]\n",
      "MSE loss: 128.9895\n",
      "Iteration: 109400\n",
      "Gradient: [ 46.4889   4.1746 -32.923   12.7011 122.6955]\n",
      "Weights: [-4.3012 -0.7294 -0.1971 -0.081   0.1412]\n",
      "MSE loss: 128.9277\n",
      "Iteration: 109500\n",
      "Gradient: [   2.4672  -51.2775 -138.0698 -183.0941   14.3501]\n",
      "Weights: [-4.3014 -0.7289 -0.1969 -0.0817  0.1413]\n",
      "MSE loss: 128.8761\n",
      "Iteration: 109600\n",
      "Gradient: [   8.4818  -48.1712 -211.8479 -107.238  -973.2938]\n",
      "Weights: [-4.3017 -0.7283 -0.1968 -0.0821  0.1414]\n",
      "MSE loss: 128.833\n",
      "Iteration: 109700\n",
      "Gradient: [  7.1143 -83.3756  83.9726 469.8304 301.5648]\n",
      "Weights: [-4.302  -0.7277 -0.1966 -0.0827  0.1417]\n",
      "MSE loss: 128.7548\n",
      "Iteration: 109800\n",
      "Gradient: [  50.9923  -67.241  -141.6414 -244.0838  150.5275]\n",
      "Weights: [-4.3022 -0.7272 -0.1963 -0.0831  0.1417]\n",
      "MSE loss: 128.7031\n",
      "Iteration: 109900\n",
      "Gradient: [  6.5656 -27.8916 -42.898  -62.9052  -7.5473]\n",
      "Weights: [-4.3025 -0.7266 -0.1962 -0.083   0.1416]\n",
      "MSE loss: 128.6741\n",
      "Iteration: 110000\n",
      "Gradient: [ 25.4487 -67.5064 134.2576 379.7817 245.811 ]\n",
      "Weights: [-4.3027 -0.726  -0.196  -0.0832  0.1418]\n",
      "MSE loss: 128.6294\n",
      "Iteration: 110100\n",
      "Gradient: [   5.7143 -118.2834 -104.5414 -272.3064 -501.1127]\n",
      "Weights: [-4.303  -0.7255 -0.196  -0.0836  0.1418]\n",
      "MSE loss: 128.5745\n",
      "Iteration: 110200\n",
      "Gradient: [   7.7961  -18.4474   60.5835 -118.322  -490.351 ]\n",
      "Weights: [-4.3032 -0.7249 -0.1959 -0.084   0.1419]\n",
      "MSE loss: 128.5125\n",
      "Iteration: 110300\n",
      "Gradient: [    2.887   -139.9     -158.3882  -386.3102 -1064.8242]\n",
      "Weights: [-4.3035 -0.7244 -0.1959 -0.0843  0.1419]\n",
      "MSE loss: 128.4922\n",
      "Iteration: 110400\n",
      "Gradient: [  29.5675  -80.9885   59.6718 -110.7278 1313.9368]\n",
      "Weights: [-4.3038 -0.7239 -0.1956 -0.0844  0.1421]\n",
      "MSE loss: 128.4362\n",
      "Iteration: 110500\n",
      "Gradient: [  43.6466  -60.6173  -51.8838   50.3238 -511.8372]\n",
      "Weights: [-4.304  -0.7234 -0.1956 -0.0851  0.1422]\n",
      "MSE loss: 128.3783\n",
      "Iteration: 110600\n",
      "Gradient: [   5.1819  -33.621    93.0507  153.8412 -169.9689]\n",
      "Weights: [-4.3043 -0.7229 -0.1954 -0.086   0.1426]\n",
      "MSE loss: 128.3084\n",
      "Iteration: 110700\n",
      "Gradient: [  22.1486 -111.3466  124.0519  409.7875  694.7936]\n",
      "Weights: [-4.3046 -0.7222 -0.1953 -0.0866  0.1428]\n",
      "MSE loss: 128.2452\n",
      "Iteration: 110800\n",
      "Gradient: [ 26.7802 -25.0641 -36.9719 -86.2011 567.0803]\n",
      "Weights: [-4.3049 -0.7216 -0.1951 -0.0865  0.1426]\n",
      "MSE loss: 128.1973\n",
      "Iteration: 110900\n",
      "Gradient: [  55.4719  -48.4481 -131.0254  -32.5375 -889.2313]\n",
      "Weights: [-4.3051 -0.721  -0.1948 -0.0871  0.1428]\n",
      "MSE loss: 128.1358\n",
      "Iteration: 111000\n",
      "Gradient: [  20.1673 -110.9607  -13.1666  307.2623  981.9078]\n",
      "Weights: [-4.3054 -0.7204 -0.1947 -0.0873  0.1429]\n",
      "MSE loss: 128.0979\n",
      "Iteration: 111100\n",
      "Gradient: [   6.5264  -95.2392  -57.3261 -474.5672  298.7341]\n",
      "Weights: [-4.3057 -0.7199 -0.1946 -0.088   0.143 ]\n",
      "MSE loss: 128.0343\n",
      "Iteration: 111200\n",
      "Gradient: [  40.4903  -21.0349  -75.6867 -506.3928  301.1848]\n",
      "Weights: [-4.306  -0.7193 -0.1945 -0.088   0.1429]\n",
      "MSE loss: 128.0136\n",
      "Iteration: 111300\n",
      "Gradient: [  61.1738  -82.8981  -82.839   -97.5216 -955.2458]\n",
      "Weights: [-4.3062 -0.7188 -0.1942 -0.0877  0.1427]\n",
      "MSE loss: 127.9823\n",
      "Iteration: 111400\n",
      "Gradient: [ 19.2535  50.9955  27.5817 106.8712 210.6003]\n",
      "Weights: [-4.3065 -0.7182 -0.1941 -0.0877  0.1428]\n",
      "MSE loss: 127.9207\n",
      "Iteration: 111500\n",
      "Gradient: [  17.4626  -55.9033 -123.131    57.5061  168.1217]\n",
      "Weights: [-4.3068 -0.7177 -0.194  -0.088   0.1429]\n",
      "MSE loss: 127.8768\n",
      "Iteration: 111600\n",
      "Gradient: [  36.7483  -48.0216  143.6063  211.4699 -528.2371]\n",
      "Weights: [-4.3072 -0.7173 -0.194  -0.0882  0.143 ]\n",
      "MSE loss: 127.8419\n",
      "Iteration: 111700\n",
      "Gradient: [ 45.321  -59.1357  14.8381 340.6725 515.4436]\n",
      "Weights: [-4.3075 -0.7168 -0.1938 -0.0885  0.143 ]\n",
      "MSE loss: 127.7905\n",
      "Iteration: 111800\n",
      "Gradient: [ 42.787  -64.5951  -2.3374 -27.9533 980.9581]\n",
      "Weights: [-4.3077 -0.7163 -0.1938 -0.0888  0.1431]\n",
      "MSE loss: 127.7437\n",
      "Iteration: 111900\n",
      "Gradient: [-16.5019 -41.1909  51.9896 124.4093 262.9289]\n",
      "Weights: [-4.308  -0.7158 -0.1937 -0.0888  0.143 ]\n",
      "MSE loss: 127.7123\n",
      "Iteration: 112000\n",
      "Gradient: [  55.972   -64.9947 -138.4271 -246.3059 -580.9205]\n",
      "Weights: [-4.3084 -0.7153 -0.1936 -0.0891  0.143 ]\n",
      "MSE loss: 127.686\n",
      "Iteration: 112100\n",
      "Gradient: [   47.3056   -73.0798   -25.0642  -389.2445 -1026.1208]\n",
      "Weights: [-4.3087 -0.7147 -0.1936 -0.0895  0.1432]\n",
      "MSE loss: 127.6206\n",
      "Iteration: 112200\n",
      "Gradient: [  -3.562   -95.9486 -186.7266   91.2381 -266.8029]\n",
      "Weights: [-4.3089 -0.7142 -0.1935 -0.0896  0.1432]\n",
      "MSE loss: 127.5848\n",
      "Iteration: 112300\n",
      "Gradient: [  16.0654  -45.0995   27.1447  314.2218 -572.8205]\n",
      "Weights: [-4.3092 -0.7136 -0.1933 -0.0897  0.1432]\n",
      "MSE loss: 127.5385\n",
      "Iteration: 112400\n",
      "Gradient: [ -12.0378  -92.5564  -12.4541  -19.818  -629.7965]\n",
      "Weights: [-4.3095 -0.7131 -0.1931 -0.0901  0.1433]\n",
      "MSE loss: 127.4856\n",
      "Iteration: 112500\n",
      "Gradient: [  20.7102  -41.2704  -67.6319  -27.8665 -153.6943]\n",
      "Weights: [-4.3098 -0.7126 -0.1929 -0.0906  0.1434]\n",
      "MSE loss: 127.44\n",
      "Iteration: 112600\n",
      "Gradient: [ 30.1631 -20.0456  54.0391 284.3364  44.3938]\n",
      "Weights: [-4.3101 -0.7121 -0.1928 -0.0907  0.1437]\n",
      "MSE loss: 127.4371\n",
      "Iteration: 112700\n",
      "Gradient: [  64.8768  -40.4761   61.0637  177.3491 1077.7132]\n",
      "Weights: [-4.3105 -0.7117 -0.1927 -0.0911  0.1437]\n",
      "MSE loss: 127.3686\n",
      "Iteration: 112800\n",
      "Gradient: [ 61.7388  29.9924  -0.8174 -11.5394  -3.9349]\n",
      "Weights: [-4.3108 -0.7112 -0.1925 -0.0916  0.1438]\n",
      "MSE loss: 127.2953\n",
      "Iteration: 112900\n",
      "Gradient: [  33.4777  -38.8566 -120.2944  -59.9853  561.7017]\n",
      "Weights: [-4.3111 -0.7107 -0.1924 -0.0921  0.1439]\n",
      "MSE loss: 127.2358\n",
      "Iteration: 113000\n",
      "Gradient: [  12.5262  -41.3948 -119.9187  135.3934   36.8445]\n",
      "Weights: [-4.3114 -0.7102 -0.1922 -0.0924  0.1441]\n",
      "MSE loss: 127.2086\n",
      "Iteration: 113100\n",
      "Gradient: [  37.4346  -55.3724 -126.2971  244.511  1259.4675]\n",
      "Weights: [-4.3117 -0.7098 -0.1922 -0.0927  0.1442]\n",
      "MSE loss: 127.1653\n",
      "Iteration: 113200\n",
      "Gradient: [  40.8156  -48.8577  135.7507   27.9675 -444.6177]\n",
      "Weights: [-4.312  -0.7093 -0.1919 -0.0929  0.144 ]\n",
      "MSE loss: 127.1058\n",
      "Iteration: 113300\n",
      "Gradient: [ 16.3446   2.0081  90.2714   3.6668 677.7326]\n",
      "Weights: [-4.3122 -0.7088 -0.1919 -0.0932  0.1442]\n",
      "MSE loss: 127.0682\n",
      "Iteration: 113400\n",
      "Gradient: [ 21.595   78.3823 -80.6883  36.5414 -78.1603]\n",
      "Weights: [-4.3126 -0.7083 -0.1917 -0.0937  0.1443]\n",
      "MSE loss: 127.0102\n",
      "Iteration: 113500\n",
      "Gradient: [  31.2016  -59.2649  140.5087 -135.8137  291.0798]\n",
      "Weights: [-4.3128 -0.7078 -0.1915 -0.094   0.1444]\n",
      "MSE loss: 126.9705\n",
      "Iteration: 113600\n",
      "Gradient: [  24.0585  -83.0071  -87.5789 -371.5929 -266.8026]\n",
      "Weights: [-4.3132 -0.7074 -0.1914 -0.0942  0.1444]\n",
      "MSE loss: 126.9249\n",
      "Iteration: 113700\n",
      "Gradient: [  25.0616  -51.1207  -42.3796   23.7405 -625.0509]\n",
      "Weights: [-4.3135 -0.7068 -0.1913 -0.0944  0.1444]\n",
      "MSE loss: 126.8849\n",
      "Iteration: 113800\n",
      "Gradient: [ 53.5235 -69.1052 -31.8504 316.2656 148.312 ]\n",
      "Weights: [-4.3138 -0.7063 -0.1912 -0.0948  0.1444]\n",
      "MSE loss: 126.8489\n",
      "Iteration: 113900\n",
      "Gradient: [ 20.9558 -57.9702 -61.9839 174.8896 203.2345]\n",
      "Weights: [-4.3141 -0.7058 -0.191  -0.0949  0.1446]\n",
      "MSE loss: 126.7978\n",
      "Iteration: 114000\n",
      "Gradient: [  38.6858  -57.092    -2.3876 -683.9187 -689.7962]\n",
      "Weights: [-4.3143 -0.7053 -0.1908 -0.0954  0.1446]\n",
      "MSE loss: 126.746\n",
      "Iteration: 114100\n",
      "Gradient: [ -23.314   -98.843   -48.9583 -153.0452 -600.5997]\n",
      "Weights: [-4.3146 -0.7048 -0.1906 -0.0958  0.1448]\n",
      "MSE loss: 126.6897\n",
      "Iteration: 114200\n",
      "Gradient: [   5.0221    4.7068  -24.2852 -314.0167 -540.2497]\n",
      "Weights: [-4.3149 -0.7044 -0.1903 -0.0959  0.1445]\n",
      "MSE loss: 126.7254\n",
      "Iteration: 114300\n",
      "Gradient: [  52.2086  -70.0042 -130.084  -128.8579 -363.5227]\n",
      "Weights: [-4.3152 -0.704  -0.1901 -0.0964  0.1449]\n",
      "MSE loss: 126.607\n",
      "Iteration: 114400\n",
      "Gradient: [  53.5369   -9.8364  -34.9442  -26.6993 -133.3652]\n",
      "Weights: [-4.3155 -0.7035 -0.1902 -0.0971  0.1451]\n",
      "MSE loss: 126.553\n",
      "Iteration: 114500\n",
      "Gradient: [ 35.3933 -72.5692 -68.5408 -30.2502 434.1834]\n",
      "Weights: [-4.3158 -0.703  -0.19   -0.0971  0.1451]\n",
      "MSE loss: 126.5208\n",
      "Iteration: 114600\n",
      "Gradient: [  15.227    -1.6153   17.3429 -320.8695   20.8887]\n",
      "Weights: [-4.316  -0.7024 -0.1899 -0.0972  0.1449]\n",
      "MSE loss: 126.5201\n",
      "Iteration: 114700\n",
      "Gradient: [  16.0967  -80.3684 -151.2011 -232.1704 -317.6192]\n",
      "Weights: [-4.3163 -0.7019 -0.1899 -0.0974  0.1451]\n",
      "MSE loss: 126.4557\n",
      "Iteration: 114800\n",
      "Gradient: [  32.9183  -71.3656   51.4928   20.2591 -278.886 ]\n",
      "Weights: [-4.3166 -0.7014 -0.1896 -0.0975  0.1451]\n",
      "MSE loss: 126.4015\n",
      "Iteration: 114900\n",
      "Gradient: [  30.2295  -54.1935   42.1923  502.8549 -437.4315]\n",
      "Weights: [-4.3169 -0.701  -0.1897 -0.0982  0.1455]\n",
      "MSE loss: 126.3612\n",
      "Iteration: 115000\n",
      "Gradient: [  36.7896  -66.3009 -103.2637  453.8023 -322.3726]\n",
      "Weights: [-4.3172 -0.7005 -0.1896 -0.0983  0.1453]\n",
      "MSE loss: 126.3166\n",
      "Iteration: 115100\n",
      "Gradient: [    4.0208  -112.5613  -205.9752  -320.2686 -1323.9029]\n",
      "Weights: [-4.3175 -0.7    -0.1897 -0.0987  0.1454]\n",
      "MSE loss: 126.3201\n",
      "Iteration: 115200\n",
      "Gradient: [  38.8708    0.7224 -115.5415  303.0315 -240.2025]\n",
      "Weights: [-4.3178 -0.6996 -0.1895 -0.0989  0.1456]\n",
      "MSE loss: 126.2321\n",
      "Iteration: 115300\n",
      "Gradient: [ 19.1968 -81.9439  -3.3131 114.1711 149.4097]\n",
      "Weights: [-4.318  -0.699  -0.1893 -0.0994  0.1457]\n",
      "MSE loss: 126.1739\n",
      "Iteration: 115400\n",
      "Gradient: [  7.3922 -36.717  -73.5254 163.2672 582.0521]\n",
      "Weights: [-4.3183 -0.6986 -0.1891 -0.0996  0.1457]\n",
      "MSE loss: 126.1377\n",
      "Iteration: 115500\n",
      "Gradient: [ 13.4974 -71.127    5.5774  73.8637 297.8973]\n",
      "Weights: [-4.3186 -0.6981 -0.1891 -0.0999  0.1459]\n",
      "MSE loss: 126.0958\n",
      "Iteration: 115600\n",
      "Gradient: [ 25.0979   6.3302 -23.0332 174.9418 373.9847]\n",
      "Weights: [-4.3189 -0.6976 -0.189  -0.0998  0.1457]\n",
      "MSE loss: 126.0673\n",
      "Iteration: 115700\n",
      "Gradient: [  45.7817   -6.0551  -27.2911   87.0293 -338.649 ]\n",
      "Weights: [-4.3192 -0.6971 -0.1889 -0.0998  0.1456]\n",
      "MSE loss: 126.0402\n",
      "Iteration: 115800\n",
      "Gradient: [ -1.288  -82.19   -16.1976  36.4975 167.9729]\n",
      "Weights: [-4.3195 -0.6966 -0.1887 -0.1003  0.1457]\n",
      "MSE loss: 125.9947\n",
      "Iteration: 115900\n",
      "Gradient: [  21.2288  -22.1828  -54.5606 -117.7061  -90.2446]\n",
      "Weights: [-4.3197 -0.6962 -0.1887 -0.1005  0.1458]\n",
      "MSE loss: 125.9491\n",
      "Iteration: 116000\n",
      "Gradient: [  32.7733 -129.3539  -37.0452  281.239   537.1613]\n",
      "Weights: [-4.32   -0.6957 -0.1887 -0.1013  0.1461]\n",
      "MSE loss: 125.8952\n",
      "Iteration: 116100\n",
      "Gradient: [ 20.3945 -54.8692  93.5248 110.458  994.3406]\n",
      "Weights: [-4.3202 -0.6952 -0.1887 -0.102   0.1464]\n",
      "MSE loss: 125.8316\n",
      "Iteration: 116200\n",
      "Gradient: [  19.4883  -66.9097 -129.3884 -413.9804 -594.3304]\n",
      "Weights: [-4.3205 -0.6947 -0.1886 -0.1023  0.1463]\n",
      "MSE loss: 125.8348\n",
      "Iteration: 116300\n",
      "Gradient: [ 24.1133 -26.0776  46.5651  63.5875 684.6604]\n",
      "Weights: [-4.3208 -0.6942 -0.1885 -0.1027  0.1467]\n",
      "MSE loss: 125.7544\n",
      "Iteration: 116400\n",
      "Gradient: [ 20.3629 -33.8796 138.9653 126.0673 345.9175]\n",
      "Weights: [-4.321  -0.6936 -0.1884 -0.1029  0.1467]\n",
      "MSE loss: 125.7\n",
      "Iteration: 116500\n",
      "Gradient: [  16.1752  -71.5291  -51.9602 -298.7655 -329.3868]\n",
      "Weights: [-4.3213 -0.6931 -0.1883 -0.1031  0.1466]\n",
      "MSE loss: 125.6708\n",
      "Iteration: 116600\n",
      "Gradient: [ 42.0965 -48.8332 -55.9436  11.1552 431.719 ]\n",
      "Weights: [-4.3216 -0.6926 -0.1882 -0.1029  0.1466]\n",
      "MSE loss: 125.6318\n",
      "Iteration: 116700\n",
      "Gradient: [ -28.3298  -34.5948 -139.2672  -45.1868  440.5801]\n",
      "Weights: [-4.3218 -0.6921 -0.188  -0.1031  0.1466]\n",
      "MSE loss: 125.5898\n",
      "Iteration: 116800\n",
      "Gradient: [  31.1903  -81.4301 -112.6258 -151.922  -170.1432]\n",
      "Weights: [-4.3221 -0.6917 -0.1879 -0.1029  0.1465]\n",
      "MSE loss: 125.5687\n",
      "Iteration: 116900\n",
      "Gradient: [  13.2256  -61.0664   68.9461 -399.7076  433.9552]\n",
      "Weights: [-4.3223 -0.6912 -0.1878 -0.1032  0.1465]\n",
      "MSE loss: 125.5284\n",
      "Iteration: 117000\n",
      "Gradient: [  -3.2499  -59.4196  -77.5736  203.2212 -229.2898]\n",
      "Weights: [-4.3226 -0.6907 -0.1878 -0.1036  0.1467]\n",
      "MSE loss: 125.4848\n",
      "Iteration: 117100\n",
      "Gradient: [  -2.5767  -49.6579  -21.7951   76.9183 -800.3437]\n",
      "Weights: [-4.3228 -0.6903 -0.1878 -0.1042  0.1468]\n",
      "MSE loss: 125.4487\n",
      "Iteration: 117200\n",
      "Gradient: [   5.3497  -21.9127 -100.0684 -223.2036  134.4176]\n",
      "Weights: [-4.3231 -0.6897 -0.1877 -0.1043  0.1468]\n",
      "MSE loss: 125.4028\n",
      "Iteration: 117300\n",
      "Gradient: [  11.8222  -76.7938  119.7348  196.4434 -806.1852]\n",
      "Weights: [-4.3234 -0.6892 -0.1874 -0.1046  0.1468]\n",
      "MSE loss: 125.3686\n",
      "Iteration: 117400\n",
      "Gradient: [  95.0965  -69.0136 -194.8762   87.2162 -311.1498]\n",
      "Weights: [-4.3236 -0.6886 -0.1872 -0.1047  0.1469]\n",
      "MSE loss: 125.3129\n",
      "Iteration: 117500\n",
      "Gradient: [   1.6215  -66.6258 -187.227  -136.7028 -179.9425]\n",
      "Weights: [-4.3239 -0.6882 -0.1871 -0.1047  0.1469]\n",
      "MSE loss: 125.28\n",
      "Iteration: 117600\n",
      "Gradient: [ -36.8924  -25.8792   70.0419  110.3789 -638.8254]\n",
      "Weights: [-4.3243 -0.6878 -0.187  -0.1048  0.1469]\n",
      "MSE loss: 125.2506\n",
      "Iteration: 117700\n",
      "Gradient: [  36.5896    2.4402  -47.3201  440.9558 -175.8063]\n",
      "Weights: [-4.3246 -0.6873 -0.187  -0.1049  0.1469]\n",
      "MSE loss: 125.2131\n",
      "Iteration: 117800\n",
      "Gradient: [  22.3278  -39.7034   11.245    83.1338 -207.7278]\n",
      "Weights: [-4.3249 -0.6868 -0.187  -0.1052  0.1471]\n",
      "MSE loss: 125.1744\n",
      "Iteration: 117900\n",
      "Gradient: [  36.0371  -37.9983  -76.6038 -241.1935  459.283 ]\n",
      "Weights: [-4.3252 -0.6864 -0.187  -0.1059  0.1474]\n",
      "MSE loss: 125.1366\n",
      "Iteration: 118000\n",
      "Gradient: [-22.0878 -24.2503  34.8646 331.9812 708.1589]\n",
      "Weights: [-4.3255 -0.6859 -0.187  -0.1065  0.1476]\n",
      "MSE loss: 125.094\n",
      "Iteration: 118100\n",
      "Gradient: [  11.2752 -115.3912  -25.3625 -309.9928  169.9819]\n",
      "Weights: [-4.3258 -0.6854 -0.1869 -0.1068  0.1475]\n",
      "MSE loss: 125.0306\n",
      "Iteration: 118200\n",
      "Gradient: [ 41.068  -79.4551  98.8056 -92.1201 142.8341]\n",
      "Weights: [-4.3261 -0.6849 -0.1866 -0.1068  0.1474]\n",
      "MSE loss: 124.9875\n",
      "Iteration: 118300\n",
      "Gradient: [  11.9722 -112.6042  -42.9715   86.622  -213.2408]\n",
      "Weights: [-4.3263 -0.6844 -0.1866 -0.1068  0.1475]\n",
      "MSE loss: 124.956\n",
      "Iteration: 118400\n",
      "Gradient: [  2.2798  34.0865 -65.3514 104.7216 234.068 ]\n",
      "Weights: [-4.3266 -0.6839 -0.1864 -0.1072  0.1477]\n",
      "MSE loss: 124.9314\n",
      "Iteration: 118500\n",
      "Gradient: [  11.1416  -49.3843  -59.2052   11.872  -517.6533]\n",
      "Weights: [-4.3269 -0.6834 -0.1864 -0.1073  0.1475]\n",
      "MSE loss: 124.8791\n",
      "Iteration: 118600\n",
      "Gradient: [  47.5331  -80.2046    9.3436  298.4038 1493.6528]\n",
      "Weights: [-4.3271 -0.6831 -0.1864 -0.1079  0.1478]\n",
      "MSE loss: 124.8346\n",
      "Iteration: 118700\n",
      "Gradient: [  40.5993 -107.5886   58.3849   95.6653 -523.1873]\n",
      "Weights: [-4.3274 -0.6826 -0.1864 -0.1081  0.1478]\n",
      "MSE loss: 124.8031\n",
      "Iteration: 118800\n",
      "Gradient: [  63.9409  -14.3857 -178.9668  -37.8558 -893.7706]\n",
      "Weights: [-4.3276 -0.6822 -0.1863 -0.1083  0.1479]\n",
      "MSE loss: 124.7595\n",
      "Iteration: 118900\n",
      "Gradient: [  29.6038  -43.9251   22.5561 -317.6445 -220.7595]\n",
      "Weights: [-4.3279 -0.6817 -0.1862 -0.1087  0.1479]\n",
      "MSE loss: 124.7215\n",
      "Iteration: 119000\n",
      "Gradient: [  26.9333  -18.5996 -162.2174  137.2601  266.9287]\n",
      "Weights: [-4.3282 -0.6813 -0.1861 -0.1091  0.1481]\n",
      "MSE loss: 124.6731\n",
      "Iteration: 119100\n",
      "Gradient: [ 38.3374   4.0456 -16.8422 344.6921 297.6858]\n",
      "Weights: [-4.3285 -0.6808 -0.186  -0.1094  0.1483]\n",
      "MSE loss: 124.6781\n",
      "Iteration: 119200\n",
      "Gradient: [  21.7609 -102.1509   14.1121  -27.3503  158.2929]\n",
      "Weights: [-4.3287 -0.6803 -0.1857 -0.1095  0.1482]\n",
      "MSE loss: 124.5941\n",
      "Iteration: 119300\n",
      "Gradient: [   7.1081  -45.9979    8.0999  522.7709 -181.7734]\n",
      "Weights: [-4.329  -0.6797 -0.1856 -0.1099  0.1484]\n",
      "MSE loss: 124.5669\n",
      "Iteration: 119400\n",
      "Gradient: [ 17.292   21.2245 313.1467 142.4522 939.5248]\n",
      "Weights: [-4.3293 -0.6793 -0.1857 -0.1104  0.1486]\n",
      "MSE loss: 124.5634\n",
      "Iteration: 119500\n",
      "Gradient: [  54.9015 -100.4179   48.003  -148.3127 -454.8211]\n",
      "Weights: [-4.3295 -0.6789 -0.1857 -0.1106  0.1484]\n",
      "MSE loss: 124.4895\n",
      "Iteration: 119600\n",
      "Gradient: [  -7.4679  -10.2119   29.7475  315.9152 -702.4842]\n",
      "Weights: [-4.3299 -0.6784 -0.1856 -0.1107  0.1486]\n",
      "MSE loss: 124.4388\n",
      "Iteration: 119700\n",
      "Gradient: [  1.6423 -33.1208  13.8447 -92.9723 526.2982]\n",
      "Weights: [-4.3301 -0.678  -0.1855 -0.1112  0.1486]\n",
      "MSE loss: 124.3926\n",
      "Iteration: 119800\n",
      "Gradient: [  22.4985   11.8661   22.9467  210.485  1325.0267]\n",
      "Weights: [-4.3304 -0.6775 -0.1853 -0.1111  0.1487]\n",
      "MSE loss: 124.3705\n",
      "Iteration: 119900\n",
      "Gradient: [ -3.4994 -24.1581 -26.2282 231.5639 959.1047]\n",
      "Weights: [-4.3307 -0.677  -0.1854 -0.1115  0.1488]\n",
      "MSE loss: 124.3268\n",
      "Iteration: 120000\n",
      "Gradient: [  39.2972   -9.423   110.4745 -279.0468  337.1278]\n",
      "Weights: [-4.331  -0.6765 -0.1851 -0.1119  0.1489]\n",
      "MSE loss: 124.2745\n",
      "Iteration: 120100\n",
      "Gradient: [  32.0427  -83.2991 -130.5101  200.9982 -141.504 ]\n",
      "Weights: [-4.3313 -0.676  -0.1851 -0.1123  0.149 ]\n",
      "MSE loss: 124.2287\n",
      "Iteration: 120200\n",
      "Gradient: [  1.9737 -71.971  -57.7582 -32.3655  -5.3345]\n",
      "Weights: [-4.3315 -0.6755 -0.185  -0.1123  0.149 ]\n",
      "MSE loss: 124.2064\n",
      "Iteration: 120300\n",
      "Gradient: [  34.722   -50.9895  -21.0447 -452.4778  118.9249]\n",
      "Weights: [-4.3318 -0.675  -0.1849 -0.1128  0.1489]\n",
      "MSE loss: 124.1672\n",
      "Iteration: 120400\n",
      "Gradient: [ 41.8302 -70.1916 -66.7989 227.7574 545.9094]\n",
      "Weights: [-4.332  -0.6745 -0.1847 -0.113   0.1491]\n",
      "MSE loss: 124.1136\n",
      "Iteration: 120500\n",
      "Gradient: [ 21.2156  46.0365  37.7245 269.1654 940.0599]\n",
      "Weights: [-4.3323 -0.6741 -0.1846 -0.1134  0.1494]\n",
      "MSE loss: 124.1359\n",
      "Iteration: 120600\n",
      "Gradient: [ 18.6653  -4.69    84.7122 320.4572  38.8029]\n",
      "Weights: [-4.3325 -0.6736 -0.1845 -0.1138  0.1493]\n",
      "MSE loss: 124.0265\n",
      "Iteration: 120700\n",
      "Gradient: [ 53.324  -29.0602 -39.4374  43.9306 833.0511]\n",
      "Weights: [-4.3328 -0.6731 -0.1844 -0.1139  0.1493]\n",
      "MSE loss: 123.9943\n",
      "Iteration: 120800\n",
      "Gradient: [ 41.5854 -11.3107 -29.273  -76.7913 402.7536]\n",
      "Weights: [-4.333  -0.6726 -0.1843 -0.1144  0.1494]\n",
      "MSE loss: 123.949\n",
      "Iteration: 120900\n",
      "Gradient: [ 39.6532 -69.0957  63.3246 186.0766 200.258 ]\n",
      "Weights: [-4.3332 -0.6721 -0.1842 -0.1149  0.1495]\n",
      "MSE loss: 123.9108\n",
      "Iteration: 121000\n",
      "Gradient: [-26.7034  11.2999 188.7962  55.3259 838.9365]\n",
      "Weights: [-4.3335 -0.6715 -0.1839 -0.1151  0.1497]\n",
      "MSE loss: 123.8697\n",
      "Iteration: 121100\n",
      "Gradient: [  14.3593  -46.7444   92.6225  101.644  -298.4861]\n",
      "Weights: [-4.3338 -0.6711 -0.184  -0.1153  0.1497]\n",
      "MSE loss: 123.8182\n",
      "Iteration: 121200\n",
      "Gradient: [  2.1072 -17.9227 -37.3619  31.4034 983.5033]\n",
      "Weights: [-4.3341 -0.6707 -0.184  -0.1155  0.1498]\n",
      "MSE loss: 123.7867\n",
      "Iteration: 121300\n",
      "Gradient: [  71.8553  -45.1657  -19.1015  -66.474  1130.7661]\n",
      "Weights: [-4.3344 -0.6702 -0.1839 -0.1155  0.1498]\n",
      "MSE loss: 123.7657\n",
      "Iteration: 121400\n",
      "Gradient: [  10.2235   17.7585   85.2008  -27.2016 -202.5526]\n",
      "Weights: [-4.3347 -0.6697 -0.1838 -0.1155  0.1497]\n",
      "MSE loss: 123.7236\n",
      "Iteration: 121500\n",
      "Gradient: [  37.0201  -95.7034  -52.9606 -182.773  -354.0723]\n",
      "Weights: [-4.3349 -0.6693 -0.1838 -0.1156  0.1496]\n",
      "MSE loss: 123.7095\n",
      "Iteration: 121600\n",
      "Gradient: [ 27.6316 -13.1104 -10.583  136.54    -0.4694]\n",
      "Weights: [-4.3353 -0.6688 -0.1836 -0.116   0.1498]\n",
      "MSE loss: 123.6497\n",
      "Iteration: 121700\n",
      "Gradient: [  22.8417   25.5389    6.3646    2.4638 1021.5569]\n",
      "Weights: [-4.3355 -0.6683 -0.1834 -0.1156  0.1498]\n",
      "MSE loss: 123.6709\n",
      "Iteration: 121800\n",
      "Gradient: [  56.8449  -25.6835 -110.9649 -281.7669  226.4209]\n",
      "Weights: [-4.3358 -0.6679 -0.1834 -0.1163  0.1499]\n",
      "MSE loss: 123.5813\n",
      "Iteration: 121900\n",
      "Gradient: [  30.7014    9.3737    3.9335 -133.1054  231.7344]\n",
      "Weights: [-4.3361 -0.6675 -0.1834 -0.1166  0.1499]\n",
      "MSE loss: 123.5544\n",
      "Iteration: 122000\n",
      "Gradient: [  24.9847  -39.332   104.8314 -154.934   900.3657]\n",
      "Weights: [-4.3363 -0.667  -0.1832 -0.1172  0.15  ]\n",
      "MSE loss: 123.507\n",
      "Iteration: 122100\n",
      "Gradient: [   7.3195  -51.6089 -146.1089  113.263    15.1758]\n",
      "Weights: [-4.3366 -0.6665 -0.1832 -0.1172  0.15  ]\n",
      "MSE loss: 123.471\n",
      "Iteration: 122200\n",
      "Gradient: [  79.0523  -53.1265   49.4453  172.1672 -381.0206]\n",
      "Weights: [-4.3369 -0.666  -0.1829 -0.1174  0.1502]\n",
      "MSE loss: 123.4298\n",
      "Iteration: 122300\n",
      "Gradient: [ 59.8876 -33.9524  52.3146 194.2618 282.8241]\n",
      "Weights: [-4.3372 -0.6656 -0.1829 -0.1174  0.1501]\n",
      "MSE loss: 123.3967\n",
      "Iteration: 122400\n",
      "Gradient: [  60.1926  -17.0554   73.0836 -189.1645 -323.5932]\n",
      "Weights: [-4.3375 -0.6652 -0.1828 -0.1177  0.1502]\n",
      "MSE loss: 123.3572\n",
      "Iteration: 122500\n",
      "Gradient: [ 29.6033 -45.8657 123.814  420.5672 303.5132]\n",
      "Weights: [-4.3378 -0.6647 -0.1828 -0.1174  0.1501]\n",
      "MSE loss: 123.3512\n",
      "Iteration: 122600\n",
      "Gradient: [  48.6546  -48.7886 -128.6761   23.7291  328.2088]\n",
      "Weights: [-4.3381 -0.6643 -0.1827 -0.1174  0.15  ]\n",
      "MSE loss: 123.3157\n",
      "Iteration: 122700\n",
      "Gradient: [  23.46    -61.7967   41.1188 -243.2122 -290.8891]\n",
      "Weights: [-4.3383 -0.6638 -0.1825 -0.1176  0.1501]\n",
      "MSE loss: 123.2817\n",
      "Iteration: 122800\n",
      "Gradient: [  70.298   -15.6256  130.2845  224.6191 1038.3826]\n",
      "Weights: [-4.3386 -0.6634 -0.1826 -0.118   0.1504]\n",
      "MSE loss: 123.2881\n",
      "Iteration: 122900\n",
      "Gradient: [  44.3616  -35.2694   76.4359 -236.489   677.6762]\n",
      "Weights: [-4.339  -0.6631 -0.1825 -0.118   0.1501]\n",
      "MSE loss: 123.2159\n",
      "Iteration: 123000\n",
      "Gradient: [ 30.634  -31.5699 -71.0186 173.6835 929.6787]\n",
      "Weights: [-4.3393 -0.6626 -0.1824 -0.1182  0.1502]\n",
      "MSE loss: 123.1828\n",
      "Iteration: 123100\n",
      "Gradient: [  43.1035   -8.8188   36.6163 -143.7014  571.0292]\n",
      "Weights: [-4.3396 -0.6621 -0.1822 -0.1186  0.1503]\n",
      "MSE loss: 123.1347\n",
      "Iteration: 123200\n",
      "Gradient: [ -13.3109  -31.6263  -21.3134 -326.0467 -952.8243]\n",
      "Weights: [-4.3399 -0.6617 -0.182  -0.1188  0.1502]\n",
      "MSE loss: 123.1198\n",
      "Iteration: 123300\n",
      "Gradient: [   7.21     -6.7956   53.9196  295.2472 -240.509 ]\n",
      "Weights: [-4.3402 -0.6612 -0.182  -0.1195  0.1506]\n",
      "MSE loss: 123.0459\n",
      "Iteration: 123400\n",
      "Gradient: [  52.0957 -137.7283 -135.356   299.8661 1330.7846]\n",
      "Weights: [-4.3405 -0.6608 -0.1819 -0.1198  0.1506]\n",
      "MSE loss: 123.0073\n",
      "Iteration: 123500\n",
      "Gradient: [   9.5861  -28.5056  -11.8312 -187.0359 -658.9232]\n",
      "Weights: [-4.3408 -0.6604 -0.1817 -0.12    0.1506]\n",
      "MSE loss: 122.9878\n",
      "Iteration: 123600\n",
      "Gradient: [  20.6375  -38.8619 -102.2886 -154.9532 -486.369 ]\n",
      "Weights: [-4.3411 -0.66   -0.1815 -0.1204  0.1506]\n",
      "MSE loss: 122.9676\n",
      "Iteration: 123700\n",
      "Gradient: [  -7.8093  -55.1603  -90.4918   80.6246 -669.4062]\n",
      "Weights: [-4.3413 -0.6596 -0.1814 -0.1202  0.1507]\n",
      "MSE loss: 122.9127\n",
      "Iteration: 123800\n",
      "Gradient: [  31.1925  -96.6194  -87.433   258.5755 -211.9541]\n",
      "Weights: [-4.3416 -0.6593 -0.1814 -0.1207  0.1508]\n",
      "MSE loss: 122.8822\n",
      "Iteration: 123900\n",
      "Gradient: [ -28.4364  -68.5457 -169.4012   99.9054 -677.4665]\n",
      "Weights: [-4.3419 -0.6588 -0.1812 -0.1212  0.151 ]\n",
      "MSE loss: 122.8304\n",
      "Iteration: 124000\n",
      "Gradient: [  17.5279   43.6355  193.8918  446.7472 1047.0693]\n",
      "Weights: [-4.3422 -0.6584 -0.1811 -0.1219  0.1513]\n",
      "MSE loss: 122.7944\n",
      "Iteration: 124100\n",
      "Gradient: [  12.9902 -127.6486  -56.6674  342.5765   37.0707]\n",
      "Weights: [-4.3424 -0.6579 -0.1811 -0.1219  0.1512]\n",
      "MSE loss: 122.7438\n",
      "Iteration: 124200\n",
      "Gradient: [ 19.4299 -73.297   50.1191 293.7958 759.1369]\n",
      "Weights: [-4.3427 -0.6574 -0.1811 -0.122   0.1512]\n",
      "MSE loss: 122.7175\n",
      "Iteration: 124300\n",
      "Gradient: [ -13.7683  -15.8585  -38.4264  174.2293 -460.9883]\n",
      "Weights: [-4.3429 -0.6569 -0.1808 -0.1223  0.1513]\n",
      "MSE loss: 122.6773\n",
      "Iteration: 124400\n",
      "Gradient: [  15.9353  -20.7028   28.3736 -108.1747 -284.1272]\n",
      "Weights: [-4.3431 -0.6565 -0.1807 -0.1225  0.1513]\n",
      "MSE loss: 122.6423\n",
      "Iteration: 124500\n",
      "Gradient: [ 41.3778   5.8763 -12.6594  85.0966 188.1201]\n",
      "Weights: [-4.3434 -0.6561 -0.1806 -0.123   0.1515]\n",
      "MSE loss: 122.6119\n",
      "Iteration: 124600\n",
      "Gradient: [  -8.97    -88.1076  146.3975  293.5768 -320.5019]\n",
      "Weights: [-4.3436 -0.6557 -0.1806 -0.1233  0.1515]\n",
      "MSE loss: 122.5675\n",
      "Iteration: 124700\n",
      "Gradient: [  41.0677  -43.4782  134.4943 -167.5846  166.165 ]\n",
      "Weights: [-4.3439 -0.6552 -0.1804 -0.1236  0.1517]\n",
      "MSE loss: 122.5411\n",
      "Iteration: 124800\n",
      "Gradient: [  20.9421  -38.477    15.6502  114.7264 -293.3462]\n",
      "Weights: [-4.3442 -0.6548 -0.1802 -0.1241  0.1518]\n",
      "MSE loss: 122.4879\n",
      "Iteration: 124900\n",
      "Gradient: [  60.3934  -65.2318  -59.5086  -69.0968 -804.1463]\n",
      "Weights: [-4.3444 -0.6543 -0.1802 -0.1243  0.1517]\n",
      "MSE loss: 122.4666\n",
      "Iteration: 125000\n",
      "Gradient: [  66.908   -35.4816   22.8122  124.6887 1266.37  ]\n",
      "Weights: [-4.3447 -0.6539 -0.1801 -0.1245  0.1519]\n",
      "MSE loss: 122.4231\n",
      "Iteration: 125100\n",
      "Gradient: [    3.1025   -61.7693   -63.8258    27.308  -2237.1739]\n",
      "Weights: [-4.3449 -0.6535 -0.1801 -0.1249  0.152 ]\n",
      "MSE loss: 122.3786\n",
      "Iteration: 125200\n",
      "Gradient: [  50.0688  -84.4186  -82.832  -161.1921  116.5992]\n",
      "Weights: [-4.3451 -0.6531 -0.1798 -0.1254  0.1521]\n",
      "MSE loss: 122.3471\n",
      "Iteration: 125300\n",
      "Gradient: [ -11.3847  -54.2442  -66.6493 -234.918   554.5045]\n",
      "Weights: [-4.3454 -0.6526 -0.1798 -0.1255  0.1521]\n",
      "MSE loss: 122.3083\n",
      "Iteration: 125400\n",
      "Gradient: [ 29.7733  -4.0649 176.3521 237.5048 597.7056]\n",
      "Weights: [-4.3457 -0.6521 -0.1797 -0.1257  0.1522]\n",
      "MSE loss: 122.2677\n",
      "Iteration: 125500\n",
      "Gradient: [  -8.1628  -37.5557  -72.4105  -14.5068 1094.1068]\n",
      "Weights: [-4.3459 -0.6516 -0.1797 -0.1261  0.1523]\n",
      "MSE loss: 122.2279\n",
      "Iteration: 125600\n",
      "Gradient: [ -19.2439  -19.8238    5.5494  191.7259 -545.0868]\n",
      "Weights: [-4.3461 -0.6511 -0.1795 -0.1263  0.1524]\n",
      "MSE loss: 122.2046\n",
      "Iteration: 125700\n",
      "Gradient: [ 42.9791 -50.4537 -91.574  151.0844  86.5674]\n",
      "Weights: [-4.3464 -0.6507 -0.1793 -0.1261  0.1521]\n",
      "MSE loss: 122.2029\n",
      "Iteration: 125800\n",
      "Gradient: [   44.8711   -34.7719  -112.3647   106.0491 -1157.3571]\n",
      "Weights: [-4.3466 -0.6502 -0.1791 -0.1262  0.1522]\n",
      "MSE loss: 122.1408\n",
      "Iteration: 125900\n",
      "Gradient: [ 41.2634  -7.1685 105.779  121.4024 -76.5656]\n",
      "Weights: [-4.3469 -0.6498 -0.179  -0.1267  0.1523]\n",
      "MSE loss: 122.1129\n",
      "Iteration: 126000\n",
      "Gradient: [  39.6896  -64.9327 -111.3396   50.2088  224.7091]\n",
      "Weights: [-4.3471 -0.6494 -0.1789 -0.1275  0.1526]\n",
      "MSE loss: 122.0476\n",
      "Iteration: 126100\n",
      "Gradient: [ -23.8365  -85.731   -34.6857 -243.3064 -140.4447]\n",
      "Weights: [-4.3474 -0.649  -0.1789 -0.1278  0.1527]\n",
      "MSE loss: 122.0101\n",
      "Iteration: 126200\n",
      "Gradient: [  32.8005  -89.5955  -94.1726 -665.1537  501.7469]\n",
      "Weights: [-4.3476 -0.6486 -0.1789 -0.1281  0.1528]\n",
      "MSE loss: 121.9832\n",
      "Iteration: 126300\n",
      "Gradient: [  11.0322  -68.5544   32.3281 -283.6446  633.8391]\n",
      "Weights: [-4.3479 -0.6481 -0.1791 -0.1285  0.153 ]\n",
      "MSE loss: 121.9427\n",
      "Iteration: 126400\n",
      "Gradient: [  12.3382 -109.9757   97.0519  231.0037 -602.6894]\n",
      "Weights: [-4.3481 -0.6476 -0.179  -0.1288  0.153 ]\n",
      "MSE loss: 121.9042\n",
      "Iteration: 126500\n",
      "Gradient: [  14.8509  -41.6497 -142.9603 -250.2111 -458.359 ]\n",
      "Weights: [-4.3483 -0.6472 -0.1787 -0.1293  0.1532]\n",
      "MSE loss: 121.8626\n",
      "Iteration: 126600\n",
      "Gradient: [ 67.3711  -7.2421  33.258   16.4051 148.9125]\n",
      "Weights: [-4.3486 -0.6466 -0.1786 -0.13    0.1534]\n",
      "MSE loss: 121.8085\n",
      "Iteration: 126700\n",
      "Gradient: [   2.9669    8.6749  -57.0446  279.7293 -158.2195]\n",
      "Weights: [-4.3488 -0.6461 -0.1785 -0.1302  0.1534]\n",
      "MSE loss: 121.7741\n",
      "Iteration: 126800\n",
      "Gradient: [ -25.3637  -52.0998   11.0785 -158.214  -495.3281]\n",
      "Weights: [-4.349  -0.6457 -0.1785 -0.1301  0.1533]\n",
      "MSE loss: 121.7582\n",
      "Iteration: 126900\n",
      "Gradient: [ 30.5152 -57.4357 -59.7705 204.7244 -28.0059]\n",
      "Weights: [-4.3492 -0.6452 -0.1784 -0.1302  0.1535]\n",
      "MSE loss: 121.7274\n",
      "Iteration: 127000\n",
      "Gradient: [ -17.056    -6.8164   35.5458  162.5536 -240.1191]\n",
      "Weights: [-4.3495 -0.6447 -0.1782 -0.1304  0.1534]\n",
      "MSE loss: 121.6811\n",
      "Iteration: 127100\n",
      "Gradient: [  31.3979  -63.9713  -17.2941  -16.645  -212.8732]\n",
      "Weights: [-4.3497 -0.6443 -0.1782 -0.1308  0.1536]\n",
      "MSE loss: 121.645\n",
      "Iteration: 127200\n",
      "Gradient: [  38.63     14.343    -4.1141   22.1942 1158.6898]\n",
      "Weights: [-4.3499 -0.6438 -0.1781 -0.1312  0.1537]\n",
      "MSE loss: 121.6144\n",
      "Iteration: 127300\n",
      "Gradient: [  -5.7672  -52.6344  -41.6649 -134.3046  288.4963]\n",
      "Weights: [-4.3501 -0.6433 -0.1779 -0.1314  0.1536]\n",
      "MSE loss: 121.5735\n",
      "Iteration: 127400\n",
      "Gradient: [  22.1711 -131.3267 -121.5662 -164.3374 -663.9496]\n",
      "Weights: [-4.3504 -0.6428 -0.1779 -0.1318  0.1537]\n",
      "MSE loss: 121.5447\n",
      "Iteration: 127500\n",
      "Gradient: [  14.2938  -51.8542  132.451  -295.423  1148.8031]\n",
      "Weights: [-4.3506 -0.6424 -0.1777 -0.1322  0.1539]\n",
      "MSE loss: 121.4899\n",
      "Iteration: 127600\n",
      "Gradient: [  57.4627  -79.0866   54.5076 -430.6323  610.7514]\n",
      "Weights: [-4.3509 -0.6419 -0.1775 -0.1324  0.1539]\n",
      "MSE loss: 121.4551\n",
      "Iteration: 127700\n",
      "Gradient: [  48.4086  -21.1382 -110.2892  -19.7322  721.1383]\n",
      "Weights: [-4.3511 -0.6414 -0.1775 -0.1324  0.154 ]\n",
      "MSE loss: 121.4418\n",
      "Iteration: 127800\n",
      "Gradient: [   14.8981   -27.5868   -80.866   -283.0746 -1084.9103]\n",
      "Weights: [-4.3513 -0.641  -0.1772 -0.1325  0.1538]\n",
      "MSE loss: 121.4046\n",
      "Iteration: 127900\n",
      "Gradient: [  14.4576  -71.6885   71.3331 -134.3219  240.284 ]\n",
      "Weights: [-4.3516 -0.6405 -0.1771 -0.1327  0.154 ]\n",
      "MSE loss: 121.3642\n",
      "Iteration: 128000\n",
      "Gradient: [  36.2271  -94.8972  -48.1518 -350.8488 -336.7423]\n",
      "Weights: [-4.3518 -0.64   -0.177  -0.133   0.154 ]\n",
      "MSE loss: 121.3271\n",
      "Iteration: 128100\n",
      "Gradient: [  39.4383  -13.0087   25.7523  128.4463 1420.6824]\n",
      "Weights: [-4.3521 -0.6397 -0.177  -0.1333  0.1541]\n",
      "MSE loss: 121.2967\n",
      "Iteration: 128200\n",
      "Gradient: [ 19.8071 -83.2784  77.2536 129.0295 358.0892]\n",
      "Weights: [-4.3524 -0.6392 -0.1769 -0.1338  0.1543]\n",
      "MSE loss: 121.2751\n",
      "Iteration: 128300\n",
      "Gradient: [  20.4352  -71.5385 -147.8741 -115.2354 -909.3921]\n",
      "Weights: [-4.3526 -0.6387 -0.1768 -0.1341  0.1542]\n",
      "MSE loss: 121.2243\n",
      "Iteration: 128400\n",
      "Gradient: [ 28.7629 -24.282  215.3144 215.4832 877.5103]\n",
      "Weights: [-4.3528 -0.6384 -0.1769 -0.1347  0.1545]\n",
      "MSE loss: 121.177\n",
      "Iteration: 128500\n",
      "Gradient: [   4.5959  -56.5286  115.7298 -108.3236 -222.026 ]\n",
      "Weights: [-4.353  -0.6379 -0.1767 -0.1348  0.1545]\n",
      "MSE loss: 121.1469\n",
      "Iteration: 128600\n",
      "Gradient: [ 94.4114   8.4659  12.6664 477.0264 458.339 ]\n",
      "Weights: [-4.3533 -0.6374 -0.1766 -0.135   0.1546]\n",
      "MSE loss: 121.116\n",
      "Iteration: 128700\n",
      "Gradient: [   9.1456  -73.5836   12.8408  369.774  -376.86  ]\n",
      "Weights: [-4.3535 -0.637  -0.1765 -0.1353  0.1547]\n",
      "MSE loss: 121.0774\n",
      "Iteration: 128800\n",
      "Gradient: [  45.2576   18.0322   11.0696  332.4552 -347.8747]\n",
      "Weights: [-4.3538 -0.6366 -0.1767 -0.1359  0.1549]\n",
      "MSE loss: 121.0381\n",
      "Iteration: 128900\n",
      "Gradient: [ -42.1453 -108.5803  -31.4581   58.7293 -263.3375]\n",
      "Weights: [-4.354  -0.6361 -0.1765 -0.1361  0.1549]\n",
      "MSE loss: 121.0022\n",
      "Iteration: 129000\n",
      "Gradient: [  25.2586  -57.8294   16.4083   99.1803 1733.7092]\n",
      "Weights: [-4.3542 -0.6356 -0.1763 -0.1361  0.1549]\n",
      "MSE loss: 120.9766\n",
      "Iteration: 129100\n",
      "Gradient: [  12.7523 -134.8469   35.3752 -149.2697  457.1336]\n",
      "Weights: [-4.3545 -0.6352 -0.1764 -0.1365  0.155 ]\n",
      "MSE loss: 120.9353\n",
      "Iteration: 129200\n",
      "Gradient: [   5.0837  -45.2762  -52.6691  212.0479 1027.069 ]\n",
      "Weights: [-4.3547 -0.6346 -0.1763 -0.1367  0.1551]\n",
      "MSE loss: 120.903\n",
      "Iteration: 129300\n",
      "Gradient: [ 49.5251 -52.1495 -96.6553 300.6405 382.8842]\n",
      "Weights: [-4.355  -0.6342 -0.1762 -0.1371  0.1552]\n",
      "MSE loss: 120.8704\n",
      "Iteration: 129400\n",
      "Gradient: [  15.1275  -88.9513 -126.6088  189.2181 -214.1342]\n",
      "Weights: [-4.3552 -0.6337 -0.1759 -0.1375  0.1551]\n",
      "MSE loss: 120.8436\n",
      "Iteration: 129500\n",
      "Gradient: [  59.6182  -34.1291  -82.0524 -264.4233 -392.2852]\n",
      "Weights: [-4.3554 -0.6332 -0.1759 -0.1379  0.1554]\n",
      "MSE loss: 120.7866\n",
      "Iteration: 129600\n",
      "Gradient: [  -8.3646  -53.988   -65.6557 -123.6798 1419.3681]\n",
      "Weights: [-4.3557 -0.6328 -0.1759 -0.1379  0.1552]\n",
      "MSE loss: 120.7769\n",
      "Iteration: 129700\n",
      "Gradient: [  43.274   -68.6084 -169.3659   -7.8449  209.7698]\n",
      "Weights: [-4.3559 -0.6323 -0.1758 -0.1381  0.1554]\n",
      "MSE loss: 120.7221\n",
      "Iteration: 129800\n",
      "Gradient: [ 48.8612 -29.9827  83.7375 193.876  184.6148]\n",
      "Weights: [-4.3561 -0.6319 -0.1758 -0.1385  0.1556]\n",
      "MSE loss: 120.6955\n",
      "Iteration: 129900\n",
      "Gradient: [    9.8238   -86.9393    14.9259    95.4834 -1436.7452]\n",
      "Weights: [-4.3563 -0.6314 -0.1755 -0.1386  0.1555]\n",
      "MSE loss: 120.6615\n",
      "Iteration: 130000\n",
      "Gradient: [ 42.8187 -12.1798  -3.2434 252.0637  14.577 ]\n",
      "Weights: [-4.3565 -0.631  -0.1754 -0.1394  0.1558]\n",
      "MSE loss: 120.6109\n",
      "Iteration: 130100\n",
      "Gradient: [  10.4211  -89.4476 -176.9016 -382.336   151.2089]\n",
      "Weights: [-4.3568 -0.6305 -0.1754 -0.1398  0.1558]\n",
      "MSE loss: 120.5802\n",
      "Iteration: 130200\n",
      "Gradient: [ -41.6066   -9.1844   -0.696   -15.3468 -330.1075]\n",
      "Weights: [-4.357  -0.63   -0.1753 -0.1403  0.156 ]\n",
      "MSE loss: 120.5328\n",
      "Iteration: 130300\n",
      "Gradient: [  36.792   -38.7698  -19.02   -226.7865 -295.9828]\n",
      "Weights: [-4.3571 -0.6295 -0.1752 -0.1405  0.156 ]\n",
      "MSE loss: 120.5211\n",
      "Iteration: 130400\n",
      "Gradient: [ 19.1128 -59.6283 -49.3251  47.5661 -17.0653]\n",
      "Weights: [-4.3574 -0.629  -0.1751 -0.1402  0.1559]\n",
      "MSE loss: 120.4829\n",
      "Iteration: 130500\n",
      "Gradient: [ 17.4096   9.2349  96.0359 532.7603 360.5556]\n",
      "Weights: [-4.3576 -0.6285 -0.175  -0.1405  0.156 ]\n",
      "MSE loss: 120.4422\n",
      "Iteration: 130600\n",
      "Gradient: [ 35.7919 -17.4937 -90.9542 265.4669 272.495 ]\n",
      "Weights: [-4.3578 -0.6281 -0.1749 -0.1405  0.156 ]\n",
      "MSE loss: 120.416\n",
      "Iteration: 130700\n",
      "Gradient: [  41.7185  -19.3703   25.8614   18.9287 -414.6242]\n",
      "Weights: [-4.3581 -0.6277 -0.1746 -0.1405  0.156 ]\n",
      "MSE loss: 120.3904\n",
      "Iteration: 130800\n",
      "Gradient: [ 57.0212 -96.4639 -61.664  -98.1746 -67.0742]\n",
      "Weights: [-4.3583 -0.6273 -0.1744 -0.1404  0.1559]\n",
      "MSE loss: 120.368\n",
      "Iteration: 130900\n",
      "Gradient: [  67.156    11.7373  -50.9574   68.7952 -623.1951]\n",
      "Weights: [-4.3585 -0.6268 -0.1743 -0.1407  0.1559]\n",
      "MSE loss: 120.335\n",
      "Iteration: 131000\n",
      "Gradient: [   91.6264   -20.7216    15.3288   213.1234 -1153.9707]\n",
      "Weights: [-4.3588 -0.6264 -0.1742 -0.1406  0.1558]\n",
      "MSE loss: 120.3268\n",
      "Iteration: 131100\n",
      "Gradient: [ 37.1396  13.6769  11.5809 174.4867 302.9591]\n",
      "Weights: [-4.3591 -0.626  -0.1741 -0.1411  0.156 ]\n",
      "MSE loss: 120.2734\n",
      "Iteration: 131200\n",
      "Gradient: [  55.6421  -71.3945 -294.6539   29.1527  498.3592]\n",
      "Weights: [-4.3593 -0.6256 -0.174  -0.1413  0.156 ]\n",
      "MSE loss: 120.2471\n",
      "Iteration: 131300\n",
      "Gradient: [ 29.9246  25.5159 -13.7626 -45.3594 832.0619]\n",
      "Weights: [-4.3596 -0.6252 -0.174  -0.1414  0.1562]\n",
      "MSE loss: 120.2466\n",
      "Iteration: 131400\n",
      "Gradient: [   3.6761  -24.8182 -172.7213  218.9021  611.5726]\n",
      "Weights: [-4.3599 -0.6248 -0.1739 -0.1418  0.1561]\n",
      "MSE loss: 120.184\n",
      "Iteration: 131500\n",
      "Gradient: [ 21.5387 -39.1022  73.5012  96.1977 549.5911]\n",
      "Weights: [-4.3602 -0.6245 -0.1738 -0.142   0.1564]\n",
      "MSE loss: 120.2127\n",
      "Iteration: 131600\n",
      "Gradient: [  33.896   -45.1549 -107.6335   40.365    30.8371]\n",
      "Weights: [-4.3604 -0.6241 -0.1738 -0.1422  0.1563]\n",
      "MSE loss: 120.1218\n",
      "Iteration: 131700\n",
      "Gradient: [  4.9445  66.4728  65.4638 351.913  157.9049]\n",
      "Weights: [-4.3607 -0.6237 -0.1738 -0.1426  0.1565]\n",
      "MSE loss: 120.0951\n",
      "Iteration: 131800\n",
      "Gradient: [  26.9079  -82.6678  -76.2182  126.4203 -488.1407]\n",
      "Weights: [-4.3609 -0.6232 -0.1738 -0.1427  0.1565]\n",
      "MSE loss: 120.0603\n",
      "Iteration: 131900\n",
      "Gradient: [  7.6103 -64.7735 -12.8286 258.4649 260.4807]\n",
      "Weights: [-4.3611 -0.6228 -0.1736 -0.1429  0.1566]\n",
      "MSE loss: 120.0542\n",
      "Iteration: 132000\n",
      "Gradient: [  48.4583  -59.3457  105.2342 -147.5031  498.6774]\n",
      "Weights: [-4.3614 -0.6224 -0.1737 -0.1434  0.1566]\n",
      "MSE loss: 119.9904\n",
      "Iteration: 132100\n",
      "Gradient: [ -13.2044  -54.7337   52.2528 -327.3501 1283.7901]\n",
      "Weights: [-4.3616 -0.622  -0.1736 -0.1438  0.1569]\n",
      "MSE loss: 119.9614\n",
      "Iteration: 132200\n",
      "Gradient: [ -22.3328  -18.7006 -166.2982  120.1738 -965.2296]\n",
      "Weights: [-4.3619 -0.6215 -0.1736 -0.1441  0.1568]\n",
      "MSE loss: 119.9235\n",
      "Iteration: 132300\n",
      "Gradient: [  20.2204  -45.4055   56.5531  -47.9207 -201.1763]\n",
      "Weights: [-4.3621 -0.6211 -0.1737 -0.1447  0.1571]\n",
      "MSE loss: 119.882\n",
      "Iteration: 132400\n",
      "Gradient: [   1.6622  -85.2878  -69.3852   32.9    -270.8506]\n",
      "Weights: [-4.3623 -0.6206 -0.1736 -0.145   0.1571]\n",
      "MSE loss: 119.8478\n",
      "Iteration: 132500\n",
      "Gradient: [ 51.2049  -2.5989  -7.9366 211.281  738.2444]\n",
      "Weights: [-4.3626 -0.6202 -0.1735 -0.1452  0.1572]\n",
      "MSE loss: 119.8182\n",
      "Iteration: 132600\n",
      "Gradient: [  24.7505 -103.7113   40.7618 -240.5324 -709.6575]\n",
      "Weights: [-4.3628 -0.6198 -0.1735 -0.1453  0.1572]\n",
      "MSE loss: 119.7899\n",
      "Iteration: 132700\n",
      "Gradient: [   6.2835  -43.7169  -10.4538 -121.3697  725.2049]\n",
      "Weights: [-4.363  -0.6193 -0.1735 -0.1456  0.1573]\n",
      "MSE loss: 119.7563\n",
      "Iteration: 132800\n",
      "Gradient: [  31.2057  -91.5787  -44.6775 -111.5097 -301.7909]\n",
      "Weights: [-4.3632 -0.6188 -0.1732 -0.1454  0.157 ]\n",
      "MSE loss: 119.7758\n",
      "Iteration: 132900\n",
      "Gradient: [  51.5033  -59.032   -77.5488 -139.7053 -189.3208]\n",
      "Weights: [-4.3634 -0.6184 -0.1732 -0.1459  0.1574]\n",
      "MSE loss: 119.6971\n",
      "Iteration: 133000\n",
      "Gradient: [ 72.0811 -50.2257  -5.1393 466.5726 668.5793]\n",
      "Weights: [-4.3637 -0.6179 -0.1732 -0.146   0.1574]\n",
      "MSE loss: 119.6661\n",
      "Iteration: 133100\n",
      "Gradient: [   7.6382  -62.4857  -91.1391  154.9492 -527.7979]\n",
      "Weights: [-4.3639 -0.6175 -0.1733 -0.1463  0.1575]\n",
      "MSE loss: 119.6353\n",
      "Iteration: 133200\n",
      "Gradient: [  -4.7066 -123.6897  -13.4216 -429.4412   88.9035]\n",
      "Weights: [-4.3641 -0.617  -0.1732 -0.1466  0.1576]\n",
      "MSE loss: 119.601\n",
      "Iteration: 133300\n",
      "Gradient: [  9.4741 -91.8194 199.892  127.3997  49.5832]\n",
      "Weights: [-4.3644 -0.6166 -0.1734 -0.1468  0.1576]\n",
      "MSE loss: 119.5742\n",
      "Iteration: 133400\n",
      "Gradient: [  -7.7925   10.7942  -67.3836 -123.366   144.9853]\n",
      "Weights: [-4.3646 -0.6161 -0.1733 -0.1471  0.1578]\n",
      "MSE loss: 119.5401\n",
      "Iteration: 133500\n",
      "Gradient: [ -10.877  -102.5747   13.7642 -127.2253 -149.0849]\n",
      "Weights: [-4.3648 -0.6156 -0.1732 -0.1475  0.1578]\n",
      "MSE loss: 119.5027\n",
      "Iteration: 133600\n",
      "Gradient: [   14.2313   -79.1209    17.4919   125.2651 -1038.4895]\n",
      "Weights: [-4.3651 -0.6151 -0.173  -0.1475  0.1578]\n",
      "MSE loss: 119.4725\n",
      "Iteration: 133700\n",
      "Gradient: [ 25.2288 -16.8238  73.0123 434.1447 243.436 ]\n",
      "Weights: [-4.3653 -0.6147 -0.1729 -0.1479  0.1579]\n",
      "MSE loss: 119.4353\n",
      "Iteration: 133800\n",
      "Gradient: [  16.9949  -32.046   -25.9672 -127.6794  246.1531]\n",
      "Weights: [-4.3655 -0.6142 -0.1728 -0.148   0.1578]\n",
      "MSE loss: 119.413\n",
      "Iteration: 133900\n",
      "Gradient: [  24.8461  -78.7755  -85.2329 -204.7573 -390.9103]\n",
      "Weights: [-4.3658 -0.6138 -0.1727 -0.1481  0.1579]\n",
      "MSE loss: 119.3822\n",
      "Iteration: 134000\n",
      "Gradient: [ 33.7664 -34.3395  98.9387 152.5561 435.6839]\n",
      "Weights: [-4.366  -0.6135 -0.1727 -0.148   0.1578]\n",
      "MSE loss: 119.362\n",
      "Iteration: 134100\n",
      "Gradient: [  1.9136 -67.2929 -21.9898  26.6913 622.2691]\n",
      "Weights: [-4.3663 -0.613  -0.1727 -0.1477  0.1577]\n",
      "MSE loss: 119.3494\n",
      "Iteration: 134200\n",
      "Gradient: [ 66.4215  30.1898 -60.1267  31.4022 216.5097]\n",
      "Weights: [-4.3665 -0.6126 -0.1726 -0.1477  0.1576]\n",
      "MSE loss: 119.3317\n",
      "Iteration: 134300\n",
      "Gradient: [ 66.6131  46.6782  54.9554 -47.338  852.6318]\n",
      "Weights: [-4.3668 -0.6122 -0.1726 -0.1476  0.1577]\n",
      "MSE loss: 119.3042\n",
      "Iteration: 134400\n",
      "Gradient: [ 35.2923 -64.1709 -48.6041 136.8116 183.3015]\n",
      "Weights: [-4.3671 -0.6119 -0.1726 -0.148   0.1577]\n",
      "MSE loss: 119.2798\n",
      "Iteration: 134500\n",
      "Gradient: [  45.1437  -15.3049   51.274    99.1884 -202.7483]\n",
      "Weights: [-4.3674 -0.6115 -0.1725 -0.1482  0.1579]\n",
      "MSE loss: 119.2516\n",
      "Iteration: 134600\n",
      "Gradient: [  19.7    -111.6552  -55.8644  -30.4184  233.0596]\n",
      "Weights: [-4.3676 -0.611  -0.1725 -0.1483  0.1578]\n",
      "MSE loss: 119.2191\n",
      "Iteration: 134700\n",
      "Gradient: [   6.3257   18.4041  -37.8688   99.3224 -122.2   ]\n",
      "Weights: [-4.3678 -0.6106 -0.1724 -0.1486  0.158 ]\n",
      "MSE loss: 119.1918\n",
      "Iteration: 134800\n",
      "Gradient: [ 27.3558 -51.8743  -5.4543 -22.9771 159.26  ]\n",
      "Weights: [-4.3681 -0.6102 -0.1724 -0.1488  0.158 ]\n",
      "MSE loss: 119.1529\n",
      "Iteration: 134900\n",
      "Gradient: [ 50.7963 -58.9007 101.2761 306.8184 409.1   ]\n",
      "Weights: [-4.3684 -0.6097 -0.1723 -0.149   0.1581]\n",
      "MSE loss: 119.1564\n",
      "Iteration: 135000\n",
      "Gradient: [  31.575   -86.5951 -126.6167  172.244   808.8488]\n",
      "Weights: [-4.3686 -0.6093 -0.1724 -0.1495  0.1583]\n",
      "MSE loss: 119.0874\n",
      "Iteration: 135100\n",
      "Gradient: [  38.3134  -62.6323  -22.4686 -209.3499 -721.9427]\n",
      "Weights: [-4.3689 -0.6089 -0.1723 -0.1496  0.1582]\n",
      "MSE loss: 119.0573\n",
      "Iteration: 135200\n",
      "Gradient: [ 2.233000e-01  1.207670e+01  6.898380e+01 -2.133760e+01  6.405922e+02]\n",
      "Weights: [-4.3691 -0.6085 -0.1723 -0.1502  0.1585]\n",
      "MSE loss: 119.021\n",
      "Iteration: 135300\n",
      "Gradient: [  63.8867  -12.1279   66.6261 -178.1222  313.8545]\n",
      "Weights: [-4.3694 -0.6081 -0.1723 -0.1506  0.1586]\n",
      "MSE loss: 118.9875\n",
      "Iteration: 135400\n",
      "Gradient: [  76.7324  -47.1368  -16.5884 -346.2081  413.5504]\n",
      "Weights: [-4.3696 -0.6077 -0.1721 -0.1506  0.1586]\n",
      "MSE loss: 118.99\n",
      "Iteration: 135500\n",
      "Gradient: [  22.0029  -68.0618  -62.3014 -287.5636  -48.3445]\n",
      "Weights: [-4.3699 -0.6073 -0.172  -0.1508  0.1584]\n",
      "MSE loss: 118.9534\n",
      "Iteration: 135600\n",
      "Gradient: [ 30.3292 -36.0483 -28.0611 235.3874 401.7901]\n",
      "Weights: [-4.3701 -0.6068 -0.1719 -0.1509  0.1587]\n",
      "MSE loss: 118.9217\n",
      "Iteration: 135700\n",
      "Gradient: [  14.0177  -69.4807   82.7074   91.5809 -238.1486]\n",
      "Weights: [-4.3704 -0.6064 -0.1718 -0.151   0.1586]\n",
      "MSE loss: 118.8791\n",
      "Iteration: 135800\n",
      "Gradient: [ 20.8472 -58.3084 -18.1568 223.9546 138.6932]\n",
      "Weights: [-4.3706 -0.606  -0.1718 -0.1516  0.1589]\n",
      "MSE loss: 118.8605\n",
      "Iteration: 135900\n",
      "Gradient: [  0.905  -33.924  -33.7015 124.4168 242.2502]\n",
      "Weights: [-4.3709 -0.6056 -0.1716 -0.1521  0.1589]\n",
      "MSE loss: 118.7957\n",
      "Iteration: 136000\n",
      "Gradient: [  70.2783  -44.0682  -75.6488 -144.5007 -649.0965]\n",
      "Weights: [-4.3711 -0.6051 -0.1715 -0.1523  0.1589]\n",
      "MSE loss: 118.7686\n",
      "Iteration: 136100\n",
      "Gradient: [  24.294   -20.364     3.9628 -194.556  -936.3429]\n",
      "Weights: [-4.3713 -0.6046 -0.1715 -0.1527  0.159 ]\n",
      "MSE loss: 118.7446\n",
      "Iteration: 136200\n",
      "Gradient: [  26.8724  -71.5912  -54.4109 -232.7963 -299.905 ]\n",
      "Weights: [-4.3716 -0.6042 -0.1714 -0.1526  0.159 ]\n",
      "MSE loss: 118.7081\n",
      "Iteration: 136300\n",
      "Gradient: [ 42.9126 -12.3711  57.0432  65.048  776.7813]\n",
      "Weights: [-4.3718 -0.6037 -0.1714 -0.1524  0.1589]\n",
      "MSE loss: 118.693\n",
      "Iteration: 136400\n",
      "Gradient: [  37.7721  -20.5948    3.7515  -29.3168 -417.0613]\n",
      "Weights: [-4.372  -0.6034 -0.1714 -0.1531  0.159 ]\n",
      "MSE loss: 118.6673\n",
      "Iteration: 136500\n",
      "Gradient: [  19.2854  -58.2454  -10.3092 -715.8071  -93.7297]\n",
      "Weights: [-4.3722 -0.6029 -0.171  -0.1529  0.1589]\n",
      "MSE loss: 118.6413\n",
      "Iteration: 136600\n",
      "Gradient: [  55.781   -50.5655  -26.9938  -69.6103 -490.1974]\n",
      "Weights: [-4.3724 -0.6025 -0.1709 -0.153   0.159 ]\n",
      "MSE loss: 118.6077\n",
      "Iteration: 136700\n",
      "Gradient: [  18.5679  -55.6707   15.8992 -219.6646  -73.5384]\n",
      "Weights: [-4.3727 -0.6021 -0.1708 -0.153   0.159 ]\n",
      "MSE loss: 118.5817\n",
      "Iteration: 136800\n",
      "Gradient: [  22.8705  -72.5337   56.818  -499.0351  -13.3062]\n",
      "Weights: [-4.3729 -0.6017 -0.1708 -0.1532  0.159 ]\n",
      "MSE loss: 118.5545\n",
      "Iteration: 136900\n",
      "Gradient: [  38.1828  -47.7252   65.0619  -36.1638 -158.3387]\n",
      "Weights: [-4.3732 -0.6014 -0.1708 -0.1534  0.159 ]\n",
      "MSE loss: 118.5317\n",
      "Iteration: 137000\n",
      "Gradient: [ 26.9078 -55.2338  28.8698 114.9189 430.1376]\n",
      "Weights: [-4.3735 -0.601  -0.1707 -0.1538  0.1592]\n",
      "MSE loss: 118.4946\n",
      "Iteration: 137100\n",
      "Gradient: [ 19.7598  13.5656  35.2108 352.0089 355.4782]\n",
      "Weights: [-4.3738 -0.6006 -0.1706 -0.1539  0.1593]\n",
      "MSE loss: 118.4761\n",
      "Iteration: 137200\n",
      "Gradient: [ 30.9746 -48.9886  32.017  504.3639 324.8356]\n",
      "Weights: [-4.374  -0.6002 -0.1706 -0.1542  0.1594]\n",
      "MSE loss: 118.4586\n",
      "Iteration: 137300\n",
      "Gradient: [ -1.1285  -3.0786  43.0913 180.7219 607.8249]\n",
      "Weights: [-4.3742 -0.5998 -0.1706 -0.1548  0.1596]\n",
      "MSE loss: 118.4065\n",
      "Iteration: 137400\n",
      "Gradient: [ 28.8947  18.1168   6.2508 -12.4766 343.8016]\n",
      "Weights: [-4.3744 -0.5994 -0.1704 -0.1553  0.1598]\n",
      "MSE loss: 118.3787\n",
      "Iteration: 137500\n",
      "Gradient: [-1.061200e+00 -9.850000e-02 -1.726920e+01  1.528092e+02  2.950325e+02]\n",
      "Weights: [-4.3746 -0.5989 -0.17   -0.1555  0.1597]\n",
      "MSE loss: 118.338\n",
      "Iteration: 137600\n",
      "Gradient: [   3.4542 -127.9728 -100.8636 -116.3186  143.7868]\n",
      "Weights: [-4.3749 -0.5986 -0.1698 -0.1559  0.1597]\n",
      "MSE loss: 118.3117\n",
      "Iteration: 137700\n",
      "Gradient: [  70.6258 -110.6036   93.0644  -20.4842 -555.8183]\n",
      "Weights: [-4.3751 -0.5982 -0.1698 -0.1564  0.1599]\n",
      "MSE loss: 118.266\n",
      "Iteration: 137800\n",
      "Gradient: [  35.8152   19.0361  -43.3628  -60.6298 -522.5432]\n",
      "Weights: [-4.3753 -0.5978 -0.1697 -0.1567  0.16  ]\n",
      "MSE loss: 118.2341\n",
      "Iteration: 137900\n",
      "Gradient: [ -16.6585  -11.7392 -237.5188 -443.8564  241.9652]\n",
      "Weights: [-4.3755 -0.5974 -0.1696 -0.157   0.16  ]\n",
      "MSE loss: 118.2134\n",
      "Iteration: 138000\n",
      "Gradient: [  8.3499 -49.3417 -14.7454 100.5924 553.5449]\n",
      "Weights: [-4.3758 -0.597  -0.1697 -0.1574  0.1602]\n",
      "MSE loss: 118.1711\n",
      "Iteration: 138100\n",
      "Gradient: [  -6.5131  -90.5039   17.933  -472.8609 -669.9259]\n",
      "Weights: [-4.376  -0.5965 -0.1696 -0.1577  0.1603]\n",
      "MSE loss: 118.1401\n",
      "Iteration: 138200\n",
      "Gradient: [ 12.2003 -50.3527  20.7866 292.0875 168.2137]\n",
      "Weights: [-4.3762 -0.5961 -0.1695 -0.1579  0.1604]\n",
      "MSE loss: 118.1116\n",
      "Iteration: 138300\n",
      "Gradient: [  24.5479  -34.5998 -188.4195 -212.6841  673.2737]\n",
      "Weights: [-4.3765 -0.5957 -0.1694 -0.1583  0.1604]\n",
      "MSE loss: 118.0822\n",
      "Iteration: 138400\n",
      "Gradient: [-6.100000e-03 -6.023810e+01  7.304520e+01 -1.842508e+02 -5.304698e+02]\n",
      "Weights: [-4.3767 -0.5954 -0.1695 -0.1586  0.1606]\n",
      "MSE loss: 118.0532\n",
      "Iteration: 138500\n",
      "Gradient: [ 34.7715 -46.9408  62.9368 170.9145 860.4846]\n",
      "Weights: [-4.3769 -0.595  -0.1694 -0.1588  0.1608]\n",
      "MSE loss: 118.0608\n",
      "Iteration: 138600\n",
      "Gradient: [  22.0124 -100.1404  -62.569   109.4363 -741.2687]\n",
      "Weights: [-4.3771 -0.5946 -0.1695 -0.1592  0.1606]\n",
      "MSE loss: 118.027\n",
      "Iteration: 138700\n",
      "Gradient: [  41.0756  -45.8047   80.8658 -108.3931  454.9488]\n",
      "Weights: [-4.3773 -0.5941 -0.1694 -0.1594  0.1609]\n",
      "MSE loss: 117.9701\n",
      "Iteration: 138800\n",
      "Gradient: [  57.6939  -38.037    -4.0781 -225.5807  -91.5192]\n",
      "Weights: [-4.3775 -0.5937 -0.1693 -0.1599  0.161 ]\n",
      "MSE loss: 117.9345\n",
      "Iteration: 138900\n",
      "Gradient: [  29.6491   17.5098   83.5214 -341.4934   26.6287]\n",
      "Weights: [-4.3777 -0.5932 -0.1692 -0.1602  0.1611]\n",
      "MSE loss: 117.9056\n",
      "Iteration: 139000\n",
      "Gradient: [  12.5652  -39.3096  -72.4712 -258.1902  990.9445]\n",
      "Weights: [-4.3779 -0.5928 -0.1693 -0.1605  0.1612]\n",
      "MSE loss: 117.8794\n",
      "Iteration: 139100\n",
      "Gradient: [  -2.7569   13.3584  -84.3297  139.831  -389.6655]\n",
      "Weights: [-4.3781 -0.5924 -0.1694 -0.1606  0.1612]\n",
      "MSE loss: 117.85\n",
      "Iteration: 139200\n",
      "Gradient: [   9.1225  -76.2301  -91.9247  -66.9384 -387.7465]\n",
      "Weights: [-4.3783 -0.592  -0.1693 -0.1609  0.1612]\n",
      "MSE loss: 117.8292\n",
      "Iteration: 139300\n",
      "Gradient: [  -1.7154  -22.6671  -90.2302 -355.721  -351.6759]\n",
      "Weights: [-4.3785 -0.5916 -0.1692 -0.1613  0.1613]\n",
      "MSE loss: 117.7922\n",
      "Iteration: 139400\n",
      "Gradient: [  29.2263   -6.127   -43.126   149.2915 1103.7445]\n",
      "Weights: [-4.3787 -0.5911 -0.1691 -0.1614  0.1614]\n",
      "MSE loss: 117.7625\n",
      "Iteration: 139500\n",
      "Gradient: [-1.4989700e+01 -6.4395300e+01 -1.1830000e-01  2.1228770e+02\n",
      " -1.0273138e+03]\n",
      "Weights: [-4.3789 -0.5906 -0.1689 -0.1614  0.1612]\n",
      "MSE loss: 117.7494\n",
      "Iteration: 139600\n",
      "Gradient: [ 14.7844  -7.5515 178.0246 264.1671 420.0354]\n",
      "Weights: [-4.3792 -0.5902 -0.1687 -0.1615  0.1613]\n",
      "MSE loss: 117.7028\n",
      "Iteration: 139700\n",
      "Gradient: [  6.7195 -74.6538  -4.3054 -72.6523 -11.9835]\n",
      "Weights: [-4.3794 -0.5897 -0.1684 -0.1614  0.1613]\n",
      "MSE loss: 117.6783\n",
      "Iteration: 139800\n",
      "Gradient: [  31.5039  -54.8103 -156.1337  598.9234   91.1912]\n",
      "Weights: [-4.3795 -0.5893 -0.1683 -0.1618  0.1614]\n",
      "MSE loss: 117.651\n",
      "Iteration: 139900\n",
      "Gradient: [  12.9886  -36.4035  -69.1926 -464.3315   84.7958]\n",
      "Weights: [-4.3797 -0.5889 -0.1682 -0.1624  0.1615]\n",
      "MSE loss: 117.6125\n",
      "Iteration: 140000\n",
      "Gradient: [  33.3914  -14.6635   69.7527  232.9321 -243.1885]\n",
      "Weights: [-4.3799 -0.5885 -0.1681 -0.1625  0.1615]\n",
      "MSE loss: 117.5891\n",
      "Iteration: 140100\n",
      "Gradient: [  13.6789  -24.6092 -167.3716    9.811  -290.5098]\n",
      "Weights: [-4.3801 -0.5882 -0.1681 -0.1628  0.1616]\n",
      "MSE loss: 117.5734\n",
      "Iteration: 140200\n",
      "Gradient: [   -8.0807   -95.1971    45.6266  -133.8595 -1010.7375]\n",
      "Weights: [-4.3803 -0.5878 -0.1679 -0.1629  0.1617]\n",
      "MSE loss: 117.536\n",
      "Iteration: 140300\n",
      "Gradient: [  -5.5635  -28.0504  127.0181   70.3773 -806.2574]\n",
      "Weights: [-4.3805 -0.5873 -0.1678 -0.1636  0.1617]\n",
      "MSE loss: 117.5282\n",
      "Iteration: 140400\n",
      "Gradient: [  10.1898  -35.1411  -91.1969 -438.7035  -59.9749]\n",
      "Weights: [-4.3808 -0.5869 -0.1676 -0.1638  0.1618]\n",
      "MSE loss: 117.4937\n",
      "Iteration: 140500\n",
      "Gradient: [  40.2979    5.194   113.1575  162.8825 -700.3038]\n",
      "Weights: [-4.381  -0.5865 -0.1677 -0.1639  0.1619]\n",
      "MSE loss: 117.447\n",
      "Iteration: 140600\n",
      "Gradient: [ 45.348   69.6736  36.6175 105.3838 598.8679]\n",
      "Weights: [-4.3812 -0.5861 -0.1675 -0.1639  0.1621]\n",
      "MSE loss: 117.468\n",
      "Iteration: 140700\n",
      "Gradient: [ -15.135   -28.0635   39.7144 -209.6307  -19.8083]\n",
      "Weights: [-4.3814 -0.5857 -0.1674 -0.1648  0.1621]\n",
      "MSE loss: 117.3828\n",
      "Iteration: 140800\n",
      "Gradient: [ 39.4117 -59.5295  25.3958  73.8643 856.9447]\n",
      "Weights: [-4.3815 -0.5852 -0.1675 -0.1644  0.162 ]\n",
      "MSE loss: 117.3653\n",
      "Iteration: 140900\n",
      "Gradient: [ 54.6706 -26.3204 138.343  223.3889 733.9244]\n",
      "Weights: [-4.3818 -0.5847 -0.1674 -0.1647  0.1622]\n",
      "MSE loss: 117.3359\n",
      "Iteration: 141000\n",
      "Gradient: [ 5.010000e-02 -7.527190e+01  5.252110e+01 -4.971929e+02 -1.277525e+02]\n",
      "Weights: [-4.382  -0.5843 -0.1674 -0.1648  0.1621]\n",
      "MSE loss: 117.312\n",
      "Iteration: 141100\n",
      "Gradient: [ 13.4465 -10.9841 -76.3788 -25.8944  14.3405]\n",
      "Weights: [-4.3822 -0.584  -0.1673 -0.1652  0.1623]\n",
      "MSE loss: 117.2909\n",
      "Iteration: 141200\n",
      "Gradient: [  20.7123  -69.0609   46.1212  237.443  1290.5688]\n",
      "Weights: [-4.3824 -0.5835 -0.167  -0.1653  0.1623]\n",
      "MSE loss: 117.2619\n",
      "Iteration: 141300\n",
      "Gradient: [  34.0612  -65.8137  -62.5617 -319.3451   52.0156]\n",
      "Weights: [-4.3826 -0.583  -0.1669 -0.165   0.1621]\n",
      "MSE loss: 117.2365\n",
      "Iteration: 141400\n",
      "Gradient: [ 43.8968 -38.8874 -29.0001  21.6869 173.7153]\n",
      "Weights: [-4.3828 -0.5827 -0.1668 -0.1655  0.1622]\n",
      "MSE loss: 117.2044\n",
      "Iteration: 141500\n",
      "Gradient: [  32.9147  -53.3947   43.4444 -163.9447 -843.8641]\n",
      "Weights: [-4.3831 -0.5823 -0.1667 -0.1659  0.1623]\n",
      "MSE loss: 117.1839\n",
      "Iteration: 141600\n",
      "Gradient: [  27.1193   -8.9125  -96.3471  -82.2985 -289.9551]\n",
      "Weights: [-4.3833 -0.5819 -0.1667 -0.1661  0.1624]\n",
      "MSE loss: 117.147\n",
      "Iteration: 141700\n",
      "Gradient: [   1.1642  -15.2613  -51.3214 -269.2075  398.2581]\n",
      "Weights: [-4.3835 -0.5815 -0.1666 -0.1665  0.1624]\n",
      "MSE loss: 117.1291\n",
      "Iteration: 141800\n",
      "Gradient: [  3.8679   7.709   93.0329 120.681  955.3587]\n",
      "Weights: [-4.3837 -0.5812 -0.1664 -0.1669  0.1628]\n",
      "MSE loss: 117.1411\n",
      "Iteration: 141900\n",
      "Gradient: [  -5.6084  -44.6152 -100.7498 -273.2594 1009.0086]\n",
      "Weights: [-4.384  -0.5808 -0.1662 -0.167   0.1626]\n",
      "MSE loss: 117.062\n",
      "Iteration: 142000\n",
      "Gradient: [ -3.5386  -5.8666  92.7949  21.8492 -91.4724]\n",
      "Weights: [-4.3842 -0.5804 -0.1661 -0.1676  0.1628]\n",
      "MSE loss: 117.0317\n",
      "Iteration: 142100\n",
      "Gradient: [  19.3858  -35.5635   27.3703  167.134  1190.4683]\n",
      "Weights: [-4.3844 -0.5801 -0.1661 -0.1676  0.1629]\n",
      "MSE loss: 117.0224\n",
      "Iteration: 142200\n",
      "Gradient: [-10.5448 -64.5496  -9.6717 411.648  206.6806]\n",
      "Weights: [-4.3847 -0.5797 -0.1662 -0.1676  0.1629]\n",
      "MSE loss: 117.01\n",
      "Iteration: 142300\n",
      "Gradient: [  44.5846  -85.0229  -40.5995  399.2537 -726.7737]\n",
      "Weights: [-4.3849 -0.5793 -0.1661 -0.1678  0.1627]\n",
      "MSE loss: 116.9821\n",
      "Iteration: 142400\n",
      "Gradient: [  63.0881  -11.9699   24.1701  543.8372 -573.6399]\n",
      "Weights: [-4.3851 -0.5789 -0.1661 -0.1681  0.1631]\n",
      "MSE loss: 116.9561\n",
      "Iteration: 142500\n",
      "Gradient: [  13.8128  -87.3748  -58.5161 -308.3756  344.8436]\n",
      "Weights: [-4.3854 -0.5785 -0.1659 -0.1686  0.1631]\n",
      "MSE loss: 116.8978\n",
      "Iteration: 142600\n",
      "Gradient: [ -11.8486  -34.8344  -28.9344  -33.9091 -375.2947]\n",
      "Weights: [-4.3855 -0.578  -0.1659 -0.1691  0.1632]\n",
      "MSE loss: 116.8695\n",
      "Iteration: 142700\n",
      "Gradient: [   7.2986  -31.5669  112.1328   44.441  1128.1394]\n",
      "Weights: [-4.3857 -0.5776 -0.1657 -0.1693  0.1634]\n",
      "MSE loss: 116.8631\n",
      "Iteration: 142800\n",
      "Gradient: [ 40.425   -5.8968 -33.1023 183.6126 -91.9863]\n",
      "Weights: [-4.3859 -0.5772 -0.1655 -0.1693  0.1632]\n",
      "MSE loss: 116.8149\n",
      "Iteration: 142900\n",
      "Gradient: [ 31.9262  -9.4615  -4.2902  82.7484 621.8485]\n",
      "Weights: [-4.3861 -0.5767 -0.1654 -0.1694  0.1633]\n",
      "MSE loss: 116.7934\n",
      "Iteration: 143000\n",
      "Gradient: [  23.3838  -30.1514  -84.8763  220.3243 -191.0267]\n",
      "Weights: [-4.3863 -0.5763 -0.1651 -0.1693  0.163 ]\n",
      "MSE loss: 116.7919\n",
      "Iteration: 143100\n",
      "Gradient: [  28.8451  -38.1016  -93.5778 -416.8578   92.646 ]\n",
      "Weights: [-4.3866 -0.576  -0.165  -0.1693  0.1631]\n",
      "MSE loss: 116.7515\n",
      "Iteration: 143200\n",
      "Gradient: [ 32.4654 -65.6923 -21.2032 -14.2504   9.1773]\n",
      "Weights: [-4.3868 -0.5756 -0.1651 -0.1697  0.1633]\n",
      "MSE loss: 116.7224\n",
      "Iteration: 143300\n",
      "Gradient: [  15.0787  -63.5188   73.1117  422.4204 -402.5632]\n",
      "Weights: [-4.387  -0.5753 -0.1649 -0.17    0.1633]\n",
      "MSE loss: 116.698\n",
      "Iteration: 143400\n",
      "Gradient: [   8.1085   -5.6654   58.717   156.6881 -689.2241]\n",
      "Weights: [-4.3873 -0.575  -0.165  -0.1701  0.1633]\n",
      "MSE loss: 116.6767\n",
      "Iteration: 143500\n",
      "Gradient: [  37.5728  -68.9317  -71.536  -440.5437 1023.5302]\n",
      "Weights: [-4.3875 -0.5746 -0.1647 -0.1701  0.1633]\n",
      "MSE loss: 116.6527\n",
      "Iteration: 143600\n",
      "Gradient: [ 58.4583 -13.2866  -1.6164 203.8793 834.8623]\n",
      "Weights: [-4.3877 -0.5742 -0.1646 -0.1701  0.1634]\n",
      "MSE loss: 116.6732\n",
      "Iteration: 143700\n",
      "Gradient: [  20.4523  -63.8298    2.3166 -259.4449 -370.6023]\n",
      "Weights: [-4.3879 -0.5738 -0.1647 -0.1703  0.1633]\n",
      "MSE loss: 116.6101\n",
      "Iteration: 143800\n",
      "Gradient: [ 45.4434 -93.2194  40.2099  91.8339 139.878 ]\n",
      "Weights: [-4.3882 -0.5734 -0.1647 -0.1706  0.1635]\n",
      "MSE loss: 116.5886\n",
      "Iteration: 143900\n",
      "Gradient: [  64.6699  -45.6728   23.1644 -194.4089   15.664 ]\n",
      "Weights: [-4.3884 -0.573  -0.1647 -0.1708  0.1636]\n",
      "MSE loss: 116.5745\n",
      "Iteration: 144000\n",
      "Gradient: [  12.2769  -26.3239   43.5269  295.9214 1494.4937]\n",
      "Weights: [-4.3887 -0.5727 -0.1647 -0.1708  0.1636]\n",
      "MSE loss: 116.5486\n",
      "Iteration: 144100\n",
      "Gradient: [   4.2456  -90.3188  -59.5451  390.6314 -216.3665]\n",
      "Weights: [-4.3889 -0.5723 -0.1647 -0.1712  0.1635]\n",
      "MSE loss: 116.5297\n",
      "Iteration: 144200\n",
      "Gradient: [ 2.810300e+01 -7.260200e+01 -9.322300e+00 -9.330000e-02 -6.390125e+02]\n",
      "Weights: [-4.3891 -0.5719 -0.1647 -0.1716  0.1636]\n",
      "MSE loss: 116.5042\n",
      "Iteration: 144300\n",
      "Gradient: [  46.0485    2.4363  -24.9914 -295.9373 -470.4132]\n",
      "Weights: [-4.3893 -0.5715 -0.1648 -0.1718  0.1638]\n",
      "MSE loss: 116.4498\n",
      "Iteration: 144400\n",
      "Gradient: [ -27.5061  -94.8729 -160.8483 -643.8664 -152.8214]\n",
      "Weights: [-4.3896 -0.5711 -0.1647 -0.1722  0.1639]\n",
      "MSE loss: 116.4278\n",
      "Iteration: 144500\n",
      "Gradient: [  -5.3809   -1.4824  187.0767    1.4251 -360.6352]\n",
      "Weights: [-4.3898 -0.5707 -0.1646 -0.172   0.1639]\n",
      "MSE loss: 116.408\n",
      "Iteration: 144600\n",
      "Gradient: [   1.5237    1.6509   48.9998  368.319  -492.9657]\n",
      "Weights: [-4.39   -0.5704 -0.1647 -0.1721  0.1639]\n",
      "MSE loss: 116.3872\n",
      "Iteration: 144700\n",
      "Gradient: [  71.4867   23.2481  -54.5447  -51.8194 -210.7731]\n",
      "Weights: [-4.3902 -0.57   -0.1645 -0.1726  0.164 ]\n",
      "MSE loss: 116.3531\n",
      "Iteration: 144800\n",
      "Gradient: [  -0.973   -14.5135  -83.0327  119.2546 -478.1175]\n",
      "Weights: [-4.3905 -0.5696 -0.1645 -0.1728  0.1641]\n",
      "MSE loss: 116.3275\n",
      "Iteration: 144900\n",
      "Gradient: [ 6.6320000e+01 -6.0277600e+01 -6.2735600e+01 -1.8530000e-01\n",
      "  1.3573745e+03]\n",
      "Weights: [-4.3907 -0.5692 -0.1645 -0.1732  0.1643]\n",
      "MSE loss: 116.3123\n",
      "Iteration: 145000\n",
      "Gradient: [  56.0826  -87.5522   61.4714 -113.5244  -38.2263]\n",
      "Weights: [-4.3909 -0.5688 -0.1644 -0.1734  0.1641]\n",
      "MSE loss: 116.2922\n",
      "Iteration: 145100\n",
      "Gradient: [  -6.7274   20.5618  177.4659 -123.6313 -431.8208]\n",
      "Weights: [-4.3911 -0.5684 -0.1644 -0.1735  0.1644]\n",
      "MSE loss: 116.2643\n",
      "Iteration: 145200\n",
      "Gradient: [   5.8422  -44.9904  149.6419  312.9806 -275.4079]\n",
      "Weights: [-4.3914 -0.5681 -0.1643 -0.1737  0.1645]\n",
      "MSE loss: 116.25\n",
      "Iteration: 145300\n",
      "Gradient: [ 16.5646  -6.488   33.6525 155.3632 221.0234]\n",
      "Weights: [-4.3916 -0.5677 -0.1645 -0.1741  0.1645]\n",
      "MSE loss: 116.1962\n",
      "Iteration: 145400\n",
      "Gradient: [   5.5008  -57.7061  -53.8721 -224.8343 -852.4251]\n",
      "Weights: [-4.3918 -0.5673 -0.1644 -0.1743  0.1644]\n",
      "MSE loss: 116.1932\n",
      "Iteration: 145500\n",
      "Gradient: [  25.4305  -20.7516  -71.7282 -205.4957  278.6873]\n",
      "Weights: [-4.392  -0.5669 -0.1643 -0.1747  0.1646]\n",
      "MSE loss: 116.1445\n",
      "Iteration: 145600\n",
      "Gradient: [ 37.3631 -45.3644 -81.001  -30.4321 153.5457]\n",
      "Weights: [-4.3922 -0.5666 -0.1642 -0.175   0.1647]\n",
      "MSE loss: 116.1259\n",
      "Iteration: 145700\n",
      "Gradient: [ -15.4457  -10.5789 -153.1394 -155.0024  -10.0845]\n",
      "Weights: [-4.3924 -0.5662 -0.1642 -0.1754  0.1649]\n",
      "MSE loss: 116.0914\n",
      "Iteration: 145800\n",
      "Gradient: [  29.3486 -127.6603 -170.567    88.1329 -588.8474]\n",
      "Weights: [-4.3926 -0.5658 -0.164  -0.1757  0.1648]\n",
      "MSE loss: 116.0776\n",
      "Iteration: 145900\n",
      "Gradient: [   36.0022   -52.6031    60.2186     7.173  -1011.1918]\n",
      "Weights: [-4.3928 -0.5654 -0.1639 -0.1762  0.165 ]\n",
      "MSE loss: 116.0335\n",
      "Iteration: 146000\n",
      "Gradient: [ -7.6526 -58.2713 116.3412 170.5216 -32.7606]\n",
      "Weights: [-4.3929 -0.565  -0.1637 -0.1763  0.1652]\n",
      "MSE loss: 116.0167\n",
      "Iteration: 146100\n",
      "Gradient: [ 56.6609   2.1173 -56.366  167.2221  22.0231]\n",
      "Weights: [-4.3931 -0.5646 -0.1636 -0.1767  0.1652]\n",
      "MSE loss: 115.9803\n",
      "Iteration: 146200\n",
      "Gradient: [  18.0664  -88.6815  -19.0703  -35.5843 -752.6301]\n",
      "Weights: [-4.3934 -0.5642 -0.1635 -0.1766  0.165 ]\n",
      "MSE loss: 115.9835\n",
      "Iteration: 146300\n",
      "Gradient: [  17.15     41.8394   37.446    50.4169 1443.3644]\n",
      "Weights: [-4.3936 -0.5639 -0.1634 -0.1769  0.1653]\n",
      "MSE loss: 115.9473\n",
      "Iteration: 146400\n",
      "Gradient: [  12.2389  -18.0969  -74.5152  -17.9763 -595.9215]\n",
      "Weights: [-4.3937 -0.5635 -0.1632 -0.177   0.1651]\n",
      "MSE loss: 115.9183\n",
      "Iteration: 146500\n",
      "Gradient: [  17.5696  -72.541   -61.0657   30.3825 -264.3579]\n",
      "Weights: [-4.394  -0.5632 -0.163  -0.1771  0.1652]\n",
      "MSE loss: 115.8896\n",
      "Iteration: 146600\n",
      "Gradient: [   2.6887  -61.4721 -163.7946  254.1881  510.6473]\n",
      "Weights: [-4.3942 -0.5627 -0.163  -0.1774  0.1654]\n",
      "MSE loss: 115.8723\n",
      "Iteration: 146700\n",
      "Gradient: [   7.9882  -68.1654 -121.4273  248.912  -143.251 ]\n",
      "Weights: [-4.3944 -0.5623 -0.1628 -0.1776  0.1652]\n",
      "MSE loss: 115.8473\n",
      "Iteration: 146800\n",
      "Gradient: [ 33.807  -15.7045  26.668   67.1219 844.8396]\n",
      "Weights: [-4.3946 -0.562  -0.1625 -0.1777  0.1654]\n",
      "MSE loss: 115.8242\n",
      "Iteration: 146900\n",
      "Gradient: [ 30.4709 -36.3917  62.2745 311.3488 511.4601]\n",
      "Weights: [-4.3948 -0.5616 -0.1626 -0.1778  0.1653]\n",
      "MSE loss: 115.7918\n",
      "Iteration: 147000\n",
      "Gradient: [  13.5516  -75.2799   -8.1921 -387.1714 -189.0361]\n",
      "Weights: [-4.395  -0.5612 -0.1624 -0.1782  0.1654]\n",
      "MSE loss: 115.7668\n",
      "Iteration: 147100\n",
      "Gradient: [ 40.6585 -30.6131 124.598  -98.5924 818.6675]\n",
      "Weights: [-4.3952 -0.5608 -0.1623 -0.1785  0.1655]\n",
      "MSE loss: 115.7374\n",
      "Iteration: 147200\n",
      "Gradient: [ -7.6166  13.1166  64.9471 -65.1834 921.9006]\n",
      "Weights: [-4.3954 -0.5604 -0.1622 -0.1785  0.1656]\n",
      "MSE loss: 115.7242\n",
      "Iteration: 147300\n",
      "Gradient: [    2.1789    24.7018   -58.1724  -215.2519 -1123.5902]\n",
      "Weights: [-4.3957 -0.5601 -0.1621 -0.1783  0.1653]\n",
      "MSE loss: 115.7123\n",
      "Iteration: 147400\n",
      "Gradient: [   7.4899  -35.8291  -59.6703  169.5235 -228.9   ]\n",
      "Weights: [-4.3959 -0.5597 -0.162  -0.1784  0.1654]\n",
      "MSE loss: 115.6846\n",
      "Iteration: 147500\n",
      "Gradient: [  10.4641  -21.6796 -149.4748 -120.5941 -112.3894]\n",
      "Weights: [-4.3961 -0.5595 -0.1621 -0.179   0.1657]\n",
      "MSE loss: 115.6618\n",
      "Iteration: 147600\n",
      "Gradient: [  21.1007   10.5058 -109.3101  247.0291  168.9393]\n",
      "Weights: [-4.3963 -0.5591 -0.162  -0.1796  0.1659]\n",
      "MSE loss: 115.6409\n",
      "Iteration: 147700\n",
      "Gradient: [  -2.422   -13.4426 -144.2077  180.2637  313.7245]\n",
      "Weights: [-4.3965 -0.5587 -0.1621 -0.1801  0.166 ]\n",
      "MSE loss: 115.5934\n",
      "Iteration: 147800\n",
      "Gradient: [  32.9231  -89.2241   89.134    21.6079 -243.3232]\n",
      "Weights: [-4.3967 -0.5583 -0.162  -0.1805  0.1661]\n",
      "MSE loss: 115.569\n",
      "Iteration: 147900\n",
      "Gradient: [-11.9048 -11.9966 114.1784 -18.1388 128.0374]\n",
      "Weights: [-4.3969 -0.5579 -0.1618 -0.1808  0.1663]\n",
      "MSE loss: 115.5632\n",
      "Iteration: 148000\n",
      "Gradient: [  23.2933  -45.6049 -105.1469  101.8128  863.2422]\n",
      "Weights: [-4.397  -0.5575 -0.1616 -0.1813  0.1663]\n",
      "MSE loss: 115.506\n",
      "Iteration: 148100\n",
      "Gradient: [  23.6355  -81.5739  -58.3071 -300.0204 -242.2847]\n",
      "Weights: [-4.3973 -0.5571 -0.1616 -0.1815  0.1664]\n",
      "MSE loss: 115.4811\n",
      "Iteration: 148200\n",
      "Gradient: [  12.557    -5.3156  122.9409 -206.2251  542.7676]\n",
      "Weights: [-4.3975 -0.5567 -0.1616 -0.1816  0.1665]\n",
      "MSE loss: 115.4714\n",
      "Iteration: 148300\n",
      "Gradient: [  24.5988  -12.8286  -45.7642  289.5374 -518.0967]\n",
      "Weights: [-4.3977 -0.5563 -0.1615 -0.182   0.1665]\n",
      "MSE loss: 115.4323\n",
      "Iteration: 148400\n",
      "Gradient: [ -21.8104  -35.6617  -22.3937   83.5096 -396.1356]\n",
      "Weights: [-4.3979 -0.5558 -0.1613 -0.1821  0.1664]\n",
      "MSE loss: 115.4486\n",
      "Iteration: 148500\n",
      "Gradient: [ 21.2034 -14.2078  42.416   32.6678 253.928 ]\n",
      "Weights: [-4.3981 -0.5554 -0.1611 -0.1822  0.1664]\n",
      "MSE loss: 115.3834\n",
      "Iteration: 148600\n",
      "Gradient: [  49.2143  -44.322   -94.9031  337.3356 -466.048 ]\n",
      "Weights: [-4.3982 -0.555  -0.161  -0.1825  0.1665]\n",
      "MSE loss: 115.3578\n",
      "Iteration: 148700\n",
      "Gradient: [  15.4891   12.0998   89.4612 -602.0357 -793.8343]\n",
      "Weights: [-4.3985 -0.5546 -0.1609 -0.1829  0.1665]\n",
      "MSE loss: 115.3929\n",
      "Iteration: 148800\n",
      "Gradient: [ 20.0229 -11.9297  43.6181 248.6089 646.7033]\n",
      "Weights: [-4.3987 -0.5543 -0.1608 -0.1832  0.1667]\n",
      "MSE loss: 115.3011\n",
      "Iteration: 148900\n",
      "Gradient: [ -28.2149  -68.8323  -60.8104  -93.8108 -189.9911]\n",
      "Weights: [-4.3989 -0.5539 -0.1607 -0.1834  0.1668]\n",
      "MSE loss: 115.2824\n",
      "Iteration: 149000\n",
      "Gradient: [  26.6332  -35.927    33.3747  132.5036 -442.2592]\n",
      "Weights: [-4.3991 -0.5535 -0.1607 -0.1834  0.1668]\n",
      "MSE loss: 115.2553\n",
      "Iteration: 149100\n",
      "Gradient: [  -12.6018   -96.0243  -208.924    250.5248 -1417.4271]\n",
      "Weights: [-4.3994 -0.5532 -0.1607 -0.1839  0.1669]\n",
      "MSE loss: 115.2474\n",
      "Iteration: 149200\n",
      "Gradient: [  47.6613  -43.4866 -124.0249 -177.908    59.1627]\n",
      "Weights: [-4.3996 -0.5528 -0.1605 -0.1842  0.1671]\n",
      "MSE loss: 115.199\n",
      "Iteration: 149300\n",
      "Gradient: [   47.1074   -46.2167   -14.2267  -104.8404 -1045.2624]\n",
      "Weights: [-4.3998 -0.5524 -0.1605 -0.1845  0.1671]\n",
      "MSE loss: 115.1753\n",
      "Iteration: 149400\n",
      "Gradient: [ 48.4    -49.4523  49.5404 -35.1048 438.4127]\n",
      "Weights: [-4.4    -0.5519 -0.1604 -0.1845  0.1671]\n",
      "MSE loss: 115.1506\n",
      "Iteration: 149500\n",
      "Gradient: [ -8.1752 -71.8011 -91.3137 -40.4373 762.421 ]\n",
      "Weights: [-4.4002 -0.5516 -0.1602 -0.1848  0.1672]\n",
      "MSE loss: 115.1335\n",
      "Iteration: 149600\n",
      "Gradient: [   0.3579  -12.0334  -68.5734   25.8479 -320.6618]\n",
      "Weights: [-4.4004 -0.5512 -0.16   -0.1853  0.1673]\n",
      "MSE loss: 115.0953\n",
      "Iteration: 149700\n",
      "Gradient: [  28.2146  -68.9682 -118.9468   40.5947  354.3691]\n",
      "Weights: [-4.4007 -0.5509 -0.1598 -0.1853  0.1673]\n",
      "MSE loss: 115.0838\n",
      "Iteration: 149800\n",
      "Gradient: [  48.8388  -50.654   -26.6838   -8.7199 -514.7331]\n",
      "Weights: [-4.4009 -0.5504 -0.1598 -0.1856  0.1673]\n",
      "MSE loss: 115.0491\n",
      "Iteration: 149900\n",
      "Gradient: [  29.6778 -100.435    -3.3548  120.6374 -361.5306]\n",
      "Weights: [-4.4011 -0.55   -0.1596 -0.1857  0.1673]\n",
      "MSE loss: 115.0267\n",
      "Iteration: 150000\n",
      "Gradient: [   26.3545   -95.9138   -32.4457  -185.8749 -1038.5528]\n",
      "Weights: [-4.4013 -0.5497 -0.1595 -0.1859  0.1673]\n",
      "MSE loss: 115.0304\n",
      "Iteration: 150100\n",
      "Gradient: [-27.4541  -9.151   29.066    5.7927 204.2586]\n",
      "Weights: [-4.4015 -0.5493 -0.1594 -0.1861  0.1675]\n",
      "MSE loss: 114.9761\n",
      "Iteration: 150200\n",
      "Gradient: [ 35.7313 -15.3245  82.3006  81.1564 868.055 ]\n",
      "Weights: [-4.4017 -0.549  -0.1595 -0.1869  0.1678]\n",
      "MSE loss: 114.9507\n",
      "Iteration: 150300\n",
      "Gradient: [ -18.3937  -54.7232 -159.5367  204.5591  606.0021]\n",
      "Weights: [-4.4019 -0.5485 -0.1593 -0.1869  0.1677]\n",
      "MSE loss: 114.9265\n",
      "Iteration: 150400\n",
      "Gradient: [   23.3404    14.2631    34.5783   201.3438 -1131.0863]\n",
      "Weights: [-4.4021 -0.5481 -0.1591 -0.1874  0.1679]\n",
      "MSE loss: 114.9\n",
      "Iteration: 150500\n",
      "Gradient: [ 26.0688 -49.1151 -37.3717 219.2158 241.9529]\n",
      "Weights: [-4.4023 -0.5477 -0.159  -0.1878  0.168 ]\n",
      "MSE loss: 114.8697\n",
      "Iteration: 150600\n",
      "Gradient: [  -7.9304  -70.3239  -14.9529 -145.5871 -887.7135]\n",
      "Weights: [-4.4024 -0.5473 -0.159  -0.1883  0.1681]\n",
      "MSE loss: 114.8476\n",
      "Iteration: 150700\n",
      "Gradient: [ 16.1799 -45.6691  36.067   40.2467 350.9934]\n",
      "Weights: [-4.4026 -0.5469 -0.1591 -0.1882  0.1681]\n",
      "MSE loss: 114.8199\n",
      "Iteration: 150800\n",
      "Gradient: [ 64.267  -31.6074 -65.853  283.6098 565.762 ]\n",
      "Weights: [-4.4028 -0.5465 -0.1589 -0.1883  0.1682]\n",
      "MSE loss: 114.8214\n",
      "Iteration: 150900\n",
      "Gradient: [  42.1963  -30.7653   -1.9444  -74.6345 -265.7331]\n",
      "Weights: [-4.403  -0.5462 -0.1587 -0.1884  0.1681]\n",
      "MSE loss: 114.7779\n",
      "Iteration: 151000\n",
      "Gradient: [  45.4242   24.6392   69.1974 -257.1923  201.2302]\n",
      "Weights: [-4.4032 -0.5458 -0.1585 -0.1886  0.1682]\n",
      "MSE loss: 114.7641\n",
      "Iteration: 151100\n",
      "Gradient: [ 14.6178  11.5463  22.9909  63.0258 976.0828]\n",
      "Weights: [-4.4034 -0.5455 -0.1583 -0.1886  0.1682]\n",
      "MSE loss: 114.7623\n",
      "Iteration: 151200\n",
      "Gradient: [  7.4261 -29.4332 -69.382  177.3866 134.2007]\n",
      "Weights: [-4.4036 -0.5452 -0.1582 -0.1889  0.1681]\n",
      "MSE loss: 114.7221\n",
      "Iteration: 151300\n",
      "Gradient: [ -4.2883   2.2107  74.9782  67.4274 580.2486]\n",
      "Weights: [-4.4038 -0.5447 -0.1581 -0.1885  0.168 ]\n",
      "MSE loss: 114.7043\n",
      "Iteration: 151400\n",
      "Gradient: [ 21.286  -98.6496 -77.4125 -93.6335 140.4797]\n",
      "Weights: [-4.404  -0.5444 -0.158  -0.1888  0.168 ]\n",
      "MSE loss: 114.6875\n",
      "Iteration: 151500\n",
      "Gradient: [ 19.8846 -46.9163 197.1442 441.2003 -61.4997]\n",
      "Weights: [-4.4042 -0.544  -0.1579 -0.1889  0.1682]\n",
      "MSE loss: 114.6929\n",
      "Iteration: 151600\n",
      "Gradient: [  14.9557  -25.3369 -113.6593 -177.0768   68.781 ]\n",
      "Weights: [-4.4044 -0.5437 -0.1579 -0.1892  0.1682]\n",
      "MSE loss: 114.6342\n",
      "Iteration: 151700\n",
      "Gradient: [  13.3952  -86.247    90.024    46.5835 -625.438 ]\n",
      "Weights: [-4.4046 -0.5433 -0.1579 -0.189   0.168 ]\n",
      "MSE loss: 114.6219\n",
      "Iteration: 151800\n",
      "Gradient: [  -9.853   -34.6758   68.199   208.4258 -736.8127]\n",
      "Weights: [-4.4049 -0.543  -0.1579 -0.1893  0.168 ]\n",
      "MSE loss: 114.623\n",
      "Iteration: 151900\n",
      "Gradient: [  62.8639  -57.1168  -34.391  -381.5828  535.1304]\n",
      "Weights: [-4.4051 -0.5426 -0.1579 -0.1897  0.1683]\n",
      "MSE loss: 114.5735\n",
      "Iteration: 152000\n",
      "Gradient: [  32.139   -35.0012 -121.6037  149.0817  537.5343]\n",
      "Weights: [-4.4053 -0.5422 -0.1579 -0.1901  0.1684]\n",
      "MSE loss: 114.5422\n",
      "Iteration: 152100\n",
      "Gradient: [  12.0382  -45.3674    3.6157 -269.185   379.4966]\n",
      "Weights: [-4.4055 -0.5419 -0.1578 -0.1905  0.1685]\n",
      "MSE loss: 114.5274\n",
      "Iteration: 152200\n",
      "Gradient: [  21.8509  -11.0425   51.8767  209.3091 -512.6434]\n",
      "Weights: [-4.4057 -0.5415 -0.1577 -0.1905  0.1685]\n",
      "MSE loss: 114.4974\n",
      "Iteration: 152300\n",
      "Gradient: [-16.3129  -0.6429  -9.3406  46.6243 276.7778]\n",
      "Weights: [-4.4059 -0.5411 -0.1576 -0.1906  0.1686]\n",
      "MSE loss: 114.4745\n",
      "Iteration: 152400\n",
      "Gradient: [  11.5854  -64.2144  -38.5197 -321.5892  569.707 ]\n",
      "Weights: [-4.4061 -0.5407 -0.1576 -0.1911  0.1687]\n",
      "MSE loss: 114.4471\n",
      "Iteration: 152500\n",
      "Gradient: [  21.9633  -80.8672  184.2921 -181.5181  253.1452]\n",
      "Weights: [-4.4063 -0.5404 -0.1575 -0.1911  0.1688]\n",
      "MSE loss: 114.4442\n",
      "Iteration: 152600\n",
      "Gradient: [   1.9616  -88.3038  -80.551  -132.628  -344.6523]\n",
      "Weights: [-4.4065 -0.54   -0.1574 -0.1913  0.1687]\n",
      "MSE loss: 114.408\n",
      "Iteration: 152700\n",
      "Gradient: [  10.5902  -20.9375  -46.7173   -5.5571 -512.6489]\n",
      "Weights: [-4.4067 -0.5396 -0.1572 -0.1915  0.1688]\n",
      "MSE loss: 114.385\n",
      "Iteration: 152800\n",
      "Gradient: [   22.2405   -23.2392   114.0705  -570.2408 -1089.5792]\n",
      "Weights: [-4.4069 -0.5391 -0.1571 -0.1912  0.1686]\n",
      "MSE loss: 114.3705\n",
      "Iteration: 152900\n",
      "Gradient: [-19.3344 -73.5677  19.4606 209.1848 292.9291]\n",
      "Weights: [-4.4071 -0.5389 -0.1571 -0.1911  0.1686]\n",
      "MSE loss: 114.36\n",
      "Iteration: 153000\n",
      "Gradient: [ -10.6598  -57.8787  -18.6749 -145.4298 -344.8485]\n",
      "Weights: [-4.4074 -0.5386 -0.1572 -0.1916  0.1687]\n",
      "MSE loss: 114.3322\n",
      "Iteration: 153100\n",
      "Gradient: [  22.6832  -11.0791   23.6327  162.2072 -627.7646]\n",
      "Weights: [-4.4076 -0.5382 -0.1572 -0.1915  0.1687]\n",
      "MSE loss: 114.3156\n",
      "Iteration: 153200\n",
      "Gradient: [-13.934    8.858  138.3931  28.1055 -81.9958]\n",
      "Weights: [-4.4078 -0.5379 -0.1571 -0.1916  0.1687]\n",
      "MSE loss: 114.2996\n",
      "Iteration: 153300\n",
      "Gradient: [  12.615   -43.3109   12.4584  188.4327 -299.5764]\n",
      "Weights: [-4.408  -0.5376 -0.157  -0.1917  0.1687]\n",
      "MSE loss: 114.2839\n",
      "Iteration: 153400\n",
      "Gradient: [  28.7563   -8.3583   97.2135   94.8596 -170.774 ]\n",
      "Weights: [-4.4082 -0.5373 -0.157  -0.1919  0.1688]\n",
      "MSE loss: 114.2679\n",
      "Iteration: 153500\n",
      "Gradient: [  30.3756   11.0458   15.0406 -230.9971  532.3711]\n",
      "Weights: [-4.4084 -0.5369 -0.1569 -0.1917  0.1688]\n",
      "MSE loss: 114.2614\n",
      "Iteration: 153600\n",
      "Gradient: [  3.018  -62.9788  29.6187 145.6787 676.4446]\n",
      "Weights: [-4.4086 -0.5366 -0.1567 -0.1917  0.1686]\n",
      "MSE loss: 114.2373\n",
      "Iteration: 153700\n",
      "Gradient: [ 47.3449 -29.809  -19.3856 475.0009 293.7016]\n",
      "Weights: [-4.4088 -0.5363 -0.1567 -0.192   0.1687]\n",
      "MSE loss: 114.2155\n",
      "Iteration: 153800\n",
      "Gradient: [   48.9721   -92.3756  -115.744   -267.5869 -1090.8077]\n",
      "Weights: [-4.4091 -0.536  -0.1566 -0.1922  0.1686]\n",
      "MSE loss: 114.2213\n",
      "Iteration: 153900\n",
      "Gradient: [ 28.4901 -56.5042  28.3746 447.2343 297.8036]\n",
      "Weights: [-4.4093 -0.5356 -0.1566 -0.1925  0.169 ]\n",
      "MSE loss: 114.1807\n",
      "Iteration: 154000\n",
      "Gradient: [  78.3029  -20.9541  -19.267   231.7405 -734.3709]\n",
      "Weights: [-4.4096 -0.5353 -0.1567 -0.1932  0.1691]\n",
      "MSE loss: 114.1315\n",
      "Iteration: 154100\n",
      "Gradient: [ 17.6014 -32.077   22.571  227.9185   5.5422]\n",
      "Weights: [-4.4098 -0.5349 -0.1565 -0.1933  0.1691]\n",
      "MSE loss: 114.1139\n",
      "Iteration: 154200\n",
      "Gradient: [-11.7186  40.4436  -2.362   30.5196 -74.4876]\n",
      "Weights: [-4.41   -0.5346 -0.1565 -0.1935  0.1692]\n",
      "MSE loss: 114.0915\n",
      "Iteration: 154300\n",
      "Gradient: [ 2.986000e-01 -4.335430e+01 -5.697930e+01 -4.935350e+01 -6.313682e+02]\n",
      "Weights: [-4.4102 -0.5342 -0.1565 -0.1939  0.1693]\n",
      "MSE loss: 114.0634\n",
      "Iteration: 154400\n",
      "Gradient: [ 36.9871  33.8164 -32.2817 133.3815 660.9899]\n",
      "Weights: [-4.4104 -0.5338 -0.1564 -0.1941  0.1694]\n",
      "MSE loss: 114.0478\n",
      "Iteration: 154500\n",
      "Gradient: [ -24.1359  -70.8321 -107.288  -165.4738   38.3559]\n",
      "Weights: [-4.4107 -0.5335 -0.1563 -0.1943  0.1695]\n",
      "MSE loss: 114.0172\n",
      "Iteration: 154600\n",
      "Gradient: [ 52.1525 -26.2999 -17.2033  31.1254 126.0376]\n",
      "Weights: [-4.4109 -0.5331 -0.1562 -0.1947  0.1696]\n",
      "MSE loss: 113.9979\n",
      "Iteration: 154700\n",
      "Gradient: [  -2.8927  -29.28     21.6066 -171.4053 -227.6004]\n",
      "Weights: [-4.4111 -0.5328 -0.1561 -0.1949  0.1695]\n",
      "MSE loss: 113.9704\n",
      "Iteration: 154800\n",
      "Gradient: [   9.3454 -109.1058 -128.2275   36.2548 -258.3655]\n",
      "Weights: [-4.4113 -0.5324 -0.1561 -0.195   0.1696]\n",
      "MSE loss: 113.9559\n",
      "Iteration: 154900\n",
      "Gradient: [ 13.5774   7.7226 114.1751 267.2067 744.0888]\n",
      "Weights: [-4.4115 -0.5321 -0.1561 -0.1952  0.1697]\n",
      "MSE loss: 113.9324\n",
      "Iteration: 155000\n",
      "Gradient: [  96.3462    4.223   -21.6791 -307.4299 -343.9354]\n",
      "Weights: [-4.4117 -0.5317 -0.1561 -0.1957  0.1698]\n",
      "MSE loss: 113.9036\n",
      "Iteration: 155100\n",
      "Gradient: [  25.0626  -21.1421  -12.5388  222.2158 -368.7223]\n",
      "Weights: [-4.4119 -0.5314 -0.156  -0.196   0.1698]\n",
      "MSE loss: 113.897\n",
      "Iteration: 155200\n",
      "Gradient: [  36.1886  -78.4243  120.7111  -41.9501 -408.1002]\n",
      "Weights: [-4.4121 -0.5311 -0.156  -0.1958  0.1698]\n",
      "MSE loss: 113.87\n",
      "Iteration: 155300\n",
      "Gradient: [  22.4398  -65.1284    8.5664 -331.0425 -837.4625]\n",
      "Weights: [-4.4123 -0.5307 -0.156  -0.1958  0.1699]\n",
      "MSE loss: 113.8541\n",
      "Iteration: 155400\n",
      "Gradient: [   -6.9602  -112.5864    43.6109   -33.5831 -1318.4956]\n",
      "Weights: [-4.4125 -0.5304 -0.156  -0.1963  0.1699]\n",
      "MSE loss: 113.8411\n",
      "Iteration: 155500\n",
      "Gradient: [  45.0032   17.952   -10.6423  139.7513 -434.3024]\n",
      "Weights: [-4.4127 -0.5301 -0.1558 -0.1963  0.1698]\n",
      "MSE loss: 113.8319\n",
      "Iteration: 155600\n",
      "Gradient: [ 47.3327  22.6834 147.1679 135.6168 -16.2014]\n",
      "Weights: [-4.4129 -0.5297 -0.1556 -0.1966  0.17  ]\n",
      "MSE loss: 113.7869\n",
      "Iteration: 155700\n",
      "Gradient: [  40.3594  -16.0395  -42.621  -142.4832  -66.5821]\n",
      "Weights: [-4.4131 -0.5293 -0.1554 -0.1967  0.17  ]\n",
      "MSE loss: 113.7652\n",
      "Iteration: 155800\n",
      "Gradient: [ 10.8432  30.5035  14.4981  58.6034 419.671 ]\n",
      "Weights: [-4.4133 -0.529  -0.1552 -0.197   0.1702]\n",
      "MSE loss: 113.7928\n",
      "Iteration: 155900\n",
      "Gradient: [  33.2447  -24.9982 -172.3876  181.4744  -28.6141]\n",
      "Weights: [-4.4136 -0.5287 -0.1552 -0.1974  0.1703]\n",
      "MSE loss: 113.7245\n",
      "Iteration: 156000\n",
      "Gradient: [ -18.4875  -83.2242    6.9714 -171.7161  350.5838]\n",
      "Weights: [-4.4137 -0.5284 -0.1551 -0.1977  0.1703]\n",
      "MSE loss: 113.6959\n",
      "Iteration: 156100\n",
      "Gradient: [  11.0777  -15.432   -63.4298  -27.0227 -373.5783]\n",
      "Weights: [-4.4139 -0.528  -0.1549 -0.1978  0.1704]\n",
      "MSE loss: 113.6836\n",
      "Iteration: 156200\n",
      "Gradient: [  9.7381 -47.8818  16.9353  97.1065 -96.1728]\n",
      "Weights: [-4.4141 -0.5277 -0.1549 -0.198   0.1704]\n",
      "MSE loss: 113.6572\n",
      "Iteration: 156300\n",
      "Gradient: [   6.4315   25.9683  109.6274 -298.4827 -404.5978]\n",
      "Weights: [-4.4143 -0.5274 -0.1549 -0.1984  0.1705]\n",
      "MSE loss: 113.6383\n",
      "Iteration: 156400\n",
      "Gradient: [  -2.2457  -18.8216   39.587  -429.0586 -227.8219]\n",
      "Weights: [-4.4145 -0.5272 -0.1548 -0.1987  0.1705]\n",
      "MSE loss: 113.6182\n",
      "Iteration: 156500\n",
      "Gradient: [  -2.3246  -35.2269 -159.9418 -306.947  -659.3435]\n",
      "Weights: [-4.4147 -0.5268 -0.1549 -0.1986  0.1705]\n",
      "MSE loss: 113.6057\n",
      "Iteration: 156600\n",
      "Gradient: [  45.4919  -39.9133 -228.9691  126.1427  216.5266]\n",
      "Weights: [-4.4149 -0.5265 -0.1547 -0.199   0.1705]\n",
      "MSE loss: 113.6007\n",
      "Iteration: 156700\n",
      "Gradient: [ 37.4615   9.0626 -23.3688 -92.7619 838.0302]\n",
      "Weights: [-4.4151 -0.5261 -0.1547 -0.199   0.1707]\n",
      "MSE loss: 113.568\n",
      "Iteration: 156800\n",
      "Gradient: [   19.9576   -24.4852    -3.2558     7.706  -1036.0063]\n",
      "Weights: [-4.4153 -0.5258 -0.1545 -0.1991  0.1706]\n",
      "MSE loss: 113.5433\n",
      "Iteration: 156900\n",
      "Gradient: [  12.4152  -56.6733  -41.6161 -156.7014 -197.1577]\n",
      "Weights: [-4.4154 -0.5254 -0.1545 -0.1993  0.1706]\n",
      "MSE loss: 113.5223\n",
      "Iteration: 157000\n",
      "Gradient: [  45.2801  -29.2105   45.8187  394.0277 1412.835 ]\n",
      "Weights: [-4.4157 -0.5251 -0.1545 -0.1997  0.1708]\n",
      "MSE loss: 113.5002\n",
      "Iteration: 157100\n",
      "Gradient: [   3.8291  -30.9164   11.6557 -187.3364   -7.2984]\n",
      "Weights: [-4.4158 -0.5247 -0.1545 -0.1996  0.1707]\n",
      "MSE loss: 113.4851\n",
      "Iteration: 157200\n",
      "Gradient: [ 40.8735 -24.9631 -17.3434 165.9927  82.336 ]\n",
      "Weights: [-4.416  -0.5243 -0.1545 -0.1997  0.1708]\n",
      "MSE loss: 113.4664\n",
      "Iteration: 157300\n",
      "Gradient: [ 44.2362 -15.5465 -51.7371 -31.723  403.9257]\n",
      "Weights: [-4.4163 -0.524  -0.1545 -0.2003  0.1711]\n",
      "MSE loss: 113.4556\n",
      "Iteration: 157400\n",
      "Gradient: [   4.9959  -50.6167   -8.4303 -100.2219    4.8026]\n",
      "Weights: [-4.4164 -0.5237 -0.1544 -0.2004  0.1709]\n",
      "MSE loss: 113.4417\n",
      "Iteration: 157500\n",
      "Gradient: [ -28.0716  -59.2253  -30.3962 -257.6869 -450.5344]\n",
      "Weights: [-4.4166 -0.5233 -0.1543 -0.2005  0.171 ]\n",
      "MSE loss: 113.404\n",
      "Iteration: 157600\n",
      "Gradient: [-5.96900e-01 -2.25300e+01  4.37154e+01  4.95914e+01  6.73479e+02]\n",
      "Weights: [-4.4168 -0.5229 -0.1542 -0.2007  0.1711]\n",
      "MSE loss: 113.388\n",
      "Iteration: 157700\n",
      "Gradient: [  11.6332  -31.8391   42.6777   83.4337 -643.8642]\n",
      "Weights: [-4.417  -0.5226 -0.1539 -0.2005  0.1709]\n",
      "MSE loss: 113.3658\n",
      "Iteration: 157800\n",
      "Gradient: [  39.0477  -56.6846   19.6672  157.5527 -649.5978]\n",
      "Weights: [-4.4172 -0.5223 -0.1539 -0.2007  0.1709]\n",
      "MSE loss: 113.3558\n",
      "Iteration: 157900\n",
      "Gradient: [   23.1444   -92.8122  -105.1461   344.547  -1520.9812]\n",
      "Weights: [-4.4174 -0.5219 -0.1537 -0.2009  0.1711]\n",
      "MSE loss: 113.3363\n",
      "Iteration: 158000\n",
      "Gradient: [  23.1647  -81.8564   41.9554 -150.542   369.2153]\n",
      "Weights: [-4.4176 -0.5216 -0.1537 -0.2011  0.1711]\n",
      "MSE loss: 113.3055\n",
      "Iteration: 158100\n",
      "Gradient: [  -9.7817 -103.4608 -115.0112  -32.5061  213.9569]\n",
      "Weights: [-4.4178 -0.5213 -0.1537 -0.2014  0.1711]\n",
      "MSE loss: 113.2982\n",
      "Iteration: 158200\n",
      "Gradient: [  -5.759   -86.8977 -126.3973 -261.7812  742.8963]\n",
      "Weights: [-4.418  -0.5209 -0.1535 -0.2016  0.1712]\n",
      "MSE loss: 113.2665\n",
      "Iteration: 158300\n",
      "Gradient: [ 28.2291 -64.4062 -21.073  -57.6309 208.934 ]\n",
      "Weights: [-4.4182 -0.5206 -0.1535 -0.202   0.1713]\n",
      "MSE loss: 113.2485\n",
      "Iteration: 158400\n",
      "Gradient: [ -21.1139 -100.4673   55.9985 -103.3966  176.978 ]\n",
      "Weights: [-4.4184 -0.5202 -0.1534 -0.2022  0.1714]\n",
      "MSE loss: 113.219\n",
      "Iteration: 158500\n",
      "Gradient: [ -1.2011 -38.0544 -27.5545 -87.2414 738.5783]\n",
      "Weights: [-4.4186 -0.5198 -0.1532 -0.2022  0.1713]\n",
      "MSE loss: 113.202\n",
      "Iteration: 158600\n",
      "Gradient: [ 51.0246 -44.11    78.5023 182.6897 516.3484]\n",
      "Weights: [-4.4187 -0.5195 -0.153  -0.2023  0.1714]\n",
      "MSE loss: 113.1858\n",
      "Iteration: 158700\n",
      "Gradient: [  24.9258  -38.4996  160.0662 -168.1118   58.7479]\n",
      "Weights: [-4.4189 -0.5192 -0.1528 -0.2022  0.1713]\n",
      "MSE loss: 113.1737\n",
      "Iteration: 158800\n",
      "Gradient: [  -9.1192  -42.794    37.4957  159.0158 -431.3095]\n",
      "Weights: [-4.4191 -0.5189 -0.1528 -0.2027  0.1715]\n",
      "MSE loss: 113.1454\n",
      "Iteration: 158900\n",
      "Gradient: [    3.5315   -53.5606   -42.4834    50.4603 -1248.4673]\n",
      "Weights: [-4.4193 -0.5185 -0.1528 -0.203   0.1715]\n",
      "MSE loss: 113.1225\n",
      "Iteration: 159000\n",
      "Gradient: [ -16.2996    6.9697  133.8332   47.7614 -311.4489]\n",
      "Weights: [-4.4196 -0.5182 -0.1527 -0.203   0.1715]\n",
      "MSE loss: 113.1062\n",
      "Iteration: 159100\n",
      "Gradient: [ 14.7245 -53.7737 -82.3582 -45.672  717.9731]\n",
      "Weights: [-4.4197 -0.5179 -0.1527 -0.2036  0.1717]\n",
      "MSE loss: 113.0808\n",
      "Iteration: 159200\n",
      "Gradient: [ 37.5958 -70.3663 -84.8129 -39.3763 168.9616]\n",
      "Weights: [-4.4199 -0.5175 -0.1528 -0.2038  0.1718]\n",
      "MSE loss: 113.061\n",
      "Iteration: 159300\n",
      "Gradient: [  19.4804   -8.4777 -134.6839   49.1405 -368.614 ]\n",
      "Weights: [-4.4201 -0.5172 -0.1529 -0.2041  0.172 ]\n",
      "MSE loss: 113.0534\n",
      "Iteration: 159400\n",
      "Gradient: [  31.3282   -5.0018  -11.5615  354.3385 -288.9767]\n",
      "Weights: [-4.4203 -0.5168 -0.1531 -0.2045  0.1721]\n",
      "MSE loss: 113.0163\n",
      "Iteration: 159500\n",
      "Gradient: [ -24.5313  -66.2329 -125.0875  127.8806 -374.979 ]\n",
      "Weights: [-4.4205 -0.5165 -0.153  -0.2052  0.1722]\n",
      "MSE loss: 112.9959\n",
      "Iteration: 159600\n",
      "Gradient: [ -8.5745  -5.9667  63.9284 229.599  464.3253]\n",
      "Weights: [-4.4207 -0.5161 -0.1528 -0.205   0.1722]\n",
      "MSE loss: 112.9764\n",
      "Iteration: 159700\n",
      "Gradient: [  22.2284   -5.5722  115.1612  257.7713 -244.2306]\n",
      "Weights: [-4.4208 -0.5157 -0.1529 -0.205   0.1722]\n",
      "MSE loss: 112.9635\n",
      "Iteration: 159800\n",
      "Gradient: [  25.2586  -56.0842 -114.4429  -88.3899  642.7368]\n",
      "Weights: [-4.421  -0.5154 -0.1528 -0.2053  0.1723]\n",
      "MSE loss: 112.938\n",
      "Iteration: 159900\n",
      "Gradient: [  47.5474  -75.1082  -84.3698 -157.6988   89.6987]\n",
      "Weights: [-4.4212 -0.5149 -0.1524 -0.2052  0.1721]\n",
      "MSE loss: 112.917\n",
      "Iteration: 160000\n",
      "Gradient: [   7.9547 -100.923   106.4443 -484.8394  209.0205]\n",
      "Weights: [-4.4214 -0.5146 -0.1524 -0.2054  0.1722]\n",
      "MSE loss: 112.8995\n",
      "Iteration: 160100\n",
      "Gradient: [  59.4438   22.1225   -3.0006 -220.3147 -229.3428]\n",
      "Weights: [-4.4216 -0.5142 -0.1523 -0.2055  0.1723]\n",
      "MSE loss: 112.8907\n",
      "Iteration: 160200\n",
      "Gradient: [  -6.5858  -21.5537   -7.5837   20.019  -100.7001]\n",
      "Weights: [-4.4218 -0.5139 -0.1522 -0.2056  0.1722]\n",
      "MSE loss: 112.8619\n",
      "Iteration: 160300\n",
      "Gradient: [ 32.863   15.8594 -58.3512 128.3036 888.8566]\n",
      "Weights: [-4.422  -0.5136 -0.1524 -0.2057  0.1723]\n",
      "MSE loss: 112.8467\n",
      "Iteration: 160400\n",
      "Gradient: [  34.7227  -59.6997   89.1597  329.217  -261.2984]\n",
      "Weights: [-4.4222 -0.5133 -0.1523 -0.2056  0.1721]\n",
      "MSE loss: 112.8454\n",
      "Iteration: 160500\n",
      "Gradient: [  19.5024    4.4892  -88.1737 -382.459   482.3663]\n",
      "Weights: [-4.4224 -0.513  -0.1522 -0.2055  0.1721]\n",
      "MSE loss: 112.8262\n",
      "Iteration: 160600\n",
      "Gradient: [  16.6145  -41.7995 -130.138   314.542   321.2436]\n",
      "Weights: [-4.4226 -0.5126 -0.1522 -0.2058  0.1722]\n",
      "MSE loss: 112.8\n",
      "Iteration: 160700\n",
      "Gradient: [  -6.383    -2.5245   35.9076  -24.5673 -352.3515]\n",
      "Weights: [-4.4228 -0.5123 -0.1522 -0.2061  0.1723]\n",
      "MSE loss: 112.7782\n",
      "Iteration: 160800\n",
      "Gradient: [ 55.8857 -39.3008 -43.9686  18.7997 558.1547]\n",
      "Weights: [-4.423  -0.512  -0.1521 -0.2063  0.1724]\n",
      "MSE loss: 112.7591\n",
      "Iteration: 160900\n",
      "Gradient: [   27.0387    -5.7404   -89.2079    53.5848 -1255.0268]\n",
      "Weights: [-4.4232 -0.5116 -0.152  -0.2064  0.1725]\n",
      "MSE loss: 112.7588\n",
      "Iteration: 161000\n",
      "Gradient: [ -23.0945  -37.8122   17.4503  -81.4428 -238.9542]\n",
      "Weights: [-4.4234 -0.5113 -0.1519 -0.2063  0.1723]\n",
      "MSE loss: 112.7307\n",
      "Iteration: 161100\n",
      "Gradient: [ 12.3017 -59.027   30.427  159.2695 630.9363]\n",
      "Weights: [-4.4235 -0.5109 -0.1519 -0.2062  0.1723]\n",
      "MSE loss: 112.7228\n",
      "Iteration: 161200\n",
      "Gradient: [ 35.4604  29.0629 -64.5956  95.5012  56.7652]\n",
      "Weights: [-4.4237 -0.5106 -0.1519 -0.2064  0.1723]\n",
      "MSE loss: 112.6992\n",
      "Iteration: 161300\n",
      "Gradient: [ 27.1132 -60.0543 -35.5258  10.0207 139.7021]\n",
      "Weights: [-4.424  -0.5103 -0.1519 -0.2063  0.1723]\n",
      "MSE loss: 112.6916\n",
      "Iteration: 161400\n",
      "Gradient: [  21.9251  -10.7818   33.1942  -25.3044 -466.5463]\n",
      "Weights: [-4.4242 -0.5101 -0.152  -0.2067  0.1725]\n",
      "MSE loss: 112.6704\n",
      "Iteration: 161500\n",
      "Gradient: [  26.0529  -52.1849 -137.8123   81.758  -148.7001]\n",
      "Weights: [-4.4244 -0.5098 -0.1518 -0.2072  0.1726]\n",
      "MSE loss: 112.639\n",
      "Iteration: 161600\n",
      "Gradient: [   7.9138  -65.4219  -10.8973 -121.4544 -155.5006]\n",
      "Weights: [-4.4247 -0.5095 -0.1519 -0.2074  0.1727]\n",
      "MSE loss: 112.6198\n",
      "Iteration: 161700\n",
      "Gradient: [ 17.7979  -8.467   57.7483 214.6546 780.4819]\n",
      "Weights: [-4.4249 -0.5091 -0.1518 -0.2072  0.1727]\n",
      "MSE loss: 112.6256\n",
      "Iteration: 161800\n",
      "Gradient: [  35.4508  -63.9489 -164.2513 -320.8662 -220.4379]\n",
      "Weights: [-4.4251 -0.5088 -0.1518 -0.2076  0.1727]\n",
      "MSE loss: 112.5886\n",
      "Iteration: 161900\n",
      "Gradient: [ -14.6868  -18.5642   57.3484 -198.0035 1075.6078]\n",
      "Weights: [-4.4253 -0.5085 -0.1518 -0.2079  0.1728]\n",
      "MSE loss: 112.5649\n",
      "Iteration: 162000\n",
      "Gradient: [ -27.8483  -46.6039   35.4963 -131.094   164.5001]\n",
      "Weights: [-4.4255 -0.5082 -0.1518 -0.208   0.1729]\n",
      "MSE loss: 112.5622\n",
      "Iteration: 162100\n",
      "Gradient: [   8.7751  -80.0294 -109.511   233.6196   75.7303]\n",
      "Weights: [-4.4257 -0.5078 -0.1517 -0.2081  0.1727]\n",
      "MSE loss: 112.5665\n",
      "Iteration: 162200\n",
      "Gradient: [  30.6562  -12.5356  -98.809    44.5733 -710.3868]\n",
      "Weights: [-4.4259 -0.5074 -0.1516 -0.2081  0.1728]\n",
      "MSE loss: 112.5146\n",
      "Iteration: 162300\n",
      "Gradient: [  22.6374   21.118   -33.0889  268.9568 -941.5015]\n",
      "Weights: [-4.4261 -0.5071 -0.1516 -0.2081  0.1727]\n",
      "MSE loss: 112.5092\n",
      "Iteration: 162400\n",
      "Gradient: [ -26.2613 -139.0321  -25.2747  -97.7771  -84.2061]\n",
      "Weights: [-4.4263 -0.5068 -0.1517 -0.2084  0.1728]\n",
      "MSE loss: 112.4947\n",
      "Iteration: 162500\n",
      "Gradient: [ -20.1863  -34.5262   44.7526   31.7401 1236.2728]\n",
      "Weights: [-4.4265 -0.5065 -0.1516 -0.2087  0.173 ]\n",
      "MSE loss: 112.4601\n",
      "Iteration: 162600\n",
      "Gradient: [ 1.044650e+01 -2.162000e-01 -1.527190e+01 -3.170900e+01  3.715034e+02]\n",
      "Weights: [-4.4267 -0.5062 -0.1517 -0.2089  0.1732]\n",
      "MSE loss: 112.4502\n",
      "Iteration: 162700\n",
      "Gradient: [  4.0146 -22.4257 -83.6806 -21.9625 253.7344]\n",
      "Weights: [-4.4269 -0.5059 -0.1517 -0.2094  0.1733]\n",
      "MSE loss: 112.4197\n",
      "Iteration: 162800\n",
      "Gradient: [  37.8509   14.0989  133.245    20.5581 -363.8969]\n",
      "Weights: [-4.4271 -0.5056 -0.1517 -0.2096  0.1733]\n",
      "MSE loss: 112.4046\n",
      "Iteration: 162900\n",
      "Gradient: [  48.0203  -79.822    -4.533   -10.8525 -938.8434]\n",
      "Weights: [-4.4273 -0.5053 -0.1515 -0.2098  0.1734]\n",
      "MSE loss: 112.3837\n",
      "Iteration: 163000\n",
      "Gradient: [ -17.0458  -32.0824  -51.8517  180.7294 -272.1827]\n",
      "Weights: [-4.4274 -0.5049 -0.1514 -0.2101  0.1734]\n",
      "MSE loss: 112.3656\n",
      "Iteration: 163100\n",
      "Gradient: [  16.3923   16.8824   91.0483  -76.0515 -423.9006]\n",
      "Weights: [-4.4276 -0.5046 -0.1514 -0.2105  0.1737]\n",
      "MSE loss: 112.3627\n",
      "Iteration: 163200\n",
      "Gradient: [  41.1772  -39.7724 -139.448  -327.9103  217.3524]\n",
      "Weights: [-4.4277 -0.5043 -0.1514 -0.2105  0.1736]\n",
      "MSE loss: 112.3299\n",
      "Iteration: 163300\n",
      "Gradient: [  25.2563  -32.7422   19.5798   37.5637 -832.4375]\n",
      "Weights: [-4.4278 -0.504  -0.1514 -0.2105  0.1735]\n",
      "MSE loss: 112.3156\n",
      "Iteration: 163400\n",
      "Gradient: [ 17.1304  24.1305  13.212  -96.5271 745.4742]\n",
      "Weights: [-4.428  -0.5037 -0.1515 -0.2108  0.1738]\n",
      "MSE loss: 112.3196\n",
      "Iteration: 163500\n",
      "Gradient: [  24.2301  -38.6047   84.1429 -232.5521  245.4868]\n",
      "Weights: [-4.4282 -0.5033 -0.1513 -0.2107  0.1734]\n",
      "MSE loss: 112.3071\n",
      "Iteration: 163600\n",
      "Gradient: [  37.8939  -83.7091   58.4091 -275.7386  756.0859]\n",
      "Weights: [-4.4283 -0.5029 -0.1511 -0.2106  0.1734]\n",
      "MSE loss: 112.2674\n",
      "Iteration: 163700\n",
      "Gradient: [   7.3659  -84.4179   -1.899  -291.551   606.005 ]\n",
      "Weights: [-4.4286 -0.5027 -0.151  -0.2104  0.1733]\n",
      "MSE loss: 112.2632\n",
      "Iteration: 163800\n",
      "Gradient: [ 17.944   -6.548  -48.7232 227.5499 105.4804]\n",
      "Weights: [-4.4288 -0.5024 -0.1508 -0.2106  0.1735]\n",
      "MSE loss: 112.246\n",
      "Iteration: 163900\n",
      "Gradient: [  62.6478   20.5582  103.1322 -117.2129  659.3579]\n",
      "Weights: [-4.429  -0.5021 -0.1509 -0.2111  0.1737]\n",
      "MSE loss: 112.2514\n",
      "Iteration: 164000\n",
      "Gradient: [  19.9461  -33.0501  -35.826   -36.351  -882.2991]\n",
      "Weights: [-4.4292 -0.5017 -0.1508 -0.2113  0.1736]\n",
      "MSE loss: 112.1964\n",
      "Iteration: 164100\n",
      "Gradient: [  -2.1123  -39.6804  -16.465  -248.3128   23.1768]\n",
      "Weights: [-4.4293 -0.5014 -0.1508 -0.2111  0.1735]\n",
      "MSE loss: 112.1935\n",
      "Iteration: 164200\n",
      "Gradient: [ 25.5618   7.8385  29.5908  35.3489 -42.9596]\n",
      "Weights: [-4.4295 -0.5011 -0.1508 -0.2111  0.1736]\n",
      "MSE loss: 112.1782\n",
      "Iteration: 164300\n",
      "Gradient: [  23.4613  -58.1306  -74.4759  240.622  -265.3159]\n",
      "Weights: [-4.4298 -0.5008 -0.1508 -0.2114  0.1736]\n",
      "MSE loss: 112.1566\n",
      "Iteration: 164400\n",
      "Gradient: [ -11.8659  -28.9168   49.5013 -200.9049 -208.1116]\n",
      "Weights: [-4.4299 -0.5005 -0.1509 -0.2118  0.1738]\n",
      "MSE loss: 112.1328\n",
      "Iteration: 164500\n",
      "Gradient: [ 10.8387 -71.4746  76.3206 -38.3927 365.3704]\n",
      "Weights: [-4.4301 -0.5002 -0.1508 -0.2122  0.174 ]\n",
      "MSE loss: 112.1156\n",
      "Iteration: 164600\n",
      "Gradient: [  26.4885   42.3686   12.8297 -138.2125 -645.0623]\n",
      "Weights: [-4.4303 -0.4999 -0.1508 -0.2125  0.174 ]\n",
      "MSE loss: 112.0893\n",
      "Iteration: 164700\n",
      "Gradient: [  36.7894  -96.6255 -187.127     8.3902 -284.1807]\n",
      "Weights: [-4.4304 -0.4996 -0.1507 -0.2124  0.1739]\n",
      "MSE loss: 112.0805\n",
      "Iteration: 164800\n",
      "Gradient: [  37.0838  -46.0613 -105.4819  -18.8529  524.5258]\n",
      "Weights: [-4.4306 -0.4992 -0.1505 -0.2125  0.1741]\n",
      "MSE loss: 112.0787\n",
      "Iteration: 164900\n",
      "Gradient: [   0.9968  -18.7598    3.3033 -344.3444  245.7878]\n",
      "Weights: [-4.4308 -0.4988 -0.1503 -0.2129  0.174 ]\n",
      "MSE loss: 112.0458\n",
      "Iteration: 165000\n",
      "Gradient: [ 13.8627 -46.5046  68.8976 200.77   560.7415]\n",
      "Weights: [-4.431  -0.4984 -0.1501 -0.2131  0.1741]\n",
      "MSE loss: 112.0133\n",
      "Iteration: 165100\n",
      "Gradient: [  11.5273   -8.6194    9.7476   29.08   1175.0421]\n",
      "Weights: [-4.4312 -0.4981 -0.15   -0.2134  0.1742]\n",
      "MSE loss: 111.9987\n",
      "Iteration: 165200\n",
      "Gradient: [ 35.1005 -76.1565 -54.4914 -42.3287 -71.8094]\n",
      "Weights: [-4.4314 -0.4978 -0.1499 -0.2137  0.1743]\n",
      "MSE loss: 111.9761\n",
      "Iteration: 165300\n",
      "Gradient: [  26.2314   -8.9475  -95.6288 -171.3263  200.3928]\n",
      "Weights: [-4.4316 -0.4975 -0.1498 -0.2139  0.1742]\n",
      "MSE loss: 111.9654\n",
      "Iteration: 165400\n",
      "Gradient: [ 15.1009   2.1745 -65.8101 348.9629 108.8804]\n",
      "Weights: [-4.4317 -0.4972 -0.1497 -0.2139  0.1742]\n",
      "MSE loss: 111.9584\n",
      "Iteration: 165500\n",
      "Gradient: [   25.1698   -58.4209    32.3018    96.3619 -1164.3115]\n",
      "Weights: [-4.4319 -0.4969 -0.1497 -0.2144  0.1743]\n",
      "MSE loss: 111.9542\n",
      "Iteration: 165600\n",
      "Gradient: [  32.2981   21.8348   67.6185 -195.4306  676.4815]\n",
      "Weights: [-4.4322 -0.4966 -0.1497 -0.2146  0.1746]\n",
      "MSE loss: 111.9083\n",
      "Iteration: 165700\n",
      "Gradient: [  21.7648   24.1687   56.7412 -333.2093  718.6826]\n",
      "Weights: [-4.4323 -0.4963 -0.1497 -0.2149  0.1745]\n",
      "MSE loss: 111.8873\n",
      "Iteration: 165800\n",
      "Gradient: [  33.4072  -84.3148   52.4703 -103.489   184.552 ]\n",
      "Weights: [-4.4325 -0.4959 -0.1497 -0.2149  0.1746]\n",
      "MSE loss: 111.8723\n",
      "Iteration: 165900\n",
      "Gradient: [  7.096  -51.5614  59.6533 262.9212 110.1729]\n",
      "Weights: [-4.4326 -0.4955 -0.1495 -0.2153  0.1747]\n",
      "MSE loss: 111.8477\n",
      "Iteration: 166000\n",
      "Gradient: [ 37.1045 -56.2636  -7.5666   0.3426 -16.9703]\n",
      "Weights: [-4.4328 -0.4952 -0.1495 -0.2154  0.1747]\n",
      "MSE loss: 111.8324\n",
      "Iteration: 166100\n",
      "Gradient: [ -33.3139    2.8137  -16.9299 -401.2155 -895.9493]\n",
      "Weights: [-4.433  -0.4948 -0.1495 -0.2156  0.1748]\n",
      "MSE loss: 111.8131\n",
      "Iteration: 166200\n",
      "Gradient: [   6.6008  -57.9933   42.8947  190.4743 -769.4732]\n",
      "Weights: [-4.4331 -0.4945 -0.1496 -0.2161  0.175 ]\n",
      "MSE loss: 111.7908\n",
      "Iteration: 166300\n",
      "Gradient: [ 20.6879 -31.1531 -85.7939 354.7465  20.7848]\n",
      "Weights: [-4.4333 -0.4942 -0.1495 -0.2162  0.1749]\n",
      "MSE loss: 111.7733\n",
      "Iteration: 166400\n",
      "Gradient: [  -6.5764    2.7236  -37.6185  586.7297 -326.3797]\n",
      "Weights: [-4.4335 -0.4939 -0.1495 -0.2163  0.175 ]\n",
      "MSE loss: 111.7579\n",
      "Iteration: 166500\n",
      "Gradient: [ -1.4909  -1.0334 -90.5792  36.0616 590.8586]\n",
      "Weights: [-4.4337 -0.4936 -0.1493 -0.2166  0.1751]\n",
      "MSE loss: 111.7381\n",
      "Iteration: 166600\n",
      "Gradient: [   3.8724  -70.0049   -7.9146 -280.5989 -184.2211]\n",
      "Weights: [-4.4338 -0.4931 -0.1493 -0.2165  0.175 ]\n",
      "MSE loss: 111.7274\n",
      "Iteration: 166700\n",
      "Gradient: [   40.1381    58.5246  -180.3806   100.0871 -1190.7735]\n",
      "Weights: [-4.434  -0.4927 -0.1492 -0.2166  0.175 ]\n",
      "MSE loss: 111.7046\n",
      "Iteration: 166800\n",
      "Gradient: [ -11.1424  -10.5221   59.6792 -326.2024 1159.1459]\n",
      "Weights: [-4.4342 -0.4925 -0.1492 -0.2172  0.1753]\n",
      "MSE loss: 111.7089\n",
      "Iteration: 166900\n",
      "Gradient: [  31.8825  -44.7689  108.8458  102.9866 -217.1194]\n",
      "Weights: [-4.4344 -0.4921 -0.1494 -0.2176  0.1754]\n",
      "MSE loss: 111.661\n",
      "Iteration: 167000\n",
      "Gradient: [  -8.1892  -42.7119  -36.2491  357.9105 -211.6133]\n",
      "Weights: [-4.4346 -0.4918 -0.1494 -0.2181  0.1754]\n",
      "MSE loss: 111.6581\n",
      "Iteration: 167100\n",
      "Gradient: [  53.8492  -26.3559 -138.7394 -157.9249  231.4753]\n",
      "Weights: [-4.4348 -0.4914 -0.1494 -0.2188  0.1758]\n",
      "MSE loss: 111.6175\n",
      "Iteration: 167200\n",
      "Gradient: [  29.8374  -33.1299  -53.8116   79.9179 -359.0636]\n",
      "Weights: [-4.4349 -0.4911 -0.1494 -0.2187  0.1757]\n",
      "MSE loss: 111.6123\n",
      "Iteration: 167300\n",
      "Gradient: [  3.0708  12.7753  84.4786  30.5509 642.7418]\n",
      "Weights: [-4.4351 -0.4907 -0.1492 -0.2185  0.1757]\n",
      "MSE loss: 111.5882\n",
      "Iteration: 167400\n",
      "Gradient: [  11.1033    2.8655   21.6838 -251.1166 1360.8755]\n",
      "Weights: [-4.4352 -0.4904 -0.1491 -0.2185  0.1757]\n",
      "MSE loss: 111.5859\n",
      "Iteration: 167500\n",
      "Gradient: [   2.6134  -76.0226  -27.7724   39.4914 -225.853 ]\n",
      "Weights: [-4.4354 -0.4901 -0.149  -0.2186  0.1756]\n",
      "MSE loss: 111.5587\n",
      "Iteration: 167600\n",
      "Gradient: [  11.7284 -101.9303 -176.3163  266.7194 -227.3727]\n",
      "Weights: [-4.4356 -0.4898 -0.149  -0.2188  0.1757]\n",
      "MSE loss: 111.5401\n",
      "Iteration: 167700\n",
      "Gradient: [  -3.7556  -45.107   -37.3128  -71.4861 -147.8404]\n",
      "Weights: [-4.4357 -0.4894 -0.1488 -0.2192  0.1758]\n",
      "MSE loss: 111.5205\n",
      "Iteration: 167800\n",
      "Gradient: [ -7.8129 -38.6041  21.1591 -86.5496 111.3292]\n",
      "Weights: [-4.4359 -0.4891 -0.1488 -0.2192  0.1757]\n",
      "MSE loss: 111.5053\n",
      "Iteration: 167900\n",
      "Gradient: [  19.3195  -21.1413  -16.6744  621.2304 1263.1839]\n",
      "Weights: [-4.4361 -0.4888 -0.1487 -0.2191  0.1759]\n",
      "MSE loss: 111.5555\n",
      "Iteration: 168000\n",
      "Gradient: [ 56.7962 -14.5529 -34.7978 164.445  867.7683]\n",
      "Weights: [-4.4362 -0.4885 -0.1487 -0.2189  0.1756]\n",
      "MSE loss: 111.4836\n",
      "Iteration: 168100\n",
      "Gradient: [ -9.942  -68.7436   0.2755 229.7583 205.0151]\n",
      "Weights: [-4.4365 -0.4882 -0.1488 -0.2193  0.1759]\n",
      "MSE loss: 111.4822\n",
      "Iteration: 168200\n",
      "Gradient: [ 53.3615 -16.8572  82.2025 281.949  279.1887]\n",
      "Weights: [-4.4366 -0.4878 -0.1486 -0.2194  0.1758]\n",
      "MSE loss: 111.4473\n",
      "Iteration: 168300\n",
      "Gradient: [  28.2788    2.232     9.8554  -50.1985 -779.8032]\n",
      "Weights: [-4.4367 -0.4875 -0.1487 -0.2195  0.1758]\n",
      "MSE loss: 111.4346\n",
      "Iteration: 168400\n",
      "Gradient: [-5.851000e-01 -4.947050e+01 -1.127715e+02 -6.385550e+01 -7.127900e+02]\n",
      "Weights: [-4.4369 -0.4872 -0.1486 -0.2195  0.1757]\n",
      "MSE loss: 111.4294\n",
      "Iteration: 168500\n",
      "Gradient: [  35.512   -58.8811   32.8826 -204.1271  590.6245]\n",
      "Weights: [-4.437  -0.4869 -0.1486 -0.2201  0.176 ]\n",
      "MSE loss: 111.3999\n",
      "Iteration: 168600\n",
      "Gradient: [ 21.2829  17.2278 -63.427   18.8727  69.8339]\n",
      "Weights: [-4.4372 -0.4865 -0.1485 -0.22    0.176 ]\n",
      "MSE loss: 111.3845\n",
      "Iteration: 168700\n",
      "Gradient: [   34.9448   -31.8094     2.3877  -398.5031 -1132.1442]\n",
      "Weights: [-4.4374 -0.4862 -0.1484 -0.2201  0.1759]\n",
      "MSE loss: 111.3675\n",
      "Iteration: 168800\n",
      "Gradient: [  19.6357   22.9051   55.5727 -190.9134 -163.6439]\n",
      "Weights: [-4.4375 -0.4859 -0.1484 -0.2204  0.176 ]\n",
      "MSE loss: 111.3496\n",
      "Iteration: 168900\n",
      "Gradient: [ 20.3404 -74.8953  21.6887 -85.4929 764.6531]\n",
      "Weights: [-4.4377 -0.4856 -0.1484 -0.2204  0.176 ]\n",
      "MSE loss: 111.3424\n",
      "Iteration: 169000\n",
      "Gradient: [ -18.7131  -96.3373   -5.3217 -304.2192  258.7265]\n",
      "Weights: [-4.4378 -0.4852 -0.1483 -0.2207  0.1761]\n",
      "MSE loss: 111.3147\n",
      "Iteration: 169100\n",
      "Gradient: [ 34.4991 -92.7693  24.4986 -22.4515 332.6505]\n",
      "Weights: [-4.438  -0.4848 -0.1483 -0.2211  0.1763]\n",
      "MSE loss: 111.2937\n",
      "Iteration: 169200\n",
      "Gradient: [   2.2596  -19.4559  -37.1279  102.4137 -545.4596]\n",
      "Weights: [-4.4382 -0.4844 -0.1484 -0.221   0.1762]\n",
      "MSE loss: 111.2819\n",
      "Iteration: 169300\n",
      "Gradient: [-48.0795 -96.7463  66.483  -33.9304 598.2323]\n",
      "Weights: [-4.4383 -0.4842 -0.1482 -0.2212  0.1762]\n",
      "MSE loss: 111.2666\n",
      "Iteration: 169400\n",
      "Gradient: [  40.797    25.55    -66.5321 -364.4223   12.7715]\n",
      "Weights: [-4.4385 -0.4838 -0.1481 -0.2213  0.1762]\n",
      "MSE loss: 111.2501\n",
      "Iteration: 169500\n",
      "Gradient: [ 27.4904 -29.9    182.8989  54.9519 642.9888]\n",
      "Weights: [-4.4387 -0.4835 -0.1481 -0.221   0.1762]\n",
      "MSE loss: 111.2561\n",
      "Iteration: 169600\n",
      "Gradient: [  55.4824   31.9103 -102.5224  126.0727 1627.3419]\n",
      "Weights: [-4.4389 -0.4832 -0.1483 -0.2219  0.1766]\n",
      "MSE loss: 111.2465\n",
      "Iteration: 169700\n",
      "Gradient: [  30.1454  -60.1089   25.876   -27.7414 -678.8916]\n",
      "Weights: [-4.4391 -0.4828 -0.1482 -0.2222  0.1766]\n",
      "MSE loss: 111.1943\n",
      "Iteration: 169800\n",
      "Gradient: [   7.2958  -45.8598    9.2927 -290.9174 -421.5361]\n",
      "Weights: [-4.4393 -0.4825 -0.1483 -0.2228  0.1767]\n",
      "MSE loss: 111.1708\n",
      "Iteration: 169900\n",
      "Gradient: [  22.1557  -63.7524   10.6674  264.0407 -332.8344]\n",
      "Weights: [-4.4394 -0.4821 -0.1483 -0.2232  0.1769]\n",
      "MSE loss: 111.1549\n",
      "Iteration: 170000\n",
      "Gradient: [ 20.1255  29.4615  59.7855 -68.2012 467.3092]\n",
      "Weights: [-4.4396 -0.4817 -0.1481 -0.2233  0.177 ]\n",
      "MSE loss: 111.1553\n",
      "Iteration: 170100\n",
      "Gradient: [  29.2692  -15.5679  -90.7771  128.76   -726.0337]\n",
      "Weights: [-4.4397 -0.4814 -0.1481 -0.223   0.1768]\n",
      "MSE loss: 111.1236\n",
      "Iteration: 170200\n",
      "Gradient: [  45.8594  -50.1412   91.4244   57.0719 -739.4128]\n",
      "Weights: [-4.4399 -0.481  -0.1476 -0.223   0.1767]\n",
      "MSE loss: 111.1068\n",
      "Iteration: 170300\n",
      "Gradient: [ -18.2074  -72.608  -118.9567  265.5957 -118.1892]\n",
      "Weights: [-4.4401 -0.4807 -0.1476 -0.2232  0.1767]\n",
      "MSE loss: 111.093\n",
      "Iteration: 170400\n",
      "Gradient: [ 44.581  -44.831    8.3086 224.6154 777.0879]\n",
      "Weights: [-4.4403 -0.4803 -0.1475 -0.2232  0.1767]\n",
      "MSE loss: 111.0748\n",
      "Iteration: 170500\n",
      "Gradient: [ 60.3038 -18.5817  65.8513 -18.3993   1.6769]\n",
      "Weights: [-4.4404 -0.48   -0.1474 -0.2233  0.1767]\n",
      "MSE loss: 111.0566\n",
      "Iteration: 170600\n",
      "Gradient: [  53.5049  -80.3756 -114.3258   74.2393  422.4234]\n",
      "Weights: [-4.4406 -0.4796 -0.1474 -0.2234  0.1767]\n",
      "MSE loss: 111.0431\n",
      "Iteration: 170700\n",
      "Gradient: [  37.9684   -9.3359  141.7951  -72.5729 -536.2512]\n",
      "Weights: [-4.4408 -0.4794 -0.1475 -0.2238  0.1769]\n",
      "MSE loss: 111.0231\n",
      "Iteration: 170800\n",
      "Gradient: [   9.4719  -19.4872    3.6093 -141.4589 -956.2555]\n",
      "Weights: [-4.441  -0.4791 -0.1474 -0.2239  0.177 ]\n",
      "MSE loss: 111.0134\n",
      "Iteration: 170900\n",
      "Gradient: [  16.6242   -9.9652  -92.7009  -69.599  -532.8077]\n",
      "Weights: [-4.4412 -0.4788 -0.1474 -0.2238  0.1768]\n",
      "MSE loss: 111.002\n",
      "Iteration: 171000\n",
      "Gradient: [  14.9588  -53.3099  143.6779 -124.606    15.6075]\n",
      "Weights: [-4.4413 -0.4785 -0.1473 -0.2236  0.1767]\n",
      "MSE loss: 110.9944\n",
      "Iteration: 171100\n",
      "Gradient: [  -3.9073  -43.9514  -59.8805 -339.7877  -71.5602]\n",
      "Weights: [-4.4415 -0.4781 -0.1473 -0.2237  0.1767]\n",
      "MSE loss: 110.9758\n",
      "Iteration: 171200\n",
      "Gradient: [  -4.318   -22.1424   96.275    22.4758 1009.3011]\n",
      "Weights: [-4.4417 -0.4779 -0.1473 -0.2237  0.1768]\n",
      "MSE loss: 110.9722\n",
      "Iteration: 171300\n",
      "Gradient: [  65.3171   37.295   162.3402 -166.1169  682.751 ]\n",
      "Weights: [-4.4419 -0.4776 -0.1473 -0.2237  0.1769]\n",
      "MSE loss: 110.9893\n",
      "Iteration: 171400\n",
      "Gradient: [  13.0369  -55.9702  151.4997 -135.5653 1058.1161]\n",
      "Weights: [-4.4421 -0.4774 -0.1474 -0.2239  0.177 ]\n",
      "MSE loss: 110.9675\n",
      "Iteration: 171500\n",
      "Gradient: [  21.0372   20.6052 -151.7106  272.7388 -288.9671]\n",
      "Weights: [-4.4423 -0.4771 -0.1472 -0.2241  0.1769]\n",
      "MSE loss: 110.9207\n",
      "Iteration: 171600\n",
      "Gradient: [ -14.2363  -47.8325  191.317  -257.6084  -39.9973]\n",
      "Weights: [-4.4426 -0.4768 -0.1472 -0.2245  0.177 ]\n",
      "MSE loss: 110.9006\n",
      "Iteration: 171700\n",
      "Gradient: [  -22.5936    19.5673    21.5129   223.1732 -1441.8685]\n",
      "Weights: [-4.4427 -0.4765 -0.1472 -0.2245  0.1769]\n",
      "MSE loss: 110.8898\n",
      "Iteration: 171800\n",
      "Gradient: [  73.0221 -103.7954  -13.679   -10.1549  554.871 ]\n",
      "Weights: [-4.443  -0.4762 -0.1471 -0.2248  0.1771]\n",
      "MSE loss: 110.8699\n",
      "Iteration: 171900\n",
      "Gradient: [  10.6565  -74.5142   41.7351 -118.6025   71.1024]\n",
      "Weights: [-4.4432 -0.4759 -0.147  -0.2249  0.1771]\n",
      "MSE loss: 110.8542\n",
      "Iteration: 172000\n",
      "Gradient: [ 23.6089  18.4401 -22.5742 -70.782  365.8508]\n",
      "Weights: [-4.4434 -0.4757 -0.147  -0.2251  0.1773]\n",
      "MSE loss: 110.8783\n",
      "Iteration: 172100\n",
      "Gradient: [   52.7255   -22.3776    52.8283  -281.4551 -1264.73  ]\n",
      "Weights: [-4.4436 -0.4755 -0.1469 -0.2254  0.1772]\n",
      "MSE loss: 110.8344\n",
      "Iteration: 172200\n",
      "Gradient: [    4.6909   -84.5931   -74.9174   -65.7826 -1401.1665]\n",
      "Weights: [-4.4437 -0.4752 -0.147  -0.226   0.1774]\n",
      "MSE loss: 110.8136\n",
      "Iteration: 172300\n",
      "Gradient: [   24.6723   -43.3278     9.965      9.1128 -1248.2061]\n",
      "Weights: [-4.4439 -0.4749 -0.147  -0.2264  0.1775]\n",
      "MSE loss: 110.8046\n",
      "Iteration: 172400\n",
      "Gradient: [   7.6331  -78.6849 -103.6319   31.9527   48.4948]\n",
      "Weights: [-4.444  -0.4746 -0.147  -0.2266  0.1776]\n",
      "MSE loss: 110.767\n",
      "Iteration: 172500\n",
      "Gradient: [  45.9535  -40.8421 -149.4813   86.5064 -100.0213]\n",
      "Weights: [-4.4442 -0.4743 -0.1469 -0.2267  0.1777]\n",
      "MSE loss: 110.7528\n",
      "Iteration: 172600\n",
      "Gradient: [  20.0208  -29.9238  -81.3463 -111.4321  -92.6595]\n",
      "Weights: [-4.4443 -0.4739 -0.1466 -0.227   0.1777]\n",
      "MSE loss: 110.7337\n",
      "Iteration: 172700\n",
      "Gradient: [ 47.8096 -79.7805  31.1024 283.46   187.1495]\n",
      "Weights: [-4.4445 -0.4736 -0.1465 -0.2271  0.1777]\n",
      "MSE loss: 110.7184\n",
      "Iteration: 172800\n",
      "Gradient: [ 20.3358 -17.0728  47.7594 -44.8158 619.6368]\n",
      "Weights: [-4.4447 -0.4733 -0.1465 -0.2277  0.178 ]\n",
      "MSE loss: 110.7015\n",
      "Iteration: 172900\n",
      "Gradient: [  30.0055 -101.8122   55.6702   79.9949 -440.3518]\n",
      "Weights: [-4.4448 -0.473  -0.1465 -0.2277  0.178 ]\n",
      "MSE loss: 110.6865\n",
      "Iteration: 173000\n",
      "Gradient: [ 26.3284  -0.9043 -63.0859 107.2718 875.6738]\n",
      "Weights: [-4.445  -0.4727 -0.1466 -0.2281  0.1781]\n",
      "MSE loss: 110.6711\n",
      "Iteration: 173100\n",
      "Gradient: [   21.6626   -13.6542    40.7751   -36.7712 -1115.0313]\n",
      "Weights: [-4.4452 -0.4723 -0.1464 -0.2279  0.1779]\n",
      "MSE loss: 110.6549\n",
      "Iteration: 173200\n",
      "Gradient: [  38.9878  -33.0485 -124.7605   -1.9754 -463.004 ]\n",
      "Weights: [-4.4453 -0.4719 -0.1463 -0.2278  0.1779]\n",
      "MSE loss: 110.6408\n",
      "Iteration: 173300\n",
      "Gradient: [  16.8851   -7.8177  158.0714 -109.7766  542.4053]\n",
      "Weights: [-4.4455 -0.4717 -0.1463 -0.2281  0.178 ]\n",
      "MSE loss: 110.6243\n",
      "Iteration: 173400\n",
      "Gradient: [ 8.6158000e+00 -8.9900000e-02  1.0033710e+02 -3.5083250e+02\n",
      " -1.0711635e+03]\n",
      "Weights: [-4.4457 -0.4713 -0.1462 -0.2275  0.1777]\n",
      "MSE loss: 110.6232\n",
      "Iteration: 173500\n",
      "Gradient: [  21.6689  -40.271    44.9671  113.7093 -538.0905]\n",
      "Weights: [-4.4458 -0.471  -0.1462 -0.2278  0.1778]\n",
      "MSE loss: 110.6125\n",
      "Iteration: 173600\n",
      "Gradient: [   1.3862  -39.1899   25.7112  151.3582 -744.8392]\n",
      "Weights: [-4.446  -0.4708 -0.1461 -0.228   0.1779]\n",
      "MSE loss: 110.591\n",
      "Iteration: 173700\n",
      "Gradient: [ -13.5449 -105.0059  -58.7463 -454.724  -434.8856]\n",
      "Weights: [-4.4462 -0.4705 -0.1461 -0.2281  0.1778]\n",
      "MSE loss: 110.6081\n",
      "Iteration: 173800\n",
      "Gradient: [ 38.0374 -69.1687 -76.9162 -27.8753 465.5215]\n",
      "Weights: [-4.4463 -0.4702 -0.1461 -0.228   0.1778]\n",
      "MSE loss: 110.5777\n",
      "Iteration: 173900\n",
      "Gradient: [  39.8742   26.2302  -36.7673 -189.1104 -264.2789]\n",
      "Weights: [-4.4466 -0.4699 -0.1462 -0.2285  0.178 ]\n",
      "MSE loss: 110.5452\n",
      "Iteration: 174000\n",
      "Gradient: [  26.3822   -4.5605   37.7374  -99.5217 -507.2491]\n",
      "Weights: [-4.4468 -0.4696 -0.1463 -0.2286  0.1781]\n",
      "MSE loss: 110.5291\n",
      "Iteration: 174100\n",
      "Gradient: [  50.3036  -24.6192  -35.2698   68.8168 -144.1343]\n",
      "Weights: [-4.447  -0.4693 -0.1463 -0.2283  0.178 ]\n",
      "MSE loss: 110.5241\n",
      "Iteration: 174200\n",
      "Gradient: [  -8.3355  -53.5185  -84.9649 -188.6006 -436.3126]\n",
      "Weights: [-4.4471 -0.469  -0.1462 -0.2287  0.178 ]\n",
      "MSE loss: 110.5275\n",
      "Iteration: 174300\n",
      "Gradient: [  29.4802  -14.5694   37.1913   63.0024 -170.9788]\n",
      "Weights: [-4.4473 -0.4687 -0.1464 -0.2287  0.1782]\n",
      "MSE loss: 110.4967\n",
      "Iteration: 174400\n",
      "Gradient: [  34.0037   -6.8264   12.4342  -57.328  -555.16  ]\n",
      "Weights: [-4.4474 -0.4684 -0.1464 -0.2292  0.1782]\n",
      "MSE loss: 110.4875\n",
      "Iteration: 174500\n",
      "Gradient: [ -11.687   -36.7921  -40.8055  183.2526 -237.9664]\n",
      "Weights: [-4.4475 -0.468  -0.1464 -0.2298  0.1785]\n",
      "MSE loss: 110.4504\n",
      "Iteration: 174600\n",
      "Gradient: [  16.8458  -64.1883 -144.3761   94.4492  566.7951]\n",
      "Weights: [-4.4477 -0.4677 -0.1465 -0.2298  0.1784]\n",
      "MSE loss: 110.4509\n",
      "Iteration: 174700\n",
      "Gradient: [ -29.7816 -111.5211 -113.4478 -231.3474 -490.5951]\n",
      "Weights: [-4.4478 -0.4674 -0.1464 -0.2299  0.1784]\n",
      "MSE loss: 110.4426\n",
      "Iteration: 174800\n",
      "Gradient: [  -0.2119  -68.1509  -35.4239  -43.472  -124.2184]\n",
      "Weights: [-4.448  -0.4671 -0.1464 -0.23    0.1787]\n",
      "MSE loss: 110.4394\n",
      "Iteration: 174900\n",
      "Gradient: [-10.6508 -38.3228 -41.6118 117.9163  -8.313 ]\n",
      "Weights: [-4.4482 -0.4669 -0.1463 -0.2302  0.1785]\n",
      "MSE loss: 110.4048\n",
      "Iteration: 175000\n",
      "Gradient: [ 39.7168 -64.2887  34.3609 -10.7069  23.639 ]\n",
      "Weights: [-4.4483 -0.4665 -0.1464 -0.2306  0.1787]\n",
      "MSE loss: 110.3821\n",
      "Iteration: 175100\n",
      "Gradient: [  -3.8515  -53.4327    1.0945  153.8456 -357.7157]\n",
      "Weights: [-4.4485 -0.4662 -0.1462 -0.2305  0.1788]\n",
      "MSE loss: 110.3868\n",
      "Iteration: 175200\n",
      "Gradient: [  -5.8862  -52.5153  -78.5414   45.2078 -710.1603]\n",
      "Weights: [-4.4487 -0.4658 -0.146  -0.2307  0.1787]\n",
      "MSE loss: 110.3459\n",
      "Iteration: 175300\n",
      "Gradient: [ 92.9018 -34.8012 -69.334   71.336  479.8615]\n",
      "Weights: [-4.4488 -0.4655 -0.1459 -0.2307  0.1786]\n",
      "MSE loss: 110.3346\n",
      "Iteration: 175400\n",
      "Gradient: [  47.0118    4.1749  -71.2211 -106.4481 -501.4014]\n",
      "Weights: [-4.4489 -0.4652 -0.1459 -0.2306  0.1786]\n",
      "MSE loss: 110.3246\n",
      "Iteration: 175500\n",
      "Gradient: [   5.9957   -4.9797  187.1154 -142.4865 -694.879 ]\n",
      "Weights: [-4.4491 -0.4649 -0.146  -0.231   0.1787]\n",
      "MSE loss: 110.3072\n",
      "Iteration: 175600\n",
      "Gradient: [   -1.5553   -70.8024  -114.0339  -160.2005 -1031.9499]\n",
      "Weights: [-4.4493 -0.4647 -0.1459 -0.2311  0.1786]\n",
      "MSE loss: 110.3552\n",
      "Iteration: 175700\n",
      "Gradient: [-15.8704  11.6649 -63.2073 120.3132  15.1237]\n",
      "Weights: [-4.4494 -0.4643 -0.1461 -0.2316  0.1789]\n",
      "MSE loss: 110.2748\n",
      "Iteration: 175800\n",
      "Gradient: [  16.5785  -85.3416 -169.8503 -135.8057 -111.174 ]\n",
      "Weights: [-4.4496 -0.464  -0.1459 -0.2314  0.1788]\n",
      "MSE loss: 110.2759\n",
      "Iteration: 175900\n",
      "Gradient: [  -9.2891   -7.7213  -58.2835 -181.3281 -217.8174]\n",
      "Weights: [-4.4497 -0.4636 -0.1459 -0.2313  0.1788]\n",
      "MSE loss: 110.2511\n",
      "Iteration: 176000\n",
      "Gradient: [-12.7416 -55.254   94.9941  94.4014 956.587 ]\n",
      "Weights: [-4.4499 -0.4633 -0.1461 -0.2317  0.179 ]\n",
      "MSE loss: 110.2424\n",
      "Iteration: 176100\n",
      "Gradient: [   1.2441  -30.9177    2.892    43.4829 1190.2335]\n",
      "Weights: [-4.45   -0.463  -0.1459 -0.2318  0.179 ]\n",
      "MSE loss: 110.2316\n",
      "Iteration: 176200\n",
      "Gradient: [   0.3012  -75.542    64.0791 -204.7672 -118.9165]\n",
      "Weights: [-4.4502 -0.4627 -0.1458 -0.2321  0.1789]\n",
      "MSE loss: 110.243\n",
      "Iteration: 176300\n",
      "Gradient: [ -3.7719 -67.2413  -6.945  230.4573 345.9749]\n",
      "Weights: [-4.4504 -0.4624 -0.1459 -0.2326  0.1793]\n",
      "MSE loss: 110.192\n",
      "Iteration: 176400\n",
      "Gradient: [-12.9498 -29.6251  42.8033  54.8138 -10.7399]\n",
      "Weights: [-4.4505 -0.4621 -0.146  -0.2329  0.1792]\n",
      "MSE loss: 110.1808\n",
      "Iteration: 176500\n",
      "Gradient: [ 35.5466 -85.8523  58.1589 215.0043 471.4705]\n",
      "Weights: [-4.4506 -0.4617 -0.1459 -0.2328  0.1792]\n",
      "MSE loss: 110.1588\n",
      "Iteration: 176600\n",
      "Gradient: [  23.0207 -106.2509   -2.5386  -19.5337 -194.3349]\n",
      "Weights: [-4.4508 -0.4614 -0.1459 -0.2331  0.1793]\n",
      "MSE loss: 110.1593\n",
      "Iteration: 176700\n",
      "Gradient: [ -12.3236   11.4386   20.974  -366.2824  448.6273]\n",
      "Weights: [-4.4509 -0.4611 -0.1459 -0.2332  0.1794]\n",
      "MSE loss: 110.1254\n",
      "Iteration: 176800\n",
      "Gradient: [   21.0818   -38.1442     7.9009  -170.2887 -1561.3484]\n",
      "Weights: [-4.4511 -0.4608 -0.1457 -0.2331  0.1791]\n",
      "MSE loss: 110.1672\n",
      "Iteration: 176900\n",
      "Gradient: [   4.9413  -46.7371  -36.4379 -124.4027  107.142 ]\n",
      "Weights: [-4.4513 -0.4606 -0.146  -0.2331  0.1793]\n",
      "MSE loss: 110.1098\n",
      "Iteration: 177000\n",
      "Gradient: [ 27.7358 -50.9028  56.2364 118.8324 855.3312]\n",
      "Weights: [-4.4514 -0.4602 -0.1458 -0.2333  0.1794]\n",
      "MSE loss: 110.0886\n",
      "Iteration: 177100\n",
      "Gradient: [  33.5671  -37.4836   73.4941 -449.8267  996.0252]\n",
      "Weights: [-4.4516 -0.4598 -0.1456 -0.2331  0.1792]\n",
      "MSE loss: 110.078\n",
      "Iteration: 177200\n",
      "Gradient: [  23.2224  -53.2408  -30.3192 -192.4221 -108.2691]\n",
      "Weights: [-4.4518 -0.4596 -0.1454 -0.2327  0.1791]\n",
      "MSE loss: 110.0774\n",
      "Iteration: 177300\n",
      "Gradient: [  -6.6398  -51.5123  127.7676 -198.7774   35.8202]\n",
      "Weights: [-4.452  -0.4593 -0.1455 -0.233   0.1792]\n",
      "MSE loss: 110.0576\n",
      "Iteration: 177400\n",
      "Gradient: [ 49.2126  -5.8469  18.5536 282.4045 726.6931]\n",
      "Weights: [-4.4521 -0.4591 -0.1454 -0.2328  0.1791]\n",
      "MSE loss: 110.0536\n",
      "Iteration: 177500\n",
      "Gradient: [  -9.2426   52.3544  103.3937 -101.7659  412.9395]\n",
      "Weights: [-4.4523 -0.4588 -0.1454 -0.2332  0.1791]\n",
      "MSE loss: 110.0486\n",
      "Iteration: 177600\n",
      "Gradient: [  -7.1591  -46.967   -30.4555  -88.1156 1941.3563]\n",
      "Weights: [-4.4525 -0.4585 -0.1453 -0.2332  0.1792]\n",
      "MSE loss: 110.0321\n",
      "Iteration: 177700\n",
      "Gradient: [ 28.3715  14.897   45.6237 159.6092 967.5657]\n",
      "Weights: [-4.4527 -0.4583 -0.1453 -0.2334  0.1794]\n",
      "MSE loss: 110.0587\n",
      "Iteration: 177800\n",
      "Gradient: [ -23.8571  -39.8341  -69.0368 -205.3843 -568.2211]\n",
      "Weights: [-4.4528 -0.458  -0.1453 -0.2337  0.1793]\n",
      "MSE loss: 110.0014\n",
      "Iteration: 177900\n",
      "Gradient: [ 22.1091 -18.225   31.199  411.0091 337.3468]\n",
      "Weights: [-4.453  -0.4577 -0.1452 -0.2338  0.1794]\n",
      "MSE loss: 109.9808\n",
      "Iteration: 178000\n",
      "Gradient: [  11.456   -51.4048  -63.0788 -603.4545 -748.5555]\n",
      "Weights: [-4.4532 -0.4574 -0.1453 -0.2342  0.1795]\n",
      "MSE loss: 109.9617\n",
      "Iteration: 178100\n",
      "Gradient: [  24.6958  -28.6946 -181.9348   -9.8559  840.418 ]\n",
      "Weights: [-4.4534 -0.4571 -0.1454 -0.2336  0.1792]\n",
      "MSE loss: 109.9809\n",
      "Iteration: 178200\n",
      "Gradient: [  -5.3098  -63.1644  -15.6674   60.6587 -343.669 ]\n",
      "Weights: [-4.4535 -0.457  -0.1454 -0.234   0.1794]\n",
      "MSE loss: 109.9501\n",
      "Iteration: 178300\n",
      "Gradient: [  32.5886   -5.4983   39.3891   36.1135 -867.4762]\n",
      "Weights: [-4.4537 -0.4567 -0.1454 -0.2338  0.1793]\n",
      "MSE loss: 109.9512\n",
      "Iteration: 178400\n",
      "Gradient: [  19.8947   20.3063  -70.8912 -132.453  -832.3737]\n",
      "Weights: [-4.4539 -0.4564 -0.1454 -0.234   0.1793]\n",
      "MSE loss: 109.9473\n",
      "Iteration: 178500\n",
      "Gradient: [   36.1038   -37.0474    27.7618    84.8147 -1010.7782]\n",
      "Weights: [-4.4541 -0.4562 -0.1453 -0.234   0.1794]\n",
      "MSE loss: 109.9172\n",
      "Iteration: 178600\n",
      "Gradient: [  39.9138   20.3721   83.2646 -314.0951 -204.1122]\n",
      "Weights: [-4.4543 -0.456  -0.1454 -0.2345  0.1796]\n",
      "MSE loss: 109.897\n",
      "Iteration: 178700\n",
      "Gradient: [ 25.3361 -76.3962  57.225   55.1417 279.7544]\n",
      "Weights: [-4.4545 -0.4557 -0.1455 -0.2351  0.1799]\n",
      "MSE loss: 109.8871\n",
      "Iteration: 178800\n",
      "Gradient: [ 57.9145 -98.5348  44.7362 -59.9028 860.0305]\n",
      "Weights: [-4.4547 -0.4554 -0.1455 -0.2355  0.1799]\n",
      "MSE loss: 109.8593\n",
      "Iteration: 178900\n",
      "Gradient: [  52.4621    1.8663   45.6211   78.0826 -200.657 ]\n",
      "Weights: [-4.4548 -0.4551 -0.1455 -0.2358  0.1801]\n",
      "MSE loss: 109.8419\n",
      "Iteration: 179000\n",
      "Gradient: [  11.3226   -8.4801 -108.2102  -98.787   479.0098]\n",
      "Weights: [-4.455  -0.4548 -0.1456 -0.2358  0.1801]\n",
      "MSE loss: 109.835\n",
      "Iteration: 179100\n",
      "Gradient: [  4.6145  20.8777   5.0548 187.9648 618.4273]\n",
      "Weights: [-4.4551 -0.4545 -0.1456 -0.2361  0.1803]\n",
      "MSE loss: 109.8579\n",
      "Iteration: 179200\n",
      "Gradient: [  28.512   -89.4542   23.4534 -242.8344 -582.9894]\n",
      "Weights: [-4.4552 -0.4542 -0.1457 -0.2365  0.1803]\n",
      "MSE loss: 109.7983\n",
      "Iteration: 179300\n",
      "Gradient: [  9.7098 -28.7312 125.3303 234.0247 408.0651]\n",
      "Weights: [-4.4554 -0.4538 -0.1456 -0.2364  0.1803]\n",
      "MSE loss: 109.7976\n",
      "Iteration: 179400\n",
      "Gradient: [  38.0475 -112.4395  -99.028  -309.7656  273.1731]\n",
      "Weights: [-4.4555 -0.4535 -0.1456 -0.2364  0.1802]\n",
      "MSE loss: 109.7787\n",
      "Iteration: 179500\n",
      "Gradient: [ 16.0561  -6.5759  27.3914 505.9087 805.7692]\n",
      "Weights: [-4.4557 -0.4532 -0.1456 -0.2369  0.1804]\n",
      "MSE loss: 109.7572\n",
      "Iteration: 179600\n",
      "Gradient: [  12.6145   30.787   -36.0637  194.5488 -203.2027]\n",
      "Weights: [-4.4558 -0.453  -0.1455 -0.2373  0.1804]\n",
      "MSE loss: 109.7513\n",
      "Iteration: 179700\n",
      "Gradient: [ -14.0923  -36.9884    5.2227  171.6401 1085.7041]\n",
      "Weights: [-4.456  -0.4527 -0.1453 -0.2373  0.1805]\n",
      "MSE loss: 109.728\n",
      "Iteration: 179800\n",
      "Gradient: [-16.6242 -86.4683 117.2503  -8.7255  25.1229]\n",
      "Weights: [-4.4561 -0.4523 -0.1451 -0.2377  0.1805]\n",
      "MSE loss: 109.7208\n",
      "Iteration: 179900\n",
      "Gradient: [  48.32     13.3599   66.8479   15.9319 -187.0695]\n",
      "Weights: [-4.4562 -0.452  -0.1449 -0.238   0.1806]\n",
      "MSE loss: 109.6956\n",
      "Iteration: 180000\n",
      "Gradient: [   2.2248  -56.0198   89.4279 -538.5342   57.8597]\n",
      "Weights: [-4.4564 -0.4517 -0.145  -0.2384  0.1809]\n",
      "MSE loss: 109.694\n",
      "Iteration: 180100\n",
      "Gradient: [  22.1559   -8.1878 -165.4554   43.6876 -323.8838]\n",
      "Weights: [-4.4565 -0.4513 -0.1449 -0.2384  0.1808]\n",
      "MSE loss: 109.6627\n",
      "Iteration: 180200\n",
      "Gradient: [  21.8611  -10.7886   42.25   -294.7396  427.7762]\n",
      "Weights: [-4.4566 -0.451  -0.1449 -0.2387  0.1809]\n",
      "MSE loss: 109.6467\n",
      "Iteration: 180300\n",
      "Gradient: [  27.3565  -59.2655  -33.0792   43.3097 -132.321 ]\n",
      "Weights: [-4.4568 -0.4507 -0.1449 -0.2388  0.1809]\n",
      "MSE loss: 109.6332\n",
      "Iteration: 180400\n",
      "Gradient: [  -4.1197  -49.0028 -189.45   -187.9789 -297.5681]\n",
      "Weights: [-4.4569 -0.4503 -0.1449 -0.2394  0.1811]\n",
      "MSE loss: 109.621\n",
      "Iteration: 180500\n",
      "Gradient: [   8.5301  -54.9684  -92.4363 -206.6873  402.4626]\n",
      "Weights: [-4.457  -0.45   -0.1446 -0.2397  0.1811]\n",
      "MSE loss: 109.601\n",
      "Iteration: 180600\n",
      "Gradient: [ 17.0622 -72.9872   7.165  -13.1211  75.1478]\n",
      "Weights: [-4.4571 -0.4497 -0.1444 -0.2396  0.181 ]\n",
      "MSE loss: 109.6004\n",
      "Iteration: 180700\n",
      "Gradient: [  11.2331  -97.8174 -132.7749 -205.751     7.0966]\n",
      "Weights: [-4.4573 -0.4493 -0.1444 -0.2395  0.181 ]\n",
      "MSE loss: 109.5765\n",
      "Iteration: 180800\n",
      "Gradient: [ -28.803   -19.4755  -91.5993   36.6457 -349.0498]\n",
      "Weights: [-4.4574 -0.449  -0.1442 -0.2397  0.1811]\n",
      "MSE loss: 109.5562\n",
      "Iteration: 180900\n",
      "Gradient: [  -1.021   -16.5563  -21.2601 -195.4475  657.7911]\n",
      "Weights: [-4.4575 -0.4486 -0.1442 -0.2399  0.1812]\n",
      "MSE loss: 109.5612\n",
      "Iteration: 181000\n",
      "Gradient: [ 23.9736 -52.1587  63.1013 277.0186 399.3078]\n",
      "Weights: [-4.4576 -0.4483 -0.1442 -0.2403  0.1813]\n",
      "MSE loss: 109.5365\n",
      "Iteration: 181100\n",
      "Gradient: [   56.4791   -60.1899   -38.4561   133.5787 -1881.2262]\n",
      "Weights: [-4.4578 -0.4481 -0.1443 -0.2405  0.1812]\n",
      "MSE loss: 109.5254\n",
      "Iteration: 181200\n",
      "Gradient: [  19.6189  -18.1261  -47.2253  -94.2843 -378.6672]\n",
      "Weights: [-4.458  -0.4478 -0.1444 -0.2409  0.1814]\n",
      "MSE loss: 109.4991\n",
      "Iteration: 181300\n",
      "Gradient: [ 37.5836 -36.3542 -61.4456 285.3105 417.6601]\n",
      "Weights: [-4.4581 -0.4475 -0.1443 -0.2412  0.1816]\n",
      "MSE loss: 109.4954\n",
      "Iteration: 181400\n",
      "Gradient: [  -0.4706  -64.4354   10.6842  248.9055 -432.1638]\n",
      "Weights: [-4.4582 -0.4471 -0.1444 -0.2414  0.1816]\n",
      "MSE loss: 109.4707\n",
      "Iteration: 181500\n",
      "Gradient: [  -36.4591  -118.3449   -10.9199   -59.2156 -1032.5305]\n",
      "Weights: [-4.4583 -0.4468 -0.1443 -0.2412  0.1814]\n",
      "MSE loss: 109.4811\n",
      "Iteration: 181600\n",
      "Gradient: [ -7.5414 -50.0716 -88.474  -56.2512  86.7104]\n",
      "Weights: [-4.4584 -0.4464 -0.1443 -0.2411  0.1814]\n",
      "MSE loss: 109.4584\n",
      "Iteration: 181700\n",
      "Gradient: [   9.6962    6.4556  -79.9152  -12.7446 -191.8366]\n",
      "Weights: [-4.4586 -0.446  -0.1441 -0.2411  0.1814]\n",
      "MSE loss: 109.4379\n",
      "Iteration: 181800\n",
      "Gradient: [  24.2303   15.5463   75.4126  -26.5732 1033.3869]\n",
      "Weights: [-4.4587 -0.4457 -0.144  -0.2407  0.1813]\n",
      "MSE loss: 109.4243\n",
      "Iteration: 181900\n",
      "Gradient: [  55.6592   -7.625   125.4137  -61.2795 -658.4037]\n",
      "Weights: [-4.4589 -0.4454 -0.1439 -0.2407  0.1812]\n",
      "MSE loss: 109.4147\n",
      "Iteration: 182000\n",
      "Gradient: [ 23.8332  53.0472  21.0788 -91.74     9.7259]\n",
      "Weights: [-4.459  -0.4451 -0.144  -0.2408  0.1813]\n",
      "MSE loss: 109.4031\n",
      "Iteration: 182100\n",
      "Gradient: [  22.7233  -31.7104 -112.3467   66.8518  460.2003]\n",
      "Weights: [-4.4592 -0.4449 -0.1439 -0.2408  0.1812]\n",
      "MSE loss: 109.3935\n",
      "Iteration: 182200\n",
      "Gradient: [  28.6307 -106.6117  -69.4601 -220.6913  222.6955]\n",
      "Weights: [-4.4593 -0.4446 -0.1438 -0.2408  0.1812]\n",
      "MSE loss: 109.3853\n",
      "Iteration: 182300\n",
      "Gradient: [ 21.7918  -7.4826  20.1546  41.9346 227.5071]\n",
      "Weights: [-4.4595 -0.4443 -0.1438 -0.2411  0.1813]\n",
      "MSE loss: 109.3661\n",
      "Iteration: 182400\n",
      "Gradient: [  16.8971  -33.7299  167.0786  -58.1964 -215.7932]\n",
      "Weights: [-4.4597 -0.4441 -0.1438 -0.2407  0.1811]\n",
      "MSE loss: 109.3666\n",
      "Iteration: 182500\n",
      "Gradient: [  27.3216  -50.2101   44.1807 -405.2893 -193.1646]\n",
      "Weights: [-4.4599 -0.4438 -0.1438 -0.2409  0.1812]\n",
      "MSE loss: 109.3507\n",
      "Iteration: 182600\n",
      "Gradient: [   5.233    24.9271  204.2425  372.8107 1071.9194]\n",
      "Weights: [-4.46   -0.4436 -0.1438 -0.2411  0.1813]\n",
      "MSE loss: 109.3425\n",
      "Iteration: 182700\n",
      "Gradient: [  18.6876  -48.6938  -41.0794 -374.2522 -526.8799]\n",
      "Weights: [-4.4602 -0.4434 -0.1439 -0.2414  0.1814]\n",
      "MSE loss: 109.3239\n",
      "Iteration: 182800\n",
      "Gradient: [ 2.579620e+01 -4.366000e-01  8.323780e+01  1.661440e+02  6.275965e+02]\n",
      "Weights: [-4.4604 -0.4431 -0.1441 -0.2421  0.1815]\n",
      "MSE loss: 109.3431\n",
      "Iteration: 182900\n",
      "Gradient: [ -21.3965  -56.7209 -102.0911   34.8108  425.7575]\n",
      "Weights: [-4.4605 -0.4428 -0.1439 -0.2417  0.1815]\n",
      "MSE loss: 109.2992\n",
      "Iteration: 183000\n",
      "Gradient: [  18.5449  -88.1887  -66.7687 -125.9964 -329.3836]\n",
      "Weights: [-4.4606 -0.4425 -0.1438 -0.2416  0.1814]\n",
      "MSE loss: 109.2922\n",
      "Iteration: 183100\n",
      "Gradient: [ -14.2877  -83.0325 -206.991   233.1113 1364.3298]\n",
      "Weights: [-4.4608 -0.4422 -0.1439 -0.2417  0.1814]\n",
      "MSE loss: 109.2784\n",
      "Iteration: 183200\n",
      "Gradient: [  43.3308   33.9097   41.3319  287.4108 -522.6028]\n",
      "Weights: [-4.461  -0.442  -0.1439 -0.242   0.1815]\n",
      "MSE loss: 109.2625\n",
      "Iteration: 183300\n",
      "Gradient: [  -4.1735  -38.3094  133.889  -102.5018  897.164 ]\n",
      "Weights: [-4.4611 -0.4417 -0.1439 -0.2419  0.1816]\n",
      "MSE loss: 109.2583\n",
      "Iteration: 183400\n",
      "Gradient: [  20.3259  -85.9525 -204.2696  -38.3813  422.1069]\n",
      "Weights: [-4.4613 -0.4414 -0.1438 -0.242   0.1814]\n",
      "MSE loss: 109.2497\n",
      "Iteration: 183500\n",
      "Gradient: [  28.1063   -4.2242  -89.5543 -468.3323 1342.5979]\n",
      "Weights: [-4.4615 -0.4412 -0.1438 -0.2424  0.1817]\n",
      "MSE loss: 109.2327\n",
      "Iteration: 183600\n",
      "Gradient: [  2.4952  12.5435 148.8182 203.6252  57.2132]\n",
      "Weights: [-4.4616 -0.4409 -0.1438 -0.2425  0.1817]\n",
      "MSE loss: 109.2212\n",
      "Iteration: 183700\n",
      "Gradient: [ 21.9058 -39.596  -52.6219 171.4901 168.5607]\n",
      "Weights: [-4.4618 -0.4407 -0.144  -0.2426  0.1817]\n",
      "MSE loss: 109.2041\n",
      "Iteration: 183800\n",
      "Gradient: [  27.1807   10.8762  120.7696  -79.7956 -244.5942]\n",
      "Weights: [-4.4619 -0.4404 -0.1439 -0.2427  0.1817]\n",
      "MSE loss: 109.1924\n",
      "Iteration: 183900\n",
      "Gradient: [   9.166  -100.7466 -143.2385 -141.6051 -776.0293]\n",
      "Weights: [-4.4621 -0.4401 -0.1438 -0.2427  0.1816]\n",
      "MSE loss: 109.2237\n",
      "Iteration: 184000\n",
      "Gradient: [  20.4064   -3.0015   71.7526  536.5248 -398.6606]\n",
      "Weights: [-4.4623 -0.4399 -0.1438 -0.2429  0.1819]\n",
      "MSE loss: 109.1761\n",
      "Iteration: 184100\n",
      "Gradient: [  8.3852 -38.2512 -99.0464 274.3817 903.9974]\n",
      "Weights: [-4.4624 -0.4397 -0.1439 -0.2433  0.182 ]\n",
      "MSE loss: 109.1541\n",
      "Iteration: 184200\n",
      "Gradient: [ -23.8521    5.0683 -206.3154 -290.5121 -249.2675]\n",
      "Weights: [-4.4626 -0.4393 -0.144  -0.2432  0.1818]\n",
      "MSE loss: 109.1493\n",
      "Iteration: 184300\n",
      "Gradient: [  17.6224 -164.355   -63.3802 -191.4805 -726.4492]\n",
      "Weights: [-4.4627 -0.4391 -0.1441 -0.2435  0.1819]\n",
      "MSE loss: 109.1609\n",
      "Iteration: 184400\n",
      "Gradient: [ 19.3484   2.5511 -41.2763 101.4058 123.4728]\n",
      "Weights: [-4.4629 -0.4388 -0.1439 -0.2436  0.1821]\n",
      "MSE loss: 109.1224\n",
      "Iteration: 184500\n",
      "Gradient: [  10.8637  -29.9739  105.3767  105.0014 -343.2095]\n",
      "Weights: [-4.463  -0.4385 -0.1439 -0.2438  0.1821]\n",
      "MSE loss: 109.1123\n",
      "Iteration: 184600\n",
      "Gradient: [ -0.546  -44.0544  -4.363  -80.59   325.4881]\n",
      "Weights: [-4.4632 -0.4382 -0.1437 -0.244   0.1822]\n",
      "MSE loss: 109.0925\n",
      "Iteration: 184700\n",
      "Gradient: [  28.3081  -27.5926   18.9055 -175.6097 1093.2171]\n",
      "Weights: [-4.4633 -0.4379 -0.1437 -0.2442  0.1822]\n",
      "MSE loss: 109.0772\n",
      "Iteration: 184800\n",
      "Gradient: [ -4.5637  14.0091 -20.0592 141.0132 323.8163]\n",
      "Weights: [-4.4634 -0.4376 -0.1437 -0.2441  0.1822]\n",
      "MSE loss: 109.073\n",
      "Iteration: 184900\n",
      "Gradient: [   19.6334    -4.8307  -104.2386   -91.0323 -1108.2428]\n",
      "Weights: [-4.4636 -0.4373 -0.1438 -0.2443  0.1821]\n",
      "MSE loss: 109.0642\n",
      "Iteration: 185000\n",
      "Gradient: [   9.0758 -110.6095   33.8235 -223.543   221.5764]\n",
      "Weights: [-4.4638 -0.437  -0.1438 -0.2443  0.1822]\n",
      "MSE loss: 109.0451\n",
      "Iteration: 185100\n",
      "Gradient: [  25.7537  -55.1017  -52.3327 -240.2627  -54.0464]\n",
      "Weights: [-4.4639 -0.4368 -0.1437 -0.2446  0.1823]\n",
      "MSE loss: 109.0365\n",
      "Iteration: 185200\n",
      "Gradient: [  43.2178   58.8437  -39.0821 -128.8413 -816.1836]\n",
      "Weights: [-4.4641 -0.4365 -0.1438 -0.2445  0.1821]\n",
      "MSE loss: 109.0419\n",
      "Iteration: 185300\n",
      "Gradient: [ 2.395700e+01  2.468000e+00  3.340720e+01  1.177000e-01 -1.543047e+02]\n",
      "Weights: [-4.4642 -0.4363 -0.1437 -0.2445  0.1823]\n",
      "MSE loss: 109.0146\n",
      "Iteration: 185400\n",
      "Gradient: [  24.4644  -45.5661  -88.363  -128.4689 -248.0692]\n",
      "Weights: [-4.4644 -0.436  -0.1437 -0.2446  0.1822]\n",
      "MSE loss: 109.0007\n",
      "Iteration: 185500\n",
      "Gradient: [ 30.0915 -19.0138  13.657  -86.1602 458.1902]\n",
      "Weights: [-4.4646 -0.4357 -0.1435 -0.2449  0.1823]\n",
      "MSE loss: 108.9885\n",
      "Iteration: 185600\n",
      "Gradient: [ 36.7256 -20.1884  50.6012 500.1066 842.1104]\n",
      "Weights: [-4.4647 -0.4355 -0.1434 -0.2451  0.1824]\n",
      "MSE loss: 108.9813\n",
      "Iteration: 185700\n",
      "Gradient: [  21.1305   -7.5568   72.0832  120.5303 -332.5916]\n",
      "Weights: [-4.4648 -0.4352 -0.1434 -0.2454  0.1824]\n",
      "MSE loss: 108.9591\n",
      "Iteration: 185800\n",
      "Gradient: [ 17.6488   3.8122 105.0181 -17.1156 244.1777]\n",
      "Weights: [-4.465  -0.435  -0.1434 -0.2456  0.1826]\n",
      "MSE loss: 108.9529\n",
      "Iteration: 185900\n",
      "Gradient: [ -12.969   -32.2744  -49.1176  -61.2726 -531.0659]\n",
      "Weights: [-4.4652 -0.4347 -0.1434 -0.2459  0.1826]\n",
      "MSE loss: 108.9375\n",
      "Iteration: 186000\n",
      "Gradient: [  22.988   -46.1885 -112.7181  -52.3455 -416.8596]\n",
      "Weights: [-4.4653 -0.4344 -0.1433 -0.2459  0.1825]\n",
      "MSE loss: 108.9371\n",
      "Iteration: 186100\n",
      "Gradient: [  -1.0627    8.7178  -98.6063   91.0369 -832.1244]\n",
      "Weights: [-4.4654 -0.4341 -0.1434 -0.2464  0.1828]\n",
      "MSE loss: 108.9065\n",
      "Iteration: 186200\n",
      "Gradient: [  -25.5974    -8.3599  -166.8762  -214.5009 -1195.9951]\n",
      "Weights: [-4.4656 -0.4337 -0.1434 -0.2463  0.1827]\n",
      "MSE loss: 108.8955\n",
      "Iteration: 186300\n",
      "Gradient: [   6.5015  -56.8419  -11.6681 -210.3066 -683.0098]\n",
      "Weights: [-4.4657 -0.4335 -0.1433 -0.2466  0.1827]\n",
      "MSE loss: 108.8889\n",
      "Iteration: 186400\n",
      "Gradient: [  -8.6614  -23.7291  -96.0622  -35.6096 -134.1416]\n",
      "Weights: [-4.4658 -0.4332 -0.1432 -0.2466  0.1828]\n",
      "MSE loss: 108.8706\n",
      "Iteration: 186500\n",
      "Gradient: [ 24.5058 -16.8037  58.744   53.3184 370.9213]\n",
      "Weights: [-4.466  -0.433  -0.1432 -0.2465  0.1829]\n",
      "MSE loss: 108.8823\n",
      "Iteration: 186600\n",
      "Gradient: [  24.7881  -37.5472    7.777   -43.0401 -143.0221]\n",
      "Weights: [-4.4661 -0.4327 -0.143  -0.2465  0.1828]\n",
      "MSE loss: 108.8671\n",
      "Iteration: 186700\n",
      "Gradient: [ 40.1984  13.071  -60.9648 119.3295 736.2231]\n",
      "Weights: [-4.4663 -0.4323 -0.1428 -0.2467  0.1827]\n",
      "MSE loss: 108.8449\n",
      "Iteration: 186800\n",
      "Gradient: [ 51.231   18.7658 -27.6533 229.1484  83.8296]\n",
      "Weights: [-4.4664 -0.432  -0.1428 -0.2467  0.1827]\n",
      "MSE loss: 108.8289\n",
      "Iteration: 186900\n",
      "Gradient: [  14.2845  -30.9978  -30.3494   -6.5564 -195.4767]\n",
      "Weights: [-4.4666 -0.4317 -0.1428 -0.247   0.1827]\n",
      "MSE loss: 108.8309\n",
      "Iteration: 187000\n",
      "Gradient: [  35.6945   -8.9306   38.8023 -194.4516   10.7538]\n",
      "Weights: [-4.4668 -0.4315 -0.1429 -0.247   0.1829]\n",
      "MSE loss: 108.8033\n",
      "Iteration: 187100\n",
      "Gradient: [ -2.3752 -25.5837  19.3808 -23.4987  -0.3995]\n",
      "Weights: [-4.4669 -0.4313 -0.1428 -0.247   0.1828]\n",
      "MSE loss: 108.7978\n",
      "Iteration: 187200\n",
      "Gradient: [   14.6883    -4.3365   141.2776  -185.3056 -1043.4898]\n",
      "Weights: [-4.4671 -0.4311 -0.1428 -0.2475  0.1829]\n",
      "MSE loss: 108.8014\n",
      "Iteration: 187300\n",
      "Gradient: [ 29.8575 -47.3287 -88.7419 237.4414 321.4702]\n",
      "Weights: [-4.4672 -0.4307 -0.1426 -0.2473  0.1829]\n",
      "MSE loss: 108.7746\n",
      "Iteration: 187400\n",
      "Gradient: [ 33.6767 -17.9492  32.6693 362.3892 196.5463]\n",
      "Weights: [-4.4674 -0.4305 -0.1427 -0.2474  0.1829]\n",
      "MSE loss: 108.766\n",
      "Iteration: 187500\n",
      "Gradient: [  10.8399  -60.3605 -175.8824 -176.9841 1142.6136]\n",
      "Weights: [-4.4676 -0.4303 -0.1427 -0.2475  0.183 ]\n",
      "MSE loss: 108.7656\n",
      "Iteration: 187600\n",
      "Gradient: [  -2.8202  -47.4685  -44.1298 -280.8995 -741.1326]\n",
      "Weights: [-4.4677 -0.43   -0.1426 -0.2476  0.1829]\n",
      "MSE loss: 108.7467\n",
      "Iteration: 187700\n",
      "Gradient: [  27.4855  -63.7221  -23.9069   89.7912 -283.147 ]\n",
      "Weights: [-4.4679 -0.4297 -0.1426 -0.2472  0.1828]\n",
      "MSE loss: 108.7376\n",
      "Iteration: 187800\n",
      "Gradient: [  27.5483  -28.2436  -19.9295 -129.465  -826.7562]\n",
      "Weights: [-4.4681 -0.4295 -0.1426 -0.2474  0.1828]\n",
      "MSE loss: 108.7332\n",
      "Iteration: 187900\n",
      "Gradient: [ -10.9268  -51.1807  -44.2122  217.3432 1078.8624]\n",
      "Weights: [-4.4682 -0.4292 -0.1426 -0.2476  0.183 ]\n",
      "MSE loss: 108.7343\n",
      "Iteration: 188000\n",
      "Gradient: [ 19.8333 -62.3929  48.3263  72.7319  80.3893]\n",
      "Weights: [-4.4684 -0.429  -0.1426 -0.2474  0.1829]\n",
      "MSE loss: 108.7298\n",
      "Iteration: 188100\n",
      "Gradient: [  31.4923  -28.5093 -140.0932 -412.3767  636.4326]\n",
      "Weights: [-4.4686 -0.4287 -0.1425 -0.2478  0.183 ]\n",
      "MSE loss: 108.6909\n",
      "Iteration: 188200\n",
      "Gradient: [  52.1704  -14.7855 -176.9836   -0.534  -439.2905]\n",
      "Weights: [-4.4687 -0.4285 -0.1425 -0.2475  0.1828]\n",
      "MSE loss: 108.6929\n",
      "Iteration: 188300\n",
      "Gradient: [  19.2955  -37.4172 -107.4895  216.1348   81.0852]\n",
      "Weights: [-4.4689 -0.4283 -0.1425 -0.2477  0.1829]\n",
      "MSE loss: 108.6779\n",
      "Iteration: 188400\n",
      "Gradient: [   8.4819   12.2785   66.0379  -25.149  -327.9314]\n",
      "Weights: [-4.4691 -0.428  -0.1426 -0.248   0.183 ]\n",
      "MSE loss: 108.6667\n",
      "Iteration: 188500\n",
      "Gradient: [  3.1712   4.5999 -30.3505 179.5171 299.8304]\n",
      "Weights: [-4.4693 -0.4278 -0.1429 -0.2484  0.1832]\n",
      "MSE loss: 108.6456\n",
      "Iteration: 188600\n",
      "Gradient: [  1.9662  -2.4962  96.6536 126.1548  36.6758]\n",
      "Weights: [-4.4694 -0.4275 -0.1429 -0.2485  0.1832]\n",
      "MSE loss: 108.6402\n",
      "Iteration: 188700\n",
      "Gradient: [  7.6952 -45.8962 128.4257 212.6697 620.3132]\n",
      "Weights: [-4.4696 -0.4271 -0.1428 -0.2486  0.1834]\n",
      "MSE loss: 108.6462\n",
      "Iteration: 188800\n",
      "Gradient: [ 13.2429  33.2316  12.9106 -53.9516 932.1257]\n",
      "Weights: [-4.4698 -0.4268 -0.1429 -0.2485  0.1833]\n",
      "MSE loss: 108.6478\n",
      "Iteration: 188900\n",
      "Gradient: [  16.2233   55.849    95.4768  172.689  1076.6124]\n",
      "Weights: [-4.4699 -0.4265 -0.1427 -0.2481  0.1832]\n",
      "MSE loss: 108.6557\n",
      "Iteration: 189000\n",
      "Gradient: [ 34.8959 -72.85     2.9361 220.2104 630.7697]\n",
      "Weights: [-4.4701 -0.4263 -0.1427 -0.2485  0.1831]\n",
      "MSE loss: 108.5926\n",
      "Iteration: 189100\n",
      "Gradient: [   2.031    -9.6191 -184.2264  -16.2843  406.3613]\n",
      "Weights: [-4.4703 -0.426  -0.1427 -0.2486  0.1831]\n",
      "MSE loss: 108.5833\n",
      "Iteration: 189200\n",
      "Gradient: [ 16.4102  27.3549 -54.6781 -78.5834 520.7875]\n",
      "Weights: [-4.4705 -0.4257 -0.1428 -0.2487  0.1832]\n",
      "MSE loss: 108.5701\n",
      "Iteration: 189300\n",
      "Gradient: [ 14.6994  16.9363 -75.5434  -0.1125  33.1582]\n",
      "Weights: [-4.4706 -0.4255 -0.1429 -0.2493  0.1836]\n",
      "MSE loss: 108.5714\n",
      "Iteration: 189400\n",
      "Gradient: [ 32.234  -56.4586 -17.6334 146.4264 509.5945]\n",
      "Weights: [-4.4708 -0.4252 -0.1427 -0.2495  0.1836]\n",
      "MSE loss: 108.5549\n",
      "Iteration: 189500\n",
      "Gradient: [  -2.1664  -10.8996  -11.9849 -168.6456 -370.2865]\n",
      "Weights: [-4.4709 -0.425  -0.1428 -0.2499  0.1836]\n",
      "MSE loss: 108.5212\n",
      "Iteration: 189600\n",
      "Gradient: [  -5.5438  -14.2354 -100.0375  323.8037   50.0987]\n",
      "Weights: [-4.471  -0.4247 -0.1428 -0.2501  0.1837]\n",
      "MSE loss: 108.5083\n",
      "Iteration: 189700\n",
      "Gradient: [ 30.0547  24.4921 110.6169 133.7372 578.3878]\n",
      "Weights: [-4.4711 -0.4244 -0.1427 -0.2503  0.1838]\n",
      "MSE loss: 108.4981\n",
      "Iteration: 189800\n",
      "Gradient: [ -24.8606  -44.8928  -24.7722  331.7808 -744.5518]\n",
      "Weights: [-4.4713 -0.4242 -0.1428 -0.2509  0.184 ]\n",
      "MSE loss: 108.4825\n",
      "Iteration: 189900\n",
      "Gradient: [  26.4001  -49.5521  -19.9478 -196.6951   48.7284]\n",
      "Weights: [-4.4714 -0.4239 -0.1429 -0.2513  0.1841]\n",
      "MSE loss: 108.4705\n",
      "Iteration: 190000\n",
      "Gradient: [  -9.8364   17.5505   45.7929   88.8987 -113.5645]\n",
      "Weights: [-4.4715 -0.4236 -0.1428 -0.2509  0.184 ]\n",
      "MSE loss: 108.4704\n",
      "Iteration: 190100\n",
      "Gradient: [  6.0684  10.5556 -52.1562 443.0224 170.3859]\n",
      "Weights: [-4.4717 -0.4233 -0.1427 -0.2512  0.1841]\n",
      "MSE loss: 108.4625\n",
      "Iteration: 190200\n",
      "Gradient: [  -0.7768  -17.2185 -101.8026 -253.5454 -142.7881]\n",
      "Weights: [-4.4718 -0.423  -0.1426 -0.2515  0.1842]\n",
      "MSE loss: 108.4374\n",
      "Iteration: 190300\n",
      "Gradient: [ 10.4351  18.7377  61.3559  57.0462 836.5068]\n",
      "Weights: [-4.4719 -0.4228 -0.1426 -0.2515  0.1843]\n",
      "MSE loss: 108.448\n",
      "Iteration: 190400\n",
      "Gradient: [    9.2373   -90.9227   -54.2287   -28.7729 -1126.2825]\n",
      "Weights: [-4.4721 -0.4226 -0.1426 -0.2518  0.1841]\n",
      "MSE loss: 108.4354\n",
      "Iteration: 190500\n",
      "Gradient: [  26.2818   -9.8366  -70.3622   38.6449 1109.0935]\n",
      "Weights: [-4.4722 -0.4223 -0.1425 -0.252   0.1843]\n",
      "MSE loss: 108.4011\n",
      "Iteration: 190600\n",
      "Gradient: [  4.4484 -70.931    6.4707 259.0158  84.6346]\n",
      "Weights: [-4.4723 -0.4219 -0.1425 -0.2522  0.1843]\n",
      "MSE loss: 108.3873\n",
      "Iteration: 190700\n",
      "Gradient: [ 16.4456 -59.5393 -93.1909 151.8104 -60.4483]\n",
      "Weights: [-4.4724 -0.4217 -0.1424 -0.2528  0.1844]\n",
      "MSE loss: 108.3984\n",
      "Iteration: 190800\n",
      "Gradient: [  -7.7154  -90.4148 -156.836   -82.2306 -282.5366]\n",
      "Weights: [-4.4725 -0.4213 -0.1423 -0.2526  0.1844]\n",
      "MSE loss: 108.3645\n",
      "Iteration: 190900\n",
      "Gradient: [ -42.4323  -19.0486   37.9653   46.5028 -324.0881]\n",
      "Weights: [-4.4726 -0.421  -0.1424 -0.2528  0.1844]\n",
      "MSE loss: 108.3499\n",
      "Iteration: 191000\n",
      "Gradient: [ 41.8432   8.4736  -5.229   80.1326 180.8491]\n",
      "Weights: [-4.4727 -0.4208 -0.1423 -0.2531  0.1846]\n",
      "MSE loss: 108.3397\n",
      "Iteration: 191100\n",
      "Gradient: [  21.7001  -15.6836   42.3874 -251.6733 -505.1386]\n",
      "Weights: [-4.4729 -0.4205 -0.1421 -0.2532  0.1845]\n",
      "MSE loss: 108.3267\n",
      "Iteration: 191200\n",
      "Gradient: [  36.0835   22.5021   88.11   -481.1138  280.8775]\n",
      "Weights: [-4.473  -0.4202 -0.1419 -0.2532  0.1845]\n",
      "MSE loss: 108.3177\n",
      "Iteration: 191300\n",
      "Gradient: [ 27.4717 -44.539   71.5675 405.3206 477.7585]\n",
      "Weights: [-4.4731 -0.4199 -0.1421 -0.2533  0.1847]\n",
      "MSE loss: 108.3393\n",
      "Iteration: 191400\n",
      "Gradient: [  29.0294    4.1774  205.8712 -212.9789  242.459 ]\n",
      "Weights: [-4.4732 -0.4196 -0.142  -0.2535  0.1847]\n",
      "MSE loss: 108.3187\n",
      "Iteration: 191500\n",
      "Gradient: [ 14.2598 -25.1545  -1.55   299.2974 182.887 ]\n",
      "Weights: [-4.4733 -0.4193 -0.1421 -0.2536  0.1846]\n",
      "MSE loss: 108.2837\n",
      "Iteration: 191600\n",
      "Gradient: [ -12.9395  -39.9548 -215.8927  382.212   101.0241]\n",
      "Weights: [-4.4735 -0.419  -0.1418 -0.2535  0.1845]\n",
      "MSE loss: 108.2754\n",
      "Iteration: 191700\n",
      "Gradient: [  14.0615  -29.2932  -90.3879 -453.3651 -574.5698]\n",
      "Weights: [-4.4736 -0.4188 -0.1418 -0.2536  0.1845]\n",
      "MSE loss: 108.2661\n",
      "Iteration: 191800\n",
      "Gradient: [   -6.0529   -95.2678   -11.6101   258.7957 -1501.7638]\n",
      "Weights: [-4.4738 -0.4185 -0.1418 -0.2537  0.1846]\n",
      "MSE loss: 108.2503\n",
      "Iteration: 191900\n",
      "Gradient: [ -22.6469  -49.987   -33.6255  384.0646 -518.8415]\n",
      "Weights: [-4.4739 -0.4183 -0.1417 -0.2541  0.1846]\n",
      "MSE loss: 108.2597\n",
      "Iteration: 192000\n",
      "Gradient: [ 1.132000e-01 -1.188980e+01 -7.573420e+01 -7.745640e+01 -1.654572e+02]\n",
      "Weights: [-4.4741 -0.418  -0.1417 -0.254   0.1847]\n",
      "MSE loss: 108.2303\n",
      "Iteration: 192100\n",
      "Gradient: [   51.1048     1.7545    79.0432   248.8561 -1208.4401]\n",
      "Weights: [-4.4742 -0.4178 -0.1416 -0.2538  0.1846]\n",
      "MSE loss: 108.2253\n",
      "Iteration: 192200\n",
      "Gradient: [  17.9838  -82.2513 -137.9533 -138.0038 -882.2946]\n",
      "Weights: [-4.4743 -0.4175 -0.1415 -0.254   0.1846]\n",
      "MSE loss: 108.2302\n",
      "Iteration: 192300\n",
      "Gradient: [ 2.81864e+01  3.42300e-01 -1.39400e+02 -4.00949e+02  9.12772e+01]\n",
      "Weights: [-4.4745 -0.4172 -0.1415 -0.2545  0.1848]\n",
      "MSE loss: 108.1976\n",
      "Iteration: 192400\n",
      "Gradient: [  21.5954  -34.8634  158.6925 -225.471   204.4952]\n",
      "Weights: [-4.4746 -0.4169 -0.1414 -0.2544  0.1848]\n",
      "MSE loss: 108.1872\n",
      "Iteration: 192500\n",
      "Gradient: [  43.293   -16.7727   76.7658 -345.5581  337.2575]\n",
      "Weights: [-4.4747 -0.4166 -0.1413 -0.2544  0.1846]\n",
      "MSE loss: 108.1958\n",
      "Iteration: 192600\n",
      "Gradient: [  37.5919  -49.8914  -49.9937  285.0811 -271.1835]\n",
      "Weights: [-4.4749 -0.4164 -0.1413 -0.2547  0.1849]\n",
      "MSE loss: 108.1636\n",
      "Iteration: 192700\n",
      "Gradient: [   3.1857   -4.8187   42.1434  229.5722 -301.655 ]\n",
      "Weights: [-4.475  -0.4162 -0.1414 -0.2553  0.1851]\n",
      "MSE loss: 108.1492\n",
      "Iteration: 192800\n",
      "Gradient: [ -14.6387   13.9465  -40.4635  276.8404 -614.9229]\n",
      "Weights: [-4.4752 -0.4159 -0.1413 -0.2552  0.185 ]\n",
      "MSE loss: 108.1415\n",
      "Iteration: 192900\n",
      "Gradient: [   12.7428   -36.326    137.9462   -11.2964 -1740.8428]\n",
      "Weights: [-4.4753 -0.4157 -0.1413 -0.2551  0.185 ]\n",
      "MSE loss: 108.1394\n",
      "Iteration: 193000\n",
      "Gradient: [  46.8349  -30.1188   99.3579 -163.4879 -426.4405]\n",
      "Weights: [-4.4754 -0.4154 -0.1413 -0.2555  0.1851]\n",
      "MSE loss: 108.1178\n",
      "Iteration: 193100\n",
      "Gradient: [ 30.5736  -5.2837  34.4378 282.0717 497.6231]\n",
      "Weights: [-4.4756 -0.4151 -0.1414 -0.2554  0.1851]\n",
      "MSE loss: 108.1126\n",
      "Iteration: 193200\n",
      "Gradient: [ 43.1457 -75.5056 -43.6095  16.8029 329.5328]\n",
      "Weights: [-4.4757 -0.4149 -0.1414 -0.2555  0.1851]\n",
      "MSE loss: 108.1003\n",
      "Iteration: 193300\n",
      "Gradient: [  22.121   -22.1795   62.1276 -145.8632   -2.1922]\n",
      "Weights: [-4.4759 -0.4147 -0.1416 -0.256   0.1853]\n",
      "MSE loss: 108.089\n",
      "Iteration: 193400\n",
      "Gradient: [   -5.0774   -28.7078  -180.9358    76.7793 -1039.8249]\n",
      "Weights: [-4.476  -0.4144 -0.1416 -0.2562  0.1853]\n",
      "MSE loss: 108.081\n",
      "Iteration: 193500\n",
      "Gradient: [-17.0961 -36.8997 -36.0415 359.9258 240.7347]\n",
      "Weights: [-4.4761 -0.4141 -0.1416 -0.256   0.1853]\n",
      "MSE loss: 108.0701\n",
      "Iteration: 193600\n",
      "Gradient: [  48.0194  -32.6943  -46.7337 -209.0467  276.9084]\n",
      "Weights: [-4.4763 -0.4139 -0.1417 -0.256   0.1851]\n",
      "MSE loss: 108.09\n",
      "Iteration: 193700\n",
      "Gradient: [   -8.7963   -66.5624   -44.4188  -498.5825 -1094.5865]\n",
      "Weights: [-4.4764 -0.4136 -0.1417 -0.2562  0.1852]\n",
      "MSE loss: 108.0709\n",
      "Iteration: 193800\n",
      "Gradient: [   8.5282   -9.1616  -88.9212 -140.074   130.6807]\n",
      "Weights: [-4.4765 -0.4134 -0.1416 -0.2561  0.1853]\n",
      "MSE loss: 108.0447\n",
      "Iteration: 193900\n",
      "Gradient: [  -7.6923  -46.3275    6.6235 -388.033  -568.2069]\n",
      "Weights: [-4.4767 -0.4131 -0.1417 -0.2561  0.1851]\n",
      "MSE loss: 108.0532\n",
      "Iteration: 194000\n",
      "Gradient: [  39.9298  -14.983  -102.0777  274.8295   -9.3335]\n",
      "Weights: [-4.4768 -0.4129 -0.1417 -0.2562  0.1853]\n",
      "MSE loss: 108.025\n",
      "Iteration: 194100\n",
      "Gradient: [  28.1325  -63.7392   -8.2974  121.6071 -193.5588]\n",
      "Weights: [-4.477  -0.4126 -0.1415 -0.2559  0.1852]\n",
      "MSE loss: 108.024\n",
      "Iteration: 194200\n",
      "Gradient: [  15.6199  -51.4183  104.0118 -106.807   535.6732]\n",
      "Weights: [-4.4771 -0.4124 -0.1415 -0.2564  0.1853]\n",
      "MSE loss: 108.0063\n",
      "Iteration: 194300\n",
      "Gradient: [   49.709     16.4986  -113.5305   -86.4404 -1362.6475]\n",
      "Weights: [-4.4772 -0.4122 -0.1414 -0.2567  0.1853]\n",
      "MSE loss: 108.0053\n",
      "Iteration: 194400\n",
      "Gradient: [  19.3606    0.9981 -198.6917 -153.9137  746.4224]\n",
      "Weights: [-4.4774 -0.4119 -0.1414 -0.2567  0.1854]\n",
      "MSE loss: 107.9829\n",
      "Iteration: 194500\n",
      "Gradient: [  18.1184  -64.7926    5.4442  202.1665 -962.1995]\n",
      "Weights: [-4.4775 -0.4116 -0.1413 -0.2564  0.1853]\n",
      "MSE loss: 107.9804\n",
      "Iteration: 194600\n",
      "Gradient: [-32.9671 -80.9699  53.4781  59.9243 859.555 ]\n",
      "Weights: [-4.4776 -0.4114 -0.1413 -0.2565  0.1853]\n",
      "MSE loss: 107.9702\n",
      "Iteration: 194700\n",
      "Gradient: [ 35.4139 -30.0904 -33.8964  87.2016 275.8421]\n",
      "Weights: [-4.4778 -0.4111 -0.1414 -0.2564  0.1852]\n",
      "MSE loss: 107.9652\n",
      "Iteration: 194800\n",
      "Gradient: [ -19.0596  -39.0309  -54.2685  209.0525 -390.0386]\n",
      "Weights: [-4.478  -0.4108 -0.1414 -0.2565  0.1852]\n",
      "MSE loss: 107.962\n",
      "Iteration: 194900\n",
      "Gradient: [ -22.7194   -2.769    13.5151  374.4592 -243.8911]\n",
      "Weights: [-4.4781 -0.4105 -0.1416 -0.2566  0.1853]\n",
      "MSE loss: 107.9479\n",
      "Iteration: 195000\n",
      "Gradient: [   5.717     9.5607 -167.269   422.5918   40.6207]\n",
      "Weights: [-4.4782 -0.4103 -0.1416 -0.2567  0.1854]\n",
      "MSE loss: 107.9355\n",
      "Iteration: 195100\n",
      "Gradient: [ 26.4248  -6.2142  33.7472 -13.5804  -0.6212]\n",
      "Weights: [-4.4784 -0.41   -0.1415 -0.2565  0.1853]\n",
      "MSE loss: 107.938\n",
      "Iteration: 195200\n",
      "Gradient: [ -22.367   -56.4024  -73.245  -154.1645 -856.1903]\n",
      "Weights: [-4.4785 -0.4098 -0.1415 -0.257   0.1852]\n",
      "MSE loss: 107.9626\n",
      "Iteration: 195300\n",
      "Gradient: [  27.2653  -28.5529  -40.5069   -4.0637 -384.5915]\n",
      "Weights: [-4.4787 -0.4096 -0.1415 -0.2569  0.1854]\n",
      "MSE loss: 107.9071\n",
      "Iteration: 195400\n",
      "Gradient: [-31.0171   6.6933  75.2544  65.1231 465.151 ]\n",
      "Weights: [-4.4788 -0.4094 -0.1415 -0.2571  0.1855]\n",
      "MSE loss: 107.8935\n",
      "Iteration: 195500\n",
      "Gradient: [  23.0478  -50.5358 -130.1581  213.3796  607.5529]\n",
      "Weights: [-4.479  -0.4091 -0.1415 -0.2576  0.1858]\n",
      "MSE loss: 107.9053\n",
      "Iteration: 195600\n",
      "Gradient: [ 45.8157 -40.8725 -40.8673  12.727  -14.7013]\n",
      "Weights: [-4.4791 -0.4088 -0.1415 -0.258   0.1857]\n",
      "MSE loss: 107.874\n",
      "Iteration: 195700\n",
      "Gradient: [  -5.3141  -37.582   -71.0115 -291.7328 -801.1686]\n",
      "Weights: [-4.4792 -0.4085 -0.1416 -0.258   0.1856]\n",
      "MSE loss: 107.877\n",
      "Iteration: 195800\n",
      "Gradient: [  -3.2016  -96.2904 -113.5198  -81.6147  438.4015]\n",
      "Weights: [-4.4794 -0.4082 -0.1415 -0.2578  0.1856]\n",
      "MSE loss: 107.8641\n",
      "Iteration: 195900\n",
      "Gradient: [ -18.2685  -77.4652  -51.9884 -295.0989  715.7331]\n",
      "Weights: [-4.4795 -0.4079 -0.1415 -0.258   0.1857]\n",
      "MSE loss: 107.8348\n",
      "Iteration: 196000\n",
      "Gradient: [ 13.0509 -52.4904 -61.6063  50.9178 607.9904]\n",
      "Weights: [-4.4797 -0.4077 -0.1415 -0.2578  0.1856]\n",
      "MSE loss: 107.8306\n",
      "Iteration: 196100\n",
      "Gradient: [   0.7645   29.3546   21.2859  -73.3293 -121.326 ]\n",
      "Weights: [-4.4798 -0.4074 -0.1415 -0.258   0.1858]\n",
      "MSE loss: 107.8263\n",
      "Iteration: 196200\n",
      "Gradient: [   8.754   -77.0393  -73.737    72.1562 -610.4271]\n",
      "Weights: [-4.4799 -0.4072 -0.1414 -0.2579  0.1856]\n",
      "MSE loss: 107.8173\n",
      "Iteration: 196300\n",
      "Gradient: [ 37.051  -19.6602  76.6545  30.4221 188.3152]\n",
      "Weights: [-4.4801 -0.407  -0.1414 -0.2578  0.1856]\n",
      "MSE loss: 107.8072\n",
      "Iteration: 196400\n",
      "Gradient: [ 62.128  -15.9937 169.455  275.466  693.4248]\n",
      "Weights: [-4.4802 -0.4067 -0.1412 -0.2575  0.1855]\n",
      "MSE loss: 107.8153\n",
      "Iteration: 196500\n",
      "Gradient: [ 64.2925   9.6338  58.4611 101.6657 -64.3141]\n",
      "Weights: [-4.4804 -0.4065 -0.1414 -0.2578  0.1856]\n",
      "MSE loss: 107.7921\n",
      "Iteration: 196600\n",
      "Gradient: [ -26.1875   -1.0634  -99.4438   66.169  -184.3951]\n",
      "Weights: [-4.4805 -0.4062 -0.1415 -0.2578  0.1855]\n",
      "MSE loss: 107.7975\n",
      "Iteration: 196700\n",
      "Gradient: [ 2.33690e+00 -7.17489e+01 -6.37000e-02 -7.83239e+01  5.88548e+01]\n",
      "Weights: [-4.4807 -0.4059 -0.1416 -0.2583  0.1857]\n",
      "MSE loss: 107.7693\n",
      "Iteration: 196800\n",
      "Gradient: [ -16.5433  -20.9699 -117.8262   41.1061 -168.0303]\n",
      "Weights: [-4.4808 -0.4057 -0.1416 -0.2587  0.1858]\n",
      "MSE loss: 107.7601\n",
      "Iteration: 196900\n",
      "Gradient: [ 41.7707 -24.4734 -25.1959 195.5242 -45.7809]\n",
      "Weights: [-4.4809 -0.4054 -0.1416 -0.2589  0.186 ]\n",
      "MSE loss: 107.7386\n",
      "Iteration: 197000\n",
      "Gradient: [  4.6815 -35.2932  72.4842 -75.0841 271.1702]\n",
      "Weights: [-4.4811 -0.4052 -0.1416 -0.2592  0.1861]\n",
      "MSE loss: 107.7294\n",
      "Iteration: 197100\n",
      "Gradient: [  22.1757    1.9806  -56.5081  206.7844 -440.9781]\n",
      "Weights: [-4.4812 -0.4049 -0.1416 -0.2588  0.186 ]\n",
      "MSE loss: 107.7351\n",
      "Iteration: 197200\n",
      "Gradient: [  37.4708  -22.5281  -41.9545  -98.3976 -628.1995]\n",
      "Weights: [-4.4814 -0.4047 -0.1415 -0.2588  0.1859]\n",
      "MSE loss: 107.7191\n",
      "Iteration: 197300\n",
      "Gradient: [  19.0404   34.3683   50.3325 -214.9685  213.8385]\n",
      "Weights: [-4.4815 -0.4044 -0.1417 -0.2588  0.186 ]\n",
      "MSE loss: 107.7128\n",
      "Iteration: 197400\n",
      "Gradient: [  -3.8195  -40.6307  -78.9922 -166.6131 -166.1937]\n",
      "Weights: [-4.4817 -0.4042 -0.1417 -0.2586  0.1858]\n",
      "MSE loss: 107.7026\n",
      "Iteration: 197500\n",
      "Gradient: [  34.9977  -19.9724  -99.8443 -213.4896 -446.6978]\n",
      "Weights: [-4.4818 -0.4038 -0.1418 -0.2585  0.1856]\n",
      "MSE loss: 107.7351\n",
      "Iteration: 197600\n",
      "Gradient: [  32.9555  -30.5781    0.624  -121.234  -302.6052]\n",
      "Weights: [-4.482  -0.4037 -0.1419 -0.2587  0.1858]\n",
      "MSE loss: 107.6963\n",
      "Iteration: 197700\n",
      "Gradient: [ -1.0436   7.4271  28.4745  21.2059 862.7828]\n",
      "Weights: [-4.4821 -0.4034 -0.142  -0.259   0.186 ]\n",
      "MSE loss: 107.6726\n",
      "Iteration: 197800\n",
      "Gradient: [  26.7496   -3.1755  -41.0134  246.3385 -604.5362]\n",
      "Weights: [-4.4822 -0.4032 -0.1419 -0.2583  0.1857]\n",
      "MSE loss: 107.681\n",
      "Iteration: 197900\n",
      "Gradient: [ -15.7706   13.236   -58.3264  125.2348 -544.7027]\n",
      "Weights: [-4.4824 -0.403  -0.142  -0.2586  0.1857]\n",
      "MSE loss: 107.6832\n",
      "Iteration: 198000\n",
      "Gradient: [   8.3851   44.0648   65.9498  -50.3884 2408.5305]\n",
      "Weights: [-4.4826 -0.4028 -0.142  -0.2583  0.1858]\n",
      "MSE loss: 107.6791\n",
      "Iteration: 198100\n",
      "Gradient: [ -16.8878  -18.8286  -37.4829   61.4407 -480.9493]\n",
      "Weights: [-4.4827 -0.4026 -0.1422 -0.2583  0.1857]\n",
      "MSE loss: 107.6566\n",
      "Iteration: 198200\n",
      "Gradient: [  10.8449   34.9883  -90.7122 -261.5367  263.5553]\n",
      "Weights: [-4.4829 -0.4023 -0.1422 -0.2584  0.1858]\n",
      "MSE loss: 107.6473\n",
      "Iteration: 198300\n",
      "Gradient: [ -17.2033  -29.5272 -106.9386  -42.9115  -63.0123]\n",
      "Weights: [-4.483  -0.4021 -0.1422 -0.2588  0.1858]\n",
      "MSE loss: 107.6488\n",
      "Iteration: 198400\n",
      "Gradient: [   4.4665 -113.2186   28.9801 -336.2923   26.2497]\n",
      "Weights: [-4.4832 -0.4019 -0.1422 -0.2592  0.1861]\n",
      "MSE loss: 107.6288\n",
      "Iteration: 198500\n",
      "Gradient: [  -7.1923  -66.1584 -100.8917 -245.0404 -195.1117]\n",
      "Weights: [-4.4834 -0.4017 -0.1424 -0.2593  0.1859]\n",
      "MSE loss: 107.6417\n",
      "Iteration: 198600\n",
      "Gradient: [ -29.3772  -86.0604  -67.4346  102.0557 1038.3725]\n",
      "Weights: [-4.4835 -0.4014 -0.1424 -0.2595  0.1862]\n",
      "MSE loss: 107.596\n",
      "Iteration: 198700\n",
      "Gradient: [  -8.9708  -71.861   -51.7186 -180.2699 -553.8809]\n",
      "Weights: [-4.4836 -0.4011 -0.1423 -0.2594  0.186 ]\n",
      "MSE loss: 107.619\n",
      "Iteration: 198800\n",
      "Gradient: [ 39.0996 -42.2329 -39.5976 -23.0476 738.8822]\n",
      "Weights: [-4.4838 -0.4009 -0.1423 -0.2595  0.1861]\n",
      "MSE loss: 107.5789\n",
      "Iteration: 198900\n",
      "Gradient: [ -22.9662   10.9809 -143.5656   17.5683  150.9391]\n",
      "Weights: [-4.4839 -0.4006 -0.1422 -0.2599  0.1861]\n",
      "MSE loss: 107.5792\n",
      "Iteration: 199000\n",
      "Gradient: [   19.9207   -32.7633    36.6843   111.5483 -1170.0774]\n",
      "Weights: [-4.484  -0.4003 -0.1422 -0.2599  0.1862]\n",
      "MSE loss: 107.5605\n",
      "Iteration: 199100\n",
      "Gradient: [  -3.0525  -40.2557  132.665   226.9996 -344.3532]\n",
      "Weights: [-4.4842 -0.4001 -0.1421 -0.2602  0.1863]\n",
      "MSE loss: 107.5446\n",
      "Iteration: 199200\n",
      "Gradient: [  19.1751    9.7542  -27.724   184.9439 -489.9979]\n",
      "Weights: [-4.4843 -0.3998 -0.1422 -0.26    0.1861]\n",
      "MSE loss: 107.5721\n",
      "Iteration: 199300\n",
      "Gradient: [  33.879   -12.0412  -60.1023 -368.5719 -467.2781]\n",
      "Weights: [-4.4844 -0.3995 -0.1422 -0.2596  0.1861]\n",
      "MSE loss: 107.5393\n",
      "Iteration: 199400\n",
      "Gradient: [ 18.5498  15.8649 122.8555 222.3564 -15.5705]\n",
      "Weights: [-4.4846 -0.3994 -0.1425 -0.259   0.186 ]\n",
      "MSE loss: 107.5573\n",
      "Iteration: 199500\n",
      "Gradient: [ 25.8469  15.6929 105.4023 -20.6138 -83.2019]\n",
      "Weights: [-4.4848 -0.3992 -0.1427 -0.259   0.1859]\n",
      "MSE loss: 107.5356\n",
      "Iteration: 199600\n",
      "Gradient: [ 12.9337 -95.3282   8.2101 331.0827 310.3   ]\n",
      "Weights: [-4.485  -0.399  -0.1428 -0.2598  0.1862]\n",
      "MSE loss: 107.5142\n",
      "Iteration: 199700\n",
      "Gradient: [  27.5407  -16.0132  -35.3639 -111.1831  107.5845]\n",
      "Weights: [-4.4851 -0.3987 -0.1427 -0.2598  0.1863]\n",
      "MSE loss: 107.5161\n",
      "Iteration: 199800\n",
      "Gradient: [  9.8605 -31.0725 -24.3565  16.9408 -28.5387]\n",
      "Weights: [-4.4853 -0.3985 -0.1427 -0.2597  0.1861]\n",
      "MSE loss: 107.5054\n",
      "Iteration: 199900\n",
      "Gradient: [  -8.5022  -32.0292  -76.9916    6.2635 -226.2208]\n",
      "Weights: [-4.4854 -0.3983 -0.1427 -0.2598  0.1862]\n",
      "MSE loss: 107.4918\n"
     ]
    }
   ],
   "source": [
    "# Обучение на полном датасете.\n",
    "# lr - const.\n",
    "weights_1, losses_1, iter_final_1, fit_time_1 = model_fit(X_train, y_train,\n",
    "                                                          learning_rate=5*[1e-7],\n",
    "                                                          tolerance=(0.2**2 * N_points),\n",
    "                                                          beta=0,\n",
    "                                                          batch_ratio=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f5b334",
   "metadata": {},
   "source": [
    "В заданное максимальное число итераций не достигнут желаемый уровень ошибки."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b50ece9",
   "metadata": {},
   "source": [
    "Попробуем задать разный learning rate для параметров модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91fc4314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Gradient: [ 16611.1108  21298.1555  38968.8642  74704.4684 164582.5614]\n",
      "Weights: [-0. -0. -0. -0. -0.]\n",
      "MSE loss: 39331.0593\n",
      "Iteration: 100\n",
      "Gradient: [  1043.4668  -1026.6181  -9024.9074 -31069.6578 -83995.3579]\n",
      "Weights: [-3.9211 -0.202   0.029   0.0196  0.0073]\n",
      "MSE loss: 3758.9952\n",
      "Iteration: 200\n",
      "Gradient: [   568.3362   -407.9371  -4947.4253 -16714.5811 -61279.9385]\n",
      "Weights: [-4.6694 -0.1175  0.0861  0.0416  0.0144]\n",
      "MSE loss: 1724.7838\n",
      "Iteration: 300\n",
      "Gradient: [   187.5375     57.2999  -2276.1904  -9578.1586 -34812.8041]\n",
      "Weights: [-5.0068 -0.1048  0.1137  0.0534  0.0186]\n",
      "MSE loss: 1191.3328\n",
      "Iteration: 400\n",
      "Gradient: [   124.8738     75.7863   -818.9668  -5569.0072 -21498.2604]\n",
      "Weights: [-5.1503 -0.1235  0.126   0.0606  0.0214]\n",
      "MSE loss: 1019.0645\n",
      "Iteration: 500\n",
      "Gradient: [   -39.3786    439.9886   -416.8938  -3859.3018 -18647.6052]\n",
      "Weights: [-5.2052 -0.159   0.1309  0.0652  0.0235]\n",
      "MSE loss: 937.8568\n",
      "Iteration: 600\n",
      "Gradient: [   -58.9449    239.5264    -40.2474  -5522.5207 -15462.6344]\n",
      "Weights: [-5.2025 -0.2009  0.1322  0.0687  0.0251]\n",
      "MSE loss: 881.5178\n",
      "Iteration: 700\n",
      "Gradient: [   -22.4497    364.3567     82.4235  -3681.2137 -12715.9096]\n",
      "Weights: [-5.1834 -0.2439  0.131   0.0716  0.0265]\n",
      "MSE loss: 833.1471\n",
      "Iteration: 800\n",
      "Gradient: [   -35.8856    548.9073    579.227   -2913.0914 -11515.5757]\n",
      "Weights: [-5.158  -0.287   0.1302  0.0741  0.0278]\n",
      "MSE loss: 789.9086\n",
      "Iteration: 900\n",
      "Gradient: [    26.7306    466.8682    140.1985  -2049.4077 -12899.1842]\n",
      "Weights: [-5.122  -0.3304  0.1283  0.0763  0.0291]\n",
      "MSE loss: 749.692\n",
      "Iteration: 1000\n",
      "Gradient: [   43.8313   254.3826    51.2575 -2067.8582 -9703.7023]\n",
      "Weights: [-5.0884 -0.3702  0.1254  0.0784  0.0303]\n",
      "MSE loss: 712.7293\n",
      "Iteration: 1100\n",
      "Gradient: [  -45.6819   497.5446   655.9553  -578.4511 -8149.9089]\n",
      "Weights: [-5.0524 -0.4091  0.1233  0.0803  0.0314]\n",
      "MSE loss: 678.751\n",
      "Iteration: 1200\n",
      "Gradient: [ 1.7815000e+00  5.0418100e+02  3.8667190e+02 -2.4963736e+03\n",
      " -8.3963607e+03]\n",
      "Weights: [-5.022  -0.4464  0.1211  0.0823  0.0325]\n",
      "MSE loss: 647.0295\n",
      "Iteration: 1300\n",
      "Gradient: [  -71.7908   362.6211    62.2259 -2896.3657 -7416.4626]\n",
      "Weights: [-4.9879 -0.4839  0.1188  0.084   0.0335]\n",
      "MSE loss: 617.0228\n",
      "Iteration: 1400\n",
      "Gradient: [   -13.7254    406.6993    354.7203  -1992.6599 -10299.0459]\n",
      "Weights: [-4.9589 -0.5203  0.1167  0.0859  0.0346]\n",
      "MSE loss: 588.5119\n",
      "Iteration: 1500\n",
      "Gradient: [ 1.0361700e+01  3.1571370e+02 -3.2496250e+02 -1.7140542e+03\n",
      " -1.1803696e+04]\n",
      "Weights: [-4.9306 -0.5534  0.1144  0.0877  0.0355]\n",
      "MSE loss: 563.2599\n",
      "Iteration: 1600\n",
      "Gradient: [  -47.5605   343.4774  -449.9995 -2655.0548 -9167.4888]\n",
      "Weights: [-4.9012 -0.5859  0.1121  0.0892  0.0365]\n",
      "MSE loss: 539.5261\n",
      "Iteration: 1700\n",
      "Gradient: [   28.6584   182.5196    57.3407 -1935.3805 -7791.3592]\n",
      "Weights: [-4.8707 -0.6178  0.1102  0.0906  0.0374]\n",
      "MSE loss: 517.1547\n",
      "Iteration: 1800\n",
      "Gradient: [  -56.783    226.0768   303.9047 -1785.489  -8026.2005]\n",
      "Weights: [-4.8394 -0.6495  0.1082  0.0921  0.0383]\n",
      "MSE loss: 495.7779\n",
      "Iteration: 1900\n",
      "Gradient: [  -38.5276   376.2927   395.2554 -2082.5437 -7006.2511]\n",
      "Weights: [-4.813  -0.6797  0.1062  0.0935  0.0392]\n",
      "MSE loss: 476.0392\n",
      "Iteration: 2000\n",
      "Gradient: [  -63.3558   368.9354    65.3838  -687.9787 -7782.6304]\n",
      "Weights: [-4.788  -0.7087  0.1044  0.0949  0.04  ]\n",
      "MSE loss: 457.5947\n",
      "Iteration: 2100\n",
      "Gradient: [  -51.4445   280.1151   158.5148 -2426.567  -8989.6615]\n",
      "Weights: [-4.7617 -0.7359  0.1024  0.0962  0.0408]\n",
      "MSE loss: 440.6882\n",
      "Iteration: 2200\n",
      "Gradient: [-7.2555000e+00  1.8584420e+02 -1.7830400e+01 -1.5816736e+03\n",
      " -8.7943243e+03]\n",
      "Weights: [-4.743  -0.7621  0.1006  0.0975  0.0416]\n",
      "MSE loss: 425.3894\n",
      "Iteration: 2300\n",
      "Gradient: [  -48.7266   100.9754   188.4945  -619.1785 -8475.4393]\n",
      "Weights: [-4.7187 -0.7871  0.099   0.0988  0.0423]\n",
      "MSE loss: 410.9837\n",
      "Iteration: 2400\n",
      "Gradient: [    7.7893   125.1957  -240.7208 -1885.1378 -6789.9967]\n",
      "Weights: [-4.6955 -0.8121  0.0971  0.1     0.0431]\n",
      "MSE loss: 397.1494\n",
      "Iteration: 2500\n",
      "Gradient: [   58.8196   202.4904   301.8007  -250.6187 -7649.994 ]\n",
      "Weights: [-4.6748 -0.835   0.0955  0.101   0.0438]\n",
      "MSE loss: 384.6696\n",
      "Iteration: 2600\n",
      "Gradient: [ 6.5514000e+00  1.8972200e+02  2.2787960e+02 -1.4989056e+03\n",
      " -7.6545827e+03]\n",
      "Weights: [-4.6557 -0.8583  0.0938  0.1021  0.0445]\n",
      "MSE loss: 372.7144\n",
      "Iteration: 2700\n",
      "Gradient: [-2.1327700e+01  2.4296420e+02  1.7526000e+00 -1.2949769e+03\n",
      " -8.2577050e+03]\n",
      "Weights: [-4.6352 -0.8791  0.0916  0.1032  0.0452]\n",
      "MSE loss: 361.7203\n",
      "Iteration: 2800\n",
      "Gradient: [   27.3323   313.3836    33.2353  -540.5228 -4618.4032]\n",
      "Weights: [-4.6146 -0.9003  0.0899  0.1042  0.0459]\n",
      "MSE loss: 351.3353\n",
      "Iteration: 2900\n",
      "Gradient: [  -18.8765   201.4403   434.5906  -892.3107 -7012.3273]\n",
      "Weights: [-4.598  -0.9206  0.0883  0.1052  0.0465]\n",
      "MSE loss: 341.8327\n",
      "Iteration: 3000\n",
      "Gradient: [   25.9854   141.237   -195.8448    60.1696 -6090.1424]\n",
      "Weights: [-4.5813 -0.9387  0.0868  0.106   0.0471]\n",
      "MSE loss: 333.2568\n",
      "Iteration: 3100\n",
      "Gradient: [  -58.472    203.0393   182.0661  -994.4444 -8257.1307]\n",
      "Weights: [-4.5654 -0.9569  0.0852  0.107   0.0477]\n",
      "MSE loss: 325.0744\n",
      "Iteration: 3200\n",
      "Gradient: [  -41.3586   108.0091   199.9384 -1403.9679 -5417.7987]\n",
      "Weights: [-4.548  -0.9751  0.0834  0.1079  0.0483]\n",
      "MSE loss: 317.1761\n",
      "Iteration: 3300\n",
      "Gradient: [ 2.1132000e+00  8.8670900e+01  1.3135250e+02 -1.7793503e+03\n",
      " -6.0847937e+03]\n",
      "Weights: [-4.5323 -0.9911  0.0817  0.1086  0.0488]\n",
      "MSE loss: 310.3374\n",
      "Iteration: 3400\n",
      "Gradient: [   22.6101   259.7716   279.233   -459.1877 -5889.9205]\n",
      "Weights: [-4.5165 -1.0073  0.08    0.1094  0.0493]\n",
      "MSE loss: 303.7269\n",
      "Iteration: 3500\n",
      "Gradient: [  -19.4557   205.871    385.0386 -1317.7486 -3282.932 ]\n",
      "Weights: [-4.5032 -1.0225  0.0785  0.1102  0.0499]\n",
      "MSE loss: 297.6403\n",
      "Iteration: 3600\n",
      "Gradient: [   18.8186   130.0148   304.8299  -459.6902 -6711.8872]\n",
      "Weights: [-4.4908 -1.0362  0.0769  0.1109  0.0504]\n",
      "MSE loss: 292.0909\n",
      "Iteration: 3700\n",
      "Gradient: [  -25.6359   219.4742   117.6451  -510.5324 -4679.1252]\n",
      "Weights: [-4.479  -1.0505  0.0751  0.1116  0.0509]\n",
      "MSE loss: 286.6309\n",
      "Iteration: 3800\n",
      "Gradient: [-3.3948000e+00  1.6017240e+02  6.5025000e+00 -2.5649630e+02\n",
      " -5.7159829e+03]\n",
      "Weights: [-4.4627 -1.0646  0.0735  0.1123  0.0513]\n",
      "MSE loss: 281.5432\n",
      "Iteration: 3900\n",
      "Gradient: [-5.8178000e+00  1.3119660e+02  1.8392710e+02 -1.3981518e+03\n",
      " -6.5498754e+03]\n",
      "Weights: [-4.4481 -1.0789  0.0719  0.1129  0.0518]\n",
      "MSE loss: 276.6391\n",
      "Iteration: 4000\n",
      "Gradient: [  -39.4869   137.0555   319.0905  -349.95   -4114.5938]\n",
      "Weights: [-4.4376 -1.0912  0.0706  0.1135  0.0523]\n",
      "MSE loss: 272.2982\n",
      "Iteration: 4100\n",
      "Gradient: [   29.1932    88.1129   117.8856  -502.8202 -4345.3298]\n",
      "Weights: [-4.4244 -1.1045  0.0691  0.1141  0.0527]\n",
      "MSE loss: 268.1029\n",
      "Iteration: 4200\n",
      "Gradient: [   11.4758   220.2772   471.7926  -770.4386 -2916.4474]\n",
      "Weights: [-4.413  -1.1166  0.068   0.1147  0.0531]\n",
      "MSE loss: 264.2946\n",
      "Iteration: 4300\n",
      "Gradient: [  -19.5847   206.0741    -8.8017    63.992  -3545.6667]\n",
      "Weights: [-4.4041 -1.1275  0.0668  0.1153  0.0535]\n",
      "MSE loss: 260.781\n",
      "Iteration: 4400\n",
      "Gradient: [  -26.6953   108.9372   116.2615   220.3866 -1777.8897]\n",
      "Weights: [-4.3931 -1.1389  0.0653  0.1157  0.0539]\n",
      "MSE loss: 257.4107\n",
      "Iteration: 4500\n",
      "Gradient: [ -101.2077   205.6732  -161.6765  -453.138  -4668.5854]\n",
      "Weights: [-4.3824 -1.1497  0.0637  0.1163  0.0543]\n",
      "MSE loss: 254.2393\n",
      "Iteration: 4600\n",
      "Gradient: [   32.7956    44.8066   207.4652  -441.3606 -5333.0461]\n",
      "Weights: [-4.3747 -1.1582  0.0624  0.1167  0.0547]\n",
      "MSE loss: 251.5565\n",
      "Iteration: 4700\n",
      "Gradient: [  -19.9141   116.1889   469.3127  -554.0565 -1743.8419]\n",
      "Weights: [-4.3679 -1.1659  0.0611  0.1172  0.0551]\n",
      "MSE loss: 248.9881\n",
      "Iteration: 4800\n",
      "Gradient: [  -32.8332    73.4964   262.7256  -578.2463 -1761.7351]\n",
      "Weights: [-4.3588 -1.1749  0.0595  0.1176  0.0554]\n",
      "MSE loss: 246.4463\n",
      "Iteration: 4900\n",
      "Gradient: [    7.1747    17.7924   192.5294  -498.1276 -4547.1247]\n",
      "Weights: [-4.3501 -1.1831  0.0582  0.1181  0.0558]\n",
      "MSE loss: 244.1502\n",
      "Iteration: 5000\n",
      "Gradient: [   -5.2337    24.0847    49.2482  -416.0371 -2546.8127]\n",
      "Weights: [-4.3427 -1.1909  0.0567  0.1184  0.0561]\n",
      "MSE loss: 241.9623\n",
      "Iteration: 5100\n",
      "Gradient: [  -37.0254   113.6743    55.45     -86.7564 -2561.2902]\n",
      "Weights: [-4.3354 -1.1989  0.0552  0.1189  0.0564]\n",
      "MSE loss: 239.8535\n",
      "Iteration: 5200\n",
      "Gradient: [  -25.9794    26.3309    12.2264    59.1946 -2953.5119]\n",
      "Weights: [-4.3258 -1.2069  0.0538  0.1192  0.0568]\n",
      "MSE loss: 237.7812\n",
      "Iteration: 5300\n",
      "Gradient: [   29.0324    -6.0452  -399.4883   744.8521 -4286.6516]\n",
      "Weights: [-4.3202 -1.2138  0.0525  0.1196  0.0571]\n",
      "MSE loss: 235.9842\n",
      "Iteration: 5400\n",
      "Gradient: [ -21.2348  115.5601  182.2588 -713.0425 -998.4609]\n",
      "Weights: [-4.3125 -1.2205  0.0513  0.1199  0.0574]\n",
      "MSE loss: 234.3187\n",
      "Iteration: 5500\n",
      "Gradient: [  -23.5626   111.2622   425.7314   156.2093 -2698.3318]\n",
      "Weights: [-4.3057 -1.2275  0.0497  0.1202  0.0577]\n",
      "MSE loss: 232.6306\n",
      "Iteration: 5600\n",
      "Gradient: [-1.0458300e+01 -3.7877000e+00  3.5225550e+02  3.9078500e+01\n",
      " -4.1997661e+03]\n",
      "Weights: [-4.2978 -1.234   0.0486  0.1205  0.0579]\n",
      "MSE loss: 231.1555\n",
      "Iteration: 5700\n",
      "Gradient: [   36.5311   -29.3869     7.3762     9.3458 -3266.0995]\n",
      "Weights: [-4.2908 -1.2393  0.0472  0.1207  0.0582]\n",
      "MSE loss: 229.8057\n",
      "Iteration: 5800\n",
      "Gradient: [  -20.8668     3.8148    34.893   -242.1613 -1513.258 ]\n",
      "Weights: [-4.2855 -1.2456  0.0459  0.1211  0.0585]\n",
      "MSE loss: 228.3988\n",
      "Iteration: 5900\n",
      "Gradient: [  -11.6536    76.5392   -47.4753   433.9396 -1488.1226]\n",
      "Weights: [-4.2805 -1.2512  0.0447  0.1214  0.0588]\n",
      "MSE loss: 227.1076\n",
      "Iteration: 6000\n",
      "Gradient: [  -29.0376    67.4972    68.5065   -55.9571 -1371.1471]\n",
      "Weights: [-4.2751 -1.2562  0.0434  0.1217  0.059 ]\n",
      "MSE loss: 225.9665\n",
      "Iteration: 6100\n",
      "Gradient: [    9.5825   -27.7313   -48.6157  -220.4414 -3463.6714]\n",
      "Weights: [-4.2703 -1.2613  0.0424  0.122   0.0593]\n",
      "MSE loss: 224.8684\n",
      "Iteration: 6200\n",
      "Gradient: [   61.1448    32.7284    86.7046  -477.0974 -2299.9513]\n",
      "Weights: [-4.2647 -1.2652  0.0411  0.1222  0.0595]\n",
      "MSE loss: 223.8803\n",
      "Iteration: 6300\n",
      "Gradient: [  -23.5811   -21.7142   163.2871  -953.419  -2665.4277]\n",
      "Weights: [-4.2613 -1.2697  0.04    0.1225  0.0597]\n",
      "MSE loss: 222.8675\n",
      "Iteration: 6400\n",
      "Gradient: [   15.0845    21.7697   297.9133   179.5279 -3921.6891]\n",
      "Weights: [-4.2574 -1.2741  0.0386  0.1227  0.06  ]\n",
      "MSE loss: 221.8802\n",
      "Iteration: 6500\n",
      "Gradient: [  -29.542    102.7688    29.3703    29.96   -4312.8023]\n",
      "Weights: [-4.2497 -1.2784  0.0373  0.1229  0.0602]\n",
      "MSE loss: 220.9452\n",
      "Iteration: 6600\n",
      "Gradient: [  -17.919     53.31     113.8767  -245.8995 -1650.3365]\n",
      "Weights: [-4.2455 -1.2829  0.0362  0.1231  0.0605]\n",
      "MSE loss: 220.0948\n",
      "Iteration: 6700\n",
      "Gradient: [   32.634     51.0759   110.4375   211.5941 -1390.2541]\n",
      "Weights: [-4.2413 -1.2863  0.035   0.1233  0.0607]\n",
      "MSE loss: 219.3184\n",
      "Iteration: 6800\n",
      "Gradient: [  -45.8155    86.2939    45.6851    58.8648 -1561.2969]\n",
      "Weights: [-4.2373 -1.2893  0.0338  0.1234  0.0609]\n",
      "MSE loss: 218.6038\n",
      "Iteration: 6900\n",
      "Gradient: [   53.64     -93.6413   377.3052   -65.1963 -2337.1005]\n",
      "Weights: [-4.2342 -1.2923  0.0325  0.1236  0.0611]\n",
      "MSE loss: 217.8568\n",
      "Iteration: 7000\n",
      "Gradient: [  29.3303  106.7351  302.036  -198.7835 -331.8307]\n",
      "Weights: [-4.2318 -1.2944  0.0313  0.1237  0.0613]\n",
      "MSE loss: 217.2139\n",
      "Iteration: 7100\n",
      "Gradient: [  -15.2473   120.5748   -17.7198    30.585  -1863.6089]\n",
      "Weights: [-4.2307 -1.2969  0.0301  0.1239  0.0615]\n",
      "MSE loss: 216.5883\n",
      "Iteration: 7200\n",
      "Gradient: [  -24.455     -4.4466   165.0115    17.4948 -3081.9594]\n",
      "Weights: [-4.2265 -1.299   0.0288  0.1241  0.0617]\n",
      "MSE loss: 215.9508\n",
      "Iteration: 7300\n",
      "Gradient: [   12.2506    53.77     151.6416   121.8892 -2081.099 ]\n",
      "Weights: [-4.2226 -1.3008  0.0274  0.1242  0.0619]\n",
      "MSE loss: 215.3429\n",
      "Iteration: 7400\n",
      "Gradient: [    5.0476   129.0618    43.1155   132.5751 -2162.6198]\n",
      "Weights: [-4.2191 -1.3032  0.0262  0.1243  0.0621]\n",
      "MSE loss: 214.7767\n",
      "Iteration: 7500\n",
      "Gradient: [ 1.1134000e+00 -2.4690900e+01  1.6089510e+02  3.9919100e+01\n",
      " -2.9675877e+03]\n",
      "Weights: [-4.2179 -1.3052  0.025   0.1244  0.0623]\n",
      "MSE loss: 214.2429\n",
      "Iteration: 7600\n",
      "Gradient: [   -5.5211    -5.6897   386.6413    25.5746 -1782.8338]\n",
      "Weights: [-4.2161 -1.3062  0.0238  0.1246  0.0624]\n",
      "MSE loss: 213.7549\n",
      "Iteration: 7700\n",
      "Gradient: [   38.0764    94.3467   -14.0896   448.6515 -1362.1963]\n",
      "Weights: [-4.2131 -1.3082  0.0225  0.1247  0.0626]\n",
      "MSE loss: 213.2408\n",
      "Iteration: 7800\n",
      "Gradient: [   36.017     36.9929   183.4332   -99.0168 -1286.6311]\n",
      "Weights: [-4.2099 -1.3096  0.0214  0.1248  0.0628]\n",
      "MSE loss: 212.7985\n",
      "Iteration: 7900\n",
      "Gradient: [ -61.0718   90.3125   75.6384 -187.7791 -199.1321]\n",
      "Weights: [-4.2086 -1.3113  0.0203  0.1249  0.063 ]\n",
      "MSE loss: 212.3286\n",
      "Iteration: 8000\n",
      "Gradient: [   -8.7982    19.0796    84.826    269.3307 -2529.8231]\n",
      "Weights: [-4.2029 -1.3141  0.0191  0.1249  0.0631]\n",
      "MSE loss: 211.8494\n",
      "Iteration: 8100\n",
      "Gradient: [  -18.7224    38.5312    90.9682    42.4225 -2123.8322]\n",
      "Weights: [-4.1995 -1.3162  0.0182  0.125   0.0633]\n",
      "MSE loss: 211.4325\n",
      "Iteration: 8200\n",
      "Gradient: [ 20.2649   4.0569  87.1692 118.6997 703.2146]\n",
      "Weights: [-4.1987 -1.3174  0.017   0.1251  0.0635]\n",
      "MSE loss: 210.9973\n",
      "Iteration: 8300\n",
      "Gradient: [   40.6474   -25.6564   166.517   -297.6739 -1935.8408]\n",
      "Weights: [-4.1983 -1.3184  0.0161  0.1252  0.0636]\n",
      "MSE loss: 210.6281\n",
      "Iteration: 8400\n",
      "Gradient: [-1.5364000e+00  2.3303400e+01  2.8709820e+02  1.6222890e+02\n",
      " -2.3530536e+03]\n",
      "Weights: [-4.1982 -1.3196  0.0151  0.1253  0.0638]\n",
      "MSE loss: 210.2874\n",
      "Iteration: 8500\n",
      "Gradient: [   10.3744    52.5046   157.0028   -52.0476 -2485.3493]\n",
      "Weights: [-4.1934 -1.3206  0.014   0.1253  0.0639]\n",
      "MSE loss: 209.9239\n",
      "Iteration: 8600\n",
      "Gradient: [-8.0930000e-01 -6.1977400e+01 -2.8105830e+02  3.7138200e+01\n",
      " -1.1650157e+03]\n",
      "Weights: [-4.1948 -1.3209  0.013   0.1253  0.0641]\n",
      "MSE loss: 209.5975\n",
      "Iteration: 8700\n",
      "Gradient: [   -6.2275   125.8701   195.1958  -305.4913 -1075.4734]\n",
      "Weights: [-4.1913 -1.3212  0.0119  0.1253  0.0642]\n",
      "MSE loss: 209.2659\n",
      "Iteration: 8800\n",
      "Gradient: [  -35.5019    59.3098   372.9099   152.0576 -2216.897 ]\n",
      "Weights: [-4.1896 -1.3216  0.0109  0.1254  0.0644]\n",
      "MSE loss: 208.9536\n",
      "Iteration: 8900\n",
      "Gradient: [  -26.9999    73.374    180.7827    88.3048 -1357.0199]\n",
      "Weights: [-4.1888 -1.322   0.0096  0.1254  0.0645]\n",
      "MSE loss: 208.6253\n",
      "Iteration: 9000\n",
      "Gradient: [-1.2942000e+00  2.7484600e+01  7.7453300e+01 -6.9784400e+01\n",
      " -1.3630346e+03]\n",
      "Weights: [-4.1898 -1.3214  0.0087  0.1254  0.0646]\n",
      "MSE loss: 208.3486\n",
      "Iteration: 9100\n",
      "Gradient: [  -10.5599    10.3502   227.5949   154.6569 -2203.7873]\n",
      "Weights: [-4.1881 -1.3218  0.0078  0.1254  0.0647]\n",
      "MSE loss: 208.071\n",
      "Iteration: 9200\n",
      "Gradient: [   30.0265   -73.1124    26.2508  -197.368  -2302.8627]\n",
      "Weights: [-4.1866 -1.3224  0.0067  0.1254  0.0649]\n",
      "MSE loss: 207.7674\n",
      "Iteration: 9300\n",
      "Gradient: [  -35.7458    13.5119   173.8184   132.8234 -2231.8514]\n",
      "Weights: [-4.186  -1.3224  0.0056  0.1255  0.065 ]\n",
      "MSE loss: 207.4575\n",
      "Iteration: 9400\n",
      "Gradient: [   32.472     31.7356   -20.4576  -323.0045 -1730.2251]\n",
      "Weights: [-4.1872 -1.3218  0.0045  0.1255  0.0651]\n",
      "MSE loss: 207.1951\n",
      "Iteration: 9500\n",
      "Gradient: [-6.2520000e-01  5.4317000e+00  2.1727330e+02 -4.9108830e+02\n",
      " -1.8416375e+03]\n",
      "Weights: [-4.1858e+00 -1.3214e+00  3.7000e-03  1.2550e-01  6.5300e-02]\n",
      "MSE loss: 206.9324\n",
      "Iteration: 9600\n",
      "Gradient: [ 25.3807  89.092  134.5007 218.9001 406.7329]\n",
      "Weights: [-4.1833e+00 -1.3216e+00  2.5000e-03  1.2550e-01  6.5400e-02]\n",
      "MSE loss: 206.6545\n",
      "Iteration: 9700\n",
      "Gradient: [ -61.7291  -48.8039   39.2593 -134.4879   32.9737]\n",
      "Weights: [-4.1833e+00 -1.3210e+00  1.5000e-03  1.2550e-01  6.5500e-02]\n",
      "MSE loss: 206.3834\n",
      "Iteration: 9800\n",
      "Gradient: [ -5.6241   1.7562 198.3172 -16.1183 163.4754]\n",
      "Weights: [-4.1803e+00 -1.3214e+00  3.0000e-04  1.2550e-01  6.5600e-02]\n",
      "MSE loss: 206.1192\n",
      "Iteration: 9900\n",
      "Gradient: [  -18.9218   -57.7013   135.3882   256.9883 -1055.0259]\n",
      "Weights: [-4.1796e+00 -1.3216e+00 -6.0000e-04  1.2550e-01  6.5700e-02]\n",
      "MSE loss: 205.8658\n",
      "Iteration: 10000\n",
      "Gradient: [   7.4909  -29.351  -112.4596 -343.8585 -281.6645]\n",
      "Weights: [-4.1797e+00 -1.3215e+00 -1.5000e-03  1.2550e-01  6.5800e-02]\n",
      "MSE loss: 205.627\n",
      "Iteration: 10100\n",
      "Gradient: [  32.1326  -28.6179   84.1714  -61.755  -542.6394]\n",
      "Weights: [-4.1756e+00 -1.3215e+00 -2.5000e-03  1.2550e-01  6.6000e-02]\n",
      "MSE loss: 205.3795\n",
      "Iteration: 10200\n",
      "Gradient: [  19.7018   19.567   -86.3344  363.3209 -950.7986]\n",
      "Weights: [-4.1781e+00 -1.3206e+00 -3.4000e-03  1.2550e-01  6.6100e-02]\n",
      "MSE loss: 205.1442\n",
      "Iteration: 10300\n",
      "Gradient: [  -36.4077   -43.0327   134.2641   132.0753 -1603.5855]\n",
      "Weights: [-4.1771 -1.3204 -0.0045  0.1255  0.0662]\n",
      "MSE loss: 204.8928\n",
      "Iteration: 10400\n",
      "Gradient: [ -11.7155   64.4901   38.8943 -234.816  -588.3862]\n",
      "Weights: [-4.1763 -1.32   -0.0055  0.1256  0.0663]\n",
      "MSE loss: 204.6607\n",
      "Iteration: 10500\n",
      "Gradient: [   27.5408    33.2443    97.3493   -69.429  -1635.1335]\n",
      "Weights: [-4.1775 -1.3197 -0.0065  0.1256  0.0664]\n",
      "MSE loss: 204.4355\n",
      "Iteration: 10600\n",
      "Gradient: [  -29.0844    82.6703    62.8548   241.2267 -1080.601 ]\n",
      "Weights: [-4.178  -1.3192 -0.0076  0.1256  0.0665]\n",
      "MSE loss: 204.1836\n",
      "Iteration: 10700\n",
      "Gradient: [   23.5045    17.2565   254.8542   133.9945 -2121.595 ]\n",
      "Weights: [-4.1728 -1.319  -0.0085  0.1255  0.0666]\n",
      "MSE loss: 203.9519\n",
      "Iteration: 10800\n",
      "Gradient: [   45.2013    45.269     19.1921  -136.3853 -2213.1897]\n",
      "Weights: [-4.1729 -1.3187 -0.0097  0.1254  0.0667]\n",
      "MSE loss: 203.7029\n",
      "Iteration: 10900\n",
      "Gradient: [  -20.7241   -20.6546   244.3571   506.514  -1287.6338]\n",
      "Weights: [-4.1736 -1.3176 -0.0107  0.1254  0.0669]\n",
      "MSE loss: 203.4604\n",
      "Iteration: 11000\n",
      "Gradient: [  20.9369   52.0436   -2.0922 -246.2695 -902.2558]\n",
      "Weights: [-4.1728 -1.3167 -0.0118  0.1254  0.067 ]\n",
      "MSE loss: 203.2182\n",
      "Iteration: 11100\n",
      "Gradient: [  -15.768     69.1686   116.8502    65.3204 -1272.6538]\n",
      "Weights: [-4.1727 -1.3163 -0.0129  0.1255  0.0671]\n",
      "MSE loss: 202.9914\n",
      "Iteration: 11200\n",
      "Gradient: [  -31.7929    63.478     63.0251  -421.6636 -1172.1995]\n",
      "Weights: [-4.1747 -1.3153 -0.0138  0.1255  0.0672]\n",
      "MSE loss: 202.761\n",
      "Iteration: 11300\n",
      "Gradient: [  10.4273   72.5013   77.0713 -639.4764  -85.4345]\n",
      "Weights: [-4.1728 -1.3147 -0.0149  0.1254  0.0673]\n",
      "MSE loss: 202.5163\n",
      "Iteration: 11400\n",
      "Gradient: [ 9.046000e-01  3.034760e+01  5.006370e+01 -4.777637e+02 -9.555197e+02]\n",
      "Weights: [-4.1707 -1.3142 -0.0158  0.1254  0.0674]\n",
      "MSE loss: 202.3262\n",
      "Iteration: 11500\n",
      "Gradient: [   14.5962     9.2249   204.6954  -353.2448 -2218.4918]\n",
      "Weights: [-4.1695 -1.3134 -0.017   0.1253  0.0675]\n",
      "MSE loss: 202.0831\n",
      "Iteration: 11600\n",
      "Gradient: [  -5.7135  -69.5128 -105.5636 -579.9984  423.2646]\n",
      "Weights: [-4.1705 -1.3125 -0.018   0.1253  0.0676]\n",
      "MSE loss: 201.8654\n",
      "Iteration: 11700\n",
      "Gradient: [ 1.433000e+00 -1.903800e+01  1.579965e+02  8.983700e+01 -1.691537e+03]\n",
      "Weights: [-4.1707 -1.3119 -0.0188  0.1253  0.0677]\n",
      "MSE loss: 201.6904\n",
      "Iteration: 11800\n",
      "Gradient: [   0.8126  -75.941    45.9691  193.3562 -435.1768]\n",
      "Weights: [-4.17   -1.3115 -0.0197  0.1253  0.0678]\n",
      "MSE loss: 201.4838\n",
      "Iteration: 11900\n",
      "Gradient: [ -64.7269  -46.5536  189.3756    8.4003 -980.2099]\n",
      "Weights: [-4.1679 -1.3104 -0.0206  0.1252  0.0679]\n",
      "MSE loss: 201.283\n",
      "Iteration: 12000\n",
      "Gradient: [  -25.4803   -54.5335   130.7612  -144.573  -1231.5824]\n",
      "Weights: [-4.1689 -1.3096 -0.0217  0.1252  0.068 ]\n",
      "MSE loss: 201.0526\n",
      "Iteration: 12100\n",
      "Gradient: [  47.9447  -38.2836  100.159    91.0717 -341.788 ]\n",
      "Weights: [-4.17   -1.3084 -0.0225  0.1251  0.0681]\n",
      "MSE loss: 200.8382\n",
      "Iteration: 12200\n",
      "Gradient: [   -9.6604   -54.1455   135.2915     8.8688 -2683.798 ]\n",
      "Weights: [-4.172  -1.3065 -0.0236  0.1251  0.0682]\n",
      "MSE loss: 200.6197\n",
      "Iteration: 12300\n",
      "Gradient: [   8.0248   44.5499 -164.5051 -185.3442 -696.536 ]\n",
      "Weights: [-4.1712 -1.3055 -0.0247  0.1251  0.0683]\n",
      "MSE loss: 200.3923\n",
      "Iteration: 12400\n",
      "Gradient: [ -24.58    -74.1114   54.9084  684.2046 -803.6775]\n",
      "Weights: [-4.1689 -1.3043 -0.0259  0.125   0.0684]\n",
      "MSE loss: 200.1657\n",
      "Iteration: 12500\n",
      "Gradient: [  -11.6133   -23.7329   -14.051     99.9804 -1938.8392]\n",
      "Weights: [-4.1654 -1.3043 -0.027   0.1249  0.0685]\n",
      "MSE loss: 199.9657\n",
      "Iteration: 12600\n",
      "Gradient: [   38.859     -3.1473   298.6669   923.0694 -1853.527 ]\n",
      "Weights: [-4.1669 -1.3025 -0.0282  0.1249  0.0686]\n",
      "MSE loss: 199.7126\n",
      "Iteration: 12700\n",
      "Gradient: [  -13.9753   -27.2282   -79.919   -145.9834 -1416.3453]\n",
      "Weights: [-4.167  -1.3011 -0.0291  0.1249  0.0687]\n",
      "MSE loss: 199.5305\n",
      "Iteration: 12800\n",
      "Gradient: [   35.3942   -90.8117   164.3178    34.7213 -1211.3153]\n",
      "Weights: [-4.1678 -1.3002 -0.03    0.1249  0.0688]\n",
      "MSE loss: 199.3217\n",
      "Iteration: 12900\n",
      "Gradient: [ -42.1462  -64.4269  183.0711  -36.4547 -485.2022]\n",
      "Weights: [-4.1698 -1.2987 -0.031   0.1249  0.0689]\n",
      "MSE loss: 199.1116\n",
      "Iteration: 13000\n",
      "Gradient: [  -23.2433   -19.714    101.4246   -22.225  -1352.8186]\n",
      "Weights: [-4.1713 -1.2973 -0.0319  0.1248  0.069 ]\n",
      "MSE loss: 198.9152\n",
      "Iteration: 13100\n",
      "Gradient: [ -26.1785   -8.8213  230.3461  171.9173 -618.4745]\n",
      "Weights: [-4.17   -1.2961 -0.0328  0.1247  0.0691]\n",
      "MSE loss: 198.7131\n",
      "Iteration: 13200\n",
      "Gradient: [ -37.3621   22.4743  132.4255  209.0004 -186.996 ]\n",
      "Weights: [-4.171  -1.2941 -0.0337  0.1247  0.0692]\n",
      "MSE loss: 198.5177\n",
      "Iteration: 13300\n",
      "Gradient: [ -25.5991   72.2847  -82.2875  273.145  -905.5972]\n",
      "Weights: [-4.1703 -1.2938 -0.0349  0.1246  0.0693]\n",
      "MSE loss: 198.3076\n",
      "Iteration: 13400\n",
      "Gradient: [ -31.0687  -39.1188   91.2875  104.1141 -414.559 ]\n",
      "Weights: [-4.1714 -1.2919 -0.0359  0.1246  0.0693]\n",
      "MSE loss: 198.0991\n",
      "Iteration: 13500\n",
      "Gradient: [  -24.7423   -21.9388    92.6051   206.3589 -1780.8614]\n",
      "Weights: [-4.1701 -1.2905 -0.037   0.1245  0.0694]\n",
      "MSE loss: 197.8912\n",
      "Iteration: 13600\n",
      "Gradient: [  14.172   -40.2009  137.2254  169.5929 -857.1285]\n",
      "Weights: [-4.1718 -1.2893 -0.038   0.1246  0.0695]\n",
      "MSE loss: 197.6973\n",
      "Iteration: 13700\n",
      "Gradient: [ -74.6483   57.5738   66.6415  235.2289 -418.1009]\n",
      "Weights: [-4.1715 -1.2877 -0.039   0.1245  0.0696]\n",
      "MSE loss: 197.4782\n",
      "Iteration: 13800\n",
      "Gradient: [ -49.7395    6.0393  174.135   -83.0397 -736.3761]\n",
      "Weights: [-4.1729 -1.2863 -0.04    0.1245  0.0697]\n",
      "MSE loss: 197.2939\n",
      "Iteration: 13900\n",
      "Gradient: [  -26.4623    63.3585    18.1351   436.109  -1025.1641]\n",
      "Weights: [-4.1714 -1.2855 -0.0409  0.1244  0.0698]\n",
      "MSE loss: 197.1029\n",
      "Iteration: 14000\n",
      "Gradient: [  12.5222  -49.0428  -42.1948 -465.4136 -835.525 ]\n",
      "Weights: [-4.1717 -1.2841 -0.042   0.1243  0.0699]\n",
      "MSE loss: 196.8949\n",
      "Iteration: 14100\n",
      "Gradient: [  -10.5002    28.0245   337.7327   290.7621 -2010.4712]\n",
      "Weights: [-4.1726 -1.2821 -0.0428  0.1242  0.07  ]\n",
      "MSE loss: 196.7088\n",
      "Iteration: 14200\n",
      "Gradient: [  -69.202    -56.1036   155.4128   -96.5637 -1383.9176]\n",
      "Weights: [-4.1728 -1.2814 -0.0437  0.1242  0.0701]\n",
      "MSE loss: 196.5187\n",
      "Iteration: 14300\n",
      "Gradient: [   20.4029    -2.7545   -19.499     53.5942 -2224.8241]\n",
      "Weights: [-4.1707 -1.2803 -0.0447  0.1241  0.0701]\n",
      "MSE loss: 196.3205\n",
      "Iteration: 14400\n",
      "Gradient: [  -27.2679   -70.0741   122.5258   482.4831 -1212.0601]\n",
      "Weights: [-4.1718 -1.2784 -0.0458  0.124   0.0702]\n",
      "MSE loss: 196.1153\n",
      "Iteration: 14500\n",
      "Gradient: [ -10.9867   82.4455   98.6751  188.5745 -467.6724]\n",
      "Weights: [-4.17   -1.277  -0.0466  0.124   0.0703]\n",
      "MSE loss: 195.9546\n",
      "Iteration: 14600\n",
      "Gradient: [ -16.644    34.8121  178.2497 -339.5004  996.5916]\n",
      "Weights: [-4.1729 -1.2756 -0.0475  0.1239  0.0704]\n",
      "MSE loss: 195.7608\n",
      "Iteration: 14700\n",
      "Gradient: [ -17.7339   -8.0671  185.1107 -168.9281 -815.5713]\n",
      "Weights: [-4.1759 -1.2737 -0.0483  0.1239  0.0705]\n",
      "MSE loss: 195.5719\n",
      "Iteration: 14800\n",
      "Gradient: [  19.6736  -51.2769  -28.9331  -93.0821 -531.9896]\n",
      "Weights: [-4.1765 -1.2716 -0.0494  0.1239  0.0706]\n",
      "MSE loss: 195.3672\n",
      "Iteration: 14900\n",
      "Gradient: [  -20.1012    58.3476   197.8258  -301.3133 -1007.8636]\n",
      "Weights: [-4.1755 -1.2702 -0.0505  0.1238  0.0706]\n",
      "MSE loss: 195.1574\n",
      "Iteration: 15000\n",
      "Gradient: [  -17.1193    34.2114   103.7659   121.5022 -1770.1229]\n",
      "Weights: [-4.1742 -1.2685 -0.0515  0.1237  0.0707]\n",
      "MSE loss: 194.959\n",
      "Iteration: 15100\n",
      "Gradient: [-16.9855 -33.8543 -57.2339 305.2272  78.6028]\n",
      "Weights: [-4.1766 -1.2665 -0.0525  0.1236  0.0708]\n",
      "MSE loss: 194.751\n",
      "Iteration: 15200\n",
      "Gradient: [ 67.2773  28.3411 245.4479 390.5379 380.8066]\n",
      "Weights: [-4.1746 -1.2651 -0.0536  0.1235  0.0709]\n",
      "MSE loss: 194.5511\n",
      "Iteration: 15300\n",
      "Gradient: [   10.7836    27.644      5.652    248.855  -1213.7919]\n",
      "Weights: [-4.1758 -1.2636 -0.0545  0.1234  0.071 ]\n",
      "MSE loss: 194.3551\n",
      "Iteration: 15400\n",
      "Gradient: [ 23.4126  22.8783 184.4656 406.3888 104.3208]\n",
      "Weights: [-4.1734 -1.2621 -0.0555  0.1234  0.0711]\n",
      "MSE loss: 194.182\n",
      "Iteration: 15500\n",
      "Gradient: [  37.6774  -11.8623  131.9987  413.4388 -102.263 ]\n",
      "Weights: [-4.1767 -1.2607 -0.0564  0.1234  0.0711]\n",
      "MSE loss: 193.9853\n",
      "Iteration: 15600\n",
      "Gradient: [  42.8852  -16.0465   25.9984 -291.2298  -22.2951]\n",
      "Weights: [-4.1766 -1.2596 -0.0572  0.1232  0.0712]\n",
      "MSE loss: 193.8208\n",
      "Iteration: 15700\n",
      "Gradient: [  14.8081  -55.4914  304.935   -46.1221 -750.184 ]\n",
      "Weights: [-4.177  -1.2581 -0.058   0.1231  0.0713]\n",
      "MSE loss: 193.6477\n",
      "Iteration: 15800\n",
      "Gradient: [    3.6524    70.3268    96.1425   373.4143 -1594.6241]\n",
      "Weights: [-4.1768 -1.256  -0.059   0.1231  0.0714]\n",
      "MSE loss: 193.4524\n",
      "Iteration: 15900\n",
      "Gradient: [  -8.8485  -31.9173  -85.1417   48.302  -933.9115]\n",
      "Weights: [-4.1782 -1.2548 -0.0598  0.123   0.0715]\n",
      "MSE loss: 193.2761\n",
      "Iteration: 16000\n",
      "Gradient: [  -5.9268   21.6724  122.8099  -95.4074 -408.8751]\n",
      "Weights: [-4.1791 -1.2529 -0.0608  0.1229  0.0716]\n",
      "MSE loss: 193.0645\n",
      "Iteration: 16100\n",
      "Gradient: [ -39.9894  -34.4619   88.5643   -7.0744 -577.06  ]\n",
      "Weights: [-4.1804 -1.2514 -0.0618  0.1228  0.0716]\n",
      "MSE loss: 192.8842\n",
      "Iteration: 16200\n",
      "Gradient: [   19.7206    14.3319   110.7076    21.1175 -2170.4039]\n",
      "Weights: [-4.1803 -1.25   -0.0626  0.1228  0.0717]\n",
      "MSE loss: 192.7072\n",
      "Iteration: 16300\n",
      "Gradient: [  19.8864   36.9914   60.9665   73.6039 -516.8209]\n",
      "Weights: [-4.1805 -1.2481 -0.0636  0.1227  0.0718]\n",
      "MSE loss: 192.5103\n",
      "Iteration: 16400\n",
      "Gradient: [ -19.4954 -134.1856  342.3778   84.251   140.0923]\n",
      "Weights: [-4.1798 -1.2465 -0.0646  0.1226  0.0719]\n",
      "MSE loss: 192.3281\n",
      "Iteration: 16500\n",
      "Gradient: [ -10.5032  -53.7683  239.7246   39.9169 -903.5164]\n",
      "Weights: [-4.1797 -1.2454 -0.0654  0.1226  0.0719]\n",
      "MSE loss: 192.1621\n",
      "Iteration: 16600\n",
      "Gradient: [   4.6842  -55.7755  123.6854   61.6593 -736.9834]\n",
      "Weights: [-4.1796 -1.2437 -0.0665  0.1225  0.072 ]\n",
      "MSE loss: 191.9552\n",
      "Iteration: 16700\n",
      "Gradient: [  18.1245  -34.4219  195.7101  188.6373 -681.1425]\n",
      "Weights: [-4.181  -1.242  -0.0674  0.1224  0.0721]\n",
      "MSE loss: 191.768\n",
      "Iteration: 16800\n",
      "Gradient: [  -11.7363   -66.6024  -107.7077   146.3647 -2362.0559]\n",
      "Weights: [-4.1826 -1.24   -0.0685  0.1224  0.0722]\n",
      "MSE loss: 191.5636\n",
      "Iteration: 16900\n",
      "Gradient: [ -44.0117   31.1128 -114.9238  322.7956 -948.1993]\n",
      "Weights: [-4.1856 -1.2375 -0.0693  0.1223  0.0723]\n",
      "MSE loss: 191.3777\n",
      "Iteration: 17000\n",
      "Gradient: [   2.1332  -54.3279   30.1961 -406.7499 -732.0135]\n",
      "Weights: [-4.1838 -1.2364 -0.0702  0.1222  0.0724]\n",
      "MSE loss: 191.2001\n",
      "Iteration: 17100\n",
      "Gradient: [  13.0459  -47.2153  219.478  -371.9541 -869.8418]\n",
      "Weights: [-4.1847 -1.2353 -0.0711  0.1221  0.0724]\n",
      "MSE loss: 191.0352\n",
      "Iteration: 17200\n",
      "Gradient: [  35.8964   83.4733  124.0152  204.6592 -669.9632]\n",
      "Weights: [-4.1827 -1.2344 -0.072   0.1221  0.0725]\n",
      "MSE loss: 190.8693\n",
      "Iteration: 17300\n",
      "Gradient: [  -11.4669    21.9985    40.2283   550.4401 -1145.514 ]\n",
      "Weights: [-4.1816 -1.2341 -0.0729  0.1221  0.0726]\n",
      "MSE loss: 190.7137\n",
      "Iteration: 17400\n",
      "Gradient: [  -9.3274  -74.8557  -19.4582  108.82   -482.8339]\n",
      "Weights: [-4.1837 -1.2325 -0.0737  0.122   0.0727]\n",
      "MSE loss: 190.5336\n",
      "Iteration: 17500\n",
      "Gradient: [   35.915     -3.3118   272.8456   371.7438 -1325.8793]\n",
      "Weights: [-4.1841 -1.2307 -0.0745  0.122   0.0728]\n",
      "MSE loss: 190.3531\n",
      "Iteration: 17600\n",
      "Gradient: [   21.1612   -71.6925   -47.1341   195.5502 -1072.9127]\n",
      "Weights: [-4.1851 -1.2292 -0.0755  0.1219  0.0728]\n",
      "MSE loss: 190.1711\n",
      "Iteration: 17700\n",
      "Gradient: [   22.7126     4.697    -52.6481     3.6732 -1040.7652]\n",
      "Weights: [-4.1851 -1.2278 -0.0767  0.1219  0.0729]\n",
      "MSE loss: 189.9651\n",
      "Iteration: 17800\n",
      "Gradient: [  11.4605  -72.529    97.6474  367.3039 -897.6449]\n",
      "Weights: [-4.1833 -1.2265 -0.0776  0.1218  0.073 ]\n",
      "MSE loss: 189.7833\n",
      "Iteration: 17900\n",
      "Gradient: [ -23.7937   44.9693  103.1317  726.8485 -120.4308]\n",
      "Weights: [-4.184  -1.2251 -0.0785  0.1217  0.0731]\n",
      "MSE loss: 189.6081\n",
      "Iteration: 18000\n",
      "Gradient: [  -48.9114   -98.4447   -23.7827  -390.9279 -1267.444 ]\n",
      "Weights: [-4.1851 -1.2235 -0.0794  0.1217  0.0732]\n",
      "MSE loss: 189.4157\n",
      "Iteration: 18100\n",
      "Gradient: [ -29.3478 -107.4846  -57.8354  273.9004 -481.0364]\n",
      "Weights: [-4.1878 -1.2207 -0.0805  0.1216  0.0733]\n",
      "MSE loss: 189.2059\n",
      "Iteration: 18200\n",
      "Gradient: [-10.2736 -54.595  183.5507 320.4471 585.6491]\n",
      "Weights: [-4.1866 -1.2192 -0.0816  0.1216  0.0733]\n",
      "MSE loss: 189.0166\n",
      "Iteration: 18300\n",
      "Gradient: [  15.6208   -5.9127  -52.3527  766.8572 -354.1248]\n",
      "Weights: [-4.1867 -1.2173 -0.0825  0.1215  0.0734]\n",
      "MSE loss: 188.8513\n",
      "Iteration: 18400\n",
      "Gradient: [ 1.828730e+01  4.538000e-01  2.521047e+02  9.727740e+01 -9.878641e+02]\n",
      "Weights: [-4.1879 -1.2154 -0.0834  0.1214  0.0735]\n",
      "MSE loss: 188.669\n",
      "Iteration: 18500\n",
      "Gradient: [  -11.5647   -11.442    -91.1529   383.779  -1387.6433]\n",
      "Weights: [-4.192  -1.2133 -0.0843  0.1214  0.0735]\n",
      "MSE loss: 188.4895\n",
      "Iteration: 18600\n",
      "Gradient: [  33.4837  -34.0842  197.0851 -245.3651 -914.0811]\n",
      "Weights: [-4.1927 -1.2117 -0.0853  0.1213  0.0736]\n",
      "MSE loss: 188.3086\n",
      "Iteration: 18700\n",
      "Gradient: [  11.382   -49.3633  238.6971 -139.1651 -704.0211]\n",
      "Weights: [-4.1898 -1.2101 -0.0863  0.1213  0.0737]\n",
      "MSE loss: 188.1135\n",
      "Iteration: 18800\n",
      "Gradient: [  46.3948   21.4139  -20.8408  -45.321  -216.6533]\n",
      "Weights: [-4.19   -1.2091 -0.0872  0.1213  0.0738]\n",
      "MSE loss: 187.9603\n",
      "Iteration: 18900\n",
      "Gradient: [ -6.0321 -14.415  -56.0516 146.489  644.5079]\n",
      "Weights: [-4.1919 -1.2069 -0.088   0.1212  0.0738]\n",
      "MSE loss: 187.7835\n",
      "Iteration: 19000\n",
      "Gradient: [  12.805   -46.4314  134.0675  248.5997 -121.6746]\n",
      "Weights: [-4.1914 -1.2052 -0.089   0.1211  0.0739]\n",
      "MSE loss: 187.5969\n",
      "Iteration: 19100\n",
      "Gradient: [  15.3627  -59.6395  -82.1074   37.1987 -319.8133]\n",
      "Weights: [-4.1925 -1.2031 -0.0899  0.121   0.074 ]\n",
      "MSE loss: 187.4101\n",
      "Iteration: 19200\n",
      "Gradient: [  34.9198    8.5135  139.4398  250.087  -422.6748]\n",
      "Weights: [-4.1939 -1.2018 -0.0909  0.1209  0.0741]\n",
      "MSE loss: 187.2302\n",
      "Iteration: 19300\n",
      "Gradient: [ 28.2337  18.6494 180.1655 -50.7203 298.3081]\n",
      "Weights: [-4.1921 -1.2002 -0.0919  0.1209  0.0742]\n",
      "MSE loss: 187.0407\n",
      "Iteration: 19400\n",
      "Gradient: [  -35.795    -35.2136   121.2305    17.9648 -1832.0151]\n",
      "Weights: [-4.1933 -1.199  -0.0929  0.1208  0.0742]\n",
      "MSE loss: 186.872\n",
      "Iteration: 19500\n",
      "Gradient: [ -51.4571  -24.4571   77.1618  435.5299 -233.5959]\n",
      "Weights: [-4.1948 -1.1973 -0.0937  0.1208  0.0743]\n",
      "MSE loss: 186.7017\n",
      "Iteration: 19600\n",
      "Gradient: [  79.6379 -115.3397  108.0978  232.376  -192.7624]\n",
      "Weights: [-4.1933 -1.1952 -0.0948  0.1207  0.0744]\n",
      "MSE loss: 186.5072\n",
      "Iteration: 19700\n",
      "Gradient: [  -3.7426   52.0454  106.3246 -378.0314 -242.3738]\n",
      "Weights: [-4.1951 -1.1933 -0.0959  0.1206  0.0745]\n",
      "MSE loss: 186.3072\n",
      "Iteration: 19800\n",
      "Gradient: [ -52.0069   78.2694   78.5065  174.3069 -963.1677]\n",
      "Weights: [-4.1971 -1.192  -0.0966  0.1206  0.0745]\n",
      "MSE loss: 186.1553\n",
      "Iteration: 19900\n",
      "Gradient: [  25.7512  -15.652   237.4401  569.1914 -927.5851]\n",
      "Weights: [-4.1957 -1.1901 -0.0976  0.1205  0.0746]\n",
      "MSE loss: 185.9682\n",
      "Iteration: 20000\n",
      "Gradient: [    4.0857  -100.6443    62.7057   587.2821 -1277.9528]\n",
      "Weights: [-4.1946 -1.1887 -0.0985  0.1205  0.0747]\n",
      "MSE loss: 185.803\n",
      "Iteration: 20100\n",
      "Gradient: [  -19.5558   -92.0624    37.4281  -163.3504 -1083.929 ]\n",
      "Weights: [-4.1974 -1.1871 -0.0993  0.1204  0.0748]\n",
      "MSE loss: 185.6361\n",
      "Iteration: 20200\n",
      "Gradient: [ 17.012   70.2144  74.2489 100.1381 251.4064]\n",
      "Weights: [-4.1958 -1.1855 -0.1003  0.1203  0.0748]\n",
      "MSE loss: 185.4614\n",
      "Iteration: 20300\n",
      "Gradient: [   -4.7243    -1.3954    97.5764    12.0716 -1138.6914]\n",
      "Weights: [-4.1975 -1.1841 -0.1012  0.1202  0.0749]\n",
      "MSE loss: 185.289\n",
      "Iteration: 20400\n",
      "Gradient: [-30.254  -19.2186   6.8789  13.5167 194.2897]\n",
      "Weights: [-4.1967 -1.1824 -0.1022  0.1201  0.075 ]\n",
      "MSE loss: 185.1041\n",
      "Iteration: 20500\n",
      "Gradient: [  -16.5337   -36.3032    15.3015   341.8964 -2481.0248]\n",
      "Weights: [-4.2021 -1.1806 -0.1029  0.1201  0.0751]\n",
      "MSE loss: 184.9554\n",
      "Iteration: 20600\n",
      "Gradient: [   15.228     41.5995     4.3766   260.4553 -1472.7628]\n",
      "Weights: [-4.2013 -1.179  -0.1038  0.1201  0.0751]\n",
      "MSE loss: 184.7913\n",
      "Iteration: 20700\n",
      "Gradient: [  27.603    76.6945  140.129   408.6848 -467.3422]\n",
      "Weights: [-4.1987 -1.1776 -0.1047  0.1199  0.0752]\n",
      "MSE loss: 184.6078\n",
      "Iteration: 20800\n",
      "Gradient: [ -26.2674   -9.4536   71.5789 -389.1834 -712.0008]\n",
      "Weights: [-4.2002 -1.1765 -0.1056  0.1199  0.0753]\n",
      "MSE loss: 184.4419\n",
      "Iteration: 20900\n",
      "Gradient: [   6.5896   21.9679  298.4154 -163.0481   20.0902]\n",
      "Weights: [-4.1991 -1.1748 -0.1066  0.1198  0.0754]\n",
      "MSE loss: 184.2612\n",
      "Iteration: 21000\n",
      "Gradient: [ -17.9643  -64.9231  220.3728  618.4337 -572.4136]\n",
      "Weights: [-4.2028 -1.1724 -0.1074  0.1197  0.0755]\n",
      "MSE loss: 184.0751\n",
      "Iteration: 21100\n",
      "Gradient: [    5.5807    20.6333   105.0364   361.1361 -1156.1874]\n",
      "Weights: [-4.2041 -1.1709 -0.1083  0.1196  0.0755]\n",
      "MSE loss: 183.9138\n",
      "Iteration: 21200\n",
      "Gradient: [    2.361    -44.5942    85.9309   105.0979 -1754.0437]\n",
      "Weights: [-4.2018 -1.1698 -0.1092  0.1195  0.0756]\n",
      "MSE loss: 183.7434\n",
      "Iteration: 21300\n",
      "Gradient: [  -18.01     -25.9808    11.8617  -124.7628 -1668.9129]\n",
      "Weights: [-4.2047 -1.168  -0.11    0.1195  0.0757]\n",
      "MSE loss: 183.5871\n",
      "Iteration: 21400\n",
      "Gradient: [ -15.0772  -43.6405   97.8847 -442.3938  473.9143]\n",
      "Weights: [-4.2034 -1.166  -0.111   0.1194  0.0758]\n",
      "MSE loss: 183.3833\n",
      "Iteration: 21500\n",
      "Gradient: [  8.9572 -51.6824  27.6496 176.3673  38.7112]\n",
      "Weights: [-4.2029 -1.1644 -0.1119  0.1193  0.0759]\n",
      "MSE loss: 183.2168\n",
      "Iteration: 21600\n",
      "Gradient: [   50.4383   -52.6298   167.7617   153.8819 -1303.8974]\n",
      "Weights: [-4.2044 -1.1624 -0.1128  0.1193  0.0759]\n",
      "MSE loss: 183.0393\n",
      "Iteration: 21700\n",
      "Gradient: [  13.4701  -55.612   114.5807  591.3282 -949.2815]\n",
      "Weights: [-4.2064 -1.1605 -0.1137  0.1192  0.076 ]\n",
      "MSE loss: 182.8735\n",
      "Iteration: 21800\n",
      "Gradient: [   -8.6243   -14.5538   144.6288   -73.6118 -1427.1197]\n",
      "Weights: [-4.2037 -1.1591 -0.1147  0.1191  0.0761]\n",
      "MSE loss: 182.7117\n",
      "Iteration: 21900\n",
      "Gradient: [   68.1784    -6.1038   253.2619   268.0201 -1382.3165]\n",
      "Weights: [-4.2051 -1.1577 -0.1156  0.1191  0.0761]\n",
      "MSE loss: 182.5375\n",
      "Iteration: 22000\n",
      "Gradient: [   -6.0919    -8.8141   -34.5592  -388.9318 -1210.8888]\n",
      "Weights: [-4.2065 -1.1561 -0.1165  0.119   0.0762]\n",
      "MSE loss: 182.3691\n",
      "Iteration: 22100\n",
      "Gradient: [  12.9952  -46.2445 -136.9197  480.5035 -753.6258]\n",
      "Weights: [-4.2069 -1.1541 -0.1175  0.1189  0.0763]\n",
      "MSE loss: 182.1805\n",
      "Iteration: 22200\n",
      "Gradient: [    3.3271   -30.5121     5.2972   367.6676 -1625.4958]\n",
      "Weights: [-4.2069 -1.152  -0.1185  0.1189  0.0764]\n",
      "MSE loss: 181.9932\n",
      "Iteration: 22300\n",
      "Gradient: [  -24.0096   -53.9815   126.8404   182.7722 -1476.113 ]\n",
      "Weights: [-4.2095 -1.1504 -0.1194  0.1188  0.0764]\n",
      "MSE loss: 181.8345\n",
      "Iteration: 22400\n",
      "Gradient: [  43.6786  -19.2051  181.597  -246.409  -431.5654]\n",
      "Weights: [-4.2109 -1.148  -0.1203  0.1187  0.0765]\n",
      "MSE loss: 181.637\n",
      "Iteration: 22500\n",
      "Gradient: [  -16.7112     6.1844   -82.7694   627.5225 -1335.5436]\n",
      "Weights: [-4.2113 -1.1461 -0.1212  0.1186  0.0766]\n",
      "MSE loss: 181.4597\n",
      "Iteration: 22600\n",
      "Gradient: [ 13.8756  11.8469 369.3045 489.3262 363.2567]\n",
      "Weights: [-4.2091 -1.1446 -0.1221  0.1185  0.0767]\n",
      "MSE loss: 181.2889\n",
      "Iteration: 22700\n",
      "Gradient: [  -5.071    15.847   140.3057 -238.8938 -905.4874]\n",
      "Weights: [-4.2118 -1.1424 -0.123   0.1184  0.0767]\n",
      "MSE loss: 181.1171\n",
      "Iteration: 22800\n",
      "Gradient: [   -7.1555  -134.5795    29.8406   -68.6124 -1559.4011]\n",
      "Weights: [-4.2114 -1.1408 -0.124   0.1183  0.0768]\n",
      "MSE loss: 180.9507\n",
      "Iteration: 22900\n",
      "Gradient: [ -64.5251  -25.1994  -40.4698  276.6514 -784.5998]\n",
      "Weights: [-4.2126 -1.1383 -0.1249  0.1183  0.0769]\n",
      "MSE loss: 180.777\n",
      "Iteration: 23000\n",
      "Gradient: [  -9.4525  -12.9972  148.9671  103.3915 -141.4438]\n",
      "Weights: [-4.2125 -1.1376 -0.1258  0.1182  0.0769]\n",
      "MSE loss: 180.6309\n",
      "Iteration: 23100\n",
      "Gradient: [   28.2409     9.7494   -83.5307    75.7024 -1112.1862]\n",
      "Weights: [-4.2149 -1.1352 -0.1266  0.1181  0.077 ]\n",
      "MSE loss: 180.4456\n",
      "Iteration: 23200\n",
      "Gradient: [  16.6146  113.2815   -3.6008  347.0362 -629.1774]\n",
      "Weights: [-4.2128 -1.1338 -0.1275  0.118   0.0771]\n",
      "MSE loss: 180.2975\n",
      "Iteration: 23300\n",
      "Gradient: [   -4.5466    23.8935    88.6336  -221.058  -1812.9374]\n",
      "Weights: [-4.2163 -1.1316 -0.1285  0.118   0.0772]\n",
      "MSE loss: 180.1154\n",
      "Iteration: 23400\n",
      "Gradient: [    7.2611    26.1531   109.7859   208.2013 -1474.7653]\n",
      "Weights: [-4.2169 -1.1297 -0.1295  0.1179  0.0772]\n",
      "MSE loss: 179.9388\n",
      "Iteration: 23500\n",
      "Gradient: [   9.3526  -78.7882  171.295  -304.6693  125.8624]\n",
      "Weights: [-4.2145 -1.1279 -0.1305  0.1179  0.0773]\n",
      "MSE loss: 179.7694\n",
      "Iteration: 23600\n",
      "Gradient: [   16.2855    25.756    191.9768   238.4813 -1193.9227]\n",
      "Weights: [-4.2156 -1.1263 -0.1315  0.1178  0.0774]\n",
      "MSE loss: 179.6044\n",
      "Iteration: 23700\n",
      "Gradient: [   34.228    -81.4467   107.9637   159.1662 -2458.6017]\n",
      "Weights: [-4.2173 -1.1249 -0.1323  0.1178  0.0774]\n",
      "MSE loss: 179.4479\n",
      "Iteration: 23800\n",
      "Gradient: [ -32.1027   22.3442  258.3033  182.4026 -660.1836]\n",
      "Weights: [-4.2185 -1.1233 -0.1331  0.1178  0.0775]\n",
      "MSE loss: 179.2831\n",
      "Iteration: 23900\n",
      "Gradient: [ -25.3842   -3.5197   72.7384  264.7675 -326.4963]\n",
      "Weights: [-4.2177 -1.1222 -0.1339  0.1177  0.0776]\n",
      "MSE loss: 179.1485\n",
      "Iteration: 24000\n",
      "Gradient: [-26.2248  19.3943 203.0254 -43.0173 184.6483]\n",
      "Weights: [-4.2183 -1.1201 -0.1347  0.1176  0.0777]\n",
      "MSE loss: 178.9848\n",
      "Iteration: 24100\n",
      "Gradient: [ -13.1931  -26.8993   21.8498  119.2356 -852.6919]\n",
      "Weights: [-4.2197 -1.1186 -0.1357  0.1175  0.0777]\n",
      "MSE loss: 178.8084\n",
      "Iteration: 24200\n",
      "Gradient: [   6.4963   16.9754  -21.1807  239.3234 -988.222 ]\n",
      "Weights: [-4.2196 -1.1175 -0.1367  0.1174  0.0778]\n",
      "MSE loss: 178.6352\n",
      "Iteration: 24300\n",
      "Gradient: [ -39.1723  -36.6223   24.269    47.6007 -602.0043]\n",
      "Weights: [-4.2202 -1.1159 -0.1376  0.1174  0.0779]\n",
      "MSE loss: 178.4696\n",
      "Iteration: 24400\n",
      "Gradient: [  22.9339   35.318    53.0929   28.4199 -372.7664]\n",
      "Weights: [-4.22   -1.1134 -0.1385  0.1174  0.078 ]\n",
      "MSE loss: 178.3037\n",
      "Iteration: 24500\n",
      "Gradient: [ -21.9354  -88.7132  122.252    33.1189 -580.2103]\n",
      "Weights: [-4.2209 -1.1118 -0.1394  0.1173  0.078 ]\n",
      "MSE loss: 178.146\n",
      "Iteration: 24600\n",
      "Gradient: [-34.0471   4.7365 158.7252  14.1344 183.7158]\n",
      "Weights: [-4.2221 -1.1096 -0.1403  0.1172  0.0781]\n",
      "MSE loss: 177.9751\n",
      "Iteration: 24700\n",
      "Gradient: [ -43.712   -65.412    -4.5521  639.6984 -475.2128]\n",
      "Weights: [-4.223  -1.1082 -0.1411  0.1172  0.0782]\n",
      "MSE loss: 177.8276\n",
      "Iteration: 24800\n",
      "Gradient: [  -27.3736   -27.5144    55.4883   -32.5006 -1188.8454]\n",
      "Weights: [-4.2249 -1.1061 -0.1419  0.1171  0.0782]\n",
      "MSE loss: 177.6767\n",
      "Iteration: 24900\n",
      "Gradient: [ 10.5767 -67.104   93.9943   7.7163 406.4432]\n",
      "Weights: [-4.2233 -1.1048 -0.1428  0.117   0.0783]\n",
      "MSE loss: 177.5108\n",
      "Iteration: 25000\n",
      "Gradient: [  18.0106   36.1843  241.1942  290.2749 -285.1745]\n",
      "Weights: [-4.2224 -1.1039 -0.1437  0.1169  0.0784]\n",
      "MSE loss: 177.3733\n",
      "Iteration: 25100\n",
      "Gradient: [ -43.3741   47.7972  145.5608 -226.4645 -327.5221]\n",
      "Weights: [-4.224  -1.1024 -0.1445  0.1169  0.0784]\n",
      "MSE loss: 177.215\n",
      "Iteration: 25200\n",
      "Gradient: [   2.3096  -46.2661  105.0065   22.2387 -629.8661]\n",
      "Weights: [-4.2221 -1.1016 -0.1453  0.1168  0.0785]\n",
      "MSE loss: 177.0779\n",
      "Iteration: 25300\n",
      "Gradient: [  -13.5143    -9.2196    95.9206   166.6199 -1190.7606]\n",
      "Weights: [-4.224  -1.0998 -0.1463  0.1168  0.0786]\n",
      "MSE loss: 176.9028\n",
      "Iteration: 25400\n",
      "Gradient: [  -3.5982  -15.4804   56.6969  190.9086 -464.4393]\n",
      "Weights: [-4.2261 -1.0979 -0.1471  0.1167  0.0786]\n",
      "MSE loss: 176.7576\n",
      "Iteration: 25500\n",
      "Gradient: [  -20.1992   -56.5895   145.7082   283.0187 -1346.3811]\n",
      "Weights: [-4.2264 -1.0962 -0.1479  0.1166  0.0787]\n",
      "MSE loss: 176.6086\n",
      "Iteration: 25600\n",
      "Gradient: [ 25.0306  25.7033  50.3965 -57.6396 103.9643]\n",
      "Weights: [-4.2235 -1.095  -0.1489  0.1166  0.0788]\n",
      "MSE loss: 176.4594\n",
      "Iteration: 25700\n",
      "Gradient: [ -15.8041   77.0967   76.1114  -87.2261 -204.9089]\n",
      "Weights: [-4.2264 -1.0939 -0.1496  0.1166  0.0788]\n",
      "MSE loss: 176.3166\n",
      "Iteration: 25800\n",
      "Gradient: [ 24.2043  76.1313  80.8895 325.5625 -52.0933]\n",
      "Weights: [-4.2262 -1.0919 -0.1506  0.1165  0.0789]\n",
      "MSE loss: 176.1427\n",
      "Iteration: 25900\n",
      "Gradient: [  -30.0861    70.3241   -17.5278    99.4149 -1327.4992]\n",
      "Weights: [-4.2272 -1.0902 -0.1514  0.1164  0.079 ]\n",
      "MSE loss: 175.9837\n",
      "Iteration: 26000\n",
      "Gradient: [ 1.4943800e+01  8.4540000e-01 -6.2447500e+01 -1.6188000e+00\n",
      " -2.5782833e+03]\n",
      "Weights: [-4.2274 -1.0887 -0.1521  0.1163  0.0791]\n",
      "MSE loss: 175.8404\n",
      "Iteration: 26100\n",
      "Gradient: [  25.9808  -39.657   174.7344 -247.2017  499.4997]\n",
      "Weights: [-4.2302 -1.0873 -0.1529  0.1162  0.0791]\n",
      "MSE loss: 175.6976\n",
      "Iteration: 26200\n",
      "Gradient: [  -43.4096    27.8396   -14.3198    94.0013 -1562.3783]\n",
      "Weights: [-4.2305 -1.0861 -0.1537  0.1162  0.0792]\n",
      "MSE loss: 175.5482\n",
      "Iteration: 26300\n",
      "Gradient: [ -54.2927  -50.6024   86.2113  369.2668 -899.6687]\n",
      "Weights: [-4.2295 -1.0848 -0.1545  0.1161  0.0793]\n",
      "MSE loss: 175.4034\n",
      "Iteration: 26400\n",
      "Gradient: [  27.7837  -28.8106   99.654   162.8622 -350.269 ]\n",
      "Weights: [-4.2293 -1.0832 -0.1555  0.116   0.0794]\n",
      "MSE loss: 175.2161\n",
      "Iteration: 26500\n",
      "Gradient: [  25.7509  -47.1624  106.614    47.3187 -749.2666]\n",
      "Weights: [-4.2292 -1.0819 -0.1563  0.116   0.0794]\n",
      "MSE loss: 175.065\n",
      "Iteration: 26600\n",
      "Gradient: [  -2.8086   19.3815  -21.7103  393.067  -600.9544]\n",
      "Weights: [-4.2294 -1.0799 -0.1571  0.1158  0.0795]\n",
      "MSE loss: 174.9148\n",
      "Iteration: 26700\n",
      "Gradient: [  11.3891  -22.9207  -72.6142  530.3919 -906.3497]\n",
      "Weights: [-4.232  -1.0785 -0.1579  0.1157  0.0796]\n",
      "MSE loss: 174.7547\n",
      "Iteration: 26800\n",
      "Gradient: [  -4.2778  -78.9912  165.9114 -291.6698 -761.0922]\n",
      "Weights: [-4.2333 -1.0757 -0.1587  0.1156  0.0797]\n",
      "MSE loss: 174.5838\n",
      "Iteration: 26900\n",
      "Gradient: [  27.1502   14.1185  -37.3021   58.3542 -711.1132]\n",
      "Weights: [-4.2325 -1.0742 -0.1597  0.1156  0.0797]\n",
      "MSE loss: 174.4308\n",
      "Iteration: 27000\n",
      "Gradient: [  -20.3684   -52.3781   246.0114  -296.3992 -1289.1876]\n",
      "Weights: [-4.2335 -1.0728 -0.1605  0.1155  0.0798]\n",
      "MSE loss: 174.2719\n",
      "Iteration: 27100\n",
      "Gradient: [ -15.461   -50.5873 -147.7996 -210.9003 -389.5902]\n",
      "Weights: [-4.2354 -1.0709 -0.1614  0.1155  0.0799]\n",
      "MSE loss: 174.1225\n",
      "Iteration: 27200\n",
      "Gradient: [ -31.274   -54.245   100.9293   44.5425 -288.2615]\n",
      "Weights: [-4.2341 -1.0696 -0.1622  0.1154  0.0799]\n",
      "MSE loss: 173.9786\n",
      "Iteration: 27300\n",
      "Gradient: [ 46.5197 -13.7873  99.2879  62.8027 316.7613]\n",
      "Weights: [-4.2312 -1.0683 -0.1632  0.1153  0.08  ]\n",
      "MSE loss: 173.8402\n",
      "Iteration: 27400\n",
      "Gradient: [ -13.9006  -44.8548  123.75   -351.3285 -575.2125]\n",
      "Weights: [-4.2351 -1.0666 -0.164   0.1153  0.0801]\n",
      "MSE loss: 173.6571\n",
      "Iteration: 27500\n",
      "Gradient: [ 26.0288 -27.3529  -1.0464 122.2258 900.3807]\n",
      "Weights: [-4.235  -1.0647 -0.165   0.1151  0.0801]\n",
      "MSE loss: 173.492\n",
      "Iteration: 27600\n",
      "Gradient: [   -3.228      2.4847   179.2392   191.8241 -2162.8835]\n",
      "Weights: [-4.2352 -1.0634 -0.1659  0.1151  0.0802]\n",
      "MSE loss: 173.3431\n",
      "Iteration: 27700\n",
      "Gradient: [  17.4326   23.9229   60.7557  129.8844 -592.2114]\n",
      "Weights: [-4.236  -1.0619 -0.1664  0.115   0.0803]\n",
      "MSE loss: 173.2109\n",
      "Iteration: 27800\n",
      "Gradient: [-33.039  -47.9972 161.4884 470.5701 746.3802]\n",
      "Weights: [-4.2384 -1.0603 -0.1673  0.115   0.0804]\n",
      "MSE loss: 173.069\n",
      "Iteration: 27900\n",
      "Gradient: [  37.0138  -35.0958   37.1134  -41.1154 -283.3964]\n",
      "Weights: [-4.2398 -1.0584 -0.168   0.115   0.0804]\n",
      "MSE loss: 172.9359\n",
      "Iteration: 28000\n",
      "Gradient: [   -8.391     43.6214   168.9391  -230.3528 -1186.7336]\n",
      "Weights: [-4.2378 -1.0571 -0.1688  0.1149  0.0805]\n",
      "MSE loss: 172.7941\n",
      "Iteration: 28100\n",
      "Gradient: [ -69.3891  -11.2726  141.0573 -291.7384 -898.0054]\n",
      "Weights: [-4.2397 -1.0558 -0.1698  0.1148  0.0805]\n",
      "MSE loss: 172.6359\n",
      "Iteration: 28200\n",
      "Gradient: [  14.7854  -27.5738 -103.3149 -183.1188 -263.0595]\n",
      "Weights: [-4.24   -1.0538 -0.1707  0.1147  0.0806]\n",
      "MSE loss: 172.4654\n",
      "Iteration: 28300\n",
      "Gradient: [  -8.3967   66.9549  163.3695 -279.0538  -67.2664]\n",
      "Weights: [-4.2415 -1.052  -0.1714  0.1147  0.0807]\n",
      "MSE loss: 172.3206\n",
      "Iteration: 28400\n",
      "Gradient: [ 15.0278  23.6799 141.0635 383.3639 736.0226]\n",
      "Weights: [-4.2412 -1.0502 -0.1723  0.1146  0.0808]\n",
      "MSE loss: 172.1684\n",
      "Iteration: 28500\n",
      "Gradient: [ 1.183000e-01 -5.604210e+01  1.575859e+02  1.556569e+02 -7.133984e+02]\n",
      "Weights: [-4.2424 -1.0489 -0.1731  0.1145  0.0808]\n",
      "MSE loss: 172.0302\n",
      "Iteration: 28600\n",
      "Gradient: [  -3.6366  -84.4384  157.5103 -386.3808 -312.114 ]\n",
      "Weights: [-4.244  -1.0474 -0.1741  0.1145  0.0809]\n",
      "MSE loss: 171.8769\n",
      "Iteration: 28700\n",
      "Gradient: [  -18.0954    -3.6179   313.6503   138.3632 -1130.0274]\n",
      "Weights: [-4.2419 -1.0464 -0.1748  0.1144  0.081 ]\n",
      "MSE loss: 171.7434\n",
      "Iteration: 28800\n",
      "Gradient: [  17.1143    2.7854  205.9491  280.0672 -512.5258]\n",
      "Weights: [-4.2432 -1.0444 -0.1758  0.1143  0.081 ]\n",
      "MSE loss: 171.5781\n",
      "Iteration: 28900\n",
      "Gradient: [  13.0216 -123.0502  118.5041  239.3709 -112.4509]\n",
      "Weights: [-4.2434 -1.0425 -0.1766  0.1143  0.0811]\n",
      "MSE loss: 171.4264\n",
      "Iteration: 29000\n",
      "Gradient: [   2.5163   48.6078   41.1254  -29.8578 -925.9624]\n",
      "Weights: [-4.2444 -1.0407 -0.1773  0.1142  0.0812]\n",
      "MSE loss: 171.2898\n",
      "Iteration: 29100\n",
      "Gradient: [ 34.9128 -53.5158  69.5091 -53.6404  15.3091]\n",
      "Weights: [-4.2416 -1.0395 -0.1783  0.114   0.0812]\n",
      "MSE loss: 171.1342\n",
      "Iteration: 29200\n",
      "Gradient: [    9.6576   -14.1678    82.3727   132.1925 -1623.9528]\n",
      "Weights: [-4.2435 -1.038  -0.1791  0.114   0.0813]\n",
      "MSE loss: 170.979\n",
      "Iteration: 29300\n",
      "Gradient: [ -26.4898  100.3218  -42.6988  -71.3806 -713.222 ]\n",
      "Weights: [-4.2438 -1.0366 -0.1799  0.1139  0.0814]\n",
      "MSE loss: 170.8394\n",
      "Iteration: 29400\n",
      "Gradient: [  -46.401    -15.7866   263.1577   -19.0997 -1121.1739]\n",
      "Weights: [-4.2454 -1.035  -0.1807  0.1139  0.0814]\n",
      "MSE loss: 170.7006\n",
      "Iteration: 29500\n",
      "Gradient: [   11.2902   -77.4937   -26.3257   357.7825 -1413.4733]\n",
      "Weights: [-4.2452 -1.0336 -0.1814  0.1139  0.0815]\n",
      "MSE loss: 170.5762\n",
      "Iteration: 29600\n",
      "Gradient: [  -21.5545  -124.4977   121.6781  -123.2428 -1441.3873]\n",
      "Weights: [-4.2452 -1.0326 -0.1823  0.1138  0.0816]\n",
      "MSE loss: 170.4304\n",
      "Iteration: 29700\n",
      "Gradient: [ 52.7023  30.3648  21.6258 446.9771  37.5239]\n",
      "Weights: [-4.2456 -1.0308 -0.1832  0.1137  0.0816]\n",
      "MSE loss: 170.2793\n",
      "Iteration: 29800\n",
      "Gradient: [  59.0021  -22.9027  281.192  -184.7449   61.6288]\n",
      "Weights: [-4.2484 -1.0289 -0.1839  0.1137  0.0817]\n",
      "MSE loss: 170.1351\n",
      "Iteration: 29900\n",
      "Gradient: [  19.847    52.9637  129.841    -4.1675 -748.286 ]\n",
      "Weights: [-4.2463 -1.0276 -0.1847  0.1136  0.0818]\n",
      "MSE loss: 170.0085\n",
      "Iteration: 30000\n",
      "Gradient: [   -6.4696   -45.9533     4.9341   -26.3551 -1457.7727]\n",
      "Weights: [-4.2501 -1.0257 -0.1855  0.1136  0.0818]\n",
      "MSE loss: 169.8586\n",
      "Iteration: 30100\n",
      "Gradient: [  13.9897  -50.6466  108.2565  -52.1914 -853.1222]\n",
      "Weights: [-4.2497 -1.0241 -0.1862  0.1135  0.0819]\n",
      "MSE loss: 169.712\n",
      "Iteration: 30200\n",
      "Gradient: [  19.7172  -14.56     37.308  -106.7841 -546.709 ]\n",
      "Weights: [-4.2505 -1.0229 -0.187   0.1134  0.082 ]\n",
      "MSE loss: 169.5792\n",
      "Iteration: 30300\n",
      "Gradient: [   5.6149   34.3836    3.3146  108.6511 -135.9178]\n",
      "Weights: [-4.2515 -1.0217 -0.1879  0.1133  0.082 ]\n",
      "MSE loss: 169.438\n",
      "Iteration: 30400\n",
      "Gradient: [   22.3151   -13.1936    65.5371   289.7241 -1758.1536]\n",
      "Weights: [-4.2506 -1.0197 -0.1886  0.1132  0.0821]\n",
      "MSE loss: 169.2883\n",
      "Iteration: 30500\n",
      "Gradient: [ -5.6833 -38.9725   9.5733 544.5684 411.4602]\n",
      "Weights: [-4.2508 -1.019  -0.1895  0.1131  0.0822]\n",
      "MSE loss: 169.1517\n",
      "Iteration: 30600\n",
      "Gradient: [ -39.6648   -8.6606  157.902   359.7299 -704.2216]\n",
      "Weights: [-4.251  -1.0174 -0.1902  0.1131  0.0823]\n",
      "MSE loss: 169.0164\n",
      "Iteration: 30700\n",
      "Gradient: [  6.3598 -47.5251  35.5956 118.6553 -32.8985]\n",
      "Weights: [-4.2514 -1.0163 -0.1912  0.1131  0.0823]\n",
      "MSE loss: 168.8645\n",
      "Iteration: 30800\n",
      "Gradient: [  22.9329  -55.6463  -23.6479 -134.4284 -403.7845]\n",
      "Weights: [-4.2511 -1.0153 -0.192   0.113   0.0824]\n",
      "MSE loss: 168.733\n",
      "Iteration: 30900\n",
      "Gradient: [ -29.2201   69.2091   71.5828  185.6344 -557.8171]\n",
      "Weights: [-4.252  -1.0129 -0.1929  0.113   0.0825]\n",
      "MSE loss: 168.5772\n",
      "Iteration: 31000\n",
      "Gradient: [  27.9291   22.6177  251.1635  379.9385 -861.385 ]\n",
      "Weights: [-4.2533 -1.0105 -0.1938  0.1129  0.0825]\n",
      "MSE loss: 168.4158\n",
      "Iteration: 31100\n",
      "Gradient: [  27.466  -100.628    58.1218  132.3701  283.2313]\n",
      "Weights: [-4.2545 -1.0092 -0.1946  0.1129  0.0826]\n",
      "MSE loss: 168.2737\n",
      "Iteration: 31200\n",
      "Gradient: [ -30.6844  -74.0613    1.8783  488.3986 -867.7239]\n",
      "Weights: [-4.2564 -1.0076 -0.1955  0.1128  0.0827]\n",
      "MSE loss: 168.1246\n",
      "Iteration: 31300\n",
      "Gradient: [ 1.025500e+00 -1.842290e+01 -7.381010e+01  1.343286e+02 -1.268476e+03]\n",
      "Weights: [-4.2551 -1.0057 -0.1963  0.1127  0.0827]\n",
      "MSE loss: 167.9793\n",
      "Iteration: 31400\n",
      "Gradient: [  24.5303   49.662   299.1092 -206.892   -79.9781]\n",
      "Weights: [-4.2553 -1.0039 -0.1971  0.1126  0.0828]\n",
      "MSE loss: 167.8342\n",
      "Iteration: 31500\n",
      "Gradient: [ -16.6023  -57.2371   61.1561  201.8771 -382.8072]\n",
      "Weights: [-4.2564 -1.0024 -0.198   0.1125  0.0829]\n",
      "MSE loss: 167.6822\n",
      "Iteration: 31600\n",
      "Gradient: [    8.7509   -48.7212    38.7748    84.0107 -1548.9915]\n",
      "Weights: [-4.2556 -1.0009 -0.1988  0.1124  0.0829]\n",
      "MSE loss: 167.5432\n",
      "Iteration: 31700\n",
      "Gradient: [ -20.6479  -56.8545  251.4976 -320.5393  732.1249]\n",
      "Weights: [-4.2559 -0.9992 -0.1997  0.1124  0.083 ]\n",
      "MSE loss: 167.4043\n",
      "Iteration: 31800\n",
      "Gradient: [  -5.2618  -13.5717   -4.3236 -169.5155 -300.2582]\n",
      "Weights: [-4.256  -0.9976 -0.2006  0.1123  0.0831]\n",
      "MSE loss: 167.2522\n",
      "Iteration: 31900\n",
      "Gradient: [   15.6261   -17.5885   249.8167   162.3671 -1377.1322]\n",
      "Weights: [-4.2575 -0.9961 -0.2013  0.1122  0.0831]\n",
      "MSE loss: 167.1109\n",
      "Iteration: 32000\n",
      "Gradient: [ -19.9316  -71.0024  306.8559  522.4546 -495.1938]\n",
      "Weights: [-4.2596 -0.9944 -0.2021  0.1122  0.0832]\n",
      "MSE loss: 166.9674\n",
      "Iteration: 32100\n",
      "Gradient: [-2.897400e+00 -6.838000e-01 -3.812580e+01  2.137198e+02 -8.473335e+02]\n",
      "Weights: [-4.2605 -0.9929 -0.2029  0.1121  0.0833]\n",
      "MSE loss: 166.8339\n",
      "Iteration: 32200\n",
      "Gradient: [ -40.2087   21.5549   35.351  -128.0014 -264.2557]\n",
      "Weights: [-4.261  -0.9913 -0.2037  0.1121  0.0833]\n",
      "MSE loss: 166.6922\n",
      "Iteration: 32300\n",
      "Gradient: [   57.3464   -21.4882   -43.7323    88.9114 -1059.8975]\n",
      "Weights: [-4.26   -0.99   -0.2045  0.112   0.0834]\n",
      "MSE loss: 166.5655\n",
      "Iteration: 32400\n",
      "Gradient: [  65.0531   29.248    -5.631   396.8278 -623.7635]\n",
      "Weights: [-4.2604 -0.9881 -0.2055  0.112   0.0835]\n",
      "MSE loss: 166.4164\n",
      "Iteration: 32500\n",
      "Gradient: [   4.2571  -46.7406  -44.4543 -153.7498 -948.107 ]\n",
      "Weights: [-4.2639 -0.9864 -0.2063  0.1119  0.0835]\n",
      "MSE loss: 166.2774\n",
      "Iteration: 32600\n",
      "Gradient: [ -21.8399  -69.436   209.5135  151.2089 -879.7809]\n",
      "Weights: [-4.2616 -0.9848 -0.2073  0.1119  0.0836]\n",
      "MSE loss: 166.1135\n",
      "Iteration: 32700\n",
      "Gradient: [  -31.8085   -16.1154   139.186    104.8199 -1326.1053]\n",
      "Weights: [-4.262  -0.9835 -0.2082  0.1118  0.0837]\n",
      "MSE loss: 165.9648\n",
      "Iteration: 32800\n",
      "Gradient: [  10.5452  -71.2699  -16.133  -219.083  -261.1107]\n",
      "Weights: [-4.2614 -0.9821 -0.2092  0.1117  0.0837]\n",
      "MSE loss: 165.817\n",
      "Iteration: 32900\n",
      "Gradient: [ 13.5702  -0.7666 131.2788 305.2648 -20.4803]\n",
      "Weights: [-4.2626 -0.9806 -0.2099  0.1117  0.0838]\n",
      "MSE loss: 165.6921\n",
      "Iteration: 33000\n",
      "Gradient: [  -10.1307   -58.2815   226.6901   298.6412 -1239.2913]\n",
      "Weights: [-4.2643 -0.9796 -0.2106  0.1117  0.0839]\n",
      "MSE loss: 165.5617\n",
      "Iteration: 33100\n",
      "Gradient: [   19.9932   -99.6826    50.5286  -188.975  -1893.5241]\n",
      "Weights: [-4.2636 -0.9781 -0.2114  0.1116  0.0839]\n",
      "MSE loss: 165.4224\n",
      "Iteration: 33200\n",
      "Gradient: [  -38.8966    -1.2383   207.6424   321.9688 -1232.9847]\n",
      "Weights: [-4.2631 -0.9768 -0.2124  0.1116  0.084 ]\n",
      "MSE loss: 165.2683\n",
      "Iteration: 33300\n",
      "Gradient: [   77.2457    33.8387   132.1879  -322.3934 -1326.1911]\n",
      "Weights: [-4.2611 -0.9759 -0.2132  0.1115  0.0841]\n",
      "MSE loss: 165.1585\n",
      "Iteration: 33400\n",
      "Gradient: [   6.3019  -50.5834   21.5714   71.0215 1281.4886]\n",
      "Weights: [-4.2658 -0.9741 -0.2139  0.1115  0.0842]\n",
      "MSE loss: 165.0145\n",
      "Iteration: 33500\n",
      "Gradient: [ -70.9781    1.8808  196.045  -323.9226 -506.9799]\n",
      "Weights: [-4.2684 -0.9716 -0.2145  0.1114  0.0842]\n",
      "MSE loss: 164.884\n",
      "Iteration: 33600\n",
      "Gradient: [  6.855   29.4111 106.7429 361.7096 134.7266]\n",
      "Weights: [-4.2656 -0.9703 -0.2155  0.1113  0.0843]\n",
      "MSE loss: 164.7334\n",
      "Iteration: 33700\n",
      "Gradient: [ -17.9088   34.1671   83.1421  111.4015 -115.3093]\n",
      "Weights: [-4.2667 -0.9689 -0.2164  0.1112  0.0843]\n",
      "MSE loss: 164.601\n",
      "Iteration: 33800\n",
      "Gradient: [  42.9097  -72.5452  138.0237  147.3139 -280.5395]\n",
      "Weights: [-4.2678 -0.9678 -0.2171  0.1112  0.0844]\n",
      "MSE loss: 164.4838\n",
      "Iteration: 33900\n",
      "Gradient: [  43.7111  -16.6177   80.5138 -118.2863 -203.8266]\n",
      "Weights: [-4.268  -0.9656 -0.218   0.1112  0.0844]\n",
      "MSE loss: 164.3439\n",
      "Iteration: 34000\n",
      "Gradient: [  39.116   -23.759   148.6274 -438.4403  -34.6495]\n",
      "Weights: [-4.2683 -0.964  -0.2188  0.1111  0.0845]\n",
      "MSE loss: 164.2117\n",
      "Iteration: 34100\n",
      "Gradient: [    5.4496   -13.828     98.6404   125.1087 -1215.115 ]\n",
      "Weights: [-4.2702 -0.9624 -0.2195  0.1111  0.0846]\n",
      "MSE loss: 164.0792\n",
      "Iteration: 34200\n",
      "Gradient: [   5.9663  -61.3616  -32.3106  323.8676 -721.5505]\n",
      "Weights: [-4.27   -0.9613 -0.2203  0.111   0.0846]\n",
      "MSE loss: 163.9567\n",
      "Iteration: 34300\n",
      "Gradient: [   30.2988    68.3639   -28.4833   -69.2358 -1572.7724]\n",
      "Weights: [-4.2709 -0.9596 -0.2211  0.1109  0.0847]\n",
      "MSE loss: 163.825\n",
      "Iteration: 34400\n",
      "Gradient: [ -27.0716   -7.4235  362.1627  165.8763 -792.1504]\n",
      "Weights: [-4.2718 -0.9575 -0.2219  0.1109  0.0848]\n",
      "MSE loss: 163.6843\n",
      "Iteration: 34500\n",
      "Gradient: [ 3.262900e+00 -4.803000e-01  1.483500e+02  3.045164e+02 -6.883554e+02]\n",
      "Weights: [-4.2719 -0.9562 -0.2227  0.1108  0.0848]\n",
      "MSE loss: 163.5486\n",
      "Iteration: 34600\n",
      "Gradient: [   -2.3599   -55.2429   216.2107    82.2608 -1207.3694]\n",
      "Weights: [-4.2726 -0.955  -0.2234  0.1107  0.0849]\n",
      "MSE loss: 163.4299\n",
      "Iteration: 34700\n",
      "Gradient: [  -10.8007   -26.4613   266.9036   223.0738 -1362.3028]\n",
      "Weights: [-4.271  -0.9538 -0.2242  0.1106  0.085 ]\n",
      "MSE loss: 163.2976\n",
      "Iteration: 34800\n",
      "Gradient: [ -9.2128  10.1419 175.6284 391.5345 -74.6396]\n",
      "Weights: [-4.2707 -0.9526 -0.225   0.1106  0.085 ]\n",
      "MSE loss: 163.1725\n",
      "Iteration: 34900\n",
      "Gradient: [  77.658   -78.4332   96.1987  119.5759 -740.285 ]\n",
      "Weights: [-4.2714 -0.9514 -0.2258  0.1105  0.0851]\n",
      "MSE loss: 163.0387\n",
      "Iteration: 35000\n",
      "Gradient: [-12.6767  35.1947  90.5623  -9.2566 820.1603]\n",
      "Weights: [-4.2724 -0.95   -0.2265  0.1105  0.0851]\n",
      "MSE loss: 162.9235\n",
      "Iteration: 35100\n",
      "Gradient: [-16.9131 -51.3743  14.9642  -6.9758 478.7114]\n",
      "Weights: [-4.2744 -0.9488 -0.2272  0.1104  0.0852]\n",
      "MSE loss: 162.8106\n",
      "Iteration: 35200\n",
      "Gradient: [ -22.7351  -63.1615  170.161  -205.329  -631.9325]\n",
      "Weights: [-4.275  -0.9472 -0.228   0.1104  0.0853]\n",
      "MSE loss: 162.6726\n",
      "Iteration: 35300\n",
      "Gradient: [  37.0022   15.3167  136.7345 -699.5086 -498.7261]\n",
      "Weights: [-4.2746 -0.9451 -0.2288  0.1103  0.0853]\n",
      "MSE loss: 162.5331\n",
      "Iteration: 35400\n",
      "Gradient: [-10.1372  -2.6387 146.4185 124.9129 -71.8066]\n",
      "Weights: [-4.2748 -0.9433 -0.2298  0.1102  0.0854]\n",
      "MSE loss: 162.3852\n",
      "Iteration: 35500\n",
      "Gradient: [ -16.5866  -14.2228   34.5388  -64.6253 -515.6226]\n",
      "Weights: [-4.2754 -0.9415 -0.2306  0.1102  0.0855]\n",
      "MSE loss: 162.242\n",
      "Iteration: 35600\n",
      "Gradient: [   16.3694   -14.5837   153.2503   161.4406 -1691.7293]\n",
      "Weights: [-4.276  -0.9397 -0.2315  0.1101  0.0855]\n",
      "MSE loss: 162.1054\n",
      "Iteration: 35700\n",
      "Gradient: [ -30.4074  -24.2739  -34.5531  343.4551 -190.6439]\n",
      "Weights: [-4.276  -0.938  -0.2324  0.1101  0.0856]\n",
      "MSE loss: 161.9648\n",
      "Iteration: 35800\n",
      "Gradient: [  -51.5665   -63.4246   -42.9256   -94.0752 -1158.9446]\n",
      "Weights: [-4.2766 -0.9373 -0.2332  0.11    0.0856]\n",
      "MSE loss: 161.8431\n",
      "Iteration: 35900\n",
      "Gradient: [    3.8227  -123.5964   117.9869  -364.2202 -1286.5855]\n",
      "Weights: [-4.2778 -0.9355 -0.2341  0.11    0.0857]\n",
      "MSE loss: 161.7063\n",
      "Iteration: 36000\n",
      "Gradient: [-16.0211   7.3082 129.1158  83.1427 270.7253]\n",
      "Weights: [-4.2778 -0.9342 -0.2348  0.1099  0.0858]\n",
      "MSE loss: 161.5743\n",
      "Iteration: 36100\n",
      "Gradient: [  29.9886  -43.9375  221.7088  197.3368 -127.5796]\n",
      "Weights: [-4.2791 -0.9325 -0.2356  0.1098  0.0858]\n",
      "MSE loss: 161.4399\n",
      "Iteration: 36200\n",
      "Gradient: [   11.3638     8.7751   -44.98    -205.8717 -1403.1031]\n",
      "Weights: [-4.2785 -0.9312 -0.2364  0.1097  0.0859]\n",
      "MSE loss: 161.3017\n",
      "Iteration: 36300\n",
      "Gradient: [ -13.4217   28.8487  168.9816 -290.1459 -527.4638]\n",
      "Weights: [-4.2786 -0.9305 -0.2374  0.1097  0.086 ]\n",
      "MSE loss: 161.1656\n",
      "Iteration: 36400\n",
      "Gradient: [ 32.4846 -37.5173 177.7893  -6.6988 441.94  ]\n",
      "Weights: [-4.2784 -0.9285 -0.2382  0.1097  0.0861]\n",
      "MSE loss: 161.0194\n",
      "Iteration: 36500\n",
      "Gradient: [ 4.1000000e-01 -6.9629500e+01  2.1941940e+02 -2.4586600e+01\n",
      " -1.3633136e+03]\n",
      "Weights: [-4.2789 -0.9268 -0.2391  0.1096  0.0861]\n",
      "MSE loss: 160.8868\n",
      "Iteration: 36600\n",
      "Gradient: [ 13.3643 -98.398   54.0274 220.236   25.3781]\n",
      "Weights: [-4.2804 -0.9246 -0.24    0.1096  0.0862]\n",
      "MSE loss: 160.7356\n",
      "Iteration: 36700\n",
      "Gradient: [ -42.3481   -1.799    67.4023 -352.6089 -903.5544]\n",
      "Weights: [-4.282  -0.9229 -0.2409  0.1095  0.0863]\n",
      "MSE loss: 160.5861\n",
      "Iteration: 36800\n",
      "Gradient: [ -68.0619   32.5868    8.0629   60.1391 -207.9428]\n",
      "Weights: [-4.282  -0.9216 -0.2417  0.1095  0.0863]\n",
      "MSE loss: 160.4662\n",
      "Iteration: 36900\n",
      "Gradient: [ -32.0658   -2.4035   88.0677  -34.0749 -363.4566]\n",
      "Weights: [-4.2811 -0.9201 -0.2427  0.1094  0.0864]\n",
      "MSE loss: 160.3282\n",
      "Iteration: 37000\n",
      "Gradient: [ 2.8870000e-01 -7.9717600e+01 -2.2342500e+01 -2.9048300e+01\n",
      " -2.0063298e+03]\n",
      "Weights: [-4.2829 -0.9188 -0.2434  0.1094  0.0865]\n",
      "MSE loss: 160.2041\n",
      "Iteration: 37100\n",
      "Gradient: [  10.1065  -25.361   -60.711  -208.8096 -939.801 ]\n",
      "Weights: [-4.2841 -0.9173 -0.2442  0.1094  0.0865]\n",
      "MSE loss: 160.0743\n",
      "Iteration: 37200\n",
      "Gradient: [   1.0507  -11.6011  201.1652 -345.5722 -168.678 ]\n",
      "Weights: [-4.2825 -0.9157 -0.245   0.1093  0.0866]\n",
      "MSE loss: 159.9402\n",
      "Iteration: 37300\n",
      "Gradient: [  29.5356  -40.367    16.4409 -108.3741 -158.1111]\n",
      "Weights: [-4.2849 -0.9145 -0.2457  0.1092  0.0867]\n",
      "MSE loss: 159.821\n",
      "Iteration: 37400\n",
      "Gradient: [ -54.0399  -41.3183   80.9987  428.7034 -586.9297]\n",
      "Weights: [-4.2866 -0.9131 -0.2464  0.1092  0.0867]\n",
      "MSE loss: 159.6932\n",
      "Iteration: 37500\n",
      "Gradient: [ 26.3949 -22.5261  54.9603 178.1523 140.3815]\n",
      "Weights: [-4.2871 -0.9111 -0.2472  0.1091  0.0868]\n",
      "MSE loss: 159.5643\n",
      "Iteration: 37600\n",
      "Gradient: [ 6.3838800e+01  4.7960000e-01  1.1628110e+02  2.3813100e+01\n",
      " -1.2099517e+03]\n",
      "Weights: [-4.2858 -0.9099 -0.2479  0.1091  0.0868]\n",
      "MSE loss: 159.4554\n",
      "Iteration: 37700\n",
      "Gradient: [   -9.5218   -27.6613    68.1788   173.689  -1444.9519]\n",
      "Weights: [-4.2877 -0.9081 -0.2488  0.109   0.0869]\n",
      "MSE loss: 159.3159\n",
      "Iteration: 37800\n",
      "Gradient: [  -5.9329   40.0002   82.4107  114.5357 -840.0916]\n",
      "Weights: [-4.2877 -0.9069 -0.2497  0.109   0.0869]\n",
      "MSE loss: 159.197\n",
      "Iteration: 37900\n",
      "Gradient: [   28.5514    23.8023   196.9854   -11.0593 -1033.1579]\n",
      "Weights: [-4.2878 -0.9052 -0.2505  0.1089  0.087 ]\n",
      "MSE loss: 159.064\n",
      "Iteration: 38000\n",
      "Gradient: [   52.1914    -5.5539    52.9462   324.4458 -1655.0621]\n",
      "Weights: [-4.2883 -0.9033 -0.2514  0.1088  0.0871]\n",
      "MSE loss: 158.9267\n",
      "Iteration: 38100\n",
      "Gradient: [  -20.9978   -26.5301   122.18    -430.1977 -1385.6576]\n",
      "Weights: [-4.2883 -0.9017 -0.2521  0.1088  0.0871]\n",
      "MSE loss: 158.8141\n",
      "Iteration: 38200\n",
      "Gradient: [  -14.6531    44.5906   110.0524   -23.1315 -1451.4019]\n",
      "Weights: [-4.2894 -0.9006 -0.2528  0.1087  0.0872]\n",
      "MSE loss: 158.6965\n",
      "Iteration: 38300\n",
      "Gradient: [  -13.3869    23.8418    49.8612     6.0265 -1390.274 ]\n",
      "Weights: [-4.2882 -0.8999 -0.2536  0.1087  0.0872]\n",
      "MSE loss: 158.5889\n",
      "Iteration: 38400\n",
      "Gradient: [  36.82    -31.079   130.6799  403.5509 -286.0086]\n",
      "Weights: [-4.2901 -0.8979 -0.2543  0.1087  0.0873]\n",
      "MSE loss: 158.4517\n",
      "Iteration: 38500\n",
      "Gradient: [  -9.4329  -46.476   -44.0122  209.2099 -668.3629]\n",
      "Weights: [-4.2905 -0.8964 -0.2553  0.1087  0.0874]\n",
      "MSE loss: 158.3161\n",
      "Iteration: 38600\n",
      "Gradient: [ -23.9904  -93.8599  172.5746  158.726  -302.5835]\n",
      "Weights: [-4.2922 -0.895  -0.256   0.1086  0.0874]\n",
      "MSE loss: 158.2046\n",
      "Iteration: 38700\n",
      "Gradient: [   3.2207  -53.6423   63.0861 -260.3427 -527.9333]\n",
      "Weights: [-4.2926 -0.8932 -0.2567  0.1086  0.0875]\n",
      "MSE loss: 158.0835\n",
      "Iteration: 38800\n",
      "Gradient: [  1.3916  19.7173 158.0904 448.3848 143.6312]\n",
      "Weights: [-4.2925 -0.8916 -0.2575  0.1085  0.0875]\n",
      "MSE loss: 157.9604\n",
      "Iteration: 38900\n",
      "Gradient: [  27.0831   -7.8473   94.1549 -182.4478 -346.4284]\n",
      "Weights: [-4.2924 -0.8903 -0.2585  0.1084  0.0876]\n",
      "MSE loss: 157.8301\n",
      "Iteration: 39000\n",
      "Gradient: [-3.278000e-01 -6.225800e+01 -9.314070e+01  3.375988e+02 -9.331030e+01]\n",
      "Weights: [-4.2939 -0.8886 -0.2592  0.1084  0.0877]\n",
      "MSE loss: 157.7074\n",
      "Iteration: 39100\n",
      "Gradient: [  -40.6357   -34.219    148.9342  -103.0018 -1132.3399]\n",
      "Weights: [-4.2955 -0.8872 -0.26    0.1084  0.0877]\n",
      "MSE loss: 157.5922\n",
      "Iteration: 39200\n",
      "Gradient: [   2.8453   -3.0965   28.6081 -108.0275 -745.2504]\n",
      "Weights: [-4.2935 -0.8859 -0.2608  0.1083  0.0878]\n",
      "MSE loss: 157.467\n",
      "Iteration: 39300\n",
      "Gradient: [  -22.9048   -76.8476    30.255   -133.8066 -1829.799 ]\n",
      "Weights: [-4.2949 -0.8845 -0.2615  0.1082  0.0878]\n",
      "MSE loss: 157.3424\n",
      "Iteration: 39400\n",
      "Gradient: [ -12.092   -29.1394  117.1882  -44.0675 -733.7798]\n",
      "Weights: [-4.2958 -0.8827 -0.2623  0.1082  0.0879]\n",
      "MSE loss: 157.2198\n",
      "Iteration: 39500\n",
      "Gradient: [ -29.0895  -48.1827  219.5752  106.1567 -957.196 ]\n",
      "Weights: [-4.2967 -0.8805 -0.2629  0.1081  0.088 ]\n",
      "MSE loss: 157.1012\n",
      "Iteration: 39600\n",
      "Gradient: [ -16.4149  -34.8421  237.476   108.9346 -366.7193]\n",
      "Weights: [-4.2958 -0.8798 -0.2636  0.108   0.088 ]\n",
      "MSE loss: 157.004\n",
      "Iteration: 39700\n",
      "Gradient: [ -34.287   -27.0417  178.0663  431.731  -898.1657]\n",
      "Weights: [-4.2962 -0.8791 -0.2644  0.108   0.0881]\n",
      "MSE loss: 156.8912\n",
      "Iteration: 39800\n",
      "Gradient: [   33.0637    31.6968    28.5465   268.2943 -1416.8753]\n",
      "Weights: [-4.2985 -0.8771 -0.265   0.1079  0.0881]\n",
      "MSE loss: 156.7673\n",
      "Iteration: 39900\n",
      "Gradient: [-29.6346  26.1998  48.5441 -23.0824 112.4688]\n",
      "Weights: [-4.2971 -0.8763 -0.266   0.1079  0.0882]\n",
      "MSE loss: 156.6367\n",
      "Iteration: 40000\n",
      "Gradient: [ 19.2145  16.6811   6.0965 484.2619 285.8819]\n",
      "Weights: [-4.2962 -0.8754 -0.2667  0.1078  0.0883]\n",
      "MSE loss: 156.5338\n",
      "Iteration: 40100\n",
      "Gradient: [  44.3157   24.0267 -108.1322  -15.1604   55.4969]\n",
      "Weights: [-4.2957 -0.8742 -0.2675  0.1078  0.0883]\n",
      "MSE loss: 156.4192\n",
      "Iteration: 40200\n",
      "Gradient: [ 30.2682 -18.3629  21.2636 537.3401 396.35  ]\n",
      "Weights: [-4.296  -0.8725 -0.2683  0.1078  0.0884]\n",
      "MSE loss: 156.2973\n",
      "Iteration: 40300\n",
      "Gradient: [   -7.0883   -29.7137   124.9005   273.9528 -1683.2264]\n",
      "Weights: [-4.2974 -0.8712 -0.2691  0.1077  0.0884]\n",
      "MSE loss: 156.1692\n",
      "Iteration: 40400\n",
      "Gradient: [   15.2094   -34.6414    89.3471   221.8317 -1813.4495]\n",
      "Weights: [-4.2991 -0.8699 -0.2698  0.1077  0.0885]\n",
      "MSE loss: 156.0465\n",
      "Iteration: 40500\n",
      "Gradient: [  26.0088  -65.7809  -46.5264 -235.1825 -143.6281]\n",
      "Weights: [-4.3007 -0.8683 -0.2704  0.1076  0.0886]\n",
      "MSE loss: 155.9348\n",
      "Iteration: 40600\n",
      "Gradient: [  15.0249   58.532   139.9688 -104.0731 -515.6176]\n",
      "Weights: [-4.2997 -0.8673 -0.2712  0.1075  0.0886]\n",
      "MSE loss: 155.8199\n",
      "Iteration: 40700\n",
      "Gradient: [  11.315  -105.1134  -93.283    64.5957   76.5415]\n",
      "Weights: [-4.2998 -0.8653 -0.2721  0.1074  0.0887]\n",
      "MSE loss: 155.6794\n",
      "Iteration: 40800\n",
      "Gradient: [  -10.6917    15.103    -45.291   -236.8472 -1370.1773]\n",
      "Weights: [-4.2989 -0.8645 -0.2728  0.1074  0.0888]\n",
      "MSE loss: 155.5696\n",
      "Iteration: 40900\n",
      "Gradient: [  17.3781    4.5818   -1.3263  316.4666 -505.9555]\n",
      "Weights: [-4.2973 -0.8641 -0.2737  0.1074  0.0888]\n",
      "MSE loss: 155.47\n",
      "Iteration: 41000\n",
      "Gradient: [  39.3152   58.181    73.059  -126.1867 -161.6554]\n",
      "Weights: [-4.3006 -0.8623 -0.2743  0.1074  0.0889]\n",
      "MSE loss: 155.3449\n",
      "Iteration: 41100\n",
      "Gradient: [ 2.933430e+01 -2.300550e+01 -1.251000e-01  1.323142e+02 -8.258314e+02]\n",
      "Weights: [-4.3008 -0.861  -0.2752  0.1073  0.089 ]\n",
      "MSE loss: 155.2085\n",
      "Iteration: 41200\n",
      "Gradient: [ -53.4696  -44.13    102.6011  -85.8469 -150.4629]\n",
      "Weights: [-4.3031 -0.8592 -0.276   0.1073  0.089 ]\n",
      "MSE loss: 155.0882\n",
      "Iteration: 41300\n",
      "Gradient: [   -6.455    -64.0998  -105.5556  -144.3367 -2010.2844]\n",
      "Weights: [-4.3028 -0.8577 -0.2767  0.1072  0.0891]\n",
      "MSE loss: 154.9729\n",
      "Iteration: 41400\n",
      "Gradient: [  14.8208   12.7577  145.3828  259.2329 -688.6541]\n",
      "Weights: [-4.3029 -0.8566 -0.2774  0.1072  0.0891]\n",
      "MSE loss: 154.8717\n",
      "Iteration: 41500\n",
      "Gradient: [  21.3196  -21.2234  180.1233  472.1182 -210.0931]\n",
      "Weights: [-4.3014 -0.855  -0.2783  0.1071  0.0892]\n",
      "MSE loss: 154.7492\n",
      "Iteration: 41600\n",
      "Gradient: [   -1.912    -36.0657    96.2344   -57.74   -1323.9645]\n",
      "Weights: [-4.3025 -0.8535 -0.2792  0.1071  0.0893]\n",
      "MSE loss: 154.6155\n",
      "Iteration: 41700\n",
      "Gradient: [   1.6992   30.0764  110.1852 -222.4654 -478.6617]\n",
      "Weights: [-4.3016 -0.8526 -0.2798  0.107   0.0893]\n",
      "MSE loss: 154.5232\n",
      "Iteration: 41800\n",
      "Gradient: [ 10.4374   7.8507 -16.7057  59.6242  93.1891]\n",
      "Weights: [-4.3029 -0.8509 -0.2805  0.1069  0.0894]\n",
      "MSE loss: 154.4001\n",
      "Iteration: 41900\n",
      "Gradient: [  -33.1376    17.5296   -84.3256    98.7635 -1168.3076]\n",
      "Weights: [-4.3039 -0.8496 -0.2812  0.1069  0.0894]\n",
      "MSE loss: 154.2951\n",
      "Iteration: 42000\n",
      "Gradient: [-50.1126 -24.545  150.4892   5.1982 610.0136]\n",
      "Weights: [-4.3047 -0.8481 -0.282   0.1068  0.0895]\n",
      "MSE loss: 154.17\n",
      "Iteration: 42100\n",
      "Gradient: [  -5.4649   25.2468  -96.9111  175.303  -782.7834]\n",
      "Weights: [-4.3042 -0.847  -0.2828  0.1068  0.0896]\n",
      "MSE loss: 154.0572\n",
      "Iteration: 42200\n",
      "Gradient: [   27.8592   -60.5809    48.5677   386.652  -1000.7078]\n",
      "Weights: [-4.3062 -0.8459 -0.2836  0.1067  0.0896]\n",
      "MSE loss: 153.9415\n",
      "Iteration: 42300\n",
      "Gradient: [   5.8008   35.7975   41.7931 -186.3772 -526.4895]\n",
      "Weights: [-4.3075 -0.8445 -0.2842  0.1067  0.0897]\n",
      "MSE loss: 153.8274\n",
      "Iteration: 42400\n",
      "Gradient: [   16.491     -9.2833   -13.5942  -295.744  -1346.4287]\n",
      "Weights: [-4.3068 -0.8431 -0.2849  0.1066  0.0898]\n",
      "MSE loss: 153.7064\n",
      "Iteration: 42500\n",
      "Gradient: [  -17.388     -2.8385   154.7304    -8.7533 -1203.1509]\n",
      "Weights: [-4.3064 -0.842  -0.2858  0.1066  0.0898]\n",
      "MSE loss: 153.588\n",
      "Iteration: 42600\n",
      "Gradient: [  -7.0471   -1.0885  157.2865 -132.9178 -815.6875]\n",
      "Weights: [-4.3085 -0.8398 -0.2863  0.1065  0.0899]\n",
      "MSE loss: 153.4686\n",
      "Iteration: 42700\n",
      "Gradient: [ -11.0119   66.894   154.6589  109.3585 -822.0467]\n",
      "Weights: [-4.308  -0.8384 -0.2872  0.1064  0.0899]\n",
      "MSE loss: 153.3462\n",
      "Iteration: 42800\n",
      "Gradient: [  -9.2017 -131.1414  290.2992  277.792  -158.5269]\n",
      "Weights: [-4.3076 -0.8377 -0.2878  0.1064  0.09  ]\n",
      "MSE loss: 153.2567\n",
      "Iteration: 42900\n",
      "Gradient: [ -5.2413 -51.849   64.6451 -99.3365 -54.1601]\n",
      "Weights: [-4.3072 -0.8365 -0.2886  0.1063  0.0901]\n",
      "MSE loss: 153.1388\n",
      "Iteration: 43000\n",
      "Gradient: [  38.9941  -23.2724   77.559    82.4126 -459.1343]\n",
      "Weights: [-4.3081 -0.8349 -0.2893  0.1063  0.0901]\n",
      "MSE loss: 153.0281\n",
      "Iteration: 43100\n",
      "Gradient: [  20.73     19.4071  228.6589  126.1194 -389.8083]\n",
      "Weights: [-4.3066 -0.8346 -0.2901  0.1062  0.0902]\n",
      "MSE loss: 152.9226\n",
      "Iteration: 43200\n",
      "Gradient: [  31.5064  -96.427    -4.631   -13.5942 -160.6869]\n",
      "Weights: [-4.3077 -0.8333 -0.2908  0.1061  0.0903]\n",
      "MSE loss: 152.8095\n",
      "Iteration: 43300\n",
      "Gradient: [ 31.1188  39.2156 167.4905  25.3455 195.0974]\n",
      "Weights: [-4.3071 -0.832  -0.2915  0.1061  0.0903]\n",
      "MSE loss: 152.7005\n",
      "Iteration: 43400\n",
      "Gradient: [ -21.7689  -42.2069  112.9729  291.3024 -612.7621]\n",
      "Weights: [-4.3095 -0.8305 -0.2922  0.106   0.0904]\n",
      "MSE loss: 152.581\n",
      "Iteration: 43500\n",
      "Gradient: [  -3.2771   18.4318   68.9708 -342.2893  491.1125]\n",
      "Weights: [-4.3104 -0.8283 -0.293   0.106   0.0904]\n",
      "MSE loss: 152.4549\n",
      "Iteration: 43600\n",
      "Gradient: [  46.6595  -55.661    96.1762  251.2616 -878.09  ]\n",
      "Weights: [-4.3117 -0.8269 -0.2938  0.1059  0.0905]\n",
      "MSE loss: 152.3386\n",
      "Iteration: 43700\n",
      "Gradient: [   1.9301  -34.935   118.1178 -207.6341  271.63  ]\n",
      "Weights: [-4.3132 -0.8254 -0.2943  0.1058  0.0906]\n",
      "MSE loss: 152.2395\n",
      "Iteration: 43800\n",
      "Gradient: [ -15.4209  -20.4911  157.5036   32.415  -522.0828]\n",
      "Weights: [-4.3122 -0.8246 -0.295   0.1058  0.0906]\n",
      "MSE loss: 152.1422\n",
      "Iteration: 43900\n",
      "Gradient: [  4.2117 -83.7456 -10.3487 -16.869  190.9757]\n",
      "Weights: [-4.3123 -0.8232 -0.2956  0.1057  0.0907]\n",
      "MSE loss: 152.0343\n",
      "Iteration: 44000\n",
      "Gradient: [ -52.5073   30.0281   97.6884  152.7194 -816.454 ]\n",
      "Weights: [-4.3129 -0.8217 -0.2963  0.1057  0.0907]\n",
      "MSE loss: 151.9267\n",
      "Iteration: 44100\n",
      "Gradient: [   37.115    -80.1284   108.0324   334.4558 -1094.4193]\n",
      "Weights: [-4.3139 -0.82   -0.297   0.1056  0.0908]\n",
      "MSE loss: 151.8074\n",
      "Iteration: 44200\n",
      "Gradient: [   -3.2617   -65.7245   144.9837   234.4099 -1084.7357]\n",
      "Weights: [-4.3158 -0.8178 -0.2979  0.1055  0.0909]\n",
      "MSE loss: 151.6677\n",
      "Iteration: 44300\n",
      "Gradient: [  0.944   -1.0225  -7.4336 213.6609 193.8759]\n",
      "Weights: [-4.3161 -0.8166 -0.2987  0.1054  0.0909]\n",
      "MSE loss: 151.5546\n",
      "Iteration: 44400\n",
      "Gradient: [  -3.3802    4.2482   87.506  -220.7905 -879.2495]\n",
      "Weights: [-4.3146 -0.8155 -0.2995  0.1053  0.091 ]\n",
      "MSE loss: 151.4475\n",
      "Iteration: 44500\n",
      "Gradient: [   28.0066   -14.2874    32.0177  -136.8201 -1508.7722]\n",
      "Weights: [-4.3154 -0.8144 -0.3001  0.1053  0.091 ]\n",
      "MSE loss: 151.3488\n",
      "Iteration: 44600\n",
      "Gradient: [ -14.5334   -9.2355   27.5671  -35.4726 -301.0794]\n",
      "Weights: [-4.315  -0.8137 -0.3009  0.1052  0.0911]\n",
      "MSE loss: 151.2429\n",
      "Iteration: 44700\n",
      "Gradient: [  -36.3366   -25.8349     8.1059    37.8655 -1381.5578]\n",
      "Weights: [-4.3152 -0.8123 -0.3014  0.1052  0.0911]\n",
      "MSE loss: 151.1456\n",
      "Iteration: 44800\n",
      "Gradient: [ -17.8852  -21.634   141.031  -287.8716  289.901 ]\n",
      "Weights: [-4.316  -0.8108 -0.3022  0.1051  0.0912]\n",
      "MSE loss: 151.0232\n",
      "Iteration: 44900\n",
      "Gradient: [   -6.3817    -4.9474    40.6872   164.3652 -1305.5856]\n",
      "Weights: [-4.317  -0.8096 -0.3029  0.1051  0.0913]\n",
      "MSE loss: 150.9101\n",
      "Iteration: 45000\n",
      "Gradient: [   -1.5183    11.6446   304.3925   -19.9278 -1362.4588]\n",
      "Weights: [-4.3169 -0.8079 -0.3036  0.105   0.0913]\n",
      "MSE loss: 150.8021\n",
      "Iteration: 45100\n",
      "Gradient: [  -20.8518    -6.8559    92.7913   290.221  -1033.8755]\n",
      "Weights: [-4.3157 -0.8068 -0.3044  0.1049  0.0914]\n",
      "MSE loss: 150.697\n",
      "Iteration: 45200\n",
      "Gradient: [ -19.889    17.4352  162.3109  224.838  -833.942 ]\n",
      "Weights: [-4.3161 -0.8056 -0.3053  0.1049  0.0914]\n",
      "MSE loss: 150.5834\n",
      "Iteration: 45300\n",
      "Gradient: [ 37.304  -13.7139 132.2153 174.0336 249.5242]\n",
      "Weights: [-4.3179 -0.8044 -0.306   0.1049  0.0915]\n",
      "MSE loss: 150.4739\n",
      "Iteration: 45400\n",
      "Gradient: [   5.7117  -75.6808   48.6674 -205.555  -859.8278]\n",
      "Weights: [-4.3167 -0.8028 -0.3069  0.1048  0.0916]\n",
      "MSE loss: 150.3515\n",
      "Iteration: 45500\n",
      "Gradient: [ -21.9776   18.0548   41.5358  354.0481 -320.8377]\n",
      "Weights: [-4.3201 -0.8013 -0.3075  0.1048  0.0916]\n",
      "MSE loss: 150.246\n",
      "Iteration: 45600\n",
      "Gradient: [  34.5442   10.7038 -110.5522   27.4237 -146.4334]\n",
      "Weights: [-4.3177 -0.8003 -0.3082  0.1047  0.0917]\n",
      "MSE loss: 150.1336\n",
      "Iteration: 45700\n",
      "Gradient: [ 7.9060000e-01  1.1802500e+01  5.5984100e+01  2.1442300e+01\n",
      " -1.9390293e+03]\n",
      "Weights: [-4.3202 -0.7988 -0.3089  0.1047  0.0918]\n",
      "MSE loss: 150.0176\n",
      "Iteration: 45800\n",
      "Gradient: [-26.8175 -57.3285 111.1414 317.4777 440.9681]\n",
      "Weights: [-4.3188 -0.798  -0.3096  0.1046  0.0918]\n",
      "MSE loss: 149.9176\n",
      "Iteration: 45900\n",
      "Gradient: [  61.9713   -3.047   281.3944 -129.9688  181.3587]\n",
      "Weights: [-4.3209 -0.7965 -0.3102  0.1045  0.0919]\n",
      "MSE loss: 149.808\n",
      "Iteration: 46000\n",
      "Gradient: [ 10.6008  14.7046 -48.8672 -44.749  -93.8257]\n",
      "Weights: [-4.3219 -0.7952 -0.3111  0.1045  0.092 ]\n",
      "MSE loss: 149.6988\n",
      "Iteration: 46100\n",
      "Gradient: [  -29.3567     7.7041   151.3658  -176.6047 -1841.4359]\n",
      "Weights: [-4.3229 -0.7935 -0.3117  0.1044  0.092 ]\n",
      "MSE loss: 149.596\n",
      "Iteration: 46200\n",
      "Gradient: [  -5.577    47.1367   77.0654   60.6367 -231.605 ]\n",
      "Weights: [-4.3225 -0.7918 -0.3124  0.1044  0.0921]\n",
      "MSE loss: 149.4896\n",
      "Iteration: 46300\n",
      "Gradient: [-1.2050700e+01 -1.3526000e+00 -5.1312800e+01  4.6466180e+02\n",
      " -1.5471413e+03]\n",
      "Weights: [-4.3226 -0.7912 -0.313   0.1043  0.0921]\n",
      "MSE loss: 149.3954\n",
      "Iteration: 46400\n",
      "Gradient: [   15.8561    -4.7691   104.7459   148.2828 -1102.4918]\n",
      "Weights: [-4.3232 -0.7899 -0.3136  0.1043  0.0922]\n",
      "MSE loss: 149.3058\n",
      "Iteration: 46500\n",
      "Gradient: [  32.5285    7.282   165.9521 -540.1289 -618.8962]\n",
      "Weights: [-4.3258 -0.7882 -0.3142  0.1042  0.0922]\n",
      "MSE loss: 149.2048\n",
      "Iteration: 46600\n",
      "Gradient: [ 50.3812   2.3688  73.3844 378.5955 636.6448]\n",
      "Weights: [-4.3248 -0.7866 -0.315   0.1042  0.0923]\n",
      "MSE loss: 149.094\n",
      "Iteration: 46700\n",
      "Gradient: [  -9.7478  -21.1337  132.8725  -41.473  -588.3992]\n",
      "Weights: [-4.3257 -0.7858 -0.3158  0.1041  0.0923]\n",
      "MSE loss: 148.9911\n",
      "Iteration: 46800\n",
      "Gradient: [  -85.6725   -93.717    300.6097   221.7892 -1656.6145]\n",
      "Weights: [-4.3265 -0.7845 -0.3165  0.1041  0.0924]\n",
      "MSE loss: 148.8898\n",
      "Iteration: 46900\n",
      "Gradient: [   25.5443    18.6828   110.1612    82.1521 -1494.4358]\n",
      "Weights: [-4.3245 -0.7839 -0.3173  0.104   0.0924]\n",
      "MSE loss: 148.7812\n",
      "Iteration: 47000\n",
      "Gradient: [   7.144    12.3015   57.4591 -268.5116 -242.8097]\n",
      "Weights: [-4.3248 -0.7828 -0.3179  0.104   0.0925]\n",
      "MSE loss: 148.6939\n",
      "Iteration: 47100\n",
      "Gradient: [ -33.4945  -61.1239   10.0803 -108.5228 -486.9112]\n",
      "Weights: [-4.324  -0.782  -0.3186  0.104   0.0926]\n",
      "MSE loss: 148.591\n",
      "Iteration: 47200\n",
      "Gradient: [   1.6862   20.8915   54.1303  127.435  -945.5616]\n",
      "Weights: [-4.3249 -0.7807 -0.3192  0.1039  0.0926]\n",
      "MSE loss: 148.4957\n",
      "Iteration: 47300\n",
      "Gradient: [  -4.5435  -49.3451   74.9271 -306.6791 -823.6932]\n",
      "Weights: [-4.3253 -0.7795 -0.3198  0.1038  0.0927]\n",
      "MSE loss: 148.4\n",
      "Iteration: 47400\n",
      "Gradient: [  14.94    -51.5565   17.5491 -192.6645   19.6035]\n",
      "Weights: [-4.3268 -0.7777 -0.3202  0.1037  0.0927]\n",
      "MSE loss: 148.3066\n",
      "Iteration: 47500\n",
      "Gradient: [ -64.0244  -40.5494   71.5849  435.5133 -528.7889]\n",
      "Weights: [-4.3283 -0.7762 -0.3207  0.1036  0.0928]\n",
      "MSE loss: 148.2202\n",
      "Iteration: 47600\n",
      "Gradient: [ 16.2886 -49.4743 -34.8019 130.3757 219.3266]\n",
      "Weights: [-4.3283 -0.7746 -0.3215  0.1035  0.0928]\n",
      "MSE loss: 148.1056\n",
      "Iteration: 47700\n",
      "Gradient: [ -31.2483   47.1309  186.5054  -31.269  -755.115 ]\n",
      "Weights: [-4.3288 -0.7736 -0.3221  0.1034  0.0929]\n",
      "MSE loss: 148.0135\n",
      "Iteration: 47800\n",
      "Gradient: [ 24.7149 -72.5141 -19.3907 -86.2075 220.5868]\n",
      "Weights: [-4.329  -0.7723 -0.3228  0.1034  0.093 ]\n",
      "MSE loss: 147.907\n",
      "Iteration: 47900\n",
      "Gradient: [  -9.2425  -31.633   136.4727 -387.8525  376.577 ]\n",
      "Weights: [-4.3283 -0.7706 -0.3236  0.1033  0.093 ]\n",
      "MSE loss: 147.7883\n",
      "Iteration: 48000\n",
      "Gradient: [  16.0005  -38.7029 -138.187   131.5077 -265.3434]\n",
      "Weights: [-4.3285 -0.7695 -0.3244  0.1033  0.0931]\n",
      "MSE loss: 147.6939\n",
      "Iteration: 48100\n",
      "Gradient: [ 16.1585  -7.0732 173.1569  39.6804 383.137 ]\n",
      "Weights: [-4.3298 -0.7681 -0.3251  0.1032  0.0931]\n",
      "MSE loss: 147.5834\n",
      "Iteration: 48200\n",
      "Gradient: [ 12.2167   7.288  309.5336 505.8575 -40.9338]\n",
      "Weights: [-4.3285 -0.767  -0.3259  0.1032  0.0932]\n",
      "MSE loss: 147.482\n",
      "Iteration: 48300\n",
      "Gradient: [   45.4246    -6.0413  -101.3085  -343.6098 -1571.2377]\n",
      "Weights: [-4.3297 -0.7659 -0.3266  0.1031  0.0932]\n",
      "MSE loss: 147.3798\n",
      "Iteration: 48400\n",
      "Gradient: [  16.5995  -40.3956   96.6325 -177.3714 -403.2676]\n",
      "Weights: [-4.33   -0.7645 -0.3273  0.1031  0.0933]\n",
      "MSE loss: 147.2755\n",
      "Iteration: 48500\n",
      "Gradient: [  26.7497  -11.2832  163.5044  -22.764  -824.625 ]\n",
      "Weights: [-4.3319 -0.7633 -0.3278  0.1031  0.0933]\n",
      "MSE loss: 147.1887\n",
      "Iteration: 48600\n",
      "Gradient: [   25.75      67.4246    19.4721   389.2936 -1158.1888]\n",
      "Weights: [-4.3304 -0.7624 -0.3285  0.1031  0.0934]\n",
      "MSE loss: 147.0976\n",
      "Iteration: 48700\n",
      "Gradient: [ -11.28    -26.6847    1.607  -509.3788  -35.9664]\n",
      "Weights: [-4.3315 -0.7614 -0.3291  0.103   0.0935]\n",
      "MSE loss: 147.0032\n",
      "Iteration: 48800\n",
      "Gradient: [ 34.4529 -11.723  -87.9831  95.9297  66.0883]\n",
      "Weights: [-4.3316 -0.7605 -0.3298  0.1029  0.0935]\n",
      "MSE loss: 146.9113\n",
      "Iteration: 48900\n",
      "Gradient: [   4.9305    5.7335  181.4358  -96.4817 -242.4055]\n",
      "Weights: [-4.3308 -0.7592 -0.3303  0.1029  0.0936]\n",
      "MSE loss: 146.817\n",
      "Iteration: 49000\n",
      "Gradient: [  -14.1404   100.8274   172.8322   -48.1076 -1200.3601]\n",
      "Weights: [-4.3317 -0.7579 -0.3311  0.1028  0.0936]\n",
      "MSE loss: 146.7113\n",
      "Iteration: 49100\n",
      "Gradient: [  -9.5346  -32.4824  118.675  -219.6528 -981.6508]\n",
      "Weights: [-4.3331 -0.7565 -0.3317  0.1027  0.0937]\n",
      "MSE loss: 146.6175\n",
      "Iteration: 49200\n",
      "Gradient: [   -2.6647    33.5347    62.7897   355.2564 -1478.2269]\n",
      "Weights: [-4.3324 -0.7551 -0.3324  0.1027  0.0937]\n",
      "MSE loss: 146.52\n",
      "Iteration: 49300\n",
      "Gradient: [ 6.4697700e+01 -1.8249000e+01 -9.4490000e-01 -5.5098730e+02\n",
      " -1.3551887e+03]\n",
      "Weights: [-4.3339 -0.7539 -0.333   0.1026  0.0938]\n",
      "MSE loss: 146.427\n",
      "Iteration: 49400\n",
      "Gradient: [  24.1892   13.2358  158.1361  176.5021 -656.2844]\n",
      "Weights: [-4.3334 -0.7527 -0.3337  0.1026  0.0938]\n",
      "MSE loss: 146.3229\n",
      "Iteration: 49500\n",
      "Gradient: [   53.0833   -21.6922   182.7885  -160.372  -1037.972 ]\n",
      "Weights: [-4.3359 -0.7505 -0.3345  0.1025  0.0939]\n",
      "MSE loss: 146.2049\n",
      "Iteration: 49600\n",
      "Gradient: [  -6.6032    2.2058  114.2784  393.1207 -190.9758]\n",
      "Weights: [-4.3369 -0.7488 -0.3352  0.1025  0.0939]\n",
      "MSE loss: 146.1037\n",
      "Iteration: 49700\n",
      "Gradient: [  2.8022  18.4274 226.4258 -75.9337 235.0987]\n",
      "Weights: [-4.3365 -0.7479 -0.336   0.1024  0.094 ]\n",
      "MSE loss: 145.9993\n",
      "Iteration: 49800\n",
      "Gradient: [  -37.2452    62.9073   -28.6403   -19.9433 -1256.1646]\n",
      "Weights: [-4.3381 -0.7466 -0.3366  0.1024  0.094 ]\n",
      "MSE loss: 145.9169\n",
      "Iteration: 49900\n",
      "Gradient: [  41.8697 -101.5962 -107.5586  -39.5772 -579.6095]\n",
      "Weights: [-4.3393 -0.7459 -0.3371  0.1024  0.0941]\n",
      "MSE loss: 145.8449\n",
      "Iteration: 50000\n",
      "Gradient: [  14.0666    4.4992   41.1509  286.893  -212.8018]\n",
      "Weights: [-4.3384 -0.7444 -0.3379  0.1024  0.0942]\n",
      "MSE loss: 145.7353\n",
      "Iteration: 50100\n",
      "Gradient: [  16.4169  -24.5735  -36.8796  -45.9375 -641.3773]\n",
      "Weights: [-4.3387 -0.7431 -0.3386  0.1024  0.0942]\n",
      "MSE loss: 145.6384\n",
      "Iteration: 50200\n",
      "Gradient: [-18.6908  10.6766 -35.4538 -18.0734 192.8244]\n",
      "Weights: [-4.3391 -0.7424 -0.3391  0.1023  0.0943]\n",
      "MSE loss: 145.5584\n",
      "Iteration: 50300\n",
      "Gradient: [  17.2584  -12.3662   45.2272  564.0508 -198.554 ]\n",
      "Weights: [-4.3388 -0.7411 -0.3398  0.1022  0.0943]\n",
      "MSE loss: 145.4572\n",
      "Iteration: 50400\n",
      "Gradient: [ -12.1999   -4.9845  157.2178 -119.2823 -368.4476]\n",
      "Weights: [-4.3396 -0.7395 -0.3406  0.1021  0.0944]\n",
      "MSE loss: 145.3451\n",
      "Iteration: 50500\n",
      "Gradient: [   3.8694  -12.1551   62.5338   75.4046 -457.0782]\n",
      "Weights: [-4.3407 -0.7383 -0.3414  0.1021  0.0944]\n",
      "MSE loss: 145.2357\n",
      "Iteration: 50600\n",
      "Gradient: [ -25.3706   -7.8017  128.6437 -153.7682  211.5006]\n",
      "Weights: [-4.3393 -0.7369 -0.342   0.102   0.0945]\n",
      "MSE loss: 145.1373\n",
      "Iteration: 50700\n",
      "Gradient: [  -37.7909    34.7974   127.3836   286.6841 -1084.5453]\n",
      "Weights: [-4.3402 -0.7355 -0.3425  0.1019  0.0946]\n",
      "MSE loss: 145.0501\n",
      "Iteration: 50800\n",
      "Gradient: [  -13.2235    -1.3255   -24.8141  -280.9139 -1263.9484]\n",
      "Weights: [-4.3412 -0.7343 -0.3431  0.1018  0.0946]\n",
      "MSE loss: 144.9621\n",
      "Iteration: 50900\n",
      "Gradient: [  -6.1729   18.1725  180.9503 -187.8739  992.963 ]\n",
      "Weights: [-4.3412 -0.7327 -0.3438  0.1018  0.0946]\n",
      "MSE loss: 144.8676\n",
      "Iteration: 51000\n",
      "Gradient: [  -30.3365   -38.2037   100.2738   457.242  -1239.2065]\n",
      "Weights: [-4.3417 -0.7314 -0.3445  0.1018  0.0947]\n",
      "MSE loss: 144.7689\n",
      "Iteration: 51100\n",
      "Gradient: [   3.493   -47.1173   89.5843 -342.8254 -742.1948]\n",
      "Weights: [-4.3436 -0.7304 -0.3451  0.1017  0.0948]\n",
      "MSE loss: 144.6827\n",
      "Iteration: 51200\n",
      "Gradient: [ -3.2005 -42.4007 -38.8956  -0.5991 472.7459]\n",
      "Weights: [-4.3449 -0.7287 -0.3458  0.1017  0.0948]\n",
      "MSE loss: 144.5804\n",
      "Iteration: 51300\n",
      "Gradient: [  -35.6352   -67.661    -63.2243   289.8198 -1050.2007]\n",
      "Weights: [-4.3429 -0.7277 -0.3464  0.1016  0.0948]\n",
      "MSE loss: 144.4929\n",
      "Iteration: 51400\n",
      "Gradient: [ -21.2509  -25.2863  -75.3946  -43.0577 -684.4706]\n",
      "Weights: [-4.3444 -0.7264 -0.3471  0.1016  0.0949]\n",
      "MSE loss: 144.4007\n",
      "Iteration: 51500\n",
      "Gradient: [-2.145000e-01  1.057000e-01 -1.213090e+02  2.423541e+02  5.178040e+01]\n",
      "Weights: [-4.3435 -0.7258 -0.3476  0.1016  0.095 ]\n",
      "MSE loss: 144.324\n",
      "Iteration: 51600\n",
      "Gradient: [ 34.1308  31.5456 -28.4387 132.6667 -13.8081]\n",
      "Weights: [-4.3451 -0.7242 -0.3483  0.1015  0.095 ]\n",
      "MSE loss: 144.2246\n",
      "Iteration: 51700\n",
      "Gradient: [  10.1627   -2.3299  -26.9881 -400.8376 -563.3059]\n",
      "Weights: [-4.3447 -0.7231 -0.3491  0.1015  0.095 ]\n",
      "MSE loss: 144.1302\n",
      "Iteration: 51800\n",
      "Gradient: [  -34.5944   -29.158     35.2829   143.5449 -1053.0557]\n",
      "Weights: [-4.3462 -0.7219 -0.3497  0.1015  0.0951]\n",
      "MSE loss: 144.0457\n",
      "Iteration: 51900\n",
      "Gradient: [ -51.2149 -109.7321  118.8361   81.4857 -118.0228]\n",
      "Weights: [-4.3482 -0.7205 -0.3504  0.1015  0.0952]\n",
      "MSE loss: 143.956\n",
      "Iteration: 52000\n",
      "Gradient: [ -28.2612  -17.6043  267.8655   -2.8246 -802.8509]\n",
      "Weights: [-4.3489 -0.7183 -0.3512  0.1014  0.0952]\n",
      "MSE loss: 143.8435\n",
      "Iteration: 52100\n",
      "Gradient: [  -2.0838  -33.0945   52.1469  178.017  -696.4109]\n",
      "Weights: [-4.3474 -0.7172 -0.3518  0.1013  0.0953]\n",
      "MSE loss: 143.7448\n",
      "Iteration: 52200\n",
      "Gradient: [  45.8463  -49.6149  113.6514   27.6227 -834.2651]\n",
      "Weights: [-4.3455 -0.7162 -0.3526  0.1012  0.0953]\n",
      "MSE loss: 143.6489\n",
      "Iteration: 52300\n",
      "Gradient: [   2.2058  -42.0922   41.1233  185.1638 -644.4641]\n",
      "Weights: [-4.3473 -0.7153 -0.3533  0.1012  0.0954]\n",
      "MSE loss: 143.5504\n",
      "Iteration: 52400\n",
      "Gradient: [ -48.8701  -78.6506   59.4423 -211.749  -578.177 ]\n",
      "Weights: [-4.3475 -0.7138 -0.3541  0.1011  0.0954]\n",
      "MSE loss: 143.4411\n",
      "Iteration: 52500\n",
      "Gradient: [  29.2377   -1.2946   14.1703   86.484  -620.3077]\n",
      "Weights: [-4.3484 -0.7128 -0.3548  0.1011  0.0955]\n",
      "MSE loss: 143.3493\n",
      "Iteration: 52600\n",
      "Gradient: [  -9.1169  -15.7398   12.3471 -142.1044 -791.8983]\n",
      "Weights: [-4.348  -0.7112 -0.3555  0.1011  0.0955]\n",
      "MSE loss: 143.2509\n",
      "Iteration: 52700\n",
      "Gradient: [ 2.8984800e+01 -1.0470000e-01  1.1733940e+02 -7.4099000e+00\n",
      " -1.3933298e+03]\n",
      "Weights: [-4.3472 -0.7103 -0.3562  0.101   0.0956]\n",
      "MSE loss: 143.1617\n",
      "Iteration: 52800\n",
      "Gradient: [ -34.801    32.1649 -187.988    68.275  -847.0768]\n",
      "Weights: [-4.348  -0.7093 -0.3568  0.101   0.0956]\n",
      "MSE loss: 143.0664\n",
      "Iteration: 52900\n",
      "Gradient: [-18.5473 -89.5108  27.7069 -93.6107 306.4996]\n",
      "Weights: [-4.3495 -0.7078 -0.3575  0.1009  0.0957]\n",
      "MSE loss: 142.9717\n",
      "Iteration: 53000\n",
      "Gradient: [   -4.6405   -40.222    129.6263   609.6757 -1596.2977]\n",
      "Weights: [-4.3498 -0.7061 -0.3581  0.1009  0.0958]\n",
      "MSE loss: 142.8773\n",
      "Iteration: 53100\n",
      "Gradient: [   37.1375   -16.8564   108.6093  -224.515  -1667.9512]\n",
      "Weights: [-4.3507 -0.7052 -0.3588  0.1008  0.0958]\n",
      "MSE loss: 142.785\n",
      "Iteration: 53200\n",
      "Gradient: [  35.9519   -4.6146  190.2482   -9.2133 -254.2312]\n",
      "Weights: [-4.3519 -0.7033 -0.3594  0.1008  0.0959]\n",
      "MSE loss: 142.6858\n",
      "Iteration: 53300\n",
      "Gradient: [ -34.79     17.4837  175.1714  -84.7437 -561.0793]\n",
      "Weights: [-4.3512 -0.7026 -0.3602  0.1007  0.0959]\n",
      "MSE loss: 142.59\n",
      "Iteration: 53400\n",
      "Gradient: [  -5.2122   -5.3725  157.7718  205.7022 -759.7466]\n",
      "Weights: [-4.3504 -0.7015 -0.3609  0.1007  0.096 ]\n",
      "MSE loss: 142.4978\n",
      "Iteration: 53500\n",
      "Gradient: [ -27.3616   44.8237   39.5869  170.1522 -730.9311]\n",
      "Weights: [-4.3515 -0.7004 -0.3617  0.1007  0.096 ]\n",
      "MSE loss: 142.3954\n",
      "Iteration: 53600\n",
      "Gradient: [   9.8295  -23.1691  147.5736 -235.8371 -850.6522]\n",
      "Weights: [-4.3506 -0.6997 -0.3622  0.1006  0.0961]\n",
      "MSE loss: 142.3166\n",
      "Iteration: 53700\n",
      "Gradient: [  -1.3804   19.1592   21.7309  254.9269 -287.362 ]\n",
      "Weights: [-4.3541 -0.6977 -0.3628  0.1006  0.0961]\n",
      "MSE loss: 142.2249\n",
      "Iteration: 53800\n",
      "Gradient: [  23.5163   16.2466   95.703   -52.5343 -815.1199]\n",
      "Weights: [-4.3511 -0.6961 -0.3636  0.1005  0.0962]\n",
      "MSE loss: 142.1312\n",
      "Iteration: 53900\n",
      "Gradient: [-4.7671000e+00 -3.6780000e-01 -8.9778100e+01  2.9401400e+02\n",
      " -1.1964746e+03]\n",
      "Weights: [-4.354  -0.6943 -0.3644  0.1005  0.0962]\n",
      "MSE loss: 142.0154\n",
      "Iteration: 54000\n",
      "Gradient: [  -46.5244   -83.4231    -8.3887  -352.3746 -1154.1047]\n",
      "Weights: [-4.3542 -0.6939 -0.3651  0.1004  0.0963]\n",
      "MSE loss: 141.9337\n",
      "Iteration: 54100\n",
      "Gradient: [   25.0455   -39.7004   114.9156    68.3485 -2033.6717]\n",
      "Weights: [-4.3544 -0.6927 -0.3657  0.1004  0.0964]\n",
      "MSE loss: 141.84\n",
      "Iteration: 54200\n",
      "Gradient: [ -11.899   -89.9625  164.5816  -70.1146 -900.5813]\n",
      "Weights: [-4.3547 -0.6916 -0.3664  0.1003  0.0964]\n",
      "MSE loss: 141.7501\n",
      "Iteration: 54300\n",
      "Gradient: [ -30.1044   -6.7336   95.9658  304.4688 -590.1279]\n",
      "Weights: [-4.3531 -0.6907 -0.3671  0.1003  0.0965]\n",
      "MSE loss: 141.663\n",
      "Iteration: 54400\n",
      "Gradient: [   9.8163  -11.3911  349.1396  395.0381 -391.2732]\n",
      "Weights: [-4.3524 -0.6891 -0.3678  0.1003  0.0965]\n",
      "MSE loss: 141.5751\n",
      "Iteration: 54500\n",
      "Gradient: [  14.5223  -73.5393  -45.4447   16.7409 -479.0191]\n",
      "Weights: [-4.3547 -0.6878 -0.3685  0.1003  0.0966]\n",
      "MSE loss: 141.4763\n",
      "Iteration: 54600\n",
      "Gradient: [ 2.342780e+01  5.827000e-01  6.919130e+01  5.413380e+01 -7.526483e+02]\n",
      "Weights: [-4.3546 -0.6863 -0.3692  0.1002  0.0966]\n",
      "MSE loss: 141.3772\n",
      "Iteration: 54700\n",
      "Gradient: [ 10.5349 -35.2432  57.7178  40.1917 373.1608]\n",
      "Weights: [-4.3577 -0.6845 -0.3698  0.1001  0.0967]\n",
      "MSE loss: 141.2811\n",
      "Iteration: 54800\n",
      "Gradient: [  33.5989  -52.575   105.7008 -160.5499 -164.3166]\n",
      "Weights: [-4.3598 -0.6827 -0.3704  0.1001  0.0967]\n",
      "MSE loss: 141.1932\n",
      "Iteration: 54900\n",
      "Gradient: [  -35.4254   -15.6296    14.4781  -128.9973 -1043.6088]\n",
      "Weights: [-4.3602 -0.6812 -0.3711  0.1     0.0968]\n",
      "MSE loss: 141.0956\n",
      "Iteration: 55000\n",
      "Gradient: [  29.7367   69.9254  -56.9683  -19.4496 -281.0946]\n",
      "Weights: [-4.3604 -0.6794 -0.3718  0.1     0.0968]\n",
      "MSE loss: 140.9991\n",
      "Iteration: 55100\n",
      "Gradient: [   -5.9196   -84.2144   198.3127   359.4302 -1022.579 ]\n",
      "Weights: [-4.3602 -0.6787 -0.3724  0.0999  0.0969]\n",
      "MSE loss: 140.9149\n",
      "Iteration: 55200\n",
      "Gradient: [ -47.309    -6.6646  144.8241 -283.8765 -862.6302]\n",
      "Weights: [-4.3597 -0.6777 -0.3732  0.0998  0.0969]\n",
      "MSE loss: 140.8124\n",
      "Iteration: 55300\n",
      "Gradient: [ -21.4337  -23.3035  184.8422 -413.9739 -188.3299]\n",
      "Weights: [-4.3611 -0.6761 -0.3738  0.0998  0.097 ]\n",
      "MSE loss: 140.722\n",
      "Iteration: 55400\n",
      "Gradient: [   51.133    -74.9913     4.9154   354.2068 -2318.0974]\n",
      "Weights: [-4.3589 -0.6752 -0.3747  0.0997  0.097 ]\n",
      "MSE loss: 140.6251\n",
      "Iteration: 55500\n",
      "Gradient: [  -2.9214   11.6909   88.8024 -405.6998 -942.7955]\n",
      "Weights: [-4.3591 -0.6741 -0.3755  0.0997  0.0971]\n",
      "MSE loss: 140.5268\n",
      "Iteration: 55600\n",
      "Gradient: [   19.935     45.7168    -3.2057   319.5011 -2507.2106]\n",
      "Weights: [-4.3601 -0.6729 -0.3761  0.0997  0.0971]\n",
      "MSE loss: 140.4357\n",
      "Iteration: 55700\n",
      "Gradient: [  16.5565    4.6849  204.3162 -127.7315   79.6497]\n",
      "Weights: [-4.3606 -0.672  -0.3767  0.0997  0.0972]\n",
      "MSE loss: 140.3591\n",
      "Iteration: 55800\n",
      "Gradient: [-23.4401 -47.8556 -21.3735 147.8314 408.5316]\n",
      "Weights: [-4.3617 -0.6704 -0.3773  0.0996  0.0972]\n",
      "MSE loss: 140.2689\n",
      "Iteration: 55900\n",
      "Gradient: [ -64.7943   61.8808  117.954   156.4236 -875.4613]\n",
      "Weights: [-4.3612 -0.6687 -0.3781  0.0996  0.0973]\n",
      "MSE loss: 140.172\n",
      "Iteration: 56000\n",
      "Gradient: [ -39.06    -33.8016  -10.2302 -317.4838 -260.4718]\n",
      "Weights: [-4.3629 -0.6678 -0.3787  0.0995  0.0973]\n",
      "MSE loss: 140.083\n",
      "Iteration: 56100\n",
      "Gradient: [-20.4434  48.3231  60.1481 164.7846  -8.0055]\n",
      "Weights: [-4.363  -0.6663 -0.3793  0.0994  0.0974]\n",
      "MSE loss: 139.9898\n",
      "Iteration: 56200\n",
      "Gradient: [   20.5329   -21.1331     7.3483  -228.0888 -1317.7887]\n",
      "Weights: [-4.3648 -0.665  -0.38    0.0993  0.0974]\n",
      "MSE loss: 139.9017\n",
      "Iteration: 56300\n",
      "Gradient: [  41.7412  -36.9072  204.1966  408.9384 1377.9594]\n",
      "Weights: [-4.3623 -0.6639 -0.3808  0.0993  0.0975]\n",
      "MSE loss: 139.8034\n",
      "Iteration: 56400\n",
      "Gradient: [ -3.2643  10.7731 -12.4345  36.384  419.5972]\n",
      "Weights: [-4.3633 -0.6628 -0.3815  0.0993  0.0976]\n",
      "MSE loss: 139.7095\n",
      "Iteration: 56500\n",
      "Gradient: [   6.6507   18.7468   35.4302  115.8847 -456.9875]\n",
      "Weights: [-4.3639 -0.6617 -0.382   0.0992  0.0976]\n",
      "MSE loss: 139.6288\n",
      "Iteration: 56600\n",
      "Gradient: [  -18.8835   -11.1437    54.2573  -103.8954 -1265.9023]\n",
      "Weights: [-4.3648 -0.6608 -0.3827  0.0992  0.0977]\n",
      "MSE loss: 139.544\n",
      "Iteration: 56700\n",
      "Gradient: [ -10.5301 -105.0641   -3.2619  -74.1774 -514.1371]\n",
      "Weights: [-4.3642 -0.6604 -0.3832  0.0991  0.0977]\n",
      "MSE loss: 139.4649\n",
      "Iteration: 56800\n",
      "Gradient: [ -3.0628  -5.7672  37.6668  77.9757 769.3832]\n",
      "Weights: [-4.3637 -0.6593 -0.384   0.0991  0.0978]\n",
      "MSE loss: 139.371\n",
      "Iteration: 56900\n",
      "Gradient: [ 10.4409  32.6628 113.5805 150.5368 518.7255]\n",
      "Weights: [-4.366  -0.6582 -0.3845  0.099   0.0978]\n",
      "MSE loss: 139.2964\n",
      "Iteration: 57000\n",
      "Gradient: [  17.0924  -16.4829  203.2626   60.9418 -926.1836]\n",
      "Weights: [-4.3662 -0.6565 -0.3851  0.099   0.0979]\n",
      "MSE loss: 139.2007\n",
      "Iteration: 57100\n",
      "Gradient: [ -0.9083   8.3526  45.4855   6.4943 494.7022]\n",
      "Weights: [-4.3666 -0.6553 -0.3856  0.0989  0.098 ]\n",
      "MSE loss: 139.1223\n",
      "Iteration: 57200\n",
      "Gradient: [   5.6623  -58.5886  157.9444   22.201  -692.7105]\n",
      "Weights: [-4.3663 -0.6539 -0.3863  0.0989  0.098 ]\n",
      "MSE loss: 139.0322\n",
      "Iteration: 57300\n",
      "Gradient: [ 15.4627 -45.6724 -55.4441  53.6652 384.1542]\n",
      "Weights: [-4.3684 -0.6525 -0.3871  0.0988  0.0981]\n",
      "MSE loss: 138.932\n",
      "Iteration: 57400\n",
      "Gradient: [    5.9402    -9.9026   175.1099   588.6287 -1187.7859]\n",
      "Weights: [-4.3681 -0.6509 -0.3879  0.0988  0.0981]\n",
      "MSE loss: 138.8278\n",
      "Iteration: 57500\n",
      "Gradient: [   33.0559   -46.8219    12.3268   194.1604 -1131.1551]\n",
      "Weights: [-4.3678 -0.6496 -0.3886  0.0987  0.0982]\n",
      "MSE loss: 138.7353\n",
      "Iteration: 57600\n",
      "Gradient: [    4.9761   -25.7457    53.0225   -32.3593 -1121.3767]\n",
      "Weights: [-4.3697 -0.6481 -0.3893  0.0987  0.0982]\n",
      "MSE loss: 138.6471\n",
      "Iteration: 57700\n",
      "Gradient: [  -17.2703    39.6917   -93.2196   128.2503 -1064.7578]\n",
      "Weights: [-4.3698 -0.6467 -0.3901  0.0986  0.0983]\n",
      "MSE loss: 138.5479\n",
      "Iteration: 57800\n",
      "Gradient: [  37.972   -39.8081   86.1807   34.9802 -963.4766]\n",
      "Weights: [-4.3688 -0.6457 -0.3907  0.0986  0.0983]\n",
      "MSE loss: 138.4634\n",
      "Iteration: 57900\n",
      "Gradient: [  11.4748  -51.3369  153.6411  340.2327 -459.7746]\n",
      "Weights: [-4.372  -0.6443 -0.3913  0.0985  0.0984]\n",
      "MSE loss: 138.3794\n",
      "Iteration: 58000\n",
      "Gradient: [  13.1682  -36.0595   31.8619 -470.3165 -616.8845]\n",
      "Weights: [-4.3725 -0.6425 -0.392   0.0985  0.0984]\n",
      "MSE loss: 138.2817\n",
      "Iteration: 58100\n",
      "Gradient: [-20.3549  41.9892 158.0994 255.1201 321.3123]\n",
      "Weights: [-4.3715 -0.642  -0.3927  0.0985  0.0985]\n",
      "MSE loss: 138.1998\n",
      "Iteration: 58200\n",
      "Gradient: [  -2.7505  -20.2169  169.7357  188.742  -618.9802]\n",
      "Weights: [-4.3701 -0.6415 -0.3933  0.0984  0.0985]\n",
      "MSE loss: 138.1257\n",
      "Iteration: 58300\n",
      "Gradient: [  10.9618  -23.9294   44.9797   -4.6309 -437.1299]\n",
      "Weights: [-4.3713 -0.6403 -0.3939  0.0984  0.0986]\n",
      "MSE loss: 138.043\n",
      "Iteration: 58400\n",
      "Gradient: [-2.589800e+00 -1.066000e+00  4.802630e+01  2.707960e+01 -1.308666e+03]\n",
      "Weights: [-4.371  -0.639  -0.3946  0.0983  0.0986]\n",
      "MSE loss: 137.9502\n",
      "Iteration: 58500\n",
      "Gradient: [   -3.1873   -63.0522   185.271   -389.2381 -1065.7095]\n",
      "Weights: [-4.3694 -0.6384 -0.3952  0.0983  0.0987]\n",
      "MSE loss: 137.8805\n",
      "Iteration: 58600\n",
      "Gradient: [   31.982     -9.4101    53.1911   143.758  -1007.3832]\n",
      "Weights: [-4.3708 -0.6377 -0.3959  0.0983  0.0987]\n",
      "MSE loss: 137.7985\n",
      "Iteration: 58700\n",
      "Gradient: [  -5.1588  -15.8771   52.2893  308.9123 -691.445 ]\n",
      "Weights: [-4.3711 -0.6364 -0.3964  0.0982  0.0988]\n",
      "MSE loss: 137.7134\n",
      "Iteration: 58800\n",
      "Gradient: [ -25.268     6.2717  166.4586   96.4393 -636.4165]\n",
      "Weights: [-4.3721 -0.635  -0.3971  0.0982  0.0989]\n",
      "MSE loss: 137.6212\n",
      "Iteration: 58900\n",
      "Gradient: [ 39.545  -11.4774 165.0647 158.6343 508.7385]\n",
      "Weights: [-4.3736 -0.6338 -0.3976  0.0982  0.0989]\n",
      "MSE loss: 137.5465\n",
      "Iteration: 59000\n",
      "Gradient: [    5.3649    15.3686    13.1253  -217.8579 -2146.197 ]\n",
      "Weights: [-4.3746 -0.6321 -0.3982  0.0981  0.099 ]\n",
      "MSE loss: 137.4618\n",
      "Iteration: 59100\n",
      "Gradient: [ -30.7056  -23.7165  164.5405  -55.3509 -557.314 ]\n",
      "Weights: [-4.3761 -0.6307 -0.3988  0.098   0.099 ]\n",
      "MSE loss: 137.3857\n",
      "Iteration: 59200\n",
      "Gradient: [ -10.8303   12.0636  114.8127   20.7304 -477.9046]\n",
      "Weights: [-4.3743 -0.6294 -0.3995  0.098   0.099 ]\n",
      "MSE loss: 137.3002\n",
      "Iteration: 59300\n",
      "Gradient: [ -10.6048   22.8435   26.7552  326.155  -307.1322]\n",
      "Weights: [-4.3745 -0.6282 -0.4004  0.098   0.0991]\n",
      "MSE loss: 137.2044\n",
      "Iteration: 59400\n",
      "Gradient: [  7.7469 -12.8279 -79.2492 155.1592 376.8389]\n",
      "Weights: [-4.3757 -0.6275 -0.4009  0.0979  0.0991]\n",
      "MSE loss: 137.1374\n",
      "Iteration: 59500\n",
      "Gradient: [   46.4535   -36.4273    23.8608   268.6387 -1223.3212]\n",
      "Weights: [-4.3746 -0.6264 -0.4016  0.0979  0.0992]\n",
      "MSE loss: 137.046\n",
      "Iteration: 59600\n",
      "Gradient: [ -18.9723  -15.8579   51.1045  220.9882 -938.2936]\n",
      "Weights: [-4.3746 -0.6255 -0.4023  0.0978  0.0992]\n",
      "MSE loss: 136.966\n",
      "Iteration: 59700\n",
      "Gradient: [ 3.116070e+01  1.024250e+01  4.175140e+01  2.400000e-03 -6.749188e+02]\n",
      "Weights: [-4.374  -0.6244 -0.4029  0.0978  0.0993]\n",
      "MSE loss: 136.8905\n",
      "Iteration: 59800\n",
      "Gradient: [   5.5527    2.6928  -93.8085  452.7019 -748.5568]\n",
      "Weights: [-4.376  -0.6227 -0.4036  0.0978  0.0993]\n",
      "MSE loss: 136.7889\n",
      "Iteration: 59900\n",
      "Gradient: [  46.4354  -20.8908  -31.4719   -8.0693 -698.5854]\n",
      "Weights: [-4.3764 -0.6218 -0.4042  0.0977  0.0994]\n",
      "MSE loss: 136.7135\n",
      "Iteration: 60000\n",
      "Gradient: [    6.7098   -13.8839    89.4073  -306.2816 -1243.5945]\n",
      "Weights: [-4.3777 -0.6202 -0.4048  0.0977  0.0994]\n",
      "MSE loss: 136.6282\n",
      "Iteration: 60100\n",
      "Gradient: [  54.4695   53.3483  180.2843  258.9902 -606.6964]\n",
      "Weights: [-4.3749 -0.6189 -0.4056  0.0976  0.0995]\n",
      "MSE loss: 136.5446\n",
      "Iteration: 60200\n",
      "Gradient: [   1.7537   20.9099  -74.0211  189.8673 -589.2642]\n",
      "Weights: [-4.377  -0.6179 -0.4062  0.0976  0.0995]\n",
      "MSE loss: 136.4555\n",
      "Iteration: 60300\n",
      "Gradient: [ -17.7697   22.77     76.789   297.87   -919.1034]\n",
      "Weights: [-4.3769 -0.6174 -0.4067  0.0975  0.0996]\n",
      "MSE loss: 136.3875\n",
      "Iteration: 60400\n",
      "Gradient: [  13.6471   23.7833   14.7054  173.7869 -475.3609]\n",
      "Weights: [-4.3768 -0.6165 -0.4072  0.0975  0.0996]\n",
      "MSE loss: 136.3206\n",
      "Iteration: 60500\n",
      "Gradient: [ -23.8084  -96.6393  166.4637   61.9103 -272.0775]\n",
      "Weights: [-4.3764 -0.6158 -0.4078  0.0974  0.0997]\n",
      "MSE loss: 136.2441\n",
      "Iteration: 60600\n",
      "Gradient: [  -5.2621   30.8866  107.7913   43.4741 -996.4848]\n",
      "Weights: [-4.3772 -0.6142 -0.4085  0.0974  0.0997]\n",
      "MSE loss: 136.1568\n",
      "Iteration: 60700\n",
      "Gradient: [ 17.0331   1.8454 -33.0209 -31.0351 -34.5427]\n",
      "Weights: [-4.3784 -0.6135 -0.4091  0.0974  0.0998]\n",
      "MSE loss: 136.0731\n",
      "Iteration: 60800\n",
      "Gradient: [   -9.2135    45.0825   208.6324  -195.3593 -1514.6552]\n",
      "Weights: [-4.3783 -0.6126 -0.4097  0.0974  0.0999]\n",
      "MSE loss: 135.994\n",
      "Iteration: 60900\n",
      "Gradient: [  26.3623  -46.7219   50.1256  329.3361 -456.5511]\n",
      "Weights: [-4.3791 -0.6113 -0.4103  0.0973  0.0999]\n",
      "MSE loss: 135.9138\n",
      "Iteration: 61000\n",
      "Gradient: [ -25.7427   50.9067   64.1612  162.4188 -652.402 ]\n",
      "Weights: [-4.3813 -0.6102 -0.4108  0.0973  0.1   ]\n",
      "MSE loss: 135.8374\n",
      "Iteration: 61100\n",
      "Gradient: [  -55.9663   -13.3187    93.261    137.21   -1337.3289]\n",
      "Weights: [-4.381  -0.6084 -0.4115  0.0972  0.1   ]\n",
      "MSE loss: 135.7454\n",
      "Iteration: 61200\n",
      "Gradient: [ 16.6999   1.6106 157.2463  41.1827 477.4226]\n",
      "Weights: [-4.379  -0.6074 -0.4123  0.0972  0.1001]\n",
      "MSE loss: 135.6693\n",
      "Iteration: 61300\n",
      "Gradient: [-30.3098   3.9614  80.2136 214.8535  13.4748]\n",
      "Weights: [-4.3815 -0.6059 -0.4129  0.0972  0.1001]\n",
      "MSE loss: 135.5784\n",
      "Iteration: 61400\n",
      "Gradient: [  -7.8997  -21.4752   97.0241  231.2718 -884.3361]\n",
      "Weights: [-4.3823 -0.605  -0.4135  0.0972  0.1002]\n",
      "MSE loss: 135.505\n",
      "Iteration: 61500\n",
      "Gradient: [  -2.0905  -34.9463   39.1918 -168.1212 -763.4037]\n",
      "Weights: [-4.3829 -0.6037 -0.4142  0.0971  0.1002]\n",
      "MSE loss: 135.4167\n",
      "Iteration: 61600\n",
      "Gradient: [   18.2014   -35.998    174.2438   630.3362 -1297.3838]\n",
      "Weights: [-4.382  -0.6028 -0.4149  0.0971  0.1003]\n",
      "MSE loss: 135.3371\n",
      "Iteration: 61700\n",
      "Gradient: [  -4.2149  -67.0758  120.5609 -565.9532 -749.4451]\n",
      "Weights: [-4.3827 -0.6018 -0.4155  0.097   0.1003]\n",
      "MSE loss: 135.261\n",
      "Iteration: 61800\n",
      "Gradient: [  12.9464   32.5685  142.1272   78.3324 -427.1602]\n",
      "Weights: [-4.3834 -0.6007 -0.4162  0.097   0.1004]\n",
      "MSE loss: 135.1829\n",
      "Iteration: 61900\n",
      "Gradient: [ 6.670000e-01 -7.463490e+01 -7.816280e+01  7.831660e+01 -7.074297e+02]\n",
      "Weights: [-4.3829 -0.5998 -0.4168  0.097   0.1004]\n",
      "MSE loss: 135.1089\n",
      "Iteration: 62000\n",
      "Gradient: [  46.3139  -43.4308   32.1991 -168.6514  393.9771]\n",
      "Weights: [-4.3832 -0.5987 -0.4173  0.097   0.1005]\n",
      "MSE loss: 135.035\n",
      "Iteration: 62100\n",
      "Gradient: [-22.6013  58.656  -51.9905 131.6975 304.1146]\n",
      "Weights: [-4.3851 -0.5971 -0.4179  0.0969  0.1005]\n",
      "MSE loss: 134.9526\n",
      "Iteration: 62200\n",
      "Gradient: [   2.5083   -3.1757  -22.0771  -27.5074 -782.6681]\n",
      "Weights: [-4.3852 -0.5961 -0.4185  0.0969  0.1005]\n",
      "MSE loss: 134.8783\n",
      "Iteration: 62300\n",
      "Gradient: [-43.3251  -4.7841 -33.4318  24.6262 -32.6004]\n",
      "Weights: [-4.3835 -0.5951 -0.4192  0.0968  0.1006]\n",
      "MSE loss: 134.8005\n",
      "Iteration: 62400\n",
      "Gradient: [ -18.996   -12.3936  139.3281 -269.2109  121.9907]\n",
      "Weights: [-4.3825 -0.5945 -0.4198  0.0967  0.1006]\n",
      "MSE loss: 134.7293\n",
      "Iteration: 62500\n",
      "Gradient: [  35.0894    7.1056   29.1106  -90.2822 -960.496 ]\n",
      "Weights: [-4.3847 -0.5937 -0.4203  0.0967  0.1007]\n",
      "MSE loss: 134.6576\n",
      "Iteration: 62600\n",
      "Gradient: [  14.8165    7.3508  151.0088  131.0569 -867.3093]\n",
      "Weights: [-4.3856 -0.5922 -0.4208  0.0967  0.1007]\n",
      "MSE loss: 134.5859\n",
      "Iteration: 62700\n",
      "Gradient: [ -15.9278  -41.5477   90.6808 -114.5429  108.8196]\n",
      "Weights: [-4.3848 -0.5911 -0.4216  0.0966  0.1008]\n",
      "MSE loss: 134.4948\n",
      "Iteration: 62800\n",
      "Gradient: [  -0.2648  -42.3717    7.7439 -118.8265  -27.8337]\n",
      "Weights: [-4.3855 -0.5904 -0.4222  0.0966  0.1008]\n",
      "MSE loss: 134.4244\n",
      "Iteration: 62900\n",
      "Gradient: [ -16.7159  -53.6865  -89.9864 -110.238   378.8058]\n",
      "Weights: [-4.386  -0.5886 -0.4228  0.0965  0.1009]\n",
      "MSE loss: 134.3398\n",
      "Iteration: 63000\n",
      "Gradient: [ -12.1264  -29.7815   57.8576  -70.2733 -528.8892]\n",
      "Weights: [-4.3872 -0.5874 -0.4233  0.0965  0.1009]\n",
      "MSE loss: 134.2691\n",
      "Iteration: 63100\n",
      "Gradient: [  33.6012   -5.4184   35.3748   72.4577 -558.7599]\n",
      "Weights: [-4.3867 -0.5859 -0.4239  0.0964  0.101 ]\n",
      "MSE loss: 134.1843\n",
      "Iteration: 63200\n",
      "Gradient: [   10.3581    10.344    -95.172      5.0977 -1086.4624]\n",
      "Weights: [-4.3884 -0.5845 -0.4244  0.0963  0.101 ]\n",
      "MSE loss: 134.1104\n",
      "Iteration: 63300\n",
      "Gradient: [  -2.2176    7.7625  -53.0828  -81.3032 -732.3296]\n",
      "Weights: [-4.3886 -0.5838 -0.425   0.0963  0.1011]\n",
      "MSE loss: 134.04\n",
      "Iteration: 63400\n",
      "Gradient: [   27.5933   -19.8102    31.1791   212.5818 -1147.9524]\n",
      "Weights: [-4.3891 -0.5826 -0.4257  0.0962  0.1011]\n",
      "MSE loss: 133.9563\n",
      "Iteration: 63500\n",
      "Gradient: [   3.6554  -45.4086   18.2451  333.9267 -384.8717]\n",
      "Weights: [-4.3901 -0.5817 -0.4263  0.0962  0.1012]\n",
      "MSE loss: 133.8819\n",
      "Iteration: 63600\n",
      "Gradient: [  30.9221   -4.7799   60.1809 -179.0918  417.295 ]\n",
      "Weights: [-4.3901 -0.5806 -0.4267  0.0962  0.1012]\n",
      "MSE loss: 133.8119\n",
      "Iteration: 63700\n",
      "Gradient: [  -27.0743   -41.6901    13.0152   213.2005 -1409.2149]\n",
      "Weights: [-4.3909 -0.5799 -0.4272  0.0962  0.1013]\n",
      "MSE loss: 133.7532\n",
      "Iteration: 63800\n",
      "Gradient: [   5.745    30.6807   35.7882  187.2489 -621.8236]\n",
      "Weights: [-4.391  -0.5785 -0.4278  0.0961  0.1013]\n",
      "MSE loss: 133.677\n",
      "Iteration: 63900\n",
      "Gradient: [ 7.9020000e-01  2.7051500e+01 -9.4361700e+01  3.3450450e+02\n",
      " -1.0065432e+03]\n",
      "Weights: [-4.3881 -0.5781 -0.4286  0.096   0.1014]\n",
      "MSE loss: 133.602\n",
      "Iteration: 64000\n",
      "Gradient: [  20.5034   31.1604   -8.962   479.6339 -935.7981]\n",
      "Weights: [-4.3876 -0.5773 -0.4292  0.096   0.1014]\n",
      "MSE loss: 133.5323\n",
      "Iteration: 64100\n",
      "Gradient: [ -38.9647  -32.6257   -1.3604 -365.8529  604.7198]\n",
      "Weights: [-4.3892 -0.5765 -0.4297  0.096   0.1015]\n",
      "MSE loss: 133.4588\n",
      "Iteration: 64200\n",
      "Gradient: [   28.6248   -19.8225    30.9814  -148.8665 -1041.1995]\n",
      "Weights: [-4.3879 -0.5754 -0.4305  0.0959  0.1015]\n",
      "MSE loss: 133.3811\n",
      "Iteration: 64300\n",
      "Gradient: [ -32.4061   26.1485   29.2904 -157.9142  246.2579]\n",
      "Weights: [-4.3905 -0.5733 -0.4311  0.0959  0.1016]\n",
      "MSE loss: 133.2838\n",
      "Iteration: 64400\n",
      "Gradient: [ -2.2502  58.9762 107.75   456.4614 -45.3074]\n",
      "Weights: [-4.3904 -0.5719 -0.4317  0.0958  0.1016]\n",
      "MSE loss: 133.212\n",
      "Iteration: 64500\n",
      "Gradient: [ -63.561   -38.8862  101.0845 -334.5169   28.2097]\n",
      "Weights: [-4.3923 -0.571  -0.4325  0.0958  0.1017]\n",
      "MSE loss: 133.1256\n",
      "Iteration: 64600\n",
      "Gradient: [  -30.6427   -17.3182    20.9416   501.4314 -1090.1122]\n",
      "Weights: [-4.3927 -0.5697 -0.4329  0.0958  0.1017]\n",
      "MSE loss: 133.0555\n",
      "Iteration: 64700\n",
      "Gradient: [ -38.0214  -84.977   137.7142 -238.92   -551.2463]\n",
      "Weights: [-4.3934 -0.5685 -0.4336  0.0957  0.1017]\n",
      "MSE loss: 132.9805\n",
      "Iteration: 64800\n",
      "Gradient: [-23.9708  45.7537 293.0038 449.2372 -87.8776]\n",
      "Weights: [-4.394  -0.5669 -0.4341  0.0957  0.1018]\n",
      "MSE loss: 132.9034\n",
      "Iteration: 64900\n",
      "Gradient: [  -8.16     50.4266   48.9515 -384.0585 -609.7886]\n",
      "Weights: [-4.3943 -0.566  -0.4349  0.0957  0.1018]\n",
      "MSE loss: 132.8272\n",
      "Iteration: 65000\n",
      "Gradient: [  8.6778  14.2444  51.7874 293.229  178.0374]\n",
      "Weights: [-4.3933 -0.5651 -0.4353  0.0956  0.1019]\n",
      "MSE loss: 132.7611\n",
      "Iteration: 65100\n",
      "Gradient: [   3.4583  -55.426   259.74     10.5362 -284.3408]\n",
      "Weights: [-4.3951 -0.5637 -0.4361  0.0956  0.1019]\n",
      "MSE loss: 132.6738\n",
      "Iteration: 65200\n",
      "Gradient: [  27.9907    2.0964  132.049    38.9802 -115.089 ]\n",
      "Weights: [-4.3961 -0.5619 -0.4367  0.0956  0.102 ]\n",
      "MSE loss: 132.5942\n",
      "Iteration: 65300\n",
      "Gradient: [ 1.0364000e+00 -1.0587500e+01  6.7863500e+01 -3.3301430e+02\n",
      " -1.0945596e+03]\n",
      "Weights: [-4.3978 -0.5607 -0.4373  0.0955  0.102 ]\n",
      "MSE loss: 132.5201\n",
      "Iteration: 65400\n",
      "Gradient: [  -22.562     -4.5778   -47.2643   -22.0047 -1190.5589]\n",
      "Weights: [-4.3967 -0.5599 -0.438   0.0955  0.1021]\n",
      "MSE loss: 132.4403\n",
      "Iteration: 65500\n",
      "Gradient: [  18.1177   11.2118   97.9926  186.7327 -382.9554]\n",
      "Weights: [-4.3967 -0.5587 -0.4385  0.0955  0.1021]\n",
      "MSE loss: 132.3674\n",
      "Iteration: 65600\n",
      "Gradient: [  -2.1371  -31.1178   86.6469 -375.6631 -608.2637]\n",
      "Weights: [-4.3958 -0.5574 -0.4393  0.0954  0.1022]\n",
      "MSE loss: 132.2834\n",
      "Iteration: 65700\n",
      "Gradient: [  -8.0776  -34.4036   10.9394 -351.2667 -604.0565]\n",
      "Weights: [-4.3969 -0.5569 -0.44    0.0954  0.1022]\n",
      "MSE loss: 132.21\n",
      "Iteration: 65800\n",
      "Gradient: [  48.3546  -49.8149  183.3881   78.3299 -875.8827]\n",
      "Weights: [-4.3963 -0.5559 -0.4406  0.0953  0.1023]\n",
      "MSE loss: 132.1357\n",
      "Iteration: 65900\n",
      "Gradient: [    7.6938   -74.2959   190.7952    34.562  -1150.2267]\n",
      "Weights: [-4.3957 -0.5552 -0.4412  0.0953  0.1023]\n",
      "MSE loss: 132.0662\n",
      "Iteration: 66000\n",
      "Gradient: [  -7.7945  -31.472    15.4073  325.1702 -325.8244]\n",
      "Weights: [-4.3982 -0.554  -0.4417  0.0953  0.1024]\n",
      "MSE loss: 131.9987\n",
      "Iteration: 66100\n",
      "Gradient: [ -26.6146 -134.959   -33.5108  -95.6389 -577.1596]\n",
      "Weights: [-4.3963 -0.5528 -0.4423  0.0953  0.1024]\n",
      "MSE loss: 131.933\n",
      "Iteration: 66200\n",
      "Gradient: [ 3.499550e+01  1.542000e-01  2.148390e+01 -1.182688e+02  6.447744e+02]\n",
      "Weights: [-4.3991 -0.5515 -0.4431  0.0953  0.1025]\n",
      "MSE loss: 131.8476\n",
      "Iteration: 66300\n",
      "Gradient: [  -4.5948   19.3003   79.1271  -79.4651 -884.8765]\n",
      "Weights: [-4.3974 -0.5503 -0.4439  0.0952  0.1025]\n",
      "MSE loss: 131.7519\n",
      "Iteration: 66400\n",
      "Gradient: [   -9.8845   -48.8924   104.8412  -275.5816 -1053.1928]\n",
      "Weights: [-4.3965 -0.5495 -0.4447  0.0952  0.1026]\n",
      "MSE loss: 131.6767\n",
      "Iteration: 66500\n",
      "Gradient: [ -13.2866  -15.4535   77.161   233.2346 -840.0043]\n",
      "Weights: [-4.3973 -0.5483 -0.4453  0.0952  0.1026]\n",
      "MSE loss: 131.5945\n",
      "Iteration: 66600\n",
      "Gradient: [ 5.2999 23.1678 70.1381 68.6172 24.1197]\n",
      "Weights: [-4.3985 -0.5472 -0.4458  0.0951  0.1027]\n",
      "MSE loss: 131.5248\n",
      "Iteration: 66700\n",
      "Gradient: [  13.4029  -15.3592  -40.2311 -271.1728 -991.7527]\n",
      "Weights: [-4.4009 -0.546  -0.4462  0.0951  0.1027]\n",
      "MSE loss: 131.4631\n",
      "Iteration: 66800\n",
      "Gradient: [ 31.1525 -75.0626 -78.7084 525.5003 186.9092]\n",
      "Weights: [-4.4007 -0.5448 -0.4469  0.095   0.1028]\n",
      "MSE loss: 131.3825\n",
      "Iteration: 66900\n",
      "Gradient: [ -22.1286   -2.9946  105.5825  -66.4937 -382.4777]\n",
      "Weights: [-4.4001 -0.5438 -0.4475  0.095   0.1028]\n",
      "MSE loss: 131.3136\n",
      "Iteration: 67000\n",
      "Gradient: [ -20.2581   35.6803  131.4181    7.8284 -241.6635]\n",
      "Weights: [-4.4009 -0.5426 -0.4481  0.095   0.1029]\n",
      "MSE loss: 131.236\n",
      "Iteration: 67100\n",
      "Gradient: [  15.045    61.2579  -33.514  -279.0484 -788.191 ]\n",
      "Weights: [-4.3991 -0.5422 -0.4487  0.0949  0.1029]\n",
      "MSE loss: 131.1771\n",
      "Iteration: 67200\n",
      "Gradient: [ -14.6172   15.2061  -95.2678  551.8942 -352.8908]\n",
      "Weights: [-4.4013 -0.5412 -0.4492  0.0949  0.103 ]\n",
      "MSE loss: 131.1033\n",
      "Iteration: 67300\n",
      "Gradient: [  28.8825  -16.846    66.8993  242.7276 -206.975 ]\n",
      "Weights: [-4.4024 -0.5395 -0.4498  0.0948  0.103 ]\n",
      "MSE loss: 131.0207\n",
      "Iteration: 67400\n",
      "Gradient: [-3.482000e-01 -2.356200e+00 -4.599370e+01  1.999173e+02 -8.226974e+02]\n",
      "Weights: [-4.4043 -0.5383 -0.4504  0.0948  0.1031]\n",
      "MSE loss: 130.9562\n",
      "Iteration: 67500\n",
      "Gradient: [-38.0883  29.3741  37.753  300.6917  55.4468]\n",
      "Weights: [-4.4041 -0.537  -0.4509  0.0947  0.1031]\n",
      "MSE loss: 130.8834\n",
      "Iteration: 67600\n",
      "Gradient: [  39.7351  -14.3085  137.4562  -23.4771 -540.5506]\n",
      "Weights: [-4.404  -0.5358 -0.4516  0.0947  0.1031]\n",
      "MSE loss: 130.8063\n",
      "Iteration: 67700\n",
      "Gradient: [  15.8135   45.0497  -47.7999 -194.408  -756.4601]\n",
      "Weights: [-4.404  -0.535  -0.4521  0.0947  0.1032]\n",
      "MSE loss: 130.7387\n",
      "Iteration: 67800\n",
      "Gradient: [ 1.1722000e+00 -2.2481000e+00  3.1486200e+01 -9.6009700e+01\n",
      " -1.6689298e+03]\n",
      "Weights: [-4.4038 -0.5339 -0.4528  0.0946  0.1032]\n",
      "MSE loss: 130.6593\n",
      "Iteration: 67900\n",
      "Gradient: [ 20.8664 -33.6499   6.7797  49.0753 230.4694]\n",
      "Weights: [-4.4041 -0.5332 -0.4532  0.0946  0.1033]\n",
      "MSE loss: 130.6\n",
      "Iteration: 68000\n",
      "Gradient: [  -9.1425  -51.293   141.5005 -194.9159  474.9669]\n",
      "Weights: [-4.4059 -0.5322 -0.4538  0.0946  0.1033]\n",
      "MSE loss: 130.5411\n",
      "Iteration: 68100\n",
      "Gradient: [ -27.5589   23.4827   29.5128  -28.3348 -506.4437]\n",
      "Weights: [-4.4047 -0.5306 -0.4545  0.0945  0.1034]\n",
      "MSE loss: 130.4558\n",
      "Iteration: 68200\n",
      "Gradient: [  -2.7215  -53.5259   -0.6876 -188.6959  472.4928]\n",
      "Weights: [-4.405  -0.5298 -0.455   0.0945  0.1034]\n",
      "MSE loss: 130.3894\n",
      "Iteration: 68300\n",
      "Gradient: [  -38.5382    -4.7422   168.766   -385.9637 -1325.6047]\n",
      "Weights: [-4.4062 -0.5285 -0.4556  0.0944  0.1035]\n",
      "MSE loss: 130.3173\n",
      "Iteration: 68400\n",
      "Gradient: [   5.2967   -3.3354   14.3736  -28.7382 -769.0201]\n",
      "Weights: [-4.4078 -0.5271 -0.4561  0.0943  0.1035]\n",
      "MSE loss: 130.253\n",
      "Iteration: 68500\n",
      "Gradient: [   7.0074  -78.2033   21.4664  225.103  -572.1997]\n",
      "Weights: [-4.4056 -0.5262 -0.4568  0.0943  0.1036]\n",
      "MSE loss: 130.1754\n",
      "Iteration: 68600\n",
      "Gradient: [ 1.459000e-01  6.933700e+00 -3.495420e+01  2.165732e+02 -4.098424e+02]\n",
      "Weights: [-4.4049 -0.5256 -0.4575  0.0943  0.1036]\n",
      "MSE loss: 130.103\n",
      "Iteration: 68700\n",
      "Gradient: [   8.0452   70.3548   87.7883 -313.6328 -970.0027]\n",
      "Weights: [-4.4058 -0.5244 -0.4581  0.0942  0.1037]\n",
      "MSE loss: 130.0308\n",
      "Iteration: 68800\n",
      "Gradient: [  -3.1084  -65.258   245.1661   77.3419 -485.7349]\n",
      "Weights: [-4.4062 -0.5233 -0.4587  0.0942  0.1037]\n",
      "MSE loss: 129.9599\n",
      "Iteration: 68900\n",
      "Gradient: [  12.1451  -42.8479    2.4143   26.4886 -227.7159]\n",
      "Weights: [-4.4052 -0.5218 -0.4594  0.0941  0.1038]\n",
      "MSE loss: 129.8768\n",
      "Iteration: 69000\n",
      "Gradient: [  11.9889  -24.363   179.1167  303.301  -855.174 ]\n",
      "Weights: [-4.4076 -0.5206 -0.4599  0.0941  0.1038]\n",
      "MSE loss: 129.8087\n",
      "Iteration: 69100\n",
      "Gradient: [ -19.2938  -68.4259  -29.7413  269.3083 -390.8362]\n",
      "Weights: [-4.4085 -0.5195 -0.4605  0.0941  0.1038]\n",
      "MSE loss: 129.7383\n",
      "Iteration: 69200\n",
      "Gradient: [  -38.0207    23.6982     9.9909   147.4682 -1627.5509]\n",
      "Weights: [-4.4086 -0.5189 -0.461   0.094   0.1039]\n",
      "MSE loss: 129.6834\n",
      "Iteration: 69300\n",
      "Gradient: [   32.1729     3.6177   141.2978   182.089  -1448.7641]\n",
      "Weights: [-4.4072 -0.5178 -0.4616  0.094   0.1039]\n",
      "MSE loss: 129.6175\n",
      "Iteration: 69400\n",
      "Gradient: [-5.0300000e-01 -1.3592200e+01  1.8261300e+01 -1.3142980e+02\n",
      " -1.3038682e+03]\n",
      "Weights: [-4.4086 -0.5165 -0.4622  0.094   0.104 ]\n",
      "MSE loss: 129.5416\n",
      "Iteration: 69500\n",
      "Gradient: [   21.3897    -1.9878   157.5473    26.5367 -1539.3677]\n",
      "Weights: [-4.4095 -0.5149 -0.4627  0.0939  0.104 ]\n",
      "MSE loss: 129.4708\n",
      "Iteration: 69600\n",
      "Gradient: [   4.4465  -24.6074  182.5555  -55.2584 -625.0363]\n",
      "Weights: [-4.4105 -0.5135 -0.4633  0.0939  0.1041]\n",
      "MSE loss: 129.4001\n",
      "Iteration: 69700\n",
      "Gradient: [ -7.3666  32.7864  53.1156  30.7952 267.609 ]\n",
      "Weights: [-4.4101 -0.5123 -0.464   0.0938  0.1041]\n",
      "MSE loss: 129.3158\n",
      "Iteration: 69800\n",
      "Gradient: [  12.8732   -4.7133   85.619   122.8881 -370.8219]\n",
      "Weights: [-4.4099 -0.5113 -0.4647  0.0937  0.1042]\n",
      "MSE loss: 129.2495\n",
      "Iteration: 69900\n",
      "Gradient: [  -2.6683   17.1249   36.4964  -10.7831 -452.1608]\n",
      "Weights: [-4.4105 -0.5103 -0.4652  0.0937  0.1042]\n",
      "MSE loss: 129.182\n",
      "Iteration: 70000\n",
      "Gradient: [  -32.0697   -37.4615   121.5873  -109.1082 -1228.8262]\n",
      "Weights: [-4.4119 -0.5092 -0.4659  0.0937  0.1043]\n",
      "MSE loss: 129.1036\n",
      "Iteration: 70100\n",
      "Gradient: [ 49.0346  38.965   25.0907  74.0572 -44.675 ]\n",
      "Weights: [-4.4119 -0.5074 -0.4664  0.0936  0.1043]\n",
      "MSE loss: 129.0293\n",
      "Iteration: 70200\n",
      "Gradient: [  -37.4148   -61.929      8.5271  -425.8121 -1197.6077]\n",
      "Weights: [-4.4144 -0.5072 -0.467   0.0936  0.1043]\n",
      "MSE loss: 128.9809\n",
      "Iteration: 70300\n",
      "Gradient: [  26.0126  -36.955    25.3886 -152.8514 -336.4252]\n",
      "Weights: [-4.4152 -0.506  -0.4674  0.0936  0.1044]\n",
      "MSE loss: 128.9207\n",
      "Iteration: 70400\n",
      "Gradient: [  -6.4228   41.4678  177.1023  -55.545  -901.0137]\n",
      "Weights: [-4.4149 -0.5044 -0.468   0.0936  0.1044]\n",
      "MSE loss: 128.8388\n",
      "Iteration: 70500\n",
      "Gradient: [-15.6958  30.6415 -13.2049 127.5354 216.7068]\n",
      "Weights: [-4.415  -0.5031 -0.4686  0.0935  0.1045]\n",
      "MSE loss: 128.7754\n",
      "Iteration: 70600\n",
      "Gradient: [ 13.2977  -7.9078  43.9799 179.802  403.1578]\n",
      "Weights: [-4.4143 -0.5023 -0.4692  0.0935  0.1045]\n",
      "MSE loss: 128.7084\n",
      "Iteration: 70700\n",
      "Gradient: [  -7.6412   26.2799  -24.0547  -13.8205 -800.5826]\n",
      "Weights: [-4.4147 -0.5015 -0.4696  0.0934  0.1045]\n",
      "MSE loss: 128.6577\n",
      "Iteration: 70800\n",
      "Gradient: [ 8.378300e+00 -4.671660e+01 -1.290914e+02  3.720000e-02 -5.822502e+02]\n",
      "Weights: [-4.4147 -0.5001 -0.4702  0.0934  0.1046]\n",
      "MSE loss: 128.5878\n",
      "Iteration: 70900\n",
      "Gradient: [  15.3878  -65.7255  115.8811  441.3135 -749.0715]\n",
      "Weights: [-4.4148 -0.4989 -0.4708  0.0933  0.1046]\n",
      "MSE loss: 128.525\n",
      "Iteration: 71000\n",
      "Gradient: [-24.6806  39.5541 120.5177 377.7874 384.98  ]\n",
      "Weights: [-4.4185 -0.4971 -0.4712  0.0933  0.1047]\n",
      "MSE loss: 128.4591\n",
      "Iteration: 71100\n",
      "Gradient: [ -27.7785  -49.1365  127.4615  272.0789 -126.0416]\n",
      "Weights: [-4.4183 -0.4961 -0.472   0.0933  0.1047]\n",
      "MSE loss: 128.3821\n",
      "Iteration: 71200\n",
      "Gradient: [   18.9896    13.6334    -9.4476  -264.1846 -1047.2915]\n",
      "Weights: [-4.4176 -0.4948 -0.4726  0.0932  0.1047]\n",
      "MSE loss: 128.3093\n",
      "Iteration: 71300\n",
      "Gradient: [   3.9847  -30.3631   91.0323  -93.9067 -535.4525]\n",
      "Weights: [-4.4164 -0.4942 -0.4733  0.0932  0.1048]\n",
      "MSE loss: 128.2433\n",
      "Iteration: 71400\n",
      "Gradient: [ -13.3021    2.0552   97.8626   87.5317 -427.7797]\n",
      "Weights: [-4.4175 -0.4932 -0.4738  0.0931  0.1048]\n",
      "MSE loss: 128.1736\n",
      "Iteration: 71500\n",
      "Gradient: [ -29.6835  -56.339    14.6863  -26.0794 -892.4435]\n",
      "Weights: [-4.4171 -0.4927 -0.4744  0.0931  0.1049]\n",
      "MSE loss: 128.1153\n",
      "Iteration: 71600\n",
      "Gradient: [  56.3309    8.455    -3.4056   82.4544 -210.1256]\n",
      "Weights: [-4.4181 -0.4914 -0.4749  0.0931  0.1049]\n",
      "MSE loss: 128.0483\n",
      "Iteration: 71700\n",
      "Gradient: [   6.9535  -54.6875   12.5452  253.0021 1355.5576]\n",
      "Weights: [-4.4186 -0.49   -0.4755  0.0931  0.105 ]\n",
      "MSE loss: 127.9782\n",
      "Iteration: 71800\n",
      "Gradient: [    7.9202    15.1092    88.8061   115.5539 -1227.2227]\n",
      "Weights: [-4.4192 -0.489  -0.4762  0.093   0.105 ]\n",
      "MSE loss: 127.9077\n",
      "Iteration: 71900\n",
      "Gradient: [  27.1646   70.705   113.994   183.2369 -860.3057]\n",
      "Weights: [-4.4181 -0.4876 -0.4767  0.093   0.1051]\n",
      "MSE loss: 127.8496\n",
      "Iteration: 72000\n",
      "Gradient: [  28.2043   11.1846   54.3686  136.5029 -973.9627]\n",
      "Weights: [-4.4188 -0.487  -0.4774  0.0929  0.1051]\n",
      "MSE loss: 127.7741\n",
      "Iteration: 72100\n",
      "Gradient: [ -42.4764  -23.0403   47.5187  115.9158 -574.6539]\n",
      "Weights: [-4.4186 -0.486  -0.478   0.0929  0.1052]\n",
      "MSE loss: 127.7051\n",
      "Iteration: 72200\n",
      "Gradient: [ -25.5071  105.8399   13.5954 -317.6672   14.0992]\n",
      "Weights: [-4.42   -0.4845 -0.4786  0.0929  0.1052]\n",
      "MSE loss: 127.6344\n",
      "Iteration: 72300\n",
      "Gradient: [   -3.9433   -26.3209    34.3572  -267.3425 -1740.4023]\n",
      "Weights: [-4.4209 -0.4834 -0.4793  0.0928  0.1052]\n",
      "MSE loss: 127.5605\n",
      "Iteration: 72400\n",
      "Gradient: [  -6.5463   15.9904  138.0748   88.1104 -425.6889]\n",
      "Weights: [-4.4198 -0.4826 -0.4798  0.0928  0.1053]\n",
      "MSE loss: 127.5003\n",
      "Iteration: 72500\n",
      "Gradient: [ 17.6226  35.1334  55.9681  76.737  -15.5531]\n",
      "Weights: [-4.4205 -0.4825 -0.4804  0.0928  0.1053]\n",
      "MSE loss: 127.4499\n",
      "Iteration: 72600\n",
      "Gradient: [  -20.7508    40.8699    37.7955  -256.1518 -1042.2961]\n",
      "Weights: [-4.4216 -0.4808 -0.4809  0.0928  0.1054]\n",
      "MSE loss: 127.3761\n",
      "Iteration: 72700\n",
      "Gradient: [ -47.3154  -29.8884   28.7502 -383.0832 -351.2651]\n",
      "Weights: [-4.4229 -0.4796 -0.4815  0.0928  0.1054]\n",
      "MSE loss: 127.3096\n",
      "Iteration: 72800\n",
      "Gradient: [ 1.2385000e+01 -2.6290000e-01  7.6579800e+01 -6.2837300e+01\n",
      " -1.0374898e+03]\n",
      "Weights: [-4.4211 -0.4788 -0.4821  0.0927  0.1055]\n",
      "MSE loss: 127.2481\n",
      "Iteration: 72900\n",
      "Gradient: [  16.9425   37.2984  139.7602   80.6037 -490.8593]\n",
      "Weights: [-4.4216 -0.4773 -0.4826  0.0927  0.1055]\n",
      "MSE loss: 127.1809\n",
      "Iteration: 73000\n",
      "Gradient: [ -24.2781  -13.0999 -108.9916  -38.6252 -755.2737]\n",
      "Weights: [-4.4238 -0.4759 -0.4832  0.0926  0.1056]\n",
      "MSE loss: 127.1068\n",
      "Iteration: 73100\n",
      "Gradient: [ -21.6251   43.0624   20.5094   41.0589 -990.0244]\n",
      "Weights: [-4.4243 -0.4751 -0.4838  0.0926  0.1056]\n",
      "MSE loss: 127.046\n",
      "Iteration: 73200\n",
      "Gradient: [    9.0309    34.6407   -43.1789    50.8849 -1335.9809]\n",
      "Weights: [-4.4247 -0.4741 -0.4844  0.0926  0.1056]\n",
      "MSE loss: 126.9858\n",
      "Iteration: 73300\n",
      "Gradient: [  -26.4115   -84.2115   158.5253    94.2677 -1005.4762]\n",
      "Weights: [-4.4239 -0.4729 -0.4852  0.0925  0.1057]\n",
      "MSE loss: 126.906\n",
      "Iteration: 73400\n",
      "Gradient: [ 56.7325 -13.0103   2.7271 310.7896 230.0441]\n",
      "Weights: [-4.423  -0.4718 -0.4857  0.0925  0.1057]\n",
      "MSE loss: 126.8458\n",
      "Iteration: 73500\n",
      "Gradient: [ -19.1852   48.5363  -17.5809  207.0601 -654.257 ]\n",
      "Weights: [-4.4243 -0.4702 -0.4862  0.0924  0.1058]\n",
      "MSE loss: 126.779\n",
      "Iteration: 73600\n",
      "Gradient: [  20.4345  -59.2131   94.0371  -20.2657 -297.5168]\n",
      "Weights: [-4.424  -0.4696 -0.4868  0.0924  0.1058]\n",
      "MSE loss: 126.7166\n",
      "Iteration: 73700\n",
      "Gradient: [ 13.9718  38.7923 -28.168  -90.3377 -57.7587]\n",
      "Weights: [-4.4238 -0.4687 -0.4875  0.0924  0.1059]\n",
      "MSE loss: 126.6479\n",
      "Iteration: 73800\n",
      "Gradient: [  -2.7721   15.0417  103.4377 -460.0545 -129.797 ]\n",
      "Weights: [-4.4247 -0.4679 -0.4881  0.0924  0.1059]\n",
      "MSE loss: 126.5866\n",
      "Iteration: 73900\n",
      "Gradient: [  16.7507   55.2544   84.4008  -39.1769 -543.0371]\n",
      "Weights: [-4.4245 -0.4668 -0.4887  0.0924  0.1059]\n",
      "MSE loss: 126.5205\n",
      "Iteration: 74000\n",
      "Gradient: [ -19.0395  -29.2014  153.1788  130.6276 -226.9828]\n",
      "Weights: [-4.4253 -0.4656 -0.4892  0.0923  0.106 ]\n",
      "MSE loss: 126.4555\n",
      "Iteration: 74100\n",
      "Gradient: [   30.1589   -60.0655    55.9908    22.5871 -1455.608 ]\n",
      "Weights: [-4.4277 -0.4641 -0.4898  0.0923  0.106 ]\n",
      "MSE loss: 126.3785\n",
      "Iteration: 74200\n",
      "Gradient: [  25.2284   14.5979  123.5031   -9.036  -528.026 ]\n",
      "Weights: [-4.4267 -0.463  -0.4906  0.0923  0.1061]\n",
      "MSE loss: 126.298\n",
      "Iteration: 74300\n",
      "Gradient: [-30.063  -86.2752 145.8638 227.8046 435.55  ]\n",
      "Weights: [-4.4275 -0.462  -0.4912  0.0922  0.1061]\n",
      "MSE loss: 126.2327\n",
      "Iteration: 74400\n",
      "Gradient: [ 30.9772 -28.583  163.214  250.4516  39.8323]\n",
      "Weights: [-4.4269 -0.4606 -0.4918  0.0922  0.1062]\n",
      "MSE loss: 126.1636\n",
      "Iteration: 74500\n",
      "Gradient: [  23.048    11.398   166.1658  276.2721 -210.251 ]\n",
      "Weights: [-4.4272 -0.4598 -0.4923  0.0921  0.1062]\n",
      "MSE loss: 126.1\n",
      "Iteration: 74600\n",
      "Gradient: [  -18.8459   -70.1607   178.7793    67.8357 -1069.0182]\n",
      "Weights: [-4.4284 -0.4591 -0.4929  0.0921  0.1063]\n",
      "MSE loss: 126.0331\n",
      "Iteration: 74700\n",
      "Gradient: [ 4.250700e+00 -1.148000e-01  1.047909e+02  2.448900e+00 -2.750089e+02]\n",
      "Weights: [-4.4279 -0.4577 -0.4935  0.0921  0.1063]\n",
      "MSE loss: 125.9672\n",
      "Iteration: 74800\n",
      "Gradient: [   5.5492    6.8685    6.7673 -116.4057  116.0838]\n",
      "Weights: [-4.4276 -0.4569 -0.4941  0.092   0.1064]\n",
      "MSE loss: 125.9091\n",
      "Iteration: 74900\n",
      "Gradient: [-2.907000e-01  2.140450e+01  8.580370e+01 -5.049983e+02 -6.529517e+02]\n",
      "Weights: [-4.4263 -0.456  -0.4947  0.092   0.1064]\n",
      "MSE loss: 125.8523\n",
      "Iteration: 75000\n",
      "Gradient: [  -3.5606  -73.2307  -45.9388 -137.5607 -639.4176]\n",
      "Weights: [-4.4293 -0.4551 -0.4952  0.092   0.1065]\n",
      "MSE loss: 125.784\n",
      "Iteration: 75100\n",
      "Gradient: [ -11.9165   19.7473   40.3317   45.0877 -877.6889]\n",
      "Weights: [-4.4302 -0.4536 -0.4958  0.092   0.1065]\n",
      "MSE loss: 125.716\n",
      "Iteration: 75200\n",
      "Gradient: [ 3.0240600e+01 -8.1915300e+01  5.7734600e+01 -2.3140000e-01\n",
      " -1.4047675e+03]\n",
      "Weights: [-4.4312 -0.4527 -0.4962  0.0919  0.1065]\n",
      "MSE loss: 125.6676\n",
      "Iteration: 75300\n",
      "Gradient: [   1.561   -35.2613  -94.6172  -89.3212 -842.4323]\n",
      "Weights: [-4.4311 -0.452  -0.4968  0.0919  0.1066]\n",
      "MSE loss: 125.6019\n",
      "Iteration: 75400\n",
      "Gradient: [ 25.723   37.7598 -45.1062  30.977  819.113 ]\n",
      "Weights: [-4.4299 -0.4509 -0.4974  0.0918  0.1066]\n",
      "MSE loss: 125.5364\n",
      "Iteration: 75500\n",
      "Gradient: [  33.3767   47.4428   64.4843  135.851  -282.1516]\n",
      "Weights: [-4.4301 -0.4498 -0.498   0.0918  0.1067]\n",
      "MSE loss: 125.4739\n",
      "Iteration: 75600\n",
      "Gradient: [  -21.395     17.7852    49.2101  -151.5665 -1063.1194]\n",
      "Weights: [-4.4317 -0.4483 -0.4986  0.0918  0.1067]\n",
      "MSE loss: 125.398\n",
      "Iteration: 75700\n",
      "Gradient: [ 23.8262 -14.7227 -64.2619 -27.443  541.0686]\n",
      "Weights: [-4.4321 -0.4474 -0.4993  0.0918  0.1068]\n",
      "MSE loss: 125.3374\n",
      "Iteration: 75800\n",
      "Gradient: [  36.6489    1.0964  140.4499  367.5295 -185.8552]\n",
      "Weights: [-4.4308 -0.4467 -0.4998  0.0917  0.1068]\n",
      "MSE loss: 125.2859\n",
      "Iteration: 75900\n",
      "Gradient: [   6.7756   18.3488   48.8893 -281.6922 -834.8949]\n",
      "Weights: [-4.4331 -0.4453 -0.5004  0.0917  0.1069]\n",
      "MSE loss: 125.2157\n",
      "Iteration: 76000\n",
      "Gradient: [-36.8565  65.576  -47.7657 235.602  351.517 ]\n",
      "Weights: [-4.4327 -0.4441 -0.5009  0.0916  0.1069]\n",
      "MSE loss: 125.1536\n",
      "Iteration: 76100\n",
      "Gradient: [ -25.6014   28.8018  173.3292  230.2485 -289.5577]\n",
      "Weights: [-4.4344 -0.4433 -0.5014  0.0916  0.1069]\n",
      "MSE loss: 125.0995\n",
      "Iteration: 76200\n",
      "Gradient: [ -11.273   -26.733   184.3171  198.578  -974.1562]\n",
      "Weights: [-4.4356 -0.4423 -0.5019  0.0916  0.107 ]\n",
      "MSE loss: 125.0417\n",
      "Iteration: 76300\n",
      "Gradient: [  37.8674  -31.2006   18.3057 -185.3035  737.9403]\n",
      "Weights: [-4.4331 -0.4418 -0.5024  0.0915  0.107 ]\n",
      "MSE loss: 124.9803\n",
      "Iteration: 76400\n",
      "Gradient: [ -58.3423  -20.2551  -32.2012  188.9357 -332.1096]\n",
      "Weights: [-4.4335 -0.4408 -0.5031  0.0915  0.1071]\n",
      "MSE loss: 124.9138\n",
      "Iteration: 76500\n",
      "Gradient: [   2.9109  -31.0937  -88.2397   79.3253 -507.8827]\n",
      "Weights: [-4.4334 -0.4397 -0.5037  0.0914  0.1071]\n",
      "MSE loss: 124.8516\n",
      "Iteration: 76600\n",
      "Gradient: [   2.3512  -27.5775  143.8404 -440.1431 -738.8009]\n",
      "Weights: [-4.4358 -0.4385 -0.5041  0.0914  0.1072]\n",
      "MSE loss: 124.7935\n",
      "Iteration: 76700\n",
      "Gradient: [  51.4685  -51.7704  -44.9504  700.8265 -688.1546]\n",
      "Weights: [-4.4333 -0.4383 -0.5046  0.0914  0.1072]\n",
      "MSE loss: 124.7468\n",
      "Iteration: 76800\n",
      "Gradient: [  40.1023  -20.505    -9.12    343.3885 -485.3095]\n",
      "Weights: [-4.434  -0.4369 -0.5051  0.0913  0.1072]\n",
      "MSE loss: 124.6881\n",
      "Iteration: 76900\n",
      "Gradient: [  35.4666  -45.4352   12.1689  -40.6893 -960.9094]\n",
      "Weights: [-4.4343 -0.4358 -0.5057  0.0913  0.1073]\n",
      "MSE loss: 124.6245\n",
      "Iteration: 77000\n",
      "Gradient: [   14.806    -59.603    129.3903   -43.1399 -1363.8522]\n",
      "Weights: [-4.4363 -0.4348 -0.5062  0.0913  0.1073]\n",
      "MSE loss: 124.559\n",
      "Iteration: 77100\n",
      "Gradient: [  31.4824  -18.8114  141.3307 -237.7539  748.7403]\n",
      "Weights: [-4.435  -0.4339 -0.5069  0.0913  0.1074]\n",
      "MSE loss: 124.494\n",
      "Iteration: 77200\n",
      "Gradient: [  -35.4853    -8.6333    26.9685   158.4496 -1444.0372]\n",
      "Weights: [-4.4364 -0.4328 -0.5075  0.0912  0.1074]\n",
      "MSE loss: 124.4226\n",
      "Iteration: 77300\n",
      "Gradient: [  -6.8612    8.6495  -81.2835 -116.9858   61.9152]\n",
      "Weights: [-4.4365 -0.4315 -0.5081  0.0912  0.1075]\n",
      "MSE loss: 124.3573\n",
      "Iteration: 77400\n",
      "Gradient: [  12.0219   45.6589  126.0465 -124.6284 -413.8379]\n",
      "Weights: [-4.4374 -0.4302 -0.5087  0.0911  0.1075]\n",
      "MSE loss: 124.2828\n",
      "Iteration: 77500\n",
      "Gradient: [-17.2971 -45.2912  -0.4592 216.3856 -85.3825]\n",
      "Weights: [-4.4394 -0.4291 -0.5092  0.0911  0.1076]\n",
      "MSE loss: 124.2307\n",
      "Iteration: 77600\n",
      "Gradient: [  2.7112 -48.2635  26.0704 218.097  332.2873]\n",
      "Weights: [-4.4378 -0.428  -0.5096  0.0911  0.1076]\n",
      "MSE loss: 124.1866\n",
      "Iteration: 77700\n",
      "Gradient: [  -5.3033  -29.1915 -209.4591   59.7    -597.2299]\n",
      "Weights: [-4.4391 -0.4269 -0.5101  0.091   0.1076]\n",
      "MSE loss: 124.1216\n",
      "Iteration: 77800\n",
      "Gradient: [  -3.7176  -34.2179  -48.6858 -167.4334 -455.0184]\n",
      "Weights: [-4.4409 -0.4259 -0.5107  0.0909  0.1077]\n",
      "MSE loss: 124.0672\n",
      "Iteration: 77900\n",
      "Gradient: [ 27.3902 -22.521  -17.3807  18.1502 425.9935]\n",
      "Weights: [-4.4396 -0.4249 -0.5111  0.0908  0.1077]\n",
      "MSE loss: 124.0048\n",
      "Iteration: 78000\n",
      "Gradient: [  -4.4876  -33.8438  169.3663 -388.3155  142.8518]\n",
      "Weights: [-4.4396 -0.4241 -0.5117  0.0908  0.1078]\n",
      "MSE loss: 123.9464\n",
      "Iteration: 78100\n",
      "Gradient: [  17.9594  -47.2906   37.74   -110.1845 -857.0851]\n",
      "Weights: [-4.441  -0.423  -0.5122  0.0908  0.1078]\n",
      "MSE loss: 123.8854\n",
      "Iteration: 78200\n",
      "Gradient: [   5.8542  -19.8598  179.0247  -82.3379 -847.1222]\n",
      "Weights: [-4.4395 -0.4226 -0.5127  0.0907  0.1079]\n",
      "MSE loss: 123.8379\n",
      "Iteration: 78300\n",
      "Gradient: [  16.0845   -4.7242   13.7873  137.1713 -328.9935]\n",
      "Weights: [-4.4394 -0.4221 -0.5133  0.0907  0.1079]\n",
      "MSE loss: 123.7768\n",
      "Iteration: 78400\n",
      "Gradient: [ -30.7001  -48.863    32.8281  162.5979 -446.7028]\n",
      "Weights: [-4.4386 -0.4211 -0.5137  0.0906  0.108 ]\n",
      "MSE loss: 123.726\n",
      "Iteration: 78500\n",
      "Gradient: [10.2274 13.1802 41.4601 41.189  70.3007]\n",
      "Weights: [-4.4397 -0.42   -0.5142  0.0906  0.108 ]\n",
      "MSE loss: 123.6657\n",
      "Iteration: 78600\n",
      "Gradient: [ -34.1355   -7.3812   23.3414  -95.3194 -210.356 ]\n",
      "Weights: [-4.4404 -0.4187 -0.5149  0.0905  0.108 ]\n",
      "MSE loss: 123.5991\n",
      "Iteration: 78700\n",
      "Gradient: [  28.6942  -53.8796   12.7618  -51.0554 -277.2091]\n",
      "Weights: [-4.4415 -0.4172 -0.5153  0.0905  0.1081]\n",
      "MSE loss: 123.5367\n",
      "Iteration: 78800\n",
      "Gradient: [ -16.4876  -44.1265  -39.1159  220.1707 -759.2558]\n",
      "Weights: [-4.4427 -0.416  -0.5158  0.0904  0.1081]\n",
      "MSE loss: 123.4815\n",
      "Iteration: 78900\n",
      "Gradient: [ -44.2157  -25.0931  -23.2091 -169.7103 -613.2808]\n",
      "Weights: [-4.4416 -0.4155 -0.5163  0.0904  0.1082]\n",
      "MSE loss: 123.4281\n",
      "Iteration: 79000\n",
      "Gradient: [ 46.598   27.4408 -56.609   21.6513 116.3367]\n",
      "Weights: [-4.4416 -0.415  -0.5167  0.0904  0.1082]\n",
      "MSE loss: 123.3841\n",
      "Iteration: 79100\n",
      "Gradient: [  -8.7125  -36.7115  121.8129  229.1158 -816.8834]\n",
      "Weights: [-4.4435 -0.413  -0.5172  0.0903  0.1082]\n",
      "MSE loss: 123.3224\n",
      "Iteration: 79200\n",
      "Gradient: [   4.0648  -83.4411  -43.9536 -179.0449  471.5465]\n",
      "Weights: [-4.4437 -0.4132 -0.5178  0.0903  0.1083]\n",
      "MSE loss: 123.2804\n",
      "Iteration: 79300\n",
      "Gradient: [  39.8502   23.1139   37.7736  229.013  -279.2659]\n",
      "Weights: [-4.4426 -0.4122 -0.5182  0.0903  0.1083]\n",
      "MSE loss: 123.2241\n",
      "Iteration: 79400\n",
      "Gradient: [   6.3991   22.8624  -41.2353 -270.9951  157.9288]\n",
      "Weights: [-4.4436 -0.4114 -0.5188  0.0902  0.1084]\n",
      "MSE loss: 123.1666\n",
      "Iteration: 79500\n",
      "Gradient: [ -24.4031   28.9487  214.2747 -146.3065 -217.324 ]\n",
      "Weights: [-4.4425 -0.4106 -0.5192  0.0902  0.1084]\n",
      "MSE loss: 123.1136\n",
      "Iteration: 79600\n",
      "Gradient: [ -42.9523   31.5287    4.0364  -87.8229 -345.507 ]\n",
      "Weights: [-4.4434 -0.4101 -0.5198  0.0902  0.1085]\n",
      "MSE loss: 123.0552\n",
      "Iteration: 79700\n",
      "Gradient: [ -0.895   16.8496   2.0486 125.5243 438.7824]\n",
      "Weights: [-4.4428 -0.4088 -0.5203  0.0901  0.1085]\n",
      "MSE loss: 122.9949\n",
      "Iteration: 79800\n",
      "Gradient: [  -41.1014   -32.3956   187.4097  -167.5752 -1506.6045]\n",
      "Weights: [-4.4439 -0.4079 -0.5209  0.0901  0.1085]\n",
      "MSE loss: 122.9354\n",
      "Iteration: 79900\n",
      "Gradient: [  33.1282   50.0624  179.4053  284.194  -843.9333]\n",
      "Weights: [-4.4444 -0.4068 -0.5214  0.0901  0.1086]\n",
      "MSE loss: 122.881\n",
      "Iteration: 80000\n",
      "Gradient: [ -3.9856  43.9885 120.4552  50.4081 254.5568]\n",
      "Weights: [-4.4443 -0.4059 -0.5219  0.09    0.1086]\n",
      "MSE loss: 122.8194\n",
      "Iteration: 80100\n",
      "Gradient: [  -6.4636   -1.2219    2.601  -224.187  -153.7237]\n",
      "Weights: [-4.445  -0.4051 -0.5224  0.09    0.1087]\n",
      "MSE loss: 122.7677\n",
      "Iteration: 80200\n",
      "Gradient: [  28.5819  -52.455    78.3338 -142.6456 -782.5996]\n",
      "Weights: [-4.4462 -0.4039 -0.5228  0.09    0.1087]\n",
      "MSE loss: 122.7152\n",
      "Iteration: 80300\n",
      "Gradient: [  -6.8783   -6.3464   62.2315  -45.0609 -567.9759]\n",
      "Weights: [-4.4468 -0.4028 -0.5234  0.0899  0.1088]\n",
      "MSE loss: 122.6543\n",
      "Iteration: 80400\n",
      "Gradient: [ 30.2665 -33.894  101.1902  -0.1957 -25.4017]\n",
      "Weights: [-4.4454 -0.4018 -0.5241  0.0899  0.1088]\n",
      "MSE loss: 122.5911\n",
      "Iteration: 80500\n",
      "Gradient: [  25.9226   -1.778   -84.1013  582.3163 -678.3608]\n",
      "Weights: [-4.4471 -0.4004 -0.5245  0.0898  0.1088]\n",
      "MSE loss: 122.5321\n",
      "Iteration: 80600\n",
      "Gradient: [   11.0968     4.8618   117.1036   238.9787 -1093.4539]\n",
      "Weights: [-4.4471 -0.3988 -0.525   0.0897  0.1089]\n",
      "MSE loss: 122.4781\n",
      "Iteration: 80700\n",
      "Gradient: [   9.5683    5.5385   27.8416 -109.054  -603.2769]\n",
      "Weights: [-4.4489 -0.3978 -0.5257  0.0897  0.1089]\n",
      "MSE loss: 122.4077\n",
      "Iteration: 80800\n",
      "Gradient: [-20.8759 -16.6741 153.4142  62.296   22.9161]\n",
      "Weights: [-4.448  -0.3975 -0.5262  0.0897  0.109 ]\n",
      "MSE loss: 122.358\n",
      "Iteration: 80900\n",
      "Gradient: [  -11.8641   -20.2615    40.974     82.5716 -1305.2695]\n",
      "Weights: [-4.4488 -0.3963 -0.5267  0.0897  0.109 ]\n",
      "MSE loss: 122.3033\n",
      "Iteration: 81000\n",
      "Gradient: [  -3.217     2.4057  104.5296 -545.4935 -939.5028]\n",
      "Weights: [-4.4498 -0.3954 -0.5273  0.0897  0.109 ]\n",
      "MSE loss: 122.2476\n",
      "Iteration: 81100\n",
      "Gradient: [  -21.0279   -11.2332    55.1128   441.0465 -1372.383 ]\n",
      "Weights: [-4.4487 -0.394  -0.5278  0.0896  0.1091]\n",
      "MSE loss: 122.1872\n",
      "Iteration: 81200\n",
      "Gradient: [ -33.4658  -87.6514   -6.1513 -529.2976 -313.6531]\n",
      "Weights: [-4.4487 -0.3933 -0.5283  0.0895  0.1091]\n",
      "MSE loss: 122.1342\n",
      "Iteration: 81300\n",
      "Gradient: [  -2.3559  -14.4904  175.5617 -274.7106   80.2901]\n",
      "Weights: [-4.4508 -0.3921 -0.5288  0.0895  0.1092]\n",
      "MSE loss: 122.0788\n",
      "Iteration: 81400\n",
      "Gradient: [  19.9749   67.5732  133.426  -176.9328 -638.5745]\n",
      "Weights: [-4.4507 -0.3908 -0.5294  0.0894  0.1092]\n",
      "MSE loss: 122.0126\n",
      "Iteration: 81500\n",
      "Gradient: [ -5.2621 -16.154  -37.9089 476.3883 -95.4444]\n",
      "Weights: [-4.4511 -0.3898 -0.5301  0.0894  0.1093]\n",
      "MSE loss: 121.9523\n",
      "Iteration: 81600\n",
      "Gradient: [  1.0379  19.2821 114.5346 129.6193 551.832 ]\n",
      "Weights: [-4.4517 -0.3883 -0.5305  0.0894  0.1093]\n",
      "MSE loss: 121.8919\n",
      "Iteration: 81700\n",
      "Gradient: [-35.9641  22.4726  87.5001 225.3108 155.6781]\n",
      "Weights: [-4.453  -0.3869 -0.5312  0.0894  0.1093]\n",
      "MSE loss: 121.8321\n",
      "Iteration: 81800\n",
      "Gradient: [ -40.4568  -22.6609  182.4745  590.3587 1101.9919]\n",
      "Weights: [-4.4532 -0.3859 -0.5318  0.0894  0.1094]\n",
      "MSE loss: 121.7708\n",
      "Iteration: 81900\n",
      "Gradient: [-4.239140e+01  7.387000e-01  1.124381e+02  1.478386e+02 -8.859833e+02]\n",
      "Weights: [-4.4532 -0.3853 -0.5323  0.0893  0.1094]\n",
      "MSE loss: 121.718\n",
      "Iteration: 82000\n",
      "Gradient: [  19.442    29.9532   88.7926  288.7801 -380.7241]\n",
      "Weights: [-4.4526 -0.3843 -0.5328  0.0893  0.1095]\n",
      "MSE loss: 121.6651\n",
      "Iteration: 82100\n",
      "Gradient: [ -17.4923  -38.8128  193.7734 -208.1693 -575.2322]\n",
      "Weights: [-4.452  -0.3837 -0.5334  0.0893  0.1095]\n",
      "MSE loss: 121.6051\n",
      "Iteration: 82200\n",
      "Gradient: [  12.5249   58.797   151.3449   -8.4682 -840.9993]\n",
      "Weights: [-4.4523 -0.3833 -0.534   0.0893  0.1095]\n",
      "MSE loss: 121.5532\n",
      "Iteration: 82300\n",
      "Gradient: [  -41.1343     5.7447   176.6339   -83.2313 -1076.3613]\n",
      "Weights: [-4.4526 -0.3824 -0.5346  0.0893  0.1096]\n",
      "MSE loss: 121.4991\n",
      "Iteration: 82400\n",
      "Gradient: [  5.2961  41.7249 -85.6397  39.5753 131.6364]\n",
      "Weights: [-4.453  -0.3813 -0.5352  0.0892  0.1096]\n",
      "MSE loss: 121.4407\n",
      "Iteration: 82500\n",
      "Gradient: [  -18.6448    -2.4893   126.605   -168.6254 -1087.0227]\n",
      "Weights: [-4.4519 -0.3806 -0.5357  0.0892  0.1097]\n",
      "MSE loss: 121.3948\n",
      "Iteration: 82600\n",
      "Gradient: [ -27.4215  -43.2343  170.6375  -88.9949 -253.2595]\n",
      "Weights: [-4.4528 -0.379  -0.5362  0.0892  0.1097]\n",
      "MSE loss: 121.332\n",
      "Iteration: 82700\n",
      "Gradient: [   8.3915   46.1882  103.7296  182.3836 -339.8398]\n",
      "Weights: [-4.4541 -0.3779 -0.5367  0.0892  0.1097]\n",
      "MSE loss: 121.2764\n",
      "Iteration: 82800\n",
      "Gradient: [   20.6206   -42.724    -33.3718    21.056  -1471.8827]\n",
      "Weights: [-4.4536 -0.3771 -0.5374  0.0891  0.1098]\n",
      "MSE loss: 121.2205\n",
      "Iteration: 82900\n",
      "Gradient: [   -4.498      9.7397  -103.8833    11.2713 -1493.3147]\n",
      "Weights: [-4.4545 -0.376  -0.5377  0.0891  0.1098]\n",
      "MSE loss: 121.1705\n",
      "Iteration: 83000\n",
      "Gradient: [   29.0038    -7.171   -124.2053   -39.859  -1380.2405]\n",
      "Weights: [-4.4554 -0.3746 -0.5383  0.0891  0.1099]\n",
      "MSE loss: 121.111\n",
      "Iteration: 83100\n",
      "Gradient: [ -17.0197  -12.1218   30.6128 -167.1453 -618.7746]\n",
      "Weights: [-4.4546 -0.3738 -0.5389  0.089   0.1099]\n",
      "MSE loss: 121.0559\n",
      "Iteration: 83200\n",
      "Gradient: [  -21.6848    -2.2252    35.9268   231.1041 -1617.085 ]\n",
      "Weights: [-4.4552 -0.3729 -0.5394  0.089   0.1099]\n",
      "MSE loss: 121.004\n",
      "Iteration: 83300\n",
      "Gradient: [    8.5614    24.688    211.2986   183.2452 -1052.7226]\n",
      "Weights: [-4.4564 -0.3721 -0.5399  0.089   0.11  ]\n",
      "MSE loss: 120.9534\n",
      "Iteration: 83400\n",
      "Gradient: [ 51.4038 -12.4844  74.8692 204.7452 406.2771]\n",
      "Weights: [-4.4548 -0.372  -0.5403  0.0889  0.11  ]\n",
      "MSE loss: 120.9114\n",
      "Iteration: 83500\n",
      "Gradient: [  -9.2123  -37.3579  -46.8942 -126.1654  423.2325]\n",
      "Weights: [-4.4554 -0.371  -0.5409  0.0889  0.1101]\n",
      "MSE loss: 120.8544\n",
      "Iteration: 83600\n",
      "Gradient: [ -2.5752  72.8136   3.1841 102.6559 748.8794]\n",
      "Weights: [-4.455  -0.3697 -0.5414  0.0889  0.1101]\n",
      "MSE loss: 120.8002\n",
      "Iteration: 83700\n",
      "Gradient: [   9.894    14.353    71.4481 -146.7541 -889.458 ]\n",
      "Weights: [-4.4565 -0.3688 -0.542   0.0888  0.1101]\n",
      "MSE loss: 120.7463\n",
      "Iteration: 83800\n",
      "Gradient: [   -9.5502   -10.704    -21.2198  -136.5494 -1182.9966]\n",
      "Weights: [-4.4553 -0.368  -0.5426  0.0888  0.1102]\n",
      "MSE loss: 120.6881\n",
      "Iteration: 83900\n",
      "Gradient: [ -13.8409    4.7993 -105.7386   80.123  -297.2783]\n",
      "Weights: [-4.4557 -0.3676 -0.543   0.0888  0.1102]\n",
      "MSE loss: 120.644\n",
      "Iteration: 84000\n",
      "Gradient: [   4.933   -22.7946 -124.4823 -221.0567 -310.3521]\n",
      "Weights: [-4.4579 -0.3665 -0.5435  0.0888  0.1103]\n",
      "MSE loss: 120.5892\n",
      "Iteration: 84100\n",
      "Gradient: [  23.2708  -20.6281   27.281   390.6627 -566.2248]\n",
      "Weights: [-4.4574 -0.3652 -0.5439  0.0888  0.1103]\n",
      "MSE loss: 120.5398\n",
      "Iteration: 84200\n",
      "Gradient: [  40.4888   37.6954  -17.4432 -129.6728 -550.1547]\n",
      "Weights: [-4.459  -0.3646 -0.5445  0.0887  0.1103]\n",
      "MSE loss: 120.4867\n",
      "Iteration: 84300\n",
      "Gradient: [   5.0009  -28.0608   52.4609 -159.855  -871.8033]\n",
      "Weights: [-4.4578 -0.3637 -0.545   0.0887  0.1104]\n",
      "MSE loss: 120.433\n",
      "Iteration: 84400\n",
      "Gradient: [ -9.1017 -32.4914 141.7322 391.0998 154.6721]\n",
      "Weights: [-4.4584 -0.3627 -0.5455  0.0886  0.1104]\n",
      "MSE loss: 120.3775\n",
      "Iteration: 84500\n",
      "Gradient: [ -10.5888    6.2462  121.2233 -109.4506  637.7375]\n",
      "Weights: [-4.4595 -0.3614 -0.5461  0.0886  0.1105]\n",
      "MSE loss: 120.3222\n",
      "Iteration: 84600\n",
      "Gradient: [-1.819000e-01  6.351730e+01  1.711214e+02 -1.761146e+02  6.195895e+02]\n",
      "Weights: [-4.4597 -0.361  -0.5466  0.0886  0.1105]\n",
      "MSE loss: 120.2752\n",
      "Iteration: 84700\n",
      "Gradient: [  18.1523  -10.4175  151.2193  -64.3258 -255.3447]\n",
      "Weights: [-4.4588 -0.3601 -0.5471  0.0886  0.1105]\n",
      "MSE loss: 120.2256\n",
      "Iteration: 84800\n",
      "Gradient: [  38.1902    7.5703  -91.9313 -121.7142 -685.0663]\n",
      "Weights: [-4.4586 -0.3593 -0.5477  0.0885  0.1106]\n",
      "MSE loss: 120.176\n",
      "Iteration: 84900\n",
      "Gradient: [ 12.0175 -31.5587  29.3294  -7.9813 487.2288]\n",
      "Weights: [-4.4588 -0.3582 -0.5482  0.0885  0.1106]\n",
      "MSE loss: 120.1147\n",
      "Iteration: 85000\n",
      "Gradient: [ -28.8435   27.6494  -31.4581   63.2313 -202.8991]\n",
      "Weights: [-4.4602 -0.3567 -0.5487  0.0885  0.1107]\n",
      "MSE loss: 120.0591\n",
      "Iteration: 85100\n",
      "Gradient: [ -27.7509   -3.7239   -7.3446   27.4426 -119.1524]\n",
      "Weights: [-4.4594 -0.3561 -0.5492  0.0885  0.1107]\n",
      "MSE loss: 120.0149\n",
      "Iteration: 85200\n",
      "Gradient: [  18.9942  -14.086   111.6501  159.6595 -323.4841]\n",
      "Weights: [-4.4608 -0.3552 -0.5497  0.0884  0.1107]\n",
      "MSE loss: 119.9608\n",
      "Iteration: 85300\n",
      "Gradient: [ -26.9636   13.6423   60.1428   76.4415 -842.1115]\n",
      "Weights: [-4.4611 -0.3538 -0.5503  0.0884  0.1108]\n",
      "MSE loss: 119.8998\n",
      "Iteration: 85400\n",
      "Gradient: [  3.2057  29.2632  57.0845  16.8088 -15.714 ]\n",
      "Weights: [-4.4634 -0.353  -0.5508  0.0884  0.1108]\n",
      "MSE loss: 119.8535\n",
      "Iteration: 85500\n",
      "Gradient: [ 1.3437000e+00  7.8286800e+01 -8.1852900e+01  3.3260490e+02\n",
      " -1.4024049e+03]\n",
      "Weights: [-4.4613 -0.3521 -0.5514  0.0884  0.1109]\n",
      "MSE loss: 119.7949\n",
      "Iteration: 85600\n",
      "Gradient: [ -44.244    66.5268  203.4468  138.2532 -479.9037]\n",
      "Weights: [-4.4627 -0.3511 -0.552   0.0884  0.1109]\n",
      "MSE loss: 119.7404\n",
      "Iteration: 85700\n",
      "Gradient: [ 11.5614 -44.746  -47.3805  98.7463 884.5601]\n",
      "Weights: [-4.4605 -0.351  -0.5524  0.0883  0.1109]\n",
      "MSE loss: 119.7043\n",
      "Iteration: 85800\n",
      "Gradient: [  43.5961    4.3968    3.4378 -170.5062 -816.2337]\n",
      "Weights: [-4.4614 -0.3501 -0.5529  0.0883  0.111 ]\n",
      "MSE loss: 119.6487\n",
      "Iteration: 85900\n",
      "Gradient: [  12.8253  -42.9792  101.1237  -94.4545 -458.2552]\n",
      "Weights: [-4.4618 -0.3499 -0.5534  0.0883  0.111 ]\n",
      "MSE loss: 119.6068\n",
      "Iteration: 86000\n",
      "Gradient: [-86.6774 -78.3304  56.5787  15.0245 271.9379]\n",
      "Weights: [-4.4617 -0.349  -0.5538  0.0882  0.1111]\n",
      "MSE loss: 119.5579\n",
      "Iteration: 86100\n",
      "Gradient: [  0.9546 -23.3579  23.0344 477.1384 130.4797]\n",
      "Weights: [-4.4627 -0.3481 -0.5542  0.0882  0.1111]\n",
      "MSE loss: 119.5163\n",
      "Iteration: 86200\n",
      "Gradient: [  -8.2164  -94.687   116.3195  197.0185 -386.017 ]\n",
      "Weights: [-4.4624 -0.3472 -0.5549  0.0882  0.1112]\n",
      "MSE loss: 119.4517\n",
      "Iteration: 86300\n",
      "Gradient: [  39.9818  -70.863    71.7096 -102.7385 1250.1043]\n",
      "Weights: [-4.4624 -0.3465 -0.5554  0.0882  0.1112]\n",
      "MSE loss: 119.4014\n",
      "Iteration: 86400\n",
      "Gradient: [   6.3      33.8827   75.4102 -339.772   373.2365]\n",
      "Weights: [-4.4621 -0.3457 -0.5558  0.0881  0.1112]\n",
      "MSE loss: 119.3586\n",
      "Iteration: 86500\n",
      "Gradient: [-7.0850000e-01 -8.1197000e+00  1.3074430e+02 -1.7896810e+02\n",
      "  1.1026865e+03]\n",
      "Weights: [-4.4631 -0.3447 -0.5562  0.0881  0.1113]\n",
      "MSE loss: 119.3106\n",
      "Iteration: 86600\n",
      "Gradient: [   12.7846   -10.2058    20.8107   115.0117 -1274.6327]\n",
      "Weights: [-4.463  -0.3438 -0.5567  0.0881  0.1113]\n",
      "MSE loss: 119.258\n",
      "Iteration: 86700\n",
      "Gradient: [ -16.4287   -4.9449  116.7887  239.1811 -300.3088]\n",
      "Weights: [-4.4632 -0.343  -0.5573  0.088   0.1114]\n",
      "MSE loss: 119.2025\n",
      "Iteration: 86800\n",
      "Gradient: [  3.6888 -10.7084  24.1849 325.1748  -8.753 ]\n",
      "Weights: [-4.4617 -0.3424 -0.5578  0.088   0.1114]\n",
      "MSE loss: 119.1583\n",
      "Iteration: 86900\n",
      "Gradient: [  -7.81    -44.4062   10.3412   49.8894 -401.0794]\n",
      "Weights: [-4.462  -0.3417 -0.5583  0.0879  0.1115]\n",
      "MSE loss: 119.1066\n",
      "Iteration: 87000\n",
      "Gradient: [ 33.6386 -13.0967 194.8324 349.7247 105.7808]\n",
      "Weights: [-4.4639 -0.3407 -0.5586  0.0879  0.1115]\n",
      "MSE loss: 119.058\n",
      "Iteration: 87100\n",
      "Gradient: [ -10.8798   47.9865  292.9301 -152.386   151.1453]\n",
      "Weights: [-4.4627 -0.3404 -0.5591  0.0879  0.1115]\n",
      "MSE loss: 119.0167\n",
      "Iteration: 87200\n",
      "Gradient: [  -5.4634   14.5689   77.0379  124.5832 -421.0814]\n",
      "Weights: [-4.4639 -0.3398 -0.5595  0.0879  0.1116]\n",
      "MSE loss: 118.9755\n",
      "Iteration: 87300\n",
      "Gradient: [  -47.3698   -55.1018   -32.0569    99.8094 -1182.3802]\n",
      "Weights: [-4.4659 -0.3385 -0.5601  0.0878  0.1116]\n",
      "MSE loss: 118.9175\n",
      "Iteration: 87400\n",
      "Gradient: [   9.6758  -27.3748   81.493     4.3952 -140.918 ]\n",
      "Weights: [-4.4648 -0.3376 -0.5606  0.0878  0.1117]\n",
      "MSE loss: 118.8645\n",
      "Iteration: 87500\n",
      "Gradient: [ -14.8603   31.8292  203.2428  471.6526 -728.9881]\n",
      "Weights: [-4.4652 -0.3365 -0.5612  0.0877  0.1117]\n",
      "MSE loss: 118.8081\n",
      "Iteration: 87600\n",
      "Gradient: [ -16.9121  -19.6545   16.2598  171.4184 -312.1903]\n",
      "Weights: [-4.4655 -0.3357 -0.5617  0.0877  0.1118]\n",
      "MSE loss: 118.7558\n",
      "Iteration: 87700\n",
      "Gradient: [ -31.9353  -69.4393  -20.3443   47.5148 -987.3332]\n",
      "Weights: [-4.4666 -0.3346 -0.5622  0.0877  0.1118]\n",
      "MSE loss: 118.7054\n",
      "Iteration: 87800\n",
      "Gradient: [  15.7257   29.4622  104.4684  115.5028 -279.9525]\n",
      "Weights: [-4.4671 -0.3335 -0.5626  0.0876  0.1118]\n",
      "MSE loss: 118.6564\n",
      "Iteration: 87900\n",
      "Gradient: [ 37.8107  23.2334 112.0005 220.4135   3.856 ]\n",
      "Weights: [-4.4658 -0.3325 -0.5629  0.0875  0.1119]\n",
      "MSE loss: 118.6187\n",
      "Iteration: 88000\n",
      "Gradient: [   -6.6752   -60.5713   -10.7525   -32.4973 -1010.0622]\n",
      "Weights: [-4.4684 -0.3319 -0.5634  0.0875  0.1119]\n",
      "MSE loss: 118.5696\n",
      "Iteration: 88100\n",
      "Gradient: [ -22.7659  -11.7443 -129.3877   58.0267 -516.5103]\n",
      "Weights: [-4.4679 -0.3311 -0.5638  0.0874  0.1119]\n",
      "MSE loss: 118.5244\n",
      "Iteration: 88200\n",
      "Gradient: [   26.114      3.5317   166.5319    80.2897 -1489.4888]\n",
      "Weights: [-4.4682 -0.33   -0.5643  0.0874  0.112 ]\n",
      "MSE loss: 118.4754\n",
      "Iteration: 88300\n",
      "Gradient: [ -13.939    13.3427  141.4667  481.7985 -623.8635]\n",
      "Weights: [-4.4671 -0.3296 -0.5648  0.0874  0.112 ]\n",
      "MSE loss: 118.4245\n",
      "Iteration: 88400\n",
      "Gradient: [  31.1867  -81.4289 -107.4093  275.3374 -783.4983]\n",
      "Weights: [-4.4673 -0.3289 -0.5654  0.0873  0.1121]\n",
      "MSE loss: 118.378\n",
      "Iteration: 88500\n",
      "Gradient: [ -3.8801  18.4956  66.5739  41.0287 -93.0288]\n",
      "Weights: [-4.4686 -0.3274 -0.5658  0.0873  0.1121]\n",
      "MSE loss: 118.3248\n",
      "Iteration: 88600\n",
      "Gradient: [  23.4647   17.2022  196.1681 -173.5323  240.9127]\n",
      "Weights: [-4.4695 -0.326  -0.5663  0.0873  0.1121]\n",
      "MSE loss: 118.272\n",
      "Iteration: 88700\n",
      "Gradient: [   4.2138  -11.6226   14.3538 -362.4142 -361.5583]\n",
      "Weights: [-4.4687 -0.325  -0.5669  0.0872  0.1122]\n",
      "MSE loss: 118.2162\n",
      "Iteration: 88800\n",
      "Gradient: [-39.3186 -42.6093  73.5451 198.4967 -65.0766]\n",
      "Weights: [-4.469  -0.3242 -0.5674  0.0872  0.1122]\n",
      "MSE loss: 118.1733\n",
      "Iteration: 88900\n",
      "Gradient: [  34.4402  -26.1149   51.5642  -33.5116 -773.9765]\n",
      "Weights: [-4.4693 -0.3228 -0.5679  0.0871  0.1122]\n",
      "MSE loss: 118.1219\n",
      "Iteration: 89000\n",
      "Gradient: [   1.5262  -49.1607  -78.5449   74.7822 -220.0728]\n",
      "Weights: [-4.4699 -0.3218 -0.5685  0.0871  0.1123]\n",
      "MSE loss: 118.0689\n",
      "Iteration: 89100\n",
      "Gradient: [-35.4183  74.1406  24.5805 -18.5092  -6.5796]\n",
      "Weights: [-4.4703 -0.3207 -0.5689  0.0871  0.1123]\n",
      "MSE loss: 118.0251\n",
      "Iteration: 89200\n",
      "Gradient: [ 9.323700e+00  1.721250e+01 -2.298000e-01 -3.684786e+02 -7.194117e+02]\n",
      "Weights: [-4.473  -0.3195 -0.5693  0.0871  0.1123]\n",
      "MSE loss: 117.9782\n",
      "Iteration: 89300\n",
      "Gradient: [ 1.827  45.0607 11.0809 70.9188 13.3809]\n",
      "Weights: [-4.4734 -0.3186 -0.5698  0.0871  0.1124]\n",
      "MSE loss: 117.9363\n",
      "Iteration: 89400\n",
      "Gradient: [  -6.0316  -43.7081   60.9084 -162.1164 -551.7884]\n",
      "Weights: [-4.4745 -0.3175 -0.5702  0.087   0.1124]\n",
      "MSE loss: 117.8915\n",
      "Iteration: 89500\n",
      "Gradient: [ 40.757   46.2407  90.4479 120.0514 851.9116]\n",
      "Weights: [-4.4729 -0.317  -0.5708  0.087   0.1125]\n",
      "MSE loss: 117.8404\n",
      "Iteration: 89600\n",
      "Gradient: [ 23.0377 -10.3921  -3.3824 238.3044 297.833 ]\n",
      "Weights: [-4.4715 -0.3164 -0.5713  0.0869  0.1125]\n",
      "MSE loss: 117.7969\n",
      "Iteration: 89700\n",
      "Gradient: [  -6.1659  -23.6254   59.2531 -162.0857  209.2784]\n",
      "Weights: [-4.4729 -0.3154 -0.5717  0.0869  0.1125]\n",
      "MSE loss: 117.7499\n",
      "Iteration: 89800\n",
      "Gradient: [ -20.6689   52.9446   58.691     8.4416 -757.3383]\n",
      "Weights: [-4.473  -0.3147 -0.5722  0.0869  0.1126]\n",
      "MSE loss: 117.7051\n",
      "Iteration: 89900\n",
      "Gradient: [   3.6307  -14.4879  142.6    -239.9278 -874.8451]\n",
      "Weights: [-4.4719 -0.3137 -0.5726  0.0868  0.1126]\n",
      "MSE loss: 117.6598\n",
      "Iteration: 90000\n",
      "Gradient: [  23.878    -4.2904  -36.9125 -229.0724  -70.1766]\n",
      "Weights: [-4.4738 -0.3129 -0.5731  0.0868  0.1126]\n",
      "MSE loss: 117.6103\n",
      "Iteration: 90100\n",
      "Gradient: [  -1.3818    0.3128   52.7163 -110.5683  146.4686]\n",
      "Weights: [-4.4737 -0.3124 -0.5735  0.0868  0.1127]\n",
      "MSE loss: 117.5706\n",
      "Iteration: 90200\n",
      "Gradient: [  -15.2769   -30.6602    82.1879    54.5766 -1068.1249]\n",
      "Weights: [-4.474  -0.3114 -0.574   0.0868  0.1127]\n",
      "MSE loss: 117.5213\n",
      "Iteration: 90300\n",
      "Gradient: [ -12.4091  -38.4426   62.6367 -127.3746  372.7245]\n",
      "Weights: [-4.4752 -0.3103 -0.5745  0.0867  0.1127]\n",
      "MSE loss: 117.4785\n",
      "Iteration: 90400\n",
      "Gradient: [ -9.28   -45.5383 160.5713 -29.0568 333.9062]\n",
      "Weights: [-4.4757 -0.3088 -0.5749  0.0867  0.1128]\n",
      "MSE loss: 117.4334\n",
      "Iteration: 90500\n",
      "Gradient: [  38.4771   12.268   250.3142 -145.5929 -408.191 ]\n",
      "Weights: [-4.4749 -0.3081 -0.5755  0.0867  0.1128]\n",
      "MSE loss: 117.383\n",
      "Iteration: 90600\n",
      "Gradient: [ 24.8925 -24.7259  84.7394 155.1752 733.6282]\n",
      "Weights: [-4.4748 -0.3073 -0.5759  0.0867  0.1128]\n",
      "MSE loss: 117.3481\n",
      "Iteration: 90700\n",
      "Gradient: [  12.7723   -1.7239   -2.9093  109.9559 -364.0626]\n",
      "Weights: [-4.4787 -0.3063 -0.5762  0.0866  0.1129]\n",
      "MSE loss: 117.3134\n",
      "Iteration: 90800\n",
      "Gradient: [  49.2996   12.6042   30.3526  482.3812 -157.7456]\n",
      "Weights: [-4.4775 -0.3052 -0.5767  0.0865  0.1129]\n",
      "MSE loss: 117.2588\n",
      "Iteration: 90900\n",
      "Gradient: [  0.4295 -43.2092 125.3995 -77.3371 328.9499]\n",
      "Weights: [-4.4768 -0.305  -0.5772  0.0865  0.1129]\n",
      "MSE loss: 117.2147\n",
      "Iteration: 91000\n",
      "Gradient: [ -13.6091  -29.2466  -90.0284  422.5071 -402.5773]\n",
      "Weights: [-4.4776 -0.3038 -0.5776  0.0865  0.113 ]\n",
      "MSE loss: 117.1748\n",
      "Iteration: 91100\n",
      "Gradient: [ -18.1997   83.6916   34.231    74.0566 -387.7184]\n",
      "Weights: [-4.4783 -0.3022 -0.5782  0.0865  0.113 ]\n",
      "MSE loss: 117.1164\n",
      "Iteration: 91200\n",
      "Gradient: [ -42.5889    4.7575   80.2004 -275.3048 1291.4038]\n",
      "Weights: [-4.4786 -0.3013 -0.5787  0.0864  0.113 ]\n",
      "MSE loss: 117.0711\n",
      "Iteration: 91300\n",
      "Gradient: [  -25.6338    34.3086   141.5918   199.9317 -1446.2186]\n",
      "Weights: [-4.478  -0.3003 -0.5792  0.0864  0.1131]\n",
      "MSE loss: 117.0245\n",
      "Iteration: 91400\n",
      "Gradient: [-26.5943 -37.6774 -35.9123 -83.3722  72.3739]\n",
      "Weights: [-4.4779 -0.2995 -0.5797  0.0863  0.1131]\n",
      "MSE loss: 116.9734\n",
      "Iteration: 91500\n",
      "Gradient: [  30.9716    4.4208  -14.8299  439.9615 -335.6353]\n",
      "Weights: [-4.4783 -0.2986 -0.5803  0.0863  0.1131]\n",
      "MSE loss: 116.9284\n",
      "Iteration: 91600\n",
      "Gradient: [  -2.7287   -7.2534  135.4744 -146.3946   54.4913]\n",
      "Weights: [-4.477  -0.298  -0.5808  0.0863  0.1132]\n",
      "MSE loss: 116.8854\n",
      "Iteration: 91700\n",
      "Gradient: [ -9.5649 -38.9739  67.9209 -57.4295 -45.6183]\n",
      "Weights: [-4.4795 -0.2972 -0.5812  0.0863  0.1132]\n",
      "MSE loss: 116.8407\n",
      "Iteration: 91800\n",
      "Gradient: [ -1.2262  12.5223  81.0536 -77.0532 808.0219]\n",
      "Weights: [-4.4809 -0.2961 -0.5817  0.0863  0.1133]\n",
      "MSE loss: 116.7928\n",
      "Iteration: 91900\n",
      "Gradient: [ 23.942  -44.6943  36.2002 104.1852 442.2509]\n",
      "Weights: [-4.4825 -0.2948 -0.582   0.0862  0.1133]\n",
      "MSE loss: 116.755\n",
      "Iteration: 92000\n",
      "Gradient: [  64.0036   -9.3593   36.0026  214.7348 -121.9162]\n",
      "Weights: [-4.4803 -0.2936 -0.5825  0.0862  0.1133]\n",
      "MSE loss: 116.7065\n",
      "Iteration: 92100\n",
      "Gradient: [  23.9418  -11.0516  133.6023    5.4599 -435.9014]\n",
      "Weights: [-4.48   -0.2929 -0.5829  0.0861  0.1134]\n",
      "MSE loss: 116.671\n",
      "Iteration: 92200\n",
      "Gradient: [  -2.9633  -12.8845  112.2136 -633.3682 -664.0875]\n",
      "Weights: [-4.4807 -0.2922 -0.5834  0.0861  0.1134]\n",
      "MSE loss: 116.6204\n",
      "Iteration: 92300\n",
      "Gradient: [-16.7988  26.7813  -1.3319  -6.0642  -4.4273]\n",
      "Weights: [-4.4811 -0.2913 -0.5838  0.086   0.1134]\n",
      "MSE loss: 116.5736\n",
      "Iteration: 92400\n",
      "Gradient: [  12.679   -46.6987    5.0803 -443.5094  551.5544]\n",
      "Weights: [-4.48   -0.2908 -0.5844  0.086   0.1135]\n",
      "MSE loss: 116.5311\n",
      "Iteration: 92500\n",
      "Gradient: [  -43.7075   -13.399    -17.7517    64.3002 -1601.2414]\n",
      "Weights: [-4.4818 -0.29   -0.5848  0.0859  0.1135]\n",
      "MSE loss: 116.4884\n",
      "Iteration: 92600\n",
      "Gradient: [  -18.8537   -39.6881    45.91      91.7086 -1759.7842]\n",
      "Weights: [-4.4824 -0.2886 -0.5852  0.0859  0.1136]\n",
      "MSE loss: 116.4362\n",
      "Iteration: 92700\n",
      "Gradient: [ -10.3617    2.6984  -47.0287 -227.1253  461.5087]\n",
      "Weights: [-4.4821 -0.2879 -0.5858  0.0858  0.1136]\n",
      "MSE loss: 116.3874\n",
      "Iteration: 92800\n",
      "Gradient: [  -8.2852  -48.4643   22.4123  -48.2564 -269.9284]\n",
      "Weights: [-4.4832 -0.2867 -0.5863  0.0858  0.1136]\n",
      "MSE loss: 116.3326\n",
      "Iteration: 92900\n",
      "Gradient: [  10.684    54.1919  -70.5517  392.489  -104.4553]\n",
      "Weights: [-4.4826 -0.2862 -0.5868  0.0858  0.1137]\n",
      "MSE loss: 116.2964\n",
      "Iteration: 93000\n",
      "Gradient: [  -9.1406  -16.2388   84.6225   11.4378 -759.102 ]\n",
      "Weights: [-4.4847 -0.2852 -0.5872  0.0858  0.1137]\n",
      "MSE loss: 116.249\n",
      "Iteration: 93100\n",
      "Gradient: [-38.0697   6.0415  89.3916 151.2053 322.9878]\n",
      "Weights: [-4.4827 -0.2841 -0.5878  0.0858  0.1138]\n",
      "MSE loss: 116.199\n",
      "Iteration: 93200\n",
      "Gradient: [-24.799  -29.0138 152.3928 191.9504 110.3734]\n",
      "Weights: [-4.4838 -0.2834 -0.5882  0.0857  0.1138]\n",
      "MSE loss: 116.1611\n",
      "Iteration: 93300\n",
      "Gradient: [   16.1558    10.7707    49.37     -27.9523 -1207.6087]\n",
      "Weights: [-4.482  -0.2831 -0.5888  0.0857  0.1138]\n",
      "MSE loss: 116.1243\n",
      "Iteration: 93400\n",
      "Gradient: [  34.2184   -4.0012   13.6966 -253.2467 -548.399 ]\n",
      "Weights: [-4.484  -0.282  -0.5892  0.0857  0.1139]\n",
      "MSE loss: 116.073\n",
      "Iteration: 93500\n",
      "Gradient: [  -13.8475   -22.0063    48.9278   -49.8443 -1156.3152]\n",
      "Weights: [-4.4859 -0.2809 -0.5896  0.0857  0.1139]\n",
      "MSE loss: 116.0323\n",
      "Iteration: 93600\n",
      "Gradient: [   8.3923    0.7937  -66.4223 -271.5642  131.7135]\n",
      "Weights: [-4.4836 -0.2803 -0.5901  0.0857  0.1139]\n",
      "MSE loss: 115.9916\n",
      "Iteration: 93700\n",
      "Gradient: [  24.3349  -46.4573 -153.0814  -14.8363  576.0967]\n",
      "Weights: [-4.4847 -0.2798 -0.5907  0.0856  0.114 ]\n",
      "MSE loss: 115.9442\n",
      "Iteration: 93800\n",
      "Gradient: [-7.9504 -3.9209 88.0481 78.6116 98.5236]\n",
      "Weights: [-4.484  -0.2789 -0.5911  0.0856  0.114 ]\n",
      "MSE loss: 115.9026\n",
      "Iteration: 93900\n",
      "Gradient: [   43.1922   -82.4987   -64.8487  -112.2053 -1050.5761]\n",
      "Weights: [-4.487  -0.2781 -0.5915  0.0856  0.114 ]\n",
      "MSE loss: 115.8676\n",
      "Iteration: 94000\n",
      "Gradient: [  13.821    -9.0326   48.8573  355.3367 -620.346 ]\n",
      "Weights: [-4.4854 -0.2773 -0.592   0.0856  0.1141]\n",
      "MSE loss: 115.8156\n",
      "Iteration: 94100\n",
      "Gradient: [   7.2527  -31.3742  162.7224  246.5594 -886.7785]\n",
      "Weights: [-4.4866 -0.2762 -0.5926  0.0855  0.1141]\n",
      "MSE loss: 115.7617\n",
      "Iteration: 94200\n",
      "Gradient: [ -16.4823  -45.6697   37.837  -140.0317 -636.765 ]\n",
      "Weights: [-4.4854 -0.2759 -0.5932  0.0855  0.1142]\n",
      "MSE loss: 115.7149\n",
      "Iteration: 94300\n",
      "Gradient: [ -6.0111  11.3434  90.323  336.3616 184.0137]\n",
      "Weights: [-4.4865 -0.2744 -0.5935  0.0855  0.1142]\n",
      "MSE loss: 115.6735\n",
      "Iteration: 94400\n",
      "Gradient: [  -4.7676  -50.155    10.8481    4.7395 -510.1948]\n",
      "Weights: [-4.4875 -0.2736 -0.594   0.0855  0.1142]\n",
      "MSE loss: 115.6285\n",
      "Iteration: 94500\n",
      "Gradient: [-11.7738  25.9076 -19.532   90.072  235.0882]\n",
      "Weights: [-4.4864 -0.2729 -0.5944  0.0854  0.1143]\n",
      "MSE loss: 115.5906\n",
      "Iteration: 94600\n",
      "Gradient: [   7.3082  -59.2805  -75.5632  -55.8461 -549.888 ]\n",
      "Weights: [-4.4863 -0.2722 -0.595   0.0854  0.1143]\n",
      "MSE loss: 115.5431\n",
      "Iteration: 94700\n",
      "Gradient: [  2.7205 -19.5146  38.2923  66.0751  87.5562]\n",
      "Weights: [-4.4859 -0.2714 -0.5954  0.0853  0.1143]\n",
      "MSE loss: 115.4993\n",
      "Iteration: 94800\n",
      "Gradient: [   6.8946   12.0564   -0.7608 -160.7007 -178.0576]\n",
      "Weights: [-4.4866 -0.2708 -0.5959  0.0853  0.1144]\n",
      "MSE loss: 115.4575\n",
      "Iteration: 94900\n",
      "Gradient: [  16.8104   -4.6683  147.4346  438.7792 -140.9269]\n",
      "Weights: [-4.4873 -0.2699 -0.5964  0.0853  0.1144]\n",
      "MSE loss: 115.414\n",
      "Iteration: 95000\n",
      "Gradient: [  -0.9779   -3.9329   43.6131 -123.6177    5.488 ]\n",
      "Weights: [-4.4884 -0.2687 -0.5969  0.0853  0.1145]\n",
      "MSE loss: 115.3651\n",
      "Iteration: 95100\n",
      "Gradient: [  41.1377  -42.9252    5.116   301.3136 -767.1454]\n",
      "Weights: [-4.4883 -0.2674 -0.5974  0.0853  0.1145]\n",
      "MSE loss: 115.3204\n",
      "Iteration: 95200\n",
      "Gradient: [ 2.1662500e+01  2.2740000e-01  7.0796000e+01  3.0541800e+02\n",
      " -1.3898697e+03]\n",
      "Weights: [-4.4883 -0.2666 -0.5979  0.0852  0.1145]\n",
      "MSE loss: 115.2742\n",
      "Iteration: 95300\n",
      "Gradient: [ -24.4915   -9.4662   72.1181 -261.1447  285.3104]\n",
      "Weights: [-4.4872 -0.2664 -0.5984  0.0852  0.1146]\n",
      "MSE loss: 115.2357\n",
      "Iteration: 95400\n",
      "Gradient: [  -3.5311   14.9038  -94.5491 -118.3348 -575.2393]\n",
      "Weights: [-4.4887 -0.2661 -0.5988  0.0852  0.1146]\n",
      "MSE loss: 115.1991\n",
      "Iteration: 95500\n",
      "Gradient: [ -40.133   -23.6231  147.0125  260.19   -919.5316]\n",
      "Weights: [-4.4885 -0.265  -0.5992  0.0852  0.1146]\n",
      "MSE loss: 115.1568\n",
      "Iteration: 95600\n",
      "Gradient: [ 17.7433 -12.7911 -53.4048 -15.9414 405.2428]\n",
      "Weights: [-4.4897 -0.2641 -0.5996  0.0852  0.1147]\n",
      "MSE loss: 115.1158\n",
      "Iteration: 95700\n",
      "Gradient: [ -18.4734  -50.1593  -56.9871   27.3813 -643.1725]\n",
      "Weights: [-4.4897 -0.2634 -0.6     0.0851  0.1147]\n",
      "MSE loss: 115.0792\n",
      "Iteration: 95800\n",
      "Gradient: [  24.156    11.8605   49.5564   40.2897 -152.5362]\n",
      "Weights: [-4.4896 -0.2623 -0.6005  0.0851  0.1147]\n",
      "MSE loss: 115.0309\n",
      "Iteration: 95900\n",
      "Gradient: [ -17.134    50.9314  -11.2359  -11.7314 -622.9034]\n",
      "Weights: [-4.4887 -0.2619 -0.6011  0.0851  0.1148]\n",
      "MSE loss: 114.9907\n",
      "Iteration: 96000\n",
      "Gradient: [  -42.234    -42.8653   145.1316   161.1655 -1065.6928]\n",
      "Weights: [-4.4902 -0.2615 -0.6015  0.0851  0.1148]\n",
      "MSE loss: 114.9551\n",
      "Iteration: 96100\n",
      "Gradient: [  10.7661   -9.3233  113.0896  169.6278 1301.8866]\n",
      "Weights: [-4.4891 -0.261  -0.602   0.0851  0.1149]\n",
      "MSE loss: 114.9111\n",
      "Iteration: 96200\n",
      "Gradient: [ -30.2211   19.1064   58.4703  105.5909 -430.7276]\n",
      "Weights: [-4.4903 -0.2599 -0.6024  0.085   0.1149]\n",
      "MSE loss: 114.8668\n",
      "Iteration: 96300\n",
      "Gradient: [  16.7472   11.8852  138.4327  173.7597 -542.1948]\n",
      "Weights: [-4.4894 -0.2592 -0.6028  0.085   0.1149]\n",
      "MSE loss: 114.8349\n",
      "Iteration: 96400\n",
      "Gradient: [ -15.3421    3.9805    6.331   198.7333 -434.0241]\n",
      "Weights: [-4.49   -0.2584 -0.6034  0.085   0.115 ]\n",
      "MSE loss: 114.7862\n",
      "Iteration: 96500\n",
      "Gradient: [ 41.0068  55.582   41.4448  20.0973 -88.1525]\n",
      "Weights: [-4.4907 -0.2575 -0.6038  0.085   0.115 ]\n",
      "MSE loss: 114.7451\n",
      "Iteration: 96600\n",
      "Gradient: [  -3.5095   21.1967  -98.1717   -5.4178 -657.3923]\n",
      "Weights: [-4.4906 -0.2565 -0.6044  0.085   0.115 ]\n",
      "MSE loss: 114.6975\n",
      "Iteration: 96700\n",
      "Gradient: [ 1.098000e-01  3.822500e+00 -5.816900e+01 -1.996355e+02 -6.230113e+02]\n",
      "Weights: [-4.4894 -0.2562 -0.6048  0.0849  0.1151]\n",
      "MSE loss: 114.6657\n",
      "Iteration: 96800\n",
      "Gradient: [   2.0234   11.4352   24.3626  -61.8571 -618.4472]\n",
      "Weights: [-4.4913 -0.2553 -0.6052  0.0849  0.1151]\n",
      "MSE loss: 114.6272\n",
      "Iteration: 96900\n",
      "Gradient: [   8.6486   33.6151   15.6976  350.0134 -186.8418]\n",
      "Weights: [-4.4926 -0.2542 -0.6056  0.0849  0.1151]\n",
      "MSE loss: 114.5842\n",
      "Iteration: 97000\n",
      "Gradient: [ -4.0772 -37.0538 149.6278  37.2878 110.9261]\n",
      "Weights: [-4.4942 -0.253  -0.606   0.0849  0.1152]\n",
      "MSE loss: 114.5435\n",
      "Iteration: 97100\n",
      "Gradient: [ 16.9866  10.1872   7.2249  94.3816 226.0222]\n",
      "Weights: [-4.493  -0.2526 -0.6065  0.0849  0.1152]\n",
      "MSE loss: 114.5034\n",
      "Iteration: 97200\n",
      "Gradient: [ 50.458  -16.6473  32.3377 -73.7969 283.5736]\n",
      "Weights: [-4.4925 -0.252  -0.6069  0.0849  0.1152]\n",
      "MSE loss: 114.4652\n",
      "Iteration: 97300\n",
      "Gradient: [   9.0623  -57.2696  -35.0196 -185.1302 -523.5925]\n",
      "Weights: [-4.493  -0.2511 -0.6074  0.0848  0.1153]\n",
      "MSE loss: 114.4248\n",
      "Iteration: 97400\n",
      "Gradient: [ 18.4217  -9.7411 -28.6832   2.0479  19.8773]\n",
      "Weights: [-4.4926 -0.2504 -0.6079  0.0848  0.1153]\n",
      "MSE loss: 114.3835\n",
      "Iteration: 97500\n",
      "Gradient: [ 37.9814 -26.5272 123.8559 287.8919 249.6118]\n",
      "Weights: [-4.4934 -0.2497 -0.6083  0.0848  0.1153]\n",
      "MSE loss: 114.3487\n",
      "Iteration: 97600\n",
      "Gradient: [  4.1535  -5.7685 102.672  370.2944 -27.8457]\n",
      "Weights: [-4.493  -0.2488 -0.6086  0.0848  0.1154]\n",
      "MSE loss: 114.3152\n",
      "Iteration: 97700\n",
      "Gradient: [  19.4392  -22.188    37.9824 -268.4876 -419.9094]\n",
      "Weights: [-4.495  -0.2478 -0.6091  0.0847  0.1154]\n",
      "MSE loss: 114.2719\n",
      "Iteration: 97800\n",
      "Gradient: [  26.1017   -2.2136  113.8229 -317.0242  -88.7107]\n",
      "Weights: [-4.4925 -0.247  -0.6096  0.0847  0.1154]\n",
      "MSE loss: 114.2328\n",
      "Iteration: 97900\n",
      "Gradient: [ -29.6159  -26.9025  121.9957   79.9986 -666.8884]\n",
      "Weights: [-4.4951 -0.2462 -0.6099  0.0847  0.1155]\n",
      "MSE loss: 114.1935\n",
      "Iteration: 98000\n",
      "Gradient: [  -5.0955   13.4293  119.5262  283.182  -514.1324]\n",
      "Weights: [-4.4963 -0.2449 -0.6104  0.0846  0.1155]\n",
      "MSE loss: 114.1526\n",
      "Iteration: 98100\n",
      "Gradient: [  17.4112   60.725   -42.9623 -195.1496 -291.3011]\n",
      "Weights: [-4.4962 -0.2437 -0.6109  0.0846  0.1155]\n",
      "MSE loss: 114.1077\n",
      "Iteration: 98200\n",
      "Gradient: [   9.5156  -40.5986   18.2376   10.0833 -268.4967]\n",
      "Weights: [-4.4961 -0.2429 -0.6114  0.0846  0.1156]\n",
      "MSE loss: 114.0616\n",
      "Iteration: 98300\n",
      "Gradient: [ 1.891000e-01 -3.626160e+01 -4.541760e+01  4.637960e+01  2.618831e+02]\n",
      "Weights: [-4.4962 -0.242  -0.6118  0.0845  0.1156]\n",
      "MSE loss: 114.0243\n",
      "Iteration: 98400\n",
      "Gradient: [  26.3601   19.8665    2.2121 -143.1909 -542.2718]\n",
      "Weights: [-4.4985 -0.241  -0.6122  0.0845  0.1156]\n",
      "MSE loss: 113.9928\n",
      "Iteration: 98500\n",
      "Gradient: [   5.4927   28.447   106.4524  210.9524 -329.6203]\n",
      "Weights: [-4.4981 -0.2397 -0.6125  0.0844  0.1156]\n",
      "MSE loss: 113.951\n",
      "Iteration: 98600\n",
      "Gradient: [   6.6909   13.5988  118.0708 -153.3393 -778.4081]\n",
      "Weights: [-4.4987 -0.2392 -0.6129  0.0844  0.1157]\n",
      "MSE loss: 113.9177\n",
      "Iteration: 98700\n",
      "Gradient: [ -36.1228   21.9458  167.2964  267.2806 -388.7621]\n",
      "Weights: [-4.4971 -0.2383 -0.6135  0.0843  0.1157]\n",
      "MSE loss: 113.8752\n",
      "Iteration: 98800\n",
      "Gradient: [  41.9601   24.7489  132.6582 -301.9341 -175.9259]\n",
      "Weights: [-4.4962 -0.2376 -0.614   0.0843  0.1157]\n",
      "MSE loss: 113.8362\n",
      "Iteration: 98900\n",
      "Gradient: [   -4.5868    45.5399   123.6345   116.3164 -1503.6019]\n",
      "Weights: [-4.4968 -0.2365 -0.6146  0.0843  0.1158]\n",
      "MSE loss: 113.7862\n",
      "Iteration: 99000\n",
      "Gradient: [  -24.7342   -15.6957    32.3148   -32.2175 -1016.8418]\n",
      "Weights: [-4.498  -0.2356 -0.615   0.0842  0.1158]\n",
      "MSE loss: 113.7425\n",
      "Iteration: 99100\n",
      "Gradient: [   6.8966   31.7252  -56.7809 -124.6301  -36.2077]\n",
      "Weights: [-4.4991 -0.2347 -0.6153  0.0842  0.1159]\n",
      "MSE loss: 113.7046\n",
      "Iteration: 99200\n",
      "Gradient: [-20.7918  38.9235  54.8907 156.2958 335.8793]\n",
      "Weights: [-4.4995 -0.2338 -0.6157  0.0842  0.1159]\n",
      "MSE loss: 113.6697\n",
      "Iteration: 99300\n",
      "Gradient: [  -5.1686    0.4565   89.7223   60.6342 -429.7319]\n",
      "Weights: [-4.4982 -0.2334 -0.6163  0.0841  0.1159]\n",
      "MSE loss: 113.6313\n",
      "Iteration: 99400\n",
      "Gradient: [-31.1783 -35.033  -15.7565 -57.3553 816.3331]\n",
      "Weights: [-4.4995 -0.2328 -0.6167  0.0841  0.116 ]\n",
      "MSE loss: 113.5882\n",
      "Iteration: 99500\n",
      "Gradient: [  -23.6618    45.4986    17.156    217.4148 -1053.6086]\n",
      "Weights: [-4.4992 -0.2314 -0.6172  0.0841  0.116 ]\n",
      "MSE loss: 113.5431\n",
      "Iteration: 99600\n",
      "Gradient: [ -38.0233  -45.8518  177.0457 -171.1372 -437.4489]\n",
      "Weights: [-4.5017 -0.2301 -0.6176  0.084   0.116 ]\n",
      "MSE loss: 113.5002\n",
      "Iteration: 99700\n",
      "Gradient: [ -40.4274   -8.8054  169.2713 -141.4463 -171.2373]\n",
      "Weights: [-4.4999 -0.2297 -0.6181  0.084   0.1161]\n",
      "MSE loss: 113.4622\n",
      "Iteration: 99800\n",
      "Gradient: [ -34.0465   12.6222   -4.2224  -54.284  -562.8787]\n",
      "Weights: [-4.5002 -0.2288 -0.6187  0.084   0.1161]\n",
      "MSE loss: 113.4109\n",
      "Iteration: 99900\n",
      "Gradient: [ 29.8024  12.2841  54.4556 -77.4577 337.9652]\n",
      "Weights: [-4.4992 -0.2281 -0.6191  0.0839  0.1161]\n",
      "MSE loss: 113.3781\n",
      "Iteration: 100000\n",
      "Gradient: [   3.7888  -34.4871  116.4613  -28.6336 -335.3028]\n",
      "Weights: [-4.5001 -0.2274 -0.6196  0.0839  0.1162]\n",
      "MSE loss: 113.3356\n",
      "Iteration: 100100\n",
      "Gradient: [    7.2715   -51.8888   127.8828    42.4528 -1163.9584]\n",
      "Weights: [-4.5029 -0.2264 -0.6199  0.0839  0.1162]\n",
      "MSE loss: 113.297\n",
      "Iteration: 100200\n",
      "Gradient: [  19.701     9.5111  111.4135   71.9896 -542.186 ]\n",
      "Weights: [-4.5028 -0.2249 -0.6203  0.0839  0.1163]\n",
      "MSE loss: 113.2567\n",
      "Iteration: 100300\n",
      "Gradient: [ -36.0214   21.5062  -10.4903  -43.367  -111.9925]\n",
      "Weights: [-4.5031 -0.2242 -0.6208  0.0839  0.1163]\n",
      "MSE loss: 113.2123\n",
      "Iteration: 100400\n",
      "Gradient: [  0.9224 -46.8538 144.1952  54.9969 773.3444]\n",
      "Weights: [-4.5026 -0.2232 -0.6212  0.0838  0.1163]\n",
      "MSE loss: 113.1793\n",
      "Iteration: 100500\n",
      "Gradient: [ -12.0566  -54.7093   54.6666 -253.0201  439.6469]\n",
      "Weights: [-4.5044 -0.222  -0.6216  0.0838  0.1163]\n",
      "MSE loss: 113.1378\n",
      "Iteration: 100600\n",
      "Gradient: [ -17.3498   14.3306  -10.1258   14.9859 -809.3782]\n",
      "Weights: [-4.5046 -0.2211 -0.6223  0.0837  0.1164]\n",
      "MSE loss: 113.0906\n",
      "Iteration: 100700\n",
      "Gradient: [  23.524    -9.0544  125.8549    9.538  -521.2343]\n",
      "Weights: [-4.5043 -0.2204 -0.6227  0.0837  0.1164]\n",
      "MSE loss: 113.0528\n",
      "Iteration: 100800\n",
      "Gradient: [  31.9043   61.9952   75.405  -261.4602  -46.3273]\n",
      "Weights: [-4.504  -0.2196 -0.6232  0.0837  0.1164]\n",
      "MSE loss: 113.0143\n",
      "Iteration: 100900\n",
      "Gradient: [  14.866    41.7951    4.155   304.1596 -197.3185]\n",
      "Weights: [-4.502  -0.2191 -0.6238  0.0837  0.1165]\n",
      "MSE loss: 112.9759\n",
      "Iteration: 101000\n",
      "Gradient: [ -11.617   -10.018   -10.4405   14.2519 -563.1999]\n",
      "Weights: [-4.5037 -0.2187 -0.6241  0.0837  0.1165]\n",
      "MSE loss: 112.9377\n",
      "Iteration: 101100\n",
      "Gradient: [ -17.9982   -1.1825   15.2273  339.2774 -293.0763]\n",
      "Weights: [-4.5038 -0.2177 -0.6245  0.0837  0.1166]\n",
      "MSE loss: 112.8978\n",
      "Iteration: 101200\n",
      "Gradient: [   9.1655   19.6403  -70.488   -95.2028 -993.1328]\n",
      "Weights: [-4.5049 -0.2172 -0.625   0.0836  0.1166]\n",
      "MSE loss: 112.8621\n",
      "Iteration: 101300\n",
      "Gradient: [  -0.9707  -10.6982  179.9701 -419.0656 -300.3948]\n",
      "Weights: [-4.5039 -0.2166 -0.6255  0.0836  0.1166]\n",
      "MSE loss: 112.8196\n",
      "Iteration: 101400\n",
      "Gradient: [ 11.9287 -19.6055 160.7583 -82.9982 862.6969]\n",
      "Weights: [-4.5046 -0.2156 -0.6259  0.0836  0.1167]\n",
      "MSE loss: 112.7851\n",
      "Iteration: 101500\n",
      "Gradient: [  -3.2845   18.3856   50.3605  194.3895 -509.1109]\n",
      "Weights: [-4.5065 -0.2145 -0.6261  0.0836  0.1167]\n",
      "MSE loss: 112.7493\n",
      "Iteration: 101600\n",
      "Gradient: [ -8.9636   7.5726 -10.67   -10.4078 -65.6725]\n",
      "Weights: [-4.5087 -0.2131 -0.6265  0.0835  0.1167]\n",
      "MSE loss: 112.7136\n",
      "Iteration: 101700\n",
      "Gradient: [  9.4819 -22.1336 148.6424 111.3399 486.9112]\n",
      "Weights: [-4.5083 -0.2122 -0.6272  0.0835  0.1168]\n",
      "MSE loss: 112.6622\n",
      "Iteration: 101800\n",
      "Gradient: [  18.7704   55.0085   80.7216   -4.9663 -893.3346]\n",
      "Weights: [-4.5075 -0.2112 -0.6276  0.0835  0.1168]\n",
      "MSE loss: 112.6222\n",
      "Iteration: 101900\n",
      "Gradient: [ -17.1545    0.8058   60.0517 -153.0089   27.1549]\n",
      "Weights: [-4.5087 -0.2104 -0.6281  0.0834  0.1168]\n",
      "MSE loss: 112.5783\n",
      "Iteration: 102000\n",
      "Gradient: [   9.9508   -5.0791   78.1007 -309.059  -173.4606]\n",
      "Weights: [-4.5082 -0.2096 -0.6286  0.0834  0.1169]\n",
      "MSE loss: 112.5383\n",
      "Iteration: 102100\n",
      "Gradient: [-14.9954 -20.2296 -14.7091 -97.243   74.5376]\n",
      "Weights: [-4.5088 -0.2086 -0.629   0.0834  0.1169]\n",
      "MSE loss: 112.5006\n",
      "Iteration: 102200\n",
      "Gradient: [   0.8021  -75.4028   20.3107  263.0355 -234.1887]\n",
      "Weights: [-4.5084 -0.2083 -0.6295  0.0833  0.1169]\n",
      "MSE loss: 112.4618\n",
      "Iteration: 102300\n",
      "Gradient: [  24.4735   94.7827  -82.3641 -265.2156  -54.0743]\n",
      "Weights: [-4.5063 -0.2076 -0.6301  0.0833  0.117 ]\n",
      "MSE loss: 112.4209\n",
      "Iteration: 102400\n",
      "Gradient: [   10.2047   -26.4005    32.4718  -127.7076 -1079.4125]\n",
      "Weights: [-4.5084 -0.207  -0.6305  0.0833  0.117 ]\n",
      "MSE loss: 112.3872\n",
      "Iteration: 102500\n",
      "Gradient: [ 17.0847 -33.4684 122.4961 231.696  148.6414]\n",
      "Weights: [-4.5088 -0.2058 -0.6307  0.0832  0.117 ]\n",
      "MSE loss: 112.3562\n",
      "Iteration: 102600\n",
      "Gradient: [   -2.9269    -7.174     56.1973  -264.6344 -1886.1923]\n",
      "Weights: [-4.5081 -0.2052 -0.6312  0.0832  0.1171]\n",
      "MSE loss: 112.318\n",
      "Iteration: 102700\n",
      "Gradient: [ -24.7311  -53.3666  161.1952 -100.5572  753.4611]\n",
      "Weights: [-4.5091 -0.2044 -0.6317  0.0832  0.1171]\n",
      "MSE loss: 112.2817\n",
      "Iteration: 102800\n",
      "Gradient: [ -29.7916  -47.3828   56.0858  -39.8882 -303.6118]\n",
      "Weights: [-4.5084 -0.2039 -0.6319  0.0831  0.1171]\n",
      "MSE loss: 112.2544\n",
      "Iteration: 102900\n",
      "Gradient: [ -5.5831 -10.6425 108.1264 -42.1852 106.3467]\n",
      "Weights: [-4.5083 -0.2027 -0.6323  0.0831  0.1172]\n",
      "MSE loss: 112.218\n",
      "Iteration: 103000\n",
      "Gradient: [ -1.0019  21.1471  93.5015  45.1763 481.9894]\n",
      "Weights: [-4.5074 -0.2024 -0.6328  0.0831  0.1172]\n",
      "MSE loss: 112.1846\n",
      "Iteration: 103100\n",
      "Gradient: [ -23.6511   -6.2537   77.7867   -7.8631 -312.6783]\n",
      "Weights: [-4.5087 -0.2021 -0.6333  0.0831  0.1172]\n",
      "MSE loss: 112.1481\n",
      "Iteration: 103200\n",
      "Gradient: [  -2.3904    7.1616   74.4669  186.0404 -298.6176]\n",
      "Weights: [-4.5091 -0.201  -0.6337  0.083   0.1173]\n",
      "MSE loss: 112.1047\n",
      "Iteration: 103300\n",
      "Gradient: [  -2.672    14.949    55.5263  184.2595 -254.3985]\n",
      "Weights: [-4.5085 -0.2004 -0.6341  0.083   0.1173]\n",
      "MSE loss: 112.0711\n",
      "Iteration: 103400\n",
      "Gradient: [  -9.4214  -42.7297    4.3755  428.3562 -543.9055]\n",
      "Weights: [-4.5117 -0.1994 -0.6343  0.083   0.1173]\n",
      "MSE loss: 112.0404\n",
      "Iteration: 103500\n",
      "Gradient: [  -16.8697    23.3293    62.2124   143.955  -1004.1047]\n",
      "Weights: [-4.5108 -0.1989 -0.6348  0.0829  0.1174]\n",
      "MSE loss: 112.0025\n",
      "Iteration: 103600\n",
      "Gradient: [ -46.6372   -5.8287  -76.6996  -80.817  -551.9907]\n",
      "Weights: [-4.5114 -0.1985 -0.6352  0.0829  0.1174]\n",
      "MSE loss: 111.9698\n",
      "Iteration: 103700\n",
      "Gradient: [   9.4338   25.07     74.2491  460.7861 -525.2215]\n",
      "Weights: [-4.5121 -0.1973 -0.6353  0.0829  0.1174]\n",
      "MSE loss: 111.9444\n",
      "Iteration: 103800\n",
      "Gradient: [-76.4221  51.393   -1.1115  99.5005  81.4178]\n",
      "Weights: [-4.5113 -0.1971 -0.6357  0.0829  0.1175]\n",
      "MSE loss: 111.914\n",
      "Iteration: 103900\n",
      "Gradient: [  14.6199  -30.6521   23.3984 -172.1085 -337.7634]\n",
      "Weights: [-4.5104 -0.1966 -0.6362  0.0828  0.1175]\n",
      "MSE loss: 111.8784\n",
      "Iteration: 104000\n",
      "Gradient: [  43.0245   13.9842  -72.7809   11.5577 -669.0104]\n",
      "Weights: [-4.51   -0.1958 -0.6368  0.0828  0.1175]\n",
      "MSE loss: 111.8337\n",
      "Iteration: 104100\n",
      "Gradient: [ 35.248    4.4727 -40.7463 176.8268 -33.1434]\n",
      "Weights: [-4.5118 -0.1953 -0.637   0.0828  0.1176]\n",
      "MSE loss: 111.806\n",
      "Iteration: 104200\n",
      "Gradient: [ -34.538   -44.2743   40.3292 -158.7989  208.4647]\n",
      "Weights: [-4.5098 -0.1949 -0.6376  0.0828  0.1176]\n",
      "MSE loss: 111.7711\n",
      "Iteration: 104300\n",
      "Gradient: [-2.8796200e+01 -1.4460200e+01 -6.8794000e+00 -2.3370000e-01\n",
      " -1.0383695e+03]\n",
      "Weights: [-4.5104 -0.1939 -0.6379  0.0828  0.1176]\n",
      "MSE loss: 111.7355\n",
      "Iteration: 104400\n",
      "Gradient: [  57.822    32.0367   63.9046 -135.6792 -778.8133]\n",
      "Weights: [-4.5105 -0.1934 -0.6384  0.0828  0.1177]\n",
      "MSE loss: 111.7024\n",
      "Iteration: 104500\n",
      "Gradient: [ -1.2723 -24.0578  -8.3945 289.1083 120.4676]\n",
      "Weights: [-4.5105 -0.1931 -0.6388  0.0827  0.1177]\n",
      "MSE loss: 111.6694\n",
      "Iteration: 104600\n",
      "Gradient: [ -12.1631  -11.4841  241.7742 -112.9642    6.5847]\n",
      "Weights: [-4.5103 -0.1923 -0.6392  0.0827  0.1177]\n",
      "MSE loss: 111.6379\n",
      "Iteration: 104700\n",
      "Gradient: [ -17.0685  -12.1857   53.3367  250.1325 -601.7063]\n",
      "Weights: [-4.5118 -0.1915 -0.6396  0.0827  0.1178]\n",
      "MSE loss: 111.5989\n",
      "Iteration: 104800\n",
      "Gradient: [ -22.0882    8.2274 -108.7618 -257.5746   64.0772]\n",
      "Weights: [-4.5138 -0.1909 -0.6399  0.0827  0.1178]\n",
      "MSE loss: 111.5705\n",
      "Iteration: 104900\n",
      "Gradient: [ -26.9461  -66.5047  -43.5648   85.9554 -574.8184]\n",
      "Weights: [-4.5122 -0.19   -0.6403  0.0827  0.1178]\n",
      "MSE loss: 111.5318\n",
      "Iteration: 105000\n",
      "Gradient: [  -6.4497  -42.7992   22.174   -76.5791 -550.3275]\n",
      "Weights: [-4.5125 -0.1888 -0.6407  0.0826  0.1179]\n",
      "MSE loss: 111.498\n",
      "Iteration: 105100\n",
      "Gradient: [   4.8411   18.5797  -52.1368  -13.3686 -172.4066]\n",
      "Weights: [-4.5131 -0.1878 -0.6413  0.0826  0.1179]\n",
      "MSE loss: 111.4527\n",
      "Iteration: 105200\n",
      "Gradient: [ -10.8687  -74.8553  -39.4381  -61.3307 -770.3974]\n",
      "Weights: [-4.5143 -0.187  -0.6416  0.0825  0.1179]\n",
      "MSE loss: 111.4179\n",
      "Iteration: 105300\n",
      "Gradient: [-23.384  -47.5171  31.4144  99.8952 785.1248]\n",
      "Weights: [-4.5127 -0.1863 -0.6421  0.0825  0.118 ]\n",
      "MSE loss: 111.3803\n",
      "Iteration: 105400\n",
      "Gradient: [ -16.906   -39.877  -113.9287 -196.9912 -377.7212]\n",
      "Weights: [-4.5131 -0.1858 -0.6425  0.0824  0.118 ]\n",
      "MSE loss: 111.3502\n",
      "Iteration: 105500\n",
      "Gradient: [ -40.0727  -28.7361   52.2048  254.7    -268.9579]\n",
      "Weights: [-4.5142 -0.1847 -0.6428  0.0824  0.118 ]\n",
      "MSE loss: 111.3151\n",
      "Iteration: 105600\n",
      "Gradient: [   26.7991    31.165    109.9291   475.4389 -1184.3763]\n",
      "Weights: [-4.5134 -0.1842 -0.6432  0.0824  0.1181]\n",
      "MSE loss: 111.2826\n",
      "Iteration: 105700\n",
      "Gradient: [  28.416   -38.7365  107.9519  130.5775 -512.5796]\n",
      "Weights: [-4.5153 -0.1833 -0.6435  0.0823  0.1181]\n",
      "MSE loss: 111.2483\n",
      "Iteration: 105800\n",
      "Gradient: [ 29.8271 -40.0095  79.869  375.8711 337.2265]\n",
      "Weights: [-4.5171 -0.1821 -0.6439  0.0823  0.1181]\n",
      "MSE loss: 111.2139\n",
      "Iteration: 105900\n",
      "Gradient: [  17.5536   -6.217   101.6781  -93.8012 -624.7411]\n",
      "Weights: [-4.5184 -0.1811 -0.6442  0.0823  0.1182]\n",
      "MSE loss: 111.186\n",
      "Iteration: 106000\n",
      "Gradient: [  13.9265  -16.8434  -33.5612  289.9715 -622.7422]\n",
      "Weights: [-4.5171 -0.1798 -0.6447  0.0822  0.1182]\n",
      "MSE loss: 111.1387\n",
      "Iteration: 106100\n",
      "Gradient: [-1.890490e+01  4.822000e-01  1.170007e+02  4.061430e+01 -7.390007e+02]\n",
      "Weights: [-4.5165 -0.1791 -0.6452  0.0821  0.1182]\n",
      "MSE loss: 111.0996\n",
      "Iteration: 106200\n",
      "Gradient: [   -5.3617    38.296     20.323   -105.5004 -1377.8669]\n",
      "Weights: [-4.5169 -0.1785 -0.6457  0.0821  0.1183]\n",
      "MSE loss: 111.065\n",
      "Iteration: 106300\n",
      "Gradient: [  4.2122 -29.8444 119.6427 -67.6    -30.3287]\n",
      "Weights: [-4.5174 -0.1776 -0.6461  0.0821  0.1183]\n",
      "MSE loss: 111.0267\n",
      "Iteration: 106400\n",
      "Gradient: [-18.5415 -10.8101  35.6263 198.8908 207.5152]\n",
      "Weights: [-4.5182 -0.1765 -0.6466  0.082   0.1183]\n",
      "MSE loss: 110.9851\n",
      "Iteration: 106500\n",
      "Gradient: [ -12.6158   64.2668  183.209  -199.1607 -850.1463]\n",
      "Weights: [-4.5187 -0.1757 -0.647   0.082   0.1184]\n",
      "MSE loss: 110.953\n",
      "Iteration: 106600\n",
      "Gradient: [  13.5592  -40.1695  -28.6871  223.0488 -201.7789]\n",
      "Weights: [-4.5192 -0.175  -0.6474  0.082   0.1184]\n",
      "MSE loss: 110.9187\n",
      "Iteration: 106700\n",
      "Gradient: [  -7.7176    7.9252   15.9885  249.6189 -157.6688]\n",
      "Weights: [-4.5184 -0.1746 -0.6478  0.082   0.1184]\n",
      "MSE loss: 110.888\n",
      "Iteration: 106800\n",
      "Gradient: [ -4.069   24.3531  20.0197 265.786  140.8498]\n",
      "Weights: [-4.5195 -0.1738 -0.6482  0.0819  0.1185]\n",
      "MSE loss: 110.8522\n",
      "Iteration: 106900\n",
      "Gradient: [  12.8188  -33.6762   10.4336  310.9907 -166.5667]\n",
      "Weights: [-4.5178 -0.173  -0.6487  0.0819  0.1185]\n",
      "MSE loss: 110.8116\n",
      "Iteration: 107000\n",
      "Gradient: [-2.5460000e-01  6.3866000e+00  1.2224740e+02 -8.1597200e+01\n",
      " -1.1122222e+03]\n",
      "Weights: [-4.5182 -0.1724 -0.6491  0.0819  0.1185]\n",
      "MSE loss: 110.7807\n",
      "Iteration: 107100\n",
      "Gradient: [ -13.829    74.2733   83.59    461.6525 -310.6349]\n",
      "Weights: [-4.5187 -0.1719 -0.6494  0.0819  0.1186]\n",
      "MSE loss: 110.7524\n",
      "Iteration: 107200\n",
      "Gradient: [  -2.9526   -5.8823 -103.3636  -85.5934 -291.2204]\n",
      "Weights: [-4.5187 -0.1717 -0.6497  0.0818  0.1186]\n",
      "MSE loss: 110.7316\n",
      "Iteration: 107300\n",
      "Gradient: [   3.4515  -32.7074  -42.5217   56.0238 -331.5677]\n",
      "Weights: [-4.518  -0.1709 -0.6502  0.0818  0.1186]\n",
      "MSE loss: 110.6953\n",
      "Iteration: 107400\n",
      "Gradient: [ -16.3406  -13.0967  -85.9394  -97.0839 -693.7614]\n",
      "Weights: [-4.5187 -0.1701 -0.6505  0.0818  0.1186]\n",
      "MSE loss: 110.666\n",
      "Iteration: 107500\n",
      "Gradient: [   2.7755   -6.2537  128.5689 -352.0433  372.0929]\n",
      "Weights: [-4.5192 -0.1685 -0.6509  0.0818  0.1187]\n",
      "MSE loss: 110.63\n",
      "Iteration: 107600\n",
      "Gradient: [ -19.7591    6.0236  124.5507  -94.3986 -830.0681]\n",
      "Weights: [-4.52   -0.1682 -0.6514  0.0817  0.1187]\n",
      "MSE loss: 110.5903\n",
      "Iteration: 107700\n",
      "Gradient: [  12.5633   38.5531   94.8406   66.6428 -419.3233]\n",
      "Weights: [-4.5189 -0.1675 -0.6517  0.0817  0.1187]\n",
      "MSE loss: 110.5638\n",
      "Iteration: 107800\n",
      "Gradient: [  5.3083 -95.4999  82.2227 140.1122 188.2788]\n",
      "Weights: [-4.5207 -0.1674 -0.652   0.0817  0.1188]\n",
      "MSE loss: 110.539\n",
      "Iteration: 107900\n",
      "Gradient: [ 45.9495 -31.617  112.2935 -46.4496 266.4932]\n",
      "Weights: [-4.5196 -0.1666 -0.6525  0.0816  0.1188]\n",
      "MSE loss: 110.5022\n",
      "Iteration: 108000\n",
      "Gradient: [   1.3647  -22.464   -13.6674  213.6324 -752.5232]\n",
      "Weights: [-4.5191 -0.1659 -0.6528  0.0816  0.1188]\n",
      "MSE loss: 110.4749\n",
      "Iteration: 108100\n",
      "Gradient: [  -2.9446   -2.325    23.3613  224.9345 -534.3393]\n",
      "Weights: [-4.5198 -0.1648 -0.6533  0.0816  0.1189]\n",
      "MSE loss: 110.435\n",
      "Iteration: 108200\n",
      "Gradient: [-15.928   13.4462  12.9132 -80.1312  57.9036]\n",
      "Weights: [-4.5232 -0.1638 -0.6535  0.0816  0.1189]\n",
      "MSE loss: 110.4058\n",
      "Iteration: 108300\n",
      "Gradient: [  31.7604   -0.6957  -62.6427  402.9713 -344.6467]\n",
      "Weights: [-4.5234 -0.1631 -0.6539  0.0816  0.1189]\n",
      "MSE loss: 110.3723\n",
      "Iteration: 108400\n",
      "Gradient: [   7.4932  -32.0338   52.6879 -218.1421 -121.2576]\n",
      "Weights: [-4.522  -0.1625 -0.6543  0.0816  0.119 ]\n",
      "MSE loss: 110.3444\n",
      "Iteration: 108500\n",
      "Gradient: [ 16.421    6.0782  -4.2066 282.9586 234.9577]\n",
      "Weights: [-4.5222 -0.1616 -0.6547  0.0815  0.119 ]\n",
      "MSE loss: 110.3131\n",
      "Iteration: 108600\n",
      "Gradient: [ 43.3033  16.1282  56.3068 219.4401 276.3808]\n",
      "Weights: [-4.5221 -0.1611 -0.6551  0.0815  0.119 ]\n",
      "MSE loss: 110.2826\n",
      "Iteration: 108700\n",
      "Gradient: [  -3.2194   33.3648   20.9926 -178.1532  -41.6562]\n",
      "Weights: [-4.5213 -0.1609 -0.6555  0.0815  0.119 ]\n",
      "MSE loss: 110.2513\n",
      "Iteration: 108800\n",
      "Gradient: [-11.1204 -22.8777  53.4258  77.4402 709.5918]\n",
      "Weights: [-4.5226 -0.1599 -0.6559  0.0814  0.1191]\n",
      "MSE loss: 110.2173\n",
      "Iteration: 108900\n",
      "Gradient: [ 1.2912000e+00  8.4120000e-01  1.3142640e+02  1.5526370e+02\n",
      " -1.1256511e+03]\n",
      "Weights: [-4.523  -0.1586 -0.6563  0.0814  0.1191]\n",
      "MSE loss: 110.1832\n",
      "Iteration: 109000\n",
      "Gradient: [ -18.7072   29.4675   55.448   286.5541 -509.4056]\n",
      "Weights: [-4.5231 -0.1584 -0.657   0.0814  0.1191]\n",
      "MSE loss: 110.1414\n",
      "Iteration: 109100\n",
      "Gradient: [-16.5946   7.7196 153.8869  38.8142 176.0364]\n",
      "Weights: [-4.5238 -0.1571 -0.6572  0.0814  0.1192]\n",
      "MSE loss: 110.1106\n",
      "Iteration: 109200\n",
      "Gradient: [  -1.5886   18.4112  -11.4992  -84.9331 -759.882 ]\n",
      "Weights: [-4.5242 -0.157  -0.6576  0.0813  0.1192]\n",
      "MSE loss: 110.0852\n",
      "Iteration: 109300\n",
      "Gradient: [  30.3449  -23.4216  106.0722 -219.6905 -435.3425]\n",
      "Weights: [-4.5237 -0.1559 -0.6581  0.0814  0.1192]\n",
      "MSE loss: 110.0503\n",
      "Iteration: 109400\n",
      "Gradient: [  29.6133  -38.2158   96.9198 -128.8393  411.8448]\n",
      "Weights: [-4.5231 -0.1559 -0.6584  0.0813  0.1192]\n",
      "MSE loss: 110.0272\n",
      "Iteration: 109500\n",
      "Gradient: [ -11.6335  -69.1996 -104.3015  191.756  -566.5849]\n",
      "Weights: [-4.5228 -0.1551 -0.659   0.0813  0.1193]\n",
      "MSE loss: 109.9851\n",
      "Iteration: 109600\n",
      "Gradient: [ -31.3471  -45.3236  119.1233  129.655  -781.95  ]\n",
      "Weights: [-4.5251 -0.1538 -0.6595  0.0814  0.1193]\n",
      "MSE loss: 109.9437\n",
      "Iteration: 109700\n",
      "Gradient: [   43.6415   -22.2724   -81.2955   105.7639 -1275.1364]\n",
      "Weights: [-4.524  -0.1533 -0.6598  0.0813  0.1193]\n",
      "MSE loss: 109.9202\n",
      "Iteration: 109800\n",
      "Gradient: [  23.2144   30.607   106.5311  205.1966 -212.7826]\n",
      "Weights: [-4.5255 -0.1526 -0.6602  0.0813  0.1194]\n",
      "MSE loss: 109.885\n",
      "Iteration: 109900\n",
      "Gradient: [  -7.5245   11.4659  113.6343 -214.6625 -720.2383]\n",
      "Weights: [-4.5258 -0.1522 -0.6605  0.0813  0.1194]\n",
      "MSE loss: 109.8618\n",
      "Iteration: 110000\n",
      "Gradient: [  -16.1086     9.3402    58.6372   287.2111 -1036.7751]\n",
      "Weights: [-4.5245 -0.1516 -0.661   0.0813  0.1194]\n",
      "MSE loss: 109.8276\n",
      "Iteration: 110100\n",
      "Gradient: [ -5.5258   8.4648 158.7792  16.1795 260.6566]\n",
      "Weights: [-4.5241 -0.151  -0.6615  0.0813  0.1195]\n",
      "MSE loss: 109.7938\n",
      "Iteration: 110200\n",
      "Gradient: [  -7.4058   16.6473  -36.2507  221.7333 -112.5382]\n",
      "Weights: [-4.5251 -0.1505 -0.6617  0.0813  0.1195]\n",
      "MSE loss: 109.7703\n",
      "Iteration: 110300\n",
      "Gradient: [-2.403000e-01 -3.079130e+01 -7.630960e+01 -1.130145e+02 -9.770102e+02]\n",
      "Weights: [-4.5266 -0.1497 -0.6622  0.0812  0.1195]\n",
      "MSE loss: 109.7382\n",
      "Iteration: 110400\n",
      "Gradient: [   7.5571   -4.2754   69.5317  343.2201 -974.5338]\n",
      "Weights: [-4.525  -0.1491 -0.6625  0.0812  0.1196]\n",
      "MSE loss: 109.7058\n",
      "Iteration: 110500\n",
      "Gradient: [ -3.6008   8.4978 102.707  104.206  645.4295]\n",
      "Weights: [-4.5251 -0.1482 -0.663   0.0812  0.1196]\n",
      "MSE loss: 109.6714\n",
      "Iteration: 110600\n",
      "Gradient: [ -16.9001   20.1405   52.6135  -34.9507 -676.5109]\n",
      "Weights: [-4.5263 -0.1471 -0.6635  0.0812  0.1196]\n",
      "MSE loss: 109.6321\n",
      "Iteration: 110700\n",
      "Gradient: [  12.2079    5.2592   32.5171   -5.9078 -936.375 ]\n",
      "Weights: [-4.5259 -0.1464 -0.6639  0.0812  0.1196]\n",
      "MSE loss: 109.6024\n",
      "Iteration: 110800\n",
      "Gradient: [   3.5552  -34.6115  105.7656  265.5816 -882.5505]\n",
      "Weights: [-4.5263 -0.145  -0.6645  0.0811  0.1197]\n",
      "MSE loss: 109.5565\n",
      "Iteration: 110900\n",
      "Gradient: [ -11.0383   33.1942  128.1593 -170.4961 -186.9873]\n",
      "Weights: [-4.5269 -0.1444 -0.6648  0.0811  0.1197]\n",
      "MSE loss: 109.5303\n",
      "Iteration: 111000\n",
      "Gradient: [ 14.0642   9.2181  24.1705 138.514  206.2067]\n",
      "Weights: [-4.5282 -0.143  -0.6652  0.0811  0.1197]\n",
      "MSE loss: 109.4986\n",
      "Iteration: 111100\n",
      "Gradient: [  20.8063 -104.1802 -113.6394   50.9064   16.9127]\n",
      "Weights: [-4.5276 -0.1426 -0.6656  0.0811  0.1198]\n",
      "MSE loss: 109.4689\n",
      "Iteration: 111200\n",
      "Gradient: [   1.1076   13.9346  135.3993 -129.3982 -643.0711]\n",
      "Weights: [-4.5284 -0.1419 -0.666   0.081   0.1198]\n",
      "MSE loss: 109.4345\n",
      "Iteration: 111300\n",
      "Gradient: [  11.3549  -25.937   212.3341  101.4    -590.3517]\n",
      "Weights: [-4.5292 -0.1406 -0.6665  0.081   0.1198]\n",
      "MSE loss: 109.3974\n",
      "Iteration: 111400\n",
      "Gradient: [  -6.5272  -41.2807    5.0133   -8.6274 -563.1889]\n",
      "Weights: [-4.5274 -0.1398 -0.6671  0.081   0.1198]\n",
      "MSE loss: 109.3614\n",
      "Iteration: 111500\n",
      "Gradient: [  -1.2374  -19.9229  126.4689 -280.9952 -169.2981]\n",
      "Weights: [-4.5258 -0.1398 -0.6675  0.081   0.1199]\n",
      "MSE loss: 109.3393\n",
      "Iteration: 111600\n",
      "Gradient: [-23.7038 -26.208  126.3184 286.432  238.7013]\n",
      "Weights: [-4.5272 -0.1389 -0.6679  0.081   0.1199]\n",
      "MSE loss: 109.2991\n",
      "Iteration: 111700\n",
      "Gradient: [ -10.6835   25.55     23.6802   77.5929 -113.1985]\n",
      "Weights: [-4.5287 -0.1379 -0.6683  0.0809  0.1199]\n",
      "MSE loss: 109.2624\n",
      "Iteration: 111800\n",
      "Gradient: [ -11.8071   13.2779   45.1143 -142.809   -23.6673]\n",
      "Weights: [-4.5286 -0.1374 -0.6688  0.0809  0.12  ]\n",
      "MSE loss: 109.2286\n",
      "Iteration: 111900\n",
      "Gradient: [-26.5469 -31.2858 -14.0088 174.8061  78.3315]\n",
      "Weights: [-4.53   -0.1365 -0.669   0.0809  0.12  ]\n",
      "MSE loss: 109.2022\n",
      "Iteration: 112000\n",
      "Gradient: [  14.6432  -22.8597   68.6441  307.5613 -365.3996]\n",
      "Weights: [-4.5319 -0.1353 -0.6695  0.0809  0.12  ]\n",
      "MSE loss: 109.1678\n",
      "Iteration: 112100\n",
      "Gradient: [  -5.8353    3.369    13.9957 -144.7109  976.0676]\n",
      "Weights: [-4.5315 -0.1345 -0.67    0.0809  0.1201]\n",
      "MSE loss: 109.1323\n",
      "Iteration: 112200\n",
      "Gradient: [  32.663    52.0662  -46.1551   77.2526 -197.6756]\n",
      "Weights: [-4.5313 -0.1336 -0.6705  0.0809  0.1201]\n",
      "MSE loss: 109.0926\n",
      "Iteration: 112300\n",
      "Gradient: [ -23.4486   14.1752  -27.2172  145.3143 -362.4188]\n",
      "Weights: [-4.5307 -0.1329 -0.6709  0.0809  0.1201]\n",
      "MSE loss: 109.0615\n",
      "Iteration: 112400\n",
      "Gradient: [ 33.5551  48.1577 104.2699  39.8267 943.6249]\n",
      "Weights: [-4.5315 -0.1325 -0.6711  0.0808  0.1202]\n",
      "MSE loss: 109.0376\n",
      "Iteration: 112500\n",
      "Gradient: [ 36.0392  -9.4659  29.7313 115.9283 468.2035]\n",
      "Weights: [-4.5309 -0.1324 -0.6716  0.0808  0.1202]\n",
      "MSE loss: 109.009\n",
      "Iteration: 112600\n",
      "Gradient: [9.850000e-02 1.513750e+01 1.185232e+02 1.584278e+02 5.050949e+02]\n",
      "Weights: [-4.5321 -0.1314 -0.6718  0.0808  0.1202]\n",
      "MSE loss: 108.9836\n",
      "Iteration: 112700\n",
      "Gradient: [   0.5603  -23.6213  -20.5485  -47.6401 -299.3987]\n",
      "Weights: [-4.5308 -0.1312 -0.6722  0.0807  0.1202]\n",
      "MSE loss: 108.9561\n",
      "Iteration: 112800\n",
      "Gradient: [   8.8302   32.2639   95.5832 -137.2404 -700.8006]\n",
      "Weights: [-4.5318 -0.1304 -0.6724  0.0807  0.1203]\n",
      "MSE loss: 108.934\n",
      "Iteration: 112900\n",
      "Gradient: [ -30.9594  -37.911   121.7402  166.6157 -500.1099]\n",
      "Weights: [-4.5318 -0.1298 -0.6729  0.0807  0.1203]\n",
      "MSE loss: 108.8998\n",
      "Iteration: 113000\n",
      "Gradient: [ 21.2118  25.5301  -3.7523  -0.6675 340.806 ]\n",
      "Weights: [-4.5314 -0.1292 -0.6734  0.0807  0.1203]\n",
      "MSE loss: 108.87\n",
      "Iteration: 113100\n",
      "Gradient: [   9.1857   19.2321   56.7019   42.3433 -420.0967]\n",
      "Weights: [-4.5319 -0.1283 -0.6737  0.0806  0.1204]\n",
      "MSE loss: 108.84\n",
      "Iteration: 113200\n",
      "Gradient: [ 27.1825  60.1646  45.5696 356.4324 301.0001]\n",
      "Weights: [-4.5318 -0.1271 -0.6743  0.0806  0.1204]\n",
      "MSE loss: 108.8011\n",
      "Iteration: 113300\n",
      "Gradient: [ -39.0708  -29.0637   18.3502 -238.9366  -51.9638]\n",
      "Weights: [-4.5341 -0.1262 -0.6745  0.0807  0.1204]\n",
      "MSE loss: 108.7749\n",
      "Iteration: 113400\n",
      "Gradient: [ -1.5433 -18.1228  50.6212  44.1102 -17.3742]\n",
      "Weights: [-4.5344 -0.1257 -0.675   0.0806  0.1205]\n",
      "MSE loss: 108.7417\n",
      "Iteration: 113500\n",
      "Gradient: [   5.5923  -60.6532  190.1886 -103.7589 -125.5594]\n",
      "Weights: [-4.5355 -0.125  -0.6752  0.0806  0.1205]\n",
      "MSE loss: 108.725\n",
      "Iteration: 113600\n",
      "Gradient: [  -24.0119    -4.0149    31.506    -39.9851 -1012.4656]\n",
      "Weights: [-4.5348 -0.1241 -0.6756  0.0806  0.1205]\n",
      "MSE loss: 108.6903\n",
      "Iteration: 113700\n",
      "Gradient: [ -14.222   -65.3974  -20.6125 -168.3248 -369.5334]\n",
      "Weights: [-4.5341 -0.1234 -0.676   0.0805  0.1205]\n",
      "MSE loss: 108.6592\n",
      "Iteration: 113800\n",
      "Gradient: [ 30.4485   1.5356 -78.58   209.0843 -20.1462]\n",
      "Weights: [-4.5339 -0.1227 -0.6764  0.0805  0.1206]\n",
      "MSE loss: 108.6323\n",
      "Iteration: 113900\n",
      "Gradient: [ 1.3228700e+01 -4.6435800e+01  4.7770000e-01  2.6433120e+02\n",
      " -1.5167233e+03]\n",
      "Weights: [-4.533  -0.1223 -0.6768  0.0805  0.1206]\n",
      "MSE loss: 108.6063\n",
      "Iteration: 114000\n",
      "Gradient: [ -30.131   -42.4368 -139.1998  -63.164   529.5707]\n",
      "Weights: [-4.5355 -0.1217 -0.6772  0.0805  0.1206]\n",
      "MSE loss: 108.5732\n",
      "Iteration: 114100\n",
      "Gradient: [-22.5723  42.851  163.2734 183.5438 -88.8554]\n",
      "Weights: [-4.5348 -0.1209 -0.6776  0.0805  0.1206]\n",
      "MSE loss: 108.5439\n",
      "Iteration: 114200\n",
      "Gradient: [  4.6418 -11.5694  34.8747 109.2498 614.3424]\n",
      "Weights: [-4.535  -0.1204 -0.6779  0.0805  0.1207]\n",
      "MSE loss: 108.5175\n",
      "Iteration: 114300\n",
      "Gradient: [ -15.5994  -19.4073  106.9531  290.4096 -484.085 ]\n",
      "Weights: [-4.5348 -0.1193 -0.6783  0.0804  0.1207]\n",
      "MSE loss: 108.4894\n",
      "Iteration: 114400\n",
      "Gradient: [  -13.0362    40.5532  -104.2108   273.1024 -1167.8359]\n",
      "Weights: [-4.5348 -0.1192 -0.6787  0.0804  0.1207]\n",
      "MSE loss: 108.465\n",
      "Iteration: 114500\n",
      "Gradient: [ -40.0111  -23.9035  -89.5284 -162.9836  517.0364]\n",
      "Weights: [-4.5363 -0.1182 -0.679   0.0804  0.1208]\n",
      "MSE loss: 108.4334\n",
      "Iteration: 114600\n",
      "Gradient: [ -32.7532    5.3396  108.0394 -129.7917 -542.8483]\n",
      "Weights: [-4.5366 -0.1175 -0.6793  0.0804  0.1208]\n",
      "MSE loss: 108.4104\n",
      "Iteration: 114700\n",
      "Gradient: [ -13.9756    8.3071 -134.5747  154.7706 -172.1267]\n",
      "Weights: [-4.5363 -0.1169 -0.6798  0.0804  0.1208]\n",
      "MSE loss: 108.3756\n",
      "Iteration: 114800\n",
      "Gradient: [  4.0653  50.9983  76.1347 -42.0346 906.8722]\n",
      "Weights: [-4.5353 -0.1167 -0.6804  0.0804  0.1208]\n",
      "MSE loss: 108.3454\n",
      "Iteration: 114900\n",
      "Gradient: [ -3.9517 -33.8839 118.8428 118.7097 132.4021]\n",
      "Weights: [-4.5351 -0.116  -0.6806  0.0803  0.1209]\n",
      "MSE loss: 108.3221\n",
      "Iteration: 115000\n",
      "Gradient: [  -3.4717  -26.1423  158.3374    5.1278 -768.3088]\n",
      "Weights: [-4.5354 -0.1159 -0.6809  0.0803  0.1209]\n",
      "MSE loss: 108.2995\n",
      "Iteration: 115100\n",
      "Gradient: [ -31.6591  -17.9124  107.7485  -38.5437 -452.9286]\n",
      "Weights: [-4.5368 -0.1148 -0.6812  0.0802  0.1209]\n",
      "MSE loss: 108.2664\n",
      "Iteration: 115200\n",
      "Gradient: [  25.5233   18.847    35.9385 -174.4438  451.3168]\n",
      "Weights: [-4.537  -0.1138 -0.6816  0.0802  0.121 ]\n",
      "MSE loss: 108.2357\n",
      "Iteration: 115300\n",
      "Gradient: [ 15.7868  26.2122  68.341   10.6598 143.7524]\n",
      "Weights: [-4.5374 -0.1128 -0.6819  0.0802  0.121 ]\n",
      "MSE loss: 108.2116\n",
      "Iteration: 115400\n",
      "Gradient: [  15.7351   42.3471  -73.2924 -307.5508  324.5185]\n",
      "Weights: [-4.5385 -0.112  -0.6822  0.0802  0.121 ]\n",
      "MSE loss: 108.1858\n",
      "Iteration: 115500\n",
      "Gradient: [-55.0724  -8.5883  30.6897 192.8249 352.0884]\n",
      "Weights: [-4.5374 -0.1116 -0.6826  0.0801  0.121 ]\n",
      "MSE loss: 108.1541\n",
      "Iteration: 115600\n",
      "Gradient: [  -2.795    -8.544   -27.9129  187.1154 -371.2957]\n",
      "Weights: [-4.5384 -0.1107 -0.683   0.0801  0.1211]\n",
      "MSE loss: 108.1249\n",
      "Iteration: 115700\n",
      "Gradient: [    4.2134    24.8616   -15.368     76.0889 -1285.0816]\n",
      "Weights: [-4.5371 -0.1107 -0.6833  0.0801  0.1211]\n",
      "MSE loss: 108.1023\n",
      "Iteration: 115800\n",
      "Gradient: [ -40.906    11.4031   85.4941  379.7058 -592.6379]\n",
      "Weights: [-4.537  -0.1098 -0.6837  0.08    0.1211]\n",
      "MSE loss: 108.0753\n",
      "Iteration: 115900\n",
      "Gradient: [   0.9159  -26.2247   49.5968  315.0195 -332.1064]\n",
      "Weights: [-4.5385 -0.1091 -0.6842  0.08    0.1212]\n",
      "MSE loss: 108.0396\n",
      "Iteration: 116000\n",
      "Gradient: [  34.594    17.9004  -33.579   439.7047 -829.8637]\n",
      "Weights: [-4.5387 -0.108  -0.6846  0.08    0.1212]\n",
      "MSE loss: 108.004\n",
      "Iteration: 116100\n",
      "Gradient: [   8.0371   44.3139  -14.4284 -311.4561 -949.3876]\n",
      "Weights: [-4.5385 -0.1075 -0.685   0.08    0.1212]\n",
      "MSE loss: 107.9733\n",
      "Iteration: 116200\n",
      "Gradient: [  -8.5521   -6.2884 -100.9922 -125.4801 -669.7247]\n",
      "Weights: [-4.5397 -0.1065 -0.6854  0.0799  0.1212]\n",
      "MSE loss: 107.9448\n",
      "Iteration: 116300\n",
      "Gradient: [  3.8237   7.0249  65.9302  96.5554 389.5562]\n",
      "Weights: [-4.5395 -0.1058 -0.6858  0.0799  0.1213]\n",
      "MSE loss: 107.9112\n",
      "Iteration: 116400\n",
      "Gradient: [ -23.2223    4.475    18.6539  -49.8108 -661.6835]\n",
      "Weights: [-4.5415 -0.1047 -0.686   0.0798  0.1213]\n",
      "MSE loss: 107.8889\n",
      "Iteration: 116500\n",
      "Gradient: [-1.52290e+01  5.69010e+01 -1.07561e+01  4.46000e-02  8.37310e+01]\n",
      "Weights: [-4.5402 -0.1041 -0.6864  0.0798  0.1213]\n",
      "MSE loss: 107.8606\n",
      "Iteration: 116600\n",
      "Gradient: [  2.341   -6.9918 143.4777  24.6385 622.1236]\n",
      "Weights: [-4.5388 -0.104  -0.6867  0.0798  0.1214]\n",
      "MSE loss: 107.8406\n",
      "Iteration: 116700\n",
      "Gradient: [  18.7757  -22.2202   23.4295  221.5463 -678.8801]\n",
      "Weights: [-4.5389 -0.1042 -0.6872  0.0798  0.1214]\n",
      "MSE loss: 107.8173\n",
      "Iteration: 116800\n",
      "Gradient: [  -7.1746   -9.3943  -19.3705  -84.0561 -527.157 ]\n",
      "Weights: [-4.5401 -0.1031 -0.6876  0.0798  0.1214]\n",
      "MSE loss: 107.7809\n",
      "Iteration: 116900\n",
      "Gradient: [  36.9381   -5.8886   43.4718  137.2175 -619.5762]\n",
      "Weights: [-4.5419 -0.102  -0.688   0.0798  0.1215]\n",
      "MSE loss: 107.746\n",
      "Iteration: 117000\n",
      "Gradient: [ 23.5798  31.3989 -86.4075 -12.9831 254.1877]\n",
      "Weights: [-4.5399 -0.1015 -0.6885  0.0798  0.1215]\n",
      "MSE loss: 107.7135\n",
      "Iteration: 117100\n",
      "Gradient: [  -8.6406  -44.0823  -45.9516 -352.8981 -845.2322]\n",
      "Weights: [-4.5418 -0.1008 -0.6888  0.0798  0.1215]\n",
      "MSE loss: 107.6886\n",
      "Iteration: 117200\n",
      "Gradient: [ -44.9107   -3.8235  -48.6295  -58.8544 -402.7457]\n",
      "Weights: [-4.5389 -0.101  -0.6892  0.0797  0.1215]\n",
      "MSE loss: 107.6695\n",
      "Iteration: 117300\n",
      "Gradient: [ -8.2942  17.8177  68.8714 -86.3423 377.056 ]\n",
      "Weights: [-4.5392 -0.1006 -0.6896  0.0797  0.1216]\n",
      "MSE loss: 107.6412\n",
      "Iteration: 117400\n",
      "Gradient: [  4.3786 -13.6695  93.8615  93.5604 346.0477]\n",
      "Weights: [-4.5394 -0.0997 -0.6901  0.0797  0.1216]\n",
      "MSE loss: 107.6091\n",
      "Iteration: 117500\n",
      "Gradient: [  20.686   -57.4672  166.4349 -332.8926 -634.1847]\n",
      "Weights: [-4.5389 -0.0994 -0.6904  0.0797  0.1216]\n",
      "MSE loss: 107.5855\n",
      "Iteration: 117600\n",
      "Gradient: [  14.4419  -30.7844 -160.6097  324.4066 -200.1235]\n",
      "Weights: [-4.5402 -0.0984 -0.6908  0.0797  0.1217]\n",
      "MSE loss: 107.5514\n",
      "Iteration: 117700\n",
      "Gradient: [ -4.4425   3.0394  37.663  -84.6562 -93.6028]\n",
      "Weights: [-4.5408 -0.0978 -0.6912  0.0797  0.1217]\n",
      "MSE loss: 107.5217\n",
      "Iteration: 117800\n",
      "Gradient: [  -7.3051  -11.9786 -184.7164 -474.7727 -553.0607]\n",
      "Weights: [-4.5408 -0.0976 -0.6914  0.0797  0.1217]\n",
      "MSE loss: 107.5009\n",
      "Iteration: 117900\n",
      "Gradient: [  -3.5998    5.4935  101.65    -31.8795 -488.9611]\n",
      "Weights: [-4.5406 -0.0972 -0.6918  0.0797  0.1218]\n",
      "MSE loss: 107.477\n",
      "Iteration: 118000\n",
      "Gradient: [  36.1051  -25.9913  -44.5587 -264.9788  321.9978]\n",
      "Weights: [-4.5405 -0.0965 -0.6922  0.0797  0.1218]\n",
      "MSE loss: 107.4464\n",
      "Iteration: 118100\n",
      "Gradient: [-13.353  -38.4715 -46.9474 -79.362  -95.964 ]\n",
      "Weights: [-4.5421 -0.0956 -0.6926  0.0796  0.1218]\n",
      "MSE loss: 107.4179\n",
      "Iteration: 118200\n",
      "Gradient: [  20.9051   37.1468   -3.7871   56.4279 -347.5253]\n",
      "Weights: [-4.5424 -0.0944 -0.6929  0.0796  0.1219]\n",
      "MSE loss: 107.386\n",
      "Iteration: 118300\n",
      "Gradient: [   5.4429  -26.5546  -41.8081  -57.0913 -631.9247]\n",
      "Weights: [-4.5427 -0.0937 -0.6932  0.0796  0.1219]\n",
      "MSE loss: 107.3594\n",
      "Iteration: 118400\n",
      "Gradient: [-19.8952 -38.8435 -32.4509 131.5128 268.6916]\n",
      "Weights: [-4.5435 -0.0932 -0.6936  0.0796  0.1219]\n",
      "MSE loss: 107.336\n",
      "Iteration: 118500\n",
      "Gradient: [ 6.994000e-01 -7.222300e+00 -5.293620e+01  3.345273e+02  7.917895e+02]\n",
      "Weights: [-4.5419 -0.0928 -0.6939  0.0795  0.1219]\n",
      "MSE loss: 107.3164\n",
      "Iteration: 118600\n",
      "Gradient: [  27.8082   -7.6384   11.3828 -393.148  -794.4685]\n",
      "Weights: [-4.5415 -0.0929 -0.6943  0.0795  0.122 ]\n",
      "MSE loss: 107.2945\n",
      "Iteration: 118700\n",
      "Gradient: [ -14.6389    0.8286   34.9413  -26.5815 -352.7137]\n",
      "Weights: [-4.5441 -0.0923 -0.6945  0.0795  0.122 ]\n",
      "MSE loss: 107.2756\n",
      "Iteration: 118800\n",
      "Gradient: [   8.4107  -13.85     -9.4932 -209.3719 -545.8586]\n",
      "Weights: [-4.5439 -0.092  -0.6948  0.0795  0.122 ]\n",
      "MSE loss: 107.2504\n",
      "Iteration: 118900\n",
      "Gradient: [   5.239    -2.0284   81.8011  105.455  -523.6998]\n",
      "Weights: [-4.5447 -0.0907 -0.6952  0.0795  0.1221]\n",
      "MSE loss: 107.2151\n",
      "Iteration: 119000\n",
      "Gradient: [   1.5066   -2.1401    7.3123  -51.3225 1082.6615]\n",
      "Weights: [-4.545  -0.0889 -0.6957  0.0794  0.1221]\n",
      "MSE loss: 107.1759\n",
      "Iteration: 119100\n",
      "Gradient: [ 20.013  -23.2293 105.3804  56.326  762.2848]\n",
      "Weights: [-4.5439 -0.0883 -0.696   0.0794  0.1221]\n",
      "MSE loss: 107.1527\n",
      "Iteration: 119200\n",
      "Gradient: [ -1.106   29.0758 130.8298  34.5717 437.6515]\n",
      "Weights: [-4.5452 -0.0875 -0.6963  0.0793  0.1221]\n",
      "MSE loss: 107.1244\n",
      "Iteration: 119300\n",
      "Gradient: [11.6475 21.6593 37.3797 15.2804 94.6266]\n",
      "Weights: [-4.5448 -0.087  -0.6969  0.0793  0.1222]\n",
      "MSE loss: 107.0918\n",
      "Iteration: 119400\n",
      "Gradient: [ -10.6909   11.3337  116.9259   88.7861 -116.8625]\n",
      "Weights: [-4.545  -0.086  -0.6973  0.0793  0.1222]\n",
      "MSE loss: 107.0618\n",
      "Iteration: 119500\n",
      "Gradient: [-34.4485 -20.8438 -59.3487 -59.025  229.4067]\n",
      "Weights: [-4.5457 -0.0855 -0.6978  0.0793  0.1222]\n",
      "MSE loss: 107.0336\n",
      "Iteration: 119600\n",
      "Gradient: [   5.721     4.2488  116.8552 -160.1147 -213.1239]\n",
      "Weights: [-4.5454 -0.0841 -0.6982  0.0793  0.1222]\n",
      "MSE loss: 107.002\n",
      "Iteration: 119700\n",
      "Gradient: [  10.156    41.5608   84.3297 -123.5952  504.6621]\n",
      "Weights: [-4.5455 -0.0831 -0.6987  0.0792  0.1222]\n",
      "MSE loss: 106.9706\n",
      "Iteration: 119800\n",
      "Gradient: [ -11.6831   14.1537   48.8265 -180.0794 -795.9743]\n",
      "Weights: [-4.5471 -0.0825 -0.6989  0.0792  0.1223]\n",
      "MSE loss: 106.9478\n",
      "Iteration: 119900\n",
      "Gradient: [-40.9669  10.5182 160.6899 296.8638  25.0351]\n",
      "Weights: [-4.5479 -0.0815 -0.6994  0.0792  0.1223]\n",
      "MSE loss: 106.9161\n",
      "Iteration: 120000\n",
      "Gradient: [ 18.7421  13.8362 104.3485 468.0818 243.7803]\n",
      "Weights: [-4.5474 -0.0808 -0.6998  0.0792  0.1223]\n",
      "MSE loss: 106.8892\n",
      "Iteration: 120100\n",
      "Gradient: [    1.6403    24.5836    28.2883  -163.1066 -1116.4462]\n",
      "Weights: [-4.5479 -0.08   -0.7001  0.0792  0.1224]\n",
      "MSE loss: 106.862\n",
      "Iteration: 120200\n",
      "Gradient: [ -3.0682  52.8951 124.5203 561.6529 -34.1964]\n",
      "Weights: [-4.5476 -0.0796 -0.7004  0.0791  0.1224]\n",
      "MSE loss: 106.8375\n",
      "Iteration: 120300\n",
      "Gradient: [  -1.4967    0.3996  -97.3156 -173.3492 -368.3902]\n",
      "Weights: [-4.5495 -0.0789 -0.7008  0.0791  0.1224]\n",
      "MSE loss: 106.8162\n",
      "Iteration: 120400\n",
      "Gradient: [  2.8899 -10.6388 -14.6078 626.5449 349.922 ]\n",
      "Weights: [-4.55   -0.0781 -0.701   0.0792  0.1224]\n",
      "MSE loss: 106.7964\n",
      "Iteration: 120500\n",
      "Gradient: [  13.29     21.1247   48.9116   94.8964 -963.7288]\n",
      "Weights: [-4.5497 -0.0773 -0.7014  0.0791  0.1224]\n",
      "MSE loss: 106.7686\n",
      "Iteration: 120600\n",
      "Gradient: [ 2.857000e-01 -2.502840e+01 -4.894310e+01 -8.310750e+01 -6.166558e+02]\n",
      "Weights: [-4.5494 -0.0768 -0.7018  0.0791  0.1225]\n",
      "MSE loss: 106.7445\n",
      "Iteration: 120700\n",
      "Gradient: [   4.3424  -35.5886   33.683   188.8842 -757.0509]\n",
      "Weights: [-4.549  -0.0765 -0.7021  0.0791  0.1225]\n",
      "MSE loss: 106.7204\n",
      "Iteration: 120800\n",
      "Gradient: [   8.0348  -10.5279   -0.5479 -366.4803 -294.5059]\n",
      "Weights: [-4.5497 -0.0759 -0.7025  0.0791  0.1225]\n",
      "MSE loss: 106.6923\n",
      "Iteration: 120900\n",
      "Gradient: [   1.2341   27.3083  -91.2083    0.9462 -525.5378]\n",
      "Weights: [-4.5512 -0.0746 -0.7029  0.0791  0.1226]\n",
      "MSE loss: 106.6638\n",
      "Iteration: 121000\n",
      "Gradient: [-15.5796 -57.9451 189.6596  42.6387 818.6985]\n",
      "Weights: [-4.5515 -0.0743 -0.7032  0.0791  0.1226]\n",
      "MSE loss: 106.6426\n",
      "Iteration: 121100\n",
      "Gradient: [  11.8087   12.4901   60.064  -105.0009   76.8078]\n",
      "Weights: [-4.5499 -0.0736 -0.7037  0.0791  0.1226]\n",
      "MSE loss: 106.6105\n",
      "Iteration: 121200\n",
      "Gradient: [  25.4678   76.2658  -60.3604  -33.6929 -507.2337]\n",
      "Weights: [-4.5497 -0.0733 -0.7041  0.079   0.1227]\n",
      "MSE loss: 106.5827\n",
      "Iteration: 121300\n",
      "Gradient: [   8.5574   42.4755   65.431  -115.2484 -652.0269]\n",
      "Weights: [-4.5491 -0.073  -0.7044  0.079   0.1227]\n",
      "MSE loss: 106.5621\n",
      "Iteration: 121400\n",
      "Gradient: [   3.3816    0.6334  -95.2453   44.4867 -103.4795]\n",
      "Weights: [-4.5491 -0.0722 -0.7047  0.079   0.1227]\n",
      "MSE loss: 106.5394\n",
      "Iteration: 121500\n",
      "Gradient: [-31.3373 -39.3643  93.6117 -60.1605 104.4452]\n",
      "Weights: [-4.5489 -0.0718 -0.7051  0.0789  0.1227]\n",
      "MSE loss: 106.5099\n",
      "Iteration: 121600\n",
      "Gradient: [ -36.7587  -40.4656   90.3279  171.4003 -150.7171]\n",
      "Weights: [-4.5502 -0.0709 -0.7056  0.0789  0.1227]\n",
      "MSE loss: 106.4826\n",
      "Iteration: 121700\n",
      "Gradient: [  16.6293   35.2316   92.7296 -110.557  -424.0244]\n",
      "Weights: [-4.5511 -0.0691 -0.706   0.0789  0.1228]\n",
      "MSE loss: 106.4462\n",
      "Iteration: 121800\n",
      "Gradient: [ 2.997000e-01 -2.577690e+01  3.422950e+01 -2.523300e+01 -3.654474e+02]\n",
      "Weights: [-4.5518 -0.0686 -0.7064  0.0789  0.1228]\n",
      "MSE loss: 106.4201\n",
      "Iteration: 121900\n",
      "Gradient: [  14.9437   28.462   127.6119   90.7314 -515.8252]\n",
      "Weights: [-4.5524 -0.0677 -0.7067  0.0789  0.1228]\n",
      "MSE loss: 106.3969\n",
      "Iteration: 122000\n",
      "Gradient: [  18.4979  -30.835   -46.03    123.6918 -123.7297]\n",
      "Weights: [-4.5526 -0.0673 -0.7069  0.0789  0.1228]\n",
      "MSE loss: 106.3786\n",
      "Iteration: 122100\n",
      "Gradient: [   1.8753  -53.0902 -140.0325  173.0751   87.2526]\n",
      "Weights: [-4.5525 -0.0669 -0.7073  0.0788  0.1229]\n",
      "MSE loss: 106.3537\n",
      "Iteration: 122200\n",
      "Gradient: [ 14.5208 -32.4466 125.8436 -56.1273 271.7699]\n",
      "Weights: [-4.5527 -0.0661 -0.7078  0.0788  0.1229]\n",
      "MSE loss: 106.3225\n",
      "Iteration: 122300\n",
      "Gradient: [  22.5363   -0.7261  -19.5397   84.2452 -228.6824]\n",
      "Weights: [-4.5534 -0.0649 -0.7082  0.0788  0.1229]\n",
      "MSE loss: 106.2909\n",
      "Iteration: 122400\n",
      "Gradient: [-11.5142 -45.9722 193.816    7.3306 -76.8964]\n",
      "Weights: [-4.5543 -0.0639 -0.7086  0.0788  0.123 ]\n",
      "MSE loss: 106.2619\n",
      "Iteration: 122500\n",
      "Gradient: [  58.4743  -82.503    91.757  -201.8425 -281.6649]\n",
      "Weights: [-4.5532 -0.0636 -0.7091  0.0788  0.123 ]\n",
      "MSE loss: 106.2339\n",
      "Iteration: 122600\n",
      "Gradient: [ -15.2637   32.8218   20.5655 -162.2866  539.2085]\n",
      "Weights: [-4.5522 -0.0629 -0.7096  0.0788  0.123 ]\n",
      "MSE loss: 106.2042\n",
      "Iteration: 122700\n",
      "Gradient: [   8.1773  -34.4673  -51.352   -41.9679 -478.6521]\n",
      "Weights: [-4.5538 -0.0628 -0.7099  0.0788  0.1231]\n",
      "MSE loss: 106.1808\n",
      "Iteration: 122800\n",
      "Gradient: [ -38.8099   52.0305   33.6112  -10.4923 -354.0645]\n",
      "Weights: [-4.554  -0.0621 -0.7103  0.0788  0.1231]\n",
      "MSE loss: 106.1529\n",
      "Iteration: 122900\n",
      "Gradient: [ -34.1676   -5.7107   36.1048 -173.6519  317.3421]\n",
      "Weights: [-4.5531 -0.0618 -0.7107  0.0788  0.1231]\n",
      "MSE loss: 106.1288\n",
      "Iteration: 123000\n",
      "Gradient: [ -25.6991  -51.0936  -38.2473 -203.5981 -276.775 ]\n",
      "Weights: [-4.5534 -0.0615 -0.7111  0.0788  0.1231]\n",
      "MSE loss: 106.1042\n",
      "Iteration: 123100\n",
      "Gradient: [  14.1123   -1.5369  -46.9912   74.9638 -353.0152]\n",
      "Weights: [-4.5528 -0.061  -0.7114  0.0788  0.1232]\n",
      "MSE loss: 106.082\n",
      "Iteration: 123200\n",
      "Gradient: [ -22.0229  -14.481    37.0797 -102.0307  185.0858]\n",
      "Weights: [-4.5526 -0.0604 -0.7117  0.0788  0.1232]\n",
      "MSE loss: 106.0604\n",
      "Iteration: 123300\n",
      "Gradient: [ 0.968   9.2038 -4.1726 31.5248 46.1632]\n",
      "Weights: [-4.5536 -0.0596 -0.7122  0.0787  0.1232]\n",
      "MSE loss: 106.0312\n",
      "Iteration: 123400\n",
      "Gradient: [  25.1397   57.3572  135.8142 -274.3922 -419.0656]\n",
      "Weights: [-4.553  -0.0592 -0.7125  0.0788  0.1233]\n",
      "MSE loss: 106.0081\n",
      "Iteration: 123500\n",
      "Gradient: [  16.1043   36.1358   90.0516  141.7901 -394.8342]\n",
      "Weights: [-4.553  -0.0591 -0.7129  0.0787  0.1233]\n",
      "MSE loss: 105.9856\n",
      "Iteration: 123600\n",
      "Gradient: [  -0.3596  -14.3109   50.9338   20.8654 -224.4199]\n",
      "Weights: [-4.5528 -0.0583 -0.7133  0.0787  0.1233]\n",
      "MSE loss: 105.9552\n",
      "Iteration: 123700\n",
      "Gradient: [-30.6536 -48.7773 -88.6358  -8.8949 -85.0513]\n",
      "Weights: [-4.554  -0.0576 -0.7135  0.0787  0.1233]\n",
      "MSE loss: 105.9369\n",
      "Iteration: 123800\n",
      "Gradient: [-28.8071 -50.2861  49.5914 184.5388  45.2691]\n",
      "Weights: [-4.5544 -0.0571 -0.7139  0.0787  0.1234]\n",
      "MSE loss: 105.9103\n",
      "Iteration: 123900\n",
      "Gradient: [  29.8944  -49.9144   78.9707 -318.192  -729.767 ]\n",
      "Weights: [-4.5557 -0.056  -0.7142  0.0787  0.1234]\n",
      "MSE loss: 105.8835\n",
      "Iteration: 124000\n",
      "Gradient: [  -4.1477   -7.0705   18.823  -290.9795 -614.1452]\n",
      "Weights: [-4.5562 -0.0549 -0.7146  0.0787  0.1234]\n",
      "MSE loss: 105.8545\n",
      "Iteration: 124100\n",
      "Gradient: [  19.2386   35.071  -127.8326  261.9419  206.3157]\n",
      "Weights: [-4.5555 -0.0547 -0.7149  0.0786  0.1234]\n",
      "MSE loss: 105.8347\n",
      "Iteration: 124200\n",
      "Gradient: [  15.3994  -14.8198   25.0418  150.1899 -157.4352]\n",
      "Weights: [-4.5554 -0.0539 -0.7154  0.0786  0.1235]\n",
      "MSE loss: 105.8073\n",
      "Iteration: 124300\n",
      "Gradient: [   6.7494   45.9171   57.2254   24.1072 -642.0232]\n",
      "Weights: [-4.5565 -0.0531 -0.7158  0.0786  0.1235]\n",
      "MSE loss: 105.7808\n",
      "Iteration: 124400\n",
      "Gradient: [-2.423330e+01  5.386000e-01  4.370400e+01  7.292815e+02  2.961228e+02]\n",
      "Weights: [-4.5588 -0.0518 -0.7161  0.0786  0.1235]\n",
      "MSE loss: 105.7548\n",
      "Iteration: 124500\n",
      "Gradient: [   1.2982  -22.5347   94.008   244.376  -713.6837]\n",
      "Weights: [-4.5576 -0.0514 -0.7164  0.0786  0.1235]\n",
      "MSE loss: 105.7309\n",
      "Iteration: 124600\n",
      "Gradient: [    9.6645   -48.3072    88.5877    29.1438 -1126.4021]\n",
      "Weights: [-4.5557 -0.0515 -0.7168  0.0786  0.1236]\n",
      "MSE loss: 105.7098\n",
      "Iteration: 124700\n",
      "Gradient: [  23.0861   64.7127   22.3181 -447.181   -28.0035]\n",
      "Weights: [-4.555  -0.0509 -0.7173  0.0785  0.1236]\n",
      "MSE loss: 105.6797\n",
      "Iteration: 124800\n",
      "Gradient: [   7.8561  -90.0837   81.8472  299.4788 -261.9899]\n",
      "Weights: [-4.5554 -0.0502 -0.7177  0.0785  0.1236]\n",
      "MSE loss: 105.6557\n",
      "Iteration: 124900\n",
      "Gradient: [ -19.2329   51.2097  -25.5522 -276.8984 -412.2689]\n",
      "Weights: [-4.5561 -0.0496 -0.7181  0.0785  0.1237]\n",
      "MSE loss: 105.6284\n",
      "Iteration: 125000\n",
      "Gradient: [-28.1065 -37.5929   6.7719  76.5655  64.9315]\n",
      "Weights: [-4.5547 -0.0493 -0.7184  0.0785  0.1237]\n",
      "MSE loss: 105.6136\n",
      "Iteration: 125100\n",
      "Gradient: [ -40.3423   41.6133   40.4942  121.7107 -837.3111]\n",
      "Weights: [-4.557  -0.0481 -0.7187  0.0785  0.1237]\n",
      "MSE loss: 105.5862\n",
      "Iteration: 125200\n",
      "Gradient: [ -26.9167  -43.3678 -158.0696  243.4637  242.3503]\n",
      "Weights: [-4.5566 -0.0474 -0.719   0.0785  0.1237]\n",
      "MSE loss: 105.5675\n",
      "Iteration: 125300\n",
      "Gradient: [  -3.9267   28.7088    4.3411   41.5987 -719.3685]\n",
      "Weights: [-4.5599 -0.0465 -0.7194  0.0785  0.1237]\n",
      "MSE loss: 105.5446\n",
      "Iteration: 125400\n",
      "Gradient: [   3.7888  -50.168   240.5032  -82.78   -302.9393]\n",
      "Weights: [-4.5582 -0.0457 -0.7198  0.0785  0.1238]\n",
      "MSE loss: 105.5094\n",
      "Iteration: 125500\n",
      "Gradient: [ -23.3057  -14.5364   -5.5613 -295.6983 -419.6675]\n",
      "Weights: [-4.5591 -0.045  -0.7203  0.0784  0.1238]\n",
      "MSE loss: 105.4803\n",
      "Iteration: 125600\n",
      "Gradient: [ 22.1698   4.3111  19.9351  27.598  779.9855]\n",
      "Weights: [-4.5579 -0.0444 -0.7205  0.0785  0.1238]\n",
      "MSE loss: 105.4627\n",
      "Iteration: 125700\n",
      "Gradient: [   5.7508  -56.4697  -13.5202  325.255  -134.4916]\n",
      "Weights: [-4.5578 -0.0443 -0.7209  0.0784  0.1238]\n",
      "MSE loss: 105.441\n",
      "Iteration: 125800\n",
      "Gradient: [-13.276  -58.0237 -76.9447 237.2308 175.0377]\n",
      "Weights: [-4.559  -0.0434 -0.7211  0.0784  0.1239]\n",
      "MSE loss: 105.4201\n",
      "Iteration: 125900\n",
      "Gradient: [-2.760000e-01  4.058090e+01  3.328789e+02 -3.452208e+02 -3.417330e+02]\n",
      "Weights: [-4.5583 -0.0428 -0.7216  0.0784  0.1239]\n",
      "MSE loss: 105.3939\n",
      "Iteration: 126000\n",
      "Gradient: [  11.1259   63.6869   50.7185  -96.668  -571.0464]\n",
      "Weights: [-4.5595 -0.0425 -0.7218  0.0784  0.1239]\n",
      "MSE loss: 105.3732\n",
      "Iteration: 126100\n",
      "Gradient: [   3.1871   58.6715  106.489   105.2575 -554.0988]\n",
      "Weights: [-4.5594 -0.0412 -0.7223  0.0784  0.124 ]\n",
      "MSE loss: 105.3443\n",
      "Iteration: 126200\n",
      "Gradient: [  -9.6306   -2.4099  -52.895   196.3555 -300.7867]\n",
      "Weights: [-4.56   -0.0407 -0.7227  0.0783  0.124 ]\n",
      "MSE loss: 105.3142\n",
      "Iteration: 126300\n",
      "Gradient: [ 2.008330e+01 -7.071000e-01  1.676407e+02  1.783270e+02 -7.837069e+02]\n",
      "Weights: [-4.5594 -0.0405 -0.723   0.0783  0.124 ]\n",
      "MSE loss: 105.295\n",
      "Iteration: 126400\n",
      "Gradient: [ -21.0697   42.3244   18.8056 -207.6266 -313.6238]\n",
      "Weights: [-4.5614 -0.0394 -0.7233  0.0783  0.124 ]\n",
      "MSE loss: 105.271\n",
      "Iteration: 126500\n",
      "Gradient: [  26.4516    6.9483 -166.8985  -36.4334 -263.146 ]\n",
      "Weights: [-4.5606 -0.0386 -0.7237  0.0783  0.1241]\n",
      "MSE loss: 105.2456\n",
      "Iteration: 126600\n",
      "Gradient: [   24.7266    45.1427    96.3388   176.5186 -1213.5035]\n",
      "Weights: [-4.5618 -0.0381 -0.724   0.0782  0.1241]\n",
      "MSE loss: 105.2267\n",
      "Iteration: 126700\n",
      "Gradient: [  29.1659  -11.8487 -139.8782  216.0816 -258.4322]\n",
      "Weights: [-4.56   -0.0375 -0.7243  0.0782  0.1241]\n",
      "MSE loss: 105.2026\n",
      "Iteration: 126800\n",
      "Gradient: [  -5.3056  -45.3424  120.9998 -135.9449 -890.795 ]\n",
      "Weights: [-4.5602 -0.037  -0.7247  0.0782  0.1241]\n",
      "MSE loss: 105.1766\n",
      "Iteration: 126900\n",
      "Gradient: [ -17.2987  -65.6573   24.2067  -85.7271 -135.933 ]\n",
      "Weights: [-4.5616 -0.0365 -0.725   0.0782  0.1242]\n",
      "MSE loss: 105.1549\n",
      "Iteration: 127000\n",
      "Gradient: [  10.4316   12.875   218.2446   -1.2988 -411.2943]\n",
      "Weights: [-4.5605 -0.0357 -0.7254  0.0781  0.1242]\n",
      "MSE loss: 105.1284\n",
      "Iteration: 127100\n",
      "Gradient: [   8.655    10.8738   55.011     6.7906 -249.2197]\n",
      "Weights: [-4.561  -0.0352 -0.7257  0.0781  0.1242]\n",
      "MSE loss: 105.1074\n",
      "Iteration: 127200\n",
      "Gradient: [   3.5806   -1.1031  166.8597 -106.4281  681.9556]\n",
      "Weights: [-4.561  -0.0348 -0.726   0.0781  0.1242]\n",
      "MSE loss: 105.0851\n",
      "Iteration: 127300\n",
      "Gradient: [    2.7877   -58.7214   154.7859    -9.6778 -1275.0488]\n",
      "Weights: [-4.5603 -0.0343 -0.7264  0.078   0.1243]\n",
      "MSE loss: 105.0574\n",
      "Iteration: 127400\n",
      "Gradient: [  14.9712  -60.3146  118.0435  226.0548 -119.2864]\n",
      "Weights: [-4.5607 -0.0337 -0.7268  0.078   0.1243]\n",
      "MSE loss: 105.0354\n",
      "Iteration: 127500\n",
      "Gradient: [  6.139  -26.8499  81.3217 -16.3423 576.7424]\n",
      "Weights: [-4.5606 -0.033  -0.7272  0.078   0.1243]\n",
      "MSE loss: 105.0128\n",
      "Iteration: 127600\n",
      "Gradient: [  -5.8973   -4.2066  157.2224   68.213  -398.6177]\n",
      "Weights: [-4.5622 -0.032  -0.7274  0.078   0.1244]\n",
      "MSE loss: 104.9866\n",
      "Iteration: 127700\n",
      "Gradient: [   10.175     -8.4559    66.5007   -28.9017 -1077.7442]\n",
      "Weights: [-4.5601 -0.0323 -0.7278  0.078   0.1244]\n",
      "MSE loss: 104.9702\n",
      "Iteration: 127800\n",
      "Gradient: [  33.0783  -13.0265  120.959   -84.1222 -636.1039]\n",
      "Weights: [-4.5615 -0.0312 -0.7281  0.0779  0.1244]\n",
      "MSE loss: 104.9393\n",
      "Iteration: 127900\n",
      "Gradient: [  -2.569   -20.853    64.3663  -39.3732 -703.2402]\n",
      "Weights: [-4.561  -0.0311 -0.7285  0.0779  0.1244]\n",
      "MSE loss: 104.9217\n",
      "Iteration: 128000\n",
      "Gradient: [ -2.0134   1.2585 111.1716 172.2742 519.8662]\n",
      "Weights: [-4.5622 -0.0304 -0.7288  0.078   0.1245]\n",
      "MSE loss: 104.8944\n",
      "Iteration: 128100\n",
      "Gradient: [   9.8841   13.8029  101.0531   69.9868 -142.9273]\n",
      "Weights: [-4.5625 -0.0298 -0.729   0.0779  0.1245]\n",
      "MSE loss: 104.8764\n",
      "Iteration: 128200\n",
      "Gradient: [  -9.4981  -15.7973   -1.9033  284.0571 -411.0405]\n",
      "Weights: [-4.5633 -0.0288 -0.7295  0.0779  0.1245]\n",
      "MSE loss: 104.8466\n",
      "Iteration: 128300\n",
      "Gradient: [  5.3612  83.8045 217.015  116.4512  -4.8543]\n",
      "Weights: [-4.5632 -0.0282 -0.7298  0.0779  0.1245]\n",
      "MSE loss: 104.8285\n",
      "Iteration: 128400\n",
      "Gradient: [-43.3042 -32.5376  43.5902  62.614   89.1209]\n",
      "Weights: [-4.5644 -0.0274 -0.7301  0.0779  0.1246]\n",
      "MSE loss: 104.8039\n",
      "Iteration: 128500\n",
      "Gradient: [  17.1399   -9.5381   -5.1446  -80.5908 -439.1784]\n",
      "Weights: [-4.564  -0.0264 -0.7304  0.0778  0.1246]\n",
      "MSE loss: 104.781\n",
      "Iteration: 128600\n",
      "Gradient: [  22.7942  -20.9492  -26.5699  -55.9562 -278.9959]\n",
      "Weights: [-4.5645 -0.0258 -0.7307  0.0778  0.1246]\n",
      "MSE loss: 104.7617\n",
      "Iteration: 128700\n",
      "Gradient: [-31.2951  30.9259 112.1601 -42.301  313.0406]\n",
      "Weights: [-4.5644 -0.0253 -0.731   0.0777  0.1246]\n",
      "MSE loss: 104.7377\n",
      "Iteration: 128800\n",
      "Gradient: [ 8.2678000e+00 -4.3640000e-01  1.0287500e+02 -8.0036100e+01\n",
      " -1.5918238e+03]\n",
      "Weights: [-4.5652 -0.0243 -0.7314  0.0778  0.1247]\n",
      "MSE loss: 104.7142\n",
      "Iteration: 128900\n",
      "Gradient: [  -8.3264   55.6022  119.4707 -105.9738  683.8233]\n",
      "Weights: [-4.5652 -0.0238 -0.7317  0.0777  0.1247]\n",
      "MSE loss: 104.6934\n",
      "Iteration: 129000\n",
      "Gradient: [  33.2107   -7.8192   94.4003  238.3813 -334.9523]\n",
      "Weights: [-4.5654 -0.0234 -0.7321  0.0777  0.1247]\n",
      "MSE loss: 104.6716\n",
      "Iteration: 129100\n",
      "Gradient: [-5.9810000e-01  9.6282000e+00 -8.3764700e+01  3.7564200e+01\n",
      " -1.1539409e+03]\n",
      "Weights: [-4.5661 -0.023  -0.7324  0.0777  0.1247]\n",
      "MSE loss: 104.6553\n",
      "Iteration: 129200\n",
      "Gradient: [   5.6156   33.2489  -30.6151  192.1477 -832.4202]\n",
      "Weights: [-4.5643 -0.0226 -0.7327  0.0777  0.1248]\n",
      "MSE loss: 104.6323\n",
      "Iteration: 129300\n",
      "Gradient: [  16.3279   63.0579  -25.2164  341.1611 -571.3641]\n",
      "Weights: [-4.565  -0.0218 -0.733   0.0776  0.1248]\n",
      "MSE loss: 104.609\n",
      "Iteration: 129400\n",
      "Gradient: [  22.5666  -23.0944  196.4302  117.6231 -846.4728]\n",
      "Weights: [-4.5668 -0.0213 -0.7333  0.0776  0.1248]\n",
      "MSE loss: 104.5887\n",
      "Iteration: 129500\n",
      "Gradient: [  18.8217    0.8729   12.3213 -446.4173   26.0921]\n",
      "Weights: [-4.566  -0.0207 -0.7335  0.0776  0.1248]\n",
      "MSE loss: 104.5688\n",
      "Iteration: 129600\n",
      "Gradient: [  -1.9511   -3.995    40.1903 -148.2865 -966.0289]\n",
      "Weights: [-4.5671 -0.0197 -0.7339  0.0775  0.1249]\n",
      "MSE loss: 104.5422\n",
      "Iteration: 129700\n",
      "Gradient: [   8.8248   16.6165   57.3415 -287.4747   81.6368]\n",
      "Weights: [-4.5657 -0.0189 -0.7344  0.0775  0.1249]\n",
      "MSE loss: 104.5133\n",
      "Iteration: 129800\n",
      "Gradient: [  -2.7311   18.2423  -89.1565   68.4178 -353.4642]\n",
      "Weights: [-4.5663 -0.0182 -0.7348  0.0775  0.1249]\n",
      "MSE loss: 104.4888\n",
      "Iteration: 129900\n",
      "Gradient: [   7.5505   23.1274  -25.1467   -6.0214 -720.5665]\n",
      "Weights: [-4.567  -0.0179 -0.7351  0.0775  0.1249]\n",
      "MSE loss: 104.4667\n",
      "Iteration: 130000\n",
      "Gradient: [  29.8272   13.0908 -111.1375  140.3398 -215.2924]\n",
      "Weights: [-4.5663 -0.0175 -0.7354  0.0775  0.125 ]\n",
      "MSE loss: 104.4456\n",
      "Iteration: 130100\n",
      "Gradient: [  41.967    24.3698  -20.8467   -5.8454 -369.5487]\n",
      "Weights: [-4.5661 -0.0172 -0.7357  0.0775  0.125 ]\n",
      "MSE loss: 104.4273\n",
      "Iteration: 130200\n",
      "Gradient: [ -28.256    10.3001 -116.4173  129.386   -54.3356]\n",
      "Weights: [-4.5674 -0.0162 -0.7361  0.0775  0.125 ]\n",
      "MSE loss: 104.403\n",
      "Iteration: 130300\n",
      "Gradient: [  2.9347  75.72   126.9916 -79.8419 124.2572]\n",
      "Weights: [-4.5678 -0.0156 -0.7364  0.0775  0.1251]\n",
      "MSE loss: 104.3771\n",
      "Iteration: 130400\n",
      "Gradient: [  36.4424   -5.8675  -50.5462  166.5308 -696.0797]\n",
      "Weights: [-4.5678 -0.0144 -0.7368  0.0774  0.1251]\n",
      "MSE loss: 104.3502\n",
      "Iteration: 130500\n",
      "Gradient: [  -9.5014    6.5712   42.223  -343.534   -70.7343]\n",
      "Weights: [-4.5671 -0.0141 -0.7373  0.0774  0.1251]\n",
      "MSE loss: 104.3246\n",
      "Iteration: 130600\n",
      "Gradient: [  1.7252  10.4241 129.4091 284.41   216.7313]\n",
      "Weights: [-4.567  -0.0137 -0.7377  0.0774  0.1251]\n",
      "MSE loss: 104.2988\n",
      "Iteration: 130700\n",
      "Gradient: [  12.5789  -72.6396   93.2488  185.4735 -472.5112]\n",
      "Weights: [-4.5673 -0.013  -0.7382  0.0774  0.1252]\n",
      "MSE loss: 104.2683\n",
      "Iteration: 130800\n",
      "Gradient: [ 46.8868 -16.404    2.8031 168.0811 725.6034]\n",
      "Weights: [-4.5663 -0.0127 -0.7385  0.0774  0.1252]\n",
      "MSE loss: 104.2532\n",
      "Iteration: 130900\n",
      "Gradient: [ -26.984   -68.4738  110.0977  -66.9728 -401.3835]\n",
      "Weights: [-4.5691 -0.0113 -0.7389  0.0773  0.1252]\n",
      "MSE loss: 104.2186\n",
      "Iteration: 131000\n",
      "Gradient: [  2.3152   1.1051  43.7436 -23.6963 244.5451]\n",
      "Weights: [-4.5696 -0.0101 -0.7392  0.0773  0.1253]\n",
      "MSE loss: 104.1904\n",
      "Iteration: 131100\n",
      "Gradient: [  25.453    30.277    87.3599  329.2878 -729.2648]\n",
      "Weights: [-4.5689 -0.0096 -0.7396  0.0773  0.1253]\n",
      "MSE loss: 104.1695\n",
      "Iteration: 131200\n",
      "Gradient: [  6.0085  -2.5298 -23.1664 223.5124  85.9429]\n",
      "Weights: [-4.5695 -0.0092 -0.7399  0.0773  0.1253]\n",
      "MSE loss: 104.1529\n",
      "Iteration: 131300\n",
      "Gradient: [ -13.2818  -56.7862  -42.6718  129.9859 -155.661 ]\n",
      "Weights: [-4.5706 -0.0086 -0.7403  0.0773  0.1253]\n",
      "MSE loss: 104.1317\n",
      "Iteration: 131400\n",
      "Gradient: [  -2.3341   15.7098  165.4344 -244.758   308.6813]\n",
      "Weights: [-4.5696 -0.0078 -0.7406  0.0773  0.1254]\n",
      "MSE loss: 104.1112\n",
      "Iteration: 131500\n",
      "Gradient: [  2.97    28.0357  83.2659 -42.2173  99.9357]\n",
      "Weights: [-4.5696 -0.0076 -0.741   0.0773  0.1254]\n",
      "MSE loss: 104.0896\n",
      "Iteration: 131600\n",
      "Gradient: [  4.0283 -11.8441 172.3716 -68.8359 472.9588]\n",
      "Weights: [-4.5695 -0.0072 -0.7413  0.0772  0.1254]\n",
      "MSE loss: 104.0703\n",
      "Iteration: 131700\n",
      "Gradient: [  18.4057  -17.2897   92.5375 -141.9413 -643.0903]\n",
      "Weights: [-4.5706 -0.0065 -0.7416  0.0772  0.1254]\n",
      "MSE loss: 104.0493\n",
      "Iteration: 131800\n",
      "Gradient: [ -0.4679  13.3524 -13.6251 114.3472 -20.1968]\n",
      "Weights: [-4.5709 -0.0054 -0.7418  0.0772  0.1254]\n",
      "MSE loss: 104.0296\n",
      "Iteration: 131900\n",
      "Gradient: [ 1.7858400e+01  1.3858000e+00  6.7738800e+01  9.1786400e+01\n",
      " -1.6785174e+03]\n",
      "Weights: [-4.5701 -0.0051 -0.7421  0.0772  0.1255]\n",
      "MSE loss: 104.0149\n",
      "Iteration: 132000\n",
      "Gradient: [  -0.4983  -86.9891  117.9966  315.0771 -181.0743]\n",
      "Weights: [-4.5711e+00 -4.4000e-03 -7.4240e-01  7.7100e-02  1.2550e-01]\n",
      "MSE loss: 103.9881\n",
      "Iteration: 132100\n",
      "Gradient: [  2.8527   2.0511  14.0096  16.2824 -81.4201]\n",
      "Weights: [-4.5719e+00 -3.9000e-03 -7.4270e-01  7.7100e-02  1.2550e-01]\n",
      "MSE loss: 103.9698\n",
      "Iteration: 132200\n",
      "Gradient: [   7.4142   42.9806   -2.9423  -90.1484 -614.9996]\n",
      "Weights: [-4.5712e+00 -3.3000e-03 -7.4320e-01  7.7100e-02  1.2550e-01]\n",
      "MSE loss: 103.9458\n",
      "Iteration: 132300\n",
      "Gradient: [  34.5433   -4.9422  -71.4473  -32.8971 -463.8735]\n",
      "Weights: [-4.5718e+00 -2.6000e-03 -7.4340e-01  7.7100e-02  1.2560e-01]\n",
      "MSE loss: 103.9275\n",
      "Iteration: 132400\n",
      "Gradient: [  -12.192    -10.3321    93.6569  -165.7981 -1305.7597]\n",
      "Weights: [-4.5724e+00 -1.8000e-03 -7.4380e-01  7.7000e-02  1.2560e-01]\n",
      "MSE loss: 103.9009\n",
      "Iteration: 132500\n",
      "Gradient: [  -25.1434    10.0138    33.0486   -36.6257 -1000.5944]\n",
      "Weights: [-4.572e+00 -1.100e-03 -7.441e-01  7.700e-02  1.256e-01]\n",
      "MSE loss: 103.8787\n",
      "Iteration: 132600\n",
      "Gradient: [-12.0977 -74.3058  46.5023 -79.2296  14.2186]\n",
      "Weights: [-4.5713e+00 -7.0000e-04 -7.4460e-01  7.7000e-02  1.2560e-01]\n",
      "MSE loss: 103.856\n",
      "Iteration: 132700\n",
      "Gradient: [  3.308   -5.744   33.9785  86.9486 365.1772]\n",
      "Weights: [-4.5721e+00  3.0000e-04 -7.4490e-01  7.7000e-02  1.2570e-01]\n",
      "MSE loss: 103.8305\n",
      "Iteration: 132800\n",
      "Gradient: [-1.98708e+01  3.30200e-01  4.06860e+01 -9.32324e+01  4.25191e+02]\n",
      "Weights: [-4.573e+00  6.000e-04 -7.453e-01  7.700e-02  1.257e-01]\n",
      "MSE loss: 103.8125\n",
      "Iteration: 132900\n",
      "Gradient: [-16.1492 -55.6804  16.6663 200.8651 213.5988]\n",
      "Weights: [-4.572e+00  1.100e-03 -7.455e-01  7.690e-02  1.257e-01]\n",
      "MSE loss: 103.7945\n",
      "Iteration: 133000\n",
      "Gradient: [   2.3178   14.0616  -34.6144  348.7188 -507.9646]\n",
      "Weights: [-4.5723e+00  1.1000e-03 -7.4580e-01  7.6900e-02  1.2570e-01]\n",
      "MSE loss: 103.7778\n",
      "Iteration: 133100\n",
      "Gradient: [   1.0274  -31.7851    3.8694  268.6318 -667.4558]\n",
      "Weights: [-4.573e+00  2.400e-03 -7.461e-01  7.690e-02  1.258e-01]\n",
      "MSE loss: 103.7534\n",
      "Iteration: 133200\n",
      "Gradient: [   3.0983  -51.1031  196.3717 -117.4644  452.0193]\n",
      "Weights: [-4.5732e+00  2.7000e-03 -7.4640e-01  7.6900e-02  1.2580e-01]\n",
      "MSE loss: 103.7338\n",
      "Iteration: 133300\n",
      "Gradient: [  13.6805    7.2194   58.4918  -83.5099 -729.3459]\n",
      "Weights: [-4.5754e+00  4.1000e-03 -7.4670e-01  7.6900e-02  1.2580e-01]\n",
      "MSE loss: 103.7119\n",
      "Iteration: 133400\n",
      "Gradient: [ -10.4655   -6.0033   43.9696   37.4876 -206.0775]\n",
      "Weights: [-4.5738e+00  4.5000e-03 -7.4710e-01  7.6900e-02  1.2580e-01]\n",
      "MSE loss: 103.6876\n",
      "Iteration: 133500\n",
      "Gradient: [ -14.6036  -36.5211    4.5381  167.1156 -577.3093]\n",
      "Weights: [-4.5741e+00  4.5000e-03 -7.4740e-01  7.6900e-02  1.2590e-01]\n",
      "MSE loss: 103.6723\n",
      "Iteration: 133600\n",
      "Gradient: [  -3.0787   34.6142   -7.1242  -34.0467 -641.8835]\n",
      "Weights: [-4.5736  0.0046 -0.7476  0.0768  0.1259]\n",
      "MSE loss: 103.661\n",
      "Iteration: 133700\n",
      "Gradient: [   23.4676    17.3059    77.1335  -415.5722 -1052.1413]\n",
      "Weights: [-4.5732  0.0047 -0.7478  0.0768  0.1259]\n",
      "MSE loss: 103.6462\n",
      "Iteration: 133800\n",
      "Gradient: [  29.8325    9.8762  166.7854 -341.7614  252.1918]\n",
      "Weights: [-4.5728  0.0058 -0.7482  0.0768  0.1259]\n",
      "MSE loss: 103.6214\n",
      "Iteration: 133900\n",
      "Gradient: [-33.5979 -52.2449  24.2224 119.2199 -24.5814]\n",
      "Weights: [-4.5748  0.0065 -0.7485  0.0767  0.1259]\n",
      "MSE loss: 103.5995\n",
      "Iteration: 134000\n",
      "Gradient: [  -4.7192  -26.6957  154.9575   94.794  -632.5874]\n",
      "Weights: [-4.5737  0.007  -0.7488  0.0767  0.126 ]\n",
      "MSE loss: 103.5798\n",
      "Iteration: 134100\n",
      "Gradient: [  10.1047  -27.8441  -87.9211  -47.3289 -747.2226]\n",
      "Weights: [-4.576   0.0077 -0.7491  0.0767  0.126 ]\n",
      "MSE loss: 103.5632\n",
      "Iteration: 134200\n",
      "Gradient: [-20.344  -21.2795 -19.6436 -22.9765 320.4614]\n",
      "Weights: [-4.5758  0.0084 -0.7493  0.0767  0.126 ]\n",
      "MSE loss: 103.5403\n",
      "Iteration: 134300\n",
      "Gradient: [  -0.5695   50.7516   47.8     -17.6022 -167.6309]\n",
      "Weights: [-4.5742  0.009  -0.7496  0.0766  0.126 ]\n",
      "MSE loss: 103.5259\n",
      "Iteration: 134400\n",
      "Gradient: [  12.0345   -2.3785  124.6729 -491.3353 -242.6871]\n",
      "Weights: [-4.575   0.0094 -0.7499  0.0766  0.1261]\n",
      "MSE loss: 103.5025\n",
      "Iteration: 134500\n",
      "Gradient: [  -9.2744  -58.567    14.5647  137.0703 -583.7956]\n",
      "Weights: [-4.5736  0.0097 -0.7503  0.0766  0.1261]\n",
      "MSE loss: 103.4836\n",
      "Iteration: 134600\n",
      "Gradient: [  -4.8078    4.02    -49.7756   66.0104 -548.9935]\n",
      "Weights: [-4.5752  0.0097 -0.7506  0.0766  0.1261]\n",
      "MSE loss: 103.4636\n",
      "Iteration: 134700\n",
      "Gradient: [   2.2341   61.0481   86.6513  181.6553 -636.0865]\n",
      "Weights: [-4.576   0.0102 -0.7508  0.0766  0.1262]\n",
      "MSE loss: 103.4433\n",
      "Iteration: 134800\n",
      "Gradient: [ -23.7073  -15.5929 -103.5307 -350.8467  -87.7074]\n",
      "Weights: [-4.5751  0.0102 -0.7512  0.0766  0.1262]\n",
      "MSE loss: 103.4248\n",
      "Iteration: 134900\n",
      "Gradient: [  -1.4401  -37.6764 -148.8707   28.1904 -597.9037]\n",
      "Weights: [-4.5761  0.0113 -0.7515  0.0766  0.1262]\n",
      "MSE loss: 103.4004\n",
      "Iteration: 135000\n",
      "Gradient: [ -22.3311  -26.4966  -39.6768  105.3921 -466.3019]\n",
      "Weights: [-4.578   0.0123 -0.7518  0.0766  0.1263]\n",
      "MSE loss: 103.3816\n",
      "Iteration: 135100\n",
      "Gradient: [   9.2449   -6.9328  131.5844  -97.4671 -552.9226]\n",
      "Weights: [-4.577   0.0132 -0.7522  0.0766  0.1263]\n",
      "MSE loss: 103.3542\n",
      "Iteration: 135200\n",
      "Gradient: [  10.9159    5.269    33.9468  473.4536 -420.2175]\n",
      "Weights: [-4.5771  0.0142 -0.7527  0.0765  0.1263]\n",
      "MSE loss: 103.3287\n",
      "Iteration: 135300\n",
      "Gradient: [ -28.8137   58.9451   76.6992  167.7985 -430.6356]\n",
      "Weights: [-4.5788  0.0155 -0.753   0.0765  0.1263]\n",
      "MSE loss: 103.3029\n",
      "Iteration: 135400\n",
      "Gradient: [   3.8022  -22.4013   52.4017  308.6793 -323.1052]\n",
      "Weights: [-4.5773  0.0159 -0.7534  0.0764  0.1263]\n",
      "MSE loss: 103.2798\n",
      "Iteration: 135500\n",
      "Gradient: [  -27.0105    42.0199   -10.2677  -380.3614 -1067.1268]\n",
      "Weights: [-4.5795  0.0163 -0.7536  0.0764  0.1264]\n",
      "MSE loss: 103.2684\n",
      "Iteration: 135600\n",
      "Gradient: [  -8.6656  -25.4562 -103.1707   96.1677 -903.1876]\n",
      "Weights: [-4.5783  0.0164 -0.7538  0.0764  0.1264]\n",
      "MSE loss: 103.2539\n",
      "Iteration: 135700\n",
      "Gradient: [ 19.5318 -55.0252   0.6375 420.0264  42.7053]\n",
      "Weights: [-4.5771  0.0173 -0.7542  0.0763  0.1264]\n",
      "MSE loss: 103.2311\n",
      "Iteration: 135800\n",
      "Gradient: [-31.4647  29.7728  42.2954   8.6164 -29.6472]\n",
      "Weights: [-4.5793  0.018  -0.7545  0.0764  0.1264]\n",
      "MSE loss: 103.2102\n",
      "Iteration: 135900\n",
      "Gradient: [ -13.2138  -15.1426   36.8268  246.6213 1178.6272]\n",
      "Weights: [-4.5798  0.0188 -0.7549  0.0764  0.1264]\n",
      "MSE loss: 103.1891\n",
      "Iteration: 136000\n",
      "Gradient: [  34.1135  -65.3406  141.0707  174.4642 -833.5258]\n",
      "Weights: [-4.5805  0.0196 -0.7551  0.0764  0.1265]\n",
      "MSE loss: 103.1711\n",
      "Iteration: 136100\n",
      "Gradient: [  -7.3668   12.825    40.1755   93.2812 -299.721 ]\n",
      "Weights: [-4.5802  0.0204 -0.7555  0.0763  0.1265]\n",
      "MSE loss: 103.1473\n",
      "Iteration: 136200\n",
      "Gradient: [  -4.8068   -1.0657   31.6169  -53.8298 -477.1836]\n",
      "Weights: [-4.578   0.0208 -0.756   0.0763  0.1265]\n",
      "MSE loss: 103.1243\n",
      "Iteration: 136300\n",
      "Gradient: [ 24.2983  73.4595  19.8497 105.1222  73.7238]\n",
      "Weights: [-4.5779  0.0212 -0.7564  0.0763  0.1265]\n",
      "MSE loss: 103.1009\n",
      "Iteration: 136400\n",
      "Gradient: [ -13.6933   15.9475   89.5866  -66.1872 -675.0544]\n",
      "Weights: [-4.5811  0.022  -0.7566  0.0763  0.1266]\n",
      "MSE loss: 103.0856\n",
      "Iteration: 136500\n",
      "Gradient: [  -0.8472  -64.8767  -17.9992   72.2608 -392.3455]\n",
      "Weights: [-4.5807  0.0228 -0.7567  0.0763  0.1266]\n",
      "MSE loss: 103.068\n",
      "Iteration: 136600\n",
      "Gradient: [ 13.0935  23.6477   3.3224 -75.0345 216.5037]\n",
      "Weights: [-4.58    0.0224 -0.7572  0.0762  0.1266]\n",
      "MSE loss: 103.0505\n",
      "Iteration: 136700\n",
      "Gradient: [  8.5884 -25.0793  88.6293  60.688  -35.0273]\n",
      "Weights: [-4.5797  0.023  -0.7575  0.0762  0.1266]\n",
      "MSE loss: 103.0297\n",
      "Iteration: 136800\n",
      "Gradient: [  10.1125  -34.6951   52.2592    0.6097 -595.7384]\n",
      "Weights: [-4.5798  0.0234 -0.7578  0.0762  0.1267]\n",
      "MSE loss: 103.0128\n",
      "Iteration: 136900\n",
      "Gradient: [  19.1057   46.4212  -20.4217  279.0281 -266.858 ]\n",
      "Weights: [-4.5803  0.0241 -0.7581  0.0762  0.1267]\n",
      "MSE loss: 102.9916\n",
      "Iteration: 137000\n",
      "Gradient: [ -37.9858  -53.7287  104.6564  184.1684 -729.73  ]\n",
      "Weights: [-4.5809  0.0246 -0.7584  0.0762  0.1267]\n",
      "MSE loss: 102.9721\n",
      "Iteration: 137100\n",
      "Gradient: [  42.2061   20.3568  -91.9423    5.1776 -895.5534]\n",
      "Weights: [-4.5816  0.0254 -0.7587  0.0762  0.1267]\n",
      "MSE loss: 102.9537\n",
      "Iteration: 137200\n",
      "Gradient: [ -20.7781    4.9405   31.3178 -122.786   278.2615]\n",
      "Weights: [-4.5822  0.0263 -0.759   0.0762  0.1268]\n",
      "MSE loss: 102.9311\n",
      "Iteration: 137300\n",
      "Gradient: [ -22.9939  -36.0672  177.4814   54.1068 -553.7006]\n",
      "Weights: [-4.5821  0.0272 -0.7594  0.0762  0.1268]\n",
      "MSE loss: 102.9097\n",
      "Iteration: 137400\n",
      "Gradient: [ -41.2279  -11.0543 -123.8803 -250.9978 -610.4087]\n",
      "Weights: [-4.5823  0.0276 -0.7599  0.0762  0.1268]\n",
      "MSE loss: 102.8832\n",
      "Iteration: 137500\n",
      "Gradient: [-13.6163   1.7475  94.3863 104.3414 784.4577]\n",
      "Weights: [-4.5819  0.0277 -0.7602  0.0762  0.1268]\n",
      "MSE loss: 102.8664\n",
      "Iteration: 137600\n",
      "Gradient: [  -3.6825   20.8094   77.6372  -70.8351 -309.6691]\n",
      "Weights: [-4.5825  0.0282 -0.7606  0.0761  0.1269]\n",
      "MSE loss: 102.8469\n",
      "Iteration: 137700\n",
      "Gradient: [  7.0069  37.854   66.5515 104.9037 398.6035]\n",
      "Weights: [-4.581   0.0287 -0.7608  0.0761  0.1269]\n",
      "MSE loss: 102.8296\n",
      "Iteration: 137800\n",
      "Gradient: [  -2.3815    6.8138   62.927  -188.7283  642.2994]\n",
      "Weights: [-4.5824  0.0293 -0.7609  0.0761  0.1269]\n",
      "MSE loss: 102.8169\n",
      "Iteration: 137900\n",
      "Gradient: [   5.0926   29.6461   19.9976 -253.4658 -294.0598]\n",
      "Weights: [-4.582   0.0296 -0.7612  0.0761  0.1269]\n",
      "MSE loss: 102.8012\n",
      "Iteration: 138000\n",
      "Gradient: [   2.6518   20.0854   95.9115  154.7195 -514.5286]\n",
      "Weights: [-4.5796  0.0296 -0.7617  0.076   0.1269]\n",
      "MSE loss: 102.7874\n",
      "Iteration: 138100\n",
      "Gradient: [  15.7878  -45.6586   18.9365  -75.6824 -368.8271]\n",
      "Weights: [-4.581   0.0301 -0.7619  0.0761  0.127 ]\n",
      "MSE loss: 102.7677\n",
      "Iteration: 138200\n",
      "Gradient: [ -16.721   -34.9669   -9.8893 -136.9215 -214.5404]\n",
      "Weights: [-4.5802  0.0305 -0.7623  0.0761  0.127 ]\n",
      "MSE loss: 102.7511\n",
      "Iteration: 138300\n",
      "Gradient: [  63.5941   38.9266  -72.8912 -175.1661 -682.5591]\n",
      "Weights: [-4.5798  0.0306 -0.7625  0.0761  0.127 ]\n",
      "MSE loss: 102.7372\n",
      "Iteration: 138400\n",
      "Gradient: [   7.8932  -12.9595   31.7601 -133.2522  210.1439]\n",
      "Weights: [-4.5805  0.0305 -0.7628  0.0761  0.127 ]\n",
      "MSE loss: 102.7196\n",
      "Iteration: 138500\n",
      "Gradient: [ -22.7991   24.6021  -18.6123  138.3144 -286.9551]\n",
      "Weights: [-4.5824  0.0314 -0.7631  0.0761  0.1271]\n",
      "MSE loss: 102.6997\n",
      "Iteration: 138600\n",
      "Gradient: [   14.1376   -59.698    -16.2318   147.1219 -1111.332 ]\n",
      "Weights: [-4.5813  0.032  -0.7634  0.0761  0.1271]\n",
      "MSE loss: 102.682\n",
      "Iteration: 138700\n",
      "Gradient: [  57.3939  -50.359   113.5378 -161.5305 -983.147 ]\n",
      "Weights: [-4.5812  0.0326 -0.7637  0.0761  0.1271]\n",
      "MSE loss: 102.6611\n",
      "Iteration: 138800\n",
      "Gradient: [   7.3193  -10.1799  116.1889 -310.2985  309.8434]\n",
      "Weights: [-4.5833  0.033  -0.764   0.0761  0.1271]\n",
      "MSE loss: 102.6464\n",
      "Iteration: 138900\n",
      "Gradient: [ 22.8529 -49.4921 117.9013  93.5309  61.3373]\n",
      "Weights: [-4.5821  0.0335 -0.7642  0.076   0.1272]\n",
      "MSE loss: 102.6268\n",
      "Iteration: 139000\n",
      "Gradient: [   3.3675   -3.7386   -4.5928 -160.597  -448.6386]\n",
      "Weights: [-4.5827  0.0345 -0.7644  0.076   0.1272]\n",
      "MSE loss: 102.6088\n",
      "Iteration: 139100\n",
      "Gradient: [ 7.8930000e-01  1.7786100e+01  1.1573000e+01 -7.0174700e+01\n",
      " -1.0387576e+03]\n",
      "Weights: [-4.5835  0.0348 -0.7648  0.076   0.1272]\n",
      "MSE loss: 102.59\n",
      "Iteration: 139200\n",
      "Gradient: [ 40.8051  14.9762   6.8944 417.3292 314.2827]\n",
      "Weights: [-4.5838  0.0353 -0.7651  0.076   0.1272]\n",
      "MSE loss: 102.5708\n",
      "Iteration: 139300\n",
      "Gradient: [   8.7445  -63.1133  104.8242  -81.0053 -685.7946]\n",
      "Weights: [-4.5842  0.0362 -0.7654  0.076   0.1272]\n",
      "MSE loss: 102.5527\n",
      "Iteration: 139400\n",
      "Gradient: [  -7.5834   38.3931  109.0757  467.2522 -930.588 ]\n",
      "Weights: [-4.5835  0.0367 -0.7658  0.0759  0.1273]\n",
      "MSE loss: 102.5299\n",
      "Iteration: 139500\n",
      "Gradient: [ -0.6257 -13.4118 116.9233 241.4349 -68.9522]\n",
      "Weights: [-4.5822  0.0368 -0.7661  0.0759  0.1273]\n",
      "MSE loss: 102.5179\n",
      "Iteration: 139600\n",
      "Gradient: [   1.2102   -0.3687 -114.8284  155.6907  203.9019]\n",
      "Weights: [-4.5825  0.0376 -0.7664  0.0759  0.1273]\n",
      "MSE loss: 102.4994\n",
      "Iteration: 139700\n",
      "Gradient: [  -20.1651   -71.4004   -33.8728  -106.4879 -1573.7758]\n",
      "Weights: [-4.584   0.0379 -0.7667  0.0759  0.1273]\n",
      "MSE loss: 102.48\n",
      "Iteration: 139800\n",
      "Gradient: [ -11.5469    3.8593   31.4458 -366.1795 -249.1614]\n",
      "Weights: [-4.5845  0.0387 -0.767   0.0759  0.1274]\n",
      "MSE loss: 102.4612\n",
      "Iteration: 139900\n",
      "Gradient: [  17.7262   35.6051   61.8115  -80.7107 -279.6258]\n",
      "Weights: [-4.5845  0.0397 -0.7672  0.0758  0.1274]\n",
      "MSE loss: 102.4423\n",
      "Iteration: 140000\n",
      "Gradient: [  24.8508   -3.418    19.6749 -223.3613   97.18  ]\n",
      "Weights: [-4.5849  0.0404 -0.7675  0.0758  0.1274]\n",
      "MSE loss: 102.4246\n",
      "Iteration: 140100\n",
      "Gradient: [ 38.6601  42.0624 184.8386 236.2975 396.033 ]\n",
      "Weights: [-4.5837  0.0409 -0.7679  0.0758  0.1274]\n",
      "MSE loss: 102.406\n",
      "Iteration: 140200\n",
      "Gradient: [  3.397    6.6374 110.6981 -10.8498 168.6971]\n",
      "Weights: [-4.5851  0.0417 -0.7681  0.0758  0.1275]\n",
      "MSE loss: 102.3828\n",
      "Iteration: 140300\n",
      "Gradient: [ 36.4836 -25.9378  51.9147 205.8458 -81.1318]\n",
      "Weights: [-4.5858  0.042  -0.7684  0.0757  0.1275]\n",
      "MSE loss: 102.3677\n",
      "Iteration: 140400\n",
      "Gradient: [  24.5789  -19.3064  143.2697 -383.2246  -17.7273]\n",
      "Weights: [-4.5854  0.0428 -0.7688  0.0757  0.1275]\n",
      "MSE loss: 102.3446\n",
      "Iteration: 140500\n",
      "Gradient: [ -20.8104  -27.6749   63.3083  -28.6503 1092.9374]\n",
      "Weights: [-4.5864  0.0432 -0.7691  0.0756  0.1275]\n",
      "MSE loss: 102.3268\n",
      "Iteration: 140600\n",
      "Gradient: [-20.2452   3.0554  46.7938 260.3867 256.5519]\n",
      "Weights: [-4.587   0.0441 -0.7694  0.0756  0.1275]\n",
      "MSE loss: 102.3077\n",
      "Iteration: 140700\n",
      "Gradient: [-19.2293  -6.1925  97.8852 215.5939  33.5331]\n",
      "Weights: [-4.5861  0.0446 -0.7698  0.0756  0.1276]\n",
      "MSE loss: 102.2849\n",
      "Iteration: 140800\n",
      "Gradient: [  30.9944   -8.3236   29.8194  162.7567 -377.1064]\n",
      "Weights: [-4.5855  0.0452 -0.7701  0.0756  0.1276]\n",
      "MSE loss: 102.2682\n",
      "Iteration: 140900\n",
      "Gradient: [ 9.490000e-02 -2.210780e+01  4.055060e+01 -1.977635e+02  5.273980e+01]\n",
      "Weights: [-4.5863  0.0455 -0.7704  0.0756  0.1276]\n",
      "MSE loss: 102.2499\n",
      "Iteration: 141000\n",
      "Gradient: [  -7.9755  -47.7599  -44.0706 -144.0763  647.941 ]\n",
      "Weights: [-4.5846  0.0458 -0.7706  0.0756  0.1276]\n",
      "MSE loss: 102.2396\n",
      "Iteration: 141100\n",
      "Gradient: [-18.1505 -91.26    19.0104 189.0679 231.2932]\n",
      "Weights: [-4.5875  0.0464 -0.7709  0.0755  0.1277]\n",
      "MSE loss: 102.2161\n",
      "Iteration: 141200\n",
      "Gradient: [  -5.6698   -1.7335   26.1096 -134.6824 -763.2047]\n",
      "Weights: [-4.5879  0.0469 -0.7712  0.0755  0.1277]\n",
      "MSE loss: 102.2002\n",
      "Iteration: 141300\n",
      "Gradient: [-20.5659  46.8656 -28.6906 164.4042 450.8553]\n",
      "Weights: [-4.5876  0.0476 -0.7715  0.0755  0.1277]\n",
      "MSE loss: 102.1771\n",
      "Iteration: 141400\n",
      "Gradient: [ 23.9723 -10.9722 119.2291 152.3907 317.7351]\n",
      "Weights: [-4.5864  0.0475 -0.7718  0.0755  0.1278]\n",
      "MSE loss: 102.1641\n",
      "Iteration: 141500\n",
      "Gradient: [  -7.5186  -44.4222   11.6337  379.8111 -570.0051]\n",
      "Weights: [-4.588   0.0491 -0.7722  0.0755  0.1278]\n",
      "MSE loss: 102.1375\n",
      "Iteration: 141600\n",
      "Gradient: [   7.5264  -15.7815   96.713  -244.5072  -87.0237]\n",
      "Weights: [-4.5881  0.0499 -0.7725  0.0755  0.1278]\n",
      "MSE loss: 102.1166\n",
      "Iteration: 141700\n",
      "Gradient: [-2.770500e+00 -5.177000e-01  6.405850e+01  1.046529e+02 -6.494579e+02]\n",
      "Weights: [-4.5877  0.0506 -0.7729  0.0755  0.1278]\n",
      "MSE loss: 102.0998\n",
      "Iteration: 141800\n",
      "Gradient: [ 21.7675 -17.1924 160.0689 -79.1535 -72.7404]\n",
      "Weights: [-4.5886  0.0511 -0.7732  0.0755  0.1278]\n",
      "MSE loss: 102.0786\n",
      "Iteration: 141900\n",
      "Gradient: [ 23.7238 -53.2975  87.2826 -21.74   314.9415]\n",
      "Weights: [-4.5897  0.0524 -0.7736  0.0754  0.1278]\n",
      "MSE loss: 102.0565\n",
      "Iteration: 142000\n",
      "Gradient: [  -21.6978   -43.7615   107.4042  -236.779  -2011.8678]\n",
      "Weights: [-4.5891  0.0529 -0.7739  0.0754  0.1279]\n",
      "MSE loss: 102.0383\n",
      "Iteration: 142100\n",
      "Gradient: [  -7.3754   -3.675    46.3439  353.2389 -294.1444]\n",
      "Weights: [-4.5891  0.0539 -0.7743  0.0754  0.1279]\n",
      "MSE loss: 102.0169\n",
      "Iteration: 142200\n",
      "Gradient: [  9.1521  -5.3768 158.6768 215.0169 165.1552]\n",
      "Weights: [-4.5906  0.0543 -0.7747  0.0754  0.1279]\n",
      "MSE loss: 101.9945\n",
      "Iteration: 142300\n",
      "Gradient: [  -6.1997   35.8437  133.7474  219.699  -708.8326]\n",
      "Weights: [-4.5906  0.0555 -0.775   0.0753  0.1279]\n",
      "MSE loss: 101.9701\n",
      "Iteration: 142400\n",
      "Gradient: [  16.862    49.5284  -19.5185  112.812  -852.9109]\n",
      "Weights: [-4.5899  0.0555 -0.7754  0.0753  0.128 ]\n",
      "MSE loss: 101.9559\n",
      "Iteration: 142500\n",
      "Gradient: [ -2.7785  56.1424  39.8321 259.0563 121.3225]\n",
      "Weights: [-4.5906  0.0564 -0.7756  0.0753  0.128 ]\n",
      "MSE loss: 101.936\n",
      "Iteration: 142600\n",
      "Gradient: [  16.5038  -31.4138  -94.5036  -30.566  -285.8753]\n",
      "Weights: [-4.592   0.0568 -0.7758  0.0753  0.128 ]\n",
      "MSE loss: 101.9231\n",
      "Iteration: 142700\n",
      "Gradient: [  -3.0416   11.6948   -9.9956  277.9075 -753.1982]\n",
      "Weights: [-4.5928  0.0576 -0.7761  0.0753  0.128 ]\n",
      "MSE loss: 101.9048\n",
      "Iteration: 142800\n",
      "Gradient: [-30.5307   3.253  -34.6325 -26.1321 -37.6314]\n",
      "Weights: [-4.5899  0.0575 -0.7765  0.0752  0.1281]\n",
      "MSE loss: 101.8881\n",
      "Iteration: 142900\n",
      "Gradient: [  1.0224 -34.9001  21.8909  28.0333 310.968 ]\n",
      "Weights: [-4.5913  0.0582 -0.7769  0.0752  0.1281]\n",
      "MSE loss: 101.8655\n",
      "Iteration: 143000\n",
      "Gradient: [ 6.6670000e-01  7.0364800e+01  1.2108300e+01  1.3083350e+02\n",
      " -1.3033143e+03]\n",
      "Weights: [-4.5907  0.0585 -0.7771  0.0752  0.1281]\n",
      "MSE loss: 101.8494\n",
      "Iteration: 143100\n",
      "Gradient: [ -11.5838   17.9739 -155.4845    7.4675  309.7132]\n",
      "Weights: [-4.5914  0.0594 -0.7774  0.0751  0.1281]\n",
      "MSE loss: 101.8289\n",
      "Iteration: 143200\n",
      "Gradient: [ -71.3817   10.04     34.8256  170.936  -519.7385]\n",
      "Weights: [-4.5911  0.0599 -0.7776  0.0751  0.1282]\n",
      "MSE loss: 101.8173\n",
      "Iteration: 143300\n",
      "Gradient: [ 25.8973  69.0223 -44.0152 315.4845 647.3645]\n",
      "Weights: [-4.5906  0.0605 -0.778   0.0751  0.1282]\n",
      "MSE loss: 101.7988\n",
      "Iteration: 143400\n",
      "Gradient: [  13.404    50.0658   46.5651  -41.4634 -273.6372]\n",
      "Weights: [-4.5926  0.0611 -0.7781  0.0751  0.1282]\n",
      "MSE loss: 101.7841\n",
      "Iteration: 143500\n",
      "Gradient: [ -29.2156  -40.4535   59.3881  171.0797 -574.4606]\n",
      "Weights: [-4.5932  0.0617 -0.7783  0.0751  0.1282]\n",
      "MSE loss: 101.7686\n",
      "Iteration: 143600\n",
      "Gradient: [ -30.1065   -5.5482    1.8179 -156.5628  218.8541]\n",
      "Weights: [-4.5921  0.0621 -0.7788  0.075   0.1282]\n",
      "MSE loss: 101.7467\n",
      "Iteration: 143700\n",
      "Gradient: [  -8.3025  -56.1951  165.342  -157.8196 -436.9088]\n",
      "Weights: [-4.5931  0.0629 -0.779   0.075   0.1283]\n",
      "MSE loss: 101.7288\n",
      "Iteration: 143800\n",
      "Gradient: [  28.6358  -19.2586  126.5792 -342.1035  479.2841]\n",
      "Weights: [-4.5924  0.0631 -0.7794  0.075   0.1283]\n",
      "MSE loss: 101.7147\n",
      "Iteration: 143900\n",
      "Gradient: [ -7.8131 -31.1769  -8.4736 317.2739 -24.9715]\n",
      "Weights: [-4.5938  0.0638 -0.7797  0.075   0.1283]\n",
      "MSE loss: 101.6928\n",
      "Iteration: 144000\n",
      "Gradient: [-4.4280400e+01 -9.4000000e-03  1.8133900e+02  5.5240000e-01\n",
      " -1.1055145e+03]\n",
      "Weights: [-4.594   0.0646 -0.78    0.075   0.1283]\n",
      "MSE loss: 101.6762\n",
      "Iteration: 144100\n",
      "Gradient: [   1.3163    7.5125    8.8483  219.2573 -450.3928]\n",
      "Weights: [-4.5946  0.0652 -0.7803  0.075   0.1283]\n",
      "MSE loss: 101.6615\n",
      "Iteration: 144200\n",
      "Gradient: [  -8.3651  -50.514    64.0811  145.8413 -401.083 ]\n",
      "Weights: [-4.5925  0.065  -0.7806  0.075   0.1284]\n",
      "MSE loss: 101.6503\n",
      "Iteration: 144300\n",
      "Gradient: [ 27.9237 -14.2456  65.6099 129.7951 289.6234]\n",
      "Weights: [-4.596   0.0662 -0.7809  0.075   0.1284]\n",
      "MSE loss: 101.635\n",
      "Iteration: 144400\n",
      "Gradient: [  -7.1073   52.4854  -12.9683   19.3385 -295.1473]\n",
      "Weights: [-4.5944  0.0662 -0.7812  0.075   0.1284]\n",
      "MSE loss: 101.619\n",
      "Iteration: 144500\n",
      "Gradient: [ -30.6427  -41.7814  167.848  -414.8219  136.0504]\n",
      "Weights: [-4.5944  0.067  -0.7815  0.0749  0.1284]\n",
      "MSE loss: 101.5948\n",
      "Iteration: 144600\n",
      "Gradient: [ 12.1286  46.8395 -38.1678 142.0254 440.9757]\n",
      "Weights: [-4.5937  0.067  -0.7818  0.0749  0.1285]\n",
      "MSE loss: 101.5819\n",
      "Iteration: 144700\n",
      "Gradient: [ -34.0553  -80.4236  103.9119  -99.9764 -145.4663]\n",
      "Weights: [-4.5936  0.0674 -0.7821  0.0749  0.1285]\n",
      "MSE loss: 101.5622\n",
      "Iteration: 144800\n",
      "Gradient: [ -44.2443   37.2592  224.7392 -173.3615  511.5539]\n",
      "Weights: [-4.5945  0.0681 -0.7824  0.0749  0.1285]\n",
      "MSE loss: 101.5447\n",
      "Iteration: 144900\n",
      "Gradient: [  -7.4847  -68.305  -157.4961  105.5165 -748.9122]\n",
      "Weights: [-4.5951  0.0694 -0.7827  0.0749  0.1285]\n",
      "MSE loss: 101.5209\n",
      "Iteration: 145000\n",
      "Gradient: [  17.1469    7.7061  -90.2904 -230.9878 -699.4178]\n",
      "Weights: [-4.596   0.0701 -0.783   0.0749  0.1286]\n",
      "MSE loss: 101.5037\n",
      "Iteration: 145100\n",
      "Gradient: [ -4.9589  -4.6507 214.5592 289.8114 127.5801]\n",
      "Weights: [-4.5962  0.0711 -0.7833  0.0748  0.1286]\n",
      "MSE loss: 101.4853\n",
      "Iteration: 145200\n",
      "Gradient: [ -25.841   -55.3797  -37.5101   82.3816 -208.0668]\n",
      "Weights: [-4.5959  0.0713 -0.7835  0.0747  0.1286]\n",
      "MSE loss: 101.472\n",
      "Iteration: 145300\n",
      "Gradient: [  8.1498  22.0664 -52.1401 126.7446 214.5267]\n",
      "Weights: [-4.5958  0.072  -0.7836  0.0747  0.1286]\n",
      "MSE loss: 101.4596\n",
      "Iteration: 145400\n",
      "Gradient: [  38.6865   -3.4961   79.902    66.8831 -382.1118]\n",
      "Weights: [-4.5974  0.0724 -0.784   0.0747  0.1286]\n",
      "MSE loss: 101.445\n",
      "Iteration: 145500\n",
      "Gradient: [ -30.9507  -26.7085   31.9663  -13.0607 -445.3133]\n",
      "Weights: [-4.5966  0.0728 -0.7842  0.0747  0.1287]\n",
      "MSE loss: 101.4294\n",
      "Iteration: 145600\n",
      "Gradient: [ -6.5454  37.5354  -2.2013 -97.9336 195.7294]\n",
      "Weights: [-4.5973  0.0735 -0.7845  0.0747  0.1287]\n",
      "MSE loss: 101.4096\n",
      "Iteration: 145700\n",
      "Gradient: [  -3.537    11.854   152.4891 -282.0168 -494.95  ]\n",
      "Weights: [-4.5958  0.0738 -0.7849  0.0746  0.1287]\n",
      "MSE loss: 101.3932\n",
      "Iteration: 145800\n",
      "Gradient: [ 1.168000e-01 -1.241540e+01  1.266297e+02  8.766340e+01 -7.782799e+02]\n",
      "Weights: [-4.5975  0.0743 -0.7852  0.0746  0.1287]\n",
      "MSE loss: 101.378\n",
      "Iteration: 145900\n",
      "Gradient: [ -17.1247   97.5701  -64.7688   21.5767 -878.6918]\n",
      "Weights: [-4.5962  0.0745 -0.7855  0.0746  0.1287]\n",
      "MSE loss: 101.3589\n",
      "Iteration: 146000\n",
      "Gradient: [ -10.7805    3.4731 -128.473   336.7364  -74.1421]\n",
      "Weights: [-4.5969  0.0749 -0.7857  0.0746  0.1288]\n",
      "MSE loss: 101.3481\n",
      "Iteration: 146100\n",
      "Gradient: [  8.8539 -47.4019  53.0762 -74.976  354.6813]\n",
      "Weights: [-4.5975  0.0754 -0.7859  0.0746  0.1288]\n",
      "MSE loss: 101.3364\n",
      "Iteration: 146200\n",
      "Gradient: [  -9.7669  -26.9877    1.2642  219.1743 -389.471 ]\n",
      "Weights: [-4.5976  0.0762 -0.7863  0.0746  0.1288]\n",
      "MSE loss: 101.3142\n",
      "Iteration: 146300\n",
      "Gradient: [  -2.3988   10.3867   25.8857 -257.6325 -663.7853]\n",
      "Weights: [-4.5971  0.0763 -0.7864  0.0746  0.1288]\n",
      "MSE loss: 101.3045\n",
      "Iteration: 146400\n",
      "Gradient: [  6.2199 -17.4595  88.3467 266.3707 108.6273]\n",
      "Weights: [-4.5968  0.0769 -0.7868  0.0746  0.1288]\n",
      "MSE loss: 101.287\n",
      "Iteration: 146500\n",
      "Gradient: [ -10.1173   12.438   147.7639  145.8931 -527.9619]\n",
      "Weights: [-4.597   0.0781 -0.7873  0.0746  0.1289]\n",
      "MSE loss: 101.261\n",
      "Iteration: 146600\n",
      "Gradient: [ 12.9704  68.7624  12.0193  57.6276 483.3021]\n",
      "Weights: [-4.5969  0.0779 -0.7877  0.0746  0.1289]\n",
      "MSE loss: 101.2431\n",
      "Iteration: 146700\n",
      "Gradient: [ 7.420000e-02 -6.566700e+00  8.219800e+01  4.027694e+02 -2.340942e+02]\n",
      "Weights: [-4.5964  0.0781 -0.7878  0.0746  0.1289]\n",
      "MSE loss: 101.2332\n",
      "Iteration: 146800\n",
      "Gradient: [ -5.4242   0.3647  96.975  162.5964 168.2129]\n",
      "Weights: [-4.5958  0.0788 -0.7882  0.0746  0.1289]\n",
      "MSE loss: 101.2166\n",
      "Iteration: 146900\n",
      "Gradient: [ 20.8317  70.0725 -67.058   38.8407  27.7941]\n",
      "Weights: [-4.5966  0.0794 -0.7886  0.0745  0.129 ]\n",
      "MSE loss: 101.1922\n",
      "Iteration: 147000\n",
      "Gradient: [ -16.4866  -23.3991  120.2658  -25.4789 -587.4005]\n",
      "Weights: [-4.5977  0.0799 -0.789   0.0745  0.129 ]\n",
      "MSE loss: 101.1742\n",
      "Iteration: 147100\n",
      "Gradient: [  -1.7652  -27.2711    4.2758   43.0527 -991.671 ]\n",
      "Weights: [-4.5976  0.0807 -0.7893  0.0745  0.129 ]\n",
      "MSE loss: 101.1548\n",
      "Iteration: 147200\n",
      "Gradient: [ -24.4619  -23.1287   95.7498  -22.5299 -228.7187]\n",
      "Weights: [-4.5975  0.0815 -0.7896  0.0745  0.129 ]\n",
      "MSE loss: 101.1375\n",
      "Iteration: 147300\n",
      "Gradient: [  -2.1457   28.063   126.1094  -19.2713 -123.0304]\n",
      "Weights: [-4.5988  0.0822 -0.7898  0.0744  0.1291]\n",
      "MSE loss: 101.1219\n",
      "Iteration: 147400\n",
      "Gradient: [ -38.5507  -14.6576   60.7443 -113.8013 -620.1186]\n",
      "Weights: [-4.5993  0.0825 -0.7901  0.0744  0.1291]\n",
      "MSE loss: 101.1076\n",
      "Iteration: 147500\n",
      "Gradient: [  18.3203  -67.0296   50.8818 -270.1873 -134.0027]\n",
      "Weights: [-4.5984  0.0833 -0.7904  0.0744  0.1291]\n",
      "MSE loss: 101.0899\n",
      "Iteration: 147600\n",
      "Gradient: [  -0.5457   54.6282  -60.984   -20.2258 -315.9848]\n",
      "Weights: [-4.5991  0.0834 -0.7907  0.0744  0.1291]\n",
      "MSE loss: 101.0754\n",
      "Iteration: 147700\n",
      "Gradient: [ -6.0231   3.6015 167.9429 211.5361 -37.3469]\n",
      "Weights: [-4.5989  0.084  -0.791   0.0744  0.1291]\n",
      "MSE loss: 101.0608\n",
      "Iteration: 147800\n",
      "Gradient: [ -22.539   -14.6194  -25.4581 -229.571  -520.8451]\n",
      "Weights: [-4.5988  0.0845 -0.7913  0.0744  0.1292]\n",
      "MSE loss: 101.0448\n",
      "Iteration: 147900\n",
      "Gradient: [ -33.9499  -23.7573  108.2362  328.6489 -257.1637]\n",
      "Weights: [-4.6003  0.0856 -0.7916  0.0744  0.1292]\n",
      "MSE loss: 101.0239\n",
      "Iteration: 148000\n",
      "Gradient: [   21.9107   -43.5255    33.5378  -144.6436 -1058.1373]\n",
      "Weights: [-4.6002  0.0855 -0.7917  0.0744  0.1292]\n",
      "MSE loss: 101.0175\n",
      "Iteration: 148100\n",
      "Gradient: [  -6.9532   28.1248   37.7654 -269.8692  644.8019]\n",
      "Weights: [-4.5994  0.0859 -0.792   0.0744  0.1292]\n",
      "MSE loss: 101.0041\n",
      "Iteration: 148200\n",
      "Gradient: [   -7.7683   -22.3346   -56.8186    87.9306 -1037.9793]\n",
      "Weights: [-4.5989  0.0865 -0.7923  0.0744  0.1292]\n",
      "MSE loss: 100.9906\n",
      "Iteration: 148300\n",
      "Gradient: [  -1.526   -35.9734 -152.5438 -277.5764 -737.9254]\n",
      "Weights: [-4.5995  0.087  -0.7926  0.0743  0.1292]\n",
      "MSE loss: 100.9744\n",
      "Iteration: 148400\n",
      "Gradient: [   31.8249   -44.1403   -27.8857  -222.7576 -1066.2688]\n",
      "Weights: [-4.6004  0.0879 -0.7929  0.0743  0.1293]\n",
      "MSE loss: 100.9557\n",
      "Iteration: 148500\n",
      "Gradient: [  -8.1692  -30.4193  123.9709 -293.03    -53.4382]\n",
      "Weights: [-4.6019  0.0887 -0.7932  0.0744  0.1293]\n",
      "MSE loss: 100.9417\n",
      "Iteration: 148600\n",
      "Gradient: [-1.9809800e+01 -2.5783800e+01  3.9677400e+01  1.4995000e+00\n",
      " -1.6326905e+03]\n",
      "Weights: [-4.6028  0.0892 -0.7935  0.0744  0.1293]\n",
      "MSE loss: 100.9257\n",
      "Iteration: 148700\n",
      "Gradient: [  44.7013    1.9726   81.8498 -116.2098 -611.5776]\n",
      "Weights: [-4.603   0.0904 -0.7939  0.0743  0.1293]\n",
      "MSE loss: 100.9001\n",
      "Iteration: 148800\n",
      "Gradient: [ -10.8159    5.1688    9.1215  228.2049 -699.9587]\n",
      "Weights: [-4.6036  0.0914 -0.7942  0.0744  0.1293]\n",
      "MSE loss: 100.8848\n",
      "Iteration: 148900\n",
      "Gradient: [  18.1405    1.4156   67.8837 -194.6293 -723.9042]\n",
      "Weights: [-4.6033  0.0919 -0.7946  0.0743  0.1294]\n",
      "MSE loss: 100.8633\n",
      "Iteration: 149000\n",
      "Gradient: [  -3.6389   -2.1957  -61.7943  100.8036 -725.0316]\n",
      "Weights: [-4.6044  0.0928 -0.7948  0.0743  0.1294]\n",
      "MSE loss: 100.8495\n",
      "Iteration: 149100\n",
      "Gradient: [  1.452  -42.7777 -48.4745 272.585  231.9866]\n",
      "Weights: [-4.6047  0.0936 -0.795   0.0742  0.1294]\n",
      "MSE loss: 100.835\n",
      "Iteration: 149200\n",
      "Gradient: [    6.792     -1.5365   -30.734     79.6656 -1358.1746]\n",
      "Weights: [-4.6034  0.0938 -0.7955  0.0742  0.1294]\n",
      "MSE loss: 100.8125\n",
      "Iteration: 149300\n",
      "Gradient: [  -6.8853  -15.3065   23.0623  195.992  -212.4598]\n",
      "Weights: [-4.6044  0.0948 -0.7958  0.0742  0.1295]\n",
      "MSE loss: 100.7927\n",
      "Iteration: 149400\n",
      "Gradient: [-16.9153 -20.683   51.606  -51.441  691.9437]\n",
      "Weights: [-4.604   0.0952 -0.7961  0.0741  0.1295]\n",
      "MSE loss: 100.7782\n",
      "Iteration: 149500\n",
      "Gradient: [ -14.9068   47.9379  108.0152  270.21   -315.3588]\n",
      "Weights: [-4.6046  0.0958 -0.7964  0.0741  0.1295]\n",
      "MSE loss: 100.7599\n",
      "Iteration: 149600\n",
      "Gradient: [-34.9739  -6.6627  59.0854  62.2759 359.5091]\n",
      "Weights: [-4.6034  0.0962 -0.7967  0.0741  0.1295]\n",
      "MSE loss: 100.7442\n",
      "Iteration: 149700\n",
      "Gradient: [-31.002  -23.1477  11.5861 116.6384 672.8153]\n",
      "Weights: [-4.6047  0.0961 -0.7969  0.0741  0.1295]\n",
      "MSE loss: 100.7329\n",
      "Iteration: 149800\n",
      "Gradient: [-31.2163   7.8117 -66.768  101.6597 176.128 ]\n",
      "Weights: [-4.6026  0.0967 -0.7972  0.0741  0.1296]\n",
      "MSE loss: 100.7207\n",
      "Iteration: 149900\n",
      "Gradient: [ -18.1057  -22.8847   58.0577  477.7919 -957.0243]\n",
      "Weights: [-4.6027  0.0968 -0.7975  0.0741  0.1296]\n",
      "MSE loss: 100.7036\n",
      "Iteration: 150000\n",
      "Gradient: [ 8.985900e+00  1.306000e-01  1.463968e+02  5.410100e+01 -3.393502e+02]\n",
      "Weights: [-4.6035  0.0975 -0.7977  0.074   0.1296]\n",
      "MSE loss: 100.687\n",
      "Iteration: 150100\n",
      "Gradient: [  34.4328  -14.4886  110.72     72.4502 -143.5344]\n",
      "Weights: [-4.6038  0.0974 -0.798   0.074   0.1296]\n",
      "MSE loss: 100.6736\n",
      "Iteration: 150200\n",
      "Gradient: [ -14.42     31.995    56.939   -12.5508 -243.6164]\n",
      "Weights: [-4.6032  0.0974 -0.7982  0.074   0.1297]\n",
      "MSE loss: 100.6649\n",
      "Iteration: 150300\n",
      "Gradient: [ -16.3639   17.7149  127.2846 -123.6062  717.1442]\n",
      "Weights: [-4.6026  0.0983 -0.7985  0.074   0.1297]\n",
      "MSE loss: 100.6519\n",
      "Iteration: 150400\n",
      "Gradient: [   9.3691  -23.5592   29.6089  215.8087 -544.5306]\n",
      "Weights: [-4.6028  0.0986 -0.7988  0.074   0.1297]\n",
      "MSE loss: 100.6367\n",
      "Iteration: 150500\n",
      "Gradient: [-17.4259 -46.7362  30.9902  29.2619 496.6613]\n",
      "Weights: [-4.6021  0.0983 -0.799   0.074   0.1297]\n",
      "MSE loss: 100.6242\n",
      "Iteration: 150600\n",
      "Gradient: [   7.3288   14.3625   11.8372   70.1634 -184.9645]\n",
      "Weights: [-4.6039  0.0991 -0.7993  0.074   0.1297]\n",
      "MSE loss: 100.6053\n",
      "Iteration: 150700\n",
      "Gradient: [ -4.453    1.3034  45.7034 105.8769 -90.4303]\n",
      "Weights: [-4.6037  0.1001 -0.7995  0.074   0.1298]\n",
      "MSE loss: 100.5899\n",
      "Iteration: 150800\n",
      "Gradient: [  52.1049   57.2424   70.9632 -111.9615 -342.4725]\n",
      "Weights: [-4.6046  0.1009 -0.7999  0.074   0.1298]\n",
      "MSE loss: 100.5713\n",
      "Iteration: 150900\n",
      "Gradient: [  29.2834    7.8488   31.7822   52.7091 -367.3685]\n",
      "Weights: [-4.6039  0.1011 -0.8003  0.074   0.1298]\n",
      "MSE loss: 100.556\n",
      "Iteration: 151000\n",
      "Gradient: [   4.4096   -8.8803  115.4883  -61.0972 -483.4897]\n",
      "Weights: [-4.6045  0.1017 -0.8004  0.074   0.1298]\n",
      "MSE loss: 100.5444\n",
      "Iteration: 151100\n",
      "Gradient: [ 37.4904  12.1845  13.3712 116.9452 -92.9763]\n",
      "Weights: [-4.6047  0.1016 -0.8007  0.074   0.1298]\n",
      "MSE loss: 100.5319\n",
      "Iteration: 151200\n",
      "Gradient: [ -2.3975   7.8553  19.8199 246.8443 142.0342]\n",
      "Weights: [-4.6038  0.1021 -0.801   0.074   0.1299]\n",
      "MSE loss: 100.5163\n",
      "Iteration: 151300\n",
      "Gradient: [ -24.7202    8.1285  125.1215  317.587  -219.4891]\n",
      "Weights: [-4.6052  0.102  -0.8014  0.074   0.1299]\n",
      "MSE loss: 100.5001\n",
      "Iteration: 151400\n",
      "Gradient: [   3.1885  -20.7513  -21.0803 -190.1673 -317.8885]\n",
      "Weights: [-4.6036  0.1026 -0.8017  0.074   0.1299]\n",
      "MSE loss: 100.4849\n",
      "Iteration: 151500\n",
      "Gradient: [  12.5925   30.486   123.2865  -43.8527 -827.0922]\n",
      "Weights: [-4.6034  0.1026 -0.8019  0.074   0.1299]\n",
      "MSE loss: 100.474\n",
      "Iteration: 151600\n",
      "Gradient: [-33.6801  53.0221  64.5338 113.4752 480.4473]\n",
      "Weights: [-4.6049  0.1034 -0.8022  0.074   0.13  ]\n",
      "MSE loss: 100.4529\n",
      "Iteration: 151700\n",
      "Gradient: [ -10.857   -25.1659  120.0348   40.4438 -716.7165]\n",
      "Weights: [-4.606   0.1042 -0.8024  0.074   0.13  ]\n",
      "MSE loss: 100.438\n",
      "Iteration: 151800\n",
      "Gradient: [   6.4551  -59.74     71.5171   79.7    -989.699 ]\n",
      "Weights: [-4.6064  0.1048 -0.8027  0.074   0.13  ]\n",
      "MSE loss: 100.4222\n",
      "Iteration: 151900\n",
      "Gradient: [  30.1027   11.0507  -11.214  -107.879  -161.454 ]\n",
      "Weights: [-4.6046  0.1051 -0.8032  0.0739  0.13  ]\n",
      "MSE loss: 100.4037\n",
      "Iteration: 152000\n",
      "Gradient: [  22.4468   46.2598    5.1284    9.7821 -294.8323]\n",
      "Weights: [-4.6073  0.1061 -0.8035  0.0739  0.13  ]\n",
      "MSE loss: 100.3882\n",
      "Iteration: 152100\n",
      "Gradient: [  35.1727   -4.3557  111.1279  184.6764 -860.8988]\n",
      "Weights: [-4.6082  0.1066 -0.8036  0.0739  0.1301]\n",
      "MSE loss: 100.3803\n",
      "Iteration: 152200\n",
      "Gradient: [  -1.6515  -71.8031  126.724   142.5428 -297.0378]\n",
      "Weights: [-4.6077  0.1071 -0.8039  0.074   0.1301]\n",
      "MSE loss: 100.3622\n",
      "Iteration: 152300\n",
      "Gradient: [  19.8778  -39.7096  -53.5857  156.0851 -567.8944]\n",
      "Weights: [-4.607   0.1079 -0.8043  0.0739  0.1301]\n",
      "MSE loss: 100.3426\n",
      "Iteration: 152400\n",
      "Gradient: [  -5.056    18.0722   54.6857 -124.3114  -30.0245]\n",
      "Weights: [-4.6064  0.1081 -0.8046  0.0739  0.1301]\n",
      "MSE loss: 100.329\n",
      "Iteration: 152500\n",
      "Gradient: [ 38.2126 -46.7576  20.8463 -13.4123 256.163 ]\n",
      "Weights: [-4.6061  0.1083 -0.8047  0.0738  0.1301]\n",
      "MSE loss: 100.3193\n",
      "Iteration: 152600\n",
      "Gradient: [  -1.7058   55.389    32.5225  449.1818 1260.2515]\n",
      "Weights: [-4.607   0.1095 -0.805   0.0739  0.1302]\n",
      "MSE loss: 100.3044\n",
      "Iteration: 152700\n",
      "Gradient: [  31.9105   26.8527  -61.0246  311.0873 -116.3418]\n",
      "Weights: [-4.6082  0.11   -0.8053  0.0738  0.1302]\n",
      "MSE loss: 100.2857\n",
      "Iteration: 152800\n",
      "Gradient: [  16.4411   71.9353   11.9559  182.3303 -543.2208]\n",
      "Weights: [-4.6087  0.1105 -0.8056  0.0738  0.1302]\n",
      "MSE loss: 100.2722\n",
      "Iteration: 152900\n",
      "Gradient: [  -7.942   -12.6522  185.8215  282.8943 -772.7434]\n",
      "Weights: [-4.61    0.1112 -0.8058  0.0738  0.1302]\n",
      "MSE loss: 100.261\n",
      "Iteration: 153000\n",
      "Gradient: [ -15.5101  -39.1234   -3.0181  317.8934 -591.8679]\n",
      "Weights: [-4.6087  0.111  -0.8061  0.0738  0.1302]\n",
      "MSE loss: 100.2509\n",
      "Iteration: 153100\n",
      "Gradient: [ -11.1654 -104.9059   25.6005   -1.6801 -558.7491]\n",
      "Weights: [-4.6094  0.112  -0.8064  0.0738  0.1303]\n",
      "MSE loss: 100.2311\n",
      "Iteration: 153200\n",
      "Gradient: [ 24.0143  39.7306  -0.6638 416.2976  18.4263]\n",
      "Weights: [-4.6084  0.1123 -0.8068  0.0738  0.1303]\n",
      "MSE loss: 100.2141\n",
      "Iteration: 153300\n",
      "Gradient: [ -17.6683  -27.6572  -68.6284 -177.2298 -486.5593]\n",
      "Weights: [-4.6086  0.1131 -0.8071  0.0738  0.1303]\n",
      "MSE loss: 100.1948\n",
      "Iteration: 153400\n",
      "Gradient: [-15.1256 -10.0874 -26.2681  -8.0163  76.3845]\n",
      "Weights: [-4.6073  0.113  -0.8074  0.0738  0.1303]\n",
      "MSE loss: 100.1847\n",
      "Iteration: 153500\n",
      "Gradient: [  32.5003  -13.0154 -136.425  -238.0145  686.6176]\n",
      "Weights: [-4.6077  0.1136 -0.8076  0.0737  0.1303]\n",
      "MSE loss: 100.1693\n",
      "Iteration: 153600\n",
      "Gradient: [ -16.4685    2.4574   -9.5473   68.7897 -509.8598]\n",
      "Weights: [-4.6081  0.1144 -0.8078  0.0737  0.1304]\n",
      "MSE loss: 100.1573\n",
      "Iteration: 153700\n",
      "Gradient: [-41.5938 -31.5695  80.5915  97.9546 451.3669]\n",
      "Weights: [-4.6079  0.1144 -0.8081  0.0737  0.1304]\n",
      "MSE loss: 100.1444\n",
      "Iteration: 153800\n",
      "Gradient: [ 21.9636 -11.2151  12.1359 101.4089 -82.7032]\n",
      "Weights: [-4.6084  0.1147 -0.8083  0.0737  0.1304]\n",
      "MSE loss: 100.1307\n",
      "Iteration: 153900\n",
      "Gradient: [  -3.1617  -87.3176   35.1512 -236.9932  108.788 ]\n",
      "Weights: [-4.61    0.1154 -0.8086  0.0736  0.1304]\n",
      "MSE loss: 100.1201\n",
      "Iteration: 154000\n",
      "Gradient: [   8.3634    5.3216   50.9944  115.7361 -475.4431]\n",
      "Weights: [-4.6086  0.1158 -0.8088  0.0737  0.1304]\n",
      "MSE loss: 100.1053\n",
      "Iteration: 154100\n",
      "Gradient: [  8.2706 -58.2554 -42.2956  25.5076 551.1619]\n",
      "Weights: [-4.6083  0.1159 -0.809   0.0736  0.1305]\n",
      "MSE loss: 100.0921\n",
      "Iteration: 154200\n",
      "Gradient: [   31.4146    14.7396   -62.1118    50.8945 -1148.3185]\n",
      "Weights: [-4.6098  0.1171 -0.8093  0.0736  0.1305]\n",
      "MSE loss: 100.0733\n",
      "Iteration: 154300\n",
      "Gradient: [ 24.906   31.8771  60.4936  86.8639 307.736 ]\n",
      "Weights: [-4.6091  0.1172 -0.8096  0.0736  0.1305]\n",
      "MSE loss: 100.06\n",
      "Iteration: 154400\n",
      "Gradient: [-36.0132  44.1771  37.7133 174.7301 874.1205]\n",
      "Weights: [-4.6092  0.1176 -0.8098  0.0736  0.1305]\n",
      "MSE loss: 100.0477\n",
      "Iteration: 154500\n",
      "Gradient: [ -5.0703  -9.1    157.8705  24.7768 -86.5782]\n",
      "Weights: [-4.6097  0.1182 -0.8102  0.0735  0.1305]\n",
      "MSE loss: 100.0297\n",
      "Iteration: 154600\n",
      "Gradient: [-25.9833 -21.9485 -21.9196 172.0713 504.2267]\n",
      "Weights: [-4.6092  0.1184 -0.8104  0.0735  0.1306]\n",
      "MSE loss: 100.0204\n",
      "Iteration: 154700\n",
      "Gradient: [   8.4338    2.9294 -118.2466 -245.3755 -830.8037]\n",
      "Weights: [-4.6113  0.1194 -0.8106  0.0735  0.1306]\n",
      "MSE loss: 100.005\n",
      "Iteration: 154800\n",
      "Gradient: [-16.1345 -13.0176  14.9502 -59.5751 -41.3138]\n",
      "Weights: [-4.611   0.1201 -0.8109  0.0735  0.1306]\n",
      "MSE loss: 99.9909\n",
      "Iteration: 154900\n",
      "Gradient: [ -23.0364    3.9615  -82.2932  260.2561 -472.2925]\n",
      "Weights: [-4.6118  0.1205 -0.8112  0.0735  0.1306]\n",
      "MSE loss: 99.9775\n",
      "Iteration: 155000\n",
      "Gradient: [ -13.8471    9.2446    5.7475  -45.1972 -133.7963]\n",
      "Weights: [-4.6124  0.1217 -0.8116  0.0735  0.1306]\n",
      "MSE loss: 99.9565\n",
      "Iteration: 155100\n",
      "Gradient: [ 15.0148  39.3953 112.4843  72.1427 -14.6174]\n",
      "Weights: [-4.6116  0.1222 -0.8118  0.0735  0.1307]\n",
      "MSE loss: 99.9449\n",
      "Iteration: 155200\n",
      "Gradient: [   1.7582   67.3643  -16.6107 -106.7939 -672.8369]\n",
      "Weights: [-4.6109  0.1221 -0.812   0.0734  0.1307]\n",
      "MSE loss: 99.9329\n",
      "Iteration: 155300\n",
      "Gradient: [   9.7029    6.0211  -47.9826  665.1793 -715.1057]\n",
      "Weights: [-4.6116  0.1224 -0.8123  0.0734  0.1307]\n",
      "MSE loss: 99.9188\n",
      "Iteration: 155400\n",
      "Gradient: [  15.7358  -26.5621   79.1022 -149.9914  250.0534]\n",
      "Weights: [-4.6123  0.1236 -0.8127  0.0734  0.1307]\n",
      "MSE loss: 99.8985\n",
      "Iteration: 155500\n",
      "Gradient: [ -27.3362   23.1132 -100.2437  -81.8344  311.0225]\n",
      "Weights: [-4.6129  0.1249 -0.813   0.0734  0.1307]\n",
      "MSE loss: 99.8801\n",
      "Iteration: 155600\n",
      "Gradient: [ -22.9504   48.4855 -134.2115 -193.6725 -636.1479]\n",
      "Weights: [-4.6126  0.1254 -0.8133  0.0733  0.1308]\n",
      "MSE loss: 99.865\n",
      "Iteration: 155700\n",
      "Gradient: [  12.3812  -40.9069   71.0596  171.4513 -414.9351]\n",
      "Weights: [-4.6121  0.125  -0.8136  0.0733  0.1308]\n",
      "MSE loss: 99.8576\n",
      "Iteration: 155800\n",
      "Gradient: [ -10.4618   20.269    24.37    -90.1279 -523.5911]\n",
      "Weights: [-4.6118  0.1259 -0.8138  0.0733  0.1308]\n",
      "MSE loss: 99.8432\n",
      "Iteration: 155900\n",
      "Gradient: [ 14.5687 -22.8402 220.3656 118.6056 184.6736]\n",
      "Weights: [-4.612   0.1261 -0.8141  0.0733  0.1308]\n",
      "MSE loss: 99.8296\n",
      "Iteration: 156000\n",
      "Gradient: [   3.1504   36.4256   42.2532  -98.581  -324.5332]\n",
      "Weights: [-4.6123  0.127  -0.8145  0.0733  0.1308]\n",
      "MSE loss: 99.8088\n",
      "Iteration: 156100\n",
      "Gradient: [ -19.275   -12.0625   25.6602   64.7374 -374.3369]\n",
      "Weights: [-4.6134  0.1273 -0.8147  0.0732  0.1309]\n",
      "MSE loss: 99.7964\n",
      "Iteration: 156200\n",
      "Gradient: [   9.7317   12.5284   59.8781 -119.524  1004.1848]\n",
      "Weights: [-4.6135  0.1275 -0.8149  0.0732  0.1309]\n",
      "MSE loss: 99.7876\n",
      "Iteration: 156300\n",
      "Gradient: [  -1.9419  -63.4057   98.6837  209.364  1036.8329]\n",
      "Weights: [-4.6126  0.1282 -0.815   0.0732  0.1309]\n",
      "MSE loss: 99.7771\n",
      "Iteration: 156400\n",
      "Gradient: [   7.8225  -38.0818  111.3441   57.8134 -646.4074]\n",
      "Weights: [-4.6126  0.1283 -0.8152  0.0732  0.1309]\n",
      "MSE loss: 99.7666\n",
      "Iteration: 156500\n",
      "Gradient: [ -12.6421  -14.7792 -118.2238 -340.2337 -997.8918]\n",
      "Weights: [-4.6143  0.1288 -0.8155  0.0731  0.1309]\n",
      "MSE loss: 99.7542\n",
      "Iteration: 156600\n",
      "Gradient: [ -12.7279  -26.0306  -78.4138  -38.5054 1111.9593]\n",
      "Weights: [-4.6139  0.1296 -0.8157  0.0731  0.131 ]\n",
      "MSE loss: 99.7354\n",
      "Iteration: 156700\n",
      "Gradient: [  37.8107   10.6289   63.6609 -101.4506  217.9614]\n",
      "Weights: [-4.6153  0.1304 -0.8159  0.0731  0.131 ]\n",
      "MSE loss: 99.7264\n",
      "Iteration: 156800\n",
      "Gradient: [ -17.6511  -50.5232   60.8438  235.2379 -824.6848]\n",
      "Weights: [-4.6131  0.1307 -0.8164  0.0732  0.131 ]\n",
      "MSE loss: 99.7095\n",
      "Iteration: 156900\n",
      "Gradient: [   30.596     -1.1134   202.8292   184.5093 -1020.3117]\n",
      "Weights: [-4.6136  0.131  -0.8167  0.0731  0.131 ]\n",
      "MSE loss: 99.6934\n",
      "Iteration: 157000\n",
      "Gradient: [ -2.8714   6.8975  51.1314 -53.9304 534.9199]\n",
      "Weights: [-4.6142  0.1316 -0.8169  0.0731  0.131 ]\n",
      "MSE loss: 99.6771\n",
      "Iteration: 157100\n",
      "Gradient: [-17.1201   6.9986   4.5583 -38.6402 178.3122]\n",
      "Weights: [-4.615   0.1321 -0.8171  0.0731  0.1311]\n",
      "MSE loss: 99.6677\n",
      "Iteration: 157200\n",
      "Gradient: [   6.2868   27.9002 -129.5926   76.2489   72.6254]\n",
      "Weights: [-4.6167  0.1328 -0.8174  0.0731  0.1311]\n",
      "MSE loss: 99.6568\n",
      "Iteration: 157300\n",
      "Gradient: [  12.4052   27.9106   38.982  -433.5533 -113.0379]\n",
      "Weights: [-4.6149  0.1334 -0.8176  0.0731  0.1311]\n",
      "MSE loss: 99.6431\n",
      "Iteration: 157400\n",
      "Gradient: [    8.8953     3.2554   108.2698   208.4902 -1238.3965]\n",
      "Weights: [-4.6145  0.1338 -0.818   0.0731  0.1311]\n",
      "MSE loss: 99.6277\n",
      "Iteration: 157500\n",
      "Gradient: [  8.0685 -25.9081  33.6598 121.7817 284.9197]\n",
      "Weights: [-4.6156  0.1337 -0.8182  0.0731  0.1311]\n",
      "MSE loss: 99.6153\n",
      "Iteration: 157600\n",
      "Gradient: [ 14.8536  44.3204 -11.9291  66.2812 595.8652]\n",
      "Weights: [-4.6159  0.1338 -0.8184  0.073   0.1311]\n",
      "MSE loss: 99.6081\n",
      "Iteration: 157700\n",
      "Gradient: [-2.300000e-01 -5.702060e+01  7.764990e+01  1.978591e+02 -3.472404e+02]\n",
      "Weights: [-4.6156  0.1347 -0.8187  0.073   0.1312]\n",
      "MSE loss: 99.5884\n",
      "Iteration: 157800\n",
      "Gradient: [  11.1079  -17.5038 -218.1264   78.6489 -848.4586]\n",
      "Weights: [-4.6162  0.1351 -0.819   0.073   0.1312]\n",
      "MSE loss: 99.5755\n",
      "Iteration: 157900\n",
      "Gradient: [ 12.3606 -55.4042  -3.5806 234.0777 382.6005]\n",
      "Weights: [-4.6163  0.1354 -0.8192  0.073   0.1312]\n",
      "MSE loss: 99.5613\n",
      "Iteration: 158000\n",
      "Gradient: [-22.4226 -44.7178  15.6483  85.5277 101.0599]\n",
      "Weights: [-4.6164  0.1362 -0.8194  0.073   0.1312]\n",
      "MSE loss: 99.5503\n",
      "Iteration: 158100\n",
      "Gradient: [  26.3605   55.4798 -175.2125   35.6723  404.3344]\n",
      "Weights: [-4.6163  0.1369 -0.8197  0.073   0.1312]\n",
      "MSE loss: 99.5338\n",
      "Iteration: 158200\n",
      "Gradient: [-8.930000e-02  3.460110e+01  7.255430e+01 -1.224932e+02 -4.152550e+01]\n",
      "Weights: [-4.6174  0.1379 -0.8201  0.073   0.1313]\n",
      "MSE loss: 99.5156\n",
      "Iteration: 158300\n",
      "Gradient: [  20.0598  -31.0551   35.0138 -242.1946  -40.4992]\n",
      "Weights: [-4.6158  0.138  -0.8205  0.073   0.1313]\n",
      "MSE loss: 99.5022\n",
      "Iteration: 158400\n",
      "Gradient: [ -22.1806   60.5177 -106.9705 -165.9656  367.6663]\n",
      "Weights: [-4.6153  0.1386 -0.8209  0.0729  0.1313]\n",
      "MSE loss: 99.4869\n",
      "Iteration: 158500\n",
      "Gradient: [  21.1564   -1.4959    4.4612 -271.1317  222.1477]\n",
      "Weights: [-4.6189  0.1391 -0.821   0.073   0.1313]\n",
      "MSE loss: 99.4802\n",
      "Iteration: 158600\n",
      "Gradient: [ -10.8686  -25.9352 -100.8574   42.3116 -897.0051]\n",
      "Weights: [-4.6186  0.1401 -0.8213  0.073   0.1314]\n",
      "MSE loss: 99.4571\n",
      "Iteration: 158700\n",
      "Gradient: [  19.8676    1.4746   71.9257 -550.5426 -766.413 ]\n",
      "Weights: [-4.618   0.1406 -0.8217  0.0729  0.1314]\n",
      "MSE loss: 99.4412\n",
      "Iteration: 158800\n",
      "Gradient: [  -5.7879  -29.604   -93.6769 -268.7756 -534.9121]\n",
      "Weights: [-4.6191  0.1411 -0.8219  0.0729  0.1314]\n",
      "MSE loss: 99.4306\n",
      "Iteration: 158900\n",
      "Gradient: [   7.7745    3.9231    4.4067 -174.9907 -129.8549]\n",
      "Weights: [-4.6193  0.1419 -0.8221  0.0729  0.1314]\n",
      "MSE loss: 99.417\n",
      "Iteration: 159000\n",
      "Gradient: [  35.9401  -80.9924  -51.1231  -59.2516 -909.0244]\n",
      "Weights: [-4.6181  0.1422 -0.8224  0.0729  0.1314]\n",
      "MSE loss: 99.403\n",
      "Iteration: 159100\n",
      "Gradient: [  46.4885    1.4945  -64.1944 -100.2615  178.1857]\n",
      "Weights: [-4.6176  0.1423 -0.8226  0.0728  0.1314]\n",
      "MSE loss: 99.3936\n",
      "Iteration: 159200\n",
      "Gradient: [ 19.1421 -78.0033 137.1565  36.7127 193.2972]\n",
      "Weights: [-4.6183  0.1425 -0.8229  0.0729  0.1315]\n",
      "MSE loss: 99.3805\n",
      "Iteration: 159300\n",
      "Gradient: [ 29.8372 -18.497   -2.6678 -21.0831 525.4232]\n",
      "Weights: [-4.6188  0.143  -0.8231  0.0729  0.1315]\n",
      "MSE loss: 99.3698\n",
      "Iteration: 159400\n",
      "Gradient: [-14.0751  35.0137  37.464  -89.2936 296.4804]\n",
      "Weights: [-4.62    0.1439 -0.8233  0.0729  0.1315]\n",
      "MSE loss: 99.3562\n",
      "Iteration: 159500\n",
      "Gradient: [ -12.1508   25.3774  -54.9726  215.1553 -843.2894]\n",
      "Weights: [-4.6184  0.1439 -0.8237  0.0729  0.1315]\n",
      "MSE loss: 99.3425\n",
      "Iteration: 159600\n",
      "Gradient: [ -22.8679    8.5037   14.1804  -44.9894 -498.4415]\n",
      "Weights: [-4.6179  0.1445 -0.8241  0.0728  0.1315]\n",
      "MSE loss: 99.325\n",
      "Iteration: 159700\n",
      "Gradient: [ -6.373   20.5678  98.771   43.0287 393.1655]\n",
      "Weights: [-4.6188  0.145  -0.8245  0.0728  0.1316]\n",
      "MSE loss: 99.3087\n",
      "Iteration: 159800\n",
      "Gradient: [  2.544  -29.7729  54.5472 294.597  317.9025]\n",
      "Weights: [-4.621   0.1457 -0.8247  0.0729  0.1316]\n",
      "MSE loss: 99.2986\n",
      "Iteration: 159900\n",
      "Gradient: [ -40.3305  -15.0944  -50.5759  -94.9374 -109.4437]\n",
      "Weights: [-4.6205  0.1463 -0.8249  0.0729  0.1316]\n",
      "MSE loss: 99.2843\n",
      "Iteration: 160000\n",
      "Gradient: [ -19.1556  -24.3701   23.0552 -283.0043  -13.4688]\n",
      "Weights: [-4.6203  0.147  -0.8252  0.0728  0.1316]\n",
      "MSE loss: 99.2699\n",
      "Iteration: 160100\n",
      "Gradient: [   1.7622  -29.5045  -68.9201  -40.994  -110.6187]\n",
      "Weights: [-4.6212  0.1474 -0.8254  0.0728  0.1316]\n",
      "MSE loss: 99.2577\n",
      "Iteration: 160200\n",
      "Gradient: [ 35.9025 -14.7963 232.0999 402.0229 482.3437]\n",
      "Weights: [-4.6201  0.1476 -0.8258  0.0728  0.1317]\n",
      "MSE loss: 99.243\n",
      "Iteration: 160300\n",
      "Gradient: [-25.2293 -38.1234 188.3326 223.2051 340.8041]\n",
      "Weights: [-4.6223  0.1486 -0.8259  0.0728  0.1317]\n",
      "MSE loss: 99.2348\n",
      "Iteration: 160400\n",
      "Gradient: [  22.4519   12.1899    7.8098  200.776  -231.2488]\n",
      "Weights: [-4.621   0.1496 -0.8262  0.0727  0.1317]\n",
      "MSE loss: 99.2162\n",
      "Iteration: 160500\n",
      "Gradient: [ 11.8891 -22.1347 120.083  135.6756 -90.3941]\n",
      "Weights: [-4.6219  0.1496 -0.8265  0.0727  0.1317]\n",
      "MSE loss: 99.2082\n",
      "Iteration: 160600\n",
      "Gradient: [ 16.0142 -22.5178 -58.0512 108.9918 230.9798]\n",
      "Weights: [-4.6219  0.1498 -0.8267  0.0727  0.1317]\n",
      "MSE loss: 99.1982\n",
      "Iteration: 160700\n",
      "Gradient: [  34.9776   -6.9431   97.5976  214.3652 -812.9788]\n",
      "Weights: [-4.6213  0.1508 -0.8269  0.0727  0.1317]\n",
      "MSE loss: 99.1855\n",
      "Iteration: 160800\n",
      "Gradient: [-39.4912 -15.352  -94.8608 144.3045 349.7833]\n",
      "Weights: [-4.6218  0.1506 -0.8271  0.0727  0.1318]\n",
      "MSE loss: 99.1757\n",
      "Iteration: 160900\n",
      "Gradient: [-1.96260e+01  3.55302e+01 -1.92500e-01 -3.31770e+01 -9.04089e+02]\n",
      "Weights: [-4.6225  0.1512 -0.8274  0.0727  0.1318]\n",
      "MSE loss: 99.1651\n",
      "Iteration: 161000\n",
      "Gradient: [  -1.272   -20.7572  147.8868  -21.2768 -493.2025]\n",
      "Weights: [-4.622   0.1518 -0.8276  0.0727  0.1318]\n",
      "MSE loss: 99.1506\n",
      "Iteration: 161100\n",
      "Gradient: [  -4.8622   51.0534  -29.9758 -533.5708  -43.4496]\n",
      "Weights: [-4.6217  0.1523 -0.8279  0.0727  0.1318]\n",
      "MSE loss: 99.1345\n",
      "Iteration: 161200\n",
      "Gradient: [   16.8965    15.7809    69.6899  -277.6834 -1234.3695]\n",
      "Weights: [-4.6223  0.1534 -0.8283  0.0726  0.1318]\n",
      "MSE loss: 99.1167\n",
      "Iteration: 161300\n",
      "Gradient: [ -22.5606   27.6026   45.1852 -102.4075 -100.7987]\n",
      "Weights: [-4.6225  0.1536 -0.8286  0.0726  0.1319]\n",
      "MSE loss: 99.1006\n",
      "Iteration: 161400\n",
      "Gradient: [ -18.575    31.0299  -64.6652 -238.4905 -490.4098]\n",
      "Weights: [-4.6204  0.1537 -0.8289  0.0726  0.1319]\n",
      "MSE loss: 99.0932\n",
      "Iteration: 161500\n",
      "Gradient: [   12.3128   -35.8057    86.7786    90.1918 -1239.9252]\n",
      "Weights: [-4.6201  0.1532 -0.8292  0.0726  0.1319]\n",
      "MSE loss: 99.077\n",
      "Iteration: 161600\n",
      "Gradient: [ -0.5443  37.9421 -59.1362 136.1994 491.4529]\n",
      "Weights: [-4.619   0.1534 -0.8295  0.0726  0.1319]\n",
      "MSE loss: 99.0691\n",
      "Iteration: 161700\n",
      "Gradient: [  12.5717  -32.5545  173.8069  308.9945 -867.5514]\n",
      "Weights: [-4.62    0.1538 -0.8296  0.0726  0.132 ]\n",
      "MSE loss: 99.0564\n",
      "Iteration: 161800\n",
      "Gradient: [   5.23    -64.3293  145.2944  108.7192 -115.4678]\n",
      "Weights: [-4.6207  0.1544 -0.83    0.0726  0.132 ]\n",
      "MSE loss: 99.0366\n",
      "Iteration: 161900\n",
      "Gradient: [ -16.7574  -34.552    38.6483  139.2112 -254.8474]\n",
      "Weights: [-4.6207  0.1545 -0.8303  0.0726  0.132 ]\n",
      "MSE loss: 99.0245\n",
      "Iteration: 162000\n",
      "Gradient: [  9.0657  -6.175  224.9629 -65.8382  42.3987]\n",
      "Weights: [-4.6201  0.1555 -0.8308  0.0726  0.132 ]\n",
      "MSE loss: 99.0084\n",
      "Iteration: 162100\n",
      "Gradient: [  5.9537 -61.2196 -31.9192  14.4109 387.9168]\n",
      "Weights: [-4.6226  0.1566 -0.8311  0.0726  0.132 ]\n",
      "MSE loss: 98.9877\n",
      "Iteration: 162200\n",
      "Gradient: [  24.8053   15.833    70.6643 -132.8641 -665.494 ]\n",
      "Weights: [-4.6225  0.1569 -0.8314  0.0726  0.1321]\n",
      "MSE loss: 98.9771\n",
      "Iteration: 162300\n",
      "Gradient: [  -7.2312   64.7384 -147.7414  104.9258  109.5315]\n",
      "Weights: [-4.6242  0.1576 -0.8316  0.0726  0.1321]\n",
      "MSE loss: 98.9687\n",
      "Iteration: 162400\n",
      "Gradient: [ 36.7843 -21.7251  37.6542 180.6974 538.3736]\n",
      "Weights: [-4.6242  0.1583 -0.8318  0.0726  0.1321]\n",
      "MSE loss: 98.9538\n",
      "Iteration: 162500\n",
      "Gradient: [  -3.4154  -13.201   -14.345  -304.095  -703.8222]\n",
      "Weights: [-4.6237  0.1589 -0.8321  0.0726  0.1321]\n",
      "MSE loss: 98.9375\n",
      "Iteration: 162600\n",
      "Gradient: [   3.9172   27.857    -2.9193 -330.1989  -69.5412]\n",
      "Weights: [-4.6228  0.1593 -0.8325  0.0725  0.1321]\n",
      "MSE loss: 98.9232\n",
      "Iteration: 162700\n",
      "Gradient: [  17.7726   33.3279  -78.2828   70.3201 -239.9021]\n",
      "Weights: [-4.6232  0.1596 -0.8327  0.0726  0.1321]\n",
      "MSE loss: 98.9135\n",
      "Iteration: 162800\n",
      "Gradient: [ -31.7725  -51.7044   85.0104  245.414  -671.6232]\n",
      "Weights: [-4.6239  0.16   -0.8328  0.0726  0.1322]\n",
      "MSE loss: 98.9048\n",
      "Iteration: 162900\n",
      "Gradient: [ -27.4339  -37.0676  -73.2281   -0.7954 -540.6356]\n",
      "Weights: [-4.6244  0.1596 -0.8331  0.0726  0.1322]\n",
      "MSE loss: 98.8997\n",
      "Iteration: 163000\n",
      "Gradient: [ -9.2629 -14.0384  28.5247 -63.0585 590.8177]\n",
      "Weights: [-4.6257  0.1606 -0.8333  0.0726  0.1322]\n",
      "MSE loss: 98.8901\n",
      "Iteration: 163100\n",
      "Gradient: [   6.1468  -15.4605   -1.7883   91.8859 -149.0377]\n",
      "Weights: [-4.6247  0.161  -0.8337  0.0726  0.1322]\n",
      "MSE loss: 98.8708\n",
      "Iteration: 163200\n",
      "Gradient: [  24.2207  -13.6782   24.3667 -281.3498   -7.9023]\n",
      "Weights: [-4.6249  0.162  -0.8339  0.0725  0.1322]\n",
      "MSE loss: 98.8552\n",
      "Iteration: 163300\n",
      "Gradient: [  17.9471   -9.0207   65.845   -47.6469 -649.9506]\n",
      "Weights: [-4.6253  0.1626 -0.8344  0.0725  0.1322]\n",
      "MSE loss: 98.8375\n",
      "Iteration: 163400\n",
      "Gradient: [  -6.6558   30.352   -79.5548   48.7871 -351.7543]\n",
      "Weights: [-4.6239  0.1627 -0.8347  0.0725  0.1323]\n",
      "MSE loss: 98.8242\n",
      "Iteration: 163500\n",
      "Gradient: [ 40.4508  44.0972  45.5872 168.9124  33.4058]\n",
      "Weights: [-4.6244  0.1629 -0.835   0.0726  0.1323]\n",
      "MSE loss: 98.8106\n",
      "Iteration: 163600\n",
      "Gradient: [  -3.47    -72.4121   63.3454   66.9023 -628.5384]\n",
      "Weights: [-4.6249  0.164  -0.8354  0.0725  0.1323]\n",
      "MSE loss: 98.793\n",
      "Iteration: 163700\n",
      "Gradient: [   -1.4595    -2.2609    11.7972    32.7206 -1161.5177]\n",
      "Weights: [-4.6254  0.1646 -0.8356  0.0726  0.1323]\n",
      "MSE loss: 98.7788\n",
      "Iteration: 163800\n",
      "Gradient: [  -8.3404   26.255   -82.9094  111.4156 -998.6162]\n",
      "Weights: [-4.6241  0.165  -0.8361  0.0725  0.1324]\n",
      "MSE loss: 98.7618\n",
      "Iteration: 163900\n",
      "Gradient: [  -5.2066   45.5988  122.6137 -178.4284 -376.5193]\n",
      "Weights: [-4.6259  0.1654 -0.8363  0.0725  0.1324]\n",
      "MSE loss: 98.7531\n",
      "Iteration: 164000\n",
      "Gradient: [-20.2431  29.3348 100.9051   3.4809 163.3387]\n",
      "Weights: [-4.6257  0.1658 -0.8365  0.0725  0.1324]\n",
      "MSE loss: 98.7411\n",
      "Iteration: 164100\n",
      "Gradient: [  7.1834  37.5562  -7.5343 -37.3711  -8.5159]\n",
      "Weights: [-4.6239  0.1659 -0.8369  0.0725  0.1324]\n",
      "MSE loss: 98.7266\n",
      "Iteration: 164200\n",
      "Gradient: [  -9.8989    8.9513  -44.9319  382.904  -552.1553]\n",
      "Weights: [-4.6241  0.166  -0.8371  0.0725  0.1324]\n",
      "MSE loss: 98.7167\n",
      "Iteration: 164300\n",
      "Gradient: [  40.6659  -21.593     3.1729 -126.3453  185.6931]\n",
      "Weights: [-4.6233  0.1663 -0.8374  0.0725  0.1325]\n",
      "MSE loss: 98.7063\n",
      "Iteration: 164400\n",
      "Gradient: [   8.7179   40.5576  -62.7089  142.4161 -787.6553]\n",
      "Weights: [-4.6244  0.167  -0.8376  0.0725  0.1325]\n",
      "MSE loss: 98.6917\n",
      "Iteration: 164500\n",
      "Gradient: [  -25.8918    33.7798   -30.8385    35.453  -1209.3712]\n",
      "Weights: [-4.6238  0.1665 -0.8378  0.0725  0.1325]\n",
      "MSE loss: 98.6852\n",
      "Iteration: 164600\n",
      "Gradient: [ -22.7421   34.4369   21.6748 -377.4294 -192.5424]\n",
      "Weights: [-4.623   0.1667 -0.8381  0.0725  0.1325]\n",
      "MSE loss: 98.6719\n",
      "Iteration: 164700\n",
      "Gradient: [  10.7302  -18.6712   17.0475   47.39   -512.567 ]\n",
      "Weights: [-4.624   0.1674 -0.8384  0.0725  0.1326]\n",
      "MSE loss: 98.6565\n",
      "Iteration: 164800\n",
      "Gradient: [   3.9616   -2.5345   26.9914 -159.3767 -655.5549]\n",
      "Weights: [-4.6237  0.1676 -0.8386  0.0725  0.1326]\n",
      "MSE loss: 98.6462\n",
      "Iteration: 164900\n",
      "Gradient: [  -5.6956   18.9184  104.2011  -79.4674 -713.0096]\n",
      "Weights: [-4.6241  0.1677 -0.8387  0.0724  0.1326]\n",
      "MSE loss: 98.6404\n",
      "Iteration: 165000\n",
      "Gradient: [-31.2767   5.3018  54.1499 -55.274   17.5793]\n",
      "Weights: [-4.625   0.1684 -0.839   0.0724  0.1326]\n",
      "MSE loss: 98.6262\n",
      "Iteration: 165100\n",
      "Gradient: [ -17.7361  -37.2554   89.2541 -264.1795 -238.7919]\n",
      "Weights: [-4.6258  0.1688 -0.8391  0.0724  0.1326]\n",
      "MSE loss: 98.6183\n",
      "Iteration: 165200\n",
      "Gradient: [   2.6144  -36.8097  -81.8662  548.1866 -464.9582]\n",
      "Weights: [-4.6255  0.1693 -0.8394  0.0724  0.1327]\n",
      "MSE loss: 98.6046\n",
      "Iteration: 165300\n",
      "Gradient: [  28.3885   -1.3688   15.9989  238.8119 -360.5882]\n",
      "Weights: [-4.6259  0.1692 -0.8395  0.0724  0.1327]\n",
      "MSE loss: 98.6012\n",
      "Iteration: 165400\n",
      "Gradient: [ -17.619    19.4603  -70.2748  -69.5464 -657.9763]\n",
      "Weights: [-4.626   0.17   -0.8397  0.0724  0.1327]\n",
      "MSE loss: 98.5866\n",
      "Iteration: 165500\n",
      "Gradient: [  22.5692  -61.5504  -15.3442 -381.852   395.1928]\n",
      "Weights: [-4.626   0.1703 -0.8399  0.0724  0.1327]\n",
      "MSE loss: 98.5774\n",
      "Iteration: 165600\n",
      "Gradient: [  19.6741  -35.4144   -5.5908 -204.8695 -722.5096]\n",
      "Weights: [-4.6269  0.1711 -0.8402  0.0724  0.1327]\n",
      "MSE loss: 98.5649\n",
      "Iteration: 165700\n",
      "Gradient: [-26.7341 -64.335  -26.6784 234.1941  99.7932]\n",
      "Weights: [-4.6265  0.1713 -0.8405  0.0724  0.1327]\n",
      "MSE loss: 98.5542\n",
      "Iteration: 165800\n",
      "Gradient: [-2.593350e+01  1.575360e+01  6.028490e+01  3.790000e-01 -7.880348e+02]\n",
      "Weights: [-4.6261  0.1725 -0.8408  0.0724  0.1328]\n",
      "MSE loss: 98.54\n",
      "Iteration: 165900\n",
      "Gradient: [  19.1206  -21.1278  173.5828  -86.514  -453.1757]\n",
      "Weights: [-4.6272  0.173  -0.8411  0.0723  0.1328]\n",
      "MSE loss: 98.5258\n",
      "Iteration: 166000\n",
      "Gradient: [-9.1789000e+00  1.0344000e+00  5.1614000e+01 -1.7806510e+02\n",
      " -1.2070429e+03]\n",
      "Weights: [-4.6271  0.1736 -0.8415  0.0723  0.1328]\n",
      "MSE loss: 98.5102\n",
      "Iteration: 166100\n",
      "Gradient: [  31.0243   -6.5659  -29.8505 -164.8347 -853.9536]\n",
      "Weights: [-4.6271  0.1742 -0.8419  0.0723  0.1328]\n",
      "MSE loss: 98.4951\n",
      "Iteration: 166200\n",
      "Gradient: [-37.9483  37.8051 -38.0725   3.4489 793.72  ]\n",
      "Weights: [-4.6279  0.1744 -0.8421  0.0723  0.1328]\n",
      "MSE loss: 98.4863\n",
      "Iteration: 166300\n",
      "Gradient: [  -7.3349   50.0113    8.6828 -151.1344  255.7884]\n",
      "Weights: [-4.6286  0.1752 -0.8423  0.0723  0.1328]\n",
      "MSE loss: 98.4749\n",
      "Iteration: 166400\n",
      "Gradient: [  17.2476    2.07     47.519   -39.7176 -771.1155]\n",
      "Weights: [-4.6278  0.1758 -0.8426  0.0723  0.1329]\n",
      "MSE loss: 98.4599\n",
      "Iteration: 166500\n",
      "Gradient: [ -17.6687    7.3746   57.8853   93.4029 -854.2224]\n",
      "Weights: [-4.6276  0.1757 -0.8429  0.0723  0.1329]\n",
      "MSE loss: 98.4495\n",
      "Iteration: 166600\n",
      "Gradient: [  25.3552    3.6358    9.1441  -90.6389 -280.6482]\n",
      "Weights: [-4.6282  0.1759 -0.8431  0.0723  0.1329]\n",
      "MSE loss: 98.4395\n",
      "Iteration: 166700\n",
      "Gradient: [  5.0158 -35.5896  69.8504  85.0872 420.1896]\n",
      "Weights: [-4.6282  0.177  -0.8434  0.0723  0.1329]\n",
      "MSE loss: 98.4251\n",
      "Iteration: 166800\n",
      "Gradient: [-15.9921   1.1104   9.4911  72.5751  84.7835]\n",
      "Weights: [-4.6269  0.1771 -0.8436  0.0723  0.1329]\n",
      "MSE loss: 98.4183\n",
      "Iteration: 166900\n",
      "Gradient: [-37.4363  31.5804 -26.4923 -75.8513 -94.8917]\n",
      "Weights: [-4.6279  0.1776 -0.8439  0.0723  0.1329]\n",
      "MSE loss: 98.4043\n",
      "Iteration: 167000\n",
      "Gradient: [ -23.1303  -13.3067  -34.8364  104.5285 -959.9162]\n",
      "Weights: [-4.6288  0.1778 -0.8441  0.0723  0.133 ]\n",
      "MSE loss: 98.3968\n",
      "Iteration: 167100\n",
      "Gradient: [ -3.1154 -33.5191  24.1706 245.3006 496.2749]\n",
      "Weights: [-4.629   0.1789 -0.8442  0.0723  0.133 ]\n",
      "MSE loss: 98.385\n",
      "Iteration: 167200\n",
      "Gradient: [ 2.242430e+01  3.099030e+01 -4.408950e+01 -3.580000e-01 -1.346302e+03]\n",
      "Weights: [-4.6289  0.1793 -0.8445  0.0722  0.133 ]\n",
      "MSE loss: 98.3747\n",
      "Iteration: 167300\n",
      "Gradient: [ -3.7204 -23.0809 -18.3766 205.4501 296.5333]\n",
      "Weights: [-4.6289  0.1799 -0.8446  0.0722  0.133 ]\n",
      "MSE loss: 98.3664\n",
      "Iteration: 167400\n",
      "Gradient: [  -3.0658   15.9355   55.8811   84.4037 -454.8081]\n",
      "Weights: [-4.6295  0.1803 -0.8448  0.0722  0.133 ]\n",
      "MSE loss: 98.3554\n",
      "Iteration: 167500\n",
      "Gradient: [  27.3538  -46.0019   84.4789  346.3204 -631.9426]\n",
      "Weights: [-4.6307  0.1807 -0.8451  0.0722  0.133 ]\n",
      "MSE loss: 98.3445\n",
      "Iteration: 167600\n",
      "Gradient: [ -24.7951   25.432    32.4039 -169.7942 -107.8046]\n",
      "Weights: [-4.6308  0.1814 -0.8454  0.0722  0.133 ]\n",
      "MSE loss: 98.3314\n",
      "Iteration: 167700\n",
      "Gradient: [ -8.9874  -3.9889 -20.1704 -42.6564 -80.4903]\n",
      "Weights: [-4.6293  0.1815 -0.8455  0.0722  0.1331]\n",
      "MSE loss: 98.3244\n",
      "Iteration: 167800\n",
      "Gradient: [ -61.4108   -9.1779   -6.4622 -534.4362 -590.7809]\n",
      "Weights: [-4.6308  0.1822 -0.8457  0.0721  0.1331]\n",
      "MSE loss: 98.3134\n",
      "Iteration: 167900\n",
      "Gradient: [ -5.0498  38.394   29.7412 168.1619 640.2634]\n",
      "Weights: [-4.6303  0.1831 -0.846   0.0721  0.1331]\n",
      "MSE loss: 98.2995\n",
      "Iteration: 168000\n",
      "Gradient: [ -36.9598   35.9828   26.7317  305.1643 -618.2897]\n",
      "Weights: [-4.6308  0.1834 -0.8463  0.0721  0.1331]\n",
      "MSE loss: 98.2886\n",
      "Iteration: 168100\n",
      "Gradient: [-2.269000e-01 -1.422220e+01  2.767340e+01  6.381490e+01  3.799859e+02]\n",
      "Weights: [-4.6307  0.1841 -0.8465  0.0721  0.1331]\n",
      "MSE loss: 98.2774\n",
      "Iteration: 168200\n",
      "Gradient: [  -6.6589   29.0243  -30.1894 -410.9901 -802.2511]\n",
      "Weights: [-4.6308  0.1843 -0.8468  0.0721  0.1331]\n",
      "MSE loss: 98.2651\n",
      "Iteration: 168300\n",
      "Gradient: [    8.8005    -8.0945    62.2082    77.2206 -1388.6254]\n",
      "Weights: [-4.6291  0.1841 -0.8471  0.0721  0.1332]\n",
      "MSE loss: 98.2568\n",
      "Iteration: 168400\n",
      "Gradient: [  17.2942  -19.791    21.1304 -108.655  -621.1447]\n",
      "Weights: [-4.6305  0.1847 -0.8475  0.0721  0.1332]\n",
      "MSE loss: 98.2422\n",
      "Iteration: 168500\n",
      "Gradient: [  37.9791   -8.081   -97.3433 -140.4343  322.9905]\n",
      "Weights: [-4.6296  0.1856 -0.8477  0.072   0.1332]\n",
      "MSE loss: 98.2296\n",
      "Iteration: 168600\n",
      "Gradient: [ -5.6845 -54.6297  52.5044 175.0757 837.6275]\n",
      "Weights: [-4.632   0.1862 -0.8478  0.0721  0.1332]\n",
      "MSE loss: 98.2188\n",
      "Iteration: 168700\n",
      "Gradient: [   5.6019   15.6337 -151.7125 -221.3368 -138.7635]\n",
      "Weights: [-4.6319  0.1869 -0.8482  0.072   0.1332]\n",
      "MSE loss: 98.2043\n",
      "Iteration: 168800\n",
      "Gradient: [  15.6443  -19.6513   -3.6132 -104.3861 -275.7675]\n",
      "Weights: [-4.6304  0.187  -0.8485  0.072   0.1332]\n",
      "MSE loss: 98.1941\n",
      "Iteration: 168900\n",
      "Gradient: [ -29.03     47.5905  -42.6507 -119.1273 -520.1346]\n",
      "Weights: [-4.6322  0.1878 -0.8488  0.0721  0.1333]\n",
      "MSE loss: 98.1776\n",
      "Iteration: 169000\n",
      "Gradient: [  11.5471   22.4832  -21.4342   31.0148 -346.5   ]\n",
      "Weights: [-4.6314  0.1878 -0.8492  0.0721  0.1333]\n",
      "MSE loss: 98.1657\n",
      "Iteration: 169100\n",
      "Gradient: [ -3.1038   6.9319 155.2477  51.2414 243.4191]\n",
      "Weights: [-4.6323  0.188  -0.8494  0.0721  0.1333]\n",
      "MSE loss: 98.1548\n",
      "Iteration: 169200\n",
      "Gradient: [  -8.3095  -18.0682  112.8179   34.8755 -976.4983]\n",
      "Weights: [-4.6306  0.1882 -0.8497  0.0721  0.1333]\n",
      "MSE loss: 98.1461\n",
      "Iteration: 169300\n",
      "Gradient: [  12.4904   13.3961   36.5714    9.6617 -752.7596]\n",
      "Weights: [-4.6302  0.1886 -0.8499  0.0721  0.1333]\n",
      "MSE loss: 98.1393\n",
      "Iteration: 169400\n",
      "Gradient: [  0.7971  36.7859  73.7491  -0.642  550.6009]\n",
      "Weights: [-4.6305  0.189  -0.8502  0.072   0.1334]\n",
      "MSE loss: 98.124\n",
      "Iteration: 169500\n",
      "Gradient: [ 22.9676  -1.8687  17.7833 257.3265 210.182 ]\n",
      "Weights: [-4.6308  0.1891 -0.8504  0.072   0.1334]\n",
      "MSE loss: 98.1144\n",
      "Iteration: 169600\n",
      "Gradient: [   4.3453  -15.176    91.469    59.6226 -307.8997]\n",
      "Weights: [-4.6313  0.1893 -0.8504  0.072   0.1334]\n",
      "MSE loss: 98.1059\n",
      "Iteration: 169700\n",
      "Gradient: [  -7.0587  -63.0436  -50.845    67.572  -457.9366]\n",
      "Weights: [-4.6317  0.1898 -0.8507  0.072   0.1335]\n",
      "MSE loss: 98.0927\n",
      "Iteration: 169800\n",
      "Gradient: [ -30.274    39.2349  -20.1831  -19.7337 -724.9565]\n",
      "Weights: [-4.6312  0.1901 -0.8509  0.072   0.1335]\n",
      "MSE loss: 98.0868\n",
      "Iteration: 169900\n",
      "Gradient: [2.397000e-01 7.732860e+01 1.089727e+02 2.447293e+02 6.963104e+02]\n",
      "Weights: [-4.6309  0.1902 -0.8513  0.072   0.1335]\n",
      "MSE loss: 98.0734\n",
      "Iteration: 170000\n",
      "Gradient: [ -11.9632   45.4058  -99.4618  -52.5158 -782.8377]\n",
      "Weights: [-4.6315  0.1909 -0.8515  0.072   0.1335]\n",
      "MSE loss: 98.0628\n",
      "Iteration: 170100\n",
      "Gradient: [  -17.2348   -25.9191   -19.8728   113.6628 -1015.3035]\n",
      "Weights: [-4.6315  0.1912 -0.8517  0.072   0.1335]\n",
      "MSE loss: 98.0545\n",
      "Iteration: 170200\n",
      "Gradient: [  10.8425   10.837    54.642  -142.5373  189.6176]\n",
      "Weights: [-4.6329  0.1918 -0.852   0.0719  0.1335]\n",
      "MSE loss: 98.0419\n",
      "Iteration: 170300\n",
      "Gradient: [ 2.776400e+00 -9.329700e+00  8.634090e+01 -2.124000e-01 -2.876366e+02]\n",
      "Weights: [-4.6314  0.1921 -0.8522  0.0719  0.1336]\n",
      "MSE loss: 98.0319\n",
      "Iteration: 170400\n",
      "Gradient: [ -15.8582  -34.6677  -40.2578 -104.301  1167.0794]\n",
      "Weights: [-4.6319  0.1922 -0.8525  0.0719  0.1336]\n",
      "MSE loss: 98.022\n",
      "Iteration: 170500\n",
      "Gradient: [  9.9681 -31.3221  68.6472  83.1646 170.0325]\n",
      "Weights: [-4.6315  0.1926 -0.8527  0.0719  0.1336]\n",
      "MSE loss: 98.0114\n",
      "Iteration: 170600\n",
      "Gradient: [   -4.2419   -17.7146   -18.7367   391.7527 -1013.6483]\n",
      "Weights: [-4.6332  0.1931 -0.8529  0.072   0.1336]\n",
      "MSE loss: 98.0006\n",
      "Iteration: 170700\n",
      "Gradient: [ -13.662    -2.1477   96.1646  -57.2799 -626.0209]\n",
      "Weights: [-4.6329  0.194  -0.8532  0.0719  0.1336]\n",
      "MSE loss: 97.9852\n",
      "Iteration: 170800\n",
      "Gradient: [  9.353    9.4137  52.7755 -14.9458  -0.5774]\n",
      "Weights: [-4.632   0.1941 -0.8535  0.0719  0.1336]\n",
      "MSE loss: 97.9743\n",
      "Iteration: 170900\n",
      "Gradient: [   2.2615  -56.8228   17.3884 -170.7403  139.3347]\n",
      "Weights: [-4.6324  0.1943 -0.8537  0.0719  0.1337]\n",
      "MSE loss: 97.9658\n",
      "Iteration: 171000\n",
      "Gradient: [-4.211000e-01 -5.878470e+01  2.112620e+01  1.669385e+02 -5.340261e+02]\n",
      "Weights: [-4.6333  0.1947 -0.8538  0.0719  0.1337]\n",
      "MSE loss: 97.9607\n",
      "Iteration: 171100\n",
      "Gradient: [ 25.6246 -21.2777  52.3039 195.0069 746.7482]\n",
      "Weights: [-4.6349  0.196  -0.854   0.0719  0.1337]\n",
      "MSE loss: 97.9463\n",
      "Iteration: 171200\n",
      "Gradient: [  36.0628   45.2906   47.1776 -102.1516  281.9903]\n",
      "Weights: [-4.6341  0.1963 -0.8542  0.0718  0.1337]\n",
      "MSE loss: 97.9376\n",
      "Iteration: 171300\n",
      "Gradient: [  4.3747 -22.0318 114.7527 127.4615 217.2245]\n",
      "Weights: [-4.6349  0.1969 -0.8545  0.0718  0.1337]\n",
      "MSE loss: 97.9259\n",
      "Iteration: 171400\n",
      "Gradient: [ -5.649  -27.1353  24.9258 -87.8385 310.2648]\n",
      "Weights: [-4.6352  0.1977 -0.8547  0.0718  0.1337]\n",
      "MSE loss: 97.912\n",
      "Iteration: 171500\n",
      "Gradient: [-25.493  -14.3275  -7.8074 127.5356 670.1116]\n",
      "Weights: [-4.6352  0.198  -0.8551  0.0718  0.1337]\n",
      "MSE loss: 97.9016\n",
      "Iteration: 171600\n",
      "Gradient: [  -5.6293   21.4454   20.6846  422.4174 -575.3112]\n",
      "Weights: [-4.6339  0.1984 -0.8552  0.0718  0.1338]\n",
      "MSE loss: 97.8979\n",
      "Iteration: 171700\n",
      "Gradient: [  -3.2138  -23.8504  -73.8206 -402.5789  272.9302]\n",
      "Weights: [-4.6343  0.1984 -0.8555  0.0718  0.1338]\n",
      "MSE loss: 97.8862\n",
      "Iteration: 171800\n",
      "Gradient: [  -0.6542  -30.1962   40.3013 -120.9524  530.1213]\n",
      "Weights: [-4.6352  0.1992 -0.8557  0.0718  0.1338]\n",
      "MSE loss: 97.8763\n",
      "Iteration: 171900\n",
      "Gradient: [  -4.0983  -35.6475  104.4148 -194.7078 -545.464 ]\n",
      "Weights: [-4.6361  0.2001 -0.8559  0.0717  0.1338]\n",
      "MSE loss: 97.8649\n",
      "Iteration: 172000\n",
      "Gradient: [  20.5614  -21.1808  103.804  -254.2632  -98.3893]\n",
      "Weights: [-4.6366  0.2006 -0.856   0.0717  0.1338]\n",
      "MSE loss: 97.8578\n",
      "Iteration: 172100\n",
      "Gradient: [ 28.7406  14.6066 -23.7684 122.3568 -85.8929]\n",
      "Weights: [-4.6374  0.201  -0.8562  0.0717  0.1338]\n",
      "MSE loss: 97.8496\n",
      "Iteration: 172200\n",
      "Gradient: [  42.4803    4.1814  -10.0824 -101.969  -638.6414]\n",
      "Weights: [-4.636   0.2013 -0.8566  0.0717  0.1338]\n",
      "MSE loss: 97.8356\n",
      "Iteration: 172300\n",
      "Gradient: [ -15.396    -3.245   -11.9341   14.8193 -332.9969]\n",
      "Weights: [-4.6383  0.2019 -0.8567  0.0717  0.1339]\n",
      "MSE loss: 97.8283\n",
      "Iteration: 172400\n",
      "Gradient: [ -6.6546 -39.78    97.39   212.8277 321.171 ]\n",
      "Weights: [-4.6383  0.2025 -0.8569  0.0717  0.1339]\n",
      "MSE loss: 97.8169\n",
      "Iteration: 172500\n",
      "Gradient: [ -30.0946   -8.1232 -146.8528  169.6465 -367.336 ]\n",
      "Weights: [-4.6374  0.2028 -0.8572  0.0716  0.1339]\n",
      "MSE loss: 97.803\n",
      "Iteration: 172600\n",
      "Gradient: [-30.7379  60.963   45.3119 188.1477  81.4324]\n",
      "Weights: [-4.6385  0.2037 -0.8575  0.0717  0.1339]\n",
      "MSE loss: 97.7923\n",
      "Iteration: 172700\n",
      "Gradient: [ -8.486  -13.0252 -85.2083 -74.6215 189.9027]\n",
      "Weights: [-4.6381  0.2043 -0.8578  0.0716  0.1339]\n",
      "MSE loss: 97.778\n",
      "Iteration: 172800\n",
      "Gradient: [ -8.8863  31.8315 -40.2727 260.6309  60.4672]\n",
      "Weights: [-4.6373  0.2045 -0.858   0.0716  0.134 ]\n",
      "MSE loss: 97.7694\n",
      "Iteration: 172900\n",
      "Gradient: [  3.6989  12.6294 -52.9492 119.3021 230.4679]\n",
      "Weights: [-4.6367  0.2049 -0.8584  0.0716  0.134 ]\n",
      "MSE loss: 97.7537\n",
      "Iteration: 173000\n",
      "Gradient: [ -14.558   -18.1302  -13.9258 -364.6355  204.6811]\n",
      "Weights: [-4.6385  0.2055 -0.8585  0.0716  0.134 ]\n",
      "MSE loss: 97.7445\n",
      "Iteration: 173100\n",
      "Gradient: [-18.0888 -42.1634  38.5513  71.6046 476.3363]\n",
      "Weights: [-4.6381  0.2058 -0.8588  0.0716  0.134 ]\n",
      "MSE loss: 97.7347\n",
      "Iteration: 173200\n",
      "Gradient: [  -8.6417   -4.9775 -123.0771   90.9349 -198.2235]\n",
      "Weights: [-4.6372  0.206  -0.8591  0.0715  0.134 ]\n",
      "MSE loss: 97.7246\n",
      "Iteration: 173300\n",
      "Gradient: [  42.3239   -1.0094   91.5607  -31.7058 -158.1238]\n",
      "Weights: [-4.6386  0.2064 -0.8593  0.0716  0.1341]\n",
      "MSE loss: 97.713\n",
      "Iteration: 173400\n",
      "Gradient: [  3.3808 -40.94    97.2848 317.5271 737.6192]\n",
      "Weights: [-4.6375  0.2067 -0.8596  0.0716  0.1341]\n",
      "MSE loss: 97.7041\n",
      "Iteration: 173500\n",
      "Gradient: [ 18.2129 -68.2732   6.057  -17.9953 251.7012]\n",
      "Weights: [-4.6382  0.2072 -0.8598  0.0716  0.1341]\n",
      "MSE loss: 97.6975\n",
      "Iteration: 173600\n",
      "Gradient: [ -16.984   -13.5992   78.9996  -23.9181 -174.4779]\n",
      "Weights: [-4.6388  0.2077 -0.8602  0.0716  0.1341]\n",
      "MSE loss: 97.6834\n",
      "Iteration: 173700\n",
      "Gradient: [ -6.0284  -1.5789  18.941  293.3385 126.479 ]\n",
      "Weights: [-4.6399  0.2084 -0.8604  0.0716  0.1341]\n",
      "MSE loss: 97.6755\n",
      "Iteration: 173800\n",
      "Gradient: [-39.1328  -8.2159 157.7384 226.6221 736.9042]\n",
      "Weights: [-4.6396  0.2088 -0.8606  0.0716  0.1341]\n",
      "MSE loss: 97.6628\n",
      "Iteration: 173900\n",
      "Gradient: [ -32.6849  -47.0912  -69.8339  402.5869 -307.3169]\n",
      "Weights: [-4.6389  0.209  -0.8609  0.0716  0.1341]\n",
      "MSE loss: 97.6526\n",
      "Iteration: 174000\n",
      "Gradient: [ -14.5029  -30.8238  115.8889  -31.5546 -384.0615]\n",
      "Weights: [-4.6377  0.209  -0.8612  0.0716  0.1341]\n",
      "MSE loss: 97.6444\n",
      "Iteration: 174100\n",
      "Gradient: [-10.3548 -25.1817  -1.1994  25.3776 148.2871]\n",
      "Weights: [-4.6385  0.2091 -0.8614  0.0716  0.1342]\n",
      "MSE loss: 97.6353\n",
      "Iteration: 174200\n",
      "Gradient: [ -8.5712 -38.6587 108.0661  66.5173 -98.1394]\n",
      "Weights: [-4.6373  0.2093 -0.8616  0.0716  0.1342]\n",
      "MSE loss: 97.625\n",
      "Iteration: 174300\n",
      "Gradient: [  3.6992  -9.6463   4.6919 -14.3894   9.5851]\n",
      "Weights: [-4.6395  0.2097 -0.8618  0.0716  0.1342]\n",
      "MSE loss: 97.6194\n",
      "Iteration: 174400\n",
      "Gradient: [   10.3605    21.2728    33.9917  -108.4144 -1220.0942]\n",
      "Weights: [-4.6391  0.2105 -0.8619  0.0716  0.1342]\n",
      "MSE loss: 97.6061\n",
      "Iteration: 174500\n",
      "Gradient: [ -17.4672   19.4569  -25.608    53.5694 -450.3642]\n",
      "Weights: [-4.6384  0.2103 -0.8622  0.0716  0.1342]\n",
      "MSE loss: 97.6004\n",
      "Iteration: 174600\n",
      "Gradient: [ -10.6701   67.1816   84.8566  -61.0455 -196.5051]\n",
      "Weights: [-4.6383  0.211  -0.8624  0.0716  0.1342]\n",
      "MSE loss: 97.5916\n",
      "Iteration: 174700\n",
      "Gradient: [  35.4151   41.0471   79.7561 -260.6956  383.3314]\n",
      "Weights: [-4.6372  0.2111 -0.8627  0.0716  0.1343]\n",
      "MSE loss: 97.584\n",
      "Iteration: 174800\n",
      "Gradient: [   7.1624   -1.6061   15.4092   51.1257 -931.0954]\n",
      "Weights: [-4.6391  0.2117 -0.8629  0.0715  0.1343]\n",
      "MSE loss: 97.5718\n",
      "Iteration: 174900\n",
      "Gradient: [   8.6129  -10.2364  114.6021   -4.6557 -581.0222]\n",
      "Weights: [-4.6385  0.2122 -0.8631  0.0715  0.1343]\n",
      "MSE loss: 97.5603\n",
      "Iteration: 175000\n",
      "Gradient: [  44.4189   -3.8775   13.2296 -200.5188 -493.7661]\n",
      "Weights: [-4.6395  0.213  -0.8634  0.0715  0.1343]\n",
      "MSE loss: 97.5469\n",
      "Iteration: 175100\n",
      "Gradient: [   8.4437  -39.181   -35.8082  244.4242 1507.2349]\n",
      "Weights: [-4.6401  0.214  -0.8636  0.0715  0.1343]\n",
      "MSE loss: 97.5362\n",
      "Iteration: 175200\n",
      "Gradient: [ 18.2354  27.2115 102.1146 411.8451 693.253 ]\n",
      "Weights: [-4.6404  0.2147 -0.8638  0.0715  0.1343]\n",
      "MSE loss: 97.5263\n",
      "Iteration: 175300\n",
      "Gradient: [  44.9031   37.9613  202.0195  150.2608 -754.8023]\n",
      "Weights: [-4.6399  0.2148 -0.8642  0.0714  0.1344]\n",
      "MSE loss: 97.5128\n",
      "Iteration: 175400\n",
      "Gradient: [ 29.947  -36.873   56.6308 -59.2414   2.8664]\n",
      "Weights: [-4.639   0.2146 -0.8643  0.0714  0.1344]\n",
      "MSE loss: 97.5095\n",
      "Iteration: 175500\n",
      "Gradient: [ -0.4388  -9.9981  94.2563 -56.1818 -44.663 ]\n",
      "Weights: [-4.6412  0.2154 -0.8645  0.0714  0.1344]\n",
      "MSE loss: 97.4984\n",
      "Iteration: 175600\n",
      "Gradient: [ 31.6863  -1.482   69.8307  54.1823 444.6611]\n",
      "Weights: [-4.6413  0.2159 -0.8647  0.0714  0.1344]\n",
      "MSE loss: 97.4901\n",
      "Iteration: 175700\n",
      "Gradient: [ 2.457500e+00  4.578000e-01 -1.063746e+02  1.277590e+01 -5.368933e+02]\n",
      "Weights: [-4.6423  0.217  -0.8649  0.0714  0.1344]\n",
      "MSE loss: 97.4771\n",
      "Iteration: 175800\n",
      "Gradient: [  1.256  -59.4836 -29.2555  24.13   436.8513]\n",
      "Weights: [-4.6405  0.2167 -0.8652  0.0714  0.1344]\n",
      "MSE loss: 97.4684\n",
      "Iteration: 175900\n",
      "Gradient: [  11.6263   43.4445   93.9266   89.4211 -514.7172]\n",
      "Weights: [-4.6415  0.2175 -0.8655  0.0714  0.1345]\n",
      "MSE loss: 97.4541\n",
      "Iteration: 176000\n",
      "Gradient: [-14.1059   1.9001 -38.3254 168.1285 403.9524]\n",
      "Weights: [-4.642   0.218  -0.8657  0.0714  0.1345]\n",
      "MSE loss: 97.4468\n",
      "Iteration: 176100\n",
      "Gradient: [   8.4876  -19.5883  115.7211 -231.2448 -117.6942]\n",
      "Weights: [-4.6404  0.2184 -0.866   0.0714  0.1345]\n",
      "MSE loss: 97.441\n",
      "Iteration: 176200\n",
      "Gradient: [  -4.6849   -7.6226   53.1286 -196.4503 -462.994 ]\n",
      "Weights: [-4.6413  0.2194 -0.8664  0.0713  0.1345]\n",
      "MSE loss: 97.4229\n",
      "Iteration: 176300\n",
      "Gradient: [  -2.1059   30.3418  -60.9045  318.2738 -372.5719]\n",
      "Weights: [-4.6425  0.2201 -0.8667  0.0714  0.1345]\n",
      "MSE loss: 97.4083\n",
      "Iteration: 176400\n",
      "Gradient: [  30.0109   32.5918   36.9343 -149.178   751.7177]\n",
      "Weights: [-4.6423  0.2198 -0.8669  0.0714  0.1345]\n",
      "MSE loss: 97.404\n",
      "Iteration: 176500\n",
      "Gradient: [  -5.8582   25.4661   47.3514 -187.1732  177.7342]\n",
      "Weights: [-4.6414  0.2202 -0.8671  0.0714  0.1346]\n",
      "MSE loss: 97.3954\n",
      "Iteration: 176600\n",
      "Gradient: [   9.3937  -25.7512  -96.6737 -149.7806  427.5961]\n",
      "Weights: [-4.6438  0.2212 -0.8674  0.0714  0.1346]\n",
      "MSE loss: 97.3832\n",
      "Iteration: 176700\n",
      "Gradient: [   8.3085  -30.0805   27.5625  247.1982 -498.3042]\n",
      "Weights: [-4.6424  0.2211 -0.8676  0.0714  0.1346]\n",
      "MSE loss: 97.3749\n",
      "Iteration: 176800\n",
      "Gradient: [  7.8374 -15.7541  83.2721 166.9972 410.3887]\n",
      "Weights: [-4.6429  0.222  -0.8679  0.0714  0.1346]\n",
      "MSE loss: 97.3629\n",
      "Iteration: 176900\n",
      "Gradient: [  -7.9569   36.0472 -118.0712  233.7621 -748.8281]\n",
      "Weights: [-4.6418  0.2218 -0.8681  0.0714  0.1346]\n",
      "MSE loss: 97.3584\n",
      "Iteration: 177000\n",
      "Gradient: [  -5.741   -32.6705   17.4542 -105.6637 -713.6006]\n",
      "Weights: [-4.6416  0.2223 -0.8684  0.0714  0.1346]\n",
      "MSE loss: 97.3484\n",
      "Iteration: 177100\n",
      "Gradient: [  15.0888   -1.5538   33.9244 -170.8066 -115.1718]\n",
      "Weights: [-4.6413  0.2227 -0.8686  0.0714  0.1346]\n",
      "MSE loss: 97.3413\n",
      "Iteration: 177200\n",
      "Gradient: [-10.3673  56.0073 -89.5067 190.0164 402.6956]\n",
      "Weights: [-4.6439  0.2233 -0.8687  0.0714  0.1347]\n",
      "MSE loss: 97.3294\n",
      "Iteration: 177300\n",
      "Gradient: [  16.2269 -118.8606   34.1555  -16.4374 -629.972 ]\n",
      "Weights: [-4.6431  0.224  -0.869   0.0713  0.1347]\n",
      "MSE loss: 97.3179\n",
      "Iteration: 177400\n",
      "Gradient: [ -5.6717   1.7827  84.3351 119.4522 504.7006]\n",
      "Weights: [-4.6436  0.2248 -0.8693  0.0713  0.1347]\n",
      "MSE loss: 97.305\n",
      "Iteration: 177500\n",
      "Gradient: [  16.3389  -16.4825  -80.5704 -118.6163 -215.3487]\n",
      "Weights: [-4.6426  0.2247 -0.8695  0.0713  0.1347]\n",
      "MSE loss: 97.2993\n",
      "Iteration: 177600\n",
      "Gradient: [  7.1779 -40.0392 -23.7978 116.4285 214.6844]\n",
      "Weights: [-4.6447  0.2253 -0.8697  0.0713  0.1347]\n",
      "MSE loss: 97.2896\n",
      "Iteration: 177700\n",
      "Gradient: [  18.3481   -4.3918  101.9015 -123.2938  287.4491]\n",
      "Weights: [-4.6454  0.2261 -0.8699  0.0713  0.1347]\n",
      "MSE loss: 97.2794\n",
      "Iteration: 177800\n",
      "Gradient: [ 12.7904 -24.6029 -50.9692 -54.1323 218.303 ]\n",
      "Weights: [-4.6445  0.2261 -0.8701  0.0712  0.1347]\n",
      "MSE loss: 97.2719\n",
      "Iteration: 177900\n",
      "Gradient: [ -39.5512   35.4776   66.5393   27.3358 -198.8052]\n",
      "Weights: [-4.6427  0.2263 -0.8704  0.0712  0.1348]\n",
      "MSE loss: 97.264\n",
      "Iteration: 178000\n",
      "Gradient: [  12.7103    9.7879  126.9799  229.1421 -918.0534]\n",
      "Weights: [-4.645   0.2267 -0.8704  0.0712  0.1348]\n",
      "MSE loss: 97.2544\n",
      "Iteration: 178100\n",
      "Gradient: [ -39.2295   63.3721   59.2156  -82.7514 -414.7153]\n",
      "Weights: [-4.6444  0.227  -0.8707  0.0712  0.1348]\n",
      "MSE loss: 97.2448\n",
      "Iteration: 178200\n",
      "Gradient: [  9.8279 -11.9191 179.388  201.2566 468.5134]\n",
      "Weights: [-4.6419  0.2268 -0.8709  0.0712  0.1348]\n",
      "MSE loss: 97.244\n",
      "Iteration: 178300\n",
      "Gradient: [-51.4994  16.1525 -52.5425 -40.453  415.3846]\n",
      "Weights: [-4.642   0.227  -0.8711  0.0711  0.1348]\n",
      "MSE loss: 97.2348\n",
      "Iteration: 178400\n",
      "Gradient: [  2.0391 -21.4164 -86.0636 268.0349 239.7929]\n",
      "Weights: [-4.6446  0.2273 -0.8713  0.0712  0.1349]\n",
      "MSE loss: 97.2236\n",
      "Iteration: 178500\n",
      "Gradient: [ -22.2878  -31.8325  -83.7342   73.5592 -595.6084]\n",
      "Weights: [-4.6439  0.2274 -0.8714  0.0711  0.1349]\n",
      "MSE loss: 97.215\n",
      "Iteration: 178600\n",
      "Gradient: [  -4.8477   19.0951   69.4706  305.6235 -701.5517]\n",
      "Weights: [-4.6452  0.2281 -0.8716  0.0712  0.1349]\n",
      "MSE loss: 97.2055\n",
      "Iteration: 178700\n",
      "Gradient: [  1.9742   1.0728  40.9704  94.1767 551.4306]\n",
      "Weights: [-4.6446  0.2285 -0.8718  0.0712  0.1349]\n",
      "MSE loss: 97.1956\n",
      "Iteration: 178800\n",
      "Gradient: [  -7.1783  -45.8435   88.649   -28.8323 1121.4535]\n",
      "Weights: [-4.6442  0.2286 -0.8721  0.0711  0.1349]\n",
      "MSE loss: 97.187\n",
      "Iteration: 178900\n",
      "Gradient: [ -20.4449  -72.1058  107.3493   44.9869 -942.6168]\n",
      "Weights: [-4.6454  0.2291 -0.8723  0.0711  0.1349]\n",
      "MSE loss: 97.1795\n",
      "Iteration: 179000\n",
      "Gradient: [  -1.4494  -12.4114  -69.0465  -19.6681 -145.1497]\n",
      "Weights: [-4.6453  0.2292 -0.8724  0.0711  0.135 ]\n",
      "MSE loss: 97.1713\n",
      "Iteration: 179100\n",
      "Gradient: [   1.8187   -2.7576   28.9893 -182.4482 -392.9105]\n",
      "Weights: [-4.6454  0.2293 -0.8726  0.0711  0.135 ]\n",
      "MSE loss: 97.1652\n",
      "Iteration: 179200\n",
      "Gradient: [-9.740000e-02  7.200490e+01 -3.595730e+01  1.667995e+02 -5.136580e+02]\n",
      "Weights: [-4.6449  0.2299 -0.8727  0.0711  0.135 ]\n",
      "MSE loss: 97.1568\n",
      "Iteration: 179300\n",
      "Gradient: [ -26.4554  -31.4767   32.4424  100.1872 -176.4344]\n",
      "Weights: [-4.6462  0.2305 -0.8729  0.071   0.135 ]\n",
      "MSE loss: 97.1486\n",
      "Iteration: 179400\n",
      "Gradient: [ -27.3489  -85.9646  -81.9572 -398.9167  -92.7122]\n",
      "Weights: [-4.6457  0.2311 -0.8732  0.071   0.135 ]\n",
      "MSE loss: 97.136\n",
      "Iteration: 179500\n",
      "Gradient: [-37.1368  -9.9687 102.9763 -52.1292  -6.1286]\n",
      "Weights: [-4.6461  0.2317 -0.8735  0.071   0.135 ]\n",
      "MSE loss: 97.1268\n",
      "Iteration: 179600\n",
      "Gradient: [ 30.311   65.8242 -30.778  313.5138 814.4972]\n",
      "Weights: [-4.6444  0.2323 -0.8737  0.071   0.135 ]\n",
      "MSE loss: 97.1221\n",
      "Iteration: 179700\n",
      "Gradient: [  -1.4683   -2.6086   46.2494  335.198  -585.9699]\n",
      "Weights: [-4.6469  0.2328 -0.8739  0.0711  0.1351]\n",
      "MSE loss: 97.1096\n",
      "Iteration: 179800\n",
      "Gradient: [  -5.5505  -31.9047  -75.7853  234.6311 -791.96  ]\n",
      "Weights: [-4.6461  0.2329 -0.8741  0.0711  0.1351]\n",
      "MSE loss: 97.1032\n",
      "Iteration: 179900\n",
      "Gradient: [-10.7637 -32.8048 -22.7088 223.4576 945.671 ]\n",
      "Weights: [-4.6461  0.2328 -0.8744  0.0711  0.1351]\n",
      "MSE loss: 97.0953\n",
      "Iteration: 180000\n",
      "Gradient: [   6.1229   -1.7326   85.7749 -272.6598  299.2769]\n",
      "Weights: [-4.6469  0.2334 -0.8745  0.0711  0.1351]\n",
      "MSE loss: 97.0854\n",
      "Iteration: 180100\n",
      "Gradient: [ -16.2312   30.744    59.6487  393.3564 -499.4969]\n",
      "Weights: [-4.6459  0.2333 -0.8747  0.071   0.1351]\n",
      "MSE loss: 97.0806\n",
      "Iteration: 180200\n",
      "Gradient: [  31.8561   31.0573   97.3046 -246.8665 -386.5221]\n",
      "Weights: [-4.6453  0.2333 -0.8748  0.0711  0.1351]\n",
      "MSE loss: 97.0769\n",
      "Iteration: 180300\n",
      "Gradient: [ -23.315   -40.2539   50.5873  -54.433  -336.3821]\n",
      "Weights: [-4.647   0.2339 -0.8751  0.0711  0.1352]\n",
      "MSE loss: 97.0652\n",
      "Iteration: 180400\n",
      "Gradient: [ 17.3372  24.4493 118.557  -79.1503 406.7589]\n",
      "Weights: [-4.645   0.2343 -0.8754  0.0711  0.1352]\n",
      "MSE loss: 97.0586\n",
      "Iteration: 180500\n",
      "Gradient: [ -13.1987    3.8464   14.5983 -187.6003 -451.4307]\n",
      "Weights: [-4.6454  0.2347 -0.8755  0.071   0.1352]\n",
      "MSE loss: 97.0506\n",
      "Iteration: 180600\n",
      "Gradient: [ -12.5093  -13.1828 -167.7383  284.0277 -267.5766]\n",
      "Weights: [-4.6459  0.2349 -0.8757  0.071   0.1352]\n",
      "MSE loss: 97.0408\n",
      "Iteration: 180700\n",
      "Gradient: [ 14.5191 -62.4738 -10.5904 -47.6501 423.0572]\n",
      "Weights: [-4.6466  0.2351 -0.876   0.071   0.1352]\n",
      "MSE loss: 97.032\n",
      "Iteration: 180800\n",
      "Gradient: [ -18.776   -21.0015  -49.6641 -209.4595 -618.052 ]\n",
      "Weights: [-4.6484  0.2359 -0.8762  0.071   0.1352]\n",
      "MSE loss: 97.0248\n",
      "Iteration: 180900\n",
      "Gradient: [ -18.0378   40.9162  -17.0071  260.5974 -985.6633]\n",
      "Weights: [-4.6487  0.2366 -0.8764  0.071   0.1352]\n",
      "MSE loss: 97.0138\n",
      "Iteration: 181000\n",
      "Gradient: [   7.9765  -17.6586   37.6759 -219.4146 -536.1244]\n",
      "Weights: [-4.6476  0.2367 -0.8766  0.071   0.1353]\n",
      "MSE loss: 97.005\n",
      "Iteration: 181100\n",
      "Gradient: [  -8.4092  -43.0554   92.1656  175.163  -851.8133]\n",
      "Weights: [-4.6478  0.2376 -0.877   0.071   0.1353]\n",
      "MSE loss: 96.9931\n",
      "Iteration: 181200\n",
      "Gradient: [  38.0099   22.2143  -94.8736    5.0718 -749.8748]\n",
      "Weights: [-4.6478  0.2379 -0.8771  0.071   0.1353]\n",
      "MSE loss: 96.9851\n",
      "Iteration: 181300\n",
      "Gradient: [  16.9207   53.6939  -74.1451 -219.3217  150.203 ]\n",
      "Weights: [-4.6455  0.2376 -0.8772  0.071   0.1353]\n",
      "MSE loss: 96.9871\n",
      "Iteration: 181400\n",
      "Gradient: [-30.2608  39.9562 126.3815  89.1915 565.1294]\n",
      "Weights: [-4.6476  0.2383 -0.8774  0.071   0.1353]\n",
      "MSE loss: 96.9748\n",
      "Iteration: 181500\n",
      "Gradient: [ -11.6857   47.4731  274.8409 -234.5063 -751.6865]\n",
      "Weights: [-4.6472  0.2388 -0.8777  0.071   0.1353]\n",
      "MSE loss: 96.9636\n",
      "Iteration: 181600\n",
      "Gradient: [   0.279   -55.6188  136.338     8.8621 -197.0067]\n",
      "Weights: [-4.6483  0.2394 -0.8779  0.071   0.1353]\n",
      "MSE loss: 96.9547\n",
      "Iteration: 181700\n",
      "Gradient: [   2.7577    9.5401   45.4718 -179.0413 -721.2421]\n",
      "Weights: [-4.6474  0.2397 -0.8782  0.0709  0.1353]\n",
      "MSE loss: 96.9448\n",
      "Iteration: 181800\n",
      "Gradient: [  17.2885   35.1532 -265.4686  128.2174 -510.8629]\n",
      "Weights: [-4.6495  0.2406 -0.8783  0.0709  0.1354]\n",
      "MSE loss: 96.9373\n",
      "Iteration: 181900\n",
      "Gradient: [ -16.9932  -25.3223   61.0315  -20.8382 -639.403 ]\n",
      "Weights: [-4.6499  0.2413 -0.8785  0.0708  0.1354]\n",
      "MSE loss: 96.929\n",
      "Iteration: 182000\n",
      "Gradient: [  -20.7342     6.7401    65.3331   -18.9149 -1120.9955]\n",
      "Weights: [-4.6488  0.2415 -0.8786  0.0708  0.1354]\n",
      "MSE loss: 96.9219\n",
      "Iteration: 182100\n",
      "Gradient: [  -3.4309   40.5132  111.7625  128.1579 -212.3391]\n",
      "Weights: [-4.6483  0.2415 -0.8789  0.0708  0.1354]\n",
      "MSE loss: 96.914\n",
      "Iteration: 182200\n",
      "Gradient: [  9.4927 116.4717 160.4515 221.0706  89.3278]\n",
      "Weights: [-4.6485  0.2418 -0.8791  0.0709  0.1354]\n",
      "MSE loss: 96.9073\n",
      "Iteration: 182300\n",
      "Gradient: [ -36.238    40.6983   59.6391 -162.8455  244.8765]\n",
      "Weights: [-4.6503  0.2425 -0.8794  0.0709  0.1354]\n",
      "MSE loss: 96.895\n",
      "Iteration: 182400\n",
      "Gradient: [  0.7519 -11.2772  69.2854 -23.5708  49.4055]\n",
      "Weights: [-4.6486  0.2425 -0.8796  0.0709  0.1354]\n",
      "MSE loss: 96.8888\n",
      "Iteration: 182500\n",
      "Gradient: [  -24.9075   -40.4795    15.9494   355.6873 -1339.6276]\n",
      "Weights: [-4.6513  0.2437 -0.8799  0.0709  0.1354]\n",
      "MSE loss: 96.8782\n",
      "Iteration: 182600\n",
      "Gradient: [   13.882     -6.7285   141.8362   270.7678 -1429.3558]\n",
      "Weights: [-4.6492  0.2437 -0.8801  0.0709  0.1355]\n",
      "MSE loss: 96.8705\n",
      "Iteration: 182700\n",
      "Gradient: [ -12.1559    7.8352   15.5667  312.3191 -302.8411]\n",
      "Weights: [-4.6497  0.2439 -0.8803  0.0709  0.1355]\n",
      "MSE loss: 96.8644\n",
      "Iteration: 182800\n",
      "Gradient: [  13.0995   30.0016   67.4722 -264.9109 -665.6314]\n",
      "Weights: [-4.6497  0.2443 -0.8804  0.0708  0.1355]\n",
      "MSE loss: 96.8583\n",
      "Iteration: 182900\n",
      "Gradient: [  -7.5673  -16.3922  -24.3211 -277.5269  411.4173]\n",
      "Weights: [-4.6503  0.2444 -0.8805  0.0709  0.1355]\n",
      "MSE loss: 96.8524\n",
      "Iteration: 183000\n",
      "Gradient: [-30.2474 -18.2279  -8.8966  71.0597 250.35  ]\n",
      "Weights: [-4.6516  0.245  -0.8806  0.0709  0.1355]\n",
      "MSE loss: 96.8484\n",
      "Iteration: 183100\n",
      "Gradient: [  42.2206   30.5008 -132.3371  112.1082  956.1363]\n",
      "Weights: [-4.6499  0.2452 -0.8808  0.0708  0.1355]\n",
      "MSE loss: 96.8386\n",
      "Iteration: 183200\n",
      "Gradient: [   22.742     24.3738    78.3858   -17.4031 -1381.5721]\n",
      "Weights: [-4.6513  0.2456 -0.8808  0.0708  0.1356]\n",
      "MSE loss: 96.8338\n",
      "Iteration: 183300\n",
      "Gradient: [ -21.138   -10.2431  -55.877   432.8028 -277.1093]\n",
      "Weights: [-4.6516  0.246  -0.8812  0.0708  0.1356]\n",
      "MSE loss: 96.8214\n",
      "Iteration: 183400\n",
      "Gradient: [ -33.2888  -47.9513  -25.2637  -14.9731 -498.2969]\n",
      "Weights: [-4.6507  0.2464 -0.8814  0.0707  0.1356]\n",
      "MSE loss: 96.8117\n",
      "Iteration: 183500\n",
      "Gradient: [  42.2203  -26.3552  -77.4792  320.0367 -650.5865]\n",
      "Weights: [-4.6519  0.247  -0.8817  0.0707  0.1356]\n",
      "MSE loss: 96.8026\n",
      "Iteration: 183600\n",
      "Gradient: [  1.4682  15.6689  20.0009 -49.298  230.7112]\n",
      "Weights: [-4.6517  0.2472 -0.8819  0.0707  0.1356]\n",
      "MSE loss: 96.7941\n",
      "Iteration: 183700\n",
      "Gradient: [  44.0479   54.7158   17.5337  -22.9233 -579.5739]\n",
      "Weights: [-4.651   0.2478 -0.8822  0.0707  0.1356]\n",
      "MSE loss: 96.7829\n",
      "Iteration: 183800\n",
      "Gradient: [ -10.7838  -68.2808  -21.3129  441.3102 -243.8895]\n",
      "Weights: [-4.6512  0.2479 -0.8825  0.0707  0.1357]\n",
      "MSE loss: 96.7729\n",
      "Iteration: 183900\n",
      "Gradient: [   1.7896   32.6645  105.5887  175.2936 -609.5542]\n",
      "Weights: [-4.6504  0.2483 -0.8826  0.0707  0.1357]\n",
      "MSE loss: 96.7681\n",
      "Iteration: 184000\n",
      "Gradient: [   7.2689   18.4636 -127.0632  -97.088   438.0577]\n",
      "Weights: [-4.6494  0.2479 -0.8828  0.0707  0.1357]\n",
      "MSE loss: 96.7633\n",
      "Iteration: 184100\n",
      "Gradient: [   2.0934  -30.1982    6.3843  492.541  -216.4068]\n",
      "Weights: [-4.6525  0.2487 -0.883   0.0707  0.1357]\n",
      "MSE loss: 96.7545\n",
      "Iteration: 184200\n",
      "Gradient: [ -45.2854  -34.8113   43.2305 -277.8607  275.0049]\n",
      "Weights: [-4.6512  0.2491 -0.8832  0.0707  0.1357]\n",
      "MSE loss: 96.7436\n",
      "Iteration: 184300\n",
      "Gradient: [   2.8094  -28.1133   89.8276 -354.5998 -345.7116]\n",
      "Weights: [-4.6504  0.2489 -0.8833  0.0707  0.1357]\n",
      "MSE loss: 96.7389\n",
      "Iteration: 184400\n",
      "Gradient: [-13.1749   3.3005 147.5155  92.6861 487.6211]\n",
      "Weights: [-4.6501  0.2489 -0.8835  0.0706  0.1358]\n",
      "MSE loss: 96.7314\n",
      "Iteration: 184500\n",
      "Gradient: [   23.0402    15.4201    39.7695   249.3376 -1325.9984]\n",
      "Weights: [-4.6508  0.2498 -0.8836  0.0706  0.1358]\n",
      "MSE loss: 96.7243\n",
      "Iteration: 184600\n",
      "Gradient: [  11.4233  -16.7242  121.6105 -128.4559 -343.2913]\n",
      "Weights: [-4.6514  0.2502 -0.8838  0.0706  0.1358]\n",
      "MSE loss: 96.7165\n",
      "Iteration: 184700\n",
      "Gradient: [ -26.0177   52.431   128.1954 -140.229  -760.9229]\n",
      "Weights: [-4.651   0.2506 -0.8841  0.0706  0.1358]\n",
      "MSE loss: 96.7064\n",
      "Iteration: 184800\n",
      "Gradient: [   9.9159   24.0547    5.3645   46.2262 -375.2253]\n",
      "Weights: [-4.6516  0.251  -0.8843  0.0706  0.1358]\n",
      "MSE loss: 96.6996\n",
      "Iteration: 184900\n",
      "Gradient: [ 5.075000e-01  2.221610e+01 -1.866500e+01  2.727380e+01 -8.097593e+02]\n",
      "Weights: [-4.6516  0.251  -0.8846  0.0706  0.1358]\n",
      "MSE loss: 96.6919\n",
      "Iteration: 185000\n",
      "Gradient: [  -0.7043   20.2187   83.4743 -107.5483 -151.3632]\n",
      "Weights: [-4.6496  0.2513 -0.8847  0.0706  0.1358]\n",
      "MSE loss: 96.691\n",
      "Iteration: 185100\n",
      "Gradient: [   6.224    21.0129   69.0631 -143.5222  418.7534]\n",
      "Weights: [-4.651   0.2518 -0.8849  0.0706  0.1358]\n",
      "MSE loss: 96.6769\n",
      "Iteration: 185200\n",
      "Gradient: [  58.2569   11.6557   35.5963  -65.3175 -525.9215]\n",
      "Weights: [-4.6508  0.2522 -0.8853  0.0706  0.1359]\n",
      "MSE loss: 96.6671\n",
      "Iteration: 185300\n",
      "Gradient: [ -10.0632   -9.5371  -95.5101    1.9261 -187.0481]\n",
      "Weights: [-4.6518  0.2527 -0.8854  0.0706  0.1359]\n",
      "MSE loss: 96.6587\n",
      "Iteration: 185400\n",
      "Gradient: [  16.8357   28.4823  -15.258  -118.542     6.862 ]\n",
      "Weights: [-4.6505  0.2525 -0.8856  0.0706  0.1359]\n",
      "MSE loss: 96.6539\n",
      "Iteration: 185500\n",
      "Gradient: [ -19.6863   -0.9229   52.0649 -137.7352  153.8886]\n",
      "Weights: [-4.6525  0.253  -0.8858  0.0706  0.1359]\n",
      "MSE loss: 96.6468\n",
      "Iteration: 185600\n",
      "Gradient: [ 3.045120e+01 -2.473000e-01  1.743272e+02  2.192715e+02  5.573591e+02]\n",
      "Weights: [-4.6518  0.2533 -0.886   0.0706  0.1359]\n",
      "MSE loss: 96.6396\n",
      "Iteration: 185700\n",
      "Gradient: [ -23.7401   62.5746  -32.3152 -104.7666  287.1132]\n",
      "Weights: [-4.6525  0.2536 -0.8862  0.0706  0.1359]\n",
      "MSE loss: 96.6286\n",
      "Iteration: 185800\n",
      "Gradient: [2.510000e-02 3.946310e+01 5.884090e+01 6.205520e+01 2.312009e+02]\n",
      "Weights: [-4.6523  0.2547 -0.8865  0.0706  0.136 ]\n",
      "MSE loss: 96.616\n",
      "Iteration: 185900\n",
      "Gradient: [-12.1511   8.711   48.4006  65.4321 552.5848]\n",
      "Weights: [-4.6541  0.2552 -0.8866  0.0705  0.136 ]\n",
      "MSE loss: 96.6086\n",
      "Iteration: 186000\n",
      "Gradient: [  -3.4788   40.5478   71.7849  -67.0935 -380.4877]\n",
      "Weights: [-4.6541  0.256  -0.8867  0.0705  0.136 ]\n",
      "MSE loss: 96.6032\n",
      "Iteration: 186100\n",
      "Gradient: [  15.2029  -46.5696  213.8709 -338.1136   76.3114]\n",
      "Weights: [-4.6549  0.2559 -0.887   0.0705  0.136 ]\n",
      "MSE loss: 96.5972\n",
      "Iteration: 186200\n",
      "Gradient: [  -9.6306  -21.5553 -104.4954   49.8015  275.3873]\n",
      "Weights: [-4.654   0.2563 -0.8873  0.0705  0.136 ]\n",
      "MSE loss: 96.5841\n",
      "Iteration: 186300\n",
      "Gradient: [ 12.2811 -51.7137  -4.0609 -39.912   84.3239]\n",
      "Weights: [-4.6538  0.2562 -0.8874  0.0705  0.136 ]\n",
      "MSE loss: 96.5796\n",
      "Iteration: 186400\n",
      "Gradient: [ 28.7162 -17.0132 104.8968 269.758  526.408 ]\n",
      "Weights: [-4.6526  0.2562 -0.8877  0.0705  0.1361]\n",
      "MSE loss: 96.5714\n",
      "Iteration: 186500\n",
      "Gradient: [ -20.7334  -36.6248   16.6709    2.943  -517.4285]\n",
      "Weights: [-4.6517  0.2561 -0.8879  0.0705  0.1361]\n",
      "MSE loss: 96.5664\n",
      "Iteration: 186600\n",
      "Gradient: [  23.3662  -26.9859  -60.5134 -394.6541 -504.2633]\n",
      "Weights: [-4.6523  0.2569 -0.888   0.0704  0.1361]\n",
      "MSE loss: 96.5571\n",
      "Iteration: 186700\n",
      "Gradient: [-13.9121  35.0125  51.1739 398.34   -30.9605]\n",
      "Weights: [-4.6534  0.2574 -0.8883  0.0705  0.1361]\n",
      "MSE loss: 96.546\n",
      "Iteration: 186800\n",
      "Gradient: [   2.8601  -42.2508  166.9562   82.4414 -445.8152]\n",
      "Weights: [-4.654   0.2574 -0.8884  0.0705  0.1361]\n",
      "MSE loss: 96.5415\n",
      "Iteration: 186900\n",
      "Gradient: [   8.86     17.3236   -9.7184   25.157  -675.9555]\n",
      "Weights: [-4.6526  0.2574 -0.8887  0.0704  0.1361]\n",
      "MSE loss: 96.532\n",
      "Iteration: 187000\n",
      "Gradient: [   3.3067   -9.957   256.2871  449.1797 -812.6505]\n",
      "Weights: [-4.6534  0.2581 -0.8888  0.0705  0.1362]\n",
      "MSE loss: 96.5251\n",
      "Iteration: 187100\n",
      "Gradient: [   1.7194    6.0661   14.7806  157.5333 -567.3887]\n",
      "Weights: [-4.6533  0.2588 -0.8892  0.0705  0.1362]\n",
      "MSE loss: 96.5146\n",
      "Iteration: 187200\n",
      "Gradient: [-7.700000e-03  3.175700e+00  8.369180e+01 -2.584268e+02  1.904975e+02]\n",
      "Weights: [-4.6537  0.2592 -0.8894  0.0705  0.1362]\n",
      "MSE loss: 96.5065\n",
      "Iteration: 187300\n",
      "Gradient: [   4.0727   11.4668  158.4189  137.6022 -425.0248]\n",
      "Weights: [-4.6532  0.2596 -0.8896  0.0705  0.1362]\n",
      "MSE loss: 96.5002\n",
      "Iteration: 187400\n",
      "Gradient: [ -7.9123 -26.9857  72.4588 -82.0179 118.2155]\n",
      "Weights: [-4.6539  0.2594 -0.8898  0.0705  0.1362]\n",
      "MSE loss: 96.4927\n",
      "Iteration: 187500\n",
      "Gradient: [ 20.444  -34.6777  16.0653  35.7317 -91.4354]\n",
      "Weights: [-4.6535  0.2595 -0.8899  0.0704  0.1362]\n",
      "MSE loss: 96.4869\n",
      "Iteration: 187600\n",
      "Gradient: [ 19.0203 -45.1358 251.3217 114.5693 -61.3275]\n",
      "Weights: [-4.652   0.2597 -0.8901  0.0704  0.1362]\n",
      "MSE loss: 96.4852\n",
      "Iteration: 187700\n",
      "Gradient: [  18.1603  -64.5856  -81.2154 -238.4139  504.5217]\n",
      "Weights: [-4.6523  0.2594 -0.8902  0.0704  0.1363]\n",
      "MSE loss: 96.481\n",
      "Iteration: 187800\n",
      "Gradient: [-1.954000e-01  4.180320e+01  5.546660e+01  1.804734e+02 -6.999371e+02]\n",
      "Weights: [-4.6522  0.2602 -0.8904  0.0704  0.1363]\n",
      "MSE loss: 96.4724\n",
      "Iteration: 187900\n",
      "Gradient: [  7.1273   6.5949 -58.8279 164.2898 158.5364]\n",
      "Weights: [-4.6538  0.2608 -0.8908  0.0704  0.1363]\n",
      "MSE loss: 96.4583\n",
      "Iteration: 188000\n",
      "Gradient: [ -17.4501   29.0197   19.6407   34.5372 -915.6588]\n",
      "Weights: [-4.652   0.2613 -0.891   0.0704  0.1363]\n",
      "MSE loss: 96.4549\n",
      "Iteration: 188100\n",
      "Gradient: [ -13.7867  -32.6804  -44.3168 -103.6888 -779.2778]\n",
      "Weights: [-4.6533  0.2614 -0.8913  0.0704  0.1363]\n",
      "MSE loss: 96.4433\n",
      "Iteration: 188200\n",
      "Gradient: [-11.5815  71.5715  60.7555 171.3244  90.7178]\n",
      "Weights: [-4.6536  0.2624 -0.8915  0.0704  0.1363]\n",
      "MSE loss: 96.4336\n",
      "Iteration: 188300\n",
      "Gradient: [-3.358000e+00  2.351000e-01  9.418050e+01 -1.124620e+02 -3.843023e+02]\n",
      "Weights: [-4.652   0.2619 -0.8916  0.0704  0.1363]\n",
      "MSE loss: 96.4342\n",
      "Iteration: 188400\n",
      "Gradient: [-21.0426  35.8879 231.3753 -44.9683  36.452 ]\n",
      "Weights: [-4.654   0.2626 -0.8917  0.0704  0.1364]\n",
      "MSE loss: 96.422\n",
      "Iteration: 188500\n",
      "Gradient: [ -17.687   -25.3498   43.7677  241.1628 -744.834 ]\n",
      "Weights: [-4.6543  0.2629 -0.892   0.0704  0.1364]\n",
      "MSE loss: 96.4146\n",
      "Iteration: 188600\n",
      "Gradient: [  14.037   -18.0468  116.6319 -123.5114   53.7166]\n",
      "Weights: [-4.6559  0.2639 -0.8921  0.0704  0.1364]\n",
      "MSE loss: 96.4066\n",
      "Iteration: 188700\n",
      "Gradient: [  -3.0431   -3.5161  -69.6901  123.3282 -213.0879]\n",
      "Weights: [-4.6562  0.2646 -0.8923  0.0704  0.1364]\n",
      "MSE loss: 96.3984\n",
      "Iteration: 188800\n",
      "Gradient: [ 18.1062 -26.2443   9.2735 -11.9672 368.0471]\n",
      "Weights: [-4.6573  0.2651 -0.8924  0.0704  0.1364]\n",
      "MSE loss: 96.3945\n",
      "Iteration: 188900\n",
      "Gradient: [  24.699    54.3334   -6.4601 -174.5975  -73.6376]\n",
      "Weights: [-4.6556  0.2656 -0.8926  0.0704  0.1364]\n",
      "MSE loss: 96.3887\n",
      "Iteration: 189000\n",
      "Gradient: [ -21.0704    6.2268   36.6546 -400.9489 -547.1837]\n",
      "Weights: [-4.6564  0.2654 -0.8927  0.0703  0.1364]\n",
      "MSE loss: 96.3837\n",
      "Iteration: 189100\n",
      "Gradient: [   16.0952   -17.7619    12.9611   -65.978  -1074.8098]\n",
      "Weights: [-4.6562  0.266  -0.893   0.0704  0.1364]\n",
      "MSE loss: 96.3719\n",
      "Iteration: 189200\n",
      "Gradient: [  16.2345   43.2052   61.2867 -341.9285  -47.1969]\n",
      "Weights: [-4.6567  0.2662 -0.8932  0.0703  0.1364]\n",
      "MSE loss: 96.3663\n",
      "Iteration: 189300\n",
      "Gradient: [ -13.5039   57.3821  -63.0438 -257.5121 -638.6266]\n",
      "Weights: [-4.6563  0.2667 -0.8933  0.0703  0.1364]\n",
      "MSE loss: 96.3594\n",
      "Iteration: 189400\n",
      "Gradient: [  13.9095   12.0292  118.4506  -95.6205 -561.9228]\n",
      "Weights: [-4.6568  0.2667 -0.8934  0.0703  0.1365]\n",
      "MSE loss: 96.3558\n",
      "Iteration: 189500\n",
      "Gradient: [  17.8454   22.7183   26.6653  -19.7145 -602.372 ]\n",
      "Weights: [-4.6568  0.2671 -0.8937  0.0703  0.1365]\n",
      "MSE loss: 96.348\n",
      "Iteration: 189600\n",
      "Gradient: [ -22.823    39.3765  -53.0473   60.2847 -105.8914]\n",
      "Weights: [-4.6568  0.2674 -0.8939  0.0703  0.1365]\n",
      "MSE loss: 96.3389\n",
      "Iteration: 189700\n",
      "Gradient: [  25.3767   10.2927  196.6356 -377.4374 1170.7633]\n",
      "Weights: [-4.6564  0.2679 -0.8942  0.0703  0.1365]\n",
      "MSE loss: 96.3302\n",
      "Iteration: 189800\n",
      "Gradient: [ -3.9081 -28.3203 137.419  419.0659  74.3773]\n",
      "Weights: [-4.6577  0.2683 -0.8944  0.0703  0.1365]\n",
      "MSE loss: 96.3238\n",
      "Iteration: 189900\n",
      "Gradient: [   8.9542   -9.8251  -49.7402  408.6807 -362.5884]\n",
      "Weights: [-4.6566  0.269  -0.8946  0.0703  0.1365]\n",
      "MSE loss: 96.3154\n",
      "Iteration: 190000\n",
      "Gradient: [ -19.6037   71.9146   -3.2867  204.9115 -396.8656]\n",
      "Weights: [-4.6577  0.2695 -0.895   0.0703  0.1365]\n",
      "MSE loss: 96.3028\n",
      "Iteration: 190100\n",
      "Gradient: [-21.8235  -9.2978  47.8283 103.2167 290.2799]\n",
      "Weights: [-4.6586  0.27   -0.8952  0.0703  0.1366]\n",
      "MSE loss: 96.2959\n",
      "Iteration: 190200\n",
      "Gradient: [  4.2053   2.7778  94.4525  42.7681 348.9779]\n",
      "Weights: [-4.6574  0.2702 -0.8953  0.0703  0.1366]\n",
      "MSE loss: 96.289\n",
      "Iteration: 190300\n",
      "Gradient: [  25.1786   29.7638   68.01     61.0168 -386.9633]\n",
      "Weights: [-4.6575  0.2707 -0.8956  0.0703  0.1366]\n",
      "MSE loss: 96.2798\n",
      "Iteration: 190400\n",
      "Gradient: [ -39.8841   50.3688  -24.3452   10.6917 -168.2059]\n",
      "Weights: [-4.6578  0.2709 -0.8959  0.0703  0.1366]\n",
      "MSE loss: 96.2712\n",
      "Iteration: 190500\n",
      "Gradient: [  24.8186   44.1532   17.0152  -51.312  -327.8086]\n",
      "Weights: [-4.6576  0.2712 -0.8961  0.0703  0.1366]\n",
      "MSE loss: 96.2636\n",
      "Iteration: 190600\n",
      "Gradient: [ -12.6966   -4.8372   36.9855 -270.039  -472.2402]\n",
      "Weights: [-4.6576  0.2714 -0.8961  0.0703  0.1366]\n",
      "MSE loss: 96.2603\n",
      "Iteration: 190700\n",
      "Gradient: [  -7.6482  -32.6252  113.2743   40.1958 -652.5758]\n",
      "Weights: [-4.657   0.2713 -0.8963  0.0703  0.1366]\n",
      "MSE loss: 96.2549\n",
      "Iteration: 190800\n",
      "Gradient: [ -13.4845   21.3678   24.598   -64.956  -120.1467]\n",
      "Weights: [-4.6568  0.2715 -0.8965  0.0703  0.1367]\n",
      "MSE loss: 96.2487\n",
      "Iteration: 190900\n",
      "Gradient: [ -26.8794  -12.827    66.374     0.2901 -231.8546]\n",
      "Weights: [-4.657   0.2715 -0.8966  0.0703  0.1367]\n",
      "MSE loss: 96.2453\n",
      "Iteration: 191000\n",
      "Gradient: [  23.0789  -41.0257   85.2906 -202.1962  685.6787]\n",
      "Weights: [-4.6565  0.2715 -0.8969  0.0702  0.1367]\n",
      "MSE loss: 96.2379\n",
      "Iteration: 191100\n",
      "Gradient: [  7.4137  42.3217 -55.0716  63.751   59.5581]\n",
      "Weights: [-4.6563  0.2715 -0.8972  0.0702  0.1367]\n",
      "MSE loss: 96.2303\n",
      "Iteration: 191200\n",
      "Gradient: [  22.4239   58.8653   32.4375  216.8563 -217.0861]\n",
      "Weights: [-4.6551  0.272  -0.8972  0.0702  0.1367]\n",
      "MSE loss: 96.231\n",
      "Iteration: 191300\n",
      "Gradient: [ -8.1812 -73.583   19.2439  58.5479 475.1983]\n",
      "Weights: [-4.6579  0.2721 -0.8972  0.0702  0.1367]\n",
      "MSE loss: 96.2237\n",
      "Iteration: 191400\n",
      "Gradient: [  14.2316   33.7618  -35.3072 -288.681  -364.3811]\n",
      "Weights: [-4.658   0.2725 -0.8973  0.0702  0.1367]\n",
      "MSE loss: 96.2173\n",
      "Iteration: 191500\n",
      "Gradient: [   2.4926   28.1005   31.0325 -304.9605  346.8145]\n",
      "Weights: [-4.6575  0.2727 -0.8975  0.0702  0.1368]\n",
      "MSE loss: 96.2123\n",
      "Iteration: 191600\n",
      "Gradient: [  27.4023  -39.0208  -52.1548 -379.8849  -10.2384]\n",
      "Weights: [-4.6567  0.2727 -0.8976  0.0702  0.1368]\n",
      "MSE loss: 96.2087\n",
      "Iteration: 191700\n",
      "Gradient: [   2.8455  -34.7208  -63.7896   84.8539 -227.6491]\n",
      "Weights: [-4.6563  0.2733 -0.8977  0.0702  0.1368]\n",
      "MSE loss: 96.2082\n",
      "Iteration: 191800\n",
      "Gradient: [-17.2522 -44.5682  71.8456 173.2949  39.2539]\n",
      "Weights: [-4.6567  0.2733 -0.8979  0.0702  0.1368]\n",
      "MSE loss: 96.1982\n",
      "Iteration: 191900\n",
      "Gradient: [ -18.7086  -11.4386  116.753  -128.9268  -66.4286]\n",
      "Weights: [-4.6588  0.2738 -0.8981  0.0702  0.1368]\n",
      "MSE loss: 96.1903\n",
      "Iteration: 192000\n",
      "Gradient: [ -11.5821  -48.1004  137.6357 -158.6791  -71.9353]\n",
      "Weights: [-4.6585  0.2745 -0.8983  0.0702  0.1368]\n",
      "MSE loss: 96.1811\n",
      "Iteration: 192100\n",
      "Gradient: [   0.7151  -11.2198   66.8    -477.5572 -189.4342]\n",
      "Weights: [-4.6587  0.275  -0.8985  0.0702  0.1368]\n",
      "MSE loss: 96.1743\n",
      "Iteration: 192200\n",
      "Gradient: [-28.5493  18.3454 166.1775 -45.6152 209.9435]\n",
      "Weights: [-4.6591  0.2752 -0.8988  0.0702  0.1368]\n",
      "MSE loss: 96.1653\n",
      "Iteration: 192300\n",
      "Gradient: [   9.8608    3.4658  -65.4403 -155.3302   15.5862]\n",
      "Weights: [-4.6582  0.2753 -0.8991  0.0702  0.1369]\n",
      "MSE loss: 96.1573\n",
      "Iteration: 192400\n",
      "Gradient: [  16.7431  -17.2054   13.7528   95.6612 -795.2252]\n",
      "Weights: [-4.6576  0.2756 -0.8993  0.0702  0.1369]\n",
      "MSE loss: 96.152\n",
      "Iteration: 192500\n",
      "Gradient: [ -7.3224 -47.403   62.3384  30.8104 319.8709]\n",
      "Weights: [-4.6591  0.2755 -0.8994  0.0702  0.1369]\n",
      "MSE loss: 96.1511\n",
      "Iteration: 192600\n",
      "Gradient: [   6.244     5.4981  116.9808 -287.0734 -708.3682]\n",
      "Weights: [-4.6598  0.2764 -0.8995  0.0702  0.1369]\n",
      "MSE loss: 96.1402\n",
      "Iteration: 192700\n",
      "Gradient: [  5.3669 -54.0351  -5.4883 332.5612 358.5104]\n",
      "Weights: [-4.6607  0.2767 -0.8997  0.0702  0.1369]\n",
      "MSE loss: 96.1365\n",
      "Iteration: 192800\n",
      "Gradient: [  18.5167   16.3145   32.296  -133.7868  -24.1607]\n",
      "Weights: [-4.6605  0.2775 -0.8998  0.0702  0.1369]\n",
      "MSE loss: 96.1303\n",
      "Iteration: 192900\n",
      "Gradient: [  10.0011   36.2591   50.8163 -219.7846 -190.4457]\n",
      "Weights: [-4.6593  0.2772 -0.9     0.0702  0.1369]\n",
      "MSE loss: 96.1231\n",
      "Iteration: 193000\n",
      "Gradient: [  -2.9112  -13.7135   62.2974   42.7802 -583.141 ]\n",
      "Weights: [-4.6589  0.2774 -0.9002  0.0702  0.1369]\n",
      "MSE loss: 96.1166\n",
      "Iteration: 193100\n",
      "Gradient: [-18.3718 -49.0842 109.1222 -88.6166  46.7789]\n",
      "Weights: [-4.6581  0.2775 -0.9004  0.0702  0.1369]\n",
      "MSE loss: 96.1145\n",
      "Iteration: 193200\n",
      "Gradient: [  24.7157   44.7632   -3.8705 -449.6359 -954.486 ]\n",
      "Weights: [-4.6575  0.2774 -0.9006  0.0702  0.1369]\n",
      "MSE loss: 96.1087\n",
      "Iteration: 193300\n",
      "Gradient: [  12.4654   24.3018   80.8217  -27.3528 -696.633 ]\n",
      "Weights: [-4.6583  0.2776 -0.9008  0.0702  0.137 ]\n",
      "MSE loss: 96.1022\n",
      "Iteration: 193400\n",
      "Gradient: [ -8.2207 -24.7829 149.1964 118.6228 529.9036]\n",
      "Weights: [-4.6571  0.2776 -0.9009  0.0702  0.137 ]\n",
      "MSE loss: 96.098\n",
      "Iteration: 193500\n",
      "Gradient: [-16.0913   1.7909  43.3356 -65.3394 423.6693]\n",
      "Weights: [-4.6588  0.2782 -0.9011  0.0702  0.137 ]\n",
      "MSE loss: 96.0882\n",
      "Iteration: 193600\n",
      "Gradient: [  22.6063   -8.5477    4.718  -193.3421  252.6619]\n",
      "Weights: [-4.6578  0.2781 -0.9013  0.0702  0.137 ]\n",
      "MSE loss: 96.0836\n",
      "Iteration: 193700\n",
      "Gradient: [-42.3514   8.9832  28.5272 -68.7613 413.3794]\n",
      "Weights: [-4.6581  0.2781 -0.9014  0.0702  0.137 ]\n",
      "MSE loss: 96.0793\n",
      "Iteration: 193800\n",
      "Gradient: [ -22.8938   41.369   -39.6297 -256.4829  725.8137]\n",
      "Weights: [-4.6583  0.2785 -0.9017  0.0702  0.137 ]\n",
      "MSE loss: 96.0734\n",
      "Iteration: 193900\n",
      "Gradient: [  12.6096   15.9557   24.882  -100.056  -675.9739]\n",
      "Weights: [-4.6595  0.2788 -0.9019  0.0702  0.1371]\n",
      "MSE loss: 96.068\n",
      "Iteration: 194000\n",
      "Gradient: [ -12.7593  -11.1159   43.1063  221.5432 -101.6551]\n",
      "Weights: [-4.6604  0.2799 -0.9022  0.0702  0.1371]\n",
      "MSE loss: 96.0533\n",
      "Iteration: 194100\n",
      "Gradient: [  37.0723    8.5854  -47.5115  114.7751 -872.7969]\n",
      "Weights: [-4.6586  0.2801 -0.9025  0.0702  0.1371]\n",
      "MSE loss: 96.0441\n",
      "Iteration: 194200\n",
      "Gradient: [  19.3061  -67.4367   70.0964 -236.6804 -408.6574]\n",
      "Weights: [-4.658   0.2803 -0.9027  0.0702  0.1371]\n",
      "MSE loss: 96.0374\n",
      "Iteration: 194300\n",
      "Gradient: [-22.7488  18.3564   3.2966  83.1668 -19.3393]\n",
      "Weights: [-4.6603  0.2809 -0.9028  0.0702  0.1371]\n",
      "MSE loss: 96.0298\n",
      "Iteration: 194400\n",
      "Gradient: [   6.6317  -15.661     2.6214  -12.5888 -886.6497]\n",
      "Weights: [-4.6592  0.2813 -0.903   0.0702  0.1371]\n",
      "MSE loss: 96.024\n",
      "Iteration: 194500\n",
      "Gradient: [  -5.163     2.683   135.03     25.6639 -194.5016]\n",
      "Weights: [-4.6598  0.282  -0.9033  0.0701  0.1371]\n",
      "MSE loss: 96.0134\n",
      "Iteration: 194600\n",
      "Gradient: [ 14.2892  35.2336  78.6386  47.4429 842.5073]\n",
      "Weights: [-4.6617  0.2829 -0.9034  0.0701  0.1372]\n",
      "MSE loss: 96.0046\n",
      "Iteration: 194700\n",
      "Gradient: [ -2.1243 -41.4294  -9.8732 470.405   23.581 ]\n",
      "Weights: [-4.6618  0.2835 -0.9036  0.0702  0.1372]\n",
      "MSE loss: 95.999\n",
      "Iteration: 194800\n",
      "Gradient: [  -1.7443  -41.3869   74.7295 -251.3721  -97.9161]\n",
      "Weights: [-4.6615  0.284  -0.9039  0.0701  0.1372]\n",
      "MSE loss: 95.9892\n",
      "Iteration: 194900\n",
      "Gradient: [  -5.9091  -44.909   142.6568  102.2023 -724.7377]\n",
      "Weights: [-4.6621  0.2845 -0.904   0.0701  0.1372]\n",
      "MSE loss: 95.9832\n",
      "Iteration: 195000\n",
      "Gradient: [   0.7954    9.6766  -53.229   180.4892 -276.7247]\n",
      "Weights: [-4.6621  0.2846 -0.9043  0.0701  0.1372]\n",
      "MSE loss: 95.9762\n",
      "Iteration: 195100\n",
      "Gradient: [ -32.8902    2.0884  -34.9001 -162.8588  274.9407]\n",
      "Weights: [-4.6612  0.285  -0.9045  0.0701  0.1372]\n",
      "MSE loss: 95.9695\n",
      "Iteration: 195200\n",
      "Gradient: [  46.857     3.6137  207.5921  306.79   -356.3367]\n",
      "Weights: [-4.6615  0.2855 -0.9048  0.0701  0.1372]\n",
      "MSE loss: 95.9594\n",
      "Iteration: 195300\n",
      "Gradient: [  -3.1628   -3.524   -11.3812  305.5133 -149.1571]\n",
      "Weights: [-4.6643  0.2871 -0.905   0.0701  0.1372]\n",
      "MSE loss: 95.9504\n",
      "Iteration: 195400\n",
      "Gradient: [ -26.7422   38.8444   80.2739  188.375  -557.9245]\n",
      "Weights: [-4.6636  0.2877 -0.9052  0.0701  0.1372]\n",
      "MSE loss: 95.9423\n",
      "Iteration: 195500\n",
      "Gradient: [  17.2668  -35.9727 -103.9544 -248.621  -766.116 ]\n",
      "Weights: [-4.6666  0.2884 -0.9053  0.0701  0.1373]\n",
      "MSE loss: 95.9443\n",
      "Iteration: 195600\n",
      "Gradient: [ -34.4908  -33.7464   19.1942  160.6587 -512.7433]\n",
      "Weights: [-4.6646  0.289  -0.9056  0.0701  0.1373]\n",
      "MSE loss: 95.9266\n",
      "Iteration: 195700\n",
      "Gradient: [ -27.3064    3.1056  -53.5676 -467.5231  331.582 ]\n",
      "Weights: [-4.6633  0.2891 -0.9059  0.0701  0.1373]\n",
      "MSE loss: 95.9205\n",
      "Iteration: 195800\n",
      "Gradient: [  8.6689  53.8511 -52.4421 262.2968  96.9689]\n",
      "Weights: [-4.664   0.2891 -0.9062  0.0701  0.1373]\n",
      "MSE loss: 95.9144\n",
      "Iteration: 195900\n",
      "Gradient: [ -36.6987  -49.681   -36.6232 -294.1068 -521.7252]\n",
      "Weights: [-4.6611  0.2889 -0.9064  0.07    0.1373]\n",
      "MSE loss: 95.9126\n",
      "Iteration: 196000\n",
      "Gradient: [ -13.976    -4.853    63.956   -66.5352 -425.6764]\n",
      "Weights: [-4.6632  0.2897 -0.9065  0.07    0.1373]\n",
      "MSE loss: 95.9009\n",
      "Iteration: 196100\n",
      "Gradient: [  8.6516 -24.2142 -18.1093 256.5449 598.4333]\n",
      "Weights: [-4.6629  0.2905 -0.9068  0.07    0.1373]\n",
      "MSE loss: 95.8918\n",
      "Iteration: 196200\n",
      "Gradient: [ -6.9116 -14.1379 -86.1233 -59.5759  61.0952]\n",
      "Weights: [-4.6646  0.291  -0.907   0.07    0.1373]\n",
      "MSE loss: 95.8837\n",
      "Iteration: 196300\n",
      "Gradient: [ -27.5119   -4.3131    1.1096 -172.668   284.7358]\n",
      "Weights: [-4.6637  0.2911 -0.9071  0.07    0.1373]\n",
      "MSE loss: 95.8791\n",
      "Iteration: 196400\n",
      "Gradient: [ -3.1565   4.9982 135.5303  74.2997 500.6357]\n",
      "Weights: [-4.6644  0.2912 -0.9072  0.07    0.1374]\n",
      "MSE loss: 95.8743\n",
      "Iteration: 196500\n",
      "Gradient: [-13.8896  11.5935  56.5331 -15.0604 424.8129]\n",
      "Weights: [-4.6631  0.2914 -0.9074  0.07    0.1374]\n",
      "MSE loss: 95.8687\n",
      "Iteration: 196600\n",
      "Gradient: [  12.9243   30.1053  -43.378  -126.4204  -37.0461]\n",
      "Weights: [-4.6627  0.2914 -0.9077  0.0699  0.1374]\n",
      "MSE loss: 95.8617\n",
      "Iteration: 196700\n",
      "Gradient: [ -10.2023  -23.7491  115.5775   97.2644 -502.7134]\n",
      "Weights: [-4.6639  0.2921 -0.9078  0.0699  0.1374]\n",
      "MSE loss: 95.8546\n",
      "Iteration: 196800\n",
      "Gradient: [   3.9798   -3.398   -56.6278 -179.9506 -489.7902]\n",
      "Weights: [-4.6639  0.2925 -0.9081  0.0699  0.1374]\n",
      "MSE loss: 95.8474\n",
      "Iteration: 196900\n",
      "Gradient: [ -13.5956  -36.0157  -14.7508 -184.9111 -928.3945]\n",
      "Weights: [-4.6637  0.293  -0.9082  0.0699  0.1374]\n",
      "MSE loss: 95.8402\n",
      "Iteration: 197000\n",
      "Gradient: [  8.9495   7.6083  58.4691 177.2087 711.6218]\n",
      "Weights: [-4.6646  0.2934 -0.9083  0.0699  0.1374]\n",
      "MSE loss: 95.8349\n",
      "Iteration: 197100\n",
      "Gradient: [   4.9614  -41.0896  100.0074   77.0085 -107.8298]\n",
      "Weights: [-4.6653  0.2942 -0.9086  0.0699  0.1374]\n",
      "MSE loss: 95.8264\n",
      "Iteration: 197200\n",
      "Gradient: [ -4.0215 -22.6046 -32.33   319.6181  41.2027]\n",
      "Weights: [-4.664   0.2944 -0.9087  0.0699  0.1375]\n",
      "MSE loss: 95.8255\n",
      "Iteration: 197300\n",
      "Gradient: [  29.3837   30.8582   46.9594 -231.9883  602.8534]\n",
      "Weights: [-4.6638  0.2946 -0.909   0.0699  0.1375]\n",
      "MSE loss: 95.8162\n",
      "Iteration: 197400\n",
      "Gradient: [ -16.7981  -61.0375 -108.113  -171.8898   68.3433]\n",
      "Weights: [-4.665   0.2949 -0.9091  0.0699  0.1375]\n",
      "MSE loss: 95.8093\n",
      "Iteration: 197500\n",
      "Gradient: [ -2.1838  -8.757   -8.8987 -37.0834  88.5505]\n",
      "Weights: [-4.6648  0.2943 -0.9093  0.0699  0.1375]\n",
      "MSE loss: 95.8059\n",
      "Iteration: 197600\n",
      "Gradient: [ -5.9095   3.982  -18.3445 -47.2501 -28.7598]\n",
      "Weights: [-4.664   0.2948 -0.9094  0.0699  0.1375]\n",
      "MSE loss: 95.7998\n",
      "Iteration: 197700\n",
      "Gradient: [  39.4444  -32.7422   71.3064 -125.1954    5.5235]\n",
      "Weights: [-4.6659  0.295  -0.9096  0.0699  0.1375]\n",
      "MSE loss: 95.7933\n",
      "Iteration: 197800\n",
      "Gradient: [ -30.4742  -30.7922   47.5795 -228.1087 -722.317 ]\n",
      "Weights: [-4.6671  0.2959 -0.9098  0.0699  0.1375]\n",
      "MSE loss: 95.786\n",
      "Iteration: 197900\n",
      "Gradient: [   4.9038   67.7189  143.2826 -368.2743  284.4286]\n",
      "Weights: [-4.6665  0.2965 -0.91    0.0699  0.1375]\n",
      "MSE loss: 95.7775\n",
      "Iteration: 198000\n",
      "Gradient: [  -3.9733   19.7777  -65.5432 -125.3269 1016.9962]\n",
      "Weights: [-4.6664  0.2966 -0.9101  0.0699  0.1376]\n",
      "MSE loss: 95.7726\n",
      "Iteration: 198100\n",
      "Gradient: [  14.4504  -21.4333 -124.2555  144.1527  600.0301]\n",
      "Weights: [-4.6661  0.2972 -0.9104  0.0699  0.1376]\n",
      "MSE loss: 95.7654\n",
      "Iteration: 198200\n",
      "Gradient: [-23.5355  13.0317 171.3589 285.4086 -19.7323]\n",
      "Weights: [-4.6664  0.2977 -0.9105  0.0699  0.1376]\n",
      "MSE loss: 95.7612\n",
      "Iteration: 198300\n",
      "Gradient: [  29.2278  -39.6239   62.3416   35.4801 -104.6922]\n",
      "Weights: [-4.6664  0.2979 -0.9106  0.0699  0.1376]\n",
      "MSE loss: 95.757\n",
      "Iteration: 198400\n",
      "Gradient: [  -1.6326  -19.5558   -7.1565 -224.9771  451.2675]\n",
      "Weights: [-4.6677  0.2987 -0.9109  0.0698  0.1376]\n",
      "MSE loss: 95.7477\n",
      "Iteration: 198500\n",
      "Gradient: [ -10.8036  -16.5049  -51.6399 -282.8055   72.1139]\n",
      "Weights: [-4.6675  0.2991 -0.9111  0.0698  0.1376]\n",
      "MSE loss: 95.7411\n",
      "Iteration: 198600\n",
      "Gradient: [  31.4746  -89.5825   76.7774 -235.1385  113.8356]\n",
      "Weights: [-4.6676  0.2994 -0.9113  0.0698  0.1376]\n",
      "MSE loss: 95.7348\n",
      "Iteration: 198700\n",
      "Gradient: [ 15.282  -28.4487 -45.5001   8.3945  17.1492]\n",
      "Weights: [-4.667   0.3    -0.9115  0.0698  0.1376]\n",
      "MSE loss: 95.7287\n",
      "Iteration: 198800\n",
      "Gradient: [  20.7631  -30.6509 -103.7617 -312.3499  624.0505]\n",
      "Weights: [-4.6667  0.3005 -0.9117  0.0698  0.1376]\n",
      "MSE loss: 95.7233\n",
      "Iteration: 198900\n",
      "Gradient: [ -11.341    12.2637    8.3549   13.1342 -227.8144]\n",
      "Weights: [-4.6684  0.3005 -0.9119  0.0698  0.1376]\n",
      "MSE loss: 95.719\n",
      "Iteration: 199000\n",
      "Gradient: [   54.5246   -28.9679    83.8281  -180.7773 -1109.843 ]\n",
      "Weights: [-4.6687  0.3014 -0.912   0.0698  0.1377]\n",
      "MSE loss: 95.7102\n",
      "Iteration: 199100\n",
      "Gradient: [  -28.1765    50.9104   131.156    -36.219  -1199.6457]\n",
      "Weights: [-4.6694  0.3015 -0.9121  0.0698  0.1377]\n",
      "MSE loss: 95.7069\n",
      "Iteration: 199200\n",
      "Gradient: [  -8.0319  -14.829    85.6004 -319.8244  -21.9972]\n",
      "Weights: [-4.6688  0.3017 -0.9123  0.0698  0.1377]\n",
      "MSE loss: 95.7005\n",
      "Iteration: 199300\n",
      "Gradient: [ -11.0319   -9.2259  107.0541 -125.8814 1148.1827]\n",
      "Weights: [-4.6671  0.3021 -0.9126  0.0698  0.1377]\n",
      "MSE loss: 95.6943\n",
      "Iteration: 199400\n",
      "Gradient: [  25.1927    9.8167  -35.5225   56.6515 -702.7117]\n",
      "Weights: [-4.6688  0.3023 -0.9128  0.0698  0.1377]\n",
      "MSE loss: 95.6854\n",
      "Iteration: 199500\n",
      "Gradient: [ 27.8231  26.6938 139.8538 535.9902 355.2581]\n",
      "Weights: [-4.6672  0.302  -0.9129  0.0698  0.1377]\n",
      "MSE loss: 95.6838\n",
      "Iteration: 199600\n",
      "Gradient: [  35.6193   24.9834   84.9992  -32.9587 -654.3826]\n",
      "Weights: [-4.6675  0.3024 -0.9131  0.0698  0.1377]\n",
      "MSE loss: 95.6762\n",
      "Iteration: 199700\n",
      "Gradient: [ -11.7157   45.8578   41.3757 -101.8101 -666.2177]\n",
      "Weights: [-4.6675  0.3025 -0.9133  0.0697  0.1378]\n",
      "MSE loss: 95.6706\n",
      "Iteration: 199800\n",
      "Gradient: [ 19.6789  34.7873  13.4888 456.6535 895.5034]\n",
      "Weights: [-4.6665  0.3027 -0.9134  0.0697  0.1378]\n",
      "MSE loss: 95.6673\n",
      "Iteration: 199900\n",
      "Gradient: [  10.5303  -28.672   -20.2543   38.0932 -105.1402]\n",
      "Weights: [-4.6675  0.303  -0.9136  0.0697  0.1378]\n",
      "MSE loss: 95.6571\n"
     ]
    }
   ],
   "source": [
    "# Обучение на полном датасете.\n",
    "# lr - индивидуальный для каждого из параметров модели.\n",
    "weights_2, losses_2, iter_final_2, fit_time_2 = model_fit(X_train, y_train,\n",
    "                                                          learning_rate=[1e-5, 1e-6, 1e-7, 1e-8, 1e-9],\n",
    "                                                          tolerance=(0.2**2 * N_points),\n",
    "                                                          beta=0,\n",
    "                                                          batch_ratio=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb1858e",
   "metadata": {},
   "source": [
    "В заданное максимальное число итераций также не достигнут желаемый уровень ошибки. Но скорость сходимости улучшилась значительно."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80ba27d",
   "metadata": {},
   "source": [
    "Попробуем обучать не на полном датасете, а на случайно выбранных значениях.\n",
    "За счет обучения на батчах обучение устойчиво при бОльших значениях learning rate. Получается, разбиение на батчи не только ускоряет вычисления, но и позволяет улучшить сходимость за счет большего learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51dbe209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Gradient: [ 1722.7644  2196.2994  3867.5832  7571.7992 14229.5169]\n",
      "Weights: [-0.001  -0.0001 -0.     -0.     -0.    ]\n",
      "MSE loss: 39312.0109\n",
      "Iteration: 100\n",
      "Gradient: [   -1.8276    24.9851    23.6876  -761.9254 -1649.0665]\n",
      "Weights: [-5.0723 -0.432   0.1535  0.078   0.0297]\n",
      "MSE loss: 701.1158\n",
      "Iteration: 200\n",
      "Gradient: [   9.1634  -14.4461   80.4348 -150.9966  -59.0444]\n",
      "Weights: [-4.8031 -0.7214  0.1299  0.0936  0.038 ]\n",
      "MSE loss: 478.0744\n",
      "Iteration: 300\n",
      "Gradient: [  -8.5658   45.9752   54.3001 -139.3846 -203.7154]\n",
      "Weights: [-4.66   -0.8985  0.0998  0.1062  0.0441]\n",
      "MSE loss: 365.4998\n",
      "Iteration: 400\n",
      "Gradient: [  11.4396   20.0832  108.2621  141.2623 -954.0145]\n",
      "Weights: [-4.4979 -1.0547  0.0887  0.1151  0.0487]\n",
      "MSE loss: 297.0236\n",
      "Iteration: 500\n",
      "Gradient: [  6.8643  34.5816 -57.0694  45.4597  39.1937]\n",
      "Weights: [-4.3603 -1.1645  0.0691  0.123   0.0523]\n",
      "MSE loss: 259.6127\n",
      "Iteration: 600\n",
      "Gradient: [  -6.4478   -7.098    67.5645   25.3931 -373.2218]\n",
      "Weights: [-4.3036 -1.2407  0.0539  0.1268  0.0554]\n",
      "MSE loss: 236.7332\n",
      "Iteration: 700\n",
      "Gradient: [  -1.292   -14.4534   88.923   -25.5222 -625.9515]\n",
      "Weights: [-4.222  -1.2911  0.0376  0.1275  0.0586]\n",
      "MSE loss: 223.8192\n",
      "Iteration: 800\n",
      "Gradient: [ -4.3393 -12.7025   9.7426 123.9612  47.3221]\n",
      "Weights: [-4.2325 -1.3142  0.0278  0.1286  0.0606]\n",
      "MSE loss: 217.6683\n",
      "Iteration: 900\n",
      "Gradient: [  10.9243   12.8557   29.8313  101.5315 -286.6364]\n",
      "Weights: [-4.195  -1.3368  0.0188  0.1296  0.0624]\n",
      "MSE loss: 212.4924\n",
      "Iteration: 1000\n",
      "Gradient: [  0.9455  12.1334   4.2779 -21.064   21.5845]\n",
      "Weights: [-4.16   -1.343   0.0121  0.1292  0.0636]\n",
      "MSE loss: 210.4195\n",
      "Iteration: 1100\n",
      "Gradient: [  2.9961  10.934  -55.9258 -34.251  293.0071]\n",
      "Weights: [-4.1542 -1.358   0.0061  0.1302  0.0647]\n",
      "MSE loss: 208.0985\n",
      "Iteration: 1200\n",
      "Gradient: [-15.9377 -21.9071  12.2022 133.2503 -47.9245]\n",
      "Weights: [-4.1848 -1.3406 -0.0044  0.128   0.0659]\n",
      "MSE loss: 206.8237\n",
      "Iteration: 1300\n",
      "Gradient: [ 21.4454 -10.7731  59.0085  65.1871 -32.7533]\n",
      "Weights: [-4.1882 -1.3225 -0.0124  0.1281  0.0668]\n",
      "MSE loss: 203.9192\n",
      "Iteration: 1400\n",
      "Gradient: [   6.778    -7.3984  -79.2231 -179.9291  309.3892]\n",
      "Weights: [-4.1571 -1.3045 -0.0257  0.1259  0.0679]\n",
      "MSE loss: 200.9399\n",
      "Iteration: 1500\n",
      "Gradient: [   2.9707   -2.0868  -40.9774   47.0746 -123.3078]\n",
      "Weights: [-4.1646 -1.2987 -0.0338  0.1261  0.069 ]\n",
      "MSE loss: 198.9217\n",
      "Iteration: 1600\n",
      "Gradient: [ -4.5415  -2.03    45.3044 -27.5386  73.9888]\n",
      "Weights: [-4.1579 -1.2843 -0.0425  0.1239  0.0696]\n",
      "MSE loss: 197.412\n",
      "Iteration: 1700\n",
      "Gradient: [  7.1182 -39.1559  -9.4464 122.6148 -33.008 ]\n",
      "Weights: [-4.1631 -1.2807 -0.0488  0.1239  0.0707]\n",
      "MSE loss: 195.4643\n",
      "Iteration: 1800\n",
      "Gradient: [  -6.7244    8.0754   34.8768  -26.4634 -628.18  ]\n",
      "Weights: [-4.1816 -1.2582 -0.0589  0.1232  0.0715]\n",
      "MSE loss: 193.3893\n",
      "Iteration: 1900\n",
      "Gradient: [ 13.3454  -8.7478  87.5642 -17.2904 -58.2233]\n",
      "Weights: [-4.1632 -1.2507 -0.0638  0.1221  0.0719]\n",
      "MSE loss: 192.6536\n",
      "Iteration: 2000\n",
      "Gradient: [11.7344  0.54   18.7925 11.6855 30.8562]\n",
      "Weights: [-4.1767 -1.2269 -0.0763  0.1209  0.0732]\n",
      "MSE loss: 189.8358\n",
      "Iteration: 2100\n",
      "Gradient: [ 14.2311 -10.0117  26.2182  86.0004 370.6368]\n",
      "Weights: [-4.1795 -1.2108 -0.0881  0.1209  0.074 ]\n",
      "MSE loss: 187.8243\n",
      "Iteration: 2200\n",
      "Gradient: [  -1.2202  -30.5353  -64.1414  124.1467 -266.5244]\n",
      "Weights: [-4.215  -1.1887 -0.0959  0.1202  0.0745]\n",
      "MSE loss: 186.5832\n",
      "Iteration: 2300\n",
      "Gradient: [   2.0211   -6.1979  -40.9559  139.7345 -305.2343]\n",
      "Weights: [-4.2354 -1.161  -0.1055  0.1189  0.0751]\n",
      "MSE loss: 185.2092\n",
      "Iteration: 2400\n",
      "Gradient: [-14.0673 -20.7973 -14.7305 -83.0705 237.7078]\n",
      "Weights: [-4.2135 -1.1467 -0.1144  0.1187  0.076 ]\n",
      "MSE loss: 182.7823\n",
      "Iteration: 2500\n",
      "Gradient: [  6.8445   0.3623   9.8851 129.4321   4.566 ]\n",
      "Weights: [-4.2315 -1.1173 -0.1267  0.1187  0.0765]\n",
      "MSE loss: 180.8339\n",
      "Iteration: 2600\n",
      "Gradient: [  4.0372   1.9344  23.5645 -50.1243 171.2403]\n",
      "Weights: [-4.2481 -1.0957 -0.1354  0.1173  0.0773]\n",
      "MSE loss: 179.0417\n",
      "Iteration: 2700\n",
      "Gradient: [-10.2139  -5.91    15.1905 141.0157   3.1593]\n",
      "Weights: [-4.2404 -1.0815 -0.1431  0.1154  0.078 ]\n",
      "MSE loss: 177.3996\n",
      "Iteration: 2800\n",
      "Gradient: [   3.098   -10.8304   -3.0965 -100.1472  117.1842]\n",
      "Weights: [-4.226  -1.0784 -0.1528  0.1155  0.0791]\n",
      "MSE loss: 175.7832\n",
      "Iteration: 2900\n",
      "Gradient: [ -11.9545  -14.3431  -50.8622   77.8622 -527.434 ]\n",
      "Weights: [-4.2579 -1.0608 -0.1586  0.1137  0.0797]\n",
      "MSE loss: 174.604\n",
      "Iteration: 3000\n",
      "Gradient: [ -10.553    -3.7472   19.5488  -85.7664 -332.7165]\n",
      "Weights: [-4.2497 -1.0608 -0.1671  0.1144  0.0805]\n",
      "MSE loss: 173.4059\n",
      "Iteration: 3100\n",
      "Gradient: [   3.7184   -5.7751  -48.8749  -26.5108 -574.6757]\n",
      "Weights: [-4.2246 -1.0421 -0.1758  0.1144  0.081 ]\n",
      "MSE loss: 172.5396\n",
      "Iteration: 3200\n",
      "Gradient: [  2.1294   6.5951  44.9864  93.7979 165.544 ]\n",
      "Weights: [-4.2348 -1.0431 -0.1825  0.1144  0.0817]\n",
      "MSE loss: 170.5499\n",
      "Iteration: 3300\n",
      "Gradient: [-11.9407   1.1373 -76.6466 -42.4561 -19.3676]\n",
      "Weights: [-4.2743 -1.0066 -0.1904  0.1135  0.0819]\n",
      "MSE loss: 169.3671\n",
      "Iteration: 3400\n",
      "Gradient: [  -3.3006    1.9929    2.0143    4.7    -191.9425]\n",
      "Weights: [-4.2683 -0.9872 -0.2023  0.1121  0.0827]\n",
      "MSE loss: 167.3715\n",
      "Iteration: 3500\n",
      "Gradient: [  11.9923  -13.9488   46.8115 -167.0876 -323.3664]\n",
      "Weights: [-4.2536 -0.9815 -0.207   0.1111  0.0834]\n",
      "MSE loss: 166.3422\n",
      "Iteration: 3600\n",
      "Gradient: [ -6.4062 -15.036    0.7859   3.136  -41.8275]\n",
      "Weights: [-4.275  -0.9733 -0.2103  0.1098  0.0843]\n",
      "MSE loss: 165.2097\n",
      "Iteration: 3700\n",
      "Gradient: [   4.4368    8.9129   18.9841  135.2775 -209.0709]\n",
      "Weights: [-4.2701 -0.9545 -0.2232  0.1098  0.0852]\n",
      "MSE loss: 163.1969\n",
      "Iteration: 3800\n",
      "Gradient: [  5.3317  11.3378 -30.5243 -33.1347 -40.1053]\n",
      "Weights: [-4.2755 -0.9482 -0.2304  0.1099  0.0859]\n",
      "MSE loss: 162.1397\n",
      "Iteration: 3900\n",
      "Gradient: [  4.0265   2.9131  35.1937   3.1747 -22.0425]\n",
      "Weights: [-4.3002 -0.9058 -0.2391  0.1079  0.0863]\n",
      "MSE loss: 160.5606\n",
      "Iteration: 4000\n",
      "Gradient: [ -5.9678  -7.6663 -98.0029  37.7918 157.3592]\n",
      "Weights: [-4.2872 -0.8967 -0.2483  0.1076  0.087 ]\n",
      "MSE loss: 159.2857\n",
      "Iteration: 4100\n",
      "Gradient: [   8.7699  -10.1735  109.3906   24.6051 -309.0996]\n",
      "Weights: [-4.2924 -0.8926 -0.2534  0.1059  0.0877]\n",
      "MSE loss: 158.1581\n",
      "Iteration: 4200\n",
      "Gradient: [   8.2395   -1.0199   36.3801 -120.947   -58.3332]\n",
      "Weights: [-4.2924 -0.878  -0.261   0.1049  0.0882]\n",
      "MSE loss: 157.1997\n",
      "Iteration: 4300\n",
      "Gradient: [   5.1108   12.7612   12.7627  130.955  -134.1545]\n",
      "Weights: [-4.3071 -0.8521 -0.2696  0.1054  0.0889]\n",
      "MSE loss: 155.7679\n",
      "Iteration: 4400\n",
      "Gradient: [   7.1445   -3.977    62.4772   32.2902 -184.9528]\n",
      "Weights: [-4.2842 -0.8482 -0.2805  0.1055  0.0894]\n",
      "MSE loss: 154.7989\n",
      "Iteration: 4500\n",
      "Gradient: [   6.1694   14.7427   -7.4994  -47.4801 -114.9211]\n",
      "Weights: [-4.3065 -0.8304 -0.2881  0.1053  0.0899]\n",
      "MSE loss: 153.2476\n",
      "Iteration: 4600\n",
      "Gradient: [  2.9395   8.031   -1.2691  46.5374 309.2315]\n",
      "Weights: [-4.3276 -0.8071 -0.2918  0.1042  0.0903]\n",
      "MSE loss: 152.4116\n",
      "Iteration: 4700\n",
      "Gradient: [   6.1191   -5.1131   41.3295   51.2468 -240.311 ]\n",
      "Weights: [-4.322  -0.7908 -0.3036  0.1036  0.0911]\n",
      "MSE loss: 150.8454\n",
      "Iteration: 4800\n",
      "Gradient: [  -7.6103  -29.26     -6.5977  -93.8318 -135.2727]\n",
      "Weights: [-4.3404 -0.7743 -0.3122  0.1025  0.0916]\n",
      "MSE loss: 149.9019\n",
      "Iteration: 4900\n",
      "Gradient: [   0.8747  -14.7054   -2.2447   46.943  -112.7973]\n",
      "Weights: [-4.3387 -0.7603 -0.3227  0.1035  0.0925]\n",
      "MSE loss: 148.0719\n",
      "Iteration: 5000\n",
      "Gradient: [ 5.2993  4.6624 35.5503 93.3805 38.2665]\n",
      "Weights: [-4.3294 -0.7669 -0.327   0.1037  0.0935]\n",
      "MSE loss: 147.4006\n",
      "Iteration: 5100\n",
      "Gradient: [  -1.5045  -19.9328   34.4048  -82.7655 -178.787 ]\n",
      "Weights: [-4.3474 -0.7565 -0.3323  0.103   0.0938]\n",
      "MSE loss: 146.8519\n",
      "Iteration: 5200\n",
      "Gradient: [  -5.8523   -2.4531   19.5359  134.0475 -712.5258]\n",
      "Weights: [-4.3368 -0.7365 -0.3392  0.1008  0.0943]\n",
      "MSE loss: 145.4133\n",
      "Iteration: 5300\n",
      "Gradient: [  17.5517  -12.8252  -12.968   -42.8787 -412.7038]\n",
      "Weights: [-4.3418 -0.726  -0.3437  0.1003  0.0946]\n",
      "MSE loss: 144.8206\n",
      "Iteration: 5400\n",
      "Gradient: [ 14.6848  -1.6635  34.2317 126.5393  48.6513]\n",
      "Weights: [-4.3494 -0.709  -0.3486  0.1     0.0952]\n",
      "MSE loss: 144.1366\n",
      "Iteration: 5500\n",
      "Gradient: [   9.2998    3.1872   40.3931  -43.5874 -102.8317]\n",
      "Weights: [-4.3458 -0.7163 -0.3517  0.0997  0.0957]\n",
      "MSE loss: 143.4322\n",
      "Iteration: 5600\n",
      "Gradient: [   7.8131  -11.4672   10.3546   25.9026 -115.8317]\n",
      "Weights: [-4.3387 -0.716  -0.3565  0.0996  0.0964]\n",
      "MSE loss: 142.7539\n",
      "Iteration: 5700\n",
      "Gradient: [   4.3252   -5.7464  -47.9515   51.2995 -135.1143]\n",
      "Weights: [-4.3444 -0.6921 -0.3681  0.0984  0.0972]\n",
      "MSE loss: 141.1838\n",
      "Iteration: 5800\n",
      "Gradient: [  21.1632   -4.5194   46.7005  -43.6057 -253.7519]\n",
      "Weights: [-4.3462 -0.6762 -0.3699  0.097   0.0977]\n",
      "MSE loss: 141.2389\n",
      "Iteration: 5900\n",
      "Gradient: [  12.9079  -21.2958    8.1259  126.7911 -136.5287]\n",
      "Weights: [-4.3376 -0.6645 -0.3769  0.096   0.0981]\n",
      "MSE loss: 140.76\n",
      "Iteration: 6000\n",
      "Gradient: [  3.7767 -28.6995 -13.3506  30.6544 -46.1202]\n",
      "Weights: [-4.3737 -0.654  -0.3802  0.0947  0.0987]\n",
      "MSE loss: 138.9651\n",
      "Iteration: 6100\n",
      "Gradient: [   6.8221   -7.5715   12.6567  -83.9837 -197.1282]\n",
      "Weights: [-4.3685 -0.6436 -0.3857  0.0947  0.0989]\n",
      "MSE loss: 138.2953\n",
      "Iteration: 6200\n",
      "Gradient: [ -4.2525  -7.5398  15.759   21.3031 160.2027]\n",
      "Weights: [-4.3834 -0.6313 -0.3933  0.0954  0.0994]\n",
      "MSE loss: 137.4697\n",
      "Iteration: 6300\n",
      "Gradient: [   4.7104  -10.6204  -23.7486 -191.0629  -80.447 ]\n",
      "Weights: [-4.4041 -0.6225 -0.4033  0.0963  0.1001]\n",
      "MSE loss: 137.5172\n",
      "Iteration: 6400\n",
      "Gradient: [  -0.4127   24.9333   16.2589   77.0184 -274.2294]\n",
      "Weights: [-4.376  -0.611  -0.4083  0.0961  0.1003]\n",
      "MSE loss: 135.9697\n",
      "Iteration: 6500\n",
      "Gradient: [   3.1319    4.6043   -6.1715   39.4756 -111.8198]\n",
      "Weights: [-4.3877 -0.6141 -0.4114  0.0949  0.1008]\n",
      "MSE loss: 135.9909\n",
      "Iteration: 6600\n",
      "Gradient: [  10.518    14.0229   -2.3126  154.0959 -245.2932]\n",
      "Weights: [-4.3674 -0.6105 -0.4159  0.0949  0.1013]\n",
      "MSE loss: 134.8723\n",
      "Iteration: 6700\n",
      "Gradient: [ 0.7475  0.8794  1.2424 73.1327 72.1216]\n",
      "Weights: [-4.3706 -0.6036 -0.421   0.0953  0.1019]\n",
      "MSE loss: 134.3372\n",
      "Iteration: 6800\n",
      "Gradient: [ -8.629    7.3451   7.9657 -33.7041  77.326 ]\n",
      "Weights: [-4.3912 -0.594  -0.4267  0.0946  0.1023]\n",
      "MSE loss: 133.8831\n",
      "Iteration: 6900\n",
      "Gradient: [ 18.5908  -8.5458  -5.9362  51.1603 121.8026]\n",
      "Weights: [-4.377  -0.5765 -0.4356  0.094   0.103 ]\n",
      "MSE loss: 132.4573\n",
      "Iteration: 7000\n",
      "Gradient: [ 16.2821  16.1021  -9.9435  52.2996 123.9694]\n",
      "Weights: [-4.3744 -0.5669 -0.4439  0.0945  0.1033]\n",
      "MSE loss: 131.7541\n",
      "Iteration: 7100\n",
      "Gradient: [   0.9911  -10.9828  -40.8826   -0.9179 -343.6821]\n",
      "Weights: [-4.3983 -0.5446 -0.4492  0.0932  0.1037]\n",
      "MSE loss: 130.7729\n",
      "Iteration: 7200\n",
      "Gradient: [ -3.1572  -0.1212  -3.8433 -29.6634  49.9008]\n",
      "Weights: [-4.4035 -0.5309 -0.4556  0.0931  0.1039]\n",
      "MSE loss: 130.0992\n",
      "Iteration: 7300\n",
      "Gradient: [  -7.7284   11.0301  -37.7142 -261.5334 -170.2165]\n",
      "Weights: [-4.4334 -0.5193 -0.4578  0.0926  0.1042]\n",
      "MSE loss: 130.621\n",
      "Iteration: 7400\n",
      "Gradient: [  3.1409   6.0532  -8.7868 -86.6031 157.391 ]\n",
      "Weights: [-4.4248 -0.5026 -0.469   0.0918  0.1049]\n",
      "MSE loss: 128.9401\n",
      "Iteration: 7500\n",
      "Gradient: [ -12.3508   -8.1774   39.3062 -101.0235  -66.6275]\n",
      "Weights: [-4.4293 -0.4965 -0.4719  0.0915  0.1054]\n",
      "MSE loss: 128.3461\n",
      "Iteration: 7600\n",
      "Gradient: [  -4.0484   -4.0406  -15.2465   38.088  -369.4745]\n",
      "Weights: [-4.4279 -0.4803 -0.4769  0.0903  0.1056]\n",
      "MSE loss: 127.5194\n",
      "Iteration: 7700\n",
      "Gradient: [  0.6566  -4.7475 -10.3702 -33.3055 379.1611]\n",
      "Weights: [-4.4207 -0.4742 -0.4842  0.0901  0.1065]\n",
      "MSE loss: 126.5442\n",
      "Iteration: 7800\n",
      "Gradient: [  0.2467 -17.1044  10.3839 -47.1517  47.1992]\n",
      "Weights: [-4.4095 -0.4774 -0.4885  0.0891  0.1073]\n",
      "MSE loss: 126.0348\n",
      "Iteration: 7900\n",
      "Gradient: [ -7.4136 -14.0924 -16.9615  -2.776  152.0104]\n",
      "Weights: [-4.4276 -0.4588 -0.4926  0.0891  0.1074]\n",
      "MSE loss: 125.4714\n",
      "Iteration: 8000\n",
      "Gradient: [   1.2714   10.2844  -22.1084   82.2381 -191.7122]\n",
      "Weights: [-4.4231 -0.4582 -0.4943  0.0886  0.108 ]\n",
      "MSE loss: 125.2841\n",
      "Iteration: 8100\n",
      "Gradient: [  -1.8801   -7.449    41.3962  154.4092 -234.0811]\n",
      "Weights: [-4.4202 -0.4438 -0.4988  0.0866  0.1083]\n",
      "MSE loss: 124.6224\n",
      "Iteration: 8200\n",
      "Gradient: [ -13.8808   -6.2204   -1.796    48.0594 -206.5605]\n",
      "Weights: [-4.4373 -0.4389 -0.5049  0.0871  0.1089]\n",
      "MSE loss: 123.9992\n",
      "Iteration: 8300\n",
      "Gradient: [-2.4947 12.2473  2.6829 15.142  27.8789]\n",
      "Weights: [-4.4352 -0.4283 -0.5121  0.0862  0.1094]\n",
      "MSE loss: 123.4028\n",
      "Iteration: 8400\n",
      "Gradient: [ -7.8429  -1.7419  57.5499 105.0813 -32.7833]\n",
      "Weights: [-4.4503 -0.4113 -0.5162  0.0863  0.1099]\n",
      "MSE loss: 122.7246\n",
      "Iteration: 8500\n",
      "Gradient: [  3.433    1.9798  40.5926 -67.0257 348.3738]\n",
      "Weights: [-4.4207 -0.4137 -0.5229  0.0855  0.1102]\n",
      "MSE loss: 122.3212\n",
      "Iteration: 8600\n",
      "Gradient: [ -14.6036  -17.7722   30.7826  -37.1846 -264.054 ]\n",
      "Weights: [-4.4555 -0.4099 -0.525   0.086   0.1105]\n",
      "MSE loss: 122.881\n",
      "Iteration: 8700\n",
      "Gradient: [  4.5099 -12.9027 -26.8429  44.9926  52.1905]\n",
      "Weights: [-4.4373 -0.3991 -0.5325  0.0869  0.1107]\n",
      "MSE loss: 121.3343\n",
      "Iteration: 8800\n",
      "Gradient: [  14.1774   13.9177   22.678   -57.4924 -106.3   ]\n",
      "Weights: [-4.4477 -0.3868 -0.5372  0.0856  0.1113]\n",
      "MSE loss: 120.7858\n",
      "Iteration: 8900\n",
      "Gradient: [   3.3557   -2.1242   14.1092  208.2448 -447.2766]\n",
      "Weights: [-4.4766 -0.3529 -0.5429  0.0839  0.1119]\n",
      "MSE loss: 119.8937\n",
      "Iteration: 9000\n",
      "Gradient: [ -2.7514  12.3122 -30.8005  85.1664  80.0851]\n",
      "Weights: [-4.4639 -0.3452 -0.5495  0.0831  0.1123]\n",
      "MSE loss: 119.0845\n",
      "Iteration: 9100\n",
      "Gradient: [-6.050000e-02  2.662930e+01  2.547280e+01  9.697710e+01  2.147756e+02]\n",
      "Weights: [-4.4683 -0.3424 -0.5535  0.0834  0.1126]\n",
      "MSE loss: 118.809\n",
      "Iteration: 9200\n",
      "Gradient: [  0.3941 -34.7779  36.4373   3.5718  73.1423]\n",
      "Weights: [-4.4679 -0.3399 -0.5541  0.0833  0.1128]\n",
      "MSE loss: 118.7318\n",
      "Iteration: 9300\n",
      "Gradient: [   4.7302   20.7509   22.6098   97.6913 -299.113 ]\n",
      "Weights: [-4.4606 -0.332  -0.5594  0.0834  0.113 ]\n",
      "MSE loss: 118.3598\n",
      "Iteration: 9400\n",
      "Gradient: [  10.8393   -3.7793    3.8979   60.8057 -110.6496]\n",
      "Weights: [-4.4667 -0.3293 -0.562   0.0834  0.1133]\n",
      "MSE loss: 118.0581\n",
      "Iteration: 9500\n",
      "Gradient: [  10.1221   24.8282  -13.6532 -100.7444 -127.832 ]\n",
      "Weights: [-4.4526 -0.3324 -0.5653  0.0813  0.1139]\n",
      "MSE loss: 117.8149\n",
      "Iteration: 9600\n",
      "Gradient: [  -2.1261    6.2862   27.8729  -66.4535 -168.2904]\n",
      "Weights: [-4.4669 -0.3235 -0.5667  0.0815  0.1141]\n",
      "MSE loss: 117.3376\n",
      "Iteration: 9700\n",
      "Gradient: [  3.204   -1.0762 -24.1091  81.8027  99.0264]\n",
      "Weights: [-4.4804 -0.3102 -0.5722  0.0818  0.1144]\n",
      "MSE loss: 116.9331\n",
      "Iteration: 9800\n",
      "Gradient: [   8.6705    1.5386   10.7529 -144.9372  -99.5079]\n",
      "Weights: [-4.4737 -0.2887 -0.581   0.0814  0.1146]\n",
      "MSE loss: 116.2494\n",
      "Iteration: 9900\n",
      "Gradient: [   7.6066   -5.9982  -10.479  -136.8529 -231.7664]\n",
      "Weights: [-4.4851 -0.268  -0.5896  0.0814  0.1147]\n",
      "MSE loss: 115.5697\n",
      "Iteration: 10000\n",
      "Gradient: [  3.475  -22.8949  31.6709   9.8481  59.8178]\n",
      "Weights: [-4.5003 -0.259  -0.5927  0.081   0.1152]\n",
      "MSE loss: 115.1019\n",
      "Iteration: 10100\n",
      "Gradient: [  -7.9585   -6.1572   -5.3499   12.8593 -172.1723]\n",
      "Weights: [-4.4811 -0.2618 -0.5977  0.0815  0.1157]\n",
      "MSE loss: 114.8884\n",
      "Iteration: 10200\n",
      "Gradient: [ -0.5503  11.4355 -69.6318  80.4109 122.5514]\n",
      "Weights: [-4.4896 -0.2563 -0.6016  0.0815  0.116 ]\n",
      "MSE loss: 114.402\n",
      "Iteration: 10300\n",
      "Gradient: [ 3.5765 16.256  44.6411 26.9691 78.6139]\n",
      "Weights: [-4.4672 -0.2477 -0.6076  0.0808  0.1162]\n",
      "MSE loss: 114.7152\n",
      "Iteration: 10400\n",
      "Gradient: [ -4.5281   9.7779  23.7948  16.1507 157.358 ]\n",
      "Weights: [-4.5188 -0.2278 -0.6101  0.0812  0.1166]\n",
      "MSE loss: 113.8765\n",
      "Iteration: 10500\n",
      "Gradient: [  -3.5351   13.6329  -60.1724 -187.2902   20.6704]\n",
      "Weights: [-4.5099 -0.2344 -0.6103  0.0796  0.1168]\n",
      "MSE loss: 113.7079\n",
      "Iteration: 10600\n",
      "Gradient: [ -0.4684 -19.2159  32.3495 123.8704  76.796 ]\n",
      "Weights: [-4.4929 -0.2331 -0.6106  0.0787  0.1172]\n",
      "MSE loss: 113.3968\n",
      "Iteration: 10700\n",
      "Gradient: [ -3.9652  -0.8349 -14.6071 131.8645 -21.4807]\n",
      "Weights: [-4.5135 -0.219  -0.613   0.0776  0.1176]\n",
      "MSE loss: 112.9709\n",
      "Iteration: 10800\n",
      "Gradient: [ -3.5451   5.2356  16.011  104.9411  28.5463]\n",
      "Weights: [-4.5119 -0.2231 -0.6159  0.0779  0.1178]\n",
      "MSE loss: 112.9051\n",
      "Iteration: 10900\n",
      "Gradient: [ -5.4424 -11.4916 -25.8469 100.31   -78.7015]\n",
      "Weights: [-4.5085 -0.2169 -0.62    0.0782  0.1177]\n",
      "MSE loss: 112.7004\n",
      "Iteration: 11000\n",
      "Gradient: [ -1.1004 -18.5433  -2.8908 -18.7745  79.5274]\n",
      "Weights: [-4.5179 -0.2031 -0.625   0.0783  0.1178]\n",
      "MSE loss: 112.3513\n",
      "Iteration: 11100\n",
      "Gradient: [  6.7391  -1.7898 -16.5772 137.1791 188.1798]\n",
      "Weights: [-4.5324 -0.1891 -0.6287  0.0788  0.1181]\n",
      "MSE loss: 112.0422\n",
      "Iteration: 11200\n",
      "Gradient: [  -6.1606   -4.5104  -12.9512 -142.0224 -414.7072]\n",
      "Weights: [-4.5238 -0.1907 -0.6349  0.0789  0.1183]\n",
      "MSE loss: 111.978\n",
      "Iteration: 11300\n",
      "Gradient: [ 10.1791  -2.6659  41.6935 -79.6071  -6.2327]\n",
      "Weights: [-4.5258 -0.1743 -0.6402  0.0784  0.1187]\n",
      "MSE loss: 111.1198\n",
      "Iteration: 11400\n",
      "Gradient: [-8.750000e-02 -1.959100e+00  1.161320e+01  4.427360e+01  1.899522e+02]\n",
      "Weights: [-4.5134 -0.1718 -0.6443  0.0788  0.1192]\n",
      "MSE loss: 110.9728\n",
      "Iteration: 11500\n",
      "Gradient: [ -13.1848   -1.1898   17.1758 -147.8692 -317.0714]\n",
      "Weights: [-4.5238 -0.1735 -0.6504  0.0788  0.1195]\n",
      "MSE loss: 110.9731\n",
      "Iteration: 11600\n",
      "Gradient: [ 11.698  -13.906   61.9597 -56.2773 186.8558]\n",
      "Weights: [-4.5288 -0.1606 -0.6545  0.0789  0.1199]\n",
      "MSE loss: 110.0338\n",
      "Iteration: 11700\n",
      "Gradient: [ -2.4556   7.3316  -1.5882 -13.0426 -19.0891]\n",
      "Weights: [-4.5114 -0.1577 -0.656   0.0777  0.12  ]\n",
      "MSE loss: 109.933\n",
      "Iteration: 11800\n",
      "Gradient: [   3.2107   17.2378  -29.35    -31.9672 -278.3141]\n",
      "Weights: [-4.5209 -0.1453 -0.6602  0.0773  0.1205]\n",
      "MSE loss: 109.4601\n",
      "Iteration: 11900\n",
      "Gradient: [  -1.3355   10.9327   -3.2792   84.1096 -168.3552]\n",
      "Weights: [-4.5364 -0.1325 -0.6645  0.0774  0.1206]\n",
      "MSE loss: 109.1166\n",
      "Iteration: 12000\n",
      "Gradient: [ -5.3071   1.6408  13.9821  45.3326 229.914 ]\n",
      "Weights: [-4.5072 -0.1475 -0.6664  0.078   0.1209]\n",
      "MSE loss: 109.2926\n",
      "Iteration: 12100\n",
      "Gradient: [  2.6479  17.1323   8.35   137.4871  24.2315]\n",
      "Weights: [-4.5118 -0.1353 -0.6717  0.0771  0.1217]\n",
      "MSE loss: 108.9058\n",
      "Iteration: 12200\n",
      "Gradient: [  0.6021   2.9816   2.5808  84.0435 -47.7641]\n",
      "Weights: [-4.5307 -0.1285 -0.6733  0.0772  0.1219]\n",
      "MSE loss: 108.4904\n",
      "Iteration: 12300\n",
      "Gradient: [ 7.2515  0.8261 16.1818 14.1914 25.9921]\n",
      "Weights: [-4.5291 -0.1246 -0.6759  0.0762  0.1221]\n",
      "MSE loss: 108.138\n",
      "Iteration: 12400\n",
      "Gradient: [  -0.6116    7.3275   29.7837   32.3218 -340.452 ]\n",
      "Weights: [-4.5425 -0.1138 -0.6796  0.076   0.1226]\n",
      "MSE loss: 107.9024\n",
      "Iteration: 12500\n",
      "Gradient: [  1.2304  14.3424 -27.6394 -70.5973  10.2428]\n",
      "Weights: [-4.524  -0.1073 -0.6829  0.0757  0.1225]\n",
      "MSE loss: 107.9903\n",
      "Iteration: 12600\n",
      "Gradient: [ 0.4915 -8.8814 17.8642 22.3293 71.9445]\n",
      "Weights: [-4.5504 -0.1012 -0.6863  0.0754  0.123 ]\n",
      "MSE loss: 107.4561\n",
      "Iteration: 12700\n",
      "Gradient: [ 0.2026 -0.7355 16.4262 33.2426 26.2136]\n",
      "Weights: [-4.5369 -0.0911 -0.6897  0.0739  0.1233]\n",
      "MSE loss: 107.0097\n",
      "Iteration: 12800\n",
      "Gradient: [ 10.6872  11.0319 -25.7869 -49.4251 -86.0189]\n",
      "Weights: [-4.5275 -0.098  -0.6916  0.0736  0.1237]\n",
      "MSE loss: 106.9317\n",
      "Iteration: 12900\n",
      "Gradient: [-9.78050e+00 -5.97740e+00  3.90209e+01  2.26000e-02  6.45578e+01]\n",
      "Weights: [-4.5441 -0.0911 -0.6939  0.0735  0.124 ]\n",
      "MSE loss: 106.6874\n",
      "Iteration: 13000\n",
      "Gradient: [ -15.7202  -14.6347   -4.8446 -146.6617 -316.8925]\n",
      "Weights: [-4.5575 -0.085  -0.6962  0.0729  0.1244]\n",
      "MSE loss: 106.8492\n",
      "Iteration: 13100\n",
      "Gradient: [  -2.632    -2.3912    7.6664  -37.1772 -137.874 ]\n",
      "Weights: [-4.5464 -0.0822 -0.6991  0.0728  0.1244]\n",
      "MSE loss: 106.4902\n",
      "Iteration: 13200\n",
      "Gradient: [ 13.7873  -6.7399  22.4538 116.8042 161.8071]\n",
      "Weights: [-4.5431 -0.0747 -0.6997  0.0725  0.1246]\n",
      "MSE loss: 106.1822\n",
      "Iteration: 13300\n",
      "Gradient: [  0.2444  -0.2542  35.1694  36.6454 148.3008]\n",
      "Weights: [-4.5485 -0.0688 -0.7014  0.0715  0.1247]\n",
      "MSE loss: 105.9569\n",
      "Iteration: 13400\n",
      "Gradient: [  4.2177   2.4032   2.9812 -16.0791 134.3369]\n",
      "Weights: [-4.5545 -0.0565 -0.7063  0.0718  0.1249]\n",
      "MSE loss: 105.6289\n",
      "Iteration: 13500\n",
      "Gradient: [   1.3856  -14.118    33.1852 -126.6832   -8.7365]\n",
      "Weights: [-4.5551 -0.0483 -0.7092  0.0715  0.1251]\n",
      "MSE loss: 105.4707\n",
      "Iteration: 13600\n",
      "Gradient: [  4.1168  15.9225  31.7558   4.0422 -57.8252]\n",
      "Weights: [-4.5486 -0.0466 -0.7132  0.0718  0.1252]\n",
      "MSE loss: 105.3446\n",
      "Iteration: 13700\n",
      "Gradient: [   4.7175    9.1541   24.9313  -31.5481 -168.6482]\n",
      "Weights: [-4.5721 -0.0329 -0.7155  0.0709  0.1253]\n",
      "MSE loss: 105.1376\n",
      "Iteration: 13800\n",
      "Gradient: [  -2.5262   -3.2243    5.8227   36.4533 -155.1169]\n",
      "Weights: [-4.5646 -0.0347 -0.7188  0.071   0.1258]\n",
      "MSE loss: 104.8273\n",
      "Iteration: 13900\n",
      "Gradient: [ -13.3311  -17.0926   -6.3921   36.1603 -159.144 ]\n",
      "Weights: [-4.5638 -0.0369 -0.7222  0.0724  0.1258]\n",
      "MSE loss: 104.8253\n",
      "Iteration: 14000\n",
      "Gradient: [  -0.7755   -9.964   -21.0634  -55.9144 -130.5129]\n",
      "Weights: [-4.5646 -0.0387 -0.72    0.0716  0.1258]\n",
      "MSE loss: 104.9845\n",
      "Iteration: 14100\n",
      "Gradient: [ -3.7354   5.4061  43.3034  -0.6272 321.5675]\n",
      "Weights: [-4.5749 -0.0236 -0.7222  0.0709  0.126 ]\n",
      "MSE loss: 104.6081\n",
      "Iteration: 14200\n",
      "Gradient: [ -10.7501    5.9357  -24.3663  -67.6038 -123.7352]\n",
      "Weights: [-4.5577 -0.0274 -0.7249  0.07    0.1262]\n",
      "MSE loss: 104.758\n",
      "Iteration: 14300\n",
      "Gradient: [  -4.7003   15.2727   -3.4105  -65.2327 -221.136 ]\n",
      "Weights: [-4.5525 -0.0366 -0.7253  0.0706  0.1266]\n",
      "MSE loss: 104.6028\n",
      "Iteration: 14400\n",
      "Gradient: [   1.3513    1.2689   34.9141    3.7858 -262.4448]\n",
      "Weights: [-4.564  -0.02   -0.7284  0.07    0.1268]\n",
      "MSE loss: 104.1146\n",
      "Iteration: 14500\n",
      "Gradient: [-6.004000e+00  2.230000e-02  1.493590e+01 -4.243600e+00 -3.696991e+02]\n",
      "Weights: [-4.5727 -0.0075 -0.7325  0.0693  0.1268]\n",
      "MSE loss: 104.0784\n",
      "Iteration: 14600\n",
      "Gradient: [  2.8054  -1.8947 -42.0587 -54.3999 158.8043]\n",
      "Weights: [-4.5641 -0.0047 -0.7361  0.0693  0.1272]\n",
      "MSE loss: 103.7576\n",
      "Iteration: 14700\n",
      "Gradient: [-4.260000e-02 -5.295200e+00 -1.671060e+01 -7.469510e+01 -2.990835e+02]\n",
      "Weights: [-4.5936  0.0053 -0.7395  0.0693  0.1277]\n",
      "MSE loss: 103.8832\n",
      "Iteration: 14800\n",
      "Gradient: [  -5.6844    6.2163   -8.3648  -61.0953 -229.8063]\n",
      "Weights: [-4.5845  0.0123 -0.7385  0.0677  0.1278]\n",
      "MSE loss: 103.3005\n",
      "Iteration: 14900\n",
      "Gradient: [ -7.004   12.0762 -24.1201 -72.2757 429.1785]\n",
      "Weights: [-4.59    0.0177 -0.7428  0.0674  0.1282]\n",
      "MSE loss: 103.151\n",
      "Iteration: 15000\n",
      "Gradient: [   2.6243   -3.3853   22.4525 -105.2215  119.8772]\n",
      "Weights: [-4.5636  0.0141 -0.7449  0.0671  0.1285]\n",
      "MSE loss: 103.0949\n",
      "Iteration: 15100\n",
      "Gradient: [   5.7253  -13.5804    8.831   -16.0365 -140.6185]\n",
      "Weights: [-4.5742  0.0121 -0.746   0.0668  0.1291]\n",
      "MSE loss: 102.7845\n",
      "Iteration: 15200\n",
      "Gradient: [   1.76     -6.313   -19.1777   30.1406 -387.26  ]\n",
      "Weights: [-4.5732  0.0129 -0.7469  0.0668  0.129 ]\n",
      "MSE loss: 102.7942\n",
      "Iteration: 15300\n",
      "Gradient: [  -0.5016    6.2045   38.7356   -9.8127 -345.5733]\n",
      "Weights: [-4.5715  0.0145 -0.748   0.0671  0.129 ]\n",
      "MSE loss: 102.7209\n",
      "Iteration: 15400\n",
      "Gradient: [   2.3314   -6.5449   16.0558   19.0258 -340.4721]\n",
      "Weights: [-4.565   0.016  -0.7491  0.0669  0.1291]\n",
      "MSE loss: 102.7336\n",
      "Iteration: 15500\n",
      "Gradient: [  -2.6763   -0.8721   -6.3975  -42.4295 -463.1034]\n",
      "Weights: [-4.5802  0.031  -0.7515  0.0653  0.1295]\n",
      "MSE loss: 102.3442\n",
      "Iteration: 15600\n",
      "Gradient: [   3.3531    0.1482   31.7715   68.4122 -101.8259]\n",
      "Weights: [-4.578   0.03   -0.753   0.0658  0.1297]\n",
      "MSE loss: 102.3343\n",
      "Iteration: 15700\n",
      "Gradient: [  8.9567   0.8745  28.459   -6.7143 -40.1861]\n",
      "Weights: [-4.5717  0.0372 -0.7574  0.0651  0.1298]\n",
      "MSE loss: 102.1955\n",
      "Iteration: 15800\n",
      "Gradient: [ 13.3994  17.9089  71.4077  63.0631 114.1325]\n",
      "Weights: [-4.5687  0.0364 -0.7567  0.0651  0.13  ]\n",
      "MSE loss: 102.3857\n",
      "Iteration: 15900\n",
      "Gradient: [ 11.7424 -11.9552 -56.2856 102.9158 -94.6835]\n",
      "Weights: [-4.5811  0.0478 -0.7606  0.0653  0.13  ]\n",
      "MSE loss: 101.9621\n",
      "Iteration: 16000\n",
      "Gradient: [ -7.0571  -5.5822 -40.7551  11.7503  58.8523]\n",
      "Weights: [-4.6079  0.0607 -0.7637  0.0642  0.1303]\n",
      "MSE loss: 101.9198\n",
      "Iteration: 16100\n",
      "Gradient: [   5.5101    1.9875   51.345    49.8718 -303.4983]\n",
      "Weights: [-4.591   0.063  -0.7669  0.0639  0.1305]\n",
      "MSE loss: 101.5143\n",
      "Iteration: 16200\n",
      "Gradient: [  1.9444  -8.6637  15.4762 -98.1694 139.5564]\n",
      "Weights: [-4.574   0.0565 -0.7686  0.0643  0.1309]\n",
      "MSE loss: 101.6525\n",
      "Iteration: 16300\n",
      "Gradient: [ -8.1045 -25.911   14.3359 -75.3153 -43.5939]\n",
      "Weights: [-4.5921  0.0591 -0.7696  0.0643  0.1311]\n",
      "MSE loss: 101.306\n",
      "Iteration: 16400\n",
      "Gradient: [   5.3793   17.9636    3.1343  -89.9661 -160.1796]\n",
      "Weights: [-4.5704  0.0581 -0.7733  0.0649  0.1311]\n",
      "MSE loss: 101.486\n",
      "Iteration: 16500\n",
      "Gradient: [  5.7293  25.1028  80.3078  97.2218 -62.0299]\n",
      "Weights: [-4.5758  0.0586 -0.7728  0.0641  0.1313]\n",
      "MSE loss: 101.2827\n",
      "Iteration: 16600\n",
      "Gradient: [-18.8684   3.6708  10.7949  61.4697 212.1553]\n",
      "Weights: [-4.5932  0.0684 -0.7769  0.0644  0.1316]\n",
      "MSE loss: 100.9591\n",
      "Iteration: 16700\n",
      "Gradient: [-10.2     -1.1099  26.9304 -20.9028  22.3607]\n",
      "Weights: [-4.611   0.0783 -0.7788  0.0651  0.1314]\n",
      "MSE loss: 101.0712\n",
      "Iteration: 16800\n",
      "Gradient: [ 2.1083  0.0767  6.45   -0.5656 -2.0895]\n",
      "Weights: [-4.5962  0.0789 -0.7811  0.0651  0.1314]\n",
      "MSE loss: 100.7804\n",
      "Iteration: 16900\n",
      "Gradient: [  -1.4203  -19.6256  -31.5209   34.0312 -192.3096]\n",
      "Weights: [-4.604   0.0815 -0.7853  0.0656  0.1316]\n",
      "MSE loss: 100.8958\n",
      "Iteration: 17000\n",
      "Gradient: [ 2.7237 11.5198 14.2735 17.9032 92.5294]\n",
      "Weights: [-4.6046  0.0881 -0.7857  0.0655  0.1317]\n",
      "MSE loss: 100.5828\n",
      "Iteration: 17100\n",
      "Gradient: [ -7.7509 -11.5974   5.3236 -33.7008  85.1911]\n",
      "Weights: [-4.6015  0.0943 -0.7914  0.0657  0.1318]\n",
      "MSE loss: 100.3648\n",
      "Iteration: 17200\n",
      "Gradient: [ -4.4947  10.6685 -27.243    6.2425 191.4171]\n",
      "Weights: [-4.6116  0.1104 -0.7934  0.0654  0.132 ]\n",
      "MSE loss: 100.2593\n",
      "Iteration: 17300\n",
      "Gradient: [  -9.9687  -11.9602  -27.1634  -97.457  -258.5817]\n",
      "Weights: [-4.6195  0.1155 -0.7956  0.0643  0.1323]\n",
      "MSE loss: 100.0284\n",
      "Iteration: 17400\n",
      "Gradient: [  11.9474    5.3278   -4.1987 -142.6998 -291.463 ]\n",
      "Weights: [-4.634   0.1345 -0.7997  0.0631  0.1325]\n",
      "MSE loss: 99.8955\n",
      "Iteration: 17500\n",
      "Gradient: [  8.0313  -6.2085  -3.0258   2.7332 -20.7131]\n",
      "Weights: [-4.615   0.1359 -0.8067  0.0629  0.1328]\n",
      "MSE loss: 99.7431\n",
      "Iteration: 17600\n",
      "Gradient: [-5.710000e-02  1.655430e+01  9.439600e+00  1.583027e+02  1.484604e+02]\n",
      "Weights: [-4.6229  0.1441 -0.8067  0.0626  0.1329]\n",
      "MSE loss: 99.4727\n",
      "Iteration: 17700\n",
      "Gradient: [ -3.6288  15.7659   1.3637 -86.7075 321.1661]\n",
      "Weights: [-4.6249  0.1403 -0.8044  0.0616  0.1332]\n",
      "MSE loss: 99.4391\n",
      "Iteration: 17800\n",
      "Gradient: [   8.4073   -4.8667    5.9392  -19.4676 -243.3916]\n",
      "Weights: [-4.6198  0.136  -0.805   0.0622  0.1335]\n",
      "MSE loss: 99.4426\n",
      "Iteration: 17900\n",
      "Gradient: [   5.943   -16.9922   -0.5684 -109.7498 -236.6738]\n",
      "Weights: [-4.6175  0.1332 -0.8071  0.062   0.1338]\n",
      "MSE loss: 99.2909\n",
      "Iteration: 18000\n",
      "Gradient: [-11.6176 -16.258  -31.7128  25.6655 -52.5523]\n",
      "Weights: [-4.6315  0.1376 -0.8099  0.0626  0.1339]\n",
      "MSE loss: 99.4676\n",
      "Iteration: 18100\n",
      "Gradient: [  -5.4163   -2.9648   10.3053  -30.5894 -131.3845]\n",
      "Weights: [-4.6102  0.1391 -0.8108  0.0627  0.1338]\n",
      "MSE loss: 99.3118\n",
      "Iteration: 18200\n",
      "Gradient: [ -8.5062 -11.1236   0.8799  13.2791  63.7694]\n",
      "Weights: [-4.6182  0.1452 -0.8125  0.0617  0.1341]\n",
      "MSE loss: 99.0455\n",
      "Iteration: 18300\n",
      "Gradient: [  1.9227  -8.6588 -16.6787 -24.5945 135.3223]\n",
      "Weights: [-4.6234  0.1537 -0.8174  0.0619  0.1343]\n",
      "MSE loss: 98.8259\n",
      "Iteration: 18400\n",
      "Gradient: [  -3.1431  -24.5447    6.7065  -13.5053 -192.3132]\n",
      "Weights: [-4.6236  0.1562 -0.8226  0.0626  0.1344]\n",
      "MSE loss: 98.8193\n",
      "Iteration: 18500\n",
      "Gradient: [  8.8455 -16.2767  19.5358  74.8044  -1.1909]\n",
      "Weights: [-4.6356  0.1732 -0.8277  0.0632  0.1344]\n",
      "MSE loss: 98.4975\n",
      "Iteration: 18600\n",
      "Gradient: [  13.7849    3.61     30.8064   14.9792 -207.5381]\n",
      "Weights: [-4.6254  0.1684 -0.8297  0.0636  0.1346]\n",
      "MSE loss: 98.4322\n",
      "Iteration: 18700\n",
      "Gradient: [   4.3622   -5.4042  -41.556   -18.5707 -118.4019]\n",
      "Weights: [-4.6251  0.1799 -0.8345  0.0631  0.1349]\n",
      "MSE loss: 98.2359\n",
      "Iteration: 18800\n",
      "Gradient: [ -7.6926  -4.4085 -11.0428 -44.7034 -32.0615]\n",
      "Weights: [-4.6421  0.1806 -0.8335  0.0631  0.1349]\n",
      "MSE loss: 98.377\n",
      "Iteration: 18900\n",
      "Gradient: [ -3.213   -2.5216 -11.2099  26.4005  67.9961]\n",
      "Weights: [-4.6235  0.183  -0.8378  0.0632  0.1353]\n",
      "MSE loss: 98.1381\n",
      "Iteration: 19000\n",
      "Gradient: [ -7.5038  12.869    2.7859 -17.7286 -14.5422]\n",
      "Weights: [-4.6309  0.1831 -0.8373  0.0622  0.1354]\n",
      "MSE loss: 98.043\n",
      "Iteration: 19100\n",
      "Gradient: [  -4.7066  -14.1568  -27.959    73.6817 -179.6319]\n",
      "Weights: [-4.647   0.1911 -0.839   0.0624  0.1355]\n",
      "MSE loss: 98.0941\n",
      "Iteration: 19200\n",
      "Gradient: [   5.1329   -2.0571   33.2796    8.5828 -132.6045]\n",
      "Weights: [-4.64    0.1985 -0.8418  0.062   0.1358]\n",
      "MSE loss: 97.8356\n",
      "Iteration: 19300\n",
      "Gradient: [-0.2012 -1.1536 32.4718 91.094  16.7897]\n",
      "Weights: [-4.6376  0.2031 -0.8425  0.0613  0.1358]\n",
      "MSE loss: 97.8129\n",
      "Iteration: 19400\n",
      "Gradient: [ 1.91600e-01 -2.82230e+00 -1.09075e+01 -9.00528e+01 -2.70876e+02]\n",
      "Weights: [-4.639   0.2046 -0.8453  0.0609  0.1361]\n",
      "MSE loss: 97.6168\n",
      "Iteration: 19500\n",
      "Gradient: [   2.0713    3.5785   10.0483   82.6697 -158.3527]\n",
      "Weights: [-4.6307  0.1904 -0.8447  0.061   0.1363]\n",
      "MSE loss: 97.9533\n",
      "Iteration: 19600\n",
      "Gradient: [ -5.4881   1.7404 -13.2703  53.6211 -55.1717]\n",
      "Weights: [-4.622   0.192  -0.8423  0.0604  0.1364]\n",
      "MSE loss: 97.8255\n",
      "Iteration: 19700\n",
      "Gradient: [-10.7329   3.9273 -16.2959  61.1473 -42.5488]\n",
      "Weights: [-4.642   0.201  -0.8436  0.0608  0.1363]\n",
      "MSE loss: 97.6778\n",
      "Iteration: 19800\n",
      "Gradient: [ -3.7059  -2.3082 -15.4704 -75.1302 223.9938]\n",
      "Weights: [-4.6447  0.2016 -0.8464  0.0607  0.1366]\n",
      "MSE loss: 97.6327\n",
      "Iteration: 19900\n",
      "Gradient: [-13.2691 -13.4818   3.482  -33.0107 -24.5647]\n",
      "Weights: [-4.6401  0.1917 -0.8475  0.0615  0.1366]\n",
      "MSE loss: 98.1129\n",
      "Iteration: 20000\n",
      "Gradient: [  3.3157  27.1495 -26.8949  52.1563 -67.8186]\n",
      "Weights: [-4.628   0.1988 -0.8495  0.0619  0.1368]\n",
      "MSE loss: 97.6784\n",
      "Iteration: 20100\n",
      "Gradient: [  -1.878    -1.221   -27.278    46.9646 -327.1836]\n",
      "Weights: [-4.6535  0.2144 -0.8537  0.0616  0.1369]\n",
      "MSE loss: 97.4806\n",
      "Iteration: 20200\n",
      "Gradient: [ -2.5396   2.6266  30.8781 -14.3127  87.3022]\n",
      "Weights: [-4.6197  0.2057 -0.8558  0.0621  0.1372]\n",
      "MSE loss: 97.7739\n",
      "Iteration: 20300\n",
      "Gradient: [ -7.0853 -16.6379  26.56    -3.9014   4.4323]\n",
      "Weights: [-4.6215  0.1974 -0.858   0.0626  0.1373]\n",
      "MSE loss: 97.4275\n",
      "Iteration: 20400\n",
      "Gradient: [ -9.0743   1.8128   0.7865 148.7873 171.3669]\n",
      "Weights: [-4.651   0.2158 -0.8604  0.063   0.1373]\n",
      "MSE loss: 97.396\n",
      "Iteration: 20500\n",
      "Gradient: [-11.1302  -9.1935  29.7362  25.3938 211.0623]\n",
      "Weights: [-4.6445  0.2241 -0.8662  0.0626  0.1373]\n",
      "MSE loss: 97.1355\n",
      "Iteration: 20600\n",
      "Gradient: [   3.9938    8.6372   10.5273  -19.2264 -121.4422]\n",
      "Weights: [-4.6354  0.2311 -0.869   0.0625  0.1373]\n",
      "MSE loss: 96.9377\n",
      "Iteration: 20700\n",
      "Gradient: [-3.8341 -3.4769 18.5543 24.2189 41.2801]\n",
      "Weights: [-4.6682  0.2449 -0.8726  0.063   0.1374]\n",
      "MSE loss: 97.165\n",
      "Iteration: 20800\n",
      "Gradient: [ 0.2548 20.2609 21.7108 -1.6858 86.7859]\n",
      "Weights: [-4.642   0.2548 -0.8742  0.0622  0.1376]\n",
      "MSE loss: 97.2747\n",
      "Iteration: 20900\n",
      "Gradient: [ -8.233  -23.6279 -45.3288 -58.9546  31.734 ]\n",
      "Weights: [-4.6575  0.2523 -0.8744  0.0618  0.1377]\n",
      "MSE loss: 96.583\n",
      "Iteration: 21000\n",
      "Gradient: [  6.3664  -1.8436 -51.215   31.5433  46.4532]\n",
      "Weights: [-4.6549  0.2556 -0.8741  0.062   0.1376]\n",
      "MSE loss: 96.6714\n",
      "Iteration: 21100\n",
      "Gradient: [ -4.1175  24.4572 -18.0452 -42.5451 -34.056 ]\n",
      "Weights: [-4.6666  0.2589 -0.8744  0.0603  0.1378]\n",
      "MSE loss: 96.7461\n",
      "Iteration: 21200\n",
      "Gradient: [   1.2781   -8.9703   18.8117 -111.1165  183.1037]\n",
      "Weights: [-4.6477  0.2531 -0.8778  0.0615  0.1382]\n",
      "MSE loss: 96.4738\n",
      "Iteration: 21300\n",
      "Gradient: [  3.3273   7.1748  -5.2739  38.3782 -47.3848]\n",
      "Weights: [-4.6602  0.2599 -0.8802  0.0617  0.1382]\n",
      "MSE loss: 96.387\n",
      "Iteration: 21400\n",
      "Gradient: [-5.1855  6.068  13.5999  3.1721 87.738 ]\n",
      "Weights: [-4.6735  0.2783 -0.8825  0.0609  0.1382]\n",
      "MSE loss: 96.3059\n",
      "Iteration: 21500\n",
      "Gradient: [   3.9469  -11.2075  -35.2864  -48.0469 -157.9756]\n",
      "Weights: [-4.6713  0.2754 -0.885   0.0612  0.1383]\n",
      "MSE loss: 96.3263\n",
      "Iteration: 21600\n",
      "Gradient: [ 11.2431  19.2556  61.5149 223.8486 254.3869]\n",
      "Weights: [-4.641   0.2627 -0.888   0.0625  0.1388]\n",
      "MSE loss: 96.4073\n",
      "Iteration: 21700\n",
      "Gradient: [ 6.68000e-02 -1.27069e+01 -7.38580e+00  3.35205e+01 -9.18057e+01]\n",
      "Weights: [-4.6512  0.2655 -0.8914  0.0623  0.1389]\n",
      "MSE loss: 96.1808\n",
      "Iteration: 21800\n",
      "Gradient: [   4.513   -15.9669   -6.1652 -188.1551 -101.064 ]\n",
      "Weights: [-4.6704  0.2782 -0.89    0.0623  0.1388]\n",
      "MSE loss: 96.1342\n",
      "Iteration: 21900\n",
      "Gradient: [  -6.7956   -0.5015  -39.2001  -39.0267 -151.2557]\n",
      "Weights: [-4.6625  0.2834 -0.8945  0.0613  0.1391]\n",
      "MSE loss: 95.8936\n",
      "Iteration: 22000\n",
      "Gradient: [   6.8003    7.682    -8.7129  -73.3498 -318.3175]\n",
      "Weights: [-4.6525  0.2885 -0.8963  0.0609  0.1392]\n",
      "MSE loss: 95.9854\n",
      "Iteration: 22100\n",
      "Gradient: [  6.018    1.468    2.5533  -9.012  160.2623]\n",
      "Weights: [-4.6568  0.2905 -0.8965  0.0617  0.1393]\n",
      "MSE loss: 96.3213\n",
      "Iteration: 22200\n",
      "Gradient: [  8.0978  -1.2898  41.9149  63.4247 -48.5265]\n",
      "Weights: [-4.6596  0.2883 -0.8988  0.0612  0.1394]\n",
      "MSE loss: 95.7738\n",
      "Iteration: 22300\n",
      "Gradient: [  1.1911 -22.7352 -44.1874  -9.9433  24.1157]\n",
      "Weights: [-4.6793  0.3067 -0.9027  0.061   0.1393]\n",
      "MSE loss: 95.7613\n",
      "Iteration: 22400\n",
      "Gradient: [-8.359100e+00 -1.090150e+01  1.123000e-01 -4.654390e+01 -1.943944e+02]\n",
      "Weights: [-4.6816  0.306  -0.904   0.0607  0.1396]\n",
      "MSE loss: 95.9034\n",
      "Iteration: 22500\n",
      "Gradient: [  4.7001  -4.3646  15.0212 162.9687 302.4149]\n",
      "Weights: [-4.6709  0.3169 -0.9102  0.0612  0.1398]\n",
      "MSE loss: 95.4185\n",
      "Iteration: 22600\n",
      "Gradient: [   2.3614  -18.5697  -29.5745  -94.5453 -149.9151]\n",
      "Weights: [-4.6799  0.3234 -0.9136  0.062   0.1398]\n",
      "MSE loss: 95.335\n",
      "Iteration: 22700\n",
      "Gradient: [-2.753000e-01 -1.534000e+00 -3.560710e+01 -1.445500e+00  3.023054e+02]\n",
      "Weights: [-4.6699  0.3189 -0.9128  0.0622  0.1398]\n",
      "MSE loss: 95.4101\n",
      "Iteration: 22800\n",
      "Gradient: [ -0.2507  21.6283 -41.0612 104.4889  97.8847]\n",
      "Weights: [-4.6857  0.3244 -0.9128  0.0621  0.1397]\n",
      "MSE loss: 95.3823\n",
      "Iteration: 22900\n",
      "Gradient: [ -5.323   11.945  -20.8557 -32.1462  16.4184]\n",
      "Weights: [-4.6762  0.3184 -0.9119  0.0619  0.14  ]\n",
      "MSE loss: 95.3998\n",
      "Iteration: 23000\n",
      "Gradient: [   6.8623  -35.3005  -13.9164   38.6912 -212.8102]\n",
      "Weights: [-4.6611  0.322  -0.9152  0.0617  0.1401]\n",
      "MSE loss: 95.6215\n",
      "Iteration: 23100\n",
      "Gradient: [ -1.418    3.9606  27.3807 147.6052 372.8632]\n",
      "Weights: [-4.6995  0.3366 -0.9186  0.0612  0.1404]\n",
      "MSE loss: 95.4011\n",
      "Iteration: 23200\n",
      "Gradient: [ -2.5136  -3.0483  31.4039 -22.356  171.4383]\n",
      "Weights: [-4.6838  0.3381 -0.9206  0.0611  0.1406]\n",
      "MSE loss: 95.1253\n",
      "Iteration: 23300\n",
      "Gradient: [  14.6937    8.5934   18.0617   88.1416 -260.9214]\n",
      "Weights: [-4.6766  0.3378 -0.9227  0.0604  0.1409]\n",
      "MSE loss: 95.02\n",
      "Iteration: 23400\n",
      "Gradient: [ -2.1526  -3.2396  18.6511  91.2585 119.0813]\n",
      "Weights: [-4.6758  0.3383 -0.9227  0.0599  0.1412]\n",
      "MSE loss: 95.0645\n",
      "Iteration: 23500\n",
      "Gradient: [ 0.8123  2.3749  0.3088 38.6486 67.3929]\n",
      "Weights: [-4.682   0.3439 -0.928   0.06    0.1415]\n",
      "MSE loss: 94.8264\n",
      "Iteration: 23600\n",
      "Gradient: [  -8.8381   15.5233    1.8615   59.0632 -215.3282]\n",
      "Weights: [-4.6775  0.343  -0.9308  0.0603  0.1416]\n",
      "MSE loss: 94.8118\n",
      "Iteration: 23700\n",
      "Gradient: [  -9.4077    1.9472    3.2551   25.2231 -175.6858]\n",
      "Weights: [-4.6569  0.3336 -0.9283  0.0604  0.1416]\n",
      "MSE loss: 95.214\n",
      "Iteration: 23800\n",
      "Gradient: [  0.7529  20.696  -11.5274  30.5943 219.4266]\n",
      "Weights: [-4.6774  0.3393 -0.9276  0.061   0.1417]\n",
      "MSE loss: 95.1246\n",
      "Iteration: 23900\n",
      "Gradient: [   3.9005  -17.148   -42.4059   79.8623 -260.7433]\n",
      "Weights: [-4.6833  0.3464 -0.9299  0.0605  0.1414]\n",
      "MSE loss: 94.8162\n",
      "Iteration: 24000\n",
      "Gradient: [ 5.480000e-02  1.043040e+01 -1.997340e+01 -8.736570e+01 -2.434867e+02]\n",
      "Weights: [-4.6915  0.3486 -0.9324  0.061   0.1414]\n",
      "MSE loss: 95.0389\n",
      "Iteration: 24100\n",
      "Gradient: [ 6.712800e+00  1.065490e+01  1.740000e-01 -2.198000e+00 -3.461496e+02]\n",
      "Weights: [-4.6755  0.3573 -0.9393  0.0617  0.1414]\n",
      "MSE loss: 94.7851\n",
      "Iteration: 24200\n",
      "Gradient: [  3.8368  -7.4436  26.9928 -73.2566 189.328 ]\n",
      "Weights: [-4.674   0.3518 -0.9408  0.063   0.1417]\n",
      "MSE loss: 94.6574\n",
      "Iteration: 24300\n",
      "Gradient: [ -12.8243   -4.3224  -62.7618  -38.8858 -128.6564]\n",
      "Weights: [-4.6678  0.3476 -0.9406  0.0632  0.1416]\n",
      "MSE loss: 94.7385\n",
      "Iteration: 24400\n",
      "Gradient: [ -1.433   -7.744   36.1519 -54.5157 -30.9066]\n",
      "Weights: [-4.6791  0.3547 -0.941   0.0632  0.1417]\n",
      "MSE loss: 94.6504\n",
      "Iteration: 24500\n",
      "Gradient: [  0.6476  -4.1351 -32.2118 108.2962  63.6643]\n",
      "Weights: [-4.6856  0.3709 -0.9471  0.0634  0.1417]\n",
      "MSE loss: 94.4874\n",
      "Iteration: 24600\n",
      "Gradient: [  9.4311  -1.5356  17.2809 -86.8432 -97.2048]\n",
      "Weights: [-4.686   0.3754 -0.9478  0.0635  0.1415]\n",
      "MSE loss: 94.5075\n",
      "Iteration: 24700\n",
      "Gradient: [  0.2757  -1.8974  45.8723  73.7173 122.3627]\n",
      "Weights: [-4.6654  0.3702 -0.9487  0.0638  0.1415]\n",
      "MSE loss: 95.1346\n",
      "Iteration: 24800\n",
      "Gradient: [ 11.9429   1.7313 -27.4257 -70.6788 229.1069]\n",
      "Weights: [-4.6736  0.3705 -0.9491  0.064   0.1415]\n",
      "MSE loss: 94.6998\n",
      "Iteration: 24900\n",
      "Gradient: [ -9.1331  -0.9494  20.8552  28.9515 -69.2719]\n",
      "Weights: [-4.6956  0.3698 -0.9472  0.0643  0.1416]\n",
      "MSE loss: 94.518\n",
      "Iteration: 25000\n",
      "Gradient: [   9.2886  -12.5513    5.2125    8.6605 -379.4106]\n",
      "Weights: [-4.6955  0.3707 -0.949   0.064   0.1416]\n",
      "MSE loss: 94.6134\n",
      "Iteration: 25100\n",
      "Gradient: [  6.6714   9.1972 -45.3275 -76.3299 -55.4503]\n",
      "Weights: [-4.6771  0.3723 -0.9518  0.064   0.1418]\n",
      "MSE loss: 94.4851\n",
      "Iteration: 25200\n",
      "Gradient: [   7.1774   -2.981    30.1637  117.4799 -291.0381]\n",
      "Weights: [-4.6746  0.3773 -0.9508  0.0644  0.1417]\n",
      "MSE loss: 95.1942\n",
      "Iteration: 25300\n",
      "Gradient: [  2.3474   3.4183  28.2868 -23.4099  36.1889]\n",
      "Weights: [-4.6968  0.3875 -0.9542  0.0644  0.1417]\n",
      "MSE loss: 94.3586\n",
      "Iteration: 25400\n",
      "Gradient: [   1.6922  -30.1498  -11.4238 -100.5612  -70.7491]\n",
      "Weights: [-4.7106  0.3947 -0.9573  0.0643  0.1418]\n",
      "MSE loss: 94.378\n",
      "Iteration: 25500\n",
      "Gradient: [  3.7917  12.4045 -15.2673 -23.6973  69.4291]\n",
      "Weights: [-4.6941  0.3986 -0.9603  0.0639  0.1419]\n",
      "MSE loss: 94.249\n",
      "Iteration: 25600\n",
      "Gradient: [ 10.0415 -13.4794 -49.9528  61.5147  35.2258]\n",
      "Weights: [-4.7003  0.4041 -0.9625  0.0632  0.1423]\n",
      "MSE loss: 94.1228\n",
      "Iteration: 25700\n",
      "Gradient: [ -3.5221  14.2057 -22.6114 -53.8144  25.1339]\n",
      "Weights: [-4.7091  0.4072 -0.9625  0.0629  0.1423]\n",
      "MSE loss: 94.1849\n",
      "Iteration: 25800\n",
      "Gradient: [ 11.9347  -5.2783  19.7443  29.722  -78.9366]\n",
      "Weights: [-4.7116  0.4114 -0.9623  0.0625  0.1424]\n",
      "MSE loss: 94.1104\n",
      "Iteration: 25900\n",
      "Gradient: [  4.071    9.7831 -13.7439 -58.5751  39.7046]\n",
      "Weights: [-4.7002  0.4104 -0.9667  0.0629  0.1428]\n",
      "MSE loss: 94.0282\n",
      "Iteration: 26000\n",
      "Gradient: [-2.9888 -8.3151  4.1434  7.7099 25.5072]\n",
      "Weights: [-4.6977  0.4107 -0.9712  0.0633  0.143 ]\n",
      "MSE loss: 93.9264\n",
      "Iteration: 26100\n",
      "Gradient: [ -8.2661   0.5131  13.7974  53.2394 -74.9983]\n",
      "Weights: [-4.6892  0.4019 -0.9717  0.0637  0.1433]\n",
      "MSE loss: 93.9572\n",
      "Iteration: 26200\n",
      "Gradient: [-14.6917  -2.3907 -13.4109  29.0457 143.8966]\n",
      "Weights: [-4.7059  0.4072 -0.974   0.0647  0.1432]\n",
      "MSE loss: 94.0649\n",
      "Iteration: 26300\n",
      "Gradient: [  0.5222  23.1421 -18.2157  33.0867 -73.8804]\n",
      "Weights: [-4.6982  0.4106 -0.9701  0.0638  0.1431]\n",
      "MSE loss: 94.1339\n",
      "Iteration: 26400\n",
      "Gradient: [ -11.9498    6.4727    6.8031 -189.8003  -56.3552]\n",
      "Weights: [-4.7053  0.4095 -0.9736  0.0645  0.1431]\n",
      "MSE loss: 93.9621\n",
      "Iteration: 26500\n",
      "Gradient: [  -3.7886  -16.0417  -50.7704 -131.6217  -30.612 ]\n",
      "Weights: [-4.691   0.4005 -0.9737  0.0646  0.1431]\n",
      "MSE loss: 94.1098\n",
      "Iteration: 26600\n",
      "Gradient: [   1.939    -1.8366    3.8346   37.158  -279.7933]\n",
      "Weights: [-4.7102  0.4223 -0.974   0.0638  0.1431]\n",
      "MSE loss: 93.8941\n",
      "Iteration: 26700\n",
      "Gradient: [   1.7474    4.4526   12.0986   48.3685 -300.2188]\n",
      "Weights: [-4.7041  0.4234 -0.9773  0.0643  0.1431]\n",
      "MSE loss: 93.8095\n",
      "Iteration: 26800\n",
      "Gradient: [  2.8751  -6.5498  30.4248  82.8901 277.4057]\n",
      "Weights: [-4.7101  0.4278 -0.977   0.0642  0.1432]\n",
      "MSE loss: 93.9058\n",
      "Iteration: 26900\n",
      "Gradient: [-3.35000e-02  1.63452e+01 -1.14846e+01  7.77301e+01  3.83872e+01]\n",
      "Weights: [-4.7112  0.4319 -0.9791  0.0631  0.1434]\n",
      "MSE loss: 93.7432\n",
      "Iteration: 27000\n",
      "Gradient: [ -1.8279 -24.563   -7.4373  11.0278 237.5435]\n",
      "Weights: [-4.721   0.4313 -0.9806  0.0637  0.1434]\n",
      "MSE loss: 93.9509\n",
      "Iteration: 27100\n",
      "Gradient: [-16.5934  -1.7511 -12.9003  43.0044 258.347 ]\n",
      "Weights: [-4.7023  0.4257 -0.9864  0.0651  0.1438]\n",
      "MSE loss: 93.6823\n",
      "Iteration: 27200\n",
      "Gradient: [ -8.4931  11.4549 -30.4086 -38.6195 354.276 ]\n",
      "Weights: [-4.7128  0.4389 -0.9848  0.064   0.1438]\n",
      "MSE loss: 93.7612\n",
      "Iteration: 27300\n",
      "Gradient: [  4.6029  -1.8023  22.4801 178.7377 205.949 ]\n",
      "Weights: [-4.7287  0.4483 -0.9868  0.064   0.1439]\n",
      "MSE loss: 93.8031\n",
      "Iteration: 27400\n",
      "Gradient: [  2.7596 -12.6073  -5.9247  79.7002 -87.7133]\n",
      "Weights: [-4.7154  0.4513 -0.9893  0.0637  0.1439]\n",
      "MSE loss: 93.6225\n",
      "Iteration: 27500\n",
      "Gradient: [ -9.5891   5.9255  50.7199 -26.3341 246.3   ]\n",
      "Weights: [-4.7271  0.4528 -0.9891  0.0644  0.1437]\n",
      "MSE loss: 93.6407\n",
      "Iteration: 27600\n",
      "Gradient: [   8.2968   -0.7735  -36.85    -72.3597 -182.1052]\n",
      "Weights: [-4.7127  0.452  -0.9934  0.0649  0.1437]\n",
      "MSE loss: 93.5224\n",
      "Iteration: 27700\n",
      "Gradient: [ -15.2407  -10.6165   -9.2034 -163.1606   60.8437]\n",
      "Weights: [-4.7285  0.4526 -0.9953  0.0653  0.1438]\n",
      "MSE loss: 93.7864\n",
      "Iteration: 27800\n",
      "Gradient: [ 6.3391 11.9689 40.8917 84.29   58.0623]\n",
      "Weights: [-4.7044  0.4482 -0.9968  0.0665  0.1441]\n",
      "MSE loss: 93.9147\n",
      "Iteration: 27900\n",
      "Gradient: [ -1.3741   0.1902 -34.1623 -77.7428 -44.4597]\n",
      "Weights: [-4.7154  0.4563 -1.0006  0.0664  0.1441]\n",
      "MSE loss: 93.3908\n",
      "Iteration: 28000\n",
      "Gradient: [ -9.9248   1.8208 -17.7425 -60.6066 -56.2281]\n",
      "Weights: [-4.7347  0.4656 -1.0012  0.0665  0.1439]\n",
      "MSE loss: 93.5408\n",
      "Iteration: 28100\n",
      "Gradient: [-6.1385  9.2918  1.4978  9.5201 50.1417]\n",
      "Weights: [-4.7287  0.4652 -1.0011  0.0656  0.144 ]\n",
      "MSE loss: 93.5058\n",
      "Iteration: 28200\n",
      "Gradient: [  9.2034 -11.3925 -22.2654  21.3638 475.4231]\n",
      "Weights: [-4.7251  0.4676 -1.0003  0.0661  0.1441]\n",
      "MSE loss: 93.7112\n",
      "Iteration: 28300\n",
      "Gradient: [  -3.5567  -11.8571  -28.9013 -202.2538   63.3949]\n",
      "Weights: [-4.7324  0.4709 -1.0024  0.0648  0.144 ]\n",
      "MSE loss: 93.737\n",
      "Iteration: 28400\n",
      "Gradient: [  -3.4259   -4.6023   11.7843   76.7482 -218.8558]\n",
      "Weights: [-4.7402  0.4687 -0.9997  0.0649  0.1442]\n",
      "MSE loss: 93.6847\n",
      "Iteration: 28500\n",
      "Gradient: [ -9.5632  -4.8407 -34.3364  35.708    9.9875]\n",
      "Weights: [-4.7216  0.4631 -1.0007  0.0646  0.1444]\n",
      "MSE loss: 93.3696\n",
      "Iteration: 28600\n",
      "Gradient: [ 7.800000e-03  6.183800e+00  3.382720e+01 -5.058870e+01  2.899281e+02]\n",
      "Weights: [-4.7208  0.4693 -1.003   0.0644  0.1444]\n",
      "MSE loss: 93.3405\n",
      "Iteration: 28700\n",
      "Gradient: [  4.6395   5.9582  81.6804  33.2392 183.1672]\n",
      "Weights: [-4.7292  0.4823 -1.0058  0.065   0.1444]\n",
      "MSE loss: 93.4171\n",
      "Iteration: 28800\n",
      "Gradient: [  -5.0072    3.3177    4.7137  -91.848  -290.5229]\n",
      "Weights: [-4.7148  0.4706 -1.006   0.0648  0.1443]\n",
      "MSE loss: 93.4771\n",
      "Iteration: 28900\n",
      "Gradient: [-7.1577 -8.8348 -0.6218 28.8094 44.3437]\n",
      "Weights: [-4.7411  0.4723 -1.0039  0.0649  0.1444]\n",
      "MSE loss: 93.7928\n",
      "Iteration: 29000\n",
      "Gradient: [   3.137     1.9952  -16.2084 -128.3852  -43.5251]\n",
      "Weights: [-4.7253  0.476  -1.0039  0.0648  0.1444]\n",
      "MSE loss: 93.3687\n",
      "Iteration: 29100\n",
      "Gradient: [ -4.0307  15.0153  -4.9288  64.19   162.0603]\n",
      "Weights: [-4.725   0.4767 -1.0084  0.0648  0.1447]\n",
      "MSE loss: 93.2324\n",
      "Iteration: 29200\n",
      "Gradient: [   0.5908   -9.7249   -5.5136   11.5275 -181.7911]\n",
      "Weights: [-4.7151  0.4735 -1.0107  0.0643  0.1451]\n",
      "MSE loss: 93.2608\n",
      "Iteration: 29300\n",
      "Gradient: [  -8.8189   -6.7325    4.5643 -123.1386   27.5023]\n",
      "Weights: [-4.7324  0.4784 -1.0101  0.064   0.1453]\n",
      "MSE loss: 93.2831\n",
      "Iteration: 29400\n",
      "Gradient: [ -2.9405  -5.3916 -13.7132  45.7027 -95.1622]\n",
      "Weights: [-4.712   0.4848 -1.0152  0.064   0.1453]\n",
      "MSE loss: 93.2942\n",
      "Iteration: 29500\n",
      "Gradient: [  -0.7935    6.2628  -13.2913   37.4249 -192.4076]\n",
      "Weights: [-4.707   0.4745 -1.0111  0.0639  0.1455]\n",
      "MSE loss: 93.4158\n",
      "Iteration: 29600\n",
      "Gradient: [  3.8636   2.2965  13.7286 -20.1195 -28.6166]\n",
      "Weights: [-4.7185  0.4726 -1.0082  0.0645  0.1453]\n",
      "MSE loss: 93.3591\n",
      "Iteration: 29700\n",
      "Gradient: [  7.4178 -19.889  -25.5856 -69.2086 -67.2289]\n",
      "Weights: [-4.7253  0.4681 -1.0107  0.0643  0.1455]\n",
      "MSE loss: 93.5655\n",
      "Iteration: 29800\n",
      "Gradient: [  8.3427  -2.3046 -15.3495 -54.9474 401.1159]\n",
      "Weights: [-4.6932  0.4742 -1.0137  0.0644  0.1456]\n",
      "MSE loss: 94.1309\n",
      "Iteration: 29900\n",
      "Gradient: [ -5.9853   4.412  -13.9054   1.3787  56.1777]\n",
      "Weights: [-4.7068  0.4746 -1.0154  0.0643  0.1456]\n",
      "MSE loss: 93.246\n",
      "Iteration: 30000\n",
      "Gradient: [  1.1683  -8.0456   7.0361 -45.3723 141.1123]\n",
      "Weights: [-4.7155  0.4837 -1.0176  0.0646  0.1459]\n",
      "MSE loss: 93.3071\n",
      "Iteration: 30100\n",
      "Gradient: [ 13.6012   6.6322  43.6913  34.4211 231.1246]\n",
      "Weights: [-4.7219  0.4887 -1.0186  0.0646  0.1461]\n",
      "MSE loss: 93.4597\n",
      "Iteration: 30200\n",
      "Gradient: [   6.2583  -20.5488  -32.1894  -85.6429 -180.3026]\n",
      "Weights: [-4.7159  0.4889 -1.0209  0.0644  0.146 ]\n",
      "MSE loss: 93.1208\n",
      "Iteration: 30300\n",
      "Gradient: [  9.5729   6.4537 -46.0561  30.5791 225.3907]\n",
      "Weights: [-4.7283  0.49   -1.0193  0.0644  0.146 ]\n",
      "MSE loss: 93.1145\n",
      "Iteration: 30400\n",
      "Gradient: [ -5.3144  -1.8097 -37.9727 -45.2789 211.0088]\n",
      "Weights: [-4.7071  0.4859 -1.0196  0.0638  0.1458]\n",
      "MSE loss: 93.2773\n",
      "Iteration: 30500\n",
      "Gradient: [  -1.1102   -5.785    -7.2764  -59.3876 -294.2619]\n",
      "Weights: [-4.7135  0.4736 -1.0153  0.0641  0.1459]\n",
      "MSE loss: 93.1943\n",
      "Iteration: 30600\n",
      "Gradient: [   8.839   -10.5156  -65.6625   16.367  -197.6988]\n",
      "Weights: [-4.7202  0.4895 -1.0191  0.0643  0.1458]\n",
      "MSE loss: 93.0883\n",
      "Iteration: 30700\n",
      "Gradient: [ -2.2019   9.8719 -18.5392  37.7548 206.0177]\n",
      "Weights: [-4.7233  0.4934 -1.0177  0.0639  0.1455]\n",
      "MSE loss: 93.1081\n",
      "Iteration: 30800\n",
      "Gradient: [ 11.165   15.2401 -16.3958 -38.9846 -71.9081]\n",
      "Weights: [-4.7023  0.491  -1.0199  0.0639  0.1456]\n",
      "MSE loss: 93.6649\n",
      "Iteration: 30900\n",
      "Gradient: [   5.494     5.7151    9.8642  -39.9681 -229.9863]\n",
      "Weights: [-4.7121  0.4897 -1.0213  0.0651  0.1456]\n",
      "MSE loss: 93.1962\n",
      "Iteration: 31000\n",
      "Gradient: [  -4.5955    0.9184  -13.5208   59.2155 -119.6721]\n",
      "Weights: [-4.7247  0.49   -1.0264  0.0664  0.1458]\n",
      "MSE loss: 93.1373\n",
      "Iteration: 31100\n",
      "Gradient: [ -4.7106   3.5077 -18.8594  47.4387 -57.3258]\n",
      "Weights: [-4.7241  0.4896 -1.027   0.0666  0.1459]\n",
      "MSE loss: 93.1199\n",
      "Iteration: 31200\n",
      "Gradient: [  0.428   16.7427 -25.0005 -29.4689  99.6013]\n",
      "Weights: [-4.7068  0.4874 -1.0264  0.0665  0.1461]\n",
      "MSE loss: 93.3359\n",
      "Iteration: 31300\n",
      "Gradient: [  -6.1534   19.4656  -17.2646   56.5826 -187.2424]\n",
      "Weights: [-4.7162  0.4853 -1.0272  0.0664  0.1461]\n",
      "MSE loss: 93.1104\n",
      "Iteration: 31400\n",
      "Gradient: [  0.7341  -2.2951   1.0574  55.3022 155.5947]\n",
      "Weights: [-4.6969  0.4856 -1.0276  0.0659  0.1463]\n",
      "MSE loss: 93.5733\n",
      "Iteration: 31500\n",
      "Gradient: [-1.8896  3.7022 17.8752 55.7987  2.9826]\n",
      "Weights: [-4.7037  0.4836 -1.0279  0.0664  0.1462]\n",
      "MSE loss: 93.1887\n",
      "Iteration: 31600\n",
      "Gradient: [  5.392   -5.2189  34.6786 -34.1376 109.3915]\n",
      "Weights: [-4.712   0.5011 -1.0303  0.0666  0.146 ]\n",
      "MSE loss: 93.4574\n",
      "Iteration: 31700\n",
      "Gradient: [  -9.3124   -2.7662   42.6269   16.4553 -104.7871]\n",
      "Weights: [-4.7302  0.5079 -1.0309  0.0665  0.1459]\n",
      "MSE loss: 92.9214\n",
      "Iteration: 31800\n",
      "Gradient: [-11.9654   6.5782  12.9437 -87.0284 -66.6894]\n",
      "Weights: [-4.7319  0.5179 -1.0354  0.0664  0.146 ]\n",
      "MSE loss: 92.8749\n",
      "Iteration: 31900\n",
      "Gradient: [  -4.3059  -27.0466    1.3737 -115.7872 -183.2603]\n",
      "Weights: [-4.7242  0.5049 -1.0336  0.0661  0.146 ]\n",
      "MSE loss: 93.1159\n",
      "Iteration: 32000\n",
      "Gradient: [  3.3363 -10.7176 -39.1166  22.0297 -27.9387]\n",
      "Weights: [-4.7176  0.506  -1.0335  0.0667  0.146 ]\n",
      "MSE loss: 93.0092\n",
      "Iteration: 32100\n",
      "Gradient: [ 5.420000e-02  1.348000e+00 -2.753270e+01 -1.547310e+01 -1.539123e+02]\n",
      "Weights: [-4.7319  0.5067 -1.0335  0.0668  0.1461]\n",
      "MSE loss: 92.9153\n",
      "Iteration: 32200\n",
      "Gradient: [ -1.3423 -11.8541  21.3532  41.6141 296.9978]\n",
      "Weights: [-4.7366  0.5167 -1.0341  0.0665  0.146 ]\n",
      "MSE loss: 92.8746\n",
      "Iteration: 32300\n",
      "Gradient: [  -1.1118    4.0735    1.9069   -5.1973 -276.3695]\n",
      "Weights: [-4.7428  0.5178 -1.0352  0.0661  0.1462]\n",
      "MSE loss: 92.9222\n",
      "Iteration: 32400\n",
      "Gradient: [ 5.5743  7.5346 -7.2953 64.1419 45.0706]\n",
      "Weights: [-4.7283  0.5172 -1.0349  0.0662  0.1461]\n",
      "MSE loss: 92.9746\n",
      "Iteration: 32500\n",
      "Gradient: [  7.7562  11.2794  -0.3485  39.2569 213.4933]\n",
      "Weights: [-4.7263  0.5101 -1.0337  0.0662  0.1462]\n",
      "MSE loss: 92.9236\n",
      "Iteration: 32600\n",
      "Gradient: [  -7.2103   -9.6331  -11.0374   22.8643 -347.338 ]\n",
      "Weights: [-4.7292  0.5105 -1.0346  0.066   0.1464]\n",
      "MSE loss: 92.8801\n",
      "Iteration: 32700\n",
      "Gradient: [  -9.757   -19.1022    2.501  -109.1039 -320.8116]\n",
      "Weights: [-4.7305  0.5162 -1.0389  0.0656  0.1464]\n",
      "MSE loss: 93.0293\n",
      "Iteration: 32800\n",
      "Gradient: [  2.2685  17.8308 -24.0677  55.3759 295.7689]\n",
      "Weights: [-4.7226  0.5192 -1.0351  0.0653  0.1463]\n",
      "MSE loss: 93.1746\n",
      "Iteration: 32900\n",
      "Gradient: [  -2.4159    8.5825   -0.4808  -28.0901 -426.3063]\n",
      "Weights: [-4.7412  0.5248 -1.0376  0.0654  0.1462]\n",
      "MSE loss: 92.9181\n",
      "Iteration: 33000\n",
      "Gradient: [  -9.9145   -3.1566  -39.5518   -9.6215 -317.879 ]\n",
      "Weights: [-4.739   0.5174 -1.0358  0.0656  0.1464]\n",
      "MSE loss: 92.8632\n",
      "Iteration: 33100\n",
      "Gradient: [ -4.1782  25.8596  24.6428 102.1138 128.9291]\n",
      "Weights: [-4.7375  0.5231 -1.0371  0.0656  0.1465]\n",
      "MSE loss: 92.9136\n",
      "Iteration: 33200\n",
      "Gradient: [  1.9974  -2.0804 -13.9318 -61.3565 -40.6163]\n",
      "Weights: [-4.7428  0.522  -1.0381  0.0661  0.1465]\n",
      "MSE loss: 92.8563\n",
      "Iteration: 33300\n",
      "Gradient: [ -2.6521 -15.6863   5.7556  -4.9803  36.5972]\n",
      "Weights: [-4.7389  0.5214 -1.0386  0.0654  0.1465]\n",
      "MSE loss: 92.88\n",
      "Iteration: 33400\n",
      "Gradient: [   4.8861   10.7737   32.2089   -1.276  -138.6961]\n",
      "Weights: [-4.7452  0.5338 -1.0384  0.0654  0.1465]\n",
      "MSE loss: 93.0621\n",
      "Iteration: 33500\n",
      "Gradient: [ -4.4086   5.3463   1.1789 -66.5281 137.0322]\n",
      "Weights: [-4.7408  0.531  -1.0374  0.0651  0.1464]\n",
      "MSE loss: 92.9181\n",
      "Iteration: 33600\n",
      "Gradient: [ 11.8901  27.3861  47.7456  46.3818 129.6818]\n",
      "Weights: [-4.7322  0.5292 -1.0395  0.0653  0.1467]\n",
      "MSE loss: 93.1224\n",
      "Iteration: 33700\n",
      "Gradient: [  5.8042   5.8831  29.678  -25.1973  28.5604]\n",
      "Weights: [-4.7563  0.5334 -1.0394  0.0651  0.1469]\n",
      "MSE loss: 93.0651\n",
      "Iteration: 33800\n",
      "Gradient: [  -2.2775  -14.3812   12.1296   37.2878 -373.1782]\n",
      "Weights: [-4.7317  0.5328 -1.0434  0.0651  0.1468]\n",
      "MSE loss: 92.8376\n",
      "Iteration: 33900\n",
      "Gradient: [ 11.3426   0.9481  -2.9852  -7.1027 127.5469]\n",
      "Weights: [-4.7433  0.5407 -1.0445  0.0646  0.1471]\n",
      "MSE loss: 92.8118\n",
      "Iteration: 34000\n",
      "Gradient: [ -2.574   -1.0957  18.3373 -24.7562  57.4531]\n",
      "Weights: [-4.7252  0.5358 -1.0445  0.0639  0.1472]\n",
      "MSE loss: 93.0211\n",
      "Iteration: 34100\n",
      "Gradient: [ 14.3884  -4.3267  -2.5545   1.7442 -19.2299]\n",
      "Weights: [-4.7346  0.5336 -1.0421  0.0635  0.1473]\n",
      "MSE loss: 92.8288\n",
      "Iteration: 34200\n",
      "Gradient: [ -0.7769   0.8107  11.0237   3.3249 151.8058]\n",
      "Weights: [-4.7433  0.5461 -1.0456  0.0643  0.1471]\n",
      "MSE loss: 92.8468\n",
      "Iteration: 34300\n",
      "Gradient: [-4.717500e+00 -7.360000e-02  1.233420e+01 -2.748810e+01  2.551329e+02]\n",
      "Weights: [-4.7588  0.5537 -1.0457  0.0644  0.1468]\n",
      "MSE loss: 92.805\n",
      "Iteration: 34400\n",
      "Gradient: [   5.7214  -15.4461   17.0848    2.3341 -135.6653]\n",
      "Weights: [-4.7407  0.546  -1.0484  0.0643  0.1471]\n",
      "MSE loss: 92.7331\n",
      "Iteration: 34500\n",
      "Gradient: [  3.478   14.3261  38.3924  46.8163 147.1039]\n",
      "Weights: [-4.7366  0.5397 -1.0458  0.0643  0.1472]\n",
      "MSE loss: 92.7748\n",
      "Iteration: 34600\n",
      "Gradient: [ -2.8106   2.9153   3.5943  34.1139 235.7993]\n",
      "Weights: [-4.7473  0.5452 -1.0476  0.0644  0.1475]\n",
      "MSE loss: 92.8026\n",
      "Iteration: 34700\n",
      "Gradient: [  -8.7636   18.2824  -24.3491  -10.861  -121.2171]\n",
      "Weights: [-4.7774  0.566  -1.0503  0.0633  0.1475]\n",
      "MSE loss: 93.07\n",
      "Iteration: 34800\n",
      "Gradient: [   2.5538    4.6318   -8.7645  -28.9658 -202.2496]\n",
      "Weights: [-4.755   0.5592 -1.0519  0.0638  0.1474]\n",
      "MSE loss: 92.6792\n",
      "Iteration: 34900\n",
      "Gradient: [-10.316   16.9899 -31.4328  -6.495  212.832 ]\n",
      "Weights: [-4.7691  0.5728 -1.0545  0.0639  0.1476]\n",
      "MSE loss: 92.8292\n",
      "Iteration: 35000\n",
      "Gradient: [  4.0439 -11.9904 -28.4083  48.7986 -87.565 ]\n",
      "Weights: [-4.7553  0.5649 -1.0541  0.0635  0.1476]\n",
      "MSE loss: 92.6643\n",
      "Iteration: 35100\n",
      "Gradient: [ -6.1641  10.6402   2.1913 -10.4746  74.9788]\n",
      "Weights: [-4.7718  0.5701 -1.055   0.064   0.1476]\n",
      "MSE loss: 92.8356\n",
      "Iteration: 35200\n",
      "Gradient: [ -10.9218   -6.6488  -15.1976  -34.7524 -162.0317]\n",
      "Weights: [-4.7723  0.5844 -1.0608  0.0638  0.1473]\n",
      "MSE loss: 92.9986\n",
      "Iteration: 35300\n",
      "Gradient: [  9.681    0.7929  26.1512  71.4014 170.9535]\n",
      "Weights: [-4.7607  0.5906 -1.0618  0.0638  0.1476]\n",
      "MSE loss: 93.1146\n",
      "Iteration: 35400\n",
      "Gradient: [   3.8792  -10.3656   13.9722   25.6863 -302.3748]\n",
      "Weights: [-4.7628  0.5841 -1.0633  0.0645  0.1477]\n",
      "MSE loss: 92.6393\n",
      "Iteration: 35500\n",
      "Gradient: [  -0.8102   -6.3822  -47.6941   35.5203 -381.3415]\n",
      "Weights: [-4.7644  0.5829 -1.0635  0.0653  0.1474]\n",
      "MSE loss: 92.6312\n",
      "Iteration: 35600\n",
      "Gradient: [  -4.2052  -10.3768  -11.2306  -61.527  -171.8666]\n",
      "Weights: [-4.7632  0.5738 -1.0642  0.0655  0.1477]\n",
      "MSE loss: 92.735\n",
      "Iteration: 35700\n",
      "Gradient: [  3.485   -0.8385   3.8409 -92.9751  86.0372]\n",
      "Weights: [-4.7517  0.5651 -1.065   0.0662  0.1479]\n",
      "MSE loss: 92.6035\n",
      "Iteration: 35800\n",
      "Gradient: [   3.7615   11.83    -61.707   -27.0155 -235.1047]\n",
      "Weights: [-4.7442  0.5675 -1.0696  0.067   0.1481]\n",
      "MSE loss: 92.5162\n",
      "Iteration: 35900\n",
      "Gradient: [-13.82     7.4142 -19.0583 -16.537  -88.8685]\n",
      "Weights: [-4.7531  0.573  -1.0724  0.0671  0.1484]\n",
      "MSE loss: 92.5099\n",
      "Iteration: 36000\n",
      "Gradient: [  5.6214 -12.7127  -6.7512  10.6507 159.4539]\n",
      "Weights: [-4.7631  0.5775 -1.0742  0.0673  0.1484]\n",
      "MSE loss: 92.645\n",
      "Iteration: 36100\n",
      "Gradient: [  1.8344 -18.3809  17.3079   1.9774 111.9815]\n",
      "Weights: [-4.7431  0.5774 -1.073   0.0669  0.1483]\n",
      "MSE loss: 92.642\n",
      "Iteration: 36200\n",
      "Gradient: [ -10.6304  -14.8396  -40.4253   -5.5099 -180.0638]\n",
      "Weights: [-4.7876  0.5997 -1.0763  0.0661  0.1482]\n",
      "MSE loss: 93.2139\n",
      "Iteration: 36300\n",
      "Gradient: [ -5.2158 -14.3942  17.3231 -19.22    -3.9832]\n",
      "Weights: [-4.7831  0.6019 -1.0778  0.0664  0.1483]\n",
      "MSE loss: 92.745\n",
      "Iteration: 36400\n",
      "Gradient: [-1.4807  7.442  53.8259 -3.7553 75.7361]\n",
      "Weights: [-4.7546  0.5896 -1.0755  0.066   0.1484]\n",
      "MSE loss: 92.4888\n",
      "Iteration: 36500\n",
      "Gradient: [ -1.0505  -6.7121   4.4182 -18.0935  85.4479]\n",
      "Weights: [-4.7489  0.5761 -1.074   0.0663  0.1487]\n",
      "MSE loss: 92.4816\n",
      "Iteration: 36600\n",
      "Gradient: [ -7.0799  14.6571  -2.7907  28.2195 -85.2764]\n",
      "Weights: [-4.7579  0.5794 -1.0756  0.0666  0.1485]\n",
      "MSE loss: 92.6066\n",
      "Iteration: 36700\n",
      "Gradient: [ 12.5876  21.1704  18.2858   5.2091 -55.7593]\n",
      "Weights: [-4.7421  0.5822 -1.0765  0.0671  0.1486]\n",
      "MSE loss: 92.8042\n",
      "Iteration: 36800\n",
      "Gradient: [  0.4704   3.2676  -7.8821 -22.0493 110.7748]\n",
      "Weights: [-4.7417  0.5738 -1.0769  0.0673  0.1487]\n",
      "MSE loss: 92.5123\n",
      "Iteration: 36900\n",
      "Gradient: [  10.6669   -4.9783   29.3369   78.3698 -188.9917]\n",
      "Weights: [-4.739   0.582  -1.0771  0.0669  0.1488]\n",
      "MSE loss: 93.0459\n",
      "Iteration: 37000\n",
      "Gradient: [-12.2991 -27.3666 -49.8656 -69.251   95.2886]\n",
      "Weights: [-4.7724  0.5958 -1.0826  0.0672  0.1486]\n",
      "MSE loss: 92.7674\n",
      "Iteration: 37100\n",
      "Gradient: [   5.8807  -18.0465  -36.0339   20.5867 -107.5259]\n",
      "Weights: [-4.7681  0.5989 -1.0812  0.0664  0.1488]\n",
      "MSE loss: 92.4559\n",
      "Iteration: 37200\n",
      "Gradient: [  6.0259   1.6111  -8.5783 -32.8483 160.9276]\n",
      "Weights: [-4.7604  0.602  -1.081   0.0664  0.1484]\n",
      "MSE loss: 92.4624\n",
      "Iteration: 37300\n",
      "Gradient: [   3.101    -2.4229  -36.6042   17.4686 -339.8461]\n",
      "Weights: [-4.7462  0.5909 -1.0799  0.0679  0.1481]\n",
      "MSE loss: 92.594\n",
      "Iteration: 37400\n",
      "Gradient: [ -8.546    1.6297   9.4362 -98.1711 150.3687]\n",
      "Weights: [-4.7703  0.5973 -1.0821  0.0683  0.1481]\n",
      "MSE loss: 92.5528\n",
      "Iteration: 37500\n",
      "Gradient: [ -5.833    8.9988  33.1986 -60.8542 -33.4081]\n",
      "Weights: [-4.7794  0.6029 -1.081   0.0681  0.1479]\n",
      "MSE loss: 92.669\n",
      "Iteration: 37600\n",
      "Gradient: [  -4.7441   10.0344  -16.33     43.8242 -192.3059]\n",
      "Weights: [-4.7684  0.5922 -1.0758  0.0672  0.1479]\n",
      "MSE loss: 92.5516\n",
      "Iteration: 37700\n",
      "Gradient: [ -1.4954  24.9203   9.3303  -8.6167 155.3031]\n",
      "Weights: [-4.7485  0.586  -1.079   0.0688  0.1479]\n",
      "MSE loss: 92.4947\n",
      "Iteration: 37800\n",
      "Gradient: [  3.3055   4.0488  18.7543  90.2186 114.3498]\n",
      "Weights: [-4.7688  0.5977 -1.0798  0.0694  0.1476]\n",
      "MSE loss: 92.4658\n",
      "Iteration: 37900\n",
      "Gradient: [ -2.2179 -16.2985 -19.4313 -77.6898 -29.8774]\n",
      "Weights: [-4.7639  0.5868 -1.0772  0.0696  0.1473]\n",
      "MSE loss: 92.5807\n",
      "Iteration: 38000\n",
      "Gradient: [  -5.7937  -11.1417  -53.2371    0.77   -194.0542]\n",
      "Weights: [-4.7579  0.583  -1.075   0.0693  0.1474]\n",
      "MSE loss: 92.4687\n",
      "Iteration: 38100\n",
      "Gradient: [11.5174 19.9965 46.4863 -4.7836 66.4834]\n",
      "Weights: [-4.7584  0.5857 -1.0755  0.07    0.1476]\n",
      "MSE loss: 92.7894\n",
      "Iteration: 38200\n",
      "Gradient: [  0.4726  -3.9279 -19.039  -94.9644 331.0722]\n",
      "Weights: [-4.7576  0.5826 -1.0754  0.0691  0.1475]\n",
      "MSE loss: 92.4794\n",
      "Iteration: 38300\n",
      "Gradient: [  -4.4105    9.0379  -15.895   172.3805 -255.9825]\n",
      "Weights: [-4.7416  0.5819 -1.0768  0.0701  0.1477]\n",
      "MSE loss: 93.1701\n",
      "Iteration: 38400\n",
      "Gradient: [   1.9818    3.4729   21.5738  -34.204  -296.8964]\n",
      "Weights: [-4.7513  0.5792 -1.0743  0.0689  0.1477]\n",
      "MSE loss: 92.5109\n",
      "Iteration: 38500\n",
      "Gradient: [   5.1971   -1.4475  -36.0325  -81.7559 -218.5022]\n",
      "Weights: [-4.7545  0.5875 -1.0745  0.068   0.1476]\n",
      "MSE loss: 92.5178\n",
      "Iteration: 38600\n",
      "Gradient: [ 1.05154e+01  1.60000e-02  1.56090e+00 -2.78928e+01 -2.74888e+02]\n",
      "Weights: [-4.7558  0.5797 -1.0757  0.0684  0.1479]\n",
      "MSE loss: 92.5033\n",
      "Iteration: 38700\n",
      "Gradient: [   2.4718   11.285     6.9947 -147.8047   53.4596]\n",
      "Weights: [-4.7473  0.5748 -1.0732  0.068   0.148 ]\n",
      "MSE loss: 92.4885\n",
      "Iteration: 38800\n",
      "Gradient: [  8.128   13.8988 -22.1363  96.8698 261.8589]\n",
      "Weights: [-4.7434  0.5656 -1.0712  0.0687  0.1479]\n",
      "MSE loss: 92.5484\n",
      "Iteration: 38900\n",
      "Gradient: [  0.4195   3.0022 -49.5732 -31.0641 192.4455]\n",
      "Weights: [-4.7429  0.5742 -1.0747  0.0685  0.148 ]\n",
      "MSE loss: 92.5292\n",
      "Iteration: 39000\n",
      "Gradient: [ -2.7502  -6.6091 -61.8922 -49.6797 -76.9755]\n",
      "Weights: [-4.7257  0.5632 -1.0751  0.0674  0.1482]\n",
      "MSE loss: 92.9726\n",
      "Iteration: 39100\n",
      "Gradient: [-16.0052   3.905    2.6914 134.6368 -50.1776]\n",
      "Weights: [-4.769   0.5772 -1.0752  0.0677  0.1483]\n",
      "MSE loss: 93.0615\n",
      "Iteration: 39200\n",
      "Gradient: [  -6.3105   12.4722   18.0349  -25.1132 -132.5365]\n",
      "Weights: [-4.75    0.5818 -1.0761  0.0673  0.1482]\n",
      "MSE loss: 92.4661\n",
      "Iteration: 39300\n",
      "Gradient: [  3.747    3.5189  23.9653  55.219  234.0633]\n",
      "Weights: [-4.7513  0.5834 -1.0759  0.0674  0.1482]\n",
      "MSE loss: 92.4711\n",
      "Iteration: 39400\n",
      "Gradient: [ 10.4045   3.2825  60.052   42.9912 284.7415]\n",
      "Weights: [-4.7345  0.5715 -1.0702  0.0667  0.1483]\n",
      "MSE loss: 93.0679\n",
      "Iteration: 39500\n",
      "Gradient: [  -2.2836   22.9651   22.9859   48.2605 -185.2574]\n",
      "Weights: [-4.7386  0.5634 -1.0684  0.0663  0.1484]\n",
      "MSE loss: 92.6034\n",
      "Iteration: 39600\n",
      "Gradient: [ 10.5254  -0.6222   1.5457 111.5123  28.6361]\n",
      "Weights: [-4.7311  0.5649 -1.0692  0.0658  0.1485]\n",
      "MSE loss: 92.749\n",
      "Iteration: 39700\n",
      "Gradient: [ -11.148     5.5162   -2.7304  -62.6407 -129.9684]\n",
      "Weights: [-4.7411  0.5681 -1.0699  0.0656  0.1483]\n",
      "MSE loss: 92.618\n",
      "Iteration: 39800\n",
      "Gradient: [ -2.874    2.6197   4.1246 -14.9628 -90.6151]\n",
      "Weights: [-4.7416  0.5731 -1.0719  0.066   0.1483]\n",
      "MSE loss: 92.5668\n",
      "Iteration: 39900\n",
      "Gradient: [ 13.2777 -19.3525 -38.9897  53.8178  49.4854]\n",
      "Weights: [-4.7306  0.571  -1.0725  0.0669  0.1483]\n",
      "MSE loss: 92.907\n",
      "Iteration: 40000\n",
      "Gradient: [   5.0971  -10.041   -41.3186  -25.1124 -113.9639]\n",
      "Weights: [-4.7446  0.5767 -1.0749  0.067   0.1483]\n",
      "MSE loss: 92.499\n",
      "Iteration: 40100\n",
      "Gradient: [-15.7006   5.8225  -8.7728   9.3026 -67.2062]\n",
      "Weights: [-4.7701  0.5834 -1.074   0.0659  0.1486]\n",
      "MSE loss: 92.8631\n",
      "Iteration: 40200\n",
      "Gradient: [  -2.7821    7.3936   34.0739   86.9617 -169.4871]\n",
      "Weights: [-4.7376  0.5813 -1.0783  0.067   0.1489]\n",
      "MSE loss: 92.9086\n",
      "Iteration: 40300\n",
      "Gradient: [ -3.8747 -14.4419   6.4107 -30.0583 145.9629]\n",
      "Weights: [-4.7497  0.5807 -1.0765  0.0663  0.1488]\n",
      "MSE loss: 92.4701\n",
      "Iteration: 40400\n",
      "Gradient: [   5.0922   -8.8709  -10.6846 -108.293  -191.719 ]\n",
      "Weights: [-4.744   0.5757 -1.0766  0.0667  0.1486]\n",
      "MSE loss: 92.5225\n",
      "Iteration: 40500\n",
      "Gradient: [ -6.5942  -8.1412  28.5863 -68.9593 193.3227]\n",
      "Weights: [-4.771   0.5949 -1.0815  0.067   0.1488]\n",
      "MSE loss: 92.5992\n",
      "Iteration: 40600\n",
      "Gradient: [   8.1952   -6.3666   38.0791   36.5595 -261.1583]\n",
      "Weights: [-4.7628  0.597  -1.0839  0.0679  0.1488]\n",
      "MSE loss: 92.468\n",
      "Iteration: 40700\n",
      "Gradient: [  -7.8294   11.4276  -28.0367   29.2053 -292.6325]\n",
      "Weights: [-4.7834  0.6126 -1.0891  0.0678  0.1488]\n",
      "MSE loss: 92.6647\n",
      "Iteration: 40800\n",
      "Gradient: [ -7.4252  14.9049   3.0629  34.5297 252.2181]\n",
      "Weights: [-4.7615  0.6104 -1.0887  0.0677  0.1487]\n",
      "MSE loss: 92.4157\n",
      "Iteration: 40900\n",
      "Gradient: [   3.1086   -8.7263  -21.4011 -106.0398  239.7818]\n",
      "Weights: [-4.7507  0.6056 -1.0912  0.0681  0.1489]\n",
      "MSE loss: 92.4542\n",
      "Iteration: 41000\n",
      "Gradient: [  2.4007  -2.8753  27.4482  17.5654 334.4223]\n",
      "Weights: [-4.7445  0.6072 -1.0929  0.0689  0.1489]\n",
      "MSE loss: 92.8944\n",
      "Iteration: 41100\n",
      "Gradient: [ -1.7312  -6.6761 -27.1251   3.7882 295.1054]\n",
      "Weights: [-4.7652  0.6013 -1.0917  0.0701  0.1489]\n",
      "MSE loss: 92.4636\n",
      "Iteration: 41200\n",
      "Gradient: [   3.326    17.1436   36.9733  -15.6156 -142.7919]\n",
      "Weights: [-4.7608  0.6064 -1.0906  0.0691  0.1487]\n",
      "MSE loss: 92.3738\n",
      "Iteration: 41300\n",
      "Gradient: [  -3.8005   13.1005   29.1864  -86.252  -563.4317]\n",
      "Weights: [-4.7627  0.5952 -1.0869  0.069   0.1489]\n",
      "MSE loss: 92.476\n",
      "Iteration: 41400\n",
      "Gradient: [ -3.1504  -4.674  -23.2712  -3.7479 144.2944]\n",
      "Weights: [-4.7565  0.5898 -1.0846  0.0685  0.1488]\n",
      "MSE loss: 92.4269\n",
      "Iteration: 41500\n",
      "Gradient: [ -5.942   -7.7964 -18.2228 -89.6414  43.5527]\n",
      "Weights: [-4.7433  0.5881 -1.0845  0.0685  0.1484]\n",
      "MSE loss: 92.494\n",
      "Iteration: 41600\n",
      "Gradient: [ -6.329   -5.5166 -35.3375 -23.5174 192.9125]\n",
      "Weights: [-4.7633  0.5998 -1.0812  0.0675  0.1482]\n",
      "MSE loss: 92.4232\n",
      "Iteration: 41700\n",
      "Gradient: [ -2.7572  11.6293  -4.8703 -49.7261 -96.3277]\n",
      "Weights: [-4.7684  0.5972 -1.0803  0.0674  0.1481]\n",
      "MSE loss: 92.5515\n",
      "Iteration: 41800\n",
      "Gradient: [  2.5237   3.1748 -22.6873 -57.342   69.7935]\n",
      "Weights: [-4.7533  0.596  -1.0799  0.0669  0.1479]\n",
      "MSE loss: 92.6935\n",
      "Iteration: 41900\n",
      "Gradient: [  3.9861  15.7007 -20.2906 -19.8263 -56.9604]\n",
      "Weights: [-4.7501  0.5891 -1.0774  0.068   0.1481]\n",
      "MSE loss: 92.636\n",
      "Iteration: 42000\n",
      "Gradient: [ -4.1142  -3.5559  19.8135  64.7701 -19.5708]\n",
      "Weights: [-4.7549  0.578  -1.0783  0.0687  0.148 ]\n",
      "MSE loss: 92.733\n",
      "Iteration: 42100\n",
      "Gradient: [ -6.8498   2.2084 -14.5627 -14.893  133.0116]\n",
      "Weights: [-4.7472  0.5745 -1.0783  0.0701  0.1478]\n",
      "MSE loss: 92.4513\n",
      "Iteration: 42200\n",
      "Gradient: [1.590000e-01 9.620000e-01 3.523010e+01 1.621449e+02 2.422231e+02]\n",
      "Weights: [-4.7542  0.5773 -1.0798  0.0704  0.148 ]\n",
      "MSE loss: 92.4916\n",
      "Iteration: 42300\n",
      "Gradient: [ -7.7033   3.0191 -12.7335  21.8299 139.2228]\n",
      "Weights: [-4.7479  0.5722 -1.0796  0.0701  0.148 ]\n",
      "MSE loss: 92.5392\n",
      "Iteration: 42400\n",
      "Gradient: [ -1.0605  -4.1645  -3.3061  12.2495 -38.5798]\n",
      "Weights: [-4.7532  0.5755 -1.0806  0.0706  0.148 ]\n",
      "MSE loss: 92.5247\n",
      "Iteration: 42500\n",
      "Gradient: [  1.7335  27.3408   5.309  -32.7304  55.5636]\n",
      "Weights: [-4.7683  0.598  -1.0821  0.0698  0.1477]\n",
      "MSE loss: 92.4361\n",
      "Iteration: 42600\n",
      "Gradient: [  5.5216  25.8466  28.3264 -25.3984 185.7239]\n",
      "Weights: [-4.7699  0.5868 -1.0787  0.0702  0.1477]\n",
      "MSE loss: 92.5982\n",
      "Iteration: 42700\n",
      "Gradient: [  -2.4919   15.2101  -35.0689  -45.0212 -114.7599]\n",
      "Weights: [-4.7733  0.5944 -1.0799  0.0698  0.1476]\n",
      "MSE loss: 92.5613\n",
      "Iteration: 42800\n",
      "Gradient: [  3.2093  -4.0544 -29.4813  81.1834  49.6051]\n",
      "Weights: [-4.7592  0.6039 -1.0821  0.0695  0.1476]\n",
      "MSE loss: 92.7494\n",
      "Iteration: 42900\n",
      "Gradient: [  6.7491  -5.0976   3.7683 -81.2543  94.0722]\n",
      "Weights: [-4.7479  0.5934 -1.0849  0.0701  0.1477]\n",
      "MSE loss: 92.5065\n",
      "Iteration: 43000\n",
      "Gradient: [ -4.3176 -23.5309 -55.7314 -18.9251 301.4947]\n",
      "Weights: [-4.7569  0.5847 -1.0844  0.0715  0.1477]\n",
      "MSE loss: 92.4679\n",
      "Iteration: 43100\n",
      "Gradient: [ -6.8273  -4.3389  11.0498 -74.6552 -23.137 ]\n",
      "Weights: [-4.7662  0.5969 -1.0863  0.072   0.1474]\n",
      "MSE loss: 92.4107\n",
      "Iteration: 43200\n",
      "Gradient: [-9.1942  1.3892  2.4679 70.5268  7.0616]\n",
      "Weights: [-4.7507  0.5848 -1.0867  0.073   0.1475]\n",
      "MSE loss: 92.3766\n",
      "Iteration: 43300\n",
      "Gradient: [  0.1106 -13.5754  -7.8876 -13.1251  61.8032]\n",
      "Weights: [-4.7485  0.5895 -1.0863  0.0722  0.1474]\n",
      "MSE loss: 92.4108\n",
      "Iteration: 43400\n",
      "Gradient: [  1.0716 -23.8872   2.1581  -1.2543  23.2465]\n",
      "Weights: [-4.7462  0.5935 -1.0899  0.0734  0.1475]\n",
      "MSE loss: 92.5872\n",
      "Iteration: 43500\n",
      "Gradient: [  -5.5526   -6.3089   11.7127    9.9538 -237.1667]\n",
      "Weights: [-4.7644  0.6036 -1.0927  0.0744  0.147 ]\n",
      "MSE loss: 92.3211\n",
      "Iteration: 43600\n",
      "Gradient: [ -5.3158   4.6185  22.3999 -92.3155 132.9218]\n",
      "Weights: [-4.7784  0.6027 -1.0899  0.0736  0.1473]\n",
      "MSE loss: 92.589\n",
      "Iteration: 43700\n",
      "Gradient: [  -8.1796    2.2142    8.2292   60.8174 -131.5911]\n",
      "Weights: [-4.7681  0.6109 -1.0922  0.0725  0.1473]\n",
      "MSE loss: 92.3584\n",
      "Iteration: 43800\n",
      "Gradient: [   0.403   -14.6556   -4.7828   71.7896 -208.052 ]\n",
      "Weights: [-4.7508  0.5962 -1.091   0.0732  0.1475]\n",
      "MSE loss: 92.3739\n",
      "Iteration: 43900\n",
      "Gradient: [ -15.0339  -23.9212   40.3701  -61.1998 -110.6271]\n",
      "Weights: [-4.7527  0.5991 -1.0938  0.073   0.1477]\n",
      "MSE loss: 92.3425\n",
      "Iteration: 44000\n",
      "Gradient: [ -0.0357 -14.8141 -34.3695 -15.7824 -29.1511]\n",
      "Weights: [-4.7506  0.604  -1.0993  0.074   0.1478]\n",
      "MSE loss: 92.3257\n",
      "Iteration: 44100\n",
      "Gradient: [ -4.7943   2.735   10.9083 -23.9909 185.0612]\n",
      "Weights: [-4.7624  0.6137 -1.1006  0.0744  0.1479]\n",
      "MSE loss: 92.4553\n",
      "Iteration: 44200\n",
      "Gradient: [  3.935   -1.6805  -0.9049  16.3157 -20.7478]\n",
      "Weights: [-4.7549  0.6007 -1.0981  0.0746  0.1476]\n",
      "MSE loss: 92.32\n",
      "Iteration: 44300\n",
      "Gradient: [ -3.392   -8.963   -0.5204 122.1241 233.9257]\n",
      "Weights: [-4.7488  0.6043 -1.0982  0.0746  0.1473]\n",
      "MSE loss: 92.39\n",
      "Iteration: 44400\n",
      "Gradient: [  4.4548  13.5894  13.5612  -9.7236 -72.6857]\n",
      "Weights: [-4.7556  0.617  -1.1013  0.0739  0.1475]\n",
      "MSE loss: 92.3972\n",
      "Iteration: 44500\n",
      "Gradient: [  -0.3617   14.1323   23.5835   55.8039 -160.8046]\n",
      "Weights: [-4.7761  0.6376 -1.1055  0.0735  0.1476]\n",
      "MSE loss: 92.3449\n",
      "Iteration: 44600\n",
      "Gradient: [ -0.4819  10.7922 -59.5472 -68.8004 248.1533]\n",
      "Weights: [-4.7828  0.6409 -1.1081  0.0741  0.1477]\n",
      "MSE loss: 92.3112\n",
      "Iteration: 44700\n",
      "Gradient: [ -6.4952  11.3363  -6.3841 -91.9212 -10.484 ]\n",
      "Weights: [-4.7695  0.6286 -1.1072  0.0747  0.1477]\n",
      "MSE loss: 92.2417\n",
      "Iteration: 44800\n",
      "Gradient: [ 4.8305  7.0763 18.2844 25.2073 34.4236]\n",
      "Weights: [-4.7522  0.6167 -1.1071  0.0753  0.1478]\n",
      "MSE loss: 92.3282\n",
      "Iteration: 44900\n",
      "Gradient: [   0.7685    4.9885  -35.1221  204.5504 -174.6783]\n",
      "Weights: [-4.776   0.6251 -1.108   0.0754  0.148 ]\n",
      "MSE loss: 92.3072\n",
      "Iteration: 45000\n",
      "Gradient: [ -16.6565   -8.5674   -3.1523  -13.3916 -266.367 ]\n",
      "Weights: [-4.781   0.6297 -1.1098  0.0751  0.148 ]\n",
      "MSE loss: 92.4298\n",
      "Iteration: 45100\n",
      "Gradient: [   2.3037   21.2961  -12.812   -40.6723 -251.8269]\n",
      "Weights: [-4.7538  0.6186 -1.1116  0.0754  0.148 ]\n",
      "MSE loss: 92.3947\n",
      "Iteration: 45200\n",
      "Gradient: [-4.60000e-03  1.10212e+01  8.79880e+00  4.22220e+01  7.71916e+01]\n",
      "Weights: [-4.776   0.6373 -1.1099  0.075   0.148 ]\n",
      "MSE loss: 92.4\n",
      "Iteration: 45300\n",
      "Gradient: [-6.319000e+00  1.014800e+00 -2.018000e-01 -5.729700e+01 -2.740463e+02]\n",
      "Weights: [-4.7666  0.6299 -1.1124  0.0745  0.1481]\n",
      "MSE loss: 92.3449\n",
      "Iteration: 45400\n",
      "Gradient: [  1.2764 -20.8655  15.5027  78.6927 -77.0367]\n",
      "Weights: [-4.791   0.6452 -1.1122  0.0737  0.1481]\n",
      "MSE loss: 92.5237\n",
      "Iteration: 45500\n",
      "Gradient: [-19.0824  -9.8915  53.2881 -79.7732  10.0228]\n",
      "Weights: [-4.7941  0.6493 -1.1106  0.0727  0.1482]\n",
      "MSE loss: 92.5327\n",
      "Iteration: 45600\n",
      "Gradient: [ -2.8531  21.8548 -16.0668   9.7728 -59.1306]\n",
      "Weights: [-4.766   0.6309 -1.108   0.0738  0.1483]\n",
      "MSE loss: 92.3943\n",
      "Iteration: 45700\n",
      "Gradient: [  1.5962  31.755   16.9706 125.3439 -14.8635]\n",
      "Weights: [-4.7644  0.6342 -1.1107  0.0743  0.1483]\n",
      "MSE loss: 92.4901\n",
      "Iteration: 45800\n",
      "Gradient: [  0.4229  13.5176  27.2094  84.0479 173.483 ]\n",
      "Weights: [-4.753   0.623  -1.1083  0.074   0.1483]\n",
      "MSE loss: 92.4204\n",
      "Iteration: 45900\n",
      "Gradient: [  2.8576  -0.6161 -14.6336 134.5997 -17.8825]\n",
      "Weights: [-4.7748  0.6245 -1.1081  0.0741  0.1482]\n",
      "MSE loss: 92.4283\n",
      "Iteration: 46000\n",
      "Gradient: [  -9.1267    0.3689   -5.4335   49.4949 -196.0566]\n",
      "Weights: [-4.7699  0.6239 -1.1062  0.0738  0.1484]\n",
      "MSE loss: 92.2858\n",
      "Iteration: 46100\n",
      "Gradient: [   2.9568   -6.6223  -24.9126  -17.4759 -347.8168]\n",
      "Weights: [-4.7769  0.63   -1.1094  0.0733  0.1485]\n",
      "MSE loss: 92.3674\n",
      "Iteration: 46200\n",
      "Gradient: [-3.627300e+00  8.760800e+00 -1.859480e+01  2.142000e-01 -3.143662e+02]\n",
      "Weights: [-4.7606  0.6303 -1.1085  0.0732  0.1484]\n",
      "MSE loss: 92.3986\n",
      "Iteration: 46300\n",
      "Gradient: [ -3.7782   9.4058   1.5006 -80.6912 216.9189]\n",
      "Weights: [-4.7803  0.6354 -1.1049  0.0727  0.1482]\n",
      "MSE loss: 92.3462\n",
      "Iteration: 46400\n",
      "Gradient: [ -1.8354   7.2762  -2.9181  51.4779 161.5384]\n",
      "Weights: [-4.7523  0.6223 -1.1061  0.0731  0.1482]\n",
      "MSE loss: 92.4418\n",
      "Iteration: 46500\n",
      "Gradient: [ 5.1678 16.257  22.8694 75.7256 18.4734]\n",
      "Weights: [-4.7639  0.6213 -1.1063  0.0735  0.1484]\n",
      "MSE loss: 92.2428\n",
      "Iteration: 46600\n",
      "Gradient: [ -6.611   14.097    0.7488 -57.0943 -19.1266]\n",
      "Weights: [-4.7708  0.6223 -1.1083  0.0734  0.1486]\n",
      "MSE loss: 92.3719\n",
      "Iteration: 46700\n",
      "Gradient: [ -6.8717   2.5589  46.281   78.1853 -52.3565]\n",
      "Weights: [-4.7708  0.6215 -1.109   0.0738  0.1486]\n",
      "MSE loss: 92.4081\n",
      "Iteration: 46800\n",
      "Gradient: [ -5.5626   8.8588 -26.6478   2.95    59.5954]\n",
      "Weights: [-4.7596  0.6269 -1.1098  0.0737  0.1485]\n",
      "MSE loss: 92.3063\n",
      "Iteration: 46900\n",
      "Gradient: [  8.1034  22.5338  33.2515  98.7942 171.6235]\n",
      "Weights: [-4.7746  0.6335 -1.1102  0.0734  0.1485]\n",
      "MSE loss: 92.24\n",
      "Iteration: 47000\n",
      "Gradient: [  -8.7231  -12.0496   11.7109 -115.3886  -52.7566]\n",
      "Weights: [-4.7732  0.625  -1.109   0.0727  0.1486]\n",
      "MSE loss: 92.6362\n",
      "Iteration: 47100\n",
      "Gradient: [ -2.3503 -10.7443 -59.5416 -83.8959 377.5085]\n",
      "Weights: [-4.7588  0.6183 -1.1082  0.0733  0.1487]\n",
      "MSE loss: 92.2665\n",
      "Iteration: 47200\n",
      "Gradient: [   7.7106    1.8961   11.2622  -66.1808 -125.102 ]\n",
      "Weights: [-4.7561  0.6229 -1.1063  0.0728  0.1487]\n",
      "MSE loss: 92.5966\n",
      "Iteration: 47300\n",
      "Gradient: [  4.1077   2.9687  13.4352 -80.0084 328.7582]\n",
      "Weights: [-4.7676  0.6291 -1.1089  0.0727  0.1489]\n",
      "MSE loss: 92.3149\n",
      "Iteration: 47400\n",
      "Gradient: [-2.8594 20.254  29.5167 -9.5554 24.5871]\n",
      "Weights: [-4.7665  0.6395 -1.1114  0.0727  0.1488]\n",
      "MSE loss: 92.4616\n",
      "Iteration: 47500\n",
      "Gradient: [   6.4895   15.5202    7.0657  -10.685  -162.4106]\n",
      "Weights: [-4.7613  0.6471 -1.1155  0.072   0.1488]\n",
      "MSE loss: 92.5265\n",
      "Iteration: 47600\n",
      "Gradient: [  -6.6832   -7.6286  -29.934   -18.3647 -122.1516]\n",
      "Weights: [-4.7669  0.6457 -1.1136  0.0725  0.1487]\n",
      "MSE loss: 92.4654\n",
      "Iteration: 47700\n",
      "Gradient: [ 1.9369 -4.1728 21.3835 41.8316 86.1348]\n",
      "Weights: [-4.7876  0.6596 -1.1141  0.0719  0.149 ]\n",
      "MSE loss: 92.9145\n",
      "Iteration: 47800\n",
      "Gradient: [ -3.9513   3.7318  -1.7331 -30.1526  23.8251]\n",
      "Weights: [-4.788   0.6629 -1.117   0.0707  0.1493]\n",
      "MSE loss: 92.3266\n",
      "Iteration: 47900\n",
      "Gradient: [  2.416   -7.6949 -41.5277   9.6099 297.3456]\n",
      "Weights: [-4.7719  0.6547 -1.1187  0.0712  0.1493]\n",
      "MSE loss: 92.3066\n",
      "Iteration: 48000\n",
      "Gradient: [4.890000e-02 1.275340e+01 5.117490e+01 5.832940e+01 2.258315e+02]\n",
      "Weights: [-4.7761  0.6477 -1.1159  0.072   0.1492]\n",
      "MSE loss: 92.2366\n",
      "Iteration: 48100\n",
      "Gradient: [  2.1653  12.8189 -35.3412 -63.3752 214.392 ]\n",
      "Weights: [-4.7711  0.6391 -1.1179  0.0723  0.1495]\n",
      "MSE loss: 92.276\n",
      "Iteration: 48200\n",
      "Gradient: [ -8.1087   8.8192 -29.1665 -59.8732 110.4304]\n",
      "Weights: [-4.771   0.6405 -1.1185  0.0731  0.1494]\n",
      "MSE loss: 92.231\n",
      "Iteration: 48300\n",
      "Gradient: [   0.6616   -1.6365   -9.1924   69.3899 -433.4868]\n",
      "Weights: [-4.7715  0.6466 -1.118   0.0725  0.1491]\n",
      "MSE loss: 92.2171\n",
      "Iteration: 48400\n",
      "Gradient: [ -3.6985 -25.1382  40.9115 -57.4117 -86.6665]\n",
      "Weights: [-4.7747  0.6508 -1.1202  0.0727  0.1492]\n",
      "MSE loss: 92.2045\n",
      "Iteration: 48500\n",
      "Gradient: [-14.3172  16.4891   3.5917   9.561  101.9632]\n",
      "Weights: [-4.7852  0.6559 -1.1235  0.0733  0.1493]\n",
      "MSE loss: 92.2923\n",
      "Iteration: 48600\n",
      "Gradient: [ -10.855    18.5964   -0.3849   80.3249 -187.9874]\n",
      "Weights: [-4.7892  0.6501 -1.1189  0.0736  0.1491]\n",
      "MSE loss: 92.37\n",
      "Iteration: 48700\n",
      "Gradient: [ -8.7468  -3.6092 -17.6061 -74.71   184.366 ]\n",
      "Weights: [-4.7938  0.6523 -1.1212  0.0735  0.1492]\n",
      "MSE loss: 92.645\n",
      "Iteration: 48800\n",
      "Gradient: [  -8.8411   22.6644   13.5528  -26.711  -116.1805]\n",
      "Weights: [-4.781   0.6649 -1.126   0.0738  0.1492]\n",
      "MSE loss: 92.2345\n",
      "Iteration: 48900\n",
      "Gradient: [  6.5694  -7.5755 -20.3795 -29.8297 -39.0437]\n",
      "Weights: [-4.7875  0.6584 -1.1204  0.0731  0.1491]\n",
      "MSE loss: 92.3019\n",
      "Iteration: 49000\n",
      "Gradient: [ -7.9351  -1.2402 -21.1994 -96.2936  18.0766]\n",
      "Weights: [-4.7785  0.6495 -1.1222  0.0729  0.1493]\n",
      "MSE loss: 92.421\n",
      "Iteration: 49100\n",
      "Gradient: [-12.2808   7.6291 -22.5232 -41.1526 -85.9125]\n",
      "Weights: [-4.7795  0.6517 -1.1207  0.0733  0.1492]\n",
      "MSE loss: 92.2131\n",
      "Iteration: 49200\n",
      "Gradient: [  -6.4414   -9.2383  -12.2189   -5.5928 -180.0208]\n",
      "Weights: [-4.7769  0.6508 -1.1193  0.0726  0.1492]\n",
      "MSE loss: 92.2078\n",
      "Iteration: 49300\n",
      "Gradient: [  1.3041  18.5643  42.765   45.6098 217.8066]\n",
      "Weights: [-4.7832  0.6526 -1.1196  0.0724  0.1494]\n",
      "MSE loss: 92.2553\n",
      "Iteration: 49400\n",
      "Gradient: [   3.6081  -23.0435   -6.3449 -122.5877 -212.121 ]\n",
      "Weights: [-4.7454  0.6375 -1.1192  0.0721  0.1494]\n",
      "MSE loss: 92.6832\n",
      "Iteration: 49500\n",
      "Gradient: [  10.9337   10.1073    1.4023 -172.0099   20.7793]\n",
      "Weights: [-4.7651  0.6396 -1.1179  0.0734  0.149 ]\n",
      "MSE loss: 92.2291\n",
      "Iteration: 49600\n",
      "Gradient: [   2.4397    3.1527    9.9439   32.1616 -165.1313]\n",
      "Weights: [-4.7572  0.6366 -1.1199  0.0737  0.1491]\n",
      "MSE loss: 92.3177\n",
      "Iteration: 49700\n",
      "Gradient: [  0.7868   1.8565  42.7833   2.6085 219.947 ]\n",
      "Weights: [-4.7591  0.6404 -1.1198  0.074   0.1494]\n",
      "MSE loss: 92.6766\n",
      "Iteration: 49800\n",
      "Gradient: [ -6.0791   0.1957  20.5288 -34.3439  24.5854]\n",
      "Weights: [-4.7569  0.6289 -1.1173  0.0735  0.1493]\n",
      "MSE loss: 92.2931\n",
      "Iteration: 49900\n",
      "Gradient: [  -6.0654  -14.111   -44.2319  -79.6624 -157.4505]\n",
      "Weights: [-4.7657  0.632  -1.1179  0.0734  0.1493]\n",
      "MSE loss: 92.3265\n",
      "Iteration: 50000\n",
      "Gradient: [  8.3306  -0.7703 -32.4055 159.319  266.5133]\n",
      "Weights: [-4.7653  0.6327 -1.1145  0.0729  0.1492]\n",
      "MSE loss: 92.2328\n",
      "Iteration: 50100\n",
      "Gradient: [  4.9011   8.46    40.5248 -31.9236 110.3704]\n",
      "Weights: [-4.7555  0.6347 -1.1145  0.0735  0.1491]\n",
      "MSE loss: 92.8141\n",
      "Iteration: 50200\n",
      "Gradient: [ -15.9614    3.5547   -7.5889  115.1913 -112.7087]\n",
      "Weights: [-4.7724  0.6246 -1.1143  0.0736  0.1492]\n",
      "MSE loss: 92.6049\n",
      "Iteration: 50300\n",
      "Gradient: [   1.0232  -15.3326  -44.948   -17.476  -278.4537]\n",
      "Weights: [-4.7625  0.6214 -1.1141  0.0741  0.1493]\n",
      "MSE loss: 92.3533\n",
      "Iteration: 50400\n",
      "Gradient: [  7.9229 -18.5728  43.5961  49.8332 -25.5808]\n",
      "Weights: [-4.7636  0.6283 -1.1139  0.0732  0.1492]\n",
      "MSE loss: 92.2524\n",
      "Iteration: 50500\n",
      "Gradient: [  -0.8341    6.0558   14.4268   24.8174 -215.0689]\n",
      "Weights: [-4.7687  0.626  -1.1107  0.0726  0.149 ]\n",
      "MSE loss: 92.335\n",
      "Iteration: 50600\n",
      "Gradient: [  1.1984 -18.4829 -31.6165 -67.2382  28.4292]\n",
      "Weights: [-4.7683  0.6235 -1.1101  0.0728  0.1488]\n",
      "MSE loss: 92.4686\n",
      "Iteration: 50700\n",
      "Gradient: [ -3.8146  10.5457  13.8431 115.5346 -65.8599]\n",
      "Weights: [-4.7598  0.6268 -1.1111  0.073   0.1488]\n",
      "MSE loss: 92.266\n",
      "Iteration: 50800\n",
      "Gradient: [   2.0091   -5.9454  -18.0259 -156.5145   54.9093]\n",
      "Weights: [-4.7667  0.6252 -1.1096  0.073   0.1487]\n",
      "MSE loss: 92.2845\n",
      "Iteration: 50900\n",
      "Gradient: [ 4.220000e-02 -1.232830e+01  3.851400e+01 -6.388210e+01  2.145705e+02]\n",
      "Weights: [-4.7647  0.6231 -1.1068  0.073   0.1488]\n",
      "MSE loss: 92.3295\n",
      "Iteration: 51000\n",
      "Gradient: [-4.5805  4.5625 21.2235 31.5331 94.5727]\n",
      "Weights: [-4.7544  0.6186 -1.108   0.0728  0.1488]\n",
      "MSE loss: 92.3015\n",
      "Iteration: 51100\n",
      "Gradient: [  6.8766 -17.4429   1.1917  63.9243 185.9479]\n",
      "Weights: [-4.7663  0.6274 -1.1081  0.0731  0.1487]\n",
      "MSE loss: 92.3038\n",
      "Iteration: 51200\n",
      "Gradient: [   0.2596  -11.6079  -42.1149  -33.8054 -177.0749]\n",
      "Weights: [-4.7586  0.6282 -1.1116  0.0732  0.1487]\n",
      "MSE loss: 92.2783\n",
      "Iteration: 51300\n",
      "Gradient: [  1.3443  -9.4171  19.9511 100.4704   8.2417]\n",
      "Weights: [-4.7569  0.631  -1.1134  0.0733  0.149 ]\n",
      "MSE loss: 92.4007\n",
      "Iteration: 51400\n",
      "Gradient: [ 13.9388  -4.8083  10.9839  -7.8641 333.198 ]\n",
      "Weights: [-4.7466  0.6291 -1.1115  0.0735  0.1488]\n",
      "MSE loss: 93.076\n",
      "Iteration: 51500\n",
      "Gradient: [ -1.9502   6.3903  42.492  -66.0882 437.1445]\n",
      "Weights: [-4.7607  0.6304 -1.1112  0.0736  0.1487]\n",
      "MSE loss: 92.4074\n",
      "Iteration: 51600\n",
      "Gradient: [  2.1352   5.8099  32.4879   0.4449 277.0126]\n",
      "Weights: [-4.7487  0.6284 -1.1135  0.0739  0.1486]\n",
      "MSE loss: 92.5076\n",
      "Iteration: 51700\n",
      "Gradient: [ 3.1955 -8.7229  2.478  29.4408  3.7778]\n",
      "Weights: [-4.7613  0.6273 -1.1119  0.0744  0.1486]\n",
      "MSE loss: 92.287\n",
      "Iteration: 51800\n",
      "Gradient: [   4.0695  -16.5938   -7.335    39.8979 -114.3379]\n",
      "Weights: [-4.747   0.6144 -1.1105  0.074   0.1489]\n",
      "MSE loss: 92.3907\n",
      "Iteration: 51900\n",
      "Gradient: [ -0.0833  -6.3571 -19.702  -38.9906  28.7491]\n",
      "Weights: [-4.7469  0.615  -1.1109  0.0729  0.1492]\n",
      "MSE loss: 92.3909\n",
      "Iteration: 52000\n",
      "Gradient: [ -2.7659   1.8525   0.1152  45.3953 -22.6089]\n",
      "Weights: [-4.7435  0.6115 -1.1098  0.0735  0.1492]\n",
      "MSE loss: 92.5011\n",
      "Iteration: 52100\n",
      "Gradient: [ -4.9223 -13.1952   6.2961 -13.043    0.1998]\n",
      "Weights: [-4.7692  0.6176 -1.1095  0.0735  0.149 ]\n",
      "MSE loss: 92.4883\n",
      "Iteration: 52200\n",
      "Gradient: [  -2.284     5.1694   34.2915  154.9464 -121.4732]\n",
      "Weights: [-4.762   0.6195 -1.1132  0.0737  0.1492]\n",
      "MSE loss: 92.3854\n",
      "Iteration: 52300\n",
      "Gradient: [  -0.3948    2.4565   41.3037   17.1775 -202.4208]\n",
      "Weights: [-4.7476  0.6202 -1.1121  0.0733  0.1489]\n",
      "MSE loss: 92.3942\n",
      "Iteration: 52400\n",
      "Gradient: [  -8.4393    7.376   -45.0451 -104.0572  -40.6506]\n",
      "Weights: [-4.7498  0.6309 -1.1154  0.0734  0.1488]\n",
      "MSE loss: 92.4657\n",
      "Iteration: 52500\n",
      "Gradient: [ -2.8303  -7.5399   7.7234  20.7808 276.2402]\n",
      "Weights: [-4.7758  0.637  -1.1123  0.0737  0.1486]\n",
      "MSE loss: 92.247\n",
      "Iteration: 52600\n",
      "Gradient: [ -11.4047   11.7402   13.7988  -63.9765 -154.4569]\n",
      "Weights: [-4.7777  0.6386 -1.1133  0.0743  0.1483]\n",
      "MSE loss: 92.242\n",
      "Iteration: 52700\n",
      "Gradient: [-0.6434 11.2996 27.5407 41.7748 36.032 ]\n",
      "Weights: [-4.7587  0.64   -1.1148  0.0742  0.1482]\n",
      "MSE loss: 92.466\n",
      "Iteration: 52800\n",
      "Gradient: [  3.9457  -4.3747  -4.3327 -52.1317 -16.0356]\n",
      "Weights: [-4.7833  0.6529 -1.1157  0.074   0.1482]\n",
      "MSE loss: 92.2716\n",
      "Iteration: 52900\n",
      "Gradient: [  -2.8887    2.8458    0.663   -57.6119 -657.7488]\n",
      "Weights: [-4.7853  0.6547 -1.1147  0.0741  0.1482]\n",
      "MSE loss: 92.4339\n",
      "Iteration: 53000\n",
      "Gradient: [  -1.2636    5.0745  -13.0312  -89.5282 -179.149 ]\n",
      "Weights: [-4.7951  0.6579 -1.1171  0.0743  0.1481]\n",
      "MSE loss: 92.3891\n",
      "Iteration: 53100\n",
      "Gradient: [ -2.6402 -23.4082  19.7472 -52.4195 -25.5107]\n",
      "Weights: [-4.7935  0.6615 -1.1176  0.0744  0.1483]\n",
      "MSE loss: 92.4139\n",
      "Iteration: 53200\n",
      "Gradient: [  14.2578    3.4285  -42.0626   77.6225 -157.8889]\n",
      "Weights: [-4.7687  0.6431 -1.1149  0.0745  0.1483]\n",
      "MSE loss: 92.3119\n",
      "Iteration: 53300\n",
      "Gradient: [  4.8831  19.2734 -25.9867  85.6517 -90.6501]\n",
      "Weights: [-4.7689  0.6471 -1.1167  0.0744  0.1482]\n",
      "MSE loss: 92.3125\n",
      "Iteration: 53400\n",
      "Gradient: [  -9.6303   -9.9367   37.2021   33.1843 -257.2127]\n",
      "Weights: [-4.7696  0.6434 -1.1128  0.074   0.1483]\n",
      "MSE loss: 92.5205\n",
      "Iteration: 53500\n",
      "Gradient: [  15.7071  -13.4884  -11.5551  -45.2431 -234.1897]\n",
      "Weights: [-4.7619  0.6351 -1.1145  0.0747  0.1486]\n",
      "MSE loss: 92.4657\n",
      "Iteration: 53600\n",
      "Gradient: [   8.6163    4.1423  -10.8591   30.1151 -487.8737]\n",
      "Weights: [-4.7609  0.639  -1.1168  0.0748  0.1484]\n",
      "MSE loss: 92.3121\n",
      "Iteration: 53700\n",
      "Gradient: [  -2.503    35.6173   -5.5701 -137.4364  -29.3992]\n",
      "Weights: [-4.7612  0.6445 -1.1192  0.0746  0.1486]\n",
      "MSE loss: 92.4395\n",
      "Iteration: 53800\n",
      "Gradient: [  -8.7353    8.7383   27.627  -165.2262  118.1138]\n",
      "Weights: [-4.7667  0.6383 -1.1208  0.0759  0.1488]\n",
      "MSE loss: 92.2191\n",
      "Iteration: 53900\n",
      "Gradient: [  2.1781   2.2878   5.9968 -15.5624 230.3367]\n",
      "Weights: [-4.7602  0.6309 -1.1177  0.076   0.1486]\n",
      "MSE loss: 92.2557\n",
      "Iteration: 54000\n",
      "Gradient: [ -5.827   -8.8868 -30.5922  49.4828 198.9973]\n",
      "Weights: [-4.7623  0.628  -1.1193  0.0774  0.1487]\n",
      "MSE loss: 92.4176\n",
      "Iteration: 54100\n",
      "Gradient: [ -2.7297   1.3735 -10.3687 103.3143 294.2209]\n",
      "Weights: [-4.7637  0.6352 -1.1195  0.0766  0.1486]\n",
      "MSE loss: 92.294\n",
      "Iteration: 54200\n",
      "Gradient: [ 5.5269 -5.1253 16.1914 28.9125 43.5984]\n",
      "Weights: [-4.7637  0.642  -1.1199  0.0758  0.1485]\n",
      "MSE loss: 92.343\n",
      "Iteration: 54300\n",
      "Gradient: [  1.5237  16.6197 -39.0116 -49.2769 -49.8367]\n",
      "Weights: [-4.7565  0.6295 -1.1171  0.0755  0.1485]\n",
      "MSE loss: 92.2531\n",
      "Iteration: 54400\n",
      "Gradient: [ -7.1672 -11.7746  15.2273 -72.8202  -9.4265]\n",
      "Weights: [-4.7655  0.6338 -1.117   0.0748  0.1485]\n",
      "MSE loss: 92.2743\n",
      "Iteration: 54500\n",
      "Gradient: [   7.0417    0.5051   33.337    61.1428 -135.6861]\n",
      "Weights: [-4.7509  0.6317 -1.1185  0.0752  0.1485]\n",
      "MSE loss: 92.4101\n",
      "Iteration: 54600\n",
      "Gradient: [  -8.3739    0.7186  -55.7181 -125.7583   16.2403]\n",
      "Weights: [-4.7867  0.6447 -1.1168  0.0747  0.1483]\n",
      "MSE loss: 92.4725\n",
      "Iteration: 54700\n",
      "Gradient: [  0.7579   5.7196  11.48   -53.7629  70.9263]\n",
      "Weights: [-4.7783  0.6349 -1.1151  0.0754  0.1483]\n",
      "MSE loss: 92.3467\n",
      "Iteration: 54800\n",
      "Gradient: [   1.1112    2.6696  -44.6962  -57.2798 -266.6342]\n",
      "Weights: [-4.7559  0.6265 -1.1148  0.0763  0.1483]\n",
      "MSE loss: 92.334\n",
      "Iteration: 54900\n",
      "Gradient: [ 11.1656  -5.7094  52.7058   5.2891 120.9044]\n",
      "Weights: [-4.7514  0.6203 -1.1152  0.0764  0.1482]\n",
      "MSE loss: 92.3083\n",
      "Iteration: 55000\n",
      "Gradient: [  -6.0548  -18.5799  -51.8934  -27.7659 -251.1645]\n",
      "Weights: [-4.7876  0.6391 -1.1167  0.0755  0.1482]\n",
      "MSE loss: 92.7312\n",
      "Iteration: 55100\n",
      "Gradient: [ -0.4603   3.7617 -36.4708 -44.3754 220.0617]\n",
      "Weights: [-4.7714  0.639  -1.1187  0.0756  0.1483]\n",
      "MSE loss: 92.253\n",
      "Iteration: 55200\n",
      "Gradient: [ -0.7643  33.1382  -2.472   17.5785 -15.636 ]\n",
      "Weights: [-4.7711  0.6454 -1.118   0.0752  0.1484]\n",
      "MSE loss: 92.2436\n",
      "Iteration: 55300\n",
      "Gradient: [  -5.1306   -3.6631   16.7142 -113.4966   49.6338]\n",
      "Weights: [-4.7955  0.6526 -1.1154  0.0742  0.1485]\n",
      "MSE loss: 92.466\n",
      "Iteration: 55400\n",
      "Gradient: [  -2.3045  -17.6948   22.2379 -132.4504 -221.1229]\n",
      "Weights: [-4.7698  0.6376 -1.1148  0.0742  0.1485]\n",
      "MSE loss: 92.2091\n",
      "Iteration: 55500\n",
      "Gradient: [ -7.2164   2.675   -9.2382 -31.9819 -39.4814]\n",
      "Weights: [-4.7713  0.641  -1.1152  0.074   0.1483]\n",
      "MSE loss: 92.2531\n",
      "Iteration: 55600\n",
      "Gradient: [   1.3431   -8.6461   17.2037 -134.3563  336.7317]\n",
      "Weights: [-4.7651  0.6346 -1.1137  0.0743  0.1487]\n",
      "MSE loss: 92.3532\n",
      "Iteration: 55700\n",
      "Gradient: [  -2.6387    4.3304    8.9056   15.9651 -184.6395]\n",
      "Weights: [-4.7744  0.6381 -1.1155  0.0739  0.1488]\n",
      "MSE loss: 92.2205\n",
      "Iteration: 55800\n",
      "Gradient: [  -7.2252    6.1961  -11.7975 -142.0026 -165.7701]\n",
      "Weights: [-4.7812  0.6304 -1.1146  0.0738  0.1488]\n",
      "MSE loss: 92.9047\n",
      "Iteration: 55900\n",
      "Gradient: [ -4.3147   0.5502  40.9591   6.1032 434.162 ]\n",
      "Weights: [-4.7817  0.634  -1.114   0.0737  0.1489]\n",
      "MSE loss: 92.4838\n",
      "Iteration: 56000\n",
      "Gradient: [ -4.0576   6.6001  42.6622 -67.8786  99.7727]\n",
      "Weights: [-4.7589  0.6286 -1.114   0.0735  0.1489]\n",
      "MSE loss: 92.2608\n",
      "Iteration: 56100\n",
      "Gradient: [  1.2777  19.4843 -41.042   44.2007 -38.0943]\n",
      "Weights: [-4.7599  0.6283 -1.1171  0.0743  0.1489]\n",
      "MSE loss: 92.3464\n",
      "Iteration: 56200\n",
      "Gradient: [ -4.5585 -21.843   -8.6539 142.7675 -27.3722]\n",
      "Weights: [-4.7795  0.6399 -1.1177  0.0748  0.1488]\n",
      "MSE loss: 92.288\n",
      "Iteration: 56300\n",
      "Gradient: [  0.4033   5.2343   2.5078   9.1087 310.205 ]\n",
      "Weights: [-4.7752  0.6431 -1.1183  0.0747  0.1488]\n",
      "MSE loss: 92.2105\n",
      "Iteration: 56400\n",
      "Gradient: [  3.5756   1.1424   3.037  -50.3566 -21.7994]\n",
      "Weights: [-4.7632  0.6341 -1.1203  0.0758  0.1486]\n",
      "MSE loss: 92.2376\n",
      "Iteration: 56500\n",
      "Gradient: [-12.4625  -6.6131  20.5804  20.3843  18.8625]\n",
      "Weights: [-4.7799  0.6409 -1.117   0.0753  0.1488]\n",
      "MSE loss: 92.4128\n",
      "Iteration: 56600\n",
      "Gradient: [  8.2834  -0.8162  15.3774 118.5698  24.5634]\n",
      "Weights: [-4.7425  0.6193 -1.1165  0.0761  0.1486]\n",
      "MSE loss: 92.4692\n",
      "Iteration: 56700\n",
      "Gradient: [  7.2673  -5.881  -23.3077 -37.188  273.933 ]\n",
      "Weights: [-4.7613  0.6347 -1.1181  0.0759  0.1486]\n",
      "MSE loss: 92.3183\n",
      "Iteration: 56800\n",
      "Gradient: [  -2.9405   12.0047   -0.1911   79.7418 -120.8619]\n",
      "Weights: [-4.766   0.6392 -1.1202  0.0759  0.1487]\n",
      "MSE loss: 92.2368\n",
      "Iteration: 56900\n",
      "Gradient: [   2.8177   -2.1263   -1.2062 -155.8221   21.3136]\n",
      "Weights: [-4.7614  0.6379 -1.1209  0.0756  0.1488]\n",
      "MSE loss: 92.2604\n",
      "Iteration: 57000\n",
      "Gradient: [ -11.5483    1.534    13.6981   28.7256 -103.7306]\n",
      "Weights: [-4.7706  0.6378 -1.1218  0.0758  0.1487]\n",
      "MSE loss: 92.2903\n",
      "Iteration: 57100\n",
      "Gradient: [ -6.7143  14.463   47.7609 -18.1814 216.5689]\n",
      "Weights: [-4.7736  0.6502 -1.1255  0.0762  0.1488]\n",
      "MSE loss: 92.1652\n",
      "Iteration: 57200\n",
      "Gradient: [  0.7492 -16.9814  -4.0591   3.7794 243.7182]\n",
      "Weights: [-4.7754  0.6482 -1.1257  0.0765  0.1489]\n",
      "MSE loss: 92.2176\n",
      "Iteration: 57300\n",
      "Gradient: [12.6377  3.9014  6.6013 -0.4691 33.6668]\n",
      "Weights: [-4.7543  0.6429 -1.1274  0.0757  0.1492]\n",
      "MSE loss: 92.3493\n",
      "Iteration: 57400\n",
      "Gradient: [  1.483  -10.5541 -19.1742  -3.3724  27.7689]\n",
      "Weights: [-4.7709  0.6417 -1.1265  0.0761  0.1492]\n",
      "MSE loss: 92.2769\n",
      "Iteration: 57500\n",
      "Gradient: [  6.107    0.6029 -22.2977  92.4483 157.8193]\n",
      "Weights: [-4.7553  0.6455 -1.1278  0.0751  0.1493]\n",
      "MSE loss: 92.338\n",
      "Iteration: 57600\n",
      "Gradient: [ -6.6926   4.9082  -3.7921 125.897   46.5789]\n",
      "Weights: [-4.7819  0.6491 -1.1276  0.0759  0.1493]\n",
      "MSE loss: 92.3568\n",
      "Iteration: 57700\n",
      "Gradient: [  8.0959  -5.8853  33.6208 106.2103 -44.5068]\n",
      "Weights: [-4.7788  0.6602 -1.127   0.0749  0.149 ]\n",
      "MSE loss: 92.1777\n",
      "Iteration: 57800\n",
      "Gradient: [  -6.5204    3.0215   -9.8814  -48.9194 -228.0391]\n",
      "Weights: [-4.7718  0.6518 -1.1269  0.0758  0.149 ]\n",
      "MSE loss: 92.1686\n",
      "Iteration: 57900\n",
      "Gradient: [  -3.4485    2.0773  -21.6653  -78.3288 -214.4974]\n",
      "Weights: [-4.7556  0.6438 -1.1304  0.0769  0.1489]\n",
      "MSE loss: 92.3243\n",
      "Iteration: 58000\n",
      "Gradient: [  1.6746  -6.9843 -25.4829 104.8251  47.9749]\n",
      "Weights: [-4.7612  0.6456 -1.1286  0.0771  0.1491]\n",
      "MSE loss: 92.3229\n",
      "Iteration: 58100\n",
      "Gradient: [ -6.9888 -22.9741  15.3616  70.5864 177.9521]\n",
      "Weights: [-4.7757  0.6569 -1.1325  0.0773  0.149 ]\n",
      "MSE loss: 92.145\n",
      "Iteration: 58200\n",
      "Gradient: [  6.9767  11.596    5.6985 -58.3334 -25.1108]\n",
      "Weights: [-4.7691  0.6478 -1.1315  0.0776  0.149 ]\n",
      "MSE loss: 92.1953\n",
      "Iteration: 58300\n",
      "Gradient: [   7.1078    5.0569    6.0632  -43.0812 -232.5304]\n",
      "Weights: [-4.7716  0.6529 -1.131   0.0774  0.1488]\n",
      "MSE loss: 92.1357\n",
      "Iteration: 58400\n",
      "Gradient: [ -3.6892 -19.3481  35.8582 -35.9063 238.9458]\n",
      "Weights: [-4.7693  0.6462 -1.1306  0.0781  0.1486]\n",
      "MSE loss: 92.2323\n",
      "Iteration: 58500\n",
      "Gradient: [  2.2769  -3.3285  -3.3209  36.369  -43.4062]\n",
      "Weights: [-4.7583  0.6425 -1.1298  0.079   0.1485]\n",
      "MSE loss: 92.2409\n",
      "Iteration: 58600\n",
      "Gradient: [ 3.815   0.8998 14.3952 21.2388  6.9204]\n",
      "Weights: [-4.7884  0.6562 -1.1308  0.078   0.1486]\n",
      "MSE loss: 92.3705\n",
      "Iteration: 58700\n",
      "Gradient: [ 1.9188 -0.2409 26.4026 20.3131 25.4546]\n",
      "Weights: [-4.7779  0.6501 -1.1307  0.0788  0.1486]\n",
      "MSE loss: 92.1895\n",
      "Iteration: 58800\n",
      "Gradient: [  3.9647   7.1433  14.6292  41.763  101.9234]\n",
      "Weights: [-4.7634  0.6513 -1.1304  0.0785  0.1483]\n",
      "MSE loss: 92.2036\n",
      "Iteration: 58900\n",
      "Gradient: [  -4.1976   18.9591  -16.6244   13.2562 -124.1151]\n",
      "Weights: [-4.7836  0.6509 -1.1305  0.0787  0.1484]\n",
      "MSE loss: 92.3758\n",
      "Iteration: 59000\n",
      "Gradient: [  -4.5163   11.8361   -3.3175  -32.4931 -122.1014]\n",
      "Weights: [-4.7521  0.6451 -1.1337  0.0784  0.1487]\n",
      "MSE loss: 92.3425\n",
      "Iteration: 59100\n",
      "Gradient: [ -0.6948  13.5099  55.958   60.8619 -93.2349]\n",
      "Weights: [-4.7736  0.6513 -1.1295  0.0781  0.1488]\n",
      "MSE loss: 92.3106\n",
      "Iteration: 59200\n",
      "Gradient: [-14.6197  -9.3506 -50.1153  53.2593 183.4558]\n",
      "Weights: [-4.7732  0.6485 -1.1306  0.0775  0.1487]\n",
      "MSE loss: 92.3476\n",
      "Iteration: 59300\n",
      "Gradient: [-10.9155  -9.1821 -20.4251  97.2293   2.2009]\n",
      "Weights: [-4.7667  0.6362 -1.1266  0.0786  0.1486]\n",
      "MSE loss: 92.2106\n",
      "Iteration: 59400\n",
      "Gradient: [   2.4786  -12.0509  -26.4202  -29.6986 -110.5628]\n",
      "Weights: [-4.7706  0.6415 -1.129   0.0789  0.1484]\n",
      "MSE loss: 92.2393\n",
      "Iteration: 59500\n",
      "Gradient: [-5.3097  6.9726 12.0624 -4.8167 89.4133]\n",
      "Weights: [-4.7669  0.6496 -1.1301  0.0793  0.1483]\n",
      "MSE loss: 92.2185\n",
      "Iteration: 59600\n",
      "Gradient: [ -3.1729  -0.6631   7.4898 -21.869   16.7522]\n",
      "Weights: [-4.7566  0.6426 -1.131   0.0788  0.1484]\n",
      "MSE loss: 92.2738\n",
      "Iteration: 59700\n",
      "Gradient: [  5.3627 -19.8896  10.4469 -30.6547  37.1912]\n",
      "Weights: [-4.7607  0.6356 -1.1304  0.0797  0.1485]\n",
      "MSE loss: 92.252\n",
      "Iteration: 59800\n",
      "Gradient: [  5.476    3.1594  23.9505 -29.9107 -62.2442]\n",
      "Weights: [-4.7848  0.6484 -1.1308  0.079   0.1485]\n",
      "MSE loss: 92.5636\n",
      "Iteration: 59900\n",
      "Gradient: [  7.1255   0.3387   3.2941  -7.9522 157.6532]\n",
      "Weights: [-4.7544  0.6432 -1.1275  0.0788  0.1483]\n",
      "MSE loss: 92.5224\n",
      "Iteration: 60000\n",
      "Gradient: [  -8.5811   -7.8897  -39.4908   46.3613 -296.7065]\n",
      "Weights: [-4.788   0.6449 -1.1262  0.0793  0.148 ]\n",
      "MSE loss: 92.5653\n",
      "Iteration: 60100\n",
      "Gradient: [  0.4855  21.9863  29.1604 153.554    5.3145]\n",
      "Weights: [-4.7918  0.6506 -1.1228  0.0779  0.1479]\n",
      "MSE loss: 92.4368\n",
      "Iteration: 60200\n",
      "Gradient: [  0.5002  -9.2313  -7.2018 -16.7884  83.609 ]\n",
      "Weights: [-4.7797  0.6552 -1.1265  0.0784  0.1479]\n",
      "MSE loss: 92.1382\n",
      "Iteration: 60300\n",
      "Gradient: [  -1.4598  -11.1452  -35.119    34.488  -168.1352]\n",
      "Weights: [-4.78    0.6502 -1.1277  0.0786  0.148 ]\n",
      "MSE loss: 92.3073\n",
      "Iteration: 60400\n",
      "Gradient: [-3.4567  0.5143 30.7907 88.6705 56.1147]\n",
      "Weights: [-4.7635  0.6394 -1.1261  0.0796  0.1482]\n",
      "MSE loss: 92.2902\n",
      "Iteration: 60500\n",
      "Gradient: [ -2.5916   2.2628 -24.4458  -6.0763  23.2636]\n",
      "Weights: [-4.7783  0.6461 -1.1268  0.0787  0.148 ]\n",
      "MSE loss: 92.3494\n",
      "Iteration: 60600\n",
      "Gradient: [  -7.6682    7.3805   17.6041 -122.3052  300.76  ]\n",
      "Weights: [-4.7757  0.651  -1.1264  0.0779  0.1483]\n",
      "MSE loss: 92.1348\n",
      "Iteration: 60700\n",
      "Gradient: [  1.0252  -1.9381  47.9688  65.1744 259.7955]\n",
      "Weights: [-4.7628  0.641  -1.1247  0.0787  0.1481]\n",
      "MSE loss: 92.2243\n",
      "Iteration: 60800\n",
      "Gradient: [  3.457   11.1729 -37.8145  18.3094 287.2757]\n",
      "Weights: [-4.757   0.6446 -1.1279  0.0781  0.1482]\n",
      "MSE loss: 92.2803\n",
      "Iteration: 60900\n",
      "Gradient: [ -10.5227    4.4158   27.4355  -43.1399 -313.8251]\n",
      "Weights: [-4.7762  0.657  -1.131   0.0779  0.1484]\n",
      "MSE loss: 92.1372\n",
      "Iteration: 61000\n",
      "Gradient: [ 1.5808  9.737   4.911  -7.9156 18.3882]\n",
      "Weights: [-4.7703  0.6548 -1.1353  0.0794  0.1486]\n",
      "MSE loss: 92.111\n",
      "Iteration: 61100\n",
      "Gradient: [  -0.8286   -2.7078    6.3216 -107.2411  -52.6921]\n",
      "Weights: [-4.785   0.6595 -1.1373  0.0801  0.1485]\n",
      "MSE loss: 92.2983\n",
      "Iteration: 61200\n",
      "Gradient: [-4.9991  5.0975  6.2554 59.2131 88.284 ]\n",
      "Weights: [-4.7791  0.6724 -1.1406  0.0805  0.1483]\n",
      "MSE loss: 92.1287\n",
      "Iteration: 61300\n",
      "Gradient: [  2.6246  14.2007  -0.6973 132.2154  14.9543]\n",
      "Weights: [-4.7651  0.6653 -1.1436  0.0821  0.1482]\n",
      "MSE loss: 92.2105\n",
      "Iteration: 61400\n",
      "Gradient: [-14.9538   1.8525  13.9435  43.7201 -66.6219]\n",
      "Weights: [-4.8     0.6704 -1.1417  0.0822  0.1482]\n",
      "MSE loss: 92.4838\n",
      "Iteration: 61500\n",
      "Gradient: [  3.1258  -0.475  -48.8197  72.582  -39.2323]\n",
      "Weights: [-4.775   0.6696 -1.1415  0.0816  0.148 ]\n",
      "MSE loss: 92.0995\n",
      "Iteration: 61600\n",
      "Gradient: [   0.3954   -7.1603  -20.677  -117.8162 -352.4114]\n",
      "Weights: [-4.7877  0.6796 -1.144   0.0814  0.1479]\n",
      "MSE loss: 92.1002\n",
      "Iteration: 61700\n",
      "Gradient: [ -5.9081 -24.9061 -49.7967  38.798  292.0956]\n",
      "Weights: [-4.7834  0.6816 -1.1481  0.0816  0.1483]\n",
      "MSE loss: 92.0673\n",
      "Iteration: 61800\n",
      "Gradient: [  4.925   -3.8813  16.3461  58.2301 -59.6694]\n",
      "Weights: [-4.8045  0.6959 -1.1477  0.0811  0.1481]\n",
      "MSE loss: 92.2117\n",
      "Iteration: 61900\n",
      "Gradient: [ -3.5934   7.6328 -39.782  146.5569 -88.2249]\n",
      "Weights: [-4.7923  0.6823 -1.1464  0.0815  0.1483]\n",
      "MSE loss: 92.0986\n",
      "Iteration: 62000\n",
      "Gradient: [  7.2833  10.0556 -50.3221 -58.601  405.2302]\n",
      "Weights: [-4.7896  0.6765 -1.1461  0.0811  0.1486]\n",
      "MSE loss: 92.1581\n",
      "Iteration: 62100\n",
      "Gradient: [  -0.4564    7.0601  -48.623   -58.5695 -230.71  ]\n",
      "Weights: [-4.7811  0.6695 -1.144   0.0807  0.1485]\n",
      "MSE loss: 92.1848\n",
      "Iteration: 62200\n",
      "Gradient: [  3.7568  13.4691 -17.1525  10.0689 -93.2955]\n",
      "Weights: [-4.7848  0.678  -1.144   0.0799  0.1486]\n",
      "MSE loss: 92.088\n",
      "Iteration: 62300\n",
      "Gradient: [ -5.726  -12.2964  16.1828  11.5294 151.422 ]\n",
      "Weights: [-4.809   0.688  -1.1454  0.0805  0.1485]\n",
      "MSE loss: 92.4876\n",
      "Iteration: 62400\n",
      "Gradient: [  -8.6512   -9.2151  -27.5473 -263.1336 -352.8347]\n",
      "Weights: [-4.7833  0.6803 -1.1492  0.0808  0.1488]\n",
      "MSE loss: 92.0719\n",
      "Iteration: 62500\n",
      "Gradient: [  0.4262   8.6488   4.9296 144.5755 105.6959]\n",
      "Weights: [-4.789   0.6808 -1.15    0.0812  0.1491]\n",
      "MSE loss: 92.1541\n",
      "Iteration: 62600\n",
      "Gradient: [-13.9915  12.3929   6.3832  -3.9413 171.4384]\n",
      "Weights: [-4.7938  0.6887 -1.1479  0.0803  0.1486]\n",
      "MSE loss: 92.1048\n",
      "Iteration: 62700\n",
      "Gradient: [  1.6846   2.8785  -0.6509 -52.6375 -86.1696]\n",
      "Weights: [-4.7972  0.6875 -1.1459  0.0793  0.1487]\n",
      "MSE loss: 92.2677\n",
      "Iteration: 62800\n",
      "Gradient: [  3.5915   1.1803  77.665   14.5078 152.9247]\n",
      "Weights: [-4.7828  0.6872 -1.1463  0.079   0.1486]\n",
      "MSE loss: 92.1879\n",
      "Iteration: 62900\n",
      "Gradient: [   1.903   -23.9954  -24.2065  -15.9742 -217.0897]\n",
      "Weights: [-4.7909  0.6941 -1.148   0.0792  0.1488]\n",
      "MSE loss: 92.131\n",
      "Iteration: 63000\n",
      "Gradient: [   1.4103    1.3774   60.3667   55.0217 -232.5554]\n",
      "Weights: [-4.7934  0.6911 -1.1491  0.0798  0.1487]\n",
      "MSE loss: 92.1702\n",
      "Iteration: 63100\n",
      "Gradient: [   3.221    15.3783  -33.8308  -89.9745 -241.1858]\n",
      "Weights: [-4.7899  0.6842 -1.1492  0.0805  0.1487]\n",
      "MSE loss: 92.1791\n",
      "Iteration: 63200\n",
      "Gradient: [  9.2807   4.5115   4.2503 -94.1083 466.2928]\n",
      "Weights: [-4.7736  0.6829 -1.1494  0.0807  0.1487]\n",
      "MSE loss: 92.1834\n",
      "Iteration: 63300\n",
      "Gradient: [  8.1436  14.0238 -21.573   94.1904  95.5144]\n",
      "Weights: [-4.7828  0.6825 -1.1502  0.081   0.1488]\n",
      "MSE loss: 92.0661\n",
      "Iteration: 63400\n",
      "Gradient: [-14.0775  12.2545 -26.228   58.7079 -16.4045]\n",
      "Weights: [-4.7842  0.6854 -1.1514  0.0809  0.1489]\n",
      "MSE loss: 92.0571\n",
      "Iteration: 63500\n",
      "Gradient: [ -7.5955   1.4713   4.5947 -76.2117 403.436 ]\n",
      "Weights: [-4.7828  0.6914 -1.1494  0.0798  0.1488]\n",
      "MSE loss: 92.1717\n",
      "Iteration: 63600\n",
      "Gradient: [  7.5697   8.6255  34.5489  -9.9166 503.3465]\n",
      "Weights: [-4.7816  0.684  -1.1483  0.0802  0.1487]\n",
      "MSE loss: 92.0851\n",
      "Iteration: 63700\n",
      "Gradient: [   3.7715    5.9341   21.5886  -22.6551 -210.8123]\n",
      "Weights: [-4.7913  0.6927 -1.1479  0.0809  0.1484]\n",
      "MSE loss: 92.2499\n",
      "Iteration: 63800\n",
      "Gradient: [  2.7897  -0.1933  -4.6951  90.4348 151.6281]\n",
      "Weights: [-4.8007  0.6843 -1.1438  0.0807  0.1483]\n",
      "MSE loss: 92.2336\n",
      "Iteration: 63900\n",
      "Gradient: [   5.7125   -3.9671    8.671     7.4156 -314.2345]\n",
      "Weights: [-4.778   0.6789 -1.1445  0.0813  0.1483]\n",
      "MSE loss: 92.2342\n",
      "Iteration: 64000\n",
      "Gradient: [   9.3213    5.0051   -9.0524  -40.2342 -621.6516]\n",
      "Weights: [-4.7936  0.6756 -1.1459  0.0816  0.1483]\n",
      "MSE loss: 92.3552\n",
      "Iteration: 64100\n",
      "Gradient: [   6.9845   -3.3456  -15.0687   96.4692 -145.0335]\n",
      "Weights: [-4.7741  0.6734 -1.1493  0.0823  0.1486]\n",
      "MSE loss: 92.0758\n",
      "Iteration: 64200\n",
      "Gradient: [  9.018    0.0891 -10.5264 -29.3821 -47.1945]\n",
      "Weights: [-4.7702  0.6771 -1.1486  0.0815  0.1486]\n",
      "MSE loss: 92.1994\n",
      "Iteration: 64300\n",
      "Gradient: [ 10.6506  11.8237  -2.5269   7.8288 -34.1272]\n",
      "Weights: [-4.77    0.6851 -1.1497  0.0813  0.1484]\n",
      "MSE loss: 92.4051\n",
      "Iteration: 64400\n",
      "Gradient: [  -1.3861   -3.8503   13.3977 -101.8496 -259.4187]\n",
      "Weights: [-4.7884  0.6889 -1.1496  0.0813  0.1484]\n",
      "MSE loss: 92.0664\n",
      "Iteration: 64500\n",
      "Gradient: [  6.4755  -6.5396  10.661  -40.1195 458.4521]\n",
      "Weights: [-4.7853  0.6952 -1.1514  0.0817  0.1486]\n",
      "MSE loss: 92.5748\n",
      "Iteration: 64600\n",
      "Gradient: [ -1.6017  -4.5236 -24.4973 -62.8663 111.6192]\n",
      "Weights: [-4.7993  0.6939 -1.1523  0.0821  0.1484]\n",
      "MSE loss: 92.1326\n",
      "Iteration: 64700\n",
      "Gradient: [  -3.6523    0.8568  -14.0361    6.325  -110.3342]\n",
      "Weights: [-4.7942  0.6909 -1.1552  0.0831  0.1483]\n",
      "MSE loss: 92.1656\n",
      "Iteration: 64800\n",
      "Gradient: [ 5.5964 11.5697 33.2452 42.629  61.251 ]\n",
      "Weights: [-4.7816  0.6902 -1.155   0.0831  0.1484]\n",
      "MSE loss: 92.0594\n",
      "Iteration: 64900\n",
      "Gradient: [ 10.6764  20.282   18.8506  39.0011 664.8257]\n",
      "Weights: [-4.7864  0.6968 -1.155   0.0836  0.1482]\n",
      "MSE loss: 92.31\n",
      "Iteration: 65000\n",
      "Gradient: [   8.1167    4.8044   -4.2052   -6.6446 -279.4191]\n",
      "Weights: [-4.7732  0.6881 -1.1586  0.0835  0.1483]\n",
      "MSE loss: 92.1927\n",
      "Iteration: 65100\n",
      "Gradient: [ -0.8418  15.2594 -15.2281 -55.4735  77.4158]\n",
      "Weights: [-4.8011  0.6997 -1.1586  0.0841  0.1485]\n",
      "MSE loss: 92.1754\n",
      "Iteration: 65200\n",
      "Gradient: [  6.412   31.7677 -12.6532   2.1681 319.0629]\n",
      "Weights: [-4.7845  0.7044 -1.1604  0.0837  0.1484]\n",
      "MSE loss: 92.3057\n",
      "Iteration: 65300\n",
      "Gradient: [   6.4515    4.3393  -39.759   -64.7558 -284.9752]\n",
      "Weights: [-4.7987  0.7034 -1.1589  0.0828  0.1484]\n",
      "MSE loss: 92.1342\n",
      "Iteration: 65400\n",
      "Gradient: [ -8.0872 -17.7784  -4.1528 -69.2894 -90.7109]\n",
      "Weights: [-4.7978  0.6992 -1.1581  0.0832  0.1483]\n",
      "MSE loss: 92.1441\n",
      "Iteration: 65500\n",
      "Gradient: [ -3.0624   3.0994  15.9982  55.7574 -17.5718]\n",
      "Weights: [-4.7797  0.6885 -1.1611  0.0842  0.1486]\n",
      "MSE loss: 92.0947\n",
      "Iteration: 65600\n",
      "Gradient: [ -7.783  -11.6524  -5.4467 -70.7486  -9.3168]\n",
      "Weights: [-4.7923  0.703  -1.1611  0.0832  0.1487]\n",
      "MSE loss: 92.0421\n",
      "Iteration: 65700\n",
      "Gradient: [   5.4667  -10.2703    2.7245  -29.9122 -148.0001]\n",
      "Weights: [-4.7965  0.7076 -1.1632  0.0832  0.1486]\n",
      "MSE loss: 92.0846\n",
      "Iteration: 65800\n",
      "Gradient: [   1.9108    3.6872   13.7947  -66.9855 -227.2521]\n",
      "Weights: [-4.7984  0.7008 -1.1637  0.0842  0.1487]\n",
      "MSE loss: 92.2304\n",
      "Iteration: 65900\n",
      "Gradient: [-13.811    1.7148 -28.0539  -6.0946 -63.8914]\n",
      "Weights: [-4.7938  0.6957 -1.164   0.0839  0.1489]\n",
      "MSE loss: 92.3458\n",
      "Iteration: 66000\n",
      "Gradient: [  4.8295 -21.8485  36.0782 -93.2721 241.1185]\n",
      "Weights: [-4.7944  0.6935 -1.1609  0.0841  0.1489]\n",
      "MSE loss: 92.1383\n",
      "Iteration: 66100\n",
      "Gradient: [ -9.6855   4.6696  -9.4161 111.6451 271.8104]\n",
      "Weights: [-4.7937  0.6978 -1.1624  0.0845  0.1486]\n",
      "MSE loss: 92.0582\n",
      "Iteration: 66200\n",
      "Gradient: [   1.0944   10.9028   -0.5474 -101.3113  102.6211]\n",
      "Weights: [-4.7898  0.6972 -1.162   0.0841  0.1486]\n",
      "MSE loss: 92.0411\n",
      "Iteration: 66300\n",
      "Gradient: [ -4.8816  21.2466   1.3529 -29.5324 353.6946]\n",
      "Weights: [-4.7926  0.709  -1.1627  0.0841  0.1484]\n",
      "MSE loss: 92.145\n",
      "Iteration: 66400\n",
      "Gradient: [   4.11     27.7977   29.056    76.9358 -131.1931]\n",
      "Weights: [-4.7955  0.7116 -1.1636  0.0842  0.1482]\n",
      "MSE loss: 92.0641\n",
      "Iteration: 66500\n",
      "Gradient: [ 10.0614  -0.6843  -5.7762  53.3423 221.1772]\n",
      "Weights: [-4.7795  0.7034 -1.1652  0.0853  0.1483]\n",
      "MSE loss: 92.1666\n",
      "Iteration: 66600\n",
      "Gradient: [-10.5349 -22.4123  -4.747  -62.5285  -9.7364]\n",
      "Weights: [-4.8036  0.7046 -1.1643  0.0854  0.1479]\n",
      "MSE loss: 92.5396\n",
      "Iteration: 66700\n",
      "Gradient: [11.9219 -6.2068 42.8107 50.9248 89.0453]\n",
      "Weights: [-4.7924  0.7103 -1.1656  0.0866  0.1478]\n",
      "MSE loss: 92.1389\n",
      "Iteration: 66800\n",
      "Gradient: [  -4.7396   -2.9375   -3.434    93.7023 -148.0147]\n",
      "Weights: [-4.8077  0.7153 -1.1675  0.0863  0.1479]\n",
      "MSE loss: 92.1144\n",
      "Iteration: 66900\n",
      "Gradient: [ 10.9679  -9.7824  24.2676   8.1018 -53.9749]\n",
      "Weights: [-4.7858  0.703  -1.1668  0.0861  0.148 ]\n",
      "MSE loss: 92.1049\n",
      "Iteration: 67000\n",
      "Gradient: [  -1.5792   -7.1497  -13.3995   72.3865 -109.488 ]\n",
      "Weights: [-4.8009  0.7144 -1.1663  0.0863  0.1479]\n",
      "MSE loss: 92.0536\n",
      "Iteration: 67100\n",
      "Gradient: [-18.0223   4.0824  22.053   -4.9888 202.2993]\n",
      "Weights: [-4.8044  0.7026 -1.1658  0.0866  0.1483]\n",
      "MSE loss: 92.229\n",
      "Iteration: 67200\n",
      "Gradient: [ -13.2635    3.6039  -11.0035  -22.8475 -269.038 ]\n",
      "Weights: [-4.7787  0.6949 -1.1637  0.0865  0.1481]\n",
      "MSE loss: 92.1156\n",
      "Iteration: 67300\n",
      "Gradient: [  -5.5302   21.8644   10.4576  130.1035 -145.8318]\n",
      "Weights: [-4.7689  0.6902 -1.1631  0.0878  0.148 ]\n",
      "MSE loss: 92.7555\n",
      "Iteration: 67400\n",
      "Gradient: [ 10.2     -9.8892  12.8198 -72.054  318.3774]\n",
      "Weights: [-4.7621  0.6846 -1.165   0.0884  0.148 ]\n",
      "MSE loss: 92.5385\n",
      "Iteration: 67500\n",
      "Gradient: [  -1.8075   -3.8103   26.3229   38.1474 -156.4243]\n",
      "Weights: [-4.7932  0.6877 -1.1638  0.0886  0.1478]\n",
      "MSE loss: 92.1593\n",
      "Iteration: 67600\n",
      "Gradient: [  -5.4128    4.265    48.0712 -155.1591   41.4893]\n",
      "Weights: [-4.7714  0.6866 -1.1646  0.0885  0.1479]\n",
      "MSE loss: 92.1348\n",
      "Iteration: 67700\n",
      "Gradient: [  -4.1936   -0.9797  -32.7559  -15.5402 -256.6008]\n",
      "Weights: [-4.7682  0.6891 -1.1669  0.0899  0.1475]\n",
      "MSE loss: 92.2558\n",
      "Iteration: 67800\n",
      "Gradient: [ -3.2036 -13.354  -24.0691  66.876  249.3592]\n",
      "Weights: [-4.7782  0.6871 -1.1665  0.0899  0.1475]\n",
      "MSE loss: 91.9359\n",
      "Iteration: 67900\n",
      "Gradient: [  1.6336  16.7426   1.2153  54.1869 255.9514]\n",
      "Weights: [-4.7683  0.6808 -1.1661  0.0903  0.1475]\n",
      "MSE loss: 92.0218\n",
      "Iteration: 68000\n",
      "Gradient: [ -8.4957  -2.7211 -21.8716   3.9302 -19.8052]\n",
      "Weights: [-4.7712  0.6828 -1.167   0.0903  0.1477]\n",
      "MSE loss: 92.0646\n",
      "Iteration: 68100\n",
      "Gradient: [   0.7716  -12.9038    0.7131  -71.1893 -116.2137]\n",
      "Weights: [-4.769   0.6772 -1.1671  0.0911  0.1475]\n",
      "MSE loss: 91.9969\n",
      "Iteration: 68200\n",
      "Gradient: [  7.271   -9.7439  -4.7778 -19.9812 169.9387]\n",
      "Weights: [-4.7676  0.6713 -1.1669  0.0918  0.1474]\n",
      "MSE loss: 92.0388\n",
      "Iteration: 68300\n",
      "Gradient: [ -5.6871   4.3088  29.7375 -25.7902  27.0827]\n",
      "Weights: [-4.7708  0.6716 -1.1645  0.0914  0.1473]\n",
      "MSE loss: 92.0019\n",
      "Iteration: 68400\n",
      "Gradient: [  4.1596   2.8259 -61.6904 -79.9788 -66.8333]\n",
      "Weights: [-4.7695  0.6758 -1.1639  0.0917  0.147 ]\n",
      "MSE loss: 91.9849\n",
      "Iteration: 68500\n",
      "Gradient: [  5.5606 -11.1237   0.214   51.5228 112.6802]\n",
      "Weights: [-4.7842  0.6728 -1.1614  0.0916  0.147 ]\n",
      "MSE loss: 92.1072\n",
      "Iteration: 68600\n",
      "Gradient: [ 12.5602   1.9688 -24.337  -17.9892 110.138 ]\n",
      "Weights: [-4.741   0.6603 -1.1608  0.0911  0.1471]\n",
      "MSE loss: 92.6129\n",
      "Iteration: 68700\n",
      "Gradient: [-5.814500e+00 -2.677180e+01 -2.223000e-01 -2.106230e+01  2.507712e+02]\n",
      "Weights: [-4.7755  0.6707 -1.161   0.0907  0.147 ]\n",
      "MSE loss: 92.0709\n",
      "Iteration: 68800\n",
      "Gradient: [  -8.1594  -32.4438   -6.1316   25.568  -173.7217]\n",
      "Weights: [-4.7782  0.6823 -1.1629  0.0908  0.1468]\n",
      "MSE loss: 91.9112\n",
      "Iteration: 68900\n",
      "Gradient: [ -3.0332 -11.3812 -11.1376  78.4664 103.7418]\n",
      "Weights: [-4.7897  0.675  -1.1595  0.0903  0.1469]\n",
      "MSE loss: 92.3707\n",
      "Iteration: 69000\n",
      "Gradient: [ -3.176   26.5492  24.364    8.0517 148.5231]\n",
      "Weights: [-4.7796  0.676  -1.158   0.0904  0.1468]\n",
      "MSE loss: 91.928\n",
      "Iteration: 69100\n",
      "Gradient: [ -4.6054  -2.2081 -24.1768  30.5232  43.9853]\n",
      "Weights: [-4.7884  0.6727 -1.1582  0.0903  0.1469]\n",
      "MSE loss: 92.2837\n",
      "Iteration: 69200\n",
      "Gradient: [   8.9171   27.3134   13.0865   63.1662 -105.1751]\n",
      "Weights: [-4.765   0.6704 -1.1575  0.09    0.1471]\n",
      "MSE loss: 92.1708\n",
      "Iteration: 69300\n",
      "Gradient: [  13.5735   -4.6196  -29.4105   23.093  -206.6146]\n",
      "Weights: [-4.767   0.6681 -1.1614  0.0914  0.147 ]\n",
      "MSE loss: 91.9742\n",
      "Iteration: 69400\n",
      "Gradient: [  15.6124   -0.9474   26.965    74.8079 -201.8725]\n",
      "Weights: [-4.7683  0.6715 -1.1583  0.0909  0.1466]\n",
      "MSE loss: 91.9679\n",
      "Iteration: 69500\n",
      "Gradient: [  8.3168  -3.9786  28.6279  24.6435 126.1409]\n",
      "Weights: [-4.7783  0.68   -1.1583  0.0903  0.1466]\n",
      "MSE loss: 91.9386\n",
      "Iteration: 69600\n",
      "Gradient: [ 0.6773 -3.0767 -7.7074 -5.7395 47.3749]\n",
      "Weights: [-4.7846  0.6878 -1.1601  0.0898  0.1466]\n",
      "MSE loss: 91.9505\n",
      "Iteration: 69700\n",
      "Gradient: [ -18.1544  -15.7408   -3.1281 -175.4946 -285.5458]\n",
      "Weights: [-4.7851  0.6853 -1.1624  0.091   0.1466]\n",
      "MSE loss: 91.9464\n",
      "Iteration: 69800\n",
      "Gradient: [ -4.331  -14.5089  11.659   21.7734 -61.692 ]\n",
      "Weights: [-4.7685  0.6831 -1.1658  0.092   0.1466]\n",
      "MSE loss: 91.9996\n",
      "Iteration: 69900\n",
      "Gradient: [  5.9198  -2.6957  10.4605  -4.5734 173.0828]\n",
      "Weights: [-4.769   0.6861 -1.1665  0.0915  0.1467]\n",
      "MSE loss: 92.028\n",
      "Iteration: 70000\n",
      "Gradient: [  0.6824   7.9093  60.6765  33.917  -35.9998]\n",
      "Weights: [-4.7827  0.6974 -1.1674  0.0916  0.1466]\n",
      "MSE loss: 91.9637\n",
      "Iteration: 70100\n",
      "Gradient: [  -2.8625  -13.9059   17.6032  -67.7649 -334.6464]\n",
      "Weights: [-4.7751  0.6899 -1.1671  0.0921  0.1464]\n",
      "MSE loss: 91.9856\n",
      "Iteration: 70200\n",
      "Gradient: [  -4.7228    1.081     4.4924   14.9163 -102.9566]\n",
      "Weights: [-4.7821  0.6893 -1.1661  0.0922  0.1466]\n",
      "MSE loss: 91.9072\n",
      "Iteration: 70300\n",
      "Gradient: [-12.2007   9.6205  -9.5972 -16.072   70.9977]\n",
      "Weights: [-4.7614  0.6719 -1.1637  0.0925  0.1466]\n",
      "MSE loss: 92.0219\n",
      "Iteration: 70400\n",
      "Gradient: [ -3.6995  12.6151  -2.1681 -19.0396  11.6279]\n",
      "Weights: [-4.7787  0.6766 -1.1626  0.0929  0.1466]\n",
      "MSE loss: 91.9378\n",
      "Iteration: 70500\n",
      "Gradient: [   1.7237  -18.6222   21.6179 -124.8056  -46.9881]\n",
      "Weights: [-4.7534  0.6756 -1.1649  0.0919  0.1466]\n",
      "MSE loss: 92.3356\n",
      "Iteration: 70600\n",
      "Gradient: [ -8.9063  -7.3395  43.2736 120.5919 -89.5148]\n",
      "Weights: [-4.7773  0.6787 -1.1644  0.093   0.1467]\n",
      "MSE loss: 91.9976\n",
      "Iteration: 70700\n",
      "Gradient: [ -4.5037  13.2771   4.7589  61.0913 -40.1294]\n",
      "Weights: [-4.7749  0.6727 -1.1646  0.0931  0.1467]\n",
      "MSE loss: 91.949\n",
      "Iteration: 70800\n",
      "Gradient: [  3.7507   3.7823 -31.9991 -72.9225 -36.9955]\n",
      "Weights: [-4.7703  0.6759 -1.165   0.0932  0.1464]\n",
      "MSE loss: 91.9097\n",
      "Iteration: 70900\n",
      "Gradient: [ -2.3891   1.2193  34.9257  31.0608 -32.4846]\n",
      "Weights: [-4.7806  0.685  -1.167   0.094   0.1462]\n",
      "MSE loss: 91.8577\n",
      "Iteration: 71000\n",
      "Gradient: [  -5.1611   -7.7526   24.1019 -124.5606   11.4227]\n",
      "Weights: [-4.7856  0.684  -1.1701  0.0945  0.1462]\n",
      "MSE loss: 92.2339\n",
      "Iteration: 71100\n",
      "Gradient: [  -8.1314  -10.6151  -13.3695  -28.06   -119.4753]\n",
      "Weights: [-4.7846  0.6841 -1.1724  0.0948  0.1463]\n",
      "MSE loss: 92.3755\n",
      "Iteration: 71200\n",
      "Gradient: [ -1.7089  26.1392  -7.5164 177.2918  89.5555]\n",
      "Weights: [-4.7684  0.6897 -1.1705  0.0945  0.1463]\n",
      "MSE loss: 92.2242\n",
      "Iteration: 71300\n",
      "Gradient: [  -6.8609    8.0351    7.8095   67.04   -167.4821]\n",
      "Weights: [-4.7894  0.6934 -1.1684  0.0939  0.1462]\n",
      "MSE loss: 91.8814\n",
      "Iteration: 71400\n",
      "Gradient: [  -2.0208   -9.4588  -13.2946   49.9402 -210.6421]\n",
      "Weights: [-4.7603  0.6827 -1.1694  0.0942  0.1462]\n",
      "MSE loss: 92.1802\n",
      "Iteration: 71500\n",
      "Gradient: [ -1.3223  -4.6108 -18.1567   3.8206 364.9462]\n",
      "Weights: [-4.7672  0.6637 -1.1682  0.0952  0.1465]\n",
      "MSE loss: 92.2412\n",
      "Iteration: 71600\n",
      "Gradient: [   6.9285   -3.7506   22.149   -16.6696 -143.9403]\n",
      "Weights: [-4.7779  0.6898 -1.1713  0.0947  0.1463]\n",
      "MSE loss: 91.8936\n",
      "Iteration: 71700\n",
      "Gradient: [ -14.1613    9.9197   54.2982  -16.0836 -116.4611]\n",
      "Weights: [-4.8081  0.7019 -1.1702  0.0937  0.1463]\n",
      "MSE loss: 92.2207\n",
      "Iteration: 71800\n",
      "Gradient: [  -2.5934    4.742    38.0661 -120.0558  147.6241]\n",
      "Weights: [-4.7968  0.7032 -1.1739  0.0938  0.1464]\n",
      "MSE loss: 91.9586\n",
      "Iteration: 71900\n",
      "Gradient: [ -5.1435  -3.1863  10.3751 -14.9948 156.688 ]\n",
      "Weights: [-4.791   0.6894 -1.174   0.0953  0.1463]\n",
      "MSE loss: 92.3091\n",
      "Iteration: 72000\n",
      "Gradient: [-10.1554  -7.5005  14.628   25.6995  75.4589]\n",
      "Weights: [-4.7934  0.6968 -1.1741  0.0949  0.1463]\n",
      "MSE loss: 91.9506\n",
      "Iteration: 72100\n",
      "Gradient: [ 1.57900e-01  2.47458e+01  1.12517e+01 -3.90799e+01 -1.61913e+02]\n",
      "Weights: [-4.7866  0.6994 -1.1733  0.0946  0.1462]\n",
      "MSE loss: 91.8471\n",
      "Iteration: 72200\n",
      "Gradient: [-8.300000e-02  1.482530e+01 -1.187330e+01  1.612828e+02 -2.612036e+02]\n",
      "Weights: [-4.7837  0.6975 -1.1737  0.0949  0.1462]\n",
      "MSE loss: 91.844\n",
      "Iteration: 72300\n",
      "Gradient: [  -4.3733   -1.0142  -58.1626 -100.6485 -242.4488]\n",
      "Weights: [-4.7861  0.6961 -1.1727  0.0944  0.1463]\n",
      "MSE loss: 91.848\n",
      "Iteration: 72400\n",
      "Gradient: [ -8.9652 -17.4438  57.0857  81.7919 217.7692]\n",
      "Weights: [-4.8091  0.7071 -1.1732  0.0939  0.1462]\n",
      "MSE loss: 92.316\n",
      "Iteration: 72500\n",
      "Gradient: [  2.3503  -2.0284  18.6265  -6.5407 267.911 ]\n",
      "Weights: [-4.7878  0.7154 -1.174   0.0935  0.146 ]\n",
      "MSE loss: 92.1871\n",
      "Iteration: 72600\n",
      "Gradient: [  15.3957   -0.7443  -24.9218   18.3403 -190.013 ]\n",
      "Weights: [-4.8037  0.7139 -1.1738  0.0936  0.1463]\n",
      "MSE loss: 91.9633\n",
      "Iteration: 72700\n",
      "Gradient: [  0.5198  10.368  -75.4047 -94.6274 -61.9717]\n",
      "Weights: [-4.7897  0.7081 -1.1739  0.0937  0.1462]\n",
      "MSE loss: 91.9099\n",
      "Iteration: 72800\n",
      "Gradient: [  7.9515 -15.717    4.1334 142.104  -69.6528]\n",
      "Weights: [-4.797   0.7073 -1.1728  0.0936  0.1461]\n",
      "MSE loss: 91.9353\n",
      "Iteration: 72900\n",
      "Gradient: [  -2.9229   -0.7504   11.0286  -61.7707 -150.1698]\n",
      "Weights: [-4.7961  0.7149 -1.1748  0.0933  0.1461]\n",
      "MSE loss: 91.9539\n",
      "Iteration: 73000\n",
      "Gradient: [  1.3667  -1.4344   6.0572 -36.3656 -32.3715]\n",
      "Weights: [-4.7915  0.708  -1.1743  0.0936  0.1464]\n",
      "MSE loss: 91.8978\n",
      "Iteration: 73100\n",
      "Gradient: [ -7.5103   1.7579  15.1031  63.3579 184.5553]\n",
      "Weights: [-4.795   0.702  -1.1701  0.093   0.1463]\n",
      "MSE loss: 91.9062\n",
      "Iteration: 73200\n",
      "Gradient: [-1.423000e-01  4.395600e+00  6.399500e+00 -1.077754e+02 -2.049137e+02]\n",
      "Weights: [-4.8082  0.7007 -1.1702  0.0928  0.1466]\n",
      "MSE loss: 92.3723\n",
      "Iteration: 73300\n",
      "Gradient: [-12.0542  -4.7068  14.4898  -5.887  102.595 ]\n",
      "Weights: [-4.7983  0.695  -1.1707  0.0926  0.1466]\n",
      "MSE loss: 92.4308\n",
      "Iteration: 73400\n",
      "Gradient: [ -5.3328  14.6733  43.4381 -45.9604 -23.783 ]\n",
      "Weights: [-4.7776  0.6978 -1.1696  0.0919  0.1467]\n",
      "MSE loss: 92.0328\n",
      "Iteration: 73500\n",
      "Gradient: [ 1.878500e+00 -2.883000e-01  5.760460e+01 -5.172210e+01 -3.522281e+02]\n",
      "Weights: [-4.7847  0.6955 -1.1667  0.092   0.1464]\n",
      "MSE loss: 91.9084\n",
      "Iteration: 73600\n",
      "Gradient: [ -19.9069   -8.8621    2.2969 -145.8966 -235.3673]\n",
      "Weights: [-4.7943  0.6861 -1.1655  0.0916  0.1466]\n",
      "MSE loss: 92.5725\n",
      "Iteration: 73700\n",
      "Gradient: [ -2.0986  -8.2308  15.1665  12.602  -53.0279]\n",
      "Weights: [-4.7964  0.6848 -1.1636  0.091   0.1468]\n",
      "MSE loss: 92.411\n",
      "Iteration: 73800\n",
      "Gradient: [  11.7331    9.4515   36.7801  -36.2096 -240.8669]\n",
      "Weights: [-4.7867  0.694  -1.1646  0.091   0.1468]\n",
      "MSE loss: 91.9821\n",
      "Iteration: 73900\n",
      "Gradient: [  1.2652  -7.9269  55.2881 114.0713  92.0389]\n",
      "Weights: [-4.794   0.6909 -1.1652  0.0919  0.1467]\n",
      "MSE loss: 91.9786\n",
      "Iteration: 74000\n",
      "Gradient: [   5.7193   -6.0101  -53.1353 -156.9042 -150.0907]\n",
      "Weights: [-4.7738  0.6812 -1.1678  0.0924  0.1468]\n",
      "MSE loss: 91.9542\n",
      "Iteration: 74100\n",
      "Gradient: [ 17.7314 -11.8634 -16.1057 105.7463 165.885 ]\n",
      "Weights: [-4.783   0.697  -1.1685  0.0926  0.1468]\n",
      "MSE loss: 92.3243\n",
      "Iteration: 74200\n",
      "Gradient: [  0.587   -0.8755 -35.942  -46.887  -20.5422]\n",
      "Weights: [-4.8024  0.7009 -1.1698  0.0931  0.1465]\n",
      "MSE loss: 92.041\n",
      "Iteration: 74300\n",
      "Gradient: [  8.7935   0.7231 -66.0189 121.1876   2.7679]\n",
      "Weights: [-4.7867  0.6931 -1.1727  0.0933  0.1466]\n",
      "MSE loss: 92.1028\n",
      "Iteration: 74400\n",
      "Gradient: [  7.975    5.086  -10.5343  72.4496 169.1208]\n",
      "Weights: [-4.7753  0.6983 -1.1747  0.0937  0.1468]\n",
      "MSE loss: 92.0753\n",
      "Iteration: 74500\n",
      "Gradient: [ -0.4035   3.5888  -0.3091 -10.0399 111.4754]\n",
      "Weights: [-4.7797  0.6863 -1.1721  0.0942  0.1469]\n",
      "MSE loss: 91.9258\n",
      "Iteration: 74600\n",
      "Gradient: [   2.4932    1.9551   35.8494 -104.886   295.5231]\n",
      "Weights: [-4.7744  0.6975 -1.1759  0.0938  0.147 ]\n",
      "MSE loss: 92.0633\n",
      "Iteration: 74700\n",
      "Gradient: [  2.0622  -4.8966 -11.5047 -15.6652  43.0036]\n",
      "Weights: [-4.8039  0.7024 -1.1748  0.0939  0.1469]\n",
      "MSE loss: 92.165\n",
      "Iteration: 74800\n",
      "Gradient: [   8.7449  -13.3094    7.7479  -72.4055 -176.7482]\n",
      "Weights: [-4.7681  0.6972 -1.176   0.0939  0.1468]\n",
      "MSE loss: 92.1716\n",
      "Iteration: 74900\n",
      "Gradient: [ -4.6004  -6.3972  -9.7795  19.8763 526.1533]\n",
      "Weights: [-4.8064  0.7174 -1.1802  0.0942  0.1467]\n",
      "MSE loss: 91.9949\n",
      "Iteration: 75000\n",
      "Gradient: [ -2.4704  21.4967  27.4103  38.8627 -71.2955]\n",
      "Weights: [-4.7887  0.7146 -1.1828  0.0959  0.1466]\n",
      "MSE loss: 92.0138\n",
      "Iteration: 75100\n",
      "Gradient: [   2.9717  -14.133   -12.1197  -33.4284 -253.9584]\n",
      "Weights: [-4.7701  0.7007 -1.1812  0.0958  0.1466]\n",
      "MSE loss: 92.074\n",
      "Iteration: 75200\n",
      "Gradient: [ 15.2906   2.0751 -11.7969 -61.3577 120.6735]\n",
      "Weights: [-4.7654  0.7114 -1.1829  0.0963  0.1462]\n",
      "MSE loss: 92.7821\n",
      "Iteration: 75300\n",
      "Gradient: [   9.3438    8.53      3.0028   29.4924 -116.146 ]\n",
      "Weights: [-4.7813  0.7058 -1.181   0.0966  0.1461]\n",
      "MSE loss: 91.8645\n",
      "Iteration: 75400\n",
      "Gradient: [  2.2012   1.6554  61.9619  54.1322 151.8449]\n",
      "Weights: [-4.7813  0.7038 -1.1807  0.0965  0.1462]\n",
      "MSE loss: 91.8354\n",
      "Iteration: 75500\n",
      "Gradient: [ -6.0443  -2.9552 -10.7511 -79.699    3.4506]\n",
      "Weights: [-4.7813  0.7024 -1.1823  0.0972  0.1462]\n",
      "MSE loss: 91.8106\n",
      "Iteration: 75600\n",
      "Gradient: [  6.3193  -7.5962  16.8851  -3.0587 451.287 ]\n",
      "Weights: [-4.7737  0.6993 -1.1802  0.0976  0.1461]\n",
      "MSE loss: 92.052\n",
      "Iteration: 75700\n",
      "Gradient: [ -1.7433  13.1705   5.9106  70.0484 162.6929]\n",
      "Weights: [-4.789   0.702  -1.1812  0.0985  0.1459]\n",
      "MSE loss: 91.8072\n",
      "Iteration: 75800\n",
      "Gradient: [  4.2085   9.3016  97.8001 -80.3707  67.7947]\n",
      "Weights: [-4.7743  0.7072 -1.1847  0.0984  0.1459]\n",
      "MSE loss: 92.0356\n",
      "Iteration: 75900\n",
      "Gradient: [  0.5002  -2.8037  -2.7389 -51.3341 -10.9826]\n",
      "Weights: [-4.7817  0.6974 -1.1843  0.0992  0.146 ]\n",
      "MSE loss: 91.8076\n",
      "Iteration: 76000\n",
      "Gradient: [  -2.5651  -15.9695  -33.3428   91.8382 -398.422 ]\n",
      "Weights: [-4.7966  0.7067 -1.1814  0.0985  0.1458]\n",
      "MSE loss: 91.8569\n",
      "Iteration: 76100\n",
      "Gradient: [  11.9567   -1.5805   51.4686 -121.1332  -96.1646]\n",
      "Weights: [-4.7866  0.6949 -1.1789  0.0987  0.1457]\n",
      "MSE loss: 91.8165\n",
      "Iteration: 76200\n",
      "Gradient: [ 12.2133   9.8742  14.8554  25.3609 140.8098]\n",
      "Weights: [-4.7622  0.6855 -1.1774  0.0989  0.1457]\n",
      "MSE loss: 92.129\n",
      "Iteration: 76300\n",
      "Gradient: [  4.8206  -3.8507  29.9876 138.1851  91.5413]\n",
      "Weights: [-4.7695  0.696  -1.1802  0.0999  0.1454]\n",
      "MSE loss: 92.2865\n",
      "Iteration: 76400\n",
      "Gradient: [ -10.657     7.9574    3.4288   61.4787 -179.9924]\n",
      "Weights: [-4.775   0.6911 -1.1814  0.1005  0.1453]\n",
      "MSE loss: 91.7788\n",
      "Iteration: 76500\n",
      "Gradient: [-9.3047  9.3105 43.7673 91.2926 30.2059]\n",
      "Weights: [-4.7867  0.7043 -1.1818  0.1002  0.1453]\n",
      "MSE loss: 91.9236\n",
      "Iteration: 76600\n",
      "Gradient: [-17.7839   4.9906 -41.5363 -33.0338 118.5236]\n",
      "Weights: [-4.7831  0.6932 -1.1833  0.0999  0.1458]\n",
      "MSE loss: 91.8548\n",
      "Iteration: 76700\n",
      "Gradient: [  2.5586 -13.0293 -37.6151   6.2987 265.1714]\n",
      "Weights: [-4.7649  0.6889 -1.1826  0.1001  0.1457]\n",
      "MSE loss: 91.9245\n",
      "Iteration: 76800\n",
      "Gradient: [ 14.5574 -14.4936  52.8563  67.0071 120.9051]\n",
      "Weights: [-4.7881  0.6963 -1.1801  0.0998  0.1455]\n",
      "MSE loss: 91.8119\n",
      "Iteration: 76900\n",
      "Gradient: [  0.3458   8.9865  93.1315  47.4812 263.8048]\n",
      "Weights: [-4.7701  0.7009 -1.1847  0.0995  0.1455]\n",
      "MSE loss: 91.9466\n",
      "Iteration: 77000\n",
      "Gradient: [  0.604    7.9014   2.8694 -63.8235  33.3916]\n",
      "Weights: [-4.7906  0.7047 -1.1855  0.1     0.1456]\n",
      "MSE loss: 91.7977\n",
      "Iteration: 77100\n",
      "Gradient: [-11.6345  -9.024  -16.6214   9.9317  24.3296]\n",
      "Weights: [-4.7891  0.6962 -1.185   0.1002  0.1457]\n",
      "MSE loss: 92.1115\n",
      "Iteration: 77200\n",
      "Gradient: [-12.1566 -17.0495 -29.8724  55.1512   1.7918]\n",
      "Weights: [-4.7841  0.7049 -1.1818  0.099   0.1456]\n",
      "MSE loss: 91.9038\n",
      "Iteration: 77300\n",
      "Gradient: [  -0.6282    5.2027  -14.3605   71.6201 -178.1616]\n",
      "Weights: [-4.7957  0.7012 -1.1822  0.0978  0.146 ]\n",
      "MSE loss: 92.1425\n",
      "Iteration: 77400\n",
      "Gradient: [ -9.3112 -11.255   30.9451  37.3972 394.1146]\n",
      "Weights: [-4.799   0.7125 -1.1832  0.0978  0.1459]\n",
      "MSE loss: 91.853\n",
      "Iteration: 77500\n",
      "Gradient: [-8.2087 17.3773 28.0012 -8.0967  5.114 ]\n",
      "Weights: [-4.7603  0.7035 -1.1885  0.099   0.1461]\n",
      "MSE loss: 92.3387\n",
      "Iteration: 77600\n",
      "Gradient: [ 1.14200e-01  2.26297e+01 -2.29151e+01 -2.09082e+01  1.88242e+02]\n",
      "Weights: [-4.7841  0.7187 -1.1942  0.0998  0.1459]\n",
      "MSE loss: 91.8732\n",
      "Iteration: 77700\n",
      "Gradient: [-10.6528  -0.9924  38.8043 -58.6449  43.7746]\n",
      "Weights: [-4.8066  0.7396 -1.1975  0.0993  0.146 ]\n",
      "MSE loss: 91.8393\n",
      "Iteration: 77800\n",
      "Gradient: [-12.6407  -2.3414 -15.1372 -50.1029  48.2544]\n",
      "Weights: [-4.8105  0.7356 -1.1985  0.0996  0.1464]\n",
      "MSE loss: 91.9292\n",
      "Iteration: 77900\n",
      "Gradient: [  9.2177  24.5956  18.1806 -28.5274 197.7661]\n",
      "Weights: [-4.7944  0.733  -1.1978  0.0996  0.1461]\n",
      "MSE loss: 91.7949\n",
      "Iteration: 78000\n",
      "Gradient: [ -2.204   -6.5462 -20.3795   7.213   49.1654]\n",
      "Weights: [-4.7866  0.7254 -1.1985  0.0997  0.1463]\n",
      "MSE loss: 91.8339\n",
      "Iteration: 78100\n",
      "Gradient: [ -6.1604  13.9848  37.6625  27.5956 117.8241]\n",
      "Weights: [-4.8015  0.7296 -1.1984  0.1002  0.1463]\n",
      "MSE loss: 91.8143\n",
      "Iteration: 78200\n",
      "Gradient: [ 4.8554  4.7121 53.9691 74.5904 87.6334]\n",
      "Weights: [-4.7649  0.7158 -1.2003  0.1003  0.1466]\n",
      "MSE loss: 92.1452\n",
      "Iteration: 78300\n",
      "Gradient: [-2.575000e-01  7.175200e+00 -1.942550e+01  5.898600e+01  3.642874e+02]\n",
      "Weights: [-4.7851  0.7104 -1.1971  0.1011  0.1465]\n",
      "MSE loss: 91.857\n",
      "Iteration: 78400\n",
      "Gradient: [  -0.8787  -15.579   -57.1488 -171.1231  230.8142]\n",
      "Weights: [-4.7945  0.7095 -1.1958  0.1014  0.1465]\n",
      "MSE loss: 92.0442\n",
      "Iteration: 78500\n",
      "Gradient: [ -5.2367  -1.5169  47.2783  68.4996 -39.4733]\n",
      "Weights: [-4.787   0.7025 -1.1938  0.1014  0.1464]\n",
      "MSE loss: 91.9819\n",
      "Iteration: 78600\n",
      "Gradient: [ -2.4351   5.1001 -13.6677  53.7572 -41.4894]\n",
      "Weights: [-4.794   0.7112 -1.1941  0.1016  0.146 ]\n",
      "MSE loss: 91.8853\n",
      "Iteration: 78700\n",
      "Gradient: [  1.2956  19.25   -24.4341   2.0587 176.0805]\n",
      "Weights: [-4.7985  0.7153 -1.1966  0.1021  0.1461]\n",
      "MSE loss: 91.9528\n",
      "Iteration: 78800\n",
      "Gradient: [  3.438    0.5955  34.4714  83.0814 156.3163]\n",
      "Weights: [-4.7868  0.7139 -1.1951  0.1021  0.146 ]\n",
      "MSE loss: 91.8765\n",
      "Iteration: 78900\n",
      "Gradient: [ -8.0583  -2.211  -10.4675 -91.1994 -87.4782]\n",
      "Weights: [-4.7815  0.7002 -1.1954  0.1022  0.1461]\n",
      "MSE loss: 92.0361\n",
      "Iteration: 79000\n",
      "Gradient: [ 7.1633 10.8523 39.0677 98.7084 49.0901]\n",
      "Weights: [-4.7874  0.7056 -1.1932  0.1023  0.1459]\n",
      "MSE loss: 91.8361\n",
      "Iteration: 79100\n",
      "Gradient: [ -0.9158 -23.7108 -19.7218  -9.5184  -5.7349]\n",
      "Weights: [-4.7663  0.7039 -1.1935  0.1012  0.146 ]\n",
      "MSE loss: 91.967\n",
      "Iteration: 79200\n",
      "Gradient: [ 10.8455   5.2631  10.1198 -67.3641  78.3302]\n",
      "Weights: [-4.7767  0.7058 -1.1948  0.1008  0.1462]\n",
      "MSE loss: 91.8588\n",
      "Iteration: 79300\n",
      "Gradient: [ 13.6214  -4.5531  -9.429   33.29   130.6359]\n",
      "Weights: [-4.7756  0.7026 -1.1906  0.1006  0.1463]\n",
      "MSE loss: 91.9534\n",
      "Iteration: 79400\n",
      "Gradient: [   6.3625   -3.3725  -16.4709  -53.5545 -265.7939]\n",
      "Weights: [-4.7918  0.7018 -1.19    0.1002  0.1462]\n",
      "MSE loss: 92.1169\n",
      "Iteration: 79500\n",
      "Gradient: [  2.305   -0.121  -11.1428  47.6662  65.1216]\n",
      "Weights: [-4.7902  0.6988 -1.1883  0.1009  0.1462]\n",
      "MSE loss: 92.0092\n",
      "Iteration: 79600\n",
      "Gradient: [  4.2771   5.7987   5.9247 -16.9905 -10.6901]\n",
      "Weights: [-4.8005  0.7173 -1.1866  0.0997  0.1458]\n",
      "MSE loss: 91.9543\n",
      "Iteration: 79700\n",
      "Gradient: [ -2.6364  10.2221 -26.766  -75.3336 214.0422]\n",
      "Weights: [-4.7968  0.7154 -1.1907  0.0999  0.1458]\n",
      "MSE loss: 91.942\n",
      "Iteration: 79800\n",
      "Gradient: [ -2.6051   4.953  -42.5813  13.1747 -21.9919]\n",
      "Weights: [-4.803   0.7217 -1.1877  0.0986  0.1456]\n",
      "MSE loss: 91.952\n",
      "Iteration: 79900\n",
      "Gradient: [ -3.9845  -8.7639 -13.2804 -28.1879  16.1719]\n",
      "Weights: [-4.7959  0.7242 -1.1869  0.0979  0.1455]\n",
      "MSE loss: 91.9254\n",
      "Iteration: 80000\n",
      "Gradient: [  4.0616  -6.5054 -14.4568   7.9018 128.3073]\n",
      "Weights: [-4.7925  0.7229 -1.1872  0.0983  0.1457]\n",
      "MSE loss: 91.8748\n",
      "Iteration: 80100\n",
      "Gradient: [-12.9672  -5.3963  -4.3343 -75.7887  64.2656]\n",
      "Weights: [-4.8116  0.7234 -1.1877  0.0988  0.1457]\n",
      "MSE loss: 92.0579\n",
      "Iteration: 80200\n",
      "Gradient: [  -7.514    -6.7943  -90.7449    2.8477 -159.6373]\n",
      "Weights: [-4.7942  0.7116 -1.1885  0.0997  0.1458]\n",
      "MSE loss: 91.8319\n",
      "Iteration: 80300\n",
      "Gradient: [ -2.4375 -11.9292  -4.1383 -99.1421 249.2137]\n",
      "Weights: [-4.7724  0.693  -1.1867  0.1001  0.1458]\n",
      "MSE loss: 91.9325\n",
      "Iteration: 80400\n",
      "Gradient: [  -0.8686   -2.3259  -37.6664   66.4922 -198.7195]\n",
      "Weights: [-4.7958  0.7065 -1.1871  0.101   0.1454]\n",
      "MSE loss: 91.8767\n",
      "Iteration: 80500\n",
      "Gradient: [ 11.4716  -1.9121 -47.9609  96.4224  63.2508]\n",
      "Weights: [-4.7676  0.6996 -1.1851  0.1007  0.1454]\n",
      "MSE loss: 92.1069\n",
      "Iteration: 80600\n",
      "Gradient: [  1.9895  17.1665  -6.9362 -74.5871  58.7594]\n",
      "Weights: [-4.7787  0.6945 -1.1827  0.1004  0.1454]\n",
      "MSE loss: 91.7668\n",
      "Iteration: 80700\n",
      "Gradient: [-1.583000e-01 -8.894000e-01  3.152030e+01 -1.824180e+01  2.803014e+02]\n",
      "Weights: [-4.7832  0.7018 -1.1845  0.1014  0.1451]\n",
      "MSE loss: 91.7554\n",
      "Iteration: 80800\n",
      "Gradient: [ -1.5759 -13.5438   1.7401  -9.5854 150.286 ]\n",
      "Weights: [-4.7807  0.7074 -1.1885  0.1018  0.1451]\n",
      "MSE loss: 91.7769\n",
      "Iteration: 80900\n",
      "Gradient: [  -0.872   -11.3539  -25.2891 -136.995  -318.0412]\n",
      "Weights: [-4.7967  0.7178 -1.1914  0.1027  0.1449]\n",
      "MSE loss: 91.7462\n",
      "Iteration: 81000\n",
      "Gradient: [  -3.68     11.9325  -17.2733  -38.6186 -117.0284]\n",
      "Weights: [-4.7935  0.7255 -1.1934  0.1027  0.1447]\n",
      "MSE loss: 91.8086\n",
      "Iteration: 81100\n",
      "Gradient: [   0.2905    4.2978  -13.5132   21.3461 -191.2906]\n",
      "Weights: [-4.8012  0.7075 -1.1897  0.1031  0.145 ]\n",
      "MSE loss: 92.0209\n",
      "Iteration: 81200\n",
      "Gradient: [ 12.7112  -2.4867  68.0818  23.4855 -49.5912]\n",
      "Weights: [-4.7861  0.7091 -1.1905  0.1037  0.1449]\n",
      "MSE loss: 91.8018\n",
      "Iteration: 81300\n",
      "Gradient: [  6.7015   1.2645 -16.073  -67.7557  41.027 ]\n",
      "Weights: [-4.7829  0.7082 -1.1904  0.1035  0.1448]\n",
      "MSE loss: 91.7505\n",
      "Iteration: 81400\n",
      "Gradient: [  -9.4692    1.9286   12.2471   66.5133 -114.2442]\n",
      "Weights: [-4.789   0.7166 -1.1924  0.1036  0.1448]\n",
      "MSE loss: 91.757\n",
      "Iteration: 81500\n",
      "Gradient: [  -7.5177   28.1545   42.4948    3.9631 -268.6287]\n",
      "Weights: [-4.7831  0.7048 -1.19    0.1036  0.1448]\n",
      "MSE loss: 91.713\n",
      "Iteration: 81600\n",
      "Gradient: [  4.7172  12.6256   7.9285  92.0608 249.1058]\n",
      "Weights: [-4.7883  0.703  -1.1885  0.1039  0.1449]\n",
      "MSE loss: 91.7736\n",
      "Iteration: 81700\n",
      "Gradient: [  3.1169  -7.2182 -25.1864   2.3678 -81.2913]\n",
      "Weights: [-4.787   0.701  -1.1877  0.1043  0.1447]\n",
      "MSE loss: 91.7765\n",
      "Iteration: 81800\n",
      "Gradient: [ -0.3692  -0.9416  -6.6005  88.2583 -31.7193]\n",
      "Weights: [-4.7727  0.6922 -1.188   0.1039  0.1449]\n",
      "MSE loss: 91.7469\n",
      "Iteration: 81900\n",
      "Gradient: [ -0.8448  -0.3603   5.2233  -6.6083 102.3371]\n",
      "Weights: [-4.7795  0.6908 -1.1903  0.1041  0.1451]\n",
      "MSE loss: 92.0076\n",
      "Iteration: 82000\n",
      "Gradient: [  4.0555   4.1681 -12.5382  28.156  288.1478]\n",
      "Weights: [-4.7913  0.7    -1.1885  0.1043  0.1448]\n",
      "MSE loss: 91.8052\n",
      "Iteration: 82100\n",
      "Gradient: [   1.4237    7.8664  -13.4876 -116.7002  435.8492]\n",
      "Weights: [-4.7733  0.698  -1.189   0.1044  0.1446]\n",
      "MSE loss: 91.7691\n",
      "Iteration: 82200\n",
      "Gradient: [  6.0448  -5.0442  -0.0916 -33.5172  -9.7947]\n",
      "Weights: [-4.7755  0.6884 -1.1881  0.1055  0.1446]\n",
      "MSE loss: 91.7373\n",
      "Iteration: 82300\n",
      "Gradient: [ -5.5971  43.1314 -13.2535 120.8708 320.8156]\n",
      "Weights: [-4.7848  0.6894 -1.1832  0.1048  0.1444]\n",
      "MSE loss: 91.7731\n",
      "Iteration: 82400\n",
      "Gradient: [ -8.6729  12.5591   3.635   74.9605 549.5023]\n",
      "Weights: [-4.7728  0.686  -1.1838  0.1051  0.1442]\n",
      "MSE loss: 91.7198\n",
      "Iteration: 82500\n",
      "Gradient: [   2.4107    8.0086  -26.5137  -52.1042 -138.2316]\n",
      "Weights: [-4.765   0.6793 -1.1805  0.1051  0.1441]\n",
      "MSE loss: 91.8498\n",
      "Iteration: 82600\n",
      "Gradient: [  -0.8701   12.9696    6.6404   99.3061 -159.7987]\n",
      "Weights: [-4.753   0.6724 -1.183   0.1058  0.1441]\n",
      "MSE loss: 91.952\n",
      "Iteration: 82700\n",
      "Gradient: [  -1.4356   -9.911    -4.1359    0.338  -335.8781]\n",
      "Weights: [-4.763   0.6791 -1.184   0.1062  0.1441]\n",
      "MSE loss: 91.8009\n",
      "Iteration: 82800\n",
      "Gradient: [  4.0168  -7.6304 -46.9923  18.8598 -64.5263]\n",
      "Weights: [-4.7623  0.6807 -1.1848  0.1061  0.1439]\n",
      "MSE loss: 91.8116\n",
      "Iteration: 82900\n",
      "Gradient: [  -1.5445   -4.5823  -17.9697   15.4558 -124.7186]\n",
      "Weights: [-4.7666  0.6749 -1.183   0.107   0.144 ]\n",
      "MSE loss: 91.7606\n",
      "Iteration: 83000\n",
      "Gradient: [  -4.6348    2.9513    8.5902    3.6723 -232.4181]\n",
      "Weights: [-4.7642  0.6697 -1.1823  0.107   0.1441]\n",
      "MSE loss: 91.7973\n",
      "Iteration: 83100\n",
      "Gradient: [ -5.4083  29.6813  45.2359 -29.7686  69.1188]\n",
      "Weights: [-4.7568  0.6652 -1.1788  0.106   0.1441]\n",
      "MSE loss: 91.8745\n",
      "Iteration: 83200\n",
      "Gradient: [ -8.3292 -20.8484   8.8541  24.6137 142.6484]\n",
      "Weights: [-4.7649  0.6687 -1.1796  0.1068  0.1437]\n",
      "MSE loss: 91.7721\n",
      "Iteration: 83300\n",
      "Gradient: [   4.7372   -5.6321  -16.2747 -194.9164   42.7149]\n",
      "Weights: [-4.7485  0.664  -1.179   0.107   0.1437]\n",
      "MSE loss: 92.0805\n",
      "Iteration: 83400\n",
      "Gradient: [  1.9802  12.0015 -30.7555  54.571  243.2344]\n",
      "Weights: [-4.7625  0.6684 -1.1808  0.107   0.1436]\n",
      "MSE loss: 91.8339\n",
      "Iteration: 83500\n",
      "Gradient: [-15.3588 -13.6299 -16.207   53.1977 129.9196]\n",
      "Weights: [-4.7865  0.6706 -1.1796  0.1074  0.1437]\n",
      "MSE loss: 92.2054\n",
      "Iteration: 83600\n",
      "Gradient: [ -0.8065 -21.4027  -1.9658 -13.7972 446.6565]\n",
      "Weights: [-4.7615  0.67   -1.1774  0.1065  0.1435]\n",
      "MSE loss: 91.8253\n",
      "Iteration: 83700\n",
      "Gradient: [   3.4189   -2.6762  -59.373    64.0829 -309.4752]\n",
      "Weights: [-4.7871  0.6879 -1.1798  0.1061  0.1435]\n",
      "MSE loss: 91.74\n",
      "Iteration: 83800\n",
      "Gradient: [  4.0228  -1.7854 -30.0114  16.8053  74.8125]\n",
      "Weights: [-4.7676  0.6712 -1.178   0.1061  0.1438]\n",
      "MSE loss: 91.7536\n",
      "Iteration: 83900\n",
      "Gradient: [  4.1229  12.6279  36.3169   5.6077 -14.6093]\n",
      "Weights: [-4.7619  0.6641 -1.1743  0.1049  0.1439]\n",
      "MSE loss: 91.7949\n",
      "Iteration: 84000\n",
      "Gradient: [   2.787    -8.7889    8.2587   15.1508 -216.1369]\n",
      "Weights: [-4.7606  0.6607 -1.1713  0.1043  0.1437]\n",
      "MSE loss: 91.817\n",
      "Iteration: 84100\n",
      "Gradient: [   4.8329  -19.1974  -25.0758  -36.8076 -187.9104]\n",
      "Weights: [-4.7594  0.6638 -1.1721  0.1042  0.1438]\n",
      "MSE loss: 91.8346\n",
      "Iteration: 84200\n",
      "Gradient: [  1.8932  -6.8715  13.9057 -11.936  419.7332]\n",
      "Weights: [-4.7904  0.6775 -1.1694  0.1042  0.1433]\n",
      "MSE loss: 91.8657\n",
      "Iteration: 84300\n",
      "Gradient: [  -3.5547   19.6493   12.5536  -17.3717 -352.5628]\n",
      "Weights: [-4.7881  0.6848 -1.1705  0.1035  0.1432]\n",
      "MSE loss: 91.8017\n",
      "Iteration: 84400\n",
      "Gradient: [ 8.71000e-02 -9.61990e+00 -3.50848e+01 -1.36699e+02 -4.41596e+01]\n",
      "Weights: [-4.7877  0.6814 -1.1708  0.1038  0.1432]\n",
      "MSE loss: 91.842\n",
      "Iteration: 84500\n",
      "Gradient: [  -8.9701   31.3169   67.2307  118.7277 -195.306 ]\n",
      "Weights: [-4.7782  0.671  -1.1703  0.1045  0.1436]\n",
      "MSE loss: 91.7851\n",
      "Iteration: 84600\n",
      "Gradient: [ -2.5732   1.8948 -28.0351 -73.6016 -44.2147]\n",
      "Weights: [-4.761   0.6559 -1.1689  0.1045  0.1436]\n",
      "MSE loss: 91.8134\n",
      "Iteration: 84700\n",
      "Gradient: [-2.1256  4.4078 18.8305 17.3396 97.2433]\n",
      "Weights: [-4.769   0.6624 -1.1694  0.1049  0.1435]\n",
      "MSE loss: 91.7738\n",
      "Iteration: 84800\n",
      "Gradient: [ -6.6925  -6.6294  -2.5176 -51.2797 209.0219]\n",
      "Weights: [-4.7749  0.6574 -1.1686  0.1049  0.1434]\n",
      "MSE loss: 92.025\n",
      "Iteration: 84900\n",
      "Gradient: [ 0.3112 -3.2656 -2.131  78.0608 35.9022]\n",
      "Weights: [-4.7578  0.658  -1.1675  0.1048  0.1434]\n",
      "MSE loss: 91.9233\n",
      "Iteration: 85000\n",
      "Gradient: [  -2.6867  -14.1138  -13.4267  -59.3018 -104.3803]\n",
      "Weights: [-4.7722  0.6511 -1.1629  0.1038  0.1433]\n",
      "MSE loss: 92.0224\n",
      "Iteration: 85100\n",
      "Gradient: [-11.9467  -2.8531 -28.4535 -28.1653 -32.3926]\n",
      "Weights: [-4.7793  0.6496 -1.1597  0.1045  0.143 ]\n",
      "MSE loss: 92.0523\n",
      "Iteration: 85200\n",
      "Gradient: [  -4.7734    3.1137  -10.1561 -135.7824  278.6228]\n",
      "Weights: [-4.7582  0.6421 -1.1585  0.1034  0.143 ]\n",
      "MSE loss: 91.9616\n",
      "Iteration: 85300\n",
      "Gradient: [  5.5967 -16.5818  -9.5216 -26.6104 -93.7845]\n",
      "Weights: [-4.7671  0.6501 -1.1578  0.1036  0.143 ]\n",
      "MSE loss: 91.8337\n",
      "Iteration: 85400\n",
      "Gradient: [  6.1335  16.5602  77.2885 -88.1802 130.9744]\n",
      "Weights: [-4.781   0.6625 -1.157   0.1034  0.1429]\n",
      "MSE loss: 92.1046\n",
      "Iteration: 85500\n",
      "Gradient: [  -3.1126    3.7729  -54.6474  -96.1485 -167.922 ]\n",
      "Weights: [-4.7815  0.6586 -1.159   0.1035  0.1428]\n",
      "MSE loss: 91.9228\n",
      "Iteration: 85600\n",
      "Gradient: [   4.6461  -12.2318  -40.0591   33.9587 -172.6826]\n",
      "Weights: [-4.7755  0.656  -1.1568  0.1028  0.1429]\n",
      "MSE loss: 91.8304\n",
      "Iteration: 85700\n",
      "Gradient: [   2.3164    3.3797  -46.0203    8.9215 -180.6423]\n",
      "Weights: [-4.7703  0.6478 -1.1586  0.104   0.143 ]\n",
      "MSE loss: 91.8441\n",
      "Iteration: 85800\n",
      "Gradient: [ -1.4221 -17.5043   3.3065 -22.0512 -21.4594]\n",
      "Weights: [-4.7668  0.6613 -1.1625  0.1035  0.143 ]\n",
      "MSE loss: 91.8398\n",
      "Iteration: 85900\n",
      "Gradient: [ -1.4589 -12.5093   2.4081  27.7538 187.0061]\n",
      "Weights: [-4.7778  0.6668 -1.1665  0.1041  0.1434]\n",
      "MSE loss: 91.807\n",
      "Iteration: 86000\n",
      "Gradient: [  9.3125  -8.0398  12.6169 -36.5452 -27.7408]\n",
      "Weights: [-4.7791  0.6726 -1.1671  0.1042  0.1433]\n",
      "MSE loss: 91.8989\n",
      "Iteration: 86100\n",
      "Gradient: [ -10.1788   -0.7357  -13.779   -67.0053 -361.9837]\n",
      "Weights: [-4.7898  0.6606 -1.1658  0.1046  0.1432]\n",
      "MSE loss: 92.4388\n",
      "Iteration: 86200\n",
      "Gradient: [   2.5802   -0.5177   -1.8352 -198.8425   66.4782]\n",
      "Weights: [-4.7712  0.6642 -1.1663  0.1047  0.1432]\n",
      "MSE loss: 91.8001\n",
      "Iteration: 86300\n",
      "Gradient: [   0.3968    6.622   -11.218     3.7138 -346.7559]\n",
      "Weights: [-4.7634  0.6528 -1.1668  0.1056  0.1431]\n",
      "MSE loss: 91.7972\n",
      "Iteration: 86400\n",
      "Gradient: [ -8.2909  -9.6308   2.1507 -32.0926 101.1918]\n",
      "Weights: [-4.7679  0.6488 -1.1677  0.1063  0.1433]\n",
      "MSE loss: 91.9427\n",
      "Iteration: 86500\n",
      "Gradient: [  4.289    4.2667  -9.8827  44.8441 308.4716]\n",
      "Weights: [-4.7813  0.6542 -1.1639  0.1051  0.1432]\n",
      "MSE loss: 92.0503\n",
      "Iteration: 86600\n",
      "Gradient: [-14.1632   2.8358 -41.2301 -35.1297 287.5618]\n",
      "Weights: [-4.7818  0.6601 -1.1613  0.1045  0.1429]\n",
      "MSE loss: 91.8802\n",
      "Iteration: 86700\n",
      "Gradient: [  -4.7113    9.6325  -17.8682 -104.6379   61.033 ]\n",
      "Weights: [-4.7743  0.6583 -1.1593  0.1037  0.1427]\n",
      "MSE loss: 91.8126\n",
      "Iteration: 86800\n",
      "Gradient: [  9.1821  32.4726  22.9302 166.0826  24.5913]\n",
      "Weights: [-4.7801  0.6743 -1.1632  0.104   0.1428]\n",
      "MSE loss: 92.0947\n",
      "Iteration: 86900\n",
      "Gradient: [ -1.7235 -18.0263  17.4378  26.9088  77.533 ]\n",
      "Weights: [-4.7994  0.6953 -1.1657  0.1037  0.1426]\n",
      "MSE loss: 92.3309\n",
      "Iteration: 87000\n",
      "Gradient: [ 0.7    -6.251  14.4313 33.0698 60.1817]\n",
      "Weights: [-4.7974  0.6933 -1.1732  0.104   0.1429]\n",
      "MSE loss: 92.0623\n",
      "Iteration: 87100\n",
      "Gradient: [   7.0436   15.1991   13.6622   18.9981 -352.1261]\n",
      "Weights: [-4.7767  0.6827 -1.174   0.1048  0.1433]\n",
      "MSE loss: 91.7835\n",
      "Iteration: 87200\n",
      "Gradient: [ -6.6072  30.3021 -25.8585 129.4854 408.1412]\n",
      "Weights: [-4.7939  0.6914 -1.174   0.1046  0.1433]\n",
      "MSE loss: 91.8786\n",
      "Iteration: 87300\n",
      "Gradient: [   1.2299  -24.6779    0.4937 -157.0703 -460.1156]\n",
      "Weights: [-4.7802  0.6774 -1.1735  0.1044  0.1434]\n",
      "MSE loss: 91.8066\n",
      "Iteration: 87400\n",
      "Gradient: [   1.5538   -4.0889   -2.8766   16.6488 -106.1912]\n",
      "Weights: [-4.7775  0.6749 -1.172   0.1047  0.1435]\n",
      "MSE loss: 91.7568\n",
      "Iteration: 87500\n",
      "Gradient: [ 7.6923 -3.0358 -4.3704 11.4915 28.7295]\n",
      "Weights: [-4.7831  0.6836 -1.173   0.1044  0.1435]\n",
      "MSE loss: 91.8649\n",
      "Iteration: 87600\n",
      "Gradient: [  -2.2511    4.6539  -35.2757   88.4379 -375.2986]\n",
      "Weights: [-4.7899  0.6836 -1.1743  0.1051  0.1432]\n",
      "MSE loss: 91.8355\n",
      "Iteration: 87700\n",
      "Gradient: [ 12.0886   7.9796 -39.8334  52.1185 184.0065]\n",
      "Weights: [-4.7951  0.687  -1.1745  0.1057  0.1431]\n",
      "MSE loss: 91.8536\n",
      "Iteration: 87800\n",
      "Gradient: [  -0.981     6.7254   33.7351 -165.0502   68.4498]\n",
      "Weights: [-4.7783  0.6708 -1.1723  0.1062  0.1431]\n",
      "MSE loss: 91.742\n",
      "Iteration: 87900\n",
      "Gradient: [  17.253     0.6696  -51.7689    4.2628 -209.729 ]\n",
      "Weights: [-4.7609  0.6637 -1.1722  0.1057  0.1433]\n",
      "MSE loss: 91.7973\n",
      "Iteration: 88000\n",
      "Gradient: [  6.5476   7.5845   0.7566 -10.5745  84.8858]\n",
      "Weights: [-4.7674  0.6647 -1.1712  0.1052  0.1435]\n",
      "MSE loss: 91.7565\n",
      "Iteration: 88100\n",
      "Gradient: [  8.3833 -14.8533  -7.8936  31.972  295.1669]\n",
      "Weights: [-4.7562  0.6557 -1.1685  0.1053  0.1435]\n",
      "MSE loss: 91.9148\n",
      "Iteration: 88200\n",
      "Gradient: [   3.1969  -27.5279   -3.589    16.861  -219.5017]\n",
      "Weights: [-4.7578  0.6559 -1.1683  0.1049  0.1436]\n",
      "MSE loss: 91.8969\n",
      "Iteration: 88300\n",
      "Gradient: [ -5.114   12.5077  27.2666 -41.1801  56.0135]\n",
      "Weights: [-4.773   0.6672 -1.1706  0.105   0.1433]\n",
      "MSE loss: 91.7764\n",
      "Iteration: 88400\n",
      "Gradient: [ -9.5903  11.6837  11.0196 113.7868 165.682 ]\n",
      "Weights: [-4.7796  0.6731 -1.1708  0.1055  0.1432]\n",
      "MSE loss: 91.7816\n",
      "Iteration: 88500\n",
      "Gradient: [   6.6552   -0.2521    8.0634  -48.6584 -103.7179]\n",
      "Weights: [-4.7861  0.6792 -1.1728  0.105   0.1434]\n",
      "MSE loss: 91.7809\n",
      "Iteration: 88600\n",
      "Gradient: [   9.2464   11.4972    7.6068  -18.0286 -146.8206]\n",
      "Weights: [-4.7645  0.6708 -1.1739  0.1051  0.1434]\n",
      "MSE loss: 91.792\n",
      "Iteration: 88700\n",
      "Gradient: [  -9.5931    4.7584   -8.1899  -51.3801 -129.9031]\n",
      "Weights: [-4.7746  0.6705 -1.1725  0.1051  0.1437]\n",
      "MSE loss: 91.8193\n",
      "Iteration: 88800\n",
      "Gradient: [  15.2118   -0.8243   18.4031   10.6473 -144.6703]\n",
      "Weights: [-4.758   0.6699 -1.176   0.1057  0.1435]\n",
      "MSE loss: 91.886\n",
      "Iteration: 88900\n",
      "Gradient: [-15.0233   9.6322  -4.5448 -32.7974 206.4117]\n",
      "Weights: [-4.787   0.6859 -1.1767  0.105   0.1434]\n",
      "MSE loss: 91.7717\n",
      "Iteration: 89000\n",
      "Gradient: [ -5.0307   1.9657  19.4973 107.8476 223.5471]\n",
      "Weights: [-4.794   0.6735 -1.1725  0.106   0.1434]\n",
      "MSE loss: 92.1105\n",
      "Iteration: 89100\n",
      "Gradient: [  7.8461 -10.6295   8.5593  50.6071 207.8105]\n",
      "Weights: [-4.7847  0.6788 -1.1725  0.1059  0.1429]\n",
      "MSE loss: 91.7639\n",
      "Iteration: 89200\n",
      "Gradient: [  6.2151   0.5073  46.0981  30.8123 129.0947]\n",
      "Weights: [-4.7645  0.677  -1.1716  0.1049  0.143 ]\n",
      "MSE loss: 92.0645\n",
      "Iteration: 89300\n",
      "Gradient: [  8.0777 -18.9829  13.6016 -84.1442 169.5531]\n",
      "Weights: [-4.7944  0.6824 -1.1754  0.1066  0.1431]\n",
      "MSE loss: 91.9059\n",
      "Iteration: 89400\n",
      "Gradient: [  -3.8578   16.3358  -36.4202  -31.6755 -162.3641]\n",
      "Weights: [-4.7855  0.677  -1.1731  0.1062  0.1432]\n",
      "MSE loss: 91.8282\n",
      "Iteration: 89500\n",
      "Gradient: [ -3.0697  -9.4058  10.3534 113.5729 125.1535]\n",
      "Weights: [-4.7882  0.6751 -1.1733  0.1057  0.143 ]\n",
      "MSE loss: 92.1005\n",
      "Iteration: 89600\n",
      "Gradient: [ 7.50000e-03  1.68086e+01 -3.69577e+01 -9.85303e+01 -6.25330e+00]\n",
      "Weights: [-4.7693  0.6734 -1.175   0.1055  0.143 ]\n",
      "MSE loss: 91.98\n",
      "Iteration: 89700\n",
      "Gradient: [ -5.2137  -0.6598   9.5741 178.4811 206.3476]\n",
      "Weights: [-4.7738  0.6736 -1.1754  0.1059  0.1435]\n",
      "MSE loss: 91.7407\n",
      "Iteration: 89800\n",
      "Gradient: [ 2.734800e+00  1.121000e-01  2.034810e+01 -6.162240e+01 -1.736947e+02]\n",
      "Weights: [-4.7653  0.667  -1.1777  0.1066  0.1437]\n",
      "MSE loss: 91.7576\n",
      "Iteration: 89900\n",
      "Gradient: [-0.5148  5.8197 14.9073 92.4095 61.7866]\n",
      "Weights: [-4.7593  0.6699 -1.1782  0.107   0.1438]\n",
      "MSE loss: 92.1949\n",
      "Iteration: 90000\n",
      "Gradient: [  1.7034   2.2112  -9.5715 -13.0635 125.7944]\n",
      "Weights: [-4.7873  0.6769 -1.178   0.1065  0.1437]\n",
      "MSE loss: 91.9179\n",
      "Iteration: 90100\n",
      "Gradient: [   4.3639   -5.6192   26.8614   37.6046 -165.7003]\n",
      "Weights: [-4.7969  0.6853 -1.1767  0.1058  0.1433]\n",
      "MSE loss: 91.9901\n",
      "Iteration: 90200\n",
      "Gradient: [  -2.8704   -0.6848   33.8269   40.746  -116.5095]\n",
      "Weights: [-4.7999  0.694  -1.1751  0.1045  0.143 ]\n",
      "MSE loss: 92.0391\n",
      "Iteration: 90300\n",
      "Gradient: [-1.427550e+01  7.293400e+00  1.096000e-01  3.023930e+01  2.110389e+02]\n",
      "Weights: [-4.7861  0.6792 -1.1739  0.1054  0.1431]\n",
      "MSE loss: 91.9143\n",
      "Iteration: 90400\n",
      "Gradient: [  -1.4745  -12.7024   -4.695    16.2791 -104.5307]\n",
      "Weights: [-4.7853  0.6824 -1.174   0.1053  0.143 ]\n",
      "MSE loss: 91.8378\n",
      "Iteration: 90500\n",
      "Gradient: [  -6.1267  -13.3959   10.7908  109.3239 -175.8177]\n",
      "Weights: [-4.7833  0.6872 -1.1762  0.1061  0.1431]\n",
      "MSE loss: 91.7815\n",
      "Iteration: 90600\n",
      "Gradient: [  -8.156   -11.3875   32.3293 -178.5555  155.0601]\n",
      "Weights: [-4.7792  0.6804 -1.1764  0.1065  0.1433]\n",
      "MSE loss: 91.8\n",
      "Iteration: 90700\n",
      "Gradient: [ -4.4671   0.209  -22.7087 -26.6474  68.6059]\n",
      "Weights: [-4.7834  0.69   -1.1782  0.1057  0.1431]\n",
      "MSE loss: 91.7337\n",
      "Iteration: 90800\n",
      "Gradient: [ 6.0854  3.9624  7.8255 43.1312 75.486 ]\n",
      "Weights: [-4.7898  0.7057 -1.1795  0.1054  0.1431]\n",
      "MSE loss: 92.1502\n",
      "Iteration: 90900\n",
      "Gradient: [ -0.9444   3.4894 -30.7628 111.588  -68.1738]\n",
      "Weights: [-4.7889  0.6863 -1.1782  0.106   0.1432]\n",
      "MSE loss: 91.8097\n",
      "Iteration: 91000\n",
      "Gradient: [  5.8559 -19.7927 -24.9148   2.4642  99.6349]\n",
      "Weights: [-4.7679  0.6889 -1.1809  0.1062  0.1433]\n",
      "MSE loss: 91.9693\n",
      "Iteration: 91100\n",
      "Gradient: [   1.2781  -11.6702    0.1542  -33.539  -123.1963]\n",
      "Weights: [-4.7814  0.6868 -1.183   0.1064  0.1436]\n",
      "MSE loss: 91.7299\n",
      "Iteration: 91200\n",
      "Gradient: [ -11.6704   10.0039  -15.2882   41.2386 -129.3988]\n",
      "Weights: [-4.7675  0.6764 -1.1831  0.1068  0.1438]\n",
      "MSE loss: 91.7422\n",
      "Iteration: 91300\n",
      "Gradient: [ -0.3709  -0.2993  61.0699  18.6838 248.0336]\n",
      "Weights: [-4.7578  0.6723 -1.1817  0.1068  0.1438]\n",
      "MSE loss: 91.8672\n",
      "Iteration: 91400\n",
      "Gradient: [  1.0186  -6.9541 -16.8398 126.662  371.1192]\n",
      "Weights: [-4.7856  0.6837 -1.1796  0.1065  0.1437]\n",
      "MSE loss: 91.8156\n",
      "Iteration: 91500\n",
      "Gradient: [ -11.7471   -5.9861   24.0947  -70.9858 -126.8686]\n",
      "Weights: [-4.7763  0.6731 -1.1787  0.1069  0.1435]\n",
      "MSE loss: 91.7592\n",
      "Iteration: 91600\n",
      "Gradient: [ -3.8588  13.2284 -62.9116 -74.9587  88.9913]\n",
      "Weights: [-4.7787  0.6751 -1.179   0.1068  0.1436]\n",
      "MSE loss: 91.7741\n",
      "Iteration: 91700\n",
      "Gradient: [ 5.5293  8.6967 14.4666 13.4502  5.669 ]\n",
      "Weights: [-4.7467  0.6667 -1.1789  0.1074  0.1437]\n",
      "MSE loss: 92.7506\n",
      "Iteration: 91800\n",
      "Gradient: [  1.1384   5.7689  -6.3742   0.5995 139.0727]\n",
      "Weights: [-4.7667  0.6642 -1.1785  0.1074  0.1435]\n",
      "MSE loss: 91.815\n",
      "Iteration: 91900\n",
      "Gradient: [  5.4585  -6.3918  59.702  -53.7058 -49.8926]\n",
      "Weights: [-4.7665  0.661  -1.1752  0.108   0.1433]\n",
      "MSE loss: 91.8525\n",
      "Iteration: 92000\n",
      "Gradient: [ -4.3049 -11.2835   4.9547  37.0482 -71.0572]\n",
      "Weights: [-4.7857  0.6739 -1.1737  0.1074  0.1428]\n",
      "MSE loss: 91.8045\n",
      "Iteration: 92100\n",
      "Gradient: [ -2.9111  15.309  -51.3029 -28.9908 -48.9371]\n",
      "Weights: [-4.7806  0.6755 -1.1745  0.1073  0.1427]\n",
      "MSE loss: 91.732\n",
      "Iteration: 92200\n",
      "Gradient: [ -8.0578   0.2    -19.8226  17.1362 173.0441]\n",
      "Weights: [-4.7887  0.6765 -1.1724  0.1078  0.1425]\n",
      "MSE loss: 91.8259\n",
      "Iteration: 92300\n",
      "Gradient: [-5.547   1.191  15.9444 33.5897 26.331 ]\n",
      "Weights: [-4.7825  0.6804 -1.1744  0.1069  0.1426]\n",
      "MSE loss: 91.753\n",
      "Iteration: 92400\n",
      "Gradient: [   8.8137  -15.1442   41.5798   59.9089 -352.2908]\n",
      "Weights: [-4.7824  0.6848 -1.1773  0.1072  0.1428]\n",
      "MSE loss: 91.7157\n",
      "Iteration: 92500\n",
      "Gradient: [ -6.5665  15.8037  -0.5842 -43.0167  70.8131]\n",
      "Weights: [-4.7733  0.6812 -1.179   0.1077  0.1429]\n",
      "MSE loss: 91.7454\n",
      "Iteration: 92600\n",
      "Gradient: [  3.0461  -4.5177   9.3275  -5.2057 150.1889]\n",
      "Weights: [-4.765   0.6786 -1.1796  0.1078  0.143 ]\n",
      "MSE loss: 91.8871\n",
      "Iteration: 92700\n",
      "Gradient: [ -5.1561  -5.8915 -53.947  -84.0814  37.4475]\n",
      "Weights: [-4.7845  0.6832 -1.181   0.1084  0.1429]\n",
      "MSE loss: 91.733\n",
      "Iteration: 92800\n",
      "Gradient: [  3.4762 -16.557    6.608  -11.1508 -47.8277]\n",
      "Weights: [-4.7743  0.6885 -1.1832  0.1083  0.1429]\n",
      "MSE loss: 91.7584\n",
      "Iteration: 92900\n",
      "Gradient: [  12.9789    9.6908  -33.2737   56.2551 -145.293 ]\n",
      "Weights: [-4.7593  0.6709 -1.1804  0.109   0.1431]\n",
      "MSE loss: 91.963\n",
      "Iteration: 93000\n",
      "Gradient: [  4.0548  10.8731  41.3664  -2.622  134.1409]\n",
      "Weights: [-4.7635  0.6831 -1.1791  0.1078  0.1429]\n",
      "MSE loss: 92.3921\n",
      "Iteration: 93100\n",
      "Gradient: [  -3.9668    1.3045  -10.9039  -22.6007 -320.274 ]\n",
      "Weights: [-4.7917  0.684  -1.1786  0.1083  0.1428]\n",
      "MSE loss: 91.8158\n",
      "Iteration: 93200\n",
      "Gradient: [  -3.0737   -2.835   -12.0456   45.9605 -319.8473]\n",
      "Weights: [-4.7803  0.679  -1.1822  0.1088  0.143 ]\n",
      "MSE loss: 91.7335\n",
      "Iteration: 93300\n",
      "Gradient: [-11.608  -15.1102  14.4848  13.8768 -83.2805]\n",
      "Weights: [-4.7846  0.6763 -1.1806  0.1084  0.143 ]\n",
      "MSE loss: 91.9993\n",
      "Iteration: 93400\n",
      "Gradient: [-2.153000e-01 -6.303000e-01  1.322300e+01 -1.327120e+02 -2.269735e+02]\n",
      "Weights: [-4.7886  0.6846 -1.1804  0.1085  0.1429]\n",
      "MSE loss: 91.7515\n",
      "Iteration: 93500\n",
      "Gradient: [ -7.8188   1.895   23.6537  44.9357 -67.5886]\n",
      "Weights: [-4.7826  0.6851 -1.1803  0.1078  0.1429]\n",
      "MSE loss: 91.6887\n",
      "Iteration: 93600\n",
      "Gradient: [  1.3618  -1.3274  -4.1916 -44.2856 -86.8573]\n",
      "Weights: [-4.7831  0.6851 -1.1833  0.1082  0.143 ]\n",
      "MSE loss: 91.7677\n",
      "Iteration: 93700\n",
      "Gradient: [  7.2655   1.5363  13.4639  27.7096 263.7393]\n",
      "Weights: [-4.778   0.6828 -1.1813  0.1085  0.143 ]\n",
      "MSE loss: 91.7417\n",
      "Iteration: 93800\n",
      "Gradient: [  4.9164  10.1844  -1.4477  46.0423 135.8327]\n",
      "Weights: [-4.7773  0.6904 -1.1817  0.1076  0.143 ]\n",
      "MSE loss: 91.7875\n",
      "Iteration: 93900\n",
      "Gradient: [  0.4456   9.1686   3.8356 -26.023  -17.1888]\n",
      "Weights: [-4.7704  0.6781 -1.1807  0.1082  0.1431]\n",
      "MSE loss: 91.7456\n",
      "Iteration: 94000\n",
      "Gradient: [  8.246  -14.4046 -22.1809 -42.4347  98.121 ]\n",
      "Weights: [-4.7785  0.6836 -1.1846  0.108   0.1432]\n",
      "MSE loss: 91.7876\n",
      "Iteration: 94100\n",
      "Gradient: [ -3.9275   4.8644  -6.2608 128.9545  45.0829]\n",
      "Weights: [-4.7856  0.6846 -1.1826  0.1085  0.1432]\n",
      "MSE loss: 91.7525\n",
      "Iteration: 94200\n",
      "Gradient: [ -7.2084  -9.6847 -27.759  121.9182 151.5152]\n",
      "Weights: [-4.7711  0.6744 -1.1835  0.108   0.1434]\n",
      "MSE loss: 91.8621\n",
      "Iteration: 94300\n",
      "Gradient: [  7.4087  11.8583  50.2076 -21.9634 181.5569]\n",
      "Weights: [-4.763   0.6759 -1.1848  0.1085  0.1436]\n",
      "MSE loss: 91.7825\n",
      "Iteration: 94400\n",
      "Gradient: [ -3.4835  -1.1764 -26.0946 -16.583   35.2553]\n",
      "Weights: [-4.7699  0.6709 -1.1863  0.1093  0.1435]\n",
      "MSE loss: 91.9426\n",
      "Iteration: 94500\n",
      "Gradient: [  0.882   -4.1415  68.2114  53.6538 195.1503]\n",
      "Weights: [-4.7642  0.6785 -1.1874  0.1095  0.1433]\n",
      "MSE loss: 91.7185\n",
      "Iteration: 94600\n",
      "Gradient: [ -7.5768 -12.2865   9.3045 -87.39   -82.1406]\n",
      "Weights: [-4.7607  0.671  -1.188   0.1104  0.1432]\n",
      "MSE loss: 91.7743\n",
      "Iteration: 94700\n",
      "Gradient: [   3.6083    2.4033   12.2312 -114.5308   36.8904]\n",
      "Weights: [-4.7644  0.6785 -1.1889  0.1099  0.1435]\n",
      "MSE loss: 91.739\n",
      "Iteration: 94800\n",
      "Gradient: [   1.1957   -6.0313  -20.8283   34.6303 -185.2848]\n",
      "Weights: [-4.7803  0.6841 -1.1886  0.11    0.1432]\n",
      "MSE loss: 91.7209\n",
      "Iteration: 94900\n",
      "Gradient: [  15.5456    8.8869  -27.6817  -68.8224 -100.8075]\n",
      "Weights: [-4.7584  0.6772 -1.1897  0.1102  0.1435]\n",
      "MSE loss: 91.8255\n",
      "Iteration: 95000\n",
      "Gradient: [  1.0681  30.3289  26.3301  -2.0688 211.0958]\n",
      "Weights: [-4.7569  0.675  -1.1846  0.11    0.1432]\n",
      "MSE loss: 92.175\n",
      "Iteration: 95100\n",
      "Gradient: [ -1.2613 -19.3836  -1.3737   7.8389 173.4717]\n",
      "Weights: [-4.7839  0.6868 -1.1871  0.1095  0.1431]\n",
      "MSE loss: 91.7429\n",
      "Iteration: 95200\n",
      "Gradient: [  3.2325  -7.298   15.76   -60.6539  88.8962]\n",
      "Weights: [-4.7912  0.7026 -1.1874  0.1087  0.1429]\n",
      "MSE loss: 91.6934\n",
      "Iteration: 95300\n",
      "Gradient: [ -2.8782   8.2419   7.5257  56.5968 -55.4769]\n",
      "Weights: [-4.779   0.6848 -1.1865  0.1102  0.1428]\n",
      "MSE loss: 91.6703\n",
      "Iteration: 95400\n",
      "Gradient: [  6.6482  -8.654  -17.2651  47.6885 204.0186]\n",
      "Weights: [-4.7668  0.6711 -1.185   0.1104  0.143 ]\n",
      "MSE loss: 91.7071\n",
      "Iteration: 95500\n",
      "Gradient: [  0.6764  11.1996  40.081   38.1546 229.302 ]\n",
      "Weights: [-4.7794  0.6828 -1.1839  0.1094  0.1429]\n",
      "MSE loss: 91.6647\n",
      "Iteration: 95600\n",
      "Gradient: [   3.0437   -1.6448  -29.7416   16.5327 -337.4678]\n",
      "Weights: [-4.8027  0.6928 -1.1858  0.1085  0.1433]\n",
      "MSE loss: 92.2058\n",
      "Iteration: 95700\n",
      "Gradient: [ 2.457  -0.9301  4.4692 94.9961 74.74  ]\n",
      "Weights: [-4.7878  0.6993 -1.1911  0.1089  0.1434]\n",
      "MSE loss: 91.6601\n",
      "Iteration: 95800\n",
      "Gradient: [   4.1735   -0.8532  -28.5012  -70.9752 -274.6067]\n",
      "Weights: [-4.7887  0.7128 -1.196   0.1088  0.1432]\n",
      "MSE loss: 91.6854\n",
      "Iteration: 95900\n",
      "Gradient: [ -4.2667  -9.2675 -11.6617  90.8963  70.2728]\n",
      "Weights: [-4.8003  0.7079 -1.1959  0.1092  0.1436]\n",
      "MSE loss: 91.8893\n",
      "Iteration: 96000\n",
      "Gradient: [  4.4773  13.0499  18.4976 181.2642  75.7967]\n",
      "Weights: [-4.7782  0.7095 -1.1945  0.1091  0.1436]\n",
      "MSE loss: 92.2844\n",
      "Iteration: 96100\n",
      "Gradient: [  -4.0126   13.9737   18.1433  -98.1661 -304.9047]\n",
      "Weights: [-4.7771  0.7078 -1.1952  0.1082  0.1436]\n",
      "MSE loss: 91.762\n",
      "Iteration: 96200\n",
      "Gradient: [  -7.7878  -12.2961   19.0787   58.1017 -252.6776]\n",
      "Weights: [-4.7877  0.7018 -1.1979  0.1089  0.1438]\n",
      "MSE loss: 92.0146\n",
      "Iteration: 96300\n",
      "Gradient: [  15.9016    9.18     31.1268  -19.2447 -158.8023]\n",
      "Weights: [-4.7734  0.6939 -1.1982  0.1101  0.1441]\n",
      "MSE loss: 91.7069\n",
      "Iteration: 96400\n",
      "Gradient: [ -1.956   11.7464 -21.318  104.6548 164.6496]\n",
      "Weights: [-4.764   0.6913 -1.1988  0.1104  0.1439]\n",
      "MSE loss: 91.7504\n",
      "Iteration: 96500\n",
      "Gradient: [  2.6507  -3.8995   8.3953 -30.5125 198.7236]\n",
      "Weights: [-4.7781  0.7022 -1.1972  0.1097  0.1438]\n",
      "MSE loss: 91.6836\n",
      "Iteration: 96600\n",
      "Gradient: [   3.2033   -4.5536  -27.3905  -65.1394 -135.8211]\n",
      "Weights: [-4.7691  0.6989 -1.1972  0.1101  0.1435]\n",
      "MSE loss: 91.7368\n",
      "Iteration: 96700\n",
      "Gradient: [  -6.3896   -6.1184    6.6929  -26.4388 -191.0222]\n",
      "Weights: [-4.7941  0.6993 -1.1942  0.1104  0.1433]\n",
      "MSE loss: 91.8494\n",
      "Iteration: 96800\n",
      "Gradient: [   5.8184  -21.0029  -26.9021   19.431  -134.6971]\n",
      "Weights: [-4.7917  0.6978 -1.1928  0.1101  0.1433]\n",
      "MSE loss: 91.7421\n",
      "Iteration: 96900\n",
      "Gradient: [ -18.469    13.4404   14.3108 -144.3738   93.3611]\n",
      "Weights: [-4.7834  0.7081 -1.195   0.1088  0.1433]\n",
      "MSE loss: 91.6865\n",
      "Iteration: 97000\n",
      "Gradient: [ 13.7633   1.3813  40.8004  69.141  173.5613]\n",
      "Weights: [-4.7824  0.7044 -1.1944  0.1096  0.1436]\n",
      "MSE loss: 91.849\n",
      "Iteration: 97100\n",
      "Gradient: [ -0.2217 -14.2521  -1.1711  23.7132 166.0169]\n",
      "Weights: [-4.7681  0.7    -1.195   0.1101  0.1435]\n",
      "MSE loss: 92.1158\n",
      "Iteration: 97200\n",
      "Gradient: [  4.258   -7.0708 -28.6733 -48.4456   3.4921]\n",
      "Weights: [-4.7773  0.6958 -1.1956  0.1102  0.1434]\n",
      "MSE loss: 91.6457\n",
      "Iteration: 97300\n",
      "Gradient: [ -7.5961  -0.4891 -28.7181  94.6246 -83.0488]\n",
      "Weights: [-4.7801  0.6933 -1.1941  0.1103  0.1436]\n",
      "MSE loss: 91.6988\n",
      "Iteration: 97400\n",
      "Gradient: [  -2.2466   -5.1844    3.4286   -9.0502 -188.0019]\n",
      "Weights: [-4.791   0.693  -1.1903  0.1103  0.1433]\n",
      "MSE loss: 91.8336\n",
      "Iteration: 97500\n",
      "Gradient: [   9.4675   32.7978   -4.8151   69.7448 -135.5329]\n",
      "Weights: [-4.783   0.7067 -1.1956  0.1097  0.1434]\n",
      "MSE loss: 91.6722\n",
      "Iteration: 97600\n",
      "Gradient: [  6.9774 -20.3106  -8.2734 139.885  -49.7368]\n",
      "Weights: [-4.7896  0.7157 -1.197   0.1105  0.143 ]\n",
      "MSE loss: 91.7792\n",
      "Iteration: 97700\n",
      "Gradient: [  -1.2771   13.5203   -3.2403 -102.691   -24.89  ]\n",
      "Weights: [-4.7997  0.7238 -1.1974  0.109   0.143 ]\n",
      "MSE loss: 91.7388\n",
      "Iteration: 97800\n",
      "Gradient: [ -0.4187  -1.3837  63.4235 -58.2069  43.6516]\n",
      "Weights: [-4.7899  0.7303 -1.1992  0.1083  0.1432]\n",
      "MSE loss: 91.9744\n",
      "Iteration: 97900\n",
      "Gradient: [ 13.3988   1.7382  16.5989 -49.6308 -97.9438]\n",
      "Weights: [-4.8086  0.7383 -1.2023  0.1085  0.1434]\n",
      "MSE loss: 91.7841\n",
      "Iteration: 98000\n",
      "Gradient: [ -8.4525  13.3086 -33.8133  -8.5494  71.8925]\n",
      "Weights: [-4.802   0.7308 -1.2028  0.1091  0.1436]\n",
      "MSE loss: 91.6865\n",
      "Iteration: 98100\n",
      "Gradient: [  0.6508 -18.0975 -11.2488 -84.2144 338.9689]\n",
      "Weights: [-4.8024  0.7257 -1.2041  0.1082  0.1441]\n",
      "MSE loss: 91.8208\n",
      "Iteration: 98200\n",
      "Gradient: [ 1.4758  5.0215  5.5227  8.9875 24.907 ]\n",
      "Weights: [-4.8048  0.7425 -1.2054  0.1083  0.1439]\n",
      "MSE loss: 91.8549\n",
      "Iteration: 98300\n",
      "Gradient: [ -27.5395   -5.3718   -8.3943   24.3809 -349.4436]\n",
      "Weights: [-4.8179  0.7497 -1.2061  0.1076  0.1436]\n",
      "MSE loss: 92.0335\n",
      "Iteration: 98400\n",
      "Gradient: [-1.904200e+00  6.744700e+00  4.170000e-02  2.535100e+00 -4.598193e+02]\n",
      "Weights: [-4.8004  0.745  -1.2055  0.1073  0.1438]\n",
      "MSE loss: 91.8771\n",
      "Iteration: 98500\n",
      "Gradient: [ -0.6789 -10.9443  51.9805  74.3035 -34.2644]\n",
      "Weights: [-4.789   0.7367 -1.2074  0.1082  0.144 ]\n",
      "MSE loss: 91.8292\n",
      "Iteration: 98600\n",
      "Gradient: [ -9.6121  16.3922 -22.2025 -68.5974 285.0294]\n",
      "Weights: [-4.8046  0.737  -1.2056  0.1081  0.1439]\n",
      "MSE loss: 91.7328\n",
      "Iteration: 98700\n",
      "Gradient: [-10.2095  -1.9638 -54.6498  92.2557  10.1902]\n",
      "Weights: [-4.8055  0.7276 -1.2022  0.1077  0.1442]\n",
      "MSE loss: 91.7636\n",
      "Iteration: 98800\n",
      "Gradient: [ 3.420000e-02  2.856690e+01 -4.187660e+01  7.356550e+01 -4.346148e+02]\n",
      "Weights: [-4.7822  0.7176 -1.2001  0.1075  0.1442]\n",
      "MSE loss: 91.7203\n",
      "Iteration: 98900\n",
      "Gradient: [ -4.0141   6.3313 -21.0323  27.8282 119.5438]\n",
      "Weights: [-4.8058  0.7201 -1.2004  0.1076  0.1442]\n",
      "MSE loss: 91.9572\n",
      "Iteration: 99000\n",
      "Gradient: [-1.386780e+01  1.214000e-01 -3.577650e+01 -7.903900e+00  2.835953e+02]\n",
      "Weights: [-4.7997  0.7144 -1.2012  0.1078  0.1444]\n",
      "MSE loss: 92.0276\n",
      "Iteration: 99100\n",
      "Gradient: [  7.8671   8.6765 -33.8682 -85.8858 102.1839]\n",
      "Weights: [-4.7925  0.7111 -1.2001  0.1081  0.1442]\n",
      "MSE loss: 91.8026\n",
      "Iteration: 99200\n",
      "Gradient: [  -8.1183    1.29      0.7597  105.8036 -111.3189]\n",
      "Weights: [-4.7863  0.7199 -1.2023  0.1084  0.1443]\n",
      "MSE loss: 91.8409\n",
      "Iteration: 99300\n",
      "Gradient: [-1.158350e+01  1.827000e-01  1.340940e+01 -4.845650e+01  3.209462e+02]\n",
      "Weights: [-4.7943  0.7151 -1.2028  0.1079  0.1445]\n",
      "MSE loss: 91.806\n",
      "Iteration: 99400\n",
      "Gradient: [  5.5338  -0.5625 -55.091   28.7644  -9.9086]\n",
      "Weights: [-4.7709  0.7034 -1.2017  0.1077  0.1446]\n",
      "MSE loss: 91.7546\n",
      "Iteration: 99500\n",
      "Gradient: [  -7.9273   -7.8075    2.939  -181.8807    8.2319]\n",
      "Weights: [-4.7795  0.6988 -1.2021  0.1079  0.1446]\n",
      "MSE loss: 92.4248\n",
      "Iteration: 99600\n",
      "Gradient: [ -0.6841   2.8674 -35.2834  66.4085  69.8662]\n",
      "Weights: [-4.778   0.7073 -1.202   0.1078  0.1446]\n",
      "MSE loss: 91.6952\n",
      "Iteration: 99700\n",
      "Gradient: [ -0.4449  16.2581 -60.4573 -84.3256 170.2818]\n",
      "Weights: [-4.7733  0.6974 -1.1973  0.1083  0.1444]\n",
      "MSE loss: 91.6969\n",
      "Iteration: 99800\n",
      "Gradient: [ -11.6393    4.2451   40.729     0.1658 -109.3574]\n",
      "Weights: [-4.7751  0.7022 -1.1978  0.1088  0.1439]\n",
      "MSE loss: 91.6647\n",
      "Iteration: 99900\n",
      "Gradient: [ -3.1949 -18.2698 -29.0628 -45.5978 -65.8514]\n",
      "Weights: [-4.7746  0.7078 -1.2006  0.1091  0.1441]\n",
      "MSE loss: 91.7362\n",
      "Iteration: 100000\n",
      "Gradient: [  -1.8455  -16.1654  -38.6417 -104.6002  337.2177]\n",
      "Weights: [-4.7917  0.7093 -1.2001  0.1095  0.144 ]\n",
      "MSE loss: 91.6969\n",
      "Iteration: 100100\n",
      "Gradient: [  6.5679   3.6875 -31.1787  31.061   39.0363]\n",
      "Weights: [-4.7614  0.6921 -1.1994  0.1102  0.144 ]\n",
      "MSE loss: 91.7974\n",
      "Iteration: 100200\n",
      "Gradient: [ 12.7223 -24.4148 -34.8642 -32.8272  35.8548]\n",
      "Weights: [-4.7787  0.6886 -1.1983  0.1108  0.1441]\n",
      "MSE loss: 91.8653\n",
      "Iteration: 100300\n",
      "Gradient: [ -17.4529   -8.7712   10.8226   84.8165 -169.5758]\n",
      "Weights: [-4.773   0.6804 -1.1971  0.1109  0.1439]\n",
      "MSE loss: 92.1498\n",
      "Iteration: 100400\n",
      "Gradient: [ 15.767    2.5783 -23.4652  60.4964 246.1441]\n",
      "Weights: [-4.7617  0.6928 -1.1999  0.1104  0.1439]\n",
      "MSE loss: 91.7849\n",
      "Iteration: 100500\n",
      "Gradient: [ -10.5608    1.5241    8.1651   55.7779 -107.8778]\n",
      "Weights: [-4.7777  0.6929 -1.1993  0.1108  0.1437]\n",
      "MSE loss: 91.817\n",
      "Iteration: 100600\n",
      "Gradient: [ 2.133000e-01  1.587090e+01  6.945300e+00  9.004690e+01 -2.155168e+02]\n",
      "Weights: [-4.7914  0.7093 -1.2014  0.1113  0.1437]\n",
      "MSE loss: 91.7233\n",
      "Iteration: 100700\n",
      "Gradient: [  10.7016   -7.7443  -15.6356 -129.4626  -79.9052]\n",
      "Weights: [-4.7863  0.7071 -1.2044  0.1106  0.1437]\n",
      "MSE loss: 92.0265\n",
      "Iteration: 100800\n",
      "Gradient: [ -7.8549  -5.2721 -24.3014 -26.6065 370.2389]\n",
      "Weights: [-4.8021  0.7209 -1.2038  0.1109  0.1436]\n",
      "MSE loss: 91.7213\n",
      "Iteration: 100900\n",
      "Gradient: [ -0.1369   8.012   -0.5595 -90.6593  95.0406]\n",
      "Weights: [-4.8001  0.7273 -1.2081  0.1119  0.1435]\n",
      "MSE loss: 91.6313\n",
      "Iteration: 101000\n",
      "Gradient: [  9.0485  16.8398 -85.9901 -47.0967  91.3122]\n",
      "Weights: [-4.7892  0.7211 -1.2069  0.1121  0.1435]\n",
      "MSE loss: 91.6476\n",
      "Iteration: 101100\n",
      "Gradient: [  -7.4248   -0.5153  -10.419   -37.9318 -109.3551]\n",
      "Weights: [-4.7925  0.7221 -1.2055  0.1112  0.1434]\n",
      "MSE loss: 91.5979\n",
      "Iteration: 101200\n",
      "Gradient: [  -1.829    17.4531   -2.2655  -67.276  -260.0076]\n",
      "Weights: [-4.7932  0.7237 -1.2071  0.1117  0.1434]\n",
      "MSE loss: 91.5858\n",
      "Iteration: 101300\n",
      "Gradient: [ -0.6857   7.3839 -16.8436 -35.7308  48.2334]\n",
      "Weights: [-4.7847  0.7165 -1.2057  0.1118  0.1435]\n",
      "MSE loss: 91.63\n",
      "Iteration: 101400\n",
      "Gradient: [-3.7365 10.8909 31.9011 16.8531 -7.2058]\n",
      "Weights: [-4.7829  0.7147 -1.2051  0.1112  0.1437]\n",
      "MSE loss: 91.6045\n",
      "Iteration: 101500\n",
      "Gradient: [   8.9823    7.9243   -5.0068   76.1361 -186.2747]\n",
      "Weights: [-4.7866  0.7121 -1.2075  0.1111  0.144 ]\n",
      "MSE loss: 91.7474\n",
      "Iteration: 101600\n",
      "Gradient: [ -1.119   -8.4898  -5.7884 -64.0577  24.3996]\n",
      "Weights: [-4.7808  0.7087 -1.2059  0.1116  0.1438]\n",
      "MSE loss: 91.6136\n",
      "Iteration: 101700\n",
      "Gradient: [   4.8494   -7.2696   36.3116 -133.961   101.371 ]\n",
      "Weights: [-4.7723  0.7095 -1.2065  0.1122  0.1437]\n",
      "MSE loss: 91.7824\n",
      "Iteration: 101800\n",
      "Gradient: [ 12.0934  15.1399  21.7346 -63.7473 -51.7348]\n",
      "Weights: [-4.7782  0.7167 -1.208   0.1119  0.1437]\n",
      "MSE loss: 91.6849\n",
      "Iteration: 101900\n",
      "Gradient: [  3.4754  -6.9089  20.1043 -12.1054 140.5551]\n",
      "Weights: [-4.7681  0.7101 -1.2077  0.1121  0.1437]\n",
      "MSE loss: 91.8251\n",
      "Iteration: 102000\n",
      "Gradient: [   1.5519    0.2834    0.8841   18.482  -116.9969]\n",
      "Weights: [-4.7822  0.7091 -1.2064  0.1126  0.1435]\n",
      "MSE loss: 91.5919\n",
      "Iteration: 102100\n",
      "Gradient: [  -3.9201  -10.7884   -4.6329    1.4042 -244.2849]\n",
      "Weights: [-4.776   0.7037 -1.2089  0.1136  0.1435]\n",
      "MSE loss: 91.6917\n",
      "Iteration: 102200\n",
      "Gradient: [  1.1074   4.67   -16.4216 -95.5214 -97.0866]\n",
      "Weights: [-4.7896  0.7171 -1.2096  0.1135  0.1432]\n",
      "MSE loss: 91.6266\n",
      "Iteration: 102300\n",
      "Gradient: [ 12.3686  -1.023  -20.7033 -69.9152 401.1747]\n",
      "Weights: [-4.7867  0.719  -1.2103  0.1132  0.1433]\n",
      "MSE loss: 91.5793\n",
      "Iteration: 102400\n",
      "Gradient: [ -18.2348   -5.4498  -29.8556   51.7643 -276.054 ]\n",
      "Weights: [-4.7922  0.7138 -1.2098  0.1136  0.1432]\n",
      "MSE loss: 91.9794\n",
      "Iteration: 102500\n",
      "Gradient: [  -7.0255   14.798    -0.6753  -32.352  -103.0667]\n",
      "Weights: [-4.783   0.7291 -1.2132  0.1136  0.1431]\n",
      "MSE loss: 91.6813\n",
      "Iteration: 102600\n",
      "Gradient: [  6.6299  -6.1413 -14.3368 -71.7638 152.5874]\n",
      "Weights: [-4.7918  0.7178 -1.214   0.1144  0.1434]\n",
      "MSE loss: 91.8163\n",
      "Iteration: 102700\n",
      "Gradient: [  -4.2422   -5.3251  -17.6639  -48.3442 -145.8853]\n",
      "Weights: [-4.8063  0.7177 -1.2141  0.1156  0.1434]\n",
      "MSE loss: 92.2392\n",
      "Iteration: 102800\n",
      "Gradient: [ 16.1303  10.8263   3.3704 -84.7507 286.8608]\n",
      "Weights: [-4.7757  0.7187 -1.213   0.115   0.1428]\n",
      "MSE loss: 91.6357\n",
      "Iteration: 102900\n",
      "Gradient: [ -9.9982 -13.0912 -17.4044 -95.8161  70.32  ]\n",
      "Weights: [-4.7972  0.7248 -1.2166  0.1154  0.1431]\n",
      "MSE loss: 91.7948\n",
      "Iteration: 103000\n",
      "Gradient: [ -2.7196  19.5933 -16.3114 -96.2662 423.9324]\n",
      "Weights: [-4.815   0.7358 -1.2173  0.1154  0.143 ]\n",
      "MSE loss: 92.1244\n",
      "Iteration: 103100\n",
      "Gradient: [  6.7259 -28.6774 -49.9978  15.2099 176.0268]\n",
      "Weights: [-4.8018  0.7236 -1.2165  0.115   0.1432]\n",
      "MSE loss: 92.3488\n",
      "Iteration: 103200\n",
      "Gradient: [ -3.0408  -0.8328   2.6174  31.875  349.1474]\n",
      "Weights: [-4.7941  0.735  -1.2159  0.1151  0.1428]\n",
      "MSE loss: 91.5574\n",
      "Iteration: 103300\n",
      "Gradient: [  4.6855   3.4762 -24.6884 117.8165 102.7723]\n",
      "Weights: [-4.8012  0.7458 -1.2207  0.1154  0.1432]\n",
      "MSE loss: 91.6698\n",
      "Iteration: 103400\n",
      "Gradient: [ -2.2951  -3.0451   7.7446 -64.5891  40.2954]\n",
      "Weights: [-4.8155  0.7611 -1.2224  0.1141  0.1431]\n",
      "MSE loss: 91.7035\n",
      "Iteration: 103500\n",
      "Gradient: [-1.2912  5.8082 34.7151 41.9714 92.0163]\n",
      "Weights: [-4.824   0.7665 -1.2228  0.1134  0.1432]\n",
      "MSE loss: 91.8226\n",
      "Iteration: 103600\n",
      "Gradient: [  2.3378   6.3527 -25.602  -37.5104 -36.0441]\n",
      "Weights: [-4.8094  0.769  -1.2282  0.1138  0.1434]\n",
      "MSE loss: 91.7181\n",
      "Iteration: 103700\n",
      "Gradient: [  9.1216   1.1499   7.8055 -27.0326 566.0493]\n",
      "Weights: [-4.8176  0.7652 -1.2254  0.1136  0.1435]\n",
      "MSE loss: 91.6963\n",
      "Iteration: 103800\n",
      "Gradient: [  2.6971  17.3425   6.1877  44.0621 155.4657]\n",
      "Weights: [-4.8024  0.7631 -1.2269  0.1141  0.1436]\n",
      "MSE loss: 91.7818\n",
      "Iteration: 103900\n",
      "Gradient: [ 20.719    7.1304  -3.6896  48.0039 381.4286]\n",
      "Weights: [-4.7854  0.7589 -1.2267  0.1135  0.1436]\n",
      "MSE loss: 92.1496\n",
      "Iteration: 104000\n",
      "Gradient: [  0.5464   2.8833  -1.3824 -17.9709 115.6128]\n",
      "Weights: [-4.8161  0.7624 -1.2294  0.1133  0.1438]\n",
      "MSE loss: 92.1369\n",
      "Iteration: 104100\n",
      "Gradient: [ -5.9408  -2.6781  13.0725 -30.5877 215.2268]\n",
      "Weights: [-4.8245  0.7657 -1.2292  0.1143  0.144 ]\n",
      "MSE loss: 91.8581\n",
      "Iteration: 104200\n",
      "Gradient: [ -5.7541   1.4564 -19.0059 -60.6023 118.1863]\n",
      "Weights: [-4.808   0.7517 -1.2261  0.1141  0.1438]\n",
      "MSE loss: 91.6789\n",
      "Iteration: 104300\n",
      "Gradient: [-15.2945  -6.7853 -30.307  -75.8464 102.325 ]\n",
      "Weights: [-4.7954  0.7434 -1.2258  0.115   0.1437]\n",
      "MSE loss: 91.542\n",
      "Iteration: 104400\n",
      "Gradient: [  6.4791   8.9865  29.7475  35.4914 117.0325]\n",
      "Weights: [-4.7882  0.7409 -1.226   0.1152  0.144 ]\n",
      "MSE loss: 91.7284\n",
      "Iteration: 104500\n",
      "Gradient: [ -1.2922  -4.0126 -38.2946 -36.8956  47.9736]\n",
      "Weights: [-4.7947  0.7408 -1.2252  0.115   0.1439]\n",
      "MSE loss: 91.5531\n",
      "Iteration: 104600\n",
      "Gradient: [ 11.5912  -5.2942  35.8703  57.2305 211.2539]\n",
      "Weights: [-4.7814  0.7436 -1.2245  0.1149  0.1435]\n",
      "MSE loss: 91.8157\n",
      "Iteration: 104700\n",
      "Gradient: [-6.9497 10.311  34.8451  5.263  -9.2306]\n",
      "Weights: [-4.788   0.7484 -1.2253  0.1149  0.1436]\n",
      "MSE loss: 91.8315\n",
      "Iteration: 104800\n",
      "Gradient: [  -0.7761    2.8523  -25.8236 -133.6917  237.544 ]\n",
      "Weights: [-4.7857  0.7346 -1.2271  0.1168  0.1435]\n",
      "MSE loss: 91.529\n",
      "Iteration: 104900\n",
      "Gradient: [   5.914   -13.6861   14.2277 -170.8308   78.3838]\n",
      "Weights: [-4.7927  0.7454 -1.2302  0.1168  0.1435]\n",
      "MSE loss: 91.5181\n",
      "Iteration: 105000\n",
      "Gradient: [ -15.1197    4.1491    2.5519   85.2508 -367.3557]\n",
      "Weights: [-4.8056  0.7486 -1.2279  0.1164  0.1435]\n",
      "MSE loss: 91.5811\n",
      "Iteration: 105100\n",
      "Gradient: [   6.2204   -1.7555   47.7135 -132.1088 -142.1092]\n",
      "Weights: [-4.79    0.7513 -1.2309  0.1173  0.1432]\n",
      "MSE loss: 91.6076\n",
      "Iteration: 105200\n",
      "Gradient: [ -5.3921  15.2767  27.263   36.6525 191.9783]\n",
      "Weights: [-4.8016  0.7577 -1.2337  0.1177  0.1432]\n",
      "MSE loss: 91.4989\n",
      "Iteration: 105300\n",
      "Gradient: [  -1.12      2.0405  -43.7903   14.4766 -144.8071]\n",
      "Weights: [-4.788   0.7575 -1.2354  0.1182  0.1434]\n",
      "MSE loss: 91.8686\n",
      "Iteration: 105400\n",
      "Gradient: [ 4.84790e+00  3.91000e-02 -5.04968e+01 -1.02073e+02 -2.68110e+00]\n",
      "Weights: [-4.8031  0.7547 -1.2342  0.1173  0.1435]\n",
      "MSE loss: 91.5697\n",
      "Iteration: 105500\n",
      "Gradient: [   4.9491   18.818    10.7951 -114.2742  -99.6734]\n",
      "Weights: [-4.7941  0.7491 -1.2363  0.118   0.1437]\n",
      "MSE loss: 91.5325\n",
      "Iteration: 105600\n",
      "Gradient: [ -6.934   -1.3563  -0.6228 -65.6195 -92.0052]\n",
      "Weights: [-4.8081  0.763  -1.2367  0.118   0.1436]\n",
      "MSE loss: 91.5699\n",
      "Iteration: 105700\n",
      "Gradient: [ -8.7172  -1.7556 -33.7319  31.3062  47.1311]\n",
      "Weights: [-4.8139  0.7582 -1.2377  0.1184  0.1436]\n",
      "MSE loss: 91.8302\n",
      "Iteration: 105800\n",
      "Gradient: [  -4.3827   -3.7049   -2.8267  -10.0243 -134.8742]\n",
      "Weights: [-4.7917  0.7462 -1.2364  0.1198  0.1433]\n",
      "MSE loss: 91.4863\n",
      "Iteration: 105900\n",
      "Gradient: [  1.9996  -4.9421  -7.4997  17.3538 -66.7271]\n",
      "Weights: [-4.7796  0.7462 -1.237   0.1194  0.1432]\n",
      "MSE loss: 91.6116\n",
      "Iteration: 106000\n",
      "Gradient: [  4.6022  17.5105 -38.2653 -10.285  139.7392]\n",
      "Weights: [-4.7821  0.7426 -1.2334  0.1195  0.143 ]\n",
      "MSE loss: 91.5644\n",
      "Iteration: 106100\n",
      "Gradient: [   3.676     8.6453   26.3581   62.4329 -203.1178]\n",
      "Weights: [-4.7875  0.7409 -1.2338  0.1191  0.1432]\n",
      "MSE loss: 91.5379\n",
      "Iteration: 106200\n",
      "Gradient: [ -4.9859  19.6252  14.4102  41.743  472.2812]\n",
      "Weights: [-4.7844  0.7373 -1.2325  0.119   0.1433]\n",
      "MSE loss: 91.5106\n",
      "Iteration: 106300\n",
      "Gradient: [  2.483    7.601  -17.7522  60.5361  26.2663]\n",
      "Weights: [-4.7831  0.7436 -1.2319  0.1194  0.1427]\n",
      "MSE loss: 91.5666\n",
      "Iteration: 106400\n",
      "Gradient: [  9.4001  -1.7624   1.639  -78.728  -43.2124]\n",
      "Weights: [-4.7875  0.7494 -1.2331  0.1191  0.1427]\n",
      "MSE loss: 91.5592\n",
      "Iteration: 106500\n",
      "Gradient: [   5.6517    4.126    -4.2889   73.8187 -106.0537]\n",
      "Weights: [-4.7916  0.7516 -1.2335  0.1202  0.1427]\n",
      "MSE loss: 91.7019\n",
      "Iteration: 106600\n",
      "Gradient: [  9.0566  -2.9645   6.6234 -57.5167  44.4267]\n",
      "Weights: [-4.7921  0.7506 -1.235   0.12    0.1426]\n",
      "MSE loss: 91.4989\n",
      "Iteration: 106700\n",
      "Gradient: [ -6.7168   0.2009 -33.9188  18.337  138.7948]\n",
      "Weights: [-4.8086  0.7595 -1.2333  0.1201  0.1425]\n",
      "MSE loss: 91.5597\n",
      "Iteration: 106800\n",
      "Gradient: [  -8.6554   13.0154  -10.5589    4.0496 -304.9414]\n",
      "Weights: [-4.8075  0.7526 -1.2327  0.1191  0.1426]\n",
      "MSE loss: 91.8321\n",
      "Iteration: 106900\n",
      "Gradient: [  3.1666   1.2147  -7.5306  20.8061 -36.1117]\n",
      "Weights: [-4.8013  0.7506 -1.2301  0.1194  0.1426]\n",
      "MSE loss: 91.4877\n",
      "Iteration: 107000\n",
      "Gradient: [-3.1841  7.7058  2.7166 70.0234 69.8398]\n",
      "Weights: [-4.8057  0.7422 -1.23    0.1199  0.1427]\n",
      "MSE loss: 91.655\n",
      "Iteration: 107100\n",
      "Gradient: [ -6.6867   6.0026 -43.9867  11.4447 368.7898]\n",
      "Weights: [-4.8032  0.7529 -1.2301  0.1196  0.1423]\n",
      "MSE loss: 91.493\n",
      "Iteration: 107200\n",
      "Gradient: [ 4.1868  2.5386 44.2469 55.4063 86.1235]\n",
      "Weights: [-4.8089  0.7552 -1.2282  0.12    0.1422]\n",
      "MSE loss: 91.6528\n",
      "Iteration: 107300\n",
      "Gradient: [  -8.3402   -7.8269   -7.605   -30.6759 -146.1058]\n",
      "Weights: [-4.8037  0.7568 -1.2283  0.1194  0.142 ]\n",
      "MSE loss: 91.5882\n",
      "Iteration: 107400\n",
      "Gradient: [   4.4201   -7.9208  -30.2519 -114.6315  -54.2957]\n",
      "Weights: [-4.8113  0.7501 -1.2265  0.1204  0.1418]\n",
      "MSE loss: 91.5695\n",
      "Iteration: 107500\n",
      "Gradient: [  13.9029   -9.7795    4.904    43.1703 -151.983 ]\n",
      "Weights: [-4.7959  0.7449 -1.2275  0.1205  0.1418]\n",
      "MSE loss: 91.4978\n",
      "Iteration: 107600\n",
      "Gradient: [ -2.7089  -0.2354  21.0049 -87.302   42.6079]\n",
      "Weights: [-4.8047  0.747  -1.2294  0.1215  0.142 ]\n",
      "MSE loss: 91.5011\n",
      "Iteration: 107700\n",
      "Gradient: [  0.5285   0.4275  24.8188 -35.7923 195.1316]\n",
      "Weights: [-4.7886  0.748  -1.2352  0.1224  0.142 ]\n",
      "MSE loss: 91.4852\n",
      "Iteration: 107800\n",
      "Gradient: [ 11.2741  10.6482  18.0675 -21.3832 123.8662]\n",
      "Weights: [-4.7861  0.7442 -1.2353  0.123   0.1421]\n",
      "MSE loss: 91.5556\n",
      "Iteration: 107900\n",
      "Gradient: [ -3.5768  -5.9607  13.1871  -4.576  445.1174]\n",
      "Weights: [-4.8011  0.7441 -1.2328  0.1232  0.1419]\n",
      "MSE loss: 91.4663\n",
      "Iteration: 108000\n",
      "Gradient: [  -4.1699  -12.9172  -56.0056   33.9608 -189.0478]\n",
      "Weights: [-4.8004  0.7435 -1.2321  0.1229  0.1417]\n",
      "MSE loss: 91.4617\n",
      "Iteration: 108100\n",
      "Gradient: [   3.0346   10.2749   16.0131 -117.17     64.185 ]\n",
      "Weights: [-4.7866  0.7473 -1.2355  0.1222  0.1419]\n",
      "MSE loss: 91.5481\n",
      "Iteration: 108200\n",
      "Gradient: [  8.2869 -16.9566  59.7095  72.4541  15.624 ]\n",
      "Weights: [-4.7732  0.7404 -1.236   0.1232  0.142 ]\n",
      "MSE loss: 91.749\n",
      "Iteration: 108300\n",
      "Gradient: [  6.6481  13.9265  24.3817  -6.6963 -31.4453]\n",
      "Weights: [-4.7935  0.7445 -1.2335  0.1236  0.1419]\n",
      "MSE loss: 91.6809\n",
      "Iteration: 108400\n",
      "Gradient: [  -1.2572  -15.6374    0.9349  -63.5919 -143.864 ]\n",
      "Weights: [-4.7995  0.7461 -1.2364  0.1239  0.1416]\n",
      "MSE loss: 91.52\n",
      "Iteration: 108500\n",
      "Gradient: [   8.9771   -7.1337   19.1884  -66.4725 -143.4302]\n",
      "Weights: [-4.8091  0.7574 -1.2353  0.1236  0.1415]\n",
      "MSE loss: 91.4944\n",
      "Iteration: 108600\n",
      "Gradient: [  1.1965   9.2811  10.6027 -49.639   19.1161]\n",
      "Weights: [-4.7973  0.751  -1.2314  0.1228  0.1416]\n",
      "MSE loss: 91.6441\n",
      "Iteration: 108700\n",
      "Gradient: [   6.037    -2.089   -18.2385  -11.825  -124.6478]\n",
      "Weights: [-4.8053  0.7534 -1.2333  0.1221  0.1416]\n",
      "MSE loss: 91.541\n",
      "Iteration: 108800\n",
      "Gradient: [  10.276    25.1364   -1.0896   67.6407 -127.9197]\n",
      "Weights: [-4.8034  0.7568 -1.2332  0.1224  0.1414]\n",
      "MSE loss: 91.5112\n",
      "Iteration: 108900\n",
      "Gradient: [ -1.3203   8.8974  31.515   80.4584 103.0547]\n",
      "Weights: [-4.8022  0.7684 -1.2368  0.1224  0.1416]\n",
      "MSE loss: 91.7769\n",
      "Iteration: 109000\n",
      "Gradient: [ -4.3223  -9.4886  -9.9489 -25.8469 -41.9647]\n",
      "Weights: [-4.8125  0.7619 -1.2369  0.1221  0.1416]\n",
      "MSE loss: 91.7945\n",
      "Iteration: 109100\n",
      "Gradient: [  4.9833   6.2946 -20.4042  90.5971  89.31  ]\n",
      "Weights: [-4.7994  0.7629 -1.2377  0.1223  0.1419]\n",
      "MSE loss: 91.5863\n",
      "Iteration: 109200\n",
      "Gradient: [  0.0798 -19.2398  -3.7447 -50.0559  22.933 ]\n",
      "Weights: [-4.8157  0.7631 -1.2402  0.1226  0.142 ]\n",
      "MSE loss: 91.8304\n",
      "Iteration: 109300\n",
      "Gradient: [  -3.2782  -20.1663  -38.5883  -82.7781 -336.2065]\n",
      "Weights: [-4.8137  0.758  -1.2388  0.1234  0.142 ]\n",
      "MSE loss: 91.5779\n",
      "Iteration: 109400\n",
      "Gradient: [  10.9097   -0.8753    2.332   -15.0596 -243.6188]\n",
      "Weights: [-4.7862  0.7415 -1.2389  0.1238  0.1421]\n",
      "MSE loss: 91.4564\n",
      "Iteration: 109500\n",
      "Gradient: [ -12.4701   -9.6151  -39.963  -124.3639   10.0058]\n",
      "Weights: [-4.8225  0.7592 -1.241   0.1234  0.1421]\n",
      "MSE loss: 92.2845\n",
      "Iteration: 109600\n",
      "Gradient: [ -6.9896  -1.7147  13.0425  71.3149 -42.6353]\n",
      "Weights: [-4.803   0.7547 -1.2395  0.1239  0.142 ]\n",
      "MSE loss: 91.4161\n",
      "Iteration: 109700\n",
      "Gradient: [   9.741    -1.3735   37.0227  -46.0227 -182.9332]\n",
      "Weights: [-4.7825  0.7496 -1.2393  0.1237  0.1421]\n",
      "MSE loss: 91.6791\n",
      "Iteration: 109800\n",
      "Gradient: [   6.011   -12.0714   12.7481  -55.7636 -111.1547]\n",
      "Weights: [-4.807   0.7514 -1.2393  0.1243  0.142 ]\n",
      "MSE loss: 91.5104\n",
      "Iteration: 109900\n",
      "Gradient: [  -5.3766    8.6356  -26.8152   -8.494  -269.7267]\n",
      "Weights: [-4.7927  0.7514 -1.2421  0.1245  0.1418]\n",
      "MSE loss: 91.4382\n",
      "Iteration: 110000\n",
      "Gradient: [ -12.0315  -32.4475   -2.9598 -124.6214 -225.2509]\n",
      "Weights: [-4.7996  0.7539 -1.244   0.124   0.1421]\n",
      "MSE loss: 91.6383\n",
      "Iteration: 110100\n",
      "Gradient: [   2.774     3.6754   25.678   -11.7739 -135.8317]\n",
      "Weights: [-4.7899  0.7498 -1.2411  0.125   0.1421]\n",
      "MSE loss: 91.5774\n",
      "Iteration: 110200\n",
      "Gradient: [  -3.4908  -22.93    -22.5115  -33.423  -147.5172]\n",
      "Weights: [-4.7988  0.7492 -1.2431  0.1247  0.1423]\n",
      "MSE loss: 91.4687\n",
      "Iteration: 110300\n",
      "Gradient: [ 0.6959  1.447  16.4673 23.2614 81.2955]\n",
      "Weights: [-4.8136  0.7641 -1.2442  0.1247  0.142 ]\n",
      "MSE loss: 91.5162\n",
      "Iteration: 110400\n",
      "Gradient: [  3.9565   2.1287  28.7774 -99.5275 -19.4999]\n",
      "Weights: [-4.7938  0.7598 -1.2436  0.1232  0.1422]\n",
      "MSE loss: 91.4507\n",
      "Iteration: 110500\n",
      "Gradient: [ -0.5382 -11.1897  -4.8703 -32.2125 108.6332]\n",
      "Weights: [-4.7983  0.7555 -1.2441  0.1238  0.1426]\n",
      "MSE loss: 91.4324\n",
      "Iteration: 110600\n",
      "Gradient: [  -5.8512   24.1328   31.4224  -14.2547 -307.8497]\n",
      "Weights: [-4.7917  0.7637 -1.2466  0.1236  0.1423]\n",
      "MSE loss: 91.4832\n",
      "Iteration: 110700\n",
      "Gradient: [ -5.44    11.6942 -28.0072 -12.4935 -49.0346]\n",
      "Weights: [-4.8057  0.7682 -1.247   0.1235  0.1422]\n",
      "MSE loss: 91.5122\n",
      "Iteration: 110800\n",
      "Gradient: [ -7.8377  -4.552  -15.5316  63.1584  24.0348]\n",
      "Weights: [-4.8021  0.7809 -1.2529  0.1232  0.1425]\n",
      "MSE loss: 91.5133\n",
      "Iteration: 110900\n",
      "Gradient: [ -6.736  -10.7347 -12.821   22.5777  89.1247]\n",
      "Weights: [-4.8206  0.7806 -1.253   0.1241  0.1426]\n",
      "MSE loss: 91.6047\n",
      "Iteration: 111000\n",
      "Gradient: [ 12.4644  -1.642   35.7525 103.9756 -58.6739]\n",
      "Weights: [-4.8022  0.7753 -1.2533  0.1236  0.1428]\n",
      "MSE loss: 91.4226\n",
      "Iteration: 111100\n",
      "Gradient: [  2.8571 -18.0769 -17.4738 -24.3419 112.9838]\n",
      "Weights: [-4.8033  0.7821 -1.253   0.1229  0.1428]\n",
      "MSE loss: 91.51\n",
      "Iteration: 111200\n",
      "Gradient: [ -2.6954  17.1372  -7.296    5.6841 -94.6068]\n",
      "Weights: [-4.8009  0.7779 -1.2553  0.1233  0.1429]\n",
      "MSE loss: 91.485\n",
      "Iteration: 111300\n",
      "Gradient: [  2.6446  20.7712 -20.5775 -92.2999 -10.9993]\n",
      "Weights: [-4.7958  0.7786 -1.256   0.1244  0.1428]\n",
      "MSE loss: 91.5536\n",
      "Iteration: 111400\n",
      "Gradient: [ -2.0911 -18.2844  22.4697  68.713  176.5478]\n",
      "Weights: [-4.8077  0.772  -1.2558  0.1247  0.1429]\n",
      "MSE loss: 91.5835\n",
      "Iteration: 111500\n",
      "Gradient: [ -7.5328  -7.2929 -14.5407 -37.7024  46.2007]\n",
      "Weights: [-4.8053  0.7738 -1.2554  0.1251  0.1428]\n",
      "MSE loss: 91.4132\n",
      "Iteration: 111600\n",
      "Gradient: [ 4.355   3.5027 -1.9312 65.2799 29.9567]\n",
      "Weights: [-4.809   0.7842 -1.2552  0.1246  0.1425]\n",
      "MSE loss: 91.4415\n",
      "Iteration: 111700\n",
      "Gradient: [ -6.5718  -4.3283   1.1849 -19.0572 -50.5608]\n",
      "Weights: [-4.7969  0.7786 -1.2576  0.125   0.1427]\n",
      "MSE loss: 91.4501\n",
      "Iteration: 111800\n",
      "Gradient: [   0.3957   -7.9091    3.1599 -171.9307  143.3362]\n",
      "Weights: [-4.7971  0.7701 -1.2549  0.1247  0.143 ]\n",
      "MSE loss: 91.4459\n",
      "Iteration: 111900\n",
      "Gradient: [  6.8828  18.8459  55.6251  39.9961 351.0035]\n",
      "Weights: [-4.795   0.7823 -1.2595  0.125   0.1428]\n",
      "MSE loss: 91.5381\n",
      "Iteration: 112000\n",
      "Gradient: [ -8.4262  33.5297  29.3911 -39.2023 100.4169]\n",
      "Weights: [-4.8074  0.782  -1.2595  0.1251  0.1427]\n",
      "MSE loss: 91.458\n",
      "Iteration: 112100\n",
      "Gradient: [ -2.9553  -1.3985 -59.1217 -83.8287  -1.772 ]\n",
      "Weights: [-4.8012  0.7773 -1.2602  0.1258  0.1428]\n",
      "MSE loss: 91.4328\n",
      "Iteration: 112200\n",
      "Gradient: [  2.2537   7.6495 -66.9861 -45.2524  70.0143]\n",
      "Weights: [-4.8017  0.7868 -1.2606  0.1255  0.1427]\n",
      "MSE loss: 91.5045\n",
      "Iteration: 112300\n",
      "Gradient: [   2.9579    0.5957    2.0556  -36.5295 -165.5348]\n",
      "Weights: [-4.8148  0.7888 -1.2596  0.1248  0.1426]\n",
      "MSE loss: 91.5296\n",
      "Iteration: 112400\n",
      "Gradient: [  5.8555   9.4678   2.017  185.5674 -58.7716]\n",
      "Weights: [-4.7973  0.7924 -1.2617  0.1254  0.1425]\n",
      "MSE loss: 91.7661\n",
      "Iteration: 112500\n",
      "Gradient: [   8.0991   -7.8543   -4.3763  -39.3659 -327.5795]\n",
      "Weights: [-4.8021  0.7864 -1.2629  0.1255  0.1428]\n",
      "MSE loss: 91.4473\n",
      "Iteration: 112600\n",
      "Gradient: [  5.742   12.0982  13.4972 109.873  302.0473]\n",
      "Weights: [-4.8057  0.7882 -1.262   0.1254  0.1428]\n",
      "MSE loss: 91.416\n",
      "Iteration: 112700\n",
      "Gradient: [-4.3524 23.5911 19.8109 -9.0032 56.5764]\n",
      "Weights: [-4.8092  0.787  -1.2622  0.1257  0.1429]\n",
      "MSE loss: 91.411\n",
      "Iteration: 112800\n",
      "Gradient: [-1.3868 -7.0691 52.2262 86.3347 53.2625]\n",
      "Weights: [-4.7932  0.7707 -1.2587  0.126   0.1429]\n",
      "MSE loss: 91.4254\n",
      "Iteration: 112900\n",
      "Gradient: [  2.8361   0.6577  36.1859 141.634   88.009 ]\n",
      "Weights: [-4.8005  0.7844 -1.2595  0.1255  0.143 ]\n",
      "MSE loss: 91.8383\n",
      "Iteration: 113000\n",
      "Gradient: [  -4.9046    2.2843   14.9141   38.3521 -246.5551]\n",
      "Weights: [-4.8094  0.7829 -1.2609  0.1254  0.1429]\n",
      "MSE loss: 91.4531\n",
      "Iteration: 113100\n",
      "Gradient: [  -3.9143  -14.9571   21.1106   61.3087 -147.8788]\n",
      "Weights: [-4.8135  0.7752 -1.2587  0.1264  0.1427]\n",
      "MSE loss: 91.6047\n",
      "Iteration: 113200\n",
      "Gradient: [ -4.9028  15.4144   0.1946 -68.98   -28.1979]\n",
      "Weights: [-4.8079  0.7814 -1.2636  0.1267  0.1427]\n",
      "MSE loss: 91.5295\n",
      "Iteration: 113300\n",
      "Gradient: [-11.5417  -0.9269  25.4113  72.9601 146.5767]\n",
      "Weights: [-4.8118  0.7773 -1.2627  0.1275  0.1428]\n",
      "MSE loss: 91.5638\n",
      "Iteration: 113400\n",
      "Gradient: [ -8.9034 -11.488  -17.0168 -38.6156 -94.4123]\n",
      "Weights: [-4.8008  0.7815 -1.2606  0.1272  0.1424]\n",
      "MSE loss: 91.4492\n",
      "Iteration: 113500\n",
      "Gradient: [  0.9082  14.1056 -30.4427 -49.5847 109.5293]\n",
      "Weights: [-4.7943  0.78   -1.2638  0.1273  0.1426]\n",
      "MSE loss: 91.4374\n",
      "Iteration: 113600\n",
      "Gradient: [  0.1998 -32.0147 -49.1473  69.6622  74.9854]\n",
      "Weights: [-4.8054  0.7884 -1.2655  0.1266  0.1427]\n",
      "MSE loss: 91.4026\n",
      "Iteration: 113700\n",
      "Gradient: [ -11.7068    2.4092   13.6576  -17.4077 -381.9866]\n",
      "Weights: [-4.8126  0.8026 -1.2661  0.1263  0.1426]\n",
      "MSE loss: 91.5398\n",
      "Iteration: 113800\n",
      "Gradient: [   1.7033   -6.3041  -50.4759   48.8386 -209.6537]\n",
      "Weights: [-4.8188  0.7974 -1.2677  0.1265  0.1426]\n",
      "MSE loss: 91.6639\n",
      "Iteration: 113900\n",
      "Gradient: [-12.7536  -3.1593 -21.4401 -60.3391 -64.1448]\n",
      "Weights: [-4.8237  0.7907 -1.2678  0.1272  0.143 ]\n",
      "MSE loss: 91.8439\n",
      "Iteration: 114000\n",
      "Gradient: [  -6.5917   -7.1642   29.1246 -120.8071  257.1152]\n",
      "Weights: [-4.8115  0.7909 -1.2678  0.1278  0.1426]\n",
      "MSE loss: 91.4226\n",
      "Iteration: 114100\n",
      "Gradient: [  -2.1872    0.4108    0.8374  -15.0411 -215.4373]\n",
      "Weights: [-4.8053  0.7948 -1.2686  0.1278  0.1424]\n",
      "MSE loss: 91.4084\n",
      "Iteration: 114200\n",
      "Gradient: [  -6.7986   -1.0064  -33.0647  -18.5057 -273.7392]\n",
      "Weights: [-4.8296  0.7995 -1.2715  0.128   0.1426]\n",
      "MSE loss: 92.1183\n",
      "Iteration: 114300\n",
      "Gradient: [-8.4146 -1.2147 36.4121 31.4933 54.0943]\n",
      "Weights: [-4.8188  0.7997 -1.2726  0.1292  0.1424]\n",
      "MSE loss: 91.4469\n",
      "Iteration: 114400\n",
      "Gradient: [  -2.0561    6.3282   16.1015   27.8107 -212.2401]\n",
      "Weights: [-4.8236  0.8075 -1.2727  0.1294  0.142 ]\n",
      "MSE loss: 91.4718\n",
      "Iteration: 114500\n",
      "Gradient: [  4.8769  21.428   29.2995  41.3186 104.2696]\n",
      "Weights: [-4.7932  0.7999 -1.2719  0.1297  0.142 ]\n",
      "MSE loss: 92.0018\n",
      "Iteration: 114600\n",
      "Gradient: [ 10.7298   3.2014   9.751  162.4387 235.9819]\n",
      "Weights: [-4.8028  0.7907 -1.2735  0.131   0.1423]\n",
      "MSE loss: 91.3559\n",
      "Iteration: 114700\n",
      "Gradient: [  0.4494  22.7183  -5.9521 -86.0276  74.3524]\n",
      "Weights: [-4.8104  0.787  -1.2712  0.1305  0.1421]\n",
      "MSE loss: 91.4957\n",
      "Iteration: 114800\n",
      "Gradient: [  4.6727  36.5979  34.9775  38.7726 484.7287]\n",
      "Weights: [-4.7977  0.79   -1.2717  0.1301  0.1421]\n",
      "MSE loss: 91.398\n",
      "Iteration: 114900\n",
      "Gradient: [ -5.8759  24.3741 -14.2594 -48.8465 -42.7284]\n",
      "Weights: [-4.796   0.7833 -1.2719  0.1312  0.142 ]\n",
      "MSE loss: 91.3529\n",
      "Iteration: 115000\n",
      "Gradient: [ -20.2672    4.0351    4.3049   92.1031 -235.2868]\n",
      "Weights: [-4.8164  0.7838 -1.2705  0.1319  0.142 ]\n",
      "MSE loss: 91.5703\n",
      "Iteration: 115100\n",
      "Gradient: [ 10.7602 -15.3925  23.0809 -77.0056 -34.7812]\n",
      "Weights: [-4.8168  0.7925 -1.2704  0.1311  0.142 ]\n",
      "MSE loss: 91.4374\n",
      "Iteration: 115200\n",
      "Gradient: [  1.5017   0.9421   1.2063 -26.6276 106.276 ]\n",
      "Weights: [-4.7941  0.7884 -1.2754  0.1321  0.1421]\n",
      "MSE loss: 91.4203\n",
      "Iteration: 115300\n",
      "Gradient: [   1.032   -32.5035   36.2114 -112.8966  184.0765]\n",
      "Weights: [-4.8007  0.7843 -1.2763  0.1334  0.142 ]\n",
      "MSE loss: 91.3313\n",
      "Iteration: 115400\n",
      "Gradient: [   7.7001    5.8162   11.5845  -88.0409 -110.7589]\n",
      "Weights: [-4.8072  0.7903 -1.2755  0.1336  0.1416]\n",
      "MSE loss: 91.2971\n",
      "Iteration: 115500\n",
      "Gradient: [ -9.326  -19.2482 -12.2477  71.8846 156.922 ]\n",
      "Weights: [-4.823   0.8002 -1.2771  0.1332  0.1417]\n",
      "MSE loss: 91.463\n",
      "Iteration: 115600\n",
      "Gradient: [ -1.9645  15.9668   0.3576  24.9069 257.9332]\n",
      "Weights: [-4.8095  0.8117 -1.2787  0.1321  0.1417]\n",
      "MSE loss: 91.5546\n",
      "Iteration: 115700\n",
      "Gradient: [   6.6586  -14.7952    4.4698  -84.6203 -411.2156]\n",
      "Weights: [-4.8098  0.8028 -1.2768  0.1322  0.1416]\n",
      "MSE loss: 91.3436\n",
      "Iteration: 115800\n",
      "Gradient: [  -0.1796    1.5266   -1.0446   81.7867 -164.2377]\n",
      "Weights: [-4.8076  0.7961 -1.2775  0.132   0.1418]\n",
      "MSE loss: 91.4634\n",
      "Iteration: 115900\n",
      "Gradient: [  -6.9299   -1.7568   33.6535  -34.0304 -198.9996]\n",
      "Weights: [-4.8     0.7927 -1.2778  0.133   0.1419]\n",
      "MSE loss: 91.3352\n",
      "Iteration: 116000\n",
      "Gradient: [ -14.4494   -5.7813  -63.2092  -51.1915 -120.8866]\n",
      "Weights: [-4.7929  0.7845 -1.277   0.1327  0.1419]\n",
      "MSE loss: 91.4119\n",
      "Iteration: 116100\n",
      "Gradient: [  7.5869  -6.8061 -13.8617 -30.355  227.1146]\n",
      "Weights: [-4.7808  0.7892 -1.2787  0.1332  0.1417]\n",
      "MSE loss: 91.7578\n",
      "Iteration: 116200\n",
      "Gradient: [ -9.0491 -18.4726   4.8837  16.1766 -84.1965]\n",
      "Weights: [-4.8171  0.802  -1.2795  0.1333  0.1417]\n",
      "MSE loss: 91.3698\n",
      "Iteration: 116300\n",
      "Gradient: [ -11.0677   25.4706   18.0434   -0.8524 -103.2282]\n",
      "Weights: [-4.8216  0.804  -1.2814  0.1335  0.1419]\n",
      "MSE loss: 91.4725\n",
      "Iteration: 116400\n",
      "Gradient: [  1.856  -10.2441  18.1525 155.574  -19.6462]\n",
      "Weights: [-4.8     0.791  -1.2794  0.1343  0.1418]\n",
      "MSE loss: 91.3127\n",
      "Iteration: 116500\n",
      "Gradient: [ -2.9121  14.8001  13.8692 -36.4641 206.7113]\n",
      "Weights: [-4.8061  0.7966 -1.2805  0.1348  0.1415]\n",
      "MSE loss: 91.2706\n",
      "Iteration: 116600\n",
      "Gradient: [   0.1655   10.5069   22.479   145.3563 -143.151 ]\n",
      "Weights: [-4.7868  0.8004 -1.2854  0.1356  0.1417]\n",
      "MSE loss: 92.0435\n",
      "Iteration: 116700\n",
      "Gradient: [   2.6382   19.7834   -5.9131  -52.298  -106.3667]\n",
      "Weights: [-4.8242  0.8206 -1.288   0.1352  0.1416]\n",
      "MSE loss: 91.3556\n",
      "Iteration: 116800\n",
      "Gradient: [ -13.6723    1.7949   46.3222   45.8308 -225.5701]\n",
      "Weights: [-4.8316  0.8235 -1.2889  0.1346  0.1414]\n",
      "MSE loss: 91.6303\n",
      "Iteration: 116900\n",
      "Gradient: [  5.7137  -8.0374  17.0909 112.1389  71.3584]\n",
      "Weights: [-4.8137  0.8198 -1.289   0.1348  0.1414]\n",
      "MSE loss: 91.358\n",
      "Iteration: 117000\n",
      "Gradient: [-2.0108 -9.0894 14.022  61.6437 58.0942]\n",
      "Weights: [-4.819   0.8208 -1.2873  0.1346  0.1417]\n",
      "MSE loss: 91.5058\n",
      "Iteration: 117100\n",
      "Gradient: [   4.6209   -7.5456  -25.0877   18.4046 -135.2583]\n",
      "Weights: [-4.8343  0.8242 -1.286   0.1341  0.1415]\n",
      "MSE loss: 91.4895\n",
      "Iteration: 117200\n",
      "Gradient: [  5.1055  11.0414 -10.3875  82.8294  14.3084]\n",
      "Weights: [-4.8169  0.8212 -1.287   0.1343  0.1415]\n",
      "MSE loss: 91.3858\n",
      "Iteration: 117300\n",
      "Gradient: [  6.8947 -21.3979   3.5399 101.4443  37.4872]\n",
      "Weights: [-4.8122  0.8153 -1.2896  0.1344  0.1417]\n",
      "MSE loss: 91.4072\n",
      "Iteration: 117400\n",
      "Gradient: [ 0.252   4.2572 27.323   5.8918 72.6995]\n",
      "Weights: [-4.8194  0.8183 -1.2926  0.1355  0.1419]\n",
      "MSE loss: 91.3457\n",
      "Iteration: 117500\n",
      "Gradient: [ -6.0082  -4.8749 -29.3078  19.8123 112.0102]\n",
      "Weights: [-4.7996  0.8117 -1.2923  0.1357  0.1418]\n",
      "MSE loss: 91.3977\n",
      "Iteration: 117600\n",
      "Gradient: [-5.7824  0.2635 15.2415 44.0343 -1.6958]\n",
      "Weights: [-4.822   0.8312 -1.2934  0.1352  0.1418]\n",
      "MSE loss: 91.4424\n",
      "Iteration: 117700\n",
      "Gradient: [  8.2141 -15.0045  -2.8051 -18.3199 -79.9973]\n",
      "Weights: [-4.829   0.8279 -1.2898  0.1344  0.1418]\n",
      "MSE loss: 91.3977\n",
      "Iteration: 117800\n",
      "Gradient: [   2.9615    3.5658  -53.1012  -90.2633 -101.6119]\n",
      "Weights: [-4.8147  0.8238 -1.2922  0.1347  0.1417]\n",
      "MSE loss: 91.3885\n",
      "Iteration: 117900\n",
      "Gradient: [ -0.0875   2.3368 -12.4531  22.1948   5.0419]\n",
      "Weights: [-4.8079  0.8151 -1.2905  0.1348  0.1418]\n",
      "MSE loss: 91.3399\n",
      "Iteration: 118000\n",
      "Gradient: [   4.7254   -4.2106    5.669    -3.3879 -201.5536]\n",
      "Weights: [-4.7954  0.8022 -1.2918  0.1351  0.1421]\n",
      "MSE loss: 91.5808\n",
      "Iteration: 118100\n",
      "Gradient: [  -8.19     -9.6691  -29.8215 -132.3141    5.737 ]\n",
      "Weights: [-4.8082  0.7995 -1.2898  0.136   0.142 ]\n",
      "MSE loss: 91.4642\n",
      "Iteration: 118200\n",
      "Gradient: [  -1.7916   -4.1472  -63.8443   25.5771 -113.1197]\n",
      "Weights: [-4.8134  0.8078 -1.2915  0.1358  0.1419]\n",
      "MSE loss: 91.4573\n",
      "Iteration: 118300\n",
      "Gradient: [  5.7753 -17.9804  55.8193  96.4825 216.659 ]\n",
      "Weights: [-4.8241  0.8275 -1.2916  0.135   0.1419]\n",
      "MSE loss: 91.4254\n",
      "Iteration: 118400\n",
      "Gradient: [ -11.9986   -5.9652  -23.3793   30.6715 -143.1382]\n",
      "Weights: [-4.8223  0.8304 -1.2956  0.1346  0.1421]\n",
      "MSE loss: 91.3641\n",
      "Iteration: 118500\n",
      "Gradient: [ -6.4855  -2.5318 -36.9089  24.5499 -15.9109]\n",
      "Weights: [-4.8263  0.8326 -1.2979  0.1356  0.1421]\n",
      "MSE loss: 91.3615\n",
      "Iteration: 118600\n",
      "Gradient: [ -1.4481 -10.0301  29.0062 110.0413 144.0629]\n",
      "Weights: [-4.8041  0.8225 -1.2996  0.1365  0.1423]\n",
      "MSE loss: 91.4456\n",
      "Iteration: 118700\n",
      "Gradient: [   9.0679    8.6624   10.2623  -31.6631 -161.4719]\n",
      "Weights: [-4.8119  0.833  -1.3052  0.1367  0.1423]\n",
      "MSE loss: 91.3687\n",
      "Iteration: 118800\n",
      "Gradient: [  1.158  -12.9973 -20.4681 -55.834  239.5444]\n",
      "Weights: [-4.8167  0.8269 -1.3035  0.1368  0.1422]\n",
      "MSE loss: 91.5293\n",
      "Iteration: 118900\n",
      "Gradient: [  -6.6004    4.879     3.568   -27.6745 -368.4919]\n",
      "Weights: [-4.8135  0.8362 -1.3026  0.1355  0.1423]\n",
      "MSE loss: 91.4117\n",
      "Iteration: 119000\n",
      "Gradient: [ 4.10000e-02  1.35209e+01  4.85346e+01  2.92444e+01 -5.38365e+01]\n",
      "Weights: [-4.8217  0.8436 -1.3011  0.1351  0.1421]\n",
      "MSE loss: 91.4568\n",
      "Iteration: 119100\n",
      "Gradient: [  -2.8042   -3.548   -14.0276   -1.0132 -343.2333]\n",
      "Weights: [-4.8186  0.84   -1.3045  0.1365  0.1423]\n",
      "MSE loss: 91.4113\n",
      "Iteration: 119200\n",
      "Gradient: [ -5.519   -6.9584  19.8363  19.4714 -17.5474]\n",
      "Weights: [-4.808   0.8356 -1.3061  0.1366  0.1425]\n",
      "MSE loss: 91.4778\n",
      "Iteration: 119300\n",
      "Gradient: [   3.8174   -6.8739  -10.8687   10.0217 -177.2558]\n",
      "Weights: [-4.8195  0.8396 -1.3067  0.1365  0.1424]\n",
      "MSE loss: 91.3562\n",
      "Iteration: 119400\n",
      "Gradient: [-1.2835 13.9051  3.9092 52.771  10.1066]\n",
      "Weights: [-4.8106  0.8367 -1.3097  0.138   0.1421]\n",
      "MSE loss: 91.4396\n",
      "Iteration: 119500\n",
      "Gradient: [ -7.1969  -4.8312 -14.4096 -62.6746 -21.5152]\n",
      "Weights: [-4.8269  0.8404 -1.3096  0.1393  0.1419]\n",
      "MSE loss: 91.3503\n",
      "Iteration: 119600\n",
      "Gradient: [   8.3235  -11.7551  -11.4763  -18.051  -336.1849]\n",
      "Weights: [-4.8151  0.8334 -1.3095  0.1398  0.1418]\n",
      "MSE loss: 91.3088\n",
      "Iteration: 119700\n",
      "Gradient: [ -4.6081  10.4173  -8.4766 -33.0414  96.2684]\n",
      "Weights: [-4.8415  0.8472 -1.3108  0.1393  0.1419]\n",
      "MSE loss: 91.6626\n",
      "Iteration: 119800\n",
      "Gradient: [  1.6957  -6.9978 -35.9223  -3.5087  73.0767]\n",
      "Weights: [-4.821   0.847  -1.309   0.1383  0.1418]\n",
      "MSE loss: 91.3794\n",
      "Iteration: 119900\n",
      "Gradient: [ -4.5731 -22.4373 -62.5762 -58.1962 108.8063]\n",
      "Weights: [-4.8231  0.8447 -1.3095  0.1386  0.1417]\n",
      "MSE loss: 91.3668\n",
      "Iteration: 120000\n",
      "Gradient: [-14.0027  -0.4122 -11.1638  43.2009 -52.6752]\n",
      "Weights: [-4.8318  0.851  -1.3101  0.1378  0.1418]\n",
      "MSE loss: 91.517\n",
      "Iteration: 120100\n",
      "Gradient: [ -0.9333   9.0682  73.3741 -17.4815 104.3157]\n",
      "Weights: [-4.8095  0.8386 -1.3097  0.1385  0.142 ]\n",
      "MSE loss: 91.4055\n",
      "Iteration: 120200\n",
      "Gradient: [  2.5426  16.8294 -28.9346  83.0271 362.6423]\n",
      "Weights: [-4.8268  0.8407 -1.3082  0.1379  0.1421]\n",
      "MSE loss: 91.3901\n",
      "Iteration: 120300\n",
      "Gradient: [  6.7245  14.6476 -15.5087  29.019  -42.1071]\n",
      "Weights: [-4.8225  0.8354 -1.3067  0.1386  0.142 ]\n",
      "MSE loss: 91.3178\n",
      "Iteration: 120400\n",
      "Gradient: [ -14.3561  -12.0822  -56.9128 -230.8989   89.2888]\n",
      "Weights: [-4.8249  0.84   -1.3099  0.1396  0.1418]\n",
      "MSE loss: 91.3334\n",
      "Iteration: 120500\n",
      "Gradient: [ -8.6993   4.4641 -23.8367  84.835  292.965 ]\n",
      "Weights: [-4.8441  0.8493 -1.3097  0.1401  0.1415]\n",
      "MSE loss: 91.577\n",
      "Iteration: 120600\n",
      "Gradient: [ -0.4156  -3.6746 -25.8311 -70.6247 116.7179]\n",
      "Weights: [-4.8388  0.8538 -1.3119  0.1402  0.1414]\n",
      "MSE loss: 91.4073\n",
      "Iteration: 120700\n",
      "Gradient: [  12.2554   -8.8274    6.338   -89.1515 -202.6821]\n",
      "Weights: [-4.8058  0.8474 -1.3119  0.1403  0.1411]\n",
      "MSE loss: 91.7933\n",
      "Iteration: 120800\n",
      "Gradient: [ -1.6326   8.78    -7.1781  72.6835 251.4608]\n",
      "Weights: [-4.8181  0.8343 -1.3104  0.1407  0.1415]\n",
      "MSE loss: 91.3047\n",
      "Iteration: 120900\n",
      "Gradient: [ -3.9228   1.5266 -36.8698 147.2483 315.5607]\n",
      "Weights: [-4.8216  0.8296 -1.3076  0.141   0.1414]\n",
      "MSE loss: 91.3169\n",
      "Iteration: 121000\n",
      "Gradient: [  1.0792   4.747   -2.3649  16.0118 121.2044]\n",
      "Weights: [-4.8078  0.8297 -1.3052  0.1407  0.1414]\n",
      "MSE loss: 91.599\n",
      "Iteration: 121100\n",
      "Gradient: [-5.702   4.8828 31.0503  7.0738  2.829 ]\n",
      "Weights: [-4.8386  0.8438 -1.3076  0.1408  0.1412]\n",
      "MSE loss: 91.4406\n",
      "Iteration: 121200\n",
      "Gradient: [  -7.0503  -16.3212  -33.4124  146.8344 -670.364 ]\n",
      "Weights: [-4.8197  0.8258 -1.3067  0.1404  0.1414]\n",
      "MSE loss: 91.5515\n",
      "Iteration: 121300\n",
      "Gradient: [  8.7869   0.6168   9.8958 134.0625 417.3218]\n",
      "Weights: [-4.8144  0.8306 -1.306   0.1408  0.1412]\n",
      "MSE loss: 91.2614\n",
      "Iteration: 121400\n",
      "Gradient: [   7.7107    1.7968   25.8336   50.3531 -135.276 ]\n",
      "Weights: [-4.8115  0.8319 -1.3079  0.1414  0.141 ]\n",
      "MSE loss: 91.2612\n",
      "Iteration: 121500\n",
      "Gradient: [  -2.4655    2.6142    4.2758 -106.1828   -8.3878]\n",
      "Weights: [-4.8073  0.8291 -1.3089  0.142   0.1412]\n",
      "MSE loss: 91.3056\n",
      "Iteration: 121600\n",
      "Gradient: [ -4.0914  26.2457  -8.494   21.6634 251.6466]\n",
      "Weights: [-4.8249  0.8262 -1.3041  0.1421  0.1411]\n",
      "MSE loss: 91.3504\n",
      "Iteration: 121700\n",
      "Gradient: [  6.2324  -8.044  -48.0245   7.9228  49.1089]\n",
      "Weights: [-4.8203  0.8311 -1.3083  0.1421  0.141 ]\n",
      "MSE loss: 91.2389\n",
      "Iteration: 121800\n",
      "Gradient: [   5.7566    1.1056  -13.8009   -1.9469 -219.4297]\n",
      "Weights: [-4.8218  0.8402 -1.3096  0.1417  0.141 ]\n",
      "MSE loss: 91.2573\n",
      "Iteration: 121900\n",
      "Gradient: [  0.433   -1.1349   6.9145 -58.8674 -53.0868]\n",
      "Weights: [-4.811   0.8306 -1.3069  0.1411  0.1409]\n",
      "MSE loss: 91.3153\n",
      "Iteration: 122000\n",
      "Gradient: [-11.2393   8.7193  -2.5223  21.4895  10.6708]\n",
      "Weights: [-4.8365  0.8339 -1.3091  0.1419  0.1414]\n",
      "MSE loss: 91.657\n",
      "Iteration: 122100\n",
      "Gradient: [ -7.2313  -0.7867 -20.7644   7.4763 -70.8922]\n",
      "Weights: [-4.8252  0.8395 -1.3115  0.1423  0.1413]\n",
      "MSE loss: 91.3047\n",
      "Iteration: 122200\n",
      "Gradient: [  -8.4602   -3.4538  -58.965  -116.5079 -312.3869]\n",
      "Weights: [-4.8019  0.8191 -1.3107  0.1432  0.1411]\n",
      "MSE loss: 91.3295\n",
      "Iteration: 122300\n",
      "Gradient: [  -1.6214   15.2309  -24.1274   58.4509 -141.058 ]\n",
      "Weights: [-4.8288  0.8185 -1.3044  0.1428  0.141 ]\n",
      "MSE loss: 91.7032\n",
      "Iteration: 122400\n",
      "Gradient: [-5.480000e-02 -1.027670e+01  1.494640e+01 -9.506270e+01 -1.542331e+02]\n",
      "Weights: [-4.8071  0.8143 -1.3054  0.1429  0.141 ]\n",
      "MSE loss: 91.2272\n",
      "Iteration: 122500\n",
      "Gradient: [ -5.4694  -3.315  -38.5026  18.1508 -48.2821]\n",
      "Weights: [-4.8122  0.8102 -1.3048  0.1438  0.1408]\n",
      "MSE loss: 91.3736\n",
      "Iteration: 122600\n",
      "Gradient: [-12.9501  14.5707 -18.0359 -55.6964 190.2448]\n",
      "Weights: [-4.808   0.8108 -1.3047  0.1442  0.1406]\n",
      "MSE loss: 91.2094\n",
      "Iteration: 122700\n",
      "Gradient: [ -5.0212 -13.5861 -12.2328 -97.7737 101.3023]\n",
      "Weights: [-4.8124  0.8027 -1.3015  0.1443  0.1404]\n",
      "MSE loss: 91.5168\n",
      "Iteration: 122800\n",
      "Gradient: [ -2.5287  10.3099  -5.7048  -1.1831 204.9055]\n",
      "Weights: [-4.8006  0.8064 -1.2998  0.1439  0.1401]\n",
      "MSE loss: 91.1704\n",
      "Iteration: 122900\n",
      "Gradient: [  -1.9346   -5.448     6.4619   11.5542 -167.6914]\n",
      "Weights: [-4.8138  0.8176 -1.3008  0.1435  0.1402]\n",
      "MSE loss: 91.1604\n",
      "Iteration: 123000\n",
      "Gradient: [ -8.2818   2.8771 -42.5183  11.4291 198.3631]\n",
      "Weights: [-4.8243  0.8168 -1.3005  0.1433  0.1402]\n",
      "MSE loss: 91.3968\n",
      "Iteration: 123100\n",
      "Gradient: [-11.2268 -19.8317 -27.669    3.6163  74.0759]\n",
      "Weights: [-4.8207  0.8096 -1.2991  0.1442  0.1401]\n",
      "MSE loss: 91.3387\n",
      "Iteration: 123200\n",
      "Gradient: [ 11.5399  11.3007   6.1345 -91.821  -80.1289]\n",
      "Weights: [-4.8155  0.8004 -1.2991  0.1453  0.14  ]\n",
      "MSE loss: 91.4311\n",
      "Iteration: 123300\n",
      "Gradient: [ -7.1835   3.9991  -5.1923 -67.0876 -37.5962]\n",
      "Weights: [-4.7765  0.7834 -1.2963  0.1456  0.1399]\n",
      "MSE loss: 91.465\n",
      "Iteration: 123400\n",
      "Gradient: [  4.6051  -6.6897  47.0102  20.483  -31.0846]\n",
      "Weights: [-4.7768  0.7914 -1.2993  0.1458  0.14  ]\n",
      "MSE loss: 91.706\n",
      "Iteration: 123500\n",
      "Gradient: [ -6.7832 -10.3988  14.9407  81.2199  69.5887]\n",
      "Weights: [-4.7976  0.7938 -1.3003  0.146   0.1402]\n",
      "MSE loss: 91.2413\n",
      "Iteration: 123600\n",
      "Gradient: [ -9.553   -5.8091   7.4688 -14.6186 319.263 ]\n",
      "Weights: [-4.7922  0.7974 -1.3002  0.1462  0.1398]\n",
      "MSE loss: 91.2333\n",
      "Iteration: 123700\n",
      "Gradient: [  -3.6649    3.6064   -2.0956 -101.0015 -269.0106]\n",
      "Weights: [-4.8013  0.804  -1.2978  0.1446  0.1396]\n",
      "MSE loss: 91.1589\n",
      "Iteration: 123800\n",
      "Gradient: [ -3.0075  -3.8984  12.9729  60.909  134.0417]\n",
      "Weights: [-4.8133  0.8012 -1.297   0.1443  0.1399]\n",
      "MSE loss: 91.3901\n",
      "Iteration: 123900\n",
      "Gradient: [ -5.9332   8.8929 -11.7842 -19.2295  65.0181]\n",
      "Weights: [-4.8059  0.8029 -1.2979  0.1443  0.1399]\n",
      "MSE loss: 91.1436\n",
      "Iteration: 124000\n",
      "Gradient: [  -5.7723   -6.1464  -15.2585  -46.7415 -229.691 ]\n",
      "Weights: [-4.8     0.7877 -1.2919  0.1443  0.1397]\n",
      "MSE loss: 91.201\n",
      "Iteration: 124100\n",
      "Gradient: [ 2.113000e-01 -1.452080e+01  1.376300e+00  1.310114e+02  4.753645e+02]\n",
      "Weights: [-4.8023  0.7923 -1.2908  0.1438  0.1397]\n",
      "MSE loss: 91.1258\n",
      "Iteration: 124200\n",
      "Gradient: [ -2.3908 -14.8098 -36.4695 -11.2259 141.298 ]\n",
      "Weights: [-4.795   0.7906 -1.2898  0.144   0.1394]\n",
      "MSE loss: 91.1462\n",
      "Iteration: 124300\n",
      "Gradient: [  1.2371   7.183    0.9144 -26.1645  93.5929]\n",
      "Weights: [-4.8055  0.7941 -1.2907  0.1444  0.1394]\n",
      "MSE loss: 91.1096\n",
      "Iteration: 124400\n",
      "Gradient: [ -0.7618   9.9595 -12.8454 -56.5937 181.6535]\n",
      "Weights: [-4.8146  0.795  -1.2922  0.1447  0.1393]\n",
      "MSE loss: 91.5738\n",
      "Iteration: 124500\n",
      "Gradient: [  5.7847  -1.0794  -5.3951  96.8809 169.7411]\n",
      "Weights: [-4.7965  0.8013 -1.2929  0.1443  0.1394]\n",
      "MSE loss: 91.3462\n",
      "Iteration: 124600\n",
      "Gradient: [  -8.5701   -6.0444   25.163    18.6646 -179.9834]\n",
      "Weights: [-4.8001  0.7932 -1.2937  0.145   0.1394]\n",
      "MSE loss: 91.1697\n",
      "Iteration: 124700\n",
      "Gradient: [  -6.6884   -7.324    -0.4863 -153.6847 -291.9764]\n",
      "Weights: [-4.8055  0.796  -1.2907  0.145   0.1392]\n",
      "MSE loss: 91.1286\n",
      "Iteration: 124800\n",
      "Gradient: [ -0.4127 -20.7767 -24.7887  17.167   82.636 ]\n",
      "Weights: [-4.8078  0.7871 -1.2895  0.1454  0.1391]\n",
      "MSE loss: 91.261\n",
      "Iteration: 124900\n",
      "Gradient: [-2.7763  0.4209 20.9234 53.7722 21.4477]\n",
      "Weights: [-4.8064  0.7882 -1.2887  0.1445  0.1394]\n",
      "MSE loss: 91.1686\n",
      "Iteration: 125000\n",
      "Gradient: [  4.2083 -11.1832  69.0128  99.6065 167.2117]\n",
      "Weights: [-4.7982  0.7873 -1.2885  0.144   0.1395]\n",
      "MSE loss: 91.1141\n",
      "Iteration: 125100\n",
      "Gradient: [  4.8684 -11.8549 -63.6915  91.4618  75.8051]\n",
      "Weights: [-4.8057  0.7993 -1.2888  0.1429  0.1394]\n",
      "MSE loss: 91.1343\n",
      "Iteration: 125200\n",
      "Gradient: [  -3.2552   -5.9333    8.8497  -22.6249 -383.3803]\n",
      "Weights: [-4.8253  0.8065 -1.2879  0.1423  0.1393]\n",
      "MSE loss: 91.3757\n",
      "Iteration: 125300\n",
      "Gradient: [ 0.2092  7.287   0.3077 67.7283  0.7858]\n",
      "Weights: [-4.8232  0.8055 -1.2889  0.1433  0.1393]\n",
      "MSE loss: 91.2465\n",
      "Iteration: 125400\n",
      "Gradient: [   8.9612    3.5517   13.7468  -59.1431 -181.6382]\n",
      "Weights: [-4.7987  0.7978 -1.2883  0.1433  0.1393]\n",
      "MSE loss: 91.2456\n",
      "Iteration: 125500\n",
      "Gradient: [-0.9445 13.6733  2.4289 50.3213 87.6078]\n",
      "Weights: [-4.8119  0.7974 -1.2893  0.1431  0.1394]\n",
      "MSE loss: 91.2811\n",
      "Iteration: 125600\n",
      "Gradient: [   8.0311   -9.3513    0.5968  -28.8535 -206.2896]\n",
      "Weights: [-4.8098  0.8048 -1.29    0.1431  0.1392]\n",
      "MSE loss: 91.157\n",
      "Iteration: 125700\n",
      "Gradient: [ -3.5928  15.5443 -33.2655  31.9387 103.9417]\n",
      "Weights: [-4.8159  0.8075 -1.2959  0.1437  0.1397]\n",
      "MSE loss: 91.2401\n",
      "Iteration: 125800\n",
      "Gradient: [  -7.3415   -0.7353  -41.8502  -72.6252 -143.6138]\n",
      "Weights: [-4.8159  0.8184 -1.3013  0.1439  0.1398]\n",
      "MSE loss: 91.2067\n",
      "Iteration: 125900\n",
      "Gradient: [   1.4652    7.0097    7.6299 -138.9911 -123.5164]\n",
      "Weights: [-4.8161  0.8259 -1.3024  0.1432  0.1401]\n",
      "MSE loss: 91.2055\n",
      "Iteration: 126000\n",
      "Gradient: [  -2.7528   11.4242   19.8656   17.7925 -255.2568]\n",
      "Weights: [-4.8146  0.8233 -1.3003  0.1436  0.1399]\n",
      "MSE loss: 91.2609\n",
      "Iteration: 126100\n",
      "Gradient: [   4.0269   10.9063   14.5445 -108.852   215.4499]\n",
      "Weights: [-4.8212  0.8329 -1.3025  0.1428  0.1399]\n",
      "MSE loss: 91.2522\n",
      "Iteration: 126200\n",
      "Gradient: [  1.1159   4.6391 -10.6463  35.1065 142.6714]\n",
      "Weights: [-4.8223  0.8223 -1.3018  0.1433  0.14  ]\n",
      "MSE loss: 91.2813\n",
      "Iteration: 126300\n",
      "Gradient: [ -2.7921   9.696   32.7631   3.8754 -20.0492]\n",
      "Weights: [-4.8219  0.8329 -1.3011  0.1424  0.14  ]\n",
      "MSE loss: 91.2858\n",
      "Iteration: 126400\n",
      "Gradient: [ -1.1031   1.4141 -26.8491  13.0903  35.0969]\n",
      "Weights: [-4.8121  0.8345 -1.3052  0.1425  0.1403]\n",
      "MSE loss: 91.42\n",
      "Iteration: 126500\n",
      "Gradient: [ -0.9212  19.7768  16.2615 -22.2671 203.1288]\n",
      "Weights: [-4.7963  0.8299 -1.3077  0.1429  0.1404]\n",
      "MSE loss: 91.735\n",
      "Iteration: 126600\n",
      "Gradient: [   7.9409    9.2632    8.7323   18.205  -242.4213]\n",
      "Weights: [-4.8164  0.8333 -1.3095  0.1433  0.1406]\n",
      "MSE loss: 91.1969\n",
      "Iteration: 126700\n",
      "Gradient: [  1.9329   8.2845  19.9079 131.6285 -16.52  ]\n",
      "Weights: [-4.824   0.8324 -1.3117  0.1444  0.1408]\n",
      "MSE loss: 91.2766\n",
      "Iteration: 126800\n",
      "Gradient: [  0.7221  -8.2193  36.487   66.226  217.8464]\n",
      "Weights: [-4.8261  0.8307 -1.3115  0.144   0.141 ]\n",
      "MSE loss: 91.3523\n",
      "Iteration: 126900\n",
      "Gradient: [ -7.8833  -1.5334  17.6966 -50.5976 172.9768]\n",
      "Weights: [-4.8119  0.8274 -1.3124  0.1436  0.1409]\n",
      "MSE loss: 91.3017\n",
      "Iteration: 127000\n",
      "Gradient: [  -6.5108  -10.2458  -53.1882  -51.0555 -166.6048]\n",
      "Weights: [-4.83    0.8321 -1.3105  0.1443  0.1409]\n",
      "MSE loss: 91.4309\n",
      "Iteration: 127100\n",
      "Gradient: [ -14.38      2.1925   24.7704 -113.3734  102.2192]\n",
      "Weights: [-4.8192  0.8238 -1.31    0.1446  0.1407]\n",
      "MSE loss: 91.2825\n",
      "Iteration: 127200\n",
      "Gradient: [  3.9332 -27.3683  12.6152 -42.2112 314.6889]\n",
      "Weights: [-4.7912  0.8095 -1.3074  0.1445  0.1406]\n",
      "MSE loss: 91.3243\n",
      "Iteration: 127300\n",
      "Gradient: [   0.8988  -12.4061  -39.8725  -63.1027 -263.3464]\n",
      "Weights: [-4.8104  0.8079 -1.3042  0.1453  0.1403]\n",
      "MSE loss: 91.2493\n",
      "Iteration: 127400\n",
      "Gradient: [-10.0531  -4.9734 -72.1861 -87.8183 -64.0187]\n",
      "Weights: [-4.8124  0.8058 -1.303   0.1451  0.1401]\n",
      "MSE loss: 91.4653\n",
      "Iteration: 127500\n",
      "Gradient: [ -7.0684 -16.0083 -40.5815  44.2467 100.668 ]\n",
      "Weights: [-4.7975  0.8061 -1.3035  0.1452  0.14  ]\n",
      "MSE loss: 91.1922\n",
      "Iteration: 127600\n",
      "Gradient: [  4.1318 -15.2047   0.4007 -44.0669 137.1346]\n",
      "Weights: [-4.8086  0.8101 -1.3017  0.145   0.1401]\n",
      "MSE loss: 91.1832\n",
      "Iteration: 127700\n",
      "Gradient: [ -4.7476 -21.2942 -48.9719 -89.6652  89.3506]\n",
      "Weights: [-4.8094  0.7958 -1.299   0.145   0.1399]\n",
      "MSE loss: 91.7172\n",
      "Iteration: 127800\n",
      "Gradient: [  2.6275  -9.8004 -57.1796 -74.9258  71.0328]\n",
      "Weights: [-4.8156  0.8107 -1.3013  0.1449  0.1399]\n",
      "MSE loss: 91.2254\n",
      "Iteration: 127900\n",
      "Gradient: [ -3.924    2.613  -50.8023 -42.8788  63.5661]\n",
      "Weights: [-4.8299  0.8205 -1.3008  0.1446  0.1399]\n",
      "MSE loss: 91.3479\n",
      "Iteration: 128000\n",
      "Gradient: [  6.3176 -16.5455 -17.8281  52.7308  63.8679]\n",
      "Weights: [-4.814   0.8154 -1.2996  0.1443  0.1399]\n",
      "MSE loss: 91.1847\n",
      "Iteration: 128100\n",
      "Gradient: [  4.0567   2.552  -38.4505  39.4726 100.808 ]\n",
      "Weights: [-4.8     0.818  -1.3008  0.143   0.1399]\n",
      "MSE loss: 91.3776\n",
      "Iteration: 128200\n",
      "Gradient: [ -2.2394   8.3522  51.2529 160.291  185.828 ]\n",
      "Weights: [-4.802   0.8098 -1.2993  0.1444  0.1398]\n",
      "MSE loss: 91.1837\n",
      "Iteration: 128300\n",
      "Gradient: [-5.0204 15.5663  8.0276 36.5837 90.2683]\n",
      "Weights: [-4.7976  0.801  -1.3001  0.1453  0.1398]\n",
      "MSE loss: 91.1637\n",
      "Iteration: 128400\n",
      "Gradient: [ -3.6994  -3.513   25.1756  66.6746 123.3356]\n",
      "Weights: [-4.7988  0.8009 -1.2998  0.1461  0.1398]\n",
      "MSE loss: 91.2209\n",
      "Iteration: 128500\n",
      "Gradient: [-2.4283 -1.3808  8.2399 39.9816 89.5717]\n",
      "Weights: [-4.7897  0.8052 -1.301   0.145   0.1399]\n",
      "MSE loss: 91.4055\n",
      "Iteration: 128600\n",
      "Gradient: [ 16.1165   1.5094 -34.2526 -90.2717 -84.386 ]\n",
      "Weights: [-4.7984  0.8108 -1.3027  0.1454  0.1397]\n",
      "MSE loss: 91.2006\n",
      "Iteration: 128700\n",
      "Gradient: [  8.3703 -17.5084  16.046  138.1358  99.9392]\n",
      "Weights: [-4.7966  0.8045 -1.3038  0.1469  0.1396]\n",
      "MSE loss: 91.143\n",
      "Iteration: 128800\n",
      "Gradient: [-9.75600e-01 -1.16457e+01  2.26800e-01  1.44700e+00  3.37830e+02]\n",
      "Weights: [-4.8145  0.8101 -1.3024  0.1468  0.1396]\n",
      "MSE loss: 91.1405\n",
      "Iteration: 128900\n",
      "Gradient: [ -9.6945  11.2857 -34.4583  95.6826 -50.3103]\n",
      "Weights: [-4.7957  0.8018 -1.2997  0.1464  0.1395]\n",
      "MSE loss: 91.1956\n",
      "Iteration: 129000\n",
      "Gradient: [ 13.9411   1.029   -6.7469 -27.0965 132.8683]\n",
      "Weights: [-4.7962  0.7983 -1.2979  0.1474  0.1394]\n",
      "MSE loss: 91.6172\n",
      "Iteration: 129100\n",
      "Gradient: [ -2.3048  16.0458 -13.3176   8.1646  31.6259]\n",
      "Weights: [-4.7728  0.7911 -1.298   0.1473  0.1391]\n",
      "MSE loss: 91.8848\n",
      "Iteration: 129200\n",
      "Gradient: [   0.921   -20.4472  -30.8621 -112.6989  125.8897]\n",
      "Weights: [-4.8088  0.7996 -1.2999  0.1472  0.1393]\n",
      "MSE loss: 91.2159\n",
      "Iteration: 129300\n",
      "Gradient: [ 2.9237 -4.9004 62.8083 61.7062 31.6471]\n",
      "Weights: [-4.7998  0.8008 -1.3001  0.1478  0.1392]\n",
      "MSE loss: 91.112\n",
      "Iteration: 129400\n",
      "Gradient: [  -0.8428   -5.3946  -53.738    73.5448 -288.1648]\n",
      "Weights: [-4.8136  0.8027 -1.2974  0.1475  0.1391]\n",
      "MSE loss: 91.1549\n",
      "Iteration: 129500\n",
      "Gradient: [-12.425   -8.8871   1.1506 -55.8529  59.2977]\n",
      "Weights: [-4.8212  0.8001 -1.2982  0.1473  0.1393]\n",
      "MSE loss: 91.4769\n",
      "Iteration: 129600\n",
      "Gradient: [ -10.7527    4.3384  -13.0946  -25.5873 -313.6483]\n",
      "Weights: [-4.8094  0.8067 -1.3002  0.1466  0.1394]\n",
      "MSE loss: 91.0877\n",
      "Iteration: 129700\n",
      "Gradient: [-13.7813  -6.208  -51.8741  34.9388  40.3021]\n",
      "Weights: [-4.818   0.8104 -1.3022  0.1471  0.1394]\n",
      "MSE loss: 91.1855\n",
      "Iteration: 129800\n",
      "Gradient: [  4.6524  13.7746  -8.8033 -11.197  -33.2515]\n",
      "Weights: [-4.8073  0.8103 -1.3003  0.147   0.1389]\n",
      "MSE loss: 91.095\n",
      "Iteration: 129900\n",
      "Gradient: [   1.5457    6.2144  -28.153    33.4349 -106.0004]\n",
      "Weights: [-4.8122  0.8082 -1.3005  0.1473  0.139 ]\n",
      "MSE loss: 91.106\n",
      "Iteration: 130000\n",
      "Gradient: [  7.3331  19.276   29.644  -48.5532 115.2467]\n",
      "Weights: [-4.8009  0.8026 -1.3013  0.148   0.1392]\n",
      "MSE loss: 91.1222\n",
      "Iteration: 130100\n",
      "Gradient: [  -8.1353   -7.5951  -14.5641  -25.5135 -411.8949]\n",
      "Weights: [-4.8152  0.8112 -1.3038  0.1478  0.139 ]\n",
      "MSE loss: 91.2605\n",
      "Iteration: 130200\n",
      "Gradient: [  1.2517 -13.0494  10.4177 -45.881  164.5978]\n",
      "Weights: [-4.8294  0.8195 -1.3051  0.1476  0.1392]\n",
      "MSE loss: 91.4132\n",
      "Iteration: 130300\n",
      "Gradient: [  11.9113    1.9103  -11.0801  138.8418 -432.7475]\n",
      "Weights: [-4.8142  0.8145 -1.3066  0.148   0.1392]\n",
      "MSE loss: 91.1863\n",
      "Iteration: 130400\n",
      "Gradient: [  3.0187 -11.4633   4.4724  19.9116 -10.1524]\n",
      "Weights: [-4.8166  0.8222 -1.3064  0.1473  0.1393]\n",
      "MSE loss: 91.0938\n",
      "Iteration: 130500\n",
      "Gradient: [  9.3559  17.0518  -5.7531  89.851  128.0674]\n",
      "Weights: [-4.8061  0.8186 -1.305   0.1472  0.1392]\n",
      "MSE loss: 91.1544\n",
      "Iteration: 130600\n",
      "Gradient: [-10.7673  -5.0212  39.4222  66.9556 154.1461]\n",
      "Weights: [-4.7927  0.8078 -1.3046  0.1477  0.1393]\n",
      "MSE loss: 91.2921\n",
      "Iteration: 130700\n",
      "Gradient: [-11.827   14.662    9.0407  96.3801 -45.0658]\n",
      "Weights: [-4.8192  0.812  -1.3017  0.1473  0.1392]\n",
      "MSE loss: 91.1572\n",
      "Iteration: 130800\n",
      "Gradient: [  -5.9061   -2.1885   -1.1758  -45.357  -316.775 ]\n",
      "Weights: [-4.7927  0.8    -1.3021  0.1474  0.1393]\n",
      "MSE loss: 91.1613\n",
      "Iteration: 130900\n",
      "Gradient: [  -5.5165   17.8219    0.9981  -81.9521 -136.5235]\n",
      "Weights: [-4.8227  0.8047 -1.3022  0.1476  0.1394]\n",
      "MSE loss: 91.6517\n",
      "Iteration: 131000\n",
      "Gradient: [  5.8543 -22.8155  -2.2403  39.1927  76.5614]\n",
      "Weights: [-4.8155  0.8145 -1.3037  0.1475  0.1392]\n",
      "MSE loss: 91.094\n",
      "Iteration: 131100\n",
      "Gradient: [ -5.3581   6.8495  34.9569  76.1047 -16.8742]\n",
      "Weights: [-4.8189  0.8147 -1.3021  0.1473  0.1393]\n",
      "MSE loss: 91.1503\n",
      "Iteration: 131200\n",
      "Gradient: [ 11.4974  -2.5624   4.0002 -12.8383  62.2534]\n",
      "Weights: [-4.7922  0.8024 -1.3047  0.1485  0.1392]\n",
      "MSE loss: 91.1682\n",
      "Iteration: 131300\n",
      "Gradient: [-10.8793  -3.3997  -6.5112 -29.3537 -39.1517]\n",
      "Weights: [-4.8069  0.813  -1.308   0.1488  0.1391]\n",
      "MSE loss: 91.0876\n",
      "Iteration: 131400\n",
      "Gradient: [ 10.437   -7.6642 -26.31    26.0382   5.3158]\n",
      "Weights: [-4.7941  0.8063 -1.3024  0.1477  0.1391]\n",
      "MSE loss: 91.2676\n",
      "Iteration: 131500\n",
      "Gradient: [ -5.4727  19.0153  15.6169  -9.3278 166.3455]\n",
      "Weights: [-4.8066  0.8059 -1.2988  0.1472  0.1391]\n",
      "MSE loss: 91.1545\n",
      "Iteration: 131600\n",
      "Gradient: [  0.8865  -9.7956  12.1755  73.8612 -68.4807]\n",
      "Weights: [-4.8064  0.8028 -1.2962  0.1464  0.139 ]\n",
      "MSE loss: 91.076\n",
      "Iteration: 131700\n",
      "Gradient: [  0.7872  15.4997  20.6013  28.4356 -57.92  ]\n",
      "Weights: [-4.8159  0.8147 -1.2982  0.1459  0.1391]\n",
      "MSE loss: 91.1222\n",
      "Iteration: 131800\n",
      "Gradient: [ 10.4349  -4.746  -35.3422 -63.2606 307.3402]\n",
      "Weights: [-4.7965  0.7995 -1.2973  0.1464  0.1392]\n",
      "MSE loss: 91.1489\n",
      "Iteration: 131900\n",
      "Gradient: [-15.5019 -16.8547  60.3988 -18.9308  81.1947]\n",
      "Weights: [-4.8106  0.7968 -1.2988  0.1468  0.1392]\n",
      "MSE loss: 91.6163\n",
      "Iteration: 132000\n",
      "Gradient: [ 11.5609  -5.3093 -20.5632 -77.8817 -28.2583]\n",
      "Weights: [-4.7972  0.8    -1.2987  0.1464  0.1392]\n",
      "MSE loss: 91.1358\n",
      "Iteration: 132100\n",
      "Gradient: [ -5.029   -5.976   -6.074  100.7786 -33.892 ]\n",
      "Weights: [-4.8239  0.8067 -1.2992  0.1471  0.1391]\n",
      "MSE loss: 91.4228\n",
      "Iteration: 132200\n",
      "Gradient: [   4.3374   -2.5934   16.0362  -84.3341 -103.3014]\n",
      "Weights: [-4.805   0.8151 -1.301   0.1467  0.1393]\n",
      "MSE loss: 91.5294\n",
      "Iteration: 132300\n",
      "Gradient: [   3.0146  -27.8356   11.8984  -37.51   -236.2959]\n",
      "Weights: [-4.7967  0.8066 -1.3037  0.1476  0.1392]\n",
      "MSE loss: 91.144\n",
      "Iteration: 132400\n",
      "Gradient: [   4.6014   -7.9696    3.9883  -22.084  -156.1334]\n",
      "Weights: [-4.8045  0.8094 -1.3008  0.147   0.1392]\n",
      "MSE loss: 91.1433\n",
      "Iteration: 132500\n",
      "Gradient: [  2.1015 -30.9003  -8.4776  83.1359 -58.3309]\n",
      "Weights: [-4.8125  0.8128 -1.3037  0.1469  0.1394]\n",
      "MSE loss: 91.0977\n",
      "Iteration: 132600\n",
      "Gradient: [ 7.612  -2.1862  4.5958  8.4543  9.5677]\n",
      "Weights: [-4.8187  0.8233 -1.3024  0.1457  0.1396]\n",
      "MSE loss: 91.2882\n",
      "Iteration: 132700\n",
      "Gradient: [  5.2634   2.7543  -1.615   86.1732 -73.7223]\n",
      "Weights: [-4.8052  0.8172 -1.3044  0.1462  0.1393]\n",
      "MSE loss: 91.1972\n",
      "Iteration: 132800\n",
      "Gradient: [   3.2408    2.3216  -16.3368  -94.1513 -105.0914]\n",
      "Weights: [-4.8213  0.827  -1.3089  0.1472  0.1393]\n",
      "MSE loss: 91.1947\n",
      "Iteration: 132900\n",
      "Gradient: [ 13.1956  -1.6117  17.255   71.3258 -60.4989]\n",
      "Weights: [-4.8166  0.824  -1.3073  0.1488  0.1391]\n",
      "MSE loss: 91.2356\n",
      "Iteration: 133000\n",
      "Gradient: [  -7.571    -2.106   -35.3841 -131.2971 -335.2479]\n",
      "Weights: [-4.8104  0.8114 -1.3073  0.1497  0.1389]\n",
      "MSE loss: 91.0667\n",
      "Iteration: 133100\n",
      "Gradient: [  7.8182  -1.7803  14.8675 -61.4935  57.8211]\n",
      "Weights: [-4.8258  0.83   -1.3087  0.1491  0.1388]\n",
      "MSE loss: 91.148\n",
      "Iteration: 133200\n",
      "Gradient: [ -3.9241   8.693  -27.14   -64.819  262.4448]\n",
      "Weights: [-4.7995  0.8094 -1.3069  0.1498  0.1389]\n",
      "MSE loss: 91.1129\n",
      "Iteration: 133300\n",
      "Gradient: [  5.3936  -4.2558  57.4596  74.2779 -39.7547]\n",
      "Weights: [-4.8164  0.8045 -1.3032  0.1504  0.1388]\n",
      "MSE loss: 91.183\n",
      "Iteration: 133400\n",
      "Gradient: [   2.7254   -8.6953  -10.0871 -172.1706  143.6031]\n",
      "Weights: [-4.8122  0.8012 -1.3013  0.1496  0.1388]\n",
      "MSE loss: 91.1153\n",
      "Iteration: 133500\n",
      "Gradient: [  7.1123  17.7431  57.9101 -98.8342 295.0838]\n",
      "Weights: [-4.8122  0.8061 -1.3025  0.1508  0.1384]\n",
      "MSE loss: 91.0636\n",
      "Iteration: 133600\n",
      "Gradient: [   5.8721   25.9935   37.253   101.4911 -378.171 ]\n",
      "Weights: [-4.7858  0.8034 -1.3047  0.1515  0.1382]\n",
      "MSE loss: 91.6106\n",
      "Iteration: 133700\n",
      "Gradient: [  4.7167   6.7832   7.1649  55.9296 435.0938]\n",
      "Weights: [-4.8076  0.8144 -1.3049  0.1514  0.138 ]\n",
      "MSE loss: 91.147\n",
      "Iteration: 133800\n",
      "Gradient: [   5.9117    0.7907   19.2224  125.0543 -176.2071]\n",
      "Weights: [-4.8137  0.8056 -1.3022  0.1511  0.1378]\n",
      "MSE loss: 91.1484\n",
      "Iteration: 133900\n",
      "Gradient: [ -8.7514 -11.0485   0.7628 134.7455  55.422 ]\n",
      "Weights: [-4.814   0.8141 -1.3031  0.1511  0.1377]\n",
      "MSE loss: 91.0414\n",
      "Iteration: 134000\n",
      "Gradient: [  -1.6932    2.034     4.1172 -152.9099  138.1415]\n",
      "Weights: [-4.8111  0.8051 -1.3038  0.1525  0.1377]\n",
      "MSE loss: 90.997\n",
      "Iteration: 134100\n",
      "Gradient: [ -4.8254  -2.7303 -22.8114 -17.5352 154.1349]\n",
      "Weights: [-4.8169  0.7973 -1.3017  0.1525  0.1378]\n",
      "MSE loss: 91.2761\n",
      "Iteration: 134200\n",
      "Gradient: [  -4.3568  -11.5577  -27.2539  -52.2795 -297.82  ]\n",
      "Weights: [-4.8046  0.7955 -1.2993  0.1521  0.1375]\n",
      "MSE loss: 91.0244\n",
      "Iteration: 134300\n",
      "Gradient: [   6.3468   -3.5192   23.361    81.2391 -157.185 ]\n",
      "Weights: [-4.8104  0.7974 -1.3013  0.1534  0.1373]\n",
      "MSE loss: 91.0639\n",
      "Iteration: 134400\n",
      "Gradient: [   1.9198  -13.2335    3.6667   89.7391 -188.1675]\n",
      "Weights: [-4.8047  0.7961 -1.297   0.1537  0.1369]\n",
      "MSE loss: 91.056\n",
      "Iteration: 134500\n",
      "Gradient: [ -6.3463 -13.9482 -17.7748  -6.3201  50.7425]\n",
      "Weights: [-4.8178  0.7989 -1.2953  0.153   0.1368]\n",
      "MSE loss: 91.0706\n",
      "Iteration: 134600\n",
      "Gradient: [  9.0422   3.9379  38.8883  74.0806 -38.3775]\n",
      "Weights: [-4.8046  0.7869 -1.2955  0.1531  0.1371]\n",
      "MSE loss: 91.0258\n",
      "Iteration: 134700\n",
      "Gradient: [  2.7392  -8.0306  10.3096 -32.9444 165.0203]\n",
      "Weights: [-4.7939  0.7798 -1.294   0.1538  0.1367]\n",
      "MSE loss: 91.0093\n",
      "Iteration: 134800\n",
      "Gradient: [  -4.428    -7.7584   -0.9238   -5.5514 -463.7382]\n",
      "Weights: [-4.7748  0.7706 -1.292   0.1542  0.1367]\n",
      "MSE loss: 91.2704\n",
      "Iteration: 134900\n",
      "Gradient: [   0.2696  -15.3003    7.9268   80.6279 -174.9621]\n",
      "Weights: [-4.7928  0.7699 -1.2914  0.155   0.1365]\n",
      "MSE loss: 90.9588\n",
      "Iteration: 135000\n",
      "Gradient: [   3.647    -5.4444  -30.7562  123.7814 -234.1257]\n",
      "Weights: [-4.7931  0.7666 -1.2908  0.155   0.1367]\n",
      "MSE loss: 90.9865\n",
      "Iteration: 135100\n",
      "Gradient: [ -3.9737 -10.0721   4.1717  48.1754 216.273 ]\n",
      "Weights: [-4.8004  0.7637 -1.2855  0.1543  0.1366]\n",
      "MSE loss: 91.0779\n",
      "Iteration: 135200\n",
      "Gradient: [  4.549   -7.8995  13.9786  12.748  -24.4184]\n",
      "Weights: [-4.8022  0.7717 -1.2871  0.1541  0.1367]\n",
      "MSE loss: 91.1262\n",
      "Iteration: 135300\n",
      "Gradient: [-8.8762 -4.773  33.585   1.3055 94.6861]\n",
      "Weights: [-4.8025  0.7752 -1.2898  0.1539  0.1366]\n",
      "MSE loss: 91.0187\n",
      "Iteration: 135400\n",
      "Gradient: [ -1.1408  -8.2905 -10.8035  43.4887 201.0849]\n",
      "Weights: [-4.7877  0.7768 -1.2913  0.154   0.1367]\n",
      "MSE loss: 91.0836\n",
      "Iteration: 135500\n",
      "Gradient: [   8.8526   -3.1629   -6.6351   39.0704 -477.9677]\n",
      "Weights: [-4.7871  0.7641 -1.2882  0.1547  0.1365]\n",
      "MSE loss: 90.967\n",
      "Iteration: 135600\n",
      "Gradient: [  -1.3356   19.6386   31.0856  -20.8625 -283.9564]\n",
      "Weights: [-4.8004  0.7687 -1.2886  0.1555  0.1364]\n",
      "MSE loss: 91.0066\n",
      "Iteration: 135700\n",
      "Gradient: [  1.0106 -19.7291  13.0901 -66.3521 -71.2841]\n",
      "Weights: [-4.7873  0.7542 -1.2857  0.1558  0.1361]\n",
      "MSE loss: 91.0216\n",
      "Iteration: 135800\n",
      "Gradient: [   5.6274    4.7287    4.3338   43.1161 -272.5706]\n",
      "Weights: [-4.7769  0.7519 -1.2831  0.1555  0.1362]\n",
      "MSE loss: 91.1901\n",
      "Iteration: 135900\n",
      "Gradient: [-10.4571  -2.6013  -4.0452 -49.2808  98.2205]\n",
      "Weights: [-4.7859  0.7386 -1.2799  0.1554  0.136 ]\n",
      "MSE loss: 91.4798\n",
      "Iteration: 136000\n",
      "Gradient: [  2.5726  -4.4758 -17.328   -7.4251 257.0464]\n",
      "Weights: [-4.7847  0.7466 -1.2793  0.1548  0.1359]\n",
      "MSE loss: 91.0556\n",
      "Iteration: 136100\n",
      "Gradient: [ -1.0977  -9.7678  34.7671 -88.684   10.6249]\n",
      "Weights: [-4.7991  0.7569 -1.2806  0.1543  0.136 ]\n",
      "MSE loss: 91.1631\n",
      "Iteration: 136200\n",
      "Gradient: [  1.5178   4.663  -20.0755 -23.2081 252.7899]\n",
      "Weights: [-4.7871  0.7578 -1.2815  0.1549  0.1362]\n",
      "MSE loss: 91.2277\n",
      "Iteration: 136300\n",
      "Gradient: [ 2.979  11.4734 12.3501 36.9973 71.3558]\n",
      "Weights: [-4.7792  0.7455 -1.2797  0.1547  0.136 ]\n",
      "MSE loss: 91.0419\n",
      "Iteration: 136400\n",
      "Gradient: [ -7.609  -16.2548  51.3971 -12.1643 -11.7843]\n",
      "Weights: [-4.7782  0.7472 -1.2805  0.155   0.1359]\n",
      "MSE loss: 91.0081\n",
      "Iteration: 136500\n",
      "Gradient: [ -1.2706  10.7446 -25.8224  -1.5122  33.2179]\n",
      "Weights: [-4.7819  0.7443 -1.2782  0.1552  0.1359]\n",
      "MSE loss: 90.9839\n",
      "Iteration: 136600\n",
      "Gradient: [ -3.7727   5.2382  48.6596  95.8061 157.6694]\n",
      "Weights: [-4.7927  0.7431 -1.2776  0.1558  0.1356]\n",
      "MSE loss: 91.185\n",
      "Iteration: 136700\n",
      "Gradient: [  5.6684  -1.5795 -24.8955 -52.8352  45.4697]\n",
      "Weights: [-4.7766  0.7433 -1.2792  0.1569  0.1355]\n",
      "MSE loss: 91.0299\n",
      "Iteration: 136800\n",
      "Gradient: [  7.0463  20.6448  10.1097  64.0042 -85.1303]\n",
      "Weights: [-4.7696  0.7434 -1.2799  0.157   0.1354]\n",
      "MSE loss: 91.1971\n",
      "Iteration: 136900\n",
      "Gradient: [ -5.0663  -6.2079   9.1146  45.6914 218.9475]\n",
      "Weights: [-4.7838  0.7457 -1.2793  0.1571  0.1353]\n",
      "MSE loss: 90.9564\n",
      "Iteration: 137000\n",
      "Gradient: [  8.0764 -12.5055  21.0198 -76.8594  -7.6621]\n",
      "Weights: [-4.785   0.7452 -1.2786  0.1566  0.1353]\n",
      "MSE loss: 90.961\n",
      "Iteration: 137100\n",
      "Gradient: [  -7.1629    4.0916   -1.1582   -2.0316 -214.1027]\n",
      "Weights: [-4.7839  0.7431 -1.2742  0.1564  0.1352]\n",
      "MSE loss: 91.05\n",
      "Iteration: 137200\n",
      "Gradient: [ -12.8992  -15.4224   22.3649  -18.0272 -196.9108]\n",
      "Weights: [-4.7896  0.7562 -1.2798  0.1562  0.1353]\n",
      "MSE loss: 90.9567\n",
      "Iteration: 137300\n",
      "Gradient: [ -8.2669   8.3227 -19.393    4.7545  94.5027]\n",
      "Weights: [-4.7945  0.7545 -1.282   0.1575  0.135 ]\n",
      "MSE loss: 91.0109\n",
      "Iteration: 137400\n",
      "Gradient: [-6.6203  2.7586 20.0699 72.2824 25.3678]\n",
      "Weights: [-4.7871  0.7483 -1.2803  0.1589  0.1349]\n",
      "MSE loss: 90.9874\n",
      "Iteration: 137500\n",
      "Gradient: [ 0.8808 -7.7806 18.3852 25.7437 53.6726]\n",
      "Weights: [-4.7965  0.7648 -1.2857  0.1589  0.1347]\n",
      "MSE loss: 90.9174\n",
      "Iteration: 137600\n",
      "Gradient: [  10.4999  -11.6932  -57.2248 -172.5417 -124.8411]\n",
      "Weights: [-4.8048  0.7712 -1.2878  0.159   0.1348]\n",
      "MSE loss: 90.9518\n",
      "Iteration: 137700\n",
      "Gradient: [  -8.9127   12.8232  -41.9316  -62.646  -323.6262]\n",
      "Weights: [-4.8068  0.781  -1.2897  0.1588  0.1347]\n",
      "MSE loss: 90.9829\n",
      "Iteration: 137800\n",
      "Gradient: [  3.8504  10.6867  -3.6444 -17.6413 133.9115]\n",
      "Weights: [-4.7904  0.766  -1.2899  0.1591  0.1349]\n",
      "MSE loss: 90.937\n",
      "Iteration: 137900\n",
      "Gradient: [-10.0697  17.5832 -33.9247 108.0435  69.1993]\n",
      "Weights: [-4.7849  0.7574 -1.2899  0.16    0.1351]\n",
      "MSE loss: 90.8936\n",
      "Iteration: 138000\n",
      "Gradient: [  -1.5581  -18.0057   17.0287   47.0638 -117.6622]\n",
      "Weights: [-4.7952  0.7506 -1.2874  0.1598  0.1351]\n",
      "MSE loss: 91.2692\n",
      "Iteration: 138100\n",
      "Gradient: [-3.270000e-02  4.439200e+00 -1.668390e+01  3.804500e+01  1.220333e+02]\n",
      "Weights: [-4.7674  0.7462 -1.2871  0.1602  0.135 ]\n",
      "MSE loss: 91.1496\n",
      "Iteration: 138200\n",
      "Gradient: [ 12.1641  -4.3411   9.7883  78.0069 -88.0682]\n",
      "Weights: [-4.7744  0.7527 -1.2915  0.1612  0.1351]\n",
      "MSE loss: 91.0453\n",
      "Iteration: 138300\n",
      "Gradient: [  -9.2449  -19.5553  -16.8931  -30.3434 -114.9427]\n",
      "Weights: [-4.7889  0.7566 -1.2945  0.1619  0.135 ]\n",
      "MSE loss: 90.9645\n",
      "Iteration: 138400\n",
      "Gradient: [   7.3746   -1.0238  -22.5172  -44.7532 -129.8207]\n",
      "Weights: [-4.7926  0.7573 -1.2923  0.1627  0.1347]\n",
      "MSE loss: 90.9426\n",
      "Iteration: 138500\n",
      "Gradient: [  3.3843   1.7843  19.2516 -34.5395 104.1292]\n",
      "Weights: [-4.7684  0.7474 -1.2885  0.1618  0.1346]\n",
      "MSE loss: 91.1789\n",
      "Iteration: 138600\n",
      "Gradient: [ -14.5434  -19.7309    6.0098 -135.7418 -238.0572]\n",
      "Weights: [-4.7951  0.7519 -1.2902  0.1624  0.1343]\n",
      "MSE loss: 91.3646\n",
      "Iteration: 138700\n",
      "Gradient: [-14.7591   8.3406  -5.0063 -71.2855 191.533 ]\n",
      "Weights: [-4.7835  0.7494 -1.2896  0.1625  0.1344]\n",
      "MSE loss: 90.9073\n",
      "Iteration: 138800\n",
      "Gradient: [  7.8691  14.6499  20.3513  77.1967 154.2349]\n",
      "Weights: [-4.7965  0.7612 -1.2928  0.1625  0.1345]\n",
      "MSE loss: 90.9314\n",
      "Iteration: 138900\n",
      "Gradient: [  4.701    7.408    4.154  -84.1652 113.0849]\n",
      "Weights: [-4.7883  0.7602 -1.2929  0.1628  0.1344]\n",
      "MSE loss: 90.8475\n",
      "Iteration: 139000\n",
      "Gradient: [   6.572     4.0352  -33.5917   51.2663 -275.8428]\n",
      "Weights: [-4.7916  0.7595 -1.2943  0.1626  0.1344]\n",
      "MSE loss: 91.068\n",
      "Iteration: 139100\n",
      "Gradient: [ -6.8163  11.7065  13.0034  40.1296 142.4368]\n",
      "Weights: [-4.8079  0.7592 -1.2931  0.1631  0.1343]\n",
      "MSE loss: 91.7077\n",
      "Iteration: 139200\n",
      "Gradient: [ 15.3056   6.6573  50.5421 -27.0774 110.7594]\n",
      "Weights: [-4.7816  0.7651 -1.2954  0.1639  0.1342]\n",
      "MSE loss: 91.0885\n",
      "Iteration: 139300\n",
      "Gradient: [  2.3748  15.0254 -19.16   -15.0333 -63.2141]\n",
      "Weights: [-4.7737  0.7597 -1.296   0.1638  0.1343]\n",
      "MSE loss: 91.0782\n",
      "Iteration: 139400\n",
      "Gradient: [-15.5469  16.7443  20.309  -90.0752 226.1908]\n",
      "Weights: [-4.789   0.7604 -1.2972  0.163   0.1347]\n",
      "MSE loss: 90.9589\n",
      "Iteration: 139500\n",
      "Gradient: [   7.7824  -17.5203  -56.2542   55.7418 -273.576 ]\n",
      "Weights: [-4.7898  0.7623 -1.2962  0.1637  0.1345]\n",
      "MSE loss: 90.8714\n",
      "Iteration: 139600\n",
      "Gradient: [  -3.4186    6.0083   10.135  -162.3522  -94.3369]\n",
      "Weights: [-4.795   0.7705 -1.2951  0.1624  0.1344]\n",
      "MSE loss: 90.8511\n",
      "Iteration: 139700\n",
      "Gradient: [  0.7257  -1.2214 -33.4292 -14.8619 275.5018]\n",
      "Weights: [-4.7977  0.769  -1.2974  0.1628  0.1346]\n",
      "MSE loss: 90.9288\n",
      "Iteration: 139800\n",
      "Gradient: [ -8.9785  11.5775 -26.4593 118.7441 -51.4141]\n",
      "Weights: [-4.8051  0.7769 -1.2933  0.1615  0.1344]\n",
      "MSE loss: 90.9082\n",
      "Iteration: 139900\n",
      "Gradient: [-0.7373 12.868  73.9392 31.8632 87.9567]\n",
      "Weights: [-4.7895  0.776  -1.2959  0.1629  0.1341]\n",
      "MSE loss: 91.0846\n",
      "Iteration: 140000\n",
      "Gradient: [   9.695    -1.8934  -21.4154   -8.815  -117.5843]\n",
      "Weights: [-4.7666  0.7569 -1.2945  0.1637  0.1343]\n",
      "MSE loss: 91.4769\n",
      "Iteration: 140100\n",
      "Gradient: [   8.6701   -1.2151   -8.4799 -100.7274  263.5432]\n",
      "Weights: [-4.781   0.7585 -1.294   0.1635  0.1344]\n",
      "MSE loss: 90.9831\n",
      "Iteration: 140200\n",
      "Gradient: [ -3.0776  15.7588   7.5643 -42.4337 -19.5792]\n",
      "Weights: [-4.8023  0.7713 -1.2955  0.1623  0.1345]\n",
      "MSE loss: 90.9207\n",
      "Iteration: 140300\n",
      "Gradient: [   8.2972  -13.382   -23.7794 -109.0392 -235.9639]\n",
      "Weights: [-4.7998  0.7655 -1.2954  0.1627  0.1344]\n",
      "MSE loss: 91.1077\n",
      "Iteration: 140400\n",
      "Gradient: [   2.2629    4.4453   -9.4512  103.457  -112.0935]\n",
      "Weights: [-4.786   0.7616 -1.2912  0.1625  0.1343]\n",
      "MSE loss: 90.9706\n",
      "Iteration: 140500\n",
      "Gradient: [  0.9974  10.3967  31.7235   5.5461 -91.5373]\n",
      "Weights: [-4.7883  0.7541 -1.2915  0.1629  0.1345]\n",
      "MSE loss: 90.8842\n",
      "Iteration: 140600\n",
      "Gradient: [ -6.6911  10.0823 -17.5012  28.5149 204.7626]\n",
      "Weights: [-4.7798  0.7458 -1.2919  0.1639  0.1343]\n",
      "MSE loss: 90.9323\n",
      "Iteration: 140700\n",
      "Gradient: [-0.7101  0.6232  7.3998 42.5981 87.2348]\n",
      "Weights: [-4.7831  0.7576 -1.2941  0.164   0.1342]\n",
      "MSE loss: 90.8851\n",
      "Iteration: 140800\n",
      "Gradient: [ -4.3545  -3.1306  10.7572 -53.1918  93.7484]\n",
      "Weights: [-4.7838  0.7601 -1.2932  0.1628  0.1342]\n",
      "MSE loss: 90.9051\n",
      "Iteration: 140900\n",
      "Gradient: [  -9.8807  -16.5587   -7.3475 -130.5522 -195.7699]\n",
      "Weights: [-4.812   0.7822 -1.2924  0.162   0.1339]\n",
      "MSE loss: 90.996\n",
      "Iteration: 141000\n",
      "Gradient: [  -7.0743    7.9334  -71.2745  -65.6641 -268.0055]\n",
      "Weights: [-4.7865  0.758  -1.2897  0.1629  0.1339]\n",
      "MSE loss: 90.8681\n",
      "Iteration: 141100\n",
      "Gradient: [  4.8045  15.8647   6.8856  23.4618 134.9801]\n",
      "Weights: [-4.7851  0.7597 -1.29    0.1626  0.1339]\n",
      "MSE loss: 90.9013\n",
      "Iteration: 141200\n",
      "Gradient: [  14.1647   14.1014  -43.2045   24.069  -258.1139]\n",
      "Weights: [-4.799   0.762  -1.2916  0.1629  0.134 ]\n",
      "MSE loss: 91.0385\n",
      "Iteration: 141300\n",
      "Gradient: [-12.7838  -2.8103  -3.3721  -3.5677 184.3609]\n",
      "Weights: [-4.8022  0.7636 -1.2912  0.1637  0.134 ]\n",
      "MSE loss: 90.9424\n",
      "Iteration: 141400\n",
      "Gradient: [  1.1424  -4.2126  10.1524 -44.2262  27.176 ]\n",
      "Weights: [-4.7916  0.7636 -1.2936  0.1638  0.1342]\n",
      "MSE loss: 90.9209\n",
      "Iteration: 141500\n",
      "Gradient: [ -2.2575   2.1921  -4.424  -88.2909  13.3558]\n",
      "Weights: [-4.7901  0.7561 -1.2929  0.1636  0.1342]\n",
      "MSE loss: 90.8962\n",
      "Iteration: 141600\n",
      "Gradient: [  -8.2469  -10.6469    1.897  -111.1151   75.3776]\n",
      "Weights: [-4.7941  0.7541 -1.292   0.1636  0.1342]\n",
      "MSE loss: 91.0166\n",
      "Iteration: 141700\n",
      "Gradient: [ -3.2013  -1.3702  17.6139 -14.9406   7.2436]\n",
      "Weights: [-4.7914  0.7527 -1.2921  0.1634  0.1344]\n",
      "MSE loss: 90.9571\n",
      "Iteration: 141800\n",
      "Gradient: [  -9.4279    6.5896  -27.8936  -59.7735 -152.4367]\n",
      "Weights: [-4.7849  0.7666 -1.2965  0.1634  0.1343]\n",
      "MSE loss: 90.8825\n",
      "Iteration: 141900\n",
      "Gradient: [   6.7878    1.9107    0.9612   29.8521 -159.298 ]\n",
      "Weights: [-4.8007  0.7767 -1.2958  0.1634  0.1341]\n",
      "MSE loss: 90.9477\n",
      "Iteration: 142000\n",
      "Gradient: [12.4473 11.2242 35.822  37.2181 64.6639]\n",
      "Weights: [-4.7977  0.7674 -1.2935  0.1638  0.1341]\n",
      "MSE loss: 90.9137\n",
      "Iteration: 142100\n",
      "Gradient: [ -6.3039   6.7069 -45.0786  14.0228  53.3744]\n",
      "Weights: [-4.8013  0.7584 -1.2927  0.1643  0.1339]\n",
      "MSE loss: 91.1134\n",
      "Iteration: 142200\n",
      "Gradient: [ -3.6701  -1.5473  -4.69   -42.4207 302.9101]\n",
      "Weights: [-4.7972  0.7649 -1.2943  0.1641  0.1341]\n",
      "MSE loss: 90.8983\n",
      "Iteration: 142300\n",
      "Gradient: [-2.022000e-01  5.664000e+00  3.312910e+01  2.698540e+01  2.659887e+02]\n",
      "Weights: [-4.787   0.7544 -1.2926  0.1636  0.1344]\n",
      "MSE loss: 90.8744\n",
      "Iteration: 142400\n",
      "Gradient: [ -2.9904 -11.7787  38.1663 -65.8254 -31.4485]\n",
      "Weights: [-4.7974  0.7564 -1.2953  0.164   0.1343]\n",
      "MSE loss: 91.3022\n",
      "Iteration: 142500\n",
      "Gradient: [ -3.9189  -8.8465 -12.6138 -45.1152 -94.4416]\n",
      "Weights: [-4.7835  0.7622 -1.2939  0.1624  0.1346]\n",
      "MSE loss: 90.9182\n",
      "Iteration: 142600\n",
      "Gradient: [ -4.482    7.8359  35.6456  -0.1284 -11.5556]\n",
      "Weights: [-4.7963  0.7657 -1.294   0.1626  0.1344]\n",
      "MSE loss: 90.8692\n",
      "Iteration: 142700\n",
      "Gradient: [-3.3591 -4.5026 -4.3505 50.728  15.7331]\n",
      "Weights: [-4.8021  0.7649 -1.2944  0.1634  0.1345]\n",
      "MSE loss: 91.0033\n",
      "Iteration: 142800\n",
      "Gradient: [   2.6233    1.9747  -31.2124   89.0232 -237.4008]\n",
      "Weights: [-4.7962  0.7651 -1.2976  0.1638  0.1344]\n",
      "MSE loss: 90.9227\n",
      "Iteration: 142900\n",
      "Gradient: [  3.9189   6.438   -1.7773  17.4586 -58.8611]\n",
      "Weights: [-4.7868  0.7679 -1.2946  0.1631  0.1342]\n",
      "MSE loss: 90.9335\n",
      "Iteration: 143000\n",
      "Gradient: [-8.060000e-02 -1.327210e+01  3.024420e+01  1.112124e+02  1.770103e+02]\n",
      "Weights: [-4.7914  0.7681 -1.2962  0.164   0.1341]\n",
      "MSE loss: 90.8649\n",
      "Iteration: 143100\n",
      "Gradient: [  9.3837  -4.5189  42.4231  46.2695 267.4766]\n",
      "Weights: [-4.8166  0.7845 -1.296   0.164   0.1338]\n",
      "MSE loss: 91.0502\n",
      "Iteration: 143200\n",
      "Gradient: [  3.4537  -2.8589  39.7002  56.1766 231.6289]\n",
      "Weights: [-4.797   0.7804 -1.3002  0.1644  0.134 ]\n",
      "MSE loss: 90.8769\n",
      "Iteration: 143300\n",
      "Gradient: [ 10.0037  -6.9037  13.5394  89.6858 278.2944]\n",
      "Weights: [-4.7837  0.772  -1.3012  0.1649  0.134 ]\n",
      "MSE loss: 90.9054\n",
      "Iteration: 143400\n",
      "Gradient: [ -10.9148   -9.2195  -45.4353  -42.6523 -107.6602]\n",
      "Weights: [-4.8229  0.7874 -1.3033  0.1648  0.134 ]\n",
      "MSE loss: 91.4747\n",
      "Iteration: 143500\n",
      "Gradient: [   4.1681   -3.7106  -53.8402   25.6127 -322.7881]\n",
      "Weights: [-4.8091  0.782  -1.3019  0.1642  0.134 ]\n",
      "MSE loss: 91.09\n",
      "Iteration: 143600\n",
      "Gradient: [ -0.3565   7.903  -28.8158 -10.8681 -59.0866]\n",
      "Weights: [-4.8115  0.7839 -1.3021  0.1643  0.134 ]\n",
      "MSE loss: 91.0527\n",
      "Iteration: 143700\n",
      "Gradient: [  1.3954   1.228    4.2866 -20.2299 -40.0675]\n",
      "Weights: [-4.7875  0.7852 -1.3055  0.1649  0.1341]\n",
      "MSE loss: 91.0574\n",
      "Iteration: 143800\n",
      "Gradient: [  0.4068   2.0414  -3.8027 -47.651   74.3315]\n",
      "Weights: [-4.8096  0.7893 -1.3063  0.1659  0.1343]\n",
      "MSE loss: 91.0572\n",
      "Iteration: 143900\n",
      "Gradient: [  5.1004  -5.9632 -56.0972 -60.3336 -21.8847]\n",
      "Weights: [-4.789   0.7767 -1.3084  0.1665  0.1344]\n",
      "MSE loss: 90.8395\n",
      "Iteration: 144000\n",
      "Gradient: [  0.1057  17.3266  27.0997  20.976  -69.385 ]\n",
      "Weights: [-4.7978  0.7754 -1.3086  0.1668  0.1345]\n",
      "MSE loss: 90.8607\n",
      "Iteration: 144100\n",
      "Gradient: [ 13.6254  -1.0572 -27.8284 -38.5039 367.2434]\n",
      "Weights: [-4.7941  0.7771 -1.3138  0.167   0.1345]\n",
      "MSE loss: 91.2106\n",
      "Iteration: 144200\n",
      "Gradient: [ 4.2518  7.2481 23.1079 69.4528 31.375 ]\n",
      "Weights: [-4.7852  0.7855 -1.314   0.1669  0.1346]\n",
      "MSE loss: 90.9782\n",
      "Iteration: 144300\n",
      "Gradient: [   6.1022    4.0745   10.8981   63.8269 -355.0386]\n",
      "Weights: [-4.8082  0.7952 -1.3162  0.1669  0.1346]\n",
      "MSE loss: 90.8389\n",
      "Iteration: 144400\n",
      "Gradient: [   3.8941   -9.7803  -17.3696  -72.2053 -393.166 ]\n",
      "Weights: [-4.7785  0.7893 -1.319   0.1671  0.1348]\n",
      "MSE loss: 91.0357\n",
      "Iteration: 144500\n",
      "Gradient: [  4.3288 -26.7154  -8.1255 -46.6942 -49.825 ]\n",
      "Weights: [-4.795   0.7897 -1.3188  0.1678  0.1347]\n",
      "MSE loss: 90.768\n",
      "Iteration: 144600\n",
      "Gradient: [  -8.4004   -5.2712   15.2577 -144.5801 -347.9906]\n",
      "Weights: [-4.8013  0.7968 -1.3175  0.1674  0.1344]\n",
      "MSE loss: 90.7502\n",
      "Iteration: 144700\n",
      "Gradient: [  8.4381  16.1579 -21.8642  18.6521  79.5309]\n",
      "Weights: [-4.7955  0.7859 -1.3135  0.1683  0.1342]\n",
      "MSE loss: 90.8416\n",
      "Iteration: 144800\n",
      "Gradient: [  0.9573 -11.879   17.0403 -41.6869 236.5189]\n",
      "Weights: [-4.8041  0.7817 -1.3126  0.1693  0.1338]\n",
      "MSE loss: 90.8617\n",
      "Iteration: 144900\n",
      "Gradient: [   3.6842   14.6305    1.8041   48.5107 -126.1773]\n",
      "Weights: [-4.8003  0.7857 -1.3131  0.1697  0.1335]\n",
      "MSE loss: 90.7385\n",
      "Iteration: 145000\n",
      "Gradient: [ -1.5153  -5.7614  -7.8551  44.2027 100.1191]\n",
      "Weights: [-4.7923  0.7844 -1.3137  0.1699  0.1336]\n",
      "MSE loss: 90.777\n",
      "Iteration: 145100\n",
      "Gradient: [  4.345   -3.1353  25.2347 -12.2617 132.5408]\n",
      "Weights: [-4.8     0.7893 -1.3143  0.1697  0.1335]\n",
      "MSE loss: 90.7318\n",
      "Iteration: 145200\n",
      "Gradient: [  5.1332  -1.2879 -17.6386  86.4525 252.6588]\n",
      "Weights: [-4.8001  0.7915 -1.3135  0.1694  0.1336]\n",
      "MSE loss: 90.8577\n",
      "Iteration: 145300\n",
      "Gradient: [ -6.2179   6.6359 -10.9235  -4.1256 180.535 ]\n",
      "Weights: [-4.811   0.7884 -1.3169  0.1706  0.1334]\n",
      "MSE loss: 91.1381\n",
      "Iteration: 145400\n",
      "Gradient: [ 7.4458 15.788  -8.7287 37.5518 -2.4932]\n",
      "Weights: [-4.8003  0.7903 -1.3156  0.1704  0.1333]\n",
      "MSE loss: 90.7225\n",
      "Iteration: 145500\n",
      "Gradient: [-8.384   1.6213 36.7741 39.0015  6.3775]\n",
      "Weights: [-4.7928  0.7869 -1.3144  0.1702  0.1332]\n",
      "MSE loss: 90.7624\n",
      "Iteration: 145600\n",
      "Gradient: [ -9.9782  19.8915   6.7105 -17.8724 -45.7528]\n",
      "Weights: [-4.7988  0.7871 -1.3146  0.1698  0.1333]\n",
      "MSE loss: 90.8882\n",
      "Iteration: 145700\n",
      "Gradient: [  -6.9646  -23.2709  -49.8898 -153.1005 -197.5322]\n",
      "Weights: [-4.7972  0.788  -1.317   0.1699  0.1336]\n",
      "MSE loss: 90.7495\n",
      "Iteration: 145800\n",
      "Gradient: [  5.5414  17.5239  22.5156 147.7556 316.2246]\n",
      "Weights: [-4.7846  0.7915 -1.3189  0.1701  0.1337]\n",
      "MSE loss: 90.9925\n",
      "Iteration: 145900\n",
      "Gradient: [ 11.0658   8.0997  37.8842 -34.8652 131.5936]\n",
      "Weights: [-4.7885  0.7992 -1.3182  0.1697  0.1338]\n",
      "MSE loss: 91.6166\n",
      "Iteration: 146000\n",
      "Gradient: [  13.2691  -29.2303   25.762    85.6879 -388.8393]\n",
      "Weights: [-4.7895  0.7867 -1.3191  0.1702  0.1335]\n",
      "MSE loss: 90.9572\n",
      "Iteration: 146100\n",
      "Gradient: [  9.1047  17.317  -35.8237  72.3213 141.8339]\n",
      "Weights: [-4.7806  0.7862 -1.3178  0.1706  0.1337]\n",
      "MSE loss: 91.1366\n",
      "Iteration: 146200\n",
      "Gradient: [   4.1816  -13.3306    6.2075  -49.979  -249.7413]\n",
      "Weights: [-4.7915  0.788  -1.3207  0.1703  0.1338]\n",
      "MSE loss: 90.7922\n",
      "Iteration: 146300\n",
      "Gradient: [-14.5892  -7.2868  -4.1478  98.0387 210.6279]\n",
      "Weights: [-4.8112  0.7895 -1.3205  0.1714  0.1338]\n",
      "MSE loss: 91.0595\n",
      "Iteration: 146400\n",
      "Gradient: [  10.6446   -2.9441    2.979    76.7676 -150.0971]\n",
      "Weights: [-4.7981  0.8    -1.3236  0.1717  0.1337]\n",
      "MSE loss: 90.8428\n",
      "Iteration: 146500\n",
      "Gradient: [ -7.0537 -12.3754 -45.9504 -29.6818 -67.4859]\n",
      "Weights: [-4.7906  0.7971 -1.324   0.1719  0.1336]\n",
      "MSE loss: 90.8146\n",
      "Iteration: 146600\n",
      "Gradient: [ -3.9636  -4.4198   3.4498 -99.8438  73.3084]\n",
      "Weights: [-4.8091  0.7979 -1.3199  0.1719  0.1334]\n",
      "MSE loss: 90.7985\n",
      "Iteration: 146700\n",
      "Gradient: [  1.8436 -16.2495 -34.129  -52.8108 132.0548]\n",
      "Weights: [-4.7999  0.7886 -1.3219  0.1722  0.1334]\n",
      "MSE loss: 90.8636\n",
      "Iteration: 146800\n",
      "Gradient: [  -3.4037   -8.8425  -11.5883   12.764  -144.6231]\n",
      "Weights: [-4.812   0.7902 -1.3198  0.1721  0.1335]\n",
      "MSE loss: 90.9508\n",
      "Iteration: 146900\n",
      "Gradient: [   4.6411   -9.3676  -19.2777 -107.271   -86.3719]\n",
      "Weights: [-4.7859  0.7806 -1.3189  0.1731  0.1333]\n",
      "MSE loss: 90.8185\n",
      "Iteration: 147000\n",
      "Gradient: [  10.4266   -4.7368  -48.5064  -25.3465 -157.1515]\n",
      "Weights: [-4.7808  0.7763 -1.3228  0.1741  0.1331]\n",
      "MSE loss: 90.8182\n",
      "Iteration: 147100\n",
      "Gradient: [  -5.2425  -13.9412   13.1726 -131.4911 -174.2337]\n",
      "Weights: [-4.7976  0.7928 -1.3235  0.1747  0.1327]\n",
      "MSE loss: 90.6589\n",
      "Iteration: 147200\n",
      "Gradient: [  5.6458  -3.9474 -13.0303  59.6604  79.0165]\n",
      "Weights: [-4.7948  0.7951 -1.3228  0.1744  0.1325]\n",
      "MSE loss: 90.7239\n",
      "Iteration: 147300\n",
      "Gradient: [  6.1415   7.1483  25.0184 -26.85   381.3116]\n",
      "Weights: [-4.7687  0.7741 -1.3206  0.1747  0.1327]\n",
      "MSE loss: 91.0188\n",
      "Iteration: 147400\n",
      "Gradient: [  2.4267  11.364   15.217   62.9656 -91.4   ]\n",
      "Weights: [-4.7905  0.7837 -1.3223  0.1753  0.1325]\n",
      "MSE loss: 90.675\n",
      "Iteration: 147500\n",
      "Gradient: [ -0.796    3.7232 -18.7315   3.412  -34.9218]\n",
      "Weights: [-4.7987  0.7824 -1.3228  0.1751  0.1327]\n",
      "MSE loss: 90.9441\n",
      "Iteration: 147600\n",
      "Gradient: [ 9.4244  8.1472 32.5733 77.7144 46.1512]\n",
      "Weights: [-4.7976  0.7822 -1.319   0.1759  0.1325]\n",
      "MSE loss: 90.8392\n",
      "Iteration: 147700\n",
      "Gradient: [   2.514   -19.4211  -29.5167 -126.4786  -70.5662]\n",
      "Weights: [-4.7778  0.7727 -1.3224  0.1757  0.1327]\n",
      "MSE loss: 90.7664\n",
      "Iteration: 147800\n",
      "Gradient: [   1.3207  -10.2735  -15.9641 -107.9138 -520.5268]\n",
      "Weights: [-4.7768  0.7706 -1.3219  0.1765  0.1327]\n",
      "MSE loss: 90.7787\n",
      "Iteration: 147900\n",
      "Gradient: [  11.471    10.0702    4.9401  -53.4408 -108.7347]\n",
      "Weights: [-4.7781  0.7668 -1.3225  0.1772  0.1326]\n",
      "MSE loss: 90.7502\n",
      "Iteration: 148000\n",
      "Gradient: [  3.3703  -3.2709  29.351  196.6011 223.521 ]\n",
      "Weights: [-4.7878  0.7683 -1.3222  0.1784  0.1324]\n",
      "MSE loss: 90.8193\n",
      "Iteration: 148100\n",
      "Gradient: [  -3.682     8.0695   -3.8901 -198.9057  107.1542]\n",
      "Weights: [-4.7974  0.7788 -1.3228  0.1778  0.1323]\n",
      "MSE loss: 90.7257\n",
      "Iteration: 148200\n",
      "Gradient: [  6.3441 -12.0252 -27.3078 110.6781 184.0548]\n",
      "Weights: [-4.7932  0.7782 -1.3223  0.1767  0.1324]\n",
      "MSE loss: 90.6892\n",
      "Iteration: 148300\n",
      "Gradient: [  -2.2753   -4.3844    6.9427  -83.1493 -105.6714]\n",
      "Weights: [-4.7834  0.7669 -1.3187  0.177   0.1324]\n",
      "MSE loss: 90.7306\n",
      "Iteration: 148400\n",
      "Gradient: [ -2.0958  -1.0166 -62.4388 -26.9093 -26.4156]\n",
      "Weights: [-4.7683  0.7606 -1.3224  0.1777  0.1323]\n",
      "MSE loss: 90.9187\n",
      "Iteration: 148500\n",
      "Gradient: [ -4.1846  -2.4633   4.3568 110.9136 -25.5426]\n",
      "Weights: [-4.7674  0.7629 -1.3217  0.1777  0.132 ]\n",
      "MSE loss: 90.961\n",
      "Iteration: 148600\n",
      "Gradient: [  8.1625   4.0577 -18.6705  11.6076 -74.6994]\n",
      "Weights: [-4.7862  0.7656 -1.3191  0.1784  0.1321]\n",
      "MSE loss: 90.7533\n",
      "Iteration: 148700\n",
      "Gradient: [  0.4845 -15.5193 -17.4214  -9.495  -80.576 ]\n",
      "Weights: [-4.7769  0.7687 -1.3208  0.1787  0.1319]\n",
      "MSE loss: 90.8224\n",
      "Iteration: 148800\n",
      "Gradient: [ 4.9298  8.3115 44.821  -4.5184 22.4683]\n",
      "Weights: [-4.786   0.7714 -1.3219  0.1791  0.1318]\n",
      "MSE loss: 90.6604\n",
      "Iteration: 148900\n",
      "Gradient: [   1.864     2.0822  -17.1842  105.8115 -150.0109]\n",
      "Weights: [-4.7737  0.7634 -1.3196  0.1798  0.1315]\n",
      "MSE loss: 90.7848\n",
      "Iteration: 149000\n",
      "Gradient: [ 6.0105  9.1163 15.916  78.2486 92.5721]\n",
      "Weights: [-4.7836  0.7655 -1.3199  0.1811  0.1311]\n",
      "MSE loss: 90.6554\n",
      "Iteration: 149100\n",
      "Gradient: [ -8.1916 -15.8733  -7.1687   9.5728 -17.5753]\n",
      "Weights: [-4.7938  0.7756 -1.3218  0.1805  0.1311]\n",
      "MSE loss: 90.6261\n",
      "Iteration: 149200\n",
      "Gradient: [ 11.9878  13.736   -1.4615 103.6847 104.7845]\n",
      "Weights: [-4.7891  0.7835 -1.3226  0.1801  0.131 ]\n",
      "MSE loss: 90.7666\n",
      "Iteration: 149300\n",
      "Gradient: [   0.2083   -6.941   -39.4648  -94.0813 -169.4562]\n",
      "Weights: [-4.793   0.7821 -1.3223  0.1798  0.1311]\n",
      "MSE loss: 90.6338\n",
      "Iteration: 149400\n",
      "Gradient: [  6.5883 -26.6693 -31.8846 -16.7886  68.8445]\n",
      "Weights: [-4.8013  0.7805 -1.3232  0.1801  0.1312]\n",
      "MSE loss: 90.7445\n",
      "Iteration: 149500\n",
      "Gradient: [ -7.0968   4.3551 -10.1201  -1.4902 -95.2246]\n",
      "Weights: [-4.7916  0.7831 -1.3247  0.1807  0.1312]\n",
      "MSE loss: 90.6833\n",
      "Iteration: 149600\n",
      "Gradient: [ 2.754400e+00 -3.120000e-02  4.694260e+01 -1.017200e+01  1.761179e+02]\n",
      "Weights: [-4.7908  0.779  -1.3224  0.18    0.1311]\n",
      "MSE loss: 90.6234\n",
      "Iteration: 149700\n",
      "Gradient: [  4.7756   0.898   -5.8389  -9.2622 -25.6938]\n",
      "Weights: [-4.7907  0.7695 -1.3203  0.1812  0.131 ]\n",
      "MSE loss: 90.6625\n",
      "Iteration: 149800\n",
      "Gradient: [  0.8525  22.7814  21.0967 -26.4923 -40.4862]\n",
      "Weights: [-4.7788  0.774  -1.3209  0.1804  0.1308]\n",
      "MSE loss: 90.7773\n",
      "Iteration: 149900\n",
      "Gradient: [  1.9248   2.2251 -44.2661  39.1675 206.1144]\n",
      "Weights: [-4.7834  0.7673 -1.3215  0.1813  0.1312]\n",
      "MSE loss: 90.7003\n",
      "Iteration: 150000\n",
      "Gradient: [  11.9671   -2.2864  -38.2427 -115.6556  -26.3549]\n",
      "Weights: [-4.7783  0.7639 -1.3204  0.1813  0.131 ]\n",
      "MSE loss: 90.6752\n",
      "Iteration: 150100\n",
      "Gradient: [ 2.65000e-01 -5.46000e-02  1.41374e+01  2.22051e+01 -5.70088e+01]\n",
      "Weights: [-4.7635  0.7544 -1.3201  0.1817  0.131 ]\n",
      "MSE loss: 90.8579\n",
      "Iteration: 150200\n",
      "Gradient: [  -8.9856  -14.8826  -33.5816 -108.0272 -260.6032]\n",
      "Weights: [-4.7836  0.7619 -1.321   0.1819  0.131 ]\n",
      "MSE loss: 90.6689\n",
      "Iteration: 150300\n",
      "Gradient: [ 1.728  -2.3745 26.4557 66.4169  9.2635]\n",
      "Weights: [-4.7693  0.7612 -1.3237  0.1822  0.1309]\n",
      "MSE loss: 90.755\n",
      "Iteration: 150400\n",
      "Gradient: [ -6.8255   6.8991 -24.8107  31.7214 167.4745]\n",
      "Weights: [-4.773   0.757  -1.3218  0.1822  0.131 ]\n",
      "MSE loss: 90.7008\n",
      "Iteration: 150500\n",
      "Gradient: [  6.3928   1.5542   9.7559  53.6837 335.1456]\n",
      "Weights: [-4.7662  0.759  -1.3214  0.1826  0.1309]\n",
      "MSE loss: 90.9943\n",
      "Iteration: 150600\n",
      "Gradient: [   5.4258  -15.1189  -10.3749  -33.9223 -100.3248]\n",
      "Weights: [-4.7793  0.7531 -1.3183  0.1826  0.1307]\n",
      "MSE loss: 90.7008\n",
      "Iteration: 150700\n",
      "Gradient: [  8.5916  -6.6761  18.6168 110.6416  90.7698]\n",
      "Weights: [-4.7857  0.7576 -1.3199  0.183   0.1306]\n",
      "MSE loss: 90.7182\n",
      "Iteration: 150800\n",
      "Gradient: [   4.2233   -1.4038  -11.5805    0.4478 -291.9295]\n",
      "Weights: [-4.7797  0.7562 -1.3196  0.1827  0.1307]\n",
      "MSE loss: 90.6758\n",
      "Iteration: 150900\n",
      "Gradient: [  7.5984  -6.5749   8.4215  24.1623 218.1209]\n",
      "Weights: [-4.7726  0.7594 -1.3192  0.182   0.1308]\n",
      "MSE loss: 90.8104\n",
      "Iteration: 151000\n",
      "Gradient: [ -9.3932   6.9334  42.559  -44.7324 -99.8633]\n",
      "Weights: [-4.778   0.7577 -1.3183  0.1821  0.1306]\n",
      "MSE loss: 90.6537\n",
      "Iteration: 151100\n",
      "Gradient: [  5.9078  -2.6075  16.9634  39.2704 231.1037]\n",
      "Weights: [-4.7892  0.757  -1.3177  0.1832  0.1307]\n",
      "MSE loss: 90.82\n",
      "Iteration: 151200\n",
      "Gradient: [ -8.9088 -23.322    6.309   -4.2972 -17.0673]\n",
      "Weights: [-4.7878  0.7569 -1.3166  0.1822  0.1305]\n",
      "MSE loss: 90.7457\n",
      "Iteration: 151300\n",
      "Gradient: [-10.7165   7.139   19.2216 -51.8885 265.1099]\n",
      "Weights: [-4.7742  0.7631 -1.3184  0.182   0.1304]\n",
      "MSE loss: 90.7715\n",
      "Iteration: 151400\n",
      "Gradient: [   4.1626   32.306    29.9054  115.4455 -122.776 ]\n",
      "Weights: [-4.7801  0.7587 -1.3159  0.182   0.1305]\n",
      "MSE loss: 90.698\n",
      "Iteration: 151500\n",
      "Gradient: [  -5.4206   -5.5166    7.6424   30.594  -135.6179]\n",
      "Weights: [-4.7857  0.7609 -1.3167  0.1816  0.1305]\n",
      "MSE loss: 90.6632\n",
      "Iteration: 151600\n",
      "Gradient: [  8.0098  -4.3425 -53.6116  14.9373 126.5563]\n",
      "Weights: [-4.783   0.758  -1.3184  0.1831  0.1304]\n",
      "MSE loss: 90.6436\n",
      "Iteration: 151700\n",
      "Gradient: [  5.6378   1.2876  18.9657 136.8648  74.0905]\n",
      "Weights: [-4.7712  0.7588 -1.3177  0.183   0.1302]\n",
      "MSE loss: 90.8424\n",
      "Iteration: 151800\n",
      "Gradient: [  -0.9527   -4.9471   23.4522 -108.5194  157.6908]\n",
      "Weights: [-4.7925  0.7605 -1.3162  0.1828  0.1302]\n",
      "MSE loss: 90.7036\n",
      "Iteration: 151900\n",
      "Gradient: [ -2.7788   4.2419  -1.2594  26.6726 -54.8607]\n",
      "Weights: [-4.7894  0.7588 -1.3161  0.1823  0.1302]\n",
      "MSE loss: 90.79\n",
      "Iteration: 152000\n",
      "Gradient: [  -2.2667   -6.2171  -11.6834    5.272  -217.3022]\n",
      "Weights: [-4.7853  0.7658 -1.3191  0.1825  0.1304]\n",
      "MSE loss: 90.6235\n",
      "Iteration: 152100\n",
      "Gradient: [ -5.4478  13.0216   2.3556 -23.7684 214.1796]\n",
      "Weights: [-4.7854  0.7635 -1.318   0.1823  0.1304]\n",
      "MSE loss: 90.625\n",
      "Iteration: 152200\n",
      "Gradient: [ -12.307    12.7326  -28.5217  -16.2881 -174.7743]\n",
      "Weights: [-4.7866  0.7514 -1.3154  0.1826  0.1304]\n",
      "MSE loss: 90.8465\n",
      "Iteration: 152300\n",
      "Gradient: [   0.5138  -13.9677   32.7192  -37.8506 -251.0644]\n",
      "Weights: [-4.7769  0.7518 -1.3167  0.1828  0.1305]\n",
      "MSE loss: 90.6822\n",
      "Iteration: 152400\n",
      "Gradient: [   6.7776   -2.0527  -10.1034 -109.6397  -13.3375]\n",
      "Weights: [-4.7927  0.7624 -1.3175  0.1817  0.1305]\n",
      "MSE loss: 90.8141\n",
      "Iteration: 152500\n",
      "Gradient: [ -5.3292   8.9962  10.49   -76.1098 250.865 ]\n",
      "Weights: [-4.7842  0.7738 -1.3201  0.1825  0.1301]\n",
      "MSE loss: 90.7195\n",
      "Iteration: 152600\n",
      "Gradient: [  2.0857  -7.6262  -1.9445  42.3174 111.6075]\n",
      "Weights: [-4.7739  0.7656 -1.3227  0.1836  0.1304]\n",
      "MSE loss: 90.7754\n",
      "Iteration: 152700\n",
      "Gradient: [  8.693   -6.2291 -32.119  -58.3513 124.0186]\n",
      "Weights: [-4.7766  0.7541 -1.3216  0.1839  0.1305]\n",
      "MSE loss: 90.7161\n",
      "Iteration: 152800\n",
      "Gradient: [ -1.4404   9.6591  31.7584   7.1595 381.2073]\n",
      "Weights: [-4.7636  0.7475 -1.3222  0.1847  0.1304]\n",
      "MSE loss: 90.7984\n",
      "Iteration: 152900\n",
      "Gradient: [-6.262700e+00  8.960000e-02 -1.119030e+01  3.551940e+01 -2.661444e+02]\n",
      "Weights: [-4.763   0.7451 -1.3224  0.1852  0.1303]\n",
      "MSE loss: 90.8468\n",
      "Iteration: 153000\n",
      "Gradient: [  3.2829  15.9653   3.0436  53.4287 122.5589]\n",
      "Weights: [-4.7839  0.7572 -1.3218  0.1849  0.1303]\n",
      "MSE loss: 90.6888\n",
      "Iteration: 153100\n",
      "Gradient: [ -6.3379  10.0094 -45.3338  60.3176  -3.9044]\n",
      "Weights: [-4.7841  0.764  -1.324   0.1844  0.1303]\n",
      "MSE loss: 90.6428\n",
      "Iteration: 153200\n",
      "Gradient: [  3.1695  -5.7376  21.6191 -24.1486 341.9987]\n",
      "Weights: [-4.7792  0.7744 -1.3263  0.1837  0.1305]\n",
      "MSE loss: 90.743\n",
      "Iteration: 153300\n",
      "Gradient: [   7.2833   18.8359   -4.6943   59.738  -111.3036]\n",
      "Weights: [-4.7819  0.7815 -1.3273  0.1839  0.1303]\n",
      "MSE loss: 90.8561\n",
      "Iteration: 153400\n",
      "Gradient: [ -8.8579   5.1364 -57.8024  -4.2007 137.6576]\n",
      "Weights: [-4.7943  0.7821 -1.3269  0.1838  0.1303]\n",
      "MSE loss: 90.5789\n",
      "Iteration: 153500\n",
      "Gradient: [ -0.968   28.3277 -48.8316 -45.7144  94.5021]\n",
      "Weights: [-4.7922  0.7798 -1.3277  0.1842  0.1302]\n",
      "MSE loss: 90.5713\n",
      "Iteration: 153600\n",
      "Gradient: [   6.9975   -1.6762   18.411    25.213  -153.8316]\n",
      "Weights: [-4.7987  0.7905 -1.3269  0.183   0.1301]\n",
      "MSE loss: 90.6374\n",
      "Iteration: 153700\n",
      "Gradient: [  9.0083   1.8457  31.7604 -20.3373 240.1664]\n",
      "Weights: [-4.7767  0.7672 -1.3231  0.1827  0.1303]\n",
      "MSE loss: 90.7767\n",
      "Iteration: 153800\n",
      "Gradient: [ -3.9484  -4.5042 -15.6142 115.7418 136.0003]\n",
      "Weights: [-4.791   0.7707 -1.3212  0.1828  0.1304]\n",
      "MSE loss: 90.6067\n",
      "Iteration: 153900\n",
      "Gradient: [  -3.5878  -14.6919  -46.6573  -86.2911 -262.6654]\n",
      "Weights: [-4.7882  0.7727 -1.3214  0.1828  0.1302]\n",
      "MSE loss: 90.6182\n",
      "Iteration: 154000\n",
      "Gradient: [  -5.5298    5.1765  -20.5475  -77.1641 -172.0423]\n",
      "Weights: [-4.8169  0.7747 -1.3209  0.183   0.1301]\n",
      "MSE loss: 91.7022\n",
      "Iteration: 154100\n",
      "Gradient: [  -5.8482   21.9459   -0.7938   77.7209 -229.5722]\n",
      "Weights: [-4.7996  0.7863 -1.3225  0.1821  0.1303]\n",
      "MSE loss: 90.688\n",
      "Iteration: 154200\n",
      "Gradient: [  4.9128  14.147   33.4301  38.562  484.385 ]\n",
      "Weights: [-4.7881  0.7829 -1.3241  0.182   0.1305]\n",
      "MSE loss: 90.7051\n",
      "Iteration: 154300\n",
      "Gradient: [  -5.3076    5.3594  -25.9162   45.0937 -201.0236]\n",
      "Weights: [-4.7925  0.7729 -1.3215  0.1823  0.1304]\n",
      "MSE loss: 90.6186\n",
      "Iteration: 154400\n",
      "Gradient: [  -6.1784  -17.8114   57.0129  124.6937 -211.2717]\n",
      "Weights: [-4.8003  0.7743 -1.322   0.1827  0.1304]\n",
      "MSE loss: 90.7218\n",
      "Iteration: 154500\n",
      "Gradient: [   4.6046    6.2572  -20.2581  -79.2788 -120.6549]\n",
      "Weights: [-4.7696  0.7723 -1.3264  0.1829  0.1305]\n",
      "MSE loss: 90.8987\n",
      "Iteration: 154600\n",
      "Gradient: [  5.8944  -0.3241   1.7662 -18.7837 135.1186]\n",
      "Weights: [-4.787   0.7763 -1.3265  0.184   0.1303]\n",
      "MSE loss: 90.5844\n",
      "Iteration: 154700\n",
      "Gradient: [   3.3309    2.3106  -16.9558  -21.8708 -199.7981]\n",
      "Weights: [-4.7715  0.7725 -1.3286  0.1847  0.1302]\n",
      "MSE loss: 90.8017\n",
      "Iteration: 154800\n",
      "Gradient: [   3.4762   15.4787   -4.1965   13.732  -214.5153]\n",
      "Weights: [-4.7902  0.7829 -1.3275  0.1839  0.1302]\n",
      "MSE loss: 90.6068\n",
      "Iteration: 154900\n",
      "Gradient: [   1.7394    6.6287   -5.3652  109.3945 -130.7557]\n",
      "Weights: [-4.7922  0.7824 -1.3248  0.1846  0.1299]\n",
      "MSE loss: 90.7905\n",
      "Iteration: 155000\n",
      "Gradient: [   2.5529   16.9141  -12.5071  -11.8685 -327.7653]\n",
      "Weights: [-4.7894  0.7708 -1.3265  0.185   0.1297]\n",
      "MSE loss: 91.0076\n",
      "Iteration: 155100\n",
      "Gradient: [  3.6832  12.9258  -7.0087 122.7004   4.5792]\n",
      "Weights: [-4.7901  0.7723 -1.3244  0.1856  0.1298]\n",
      "MSE loss: 90.6126\n",
      "Iteration: 155200\n",
      "Gradient: [ -4.621   -5.131   13.3753 -89.5294  52.2541]\n",
      "Weights: [-4.8008  0.7748 -1.3256  0.1865  0.1297]\n",
      "MSE loss: 90.6862\n",
      "Iteration: 155300\n",
      "Gradient: [ 1.2083  9.8351 20.6588 66.1233 82.1299]\n",
      "Weights: [-4.7733  0.7687 -1.3264  0.1865  0.1297]\n",
      "MSE loss: 90.9009\n",
      "Iteration: 155400\n",
      "Gradient: [  -5.6499  -17.5339   -1.0799   25.1807 -129.6681]\n",
      "Weights: [-4.7872  0.7716 -1.324   0.186   0.1294]\n",
      "MSE loss: 90.597\n",
      "Iteration: 155500\n",
      "Gradient: [  -1.8501  -11.3097  -35.7491  -98.7626 -191.8533]\n",
      "Weights: [-4.8158  0.7785 -1.3222  0.1851  0.1297]\n",
      "MSE loss: 91.0327\n",
      "Iteration: 155600\n",
      "Gradient: [  -1.3905    6.5786  -11.5484  -17.6099 -323.0635]\n",
      "Weights: [-4.7715  0.7552 -1.321   0.1852  0.1297]\n",
      "MSE loss: 90.7373\n",
      "Iteration: 155700\n",
      "Gradient: [ -0.8052 -10.3126  30.931  -32.1723 172.8802]\n",
      "Weights: [-4.7887  0.7646 -1.3236  0.1849  0.13  ]\n",
      "MSE loss: 90.6768\n",
      "Iteration: 155800\n",
      "Gradient: [-13.4903 -17.5151  -9.5547  35.9078  79.9508]\n",
      "Weights: [-4.7893  0.7776 -1.3271  0.1851  0.13  ]\n",
      "MSE loss: 90.5767\n",
      "Iteration: 155900\n",
      "Gradient: [  -2.4818    9.6887  -41.838     2.3406 -226.6899]\n",
      "Weights: [-4.7911  0.7735 -1.3227  0.1855  0.1296]\n",
      "MSE loss: 90.6537\n",
      "Iteration: 156000\n",
      "Gradient: [  -3.1163   -4.8042  -41.8782 -134.1254 -368.5129]\n",
      "Weights: [-4.7865  0.7662 -1.3226  0.1854  0.1295]\n",
      "MSE loss: 90.6613\n",
      "Iteration: 156100\n",
      "Gradient: [ -2.3294  -8.4811 -14.7086  -2.0858  94.8697]\n",
      "Weights: [-4.7793  0.7617 -1.3201  0.1856  0.1293]\n",
      "MSE loss: 90.646\n",
      "Iteration: 156200\n",
      "Gradient: [-16.4888   0.8731 -42.8284 -22.353   90.3822]\n",
      "Weights: [-4.7977  0.7647 -1.3218  0.1862  0.1294]\n",
      "MSE loss: 90.8349\n",
      "Iteration: 156300\n",
      "Gradient: [  -5.3491  -13.3836  -19.8637 -113.9542  -19.9324]\n",
      "Weights: [-4.7907  0.7677 -1.3217  0.1858  0.1293]\n",
      "MSE loss: 90.6324\n",
      "Iteration: 156400\n",
      "Gradient: [  0.9518 -14.8528 -30.685  149.6911 -17.4992]\n",
      "Weights: [-4.7867  0.7717 -1.3233  0.1867  0.1293]\n",
      "MSE loss: 90.6738\n",
      "Iteration: 156500\n",
      "Gradient: [ 2.2762  7.9145 -8.7828 66.0543 16.2378]\n",
      "Weights: [-4.7924  0.7733 -1.3213  0.1868  0.1291]\n",
      "MSE loss: 90.8152\n",
      "Iteration: 156600\n",
      "Gradient: [   2.5699  -11.5646    7.9975  -22.2615 -202.2977]\n",
      "Weights: [-4.8003  0.769  -1.3223  0.1863  0.1291]\n",
      "MSE loss: 91.0101\n",
      "Iteration: 156700\n",
      "Gradient: [  3.6262  22.7089  40.039   49.587  343.7791]\n",
      "Weights: [-4.8166  0.7873 -1.3207  0.1858  0.1293]\n",
      "MSE loss: 91.2355\n",
      "Iteration: 156800\n",
      "Gradient: [   7.5427  -12.4358   -7.4627  106.6718 -115.575 ]\n",
      "Weights: [-4.7828  0.7797 -1.3262  0.1856  0.1292]\n",
      "MSE loss: 90.8019\n",
      "Iteration: 156900\n",
      "Gradient: [  -7.2838   13.025    12.0898  -60.0334 -283.3464]\n",
      "Weights: [-4.7936  0.7712 -1.3246  0.1863  0.1295]\n",
      "MSE loss: 90.6214\n",
      "Iteration: 157000\n",
      "Gradient: [  0.3311   6.4032  45.5945  -0.8629 163.1597]\n",
      "Weights: [-4.7798  0.7757 -1.3257  0.1856  0.1297]\n",
      "MSE loss: 90.8254\n",
      "Iteration: 157100\n",
      "Gradient: [   3.3339  -10.3495    6.5692  -59.4276 -177.452 ]\n",
      "Weights: [-4.7937  0.7724 -1.3296  0.186   0.1298]\n",
      "MSE loss: 91.0182\n",
      "Iteration: 157200\n",
      "Gradient: [  2.6718  11.8278 -27.9405  45.1077  13.0843]\n",
      "Weights: [-4.7899  0.7732 -1.3302  0.1861  0.1302]\n",
      "MSE loss: 90.6019\n",
      "Iteration: 157300\n",
      "Gradient: [   7.4393   12.4977  -18.2759   49.1975 -370.4555]\n",
      "Weights: [-4.8133  0.7982 -1.332   0.1852  0.1303]\n",
      "MSE loss: 90.8634\n",
      "Iteration: 157400\n",
      "Gradient: [   6.2486   21.9688   20.9757   40.7874 -274.7066]\n",
      "Weights: [-4.7845  0.7779 -1.3294  0.1863  0.1301]\n",
      "MSE loss: 90.7801\n",
      "Iteration: 157500\n",
      "Gradient: [  7.9991   8.3018  35.6653 -26.6152  27.7579]\n",
      "Weights: [-4.7977  0.7869 -1.33    0.1858  0.1299]\n",
      "MSE loss: 90.606\n",
      "Iteration: 157600\n",
      "Gradient: [ 1.340000e-01  7.170500e+00 -5.228900e+00 -3.218460e+01  2.052069e+02]\n",
      "Weights: [-4.8032  0.7828 -1.3279  0.1856  0.1298]\n",
      "MSE loss: 90.6447\n",
      "Iteration: 157700\n",
      "Gradient: [  -8.7832   -8.7883   -5.037    61.8151 -175.5022]\n",
      "Weights: [-4.8176  0.7924 -1.3281  0.1855  0.1297]\n",
      "MSE loss: 90.8474\n",
      "Iteration: 157800\n",
      "Gradient: [ -4.1121  -7.6986 -10.8428  21.631  203.6281]\n",
      "Weights: [-4.8136  0.8001 -1.3312  0.1847  0.13  ]\n",
      "MSE loss: 90.6845\n",
      "Iteration: 157900\n",
      "Gradient: [  7.3006   4.4667 -38.2819 -90.0304 -26.4102]\n",
      "Weights: [-4.8094  0.798  -1.3311  0.1842  0.1302]\n",
      "MSE loss: 90.6348\n",
      "Iteration: 158000\n",
      "Gradient: [ -2.8072  21.1155  33.4031 -15.1284 -36.3427]\n",
      "Weights: [-4.8016  0.7919 -1.3318  0.1853  0.1301]\n",
      "MSE loss: 90.5774\n",
      "Iteration: 158100\n",
      "Gradient: [ -7.5713  20.4703 -42.0852 -47.594    4.8894]\n",
      "Weights: [-4.7977  0.7881 -1.3304  0.1852  0.13  ]\n",
      "MSE loss: 90.565\n",
      "Iteration: 158200\n",
      "Gradient: [   5.931     8.0217   20.117  -100.9048 -270.4777]\n",
      "Weights: [-4.795   0.7841 -1.3326  0.1854  0.1302]\n",
      "MSE loss: 90.5873\n",
      "Iteration: 158300\n",
      "Gradient: [-1.5108 -8.8724 -1.9646 74.5756 40.3523]\n",
      "Weights: [-4.7868  0.7813 -1.3325  0.1856  0.1306]\n",
      "MSE loss: 90.7349\n",
      "Iteration: 158400\n",
      "Gradient: [  4.7004  13.6544  15.5701  75.8966 212.7976]\n",
      "Weights: [-4.7954  0.7821 -1.3346  0.1857  0.1307]\n",
      "MSE loss: 90.604\n",
      "Iteration: 158500\n",
      "Gradient: [  3.4257  12.6542  25.0546  95.8316 -14.0929]\n",
      "Weights: [-4.7768  0.7791 -1.3338  0.185   0.1307]\n",
      "MSE loss: 90.7113\n",
      "Iteration: 158600\n",
      "Gradient: [-11.0258   7.8392  -6.136   34.3038  38.8753]\n",
      "Weights: [-4.8031  0.7859 -1.3333  0.1846  0.1307]\n",
      "MSE loss: 90.7116\n",
      "Iteration: 158700\n",
      "Gradient: [   1.2539   -6.7655   10.0587  -53.219  -163.9556]\n",
      "Weights: [-4.7935  0.785  -1.3348  0.1847  0.1309]\n",
      "MSE loss: 90.5634\n",
      "Iteration: 158800\n",
      "Gradient: [   0.3186    9.7751   -3.1113   10.2631 -185.9663]\n",
      "Weights: [-4.8044  0.7899 -1.3343  0.1844  0.1308]\n",
      "MSE loss: 90.6817\n",
      "Iteration: 158900\n",
      "Gradient: [  -5.4035   -6.5302   26.6456   55.775  -402.023 ]\n",
      "Weights: [-4.7907  0.7814 -1.331   0.1844  0.1308]\n",
      "MSE loss: 90.6386\n",
      "Iteration: 159000\n",
      "Gradient: [   3.4456   -4.0877   49.4285   -9.0675 -174.8648]\n",
      "Weights: [-4.7985  0.7853 -1.3315  0.1849  0.1309]\n",
      "MSE loss: 91.0019\n",
      "Iteration: 159100\n",
      "Gradient: [  1.3302  -3.8736  10.718  -44.9079  33.0483]\n",
      "Weights: [-4.8109  0.7902 -1.3332  0.1843  0.1307]\n",
      "MSE loss: 90.8908\n",
      "Iteration: 159200\n",
      "Gradient: [ 10.9099  -2.0762  -8.6711 118.0564 253.4478]\n",
      "Weights: [-4.7932  0.7905 -1.3362  0.1849  0.1305]\n",
      "MSE loss: 90.588\n",
      "Iteration: 159300\n",
      "Gradient: [  11.1107  -12.0222   14.9109   18.929  -410.1544]\n",
      "Weights: [-4.7739  0.7864 -1.3368  0.1855  0.1307]\n",
      "MSE loss: 91.0057\n",
      "Iteration: 159400\n",
      "Gradient: [  -5.403   -21.6338  -16.1282    9.9828 -176.8646]\n",
      "Weights: [-4.8069  0.7891 -1.3375  0.1846  0.1307]\n",
      "MSE loss: 91.5107\n",
      "Iteration: 159500\n",
      "Gradient: [ -2.2625 -10.8889  -6.4606 -64.6011 127.5555]\n",
      "Weights: [-4.7929  0.7925 -1.3352  0.1841  0.1308]\n",
      "MSE loss: 90.5612\n",
      "Iteration: 159600\n",
      "Gradient: [  2.069   13.4226  37.8526  17.4768 246.0776]\n",
      "Weights: [-4.8002  0.7889 -1.3343  0.1848  0.1308]\n",
      "MSE loss: 90.5954\n",
      "Iteration: 159700\n",
      "Gradient: [   0.6613   -6.9761   -3.3366  -58.0013 -385.639 ]\n",
      "Weights: [-4.7809  0.7805 -1.3339  0.1847  0.1306]\n",
      "MSE loss: 90.6263\n",
      "Iteration: 159800\n",
      "Gradient: [ -0.2738 -12.5246   1.0791 -59.985  -94.6924]\n",
      "Weights: [-4.8064  0.7852 -1.3329  0.1846  0.1304]\n",
      "MSE loss: 91.2703\n",
      "Iteration: 159900\n",
      "Gradient: [ 13.8499  12.5133  19.933  113.0313 -65.1791]\n",
      "Weights: [-4.7855  0.7832 -1.3302  0.1851  0.1303]\n",
      "MSE loss: 90.7633\n",
      "Iteration: 160000\n",
      "Gradient: [ 3.241   1.0853 -8.2084 77.652  -2.1978]\n",
      "Weights: [-4.7697  0.7747 -1.3335  0.1851  0.1308]\n",
      "MSE loss: 90.8552\n",
      "Iteration: 160100\n",
      "Gradient: [   0.655    19.0454  -33.3809 -102.9624   66.3894]\n",
      "Weights: [-4.7952  0.7945 -1.3373  0.1844  0.1307]\n",
      "MSE loss: 90.5655\n",
      "Iteration: 160200\n",
      "Gradient: [   0.7136   -1.949   -59.4248   -7.9035 -397.563 ]\n",
      "Weights: [-4.7904  0.7723 -1.3336  0.1856  0.1308]\n",
      "MSE loss: 90.8165\n",
      "Iteration: 160300\n",
      "Gradient: [ 11.293  -17.4533  17.1797 134.5869 217.6485]\n",
      "Weights: [-4.7615  0.762  -1.3307  0.1861  0.1306]\n",
      "MSE loss: 91.0264\n",
      "Iteration: 160400\n",
      "Gradient: [  0.8922  -0.2592 -32.9616 -54.3451 -13.857 ]\n",
      "Weights: [-4.7856  0.7625 -1.3302  0.1861  0.1308]\n",
      "MSE loss: 90.7969\n",
      "Iteration: 160500\n",
      "Gradient: [ 12.1159   3.4743  42.4734  55.6919 250.3274]\n",
      "Weights: [-4.7817  0.7703 -1.3311  0.1853  0.1307]\n",
      "MSE loss: 90.6039\n",
      "Iteration: 160600\n",
      "Gradient: [  -0.7982   -6.0268  -18.6817 -125.616  -238.1872]\n",
      "Weights: [-4.8     0.7808 -1.3289  0.1844  0.1303]\n",
      "MSE loss: 90.6781\n",
      "Iteration: 160700\n",
      "Gradient: [ -7.2979  15.514    8.0647  26.6773 198.3719]\n",
      "Weights: [-4.7907  0.7851 -1.329   0.1839  0.1303]\n",
      "MSE loss: 90.6018\n",
      "Iteration: 160800\n",
      "Gradient: [  0.8284  10.4083 -26.6566 -16.3055 -32.1768]\n",
      "Weights: [-4.7857  0.7865 -1.3314  0.1848  0.1301]\n",
      "MSE loss: 90.6568\n",
      "Iteration: 160900\n",
      "Gradient: [  8.248    4.344    9.7794 -47.263  434.0313]\n",
      "Weights: [-4.7933  0.7939 -1.3322  0.1843  0.1303]\n",
      "MSE loss: 90.6774\n",
      "Iteration: 161000\n",
      "Gradient: [ -5.6988 -11.5673  -6.1276  26.7602 158.0248]\n",
      "Weights: [-4.8085  0.7903 -1.3285  0.1834  0.1302]\n",
      "MSE loss: 90.851\n",
      "Iteration: 161100\n",
      "Gradient: [  2.7613  -9.6631 -51.2646  27.9773  91.5887]\n",
      "Weights: [-4.8144  0.7937 -1.3301  0.1841  0.1301]\n",
      "MSE loss: 90.8935\n",
      "Iteration: 161200\n",
      "Gradient: [ -6.0908 -19.3768  28.3176  32.4354 -74.499 ]\n",
      "Weights: [-4.807   0.7937 -1.3287  0.184   0.1301]\n",
      "MSE loss: 90.6332\n",
      "Iteration: 161300\n",
      "Gradient: [  7.4625  -6.7781  41.3883 -54.9203  53.646 ]\n",
      "Weights: [-4.7965  0.7829 -1.3311  0.1848  0.1302]\n",
      "MSE loss: 90.7462\n",
      "Iteration: 161400\n",
      "Gradient: [-11.6748  20.7391   4.8068 -12.0573 214.6183]\n",
      "Weights: [-4.8115  0.7906 -1.3326  0.1844  0.1305]\n",
      "MSE loss: 90.89\n",
      "Iteration: 161500\n",
      "Gradient: [  7.638   -5.5043  11.6561 120.9615 224.3493]\n",
      "Weights: [-4.786   0.7884 -1.3334  0.1838  0.1306]\n",
      "MSE loss: 90.6348\n",
      "Iteration: 161600\n",
      "Gradient: [ -4.0708   7.1333 -13.2239 -13.5502 465.3965]\n",
      "Weights: [-4.7992  0.7934 -1.3343  0.1847  0.1305]\n",
      "MSE loss: 90.5455\n",
      "Iteration: 161700\n",
      "Gradient: [ 9.1531  4.0772 31.5334 43.021  90.3239]\n",
      "Weights: [-4.7982  0.8005 -1.3348  0.1841  0.1306]\n",
      "MSE loss: 90.7072\n",
      "Iteration: 161800\n",
      "Gradient: [ -5.9549   5.1329 -25.2698  52.1346 134.3646]\n",
      "Weights: [-4.8124  0.8013 -1.3358  0.1847  0.1305]\n",
      "MSE loss: 90.6523\n",
      "Iteration: 161900\n",
      "Gradient: [  1.8379  -9.2173  33.3592 -14.63    33.8171]\n",
      "Weights: [-4.8012  0.7902 -1.3363  0.1845  0.1308]\n",
      "MSE loss: 90.7413\n",
      "Iteration: 162000\n",
      "Gradient: [ -1.2356  -6.5633   6.1603  -0.628  -74.7662]\n",
      "Weights: [-4.8002  0.8062 -1.338   0.184   0.1308]\n",
      "MSE loss: 90.6569\n",
      "Iteration: 162100\n",
      "Gradient: [ 3.480000e-02  2.235370e+01 -5.889150e+01  3.707590e+01  1.650195e+02]\n",
      "Weights: [-4.8089  0.8043 -1.3398  0.1849  0.1307]\n",
      "MSE loss: 90.5794\n",
      "Iteration: 162200\n",
      "Gradient: [  -3.4993    3.6761   27.2992 -103.715    35.3514]\n",
      "Weights: [-4.7885  0.7911 -1.3387  0.185   0.1308]\n",
      "MSE loss: 90.5452\n",
      "Iteration: 162300\n",
      "Gradient: [  0.8896  -1.9742  21.759  -47.1321 -50.8045]\n",
      "Weights: [-4.8107  0.8045 -1.3415  0.1856  0.1307]\n",
      "MSE loss: 90.6193\n",
      "Iteration: 162400\n",
      "Gradient: [  -5.9988   17.9441  -15.3649   23.1494 -252.9765]\n",
      "Weights: [-4.7872  0.798  -1.347   0.1868  0.1309]\n",
      "MSE loss: 90.5484\n",
      "Iteration: 162500\n",
      "Gradient: [-17.9389 -24.0703 -19.2172  29.7926   8.3545]\n",
      "Weights: [-4.8093  0.8024 -1.3483  0.187   0.131 ]\n",
      "MSE loss: 90.9715\n",
      "Iteration: 162600\n",
      "Gradient: [  7.8598  16.6352  85.5943  17.8601 200.0799]\n",
      "Weights: [-4.7902  0.806  -1.3505  0.1881  0.1309]\n",
      "MSE loss: 90.6485\n",
      "Iteration: 162700\n",
      "Gradient: [   4.4064   21.7769   29.2156  128.8206 -173.1423]\n",
      "Weights: [-4.7792  0.8037 -1.3513  0.1887  0.131 ]\n",
      "MSE loss: 91.3183\n",
      "Iteration: 162800\n",
      "Gradient: [  0.8098  -7.8624 -16.7748 -74.8364 -51.7784]\n",
      "Weights: [-4.7984  0.8129 -1.3567  0.1883  0.131 ]\n",
      "MSE loss: 90.5621\n",
      "Iteration: 162900\n",
      "Gradient: [ 0.3988 10.5323 20.2209 -8.1331 42.0218]\n",
      "Weights: [-4.7981  0.8212 -1.3571  0.1885  0.1309]\n",
      "MSE loss: 90.5327\n",
      "Iteration: 163000\n",
      "Gradient: [  -1.4152   -1.8912   21.5312 -129.2187 -225.8393]\n",
      "Weights: [-4.7966  0.8198 -1.3577  0.1879  0.131 ]\n",
      "MSE loss: 90.4896\n",
      "Iteration: 163100\n",
      "Gradient: [  -1.1349   23.0399    7.3914  -17.8732 -341.5712]\n",
      "Weights: [-4.8079  0.8199 -1.3575  0.1883  0.131 ]\n",
      "MSE loss: 90.5315\n",
      "Iteration: 163200\n",
      "Gradient: [  -3.7741    6.8648    8.417     7.6076 -608.6635]\n",
      "Weights: [-4.8179  0.8164 -1.3587  0.1893  0.1311]\n",
      "MSE loss: 90.9783\n",
      "Iteration: 163300\n",
      "Gradient: [ -4.7991  29.2037  53.5924  65.0152 -22.5946]\n",
      "Weights: [-4.7981  0.8246 -1.3585  0.1889  0.131 ]\n",
      "MSE loss: 90.7466\n",
      "Iteration: 163400\n",
      "Gradient: [  5.959  -21.4153 -15.326    7.6009 -63.9589]\n",
      "Weights: [-4.7979  0.8277 -1.3566  0.1885  0.1309]\n",
      "MSE loss: 91.3321\n",
      "Iteration: 163500\n",
      "Gradient: [  6.1497   0.4042   8.3739  92.4213 134.6834]\n",
      "Weights: [-4.8251  0.8263 -1.3566  0.1887  0.1307]\n",
      "MSE loss: 90.8276\n",
      "Iteration: 163600\n",
      "Gradient: [ -1.7885  14.8701   3.5961  89.2806 262.8168]\n",
      "Weights: [-4.802   0.8191 -1.3579  0.1895  0.1309]\n",
      "MSE loss: 90.5485\n",
      "Iteration: 163700\n",
      "Gradient: [ -7.1278   0.7073  -1.7643 104.2077 247.1718]\n",
      "Weights: [-4.817   0.8193 -1.3595  0.1905  0.1309]\n",
      "MSE loss: 90.7448\n",
      "Iteration: 163800\n",
      "Gradient: [ -3.8377   1.8614   1.7832  13.6347 -30.5528]\n",
      "Weights: [-4.7875  0.8149 -1.362   0.1898  0.1311]\n",
      "MSE loss: 90.5382\n",
      "Iteration: 163900\n",
      "Gradient: [  6.9477 -36.1069 -11.2173  14.0575 -84.9938]\n",
      "Weights: [-4.8016  0.8241 -1.3635  0.1891  0.1311]\n",
      "MSE loss: 90.519\n",
      "Iteration: 164000\n",
      "Gradient: [  4.7786 -14.3681  -9.163   60.2853 184.6971]\n",
      "Weights: [-4.8094  0.8263 -1.3629  0.1893  0.1311]\n",
      "MSE loss: 90.5054\n",
      "Iteration: 164100\n",
      "Gradient: [ 0.6663  5.2381 14.5049 21.5566 72.9882]\n",
      "Weights: [-4.7991  0.8247 -1.362   0.1886  0.1313]\n",
      "MSE loss: 90.4754\n",
      "Iteration: 164200\n",
      "Gradient: [ 1.534130e+01 -1.210000e-02  6.813550e+01  4.127710e+01  1.962982e+02]\n",
      "Weights: [-4.808   0.8312 -1.3606  0.1883  0.1313]\n",
      "MSE loss: 90.6265\n",
      "Iteration: 164300\n",
      "Gradient: [  16.6156  -26.4944   27.5249  -54.4734 -135.4654]\n",
      "Weights: [-4.7888  0.8242 -1.3647  0.1886  0.1313]\n",
      "MSE loss: 90.6079\n",
      "Iteration: 164400\n",
      "Gradient: [  5.5867  -9.1654  40.1341  -0.6057 128.8094]\n",
      "Weights: [-4.7996  0.8256 -1.3614  0.1879  0.1313]\n",
      "MSE loss: 90.4721\n",
      "Iteration: 164500\n",
      "Gradient: [  7.1938  -1.2131  12.4754 -55.3946 359.8731]\n",
      "Weights: [-4.7903  0.8254 -1.3609  0.1878  0.1312]\n",
      "MSE loss: 90.661\n",
      "Iteration: 164600\n",
      "Gradient: [  9.206    4.9333 -48.6827   0.5433 163.8792]\n",
      "Weights: [-4.7963  0.8298 -1.3624  0.1887  0.1312]\n",
      "MSE loss: 90.6976\n",
      "Iteration: 164700\n",
      "Gradient: [ -9.7031 -19.9923 -53.3936  27.7957  36.543 ]\n",
      "Weights: [-4.7948  0.8222 -1.3668  0.1894  0.1314]\n",
      "MSE loss: 90.5804\n",
      "Iteration: 164800\n",
      "Gradient: [ 3.664000e-01 -9.584900e+00 -4.229230e+01  4.358990e+01 -4.018422e+02]\n",
      "Weights: [-4.8158  0.8275 -1.3655  0.1899  0.1312]\n",
      "MSE loss: 90.7523\n",
      "Iteration: 164900\n",
      "Gradient: [-11.7678   3.8623  10.6379   7.2665 139.5248]\n",
      "Weights: [-4.8009  0.8266 -1.3635  0.1897  0.131 ]\n",
      "MSE loss: 90.449\n",
      "Iteration: 165000\n",
      "Gradient: [ -0.9162 -10.7921  11.7117 -11.5207 -45.6832]\n",
      "Weights: [-4.8041  0.8294 -1.3646  0.1897  0.1312]\n",
      "MSE loss: 90.4927\n",
      "Iteration: 165100\n",
      "Gradient: [ -1.0643  -8.5019  19.4108 -25.1083 -78.8876]\n",
      "Weights: [-4.8051  0.8275 -1.3665  0.1898  0.1314]\n",
      "MSE loss: 90.4697\n",
      "Iteration: 165200\n",
      "Gradient: [  -3.0297  -10.6202   41.0258   40.5853 -278.7144]\n",
      "Weights: [-4.7898  0.8235 -1.3692  0.191   0.1312]\n",
      "MSE loss: 90.5133\n",
      "Iteration: 165300\n",
      "Gradient: [  -6.9846   -7.7314    8.4278   60.9792 -104.6875]\n",
      "Weights: [-4.8018  0.8193 -1.3672  0.1908  0.1312]\n",
      "MSE loss: 90.6809\n",
      "Iteration: 165400\n",
      "Gradient: [  4.0645   4.782   27.4887 -73.2546  24.7711]\n",
      "Weights: [-4.8047  0.8326 -1.3699  0.1906  0.1313]\n",
      "MSE loss: 90.4364\n",
      "Iteration: 165500\n",
      "Gradient: [ -6.923   17.9928 -22.4228 100.7564 192.2062]\n",
      "Weights: [-4.8027  0.8313 -1.372   0.1916  0.131 ]\n",
      "MSE loss: 90.5281\n",
      "Iteration: 165600\n",
      "Gradient: [ -4.7783   7.9346 -31.8958  38.2432 394.643 ]\n",
      "Weights: [-4.8119  0.8349 -1.3721  0.1917  0.131 ]\n",
      "MSE loss: 90.5586\n",
      "Iteration: 165700\n",
      "Gradient: [11.197  -0.5234 20.519  48.7674  6.35  ]\n",
      "Weights: [-4.794   0.8357 -1.3727  0.1925  0.1308]\n",
      "MSE loss: 90.652\n",
      "Iteration: 165800\n",
      "Gradient: [ -2.6542  -5.7839  23.383  -56.6919  86.7967]\n",
      "Weights: [-4.8102  0.8467 -1.3723  0.1915  0.1308]\n",
      "MSE loss: 90.4807\n",
      "Iteration: 165900\n",
      "Gradient: [-11.8501   6.7735  -4.5041  29.376  354.9385]\n",
      "Weights: [-4.83    0.8503 -1.3726  0.1919  0.1306]\n",
      "MSE loss: 90.6688\n",
      "Iteration: 166000\n",
      "Gradient: [  1.9013 -18.2764 -13.1465 -32.8325 186.5917]\n",
      "Weights: [-4.81    0.8408 -1.3729  0.1921  0.1307]\n",
      "MSE loss: 90.4161\n",
      "Iteration: 166100\n",
      "Gradient: [  10.631     5.2599  -23.9642   74.689  -130.3805]\n",
      "Weights: [-4.8052  0.8416 -1.3709  0.1921  0.1307]\n",
      "MSE loss: 90.5994\n",
      "Iteration: 166200\n",
      "Gradient: [  10.3795   11.3925   29.3428   13.6587 -227.5185]\n",
      "Weights: [-4.8083  0.8453 -1.3713  0.1921  0.1303]\n",
      "MSE loss: 90.4763\n",
      "Iteration: 166300\n",
      "Gradient: [  3.2081  18.7178  35.9969 146.4483 296.1923]\n",
      "Weights: [-4.8195  0.8466 -1.3667  0.1915  0.1302]\n",
      "MSE loss: 90.5559\n",
      "Iteration: 166400\n",
      "Gradient: [ -1.0823 -11.9853   1.3109  50.8064  63.2094]\n",
      "Weights: [-4.8073  0.8362 -1.3696  0.193   0.1303]\n",
      "MSE loss: 90.4135\n",
      "Iteration: 166500\n",
      "Gradient: [ -2.7123  -5.771   47.5553  66.8756 -30.3663]\n",
      "Weights: [-4.7991  0.8376 -1.3701  0.1935  0.13  ]\n",
      "MSE loss: 90.6125\n",
      "Iteration: 166600\n",
      "Gradient: [ -0.2391  21.3965  15.5337 129.2167 116.5628]\n",
      "Weights: [-4.8008  0.8344 -1.3683  0.1932  0.1298]\n",
      "MSE loss: 90.4813\n",
      "Iteration: 166700\n",
      "Gradient: [ 6.080000e-02 -1.757930e+01  7.837200e+00  1.213819e+02  9.723740e+01]\n",
      "Weights: [-4.8292  0.8417 -1.3646  0.1925  0.1297]\n",
      "MSE loss: 90.6321\n",
      "Iteration: 166800\n",
      "Gradient: [  3.0887  -0.1743 -22.1239 -48.8763  60.7948]\n",
      "Weights: [-4.8261  0.8408 -1.367   0.1932  0.1296]\n",
      "MSE loss: 90.7152\n",
      "Iteration: 166900\n",
      "Gradient: [ -0.6844   2.1184  50.6668  56.9029 140.8537]\n",
      "Weights: [-4.8123  0.8428 -1.3673  0.1934  0.1294]\n",
      "MSE loss: 90.5112\n",
      "Iteration: 167000\n",
      "Gradient: [ -4.4288 -12.9448 -34.4285 -77.3344 -71.6156]\n",
      "Weights: [-4.812   0.8416 -1.3689  0.1945  0.1295]\n",
      "MSE loss: 90.5362\n",
      "Iteration: 167100\n",
      "Gradient: [ -1.5008  19.9744  40.1392 158.6382  23.1575]\n",
      "Weights: [-4.8208  0.8404 -1.3683  0.1943  0.1298]\n",
      "MSE loss: 90.5121\n",
      "Iteration: 167200\n",
      "Gradient: [   0.8712  -18.0949  -32.6767  -33.697  -156.9566]\n",
      "Weights: [-4.7889  0.8222 -1.3702  0.1946  0.13  ]\n",
      "MSE loss: 90.4828\n",
      "Iteration: 167300\n",
      "Gradient: [  -5.0381  -22.0395  -27.0872 -176.5491 -241.7286]\n",
      "Weights: [-4.797   0.8169 -1.3689  0.1941  0.1301]\n",
      "MSE loss: 90.7793\n",
      "Iteration: 167400\n",
      "Gradient: [  -5.6437  -21.2479  -20.7864    0.4488 -224.3689]\n",
      "Weights: [-4.8013  0.82   -1.3694  0.195   0.1302]\n",
      "MSE loss: 90.4216\n",
      "Iteration: 167500\n",
      "Gradient: [  2.7221  -2.5186  10.532  -36.1657  35.6362]\n",
      "Weights: [-4.8125  0.8439 -1.3728  0.1945  0.13  ]\n",
      "MSE loss: 90.4547\n",
      "Iteration: 167600\n",
      "Gradient: [  -3.5553  -18.9038  -32.1828 -120.4086  -92.9694]\n",
      "Weights: [-4.8188  0.8522 -1.3741  0.1939  0.1298]\n",
      "MSE loss: 90.445\n",
      "Iteration: 167700\n",
      "Gradient: [  10.4616    4.0594    7.0664  -61.4884 -135.3608]\n",
      "Weights: [-4.8021  0.839  -1.3719  0.1933  0.13  ]\n",
      "MSE loss: 90.4651\n",
      "Iteration: 167800\n",
      "Gradient: [ -12.4576   -6.7385  -10.3161 -103.2507 -221.1553]\n",
      "Weights: [-4.8109  0.8387 -1.3726  0.1946  0.13  ]\n",
      "MSE loss: 90.3705\n",
      "Iteration: 167900\n",
      "Gradient: [ -2.9051  -4.0365 -30.034   82.4628  80.0572]\n",
      "Weights: [-4.8121  0.8377 -1.3728  0.1949  0.1298]\n",
      "MSE loss: 90.4184\n",
      "Iteration: 168000\n",
      "Gradient: [-6.425000e-01 -1.602000e-01 -1.857590e+01  9.394700e+00 -2.885832e+02]\n",
      "Weights: [-4.7988  0.845  -1.3736  0.1945  0.1297]\n",
      "MSE loss: 90.742\n",
      "Iteration: 168100\n",
      "Gradient: [  -3.5135  -26.4526  -49.501    46.1842 -359.2303]\n",
      "Weights: [-4.8206  0.8374 -1.3745  0.1952  0.1301]\n",
      "MSE loss: 90.6377\n",
      "Iteration: 168200\n",
      "Gradient: [  2.9947   2.5962 -33.0552 -88.5055  43.8604]\n",
      "Weights: [-4.8052  0.8392 -1.377   0.1958  0.1302]\n",
      "MSE loss: 90.4075\n",
      "Iteration: 168300\n",
      "Gradient: [  0.6334  14.819   -3.1929  53.0965 192.0341]\n",
      "Weights: [-4.8069  0.8354 -1.3752  0.1957  0.1302]\n",
      "MSE loss: 90.4132\n",
      "Iteration: 168400\n",
      "Gradient: [ 4.676300e+00  3.949000e-01  1.952620e+01 -2.631050e+01 -5.253199e+02]\n",
      "Weights: [-4.7947  0.8306 -1.3756  0.1954  0.1301]\n",
      "MSE loss: 90.4335\n",
      "Iteration: 168500\n",
      "Gradient: [  7.1689 -19.4344  11.6474  14.2762  70.8023]\n",
      "Weights: [-4.804   0.8253 -1.373   0.1952  0.1305]\n",
      "MSE loss: 90.453\n",
      "Iteration: 168600\n",
      "Gradient: [ -4.0191  -7.0223  37.8709 104.0449  97.396 ]\n",
      "Weights: [-4.8066  0.8352 -1.3749  0.1948  0.1302]\n",
      "MSE loss: 90.3919\n",
      "Iteration: 168700\n",
      "Gradient: [ -6.2319  -2.8455   3.9549 -44.3674  -7.3913]\n",
      "Weights: [-4.8052  0.831  -1.3754  0.1951  0.1303]\n",
      "MSE loss: 90.4249\n",
      "Iteration: 168800\n",
      "Gradient: [  -2.7485  -18.8333   28.7086  -85.7135 -397.6555]\n",
      "Weights: [-4.7995  0.833  -1.3755  0.1956  0.1302]\n",
      "MSE loss: 90.406\n",
      "Iteration: 168900\n",
      "Gradient: [  15.5461   10.0753   19.6119  123.8005 -214.3505]\n",
      "Weights: [-4.7839  0.8252 -1.3721  0.1955  0.1299]\n",
      "MSE loss: 90.6907\n",
      "Iteration: 169000\n",
      "Gradient: [  -2.9269   -4.6935  -16.5026  -12.6003 -254.4826]\n",
      "Weights: [-4.8108  0.8317 -1.3734  0.195   0.13  ]\n",
      "MSE loss: 90.6366\n",
      "Iteration: 169100\n",
      "Gradient: [ 9.389100e+00  2.208000e-01 -4.620040e+01  9.002670e+01  3.133505e+02]\n",
      "Weights: [-4.8104  0.8383 -1.3753  0.1956  0.1301]\n",
      "MSE loss: 90.3696\n",
      "Iteration: 169200\n",
      "Gradient: [  5.3204   7.5173   9.372   72.3055 -82.3296]\n",
      "Weights: [-4.8097  0.841  -1.3765  0.1955  0.1298]\n",
      "MSE loss: 90.4366\n",
      "Iteration: 169300\n",
      "Gradient: [   1.1143   -5.3147   -2.7488   22.7457 -107.409 ]\n",
      "Weights: [-4.8083  0.845  -1.3772  0.196   0.1297]\n",
      "MSE loss: 90.3744\n",
      "Iteration: 169400\n",
      "Gradient: [ 9.5636  0.5068 28.0424 88.9548 99.2241]\n",
      "Weights: [-4.8024  0.841  -1.3774  0.1965  0.13  ]\n",
      "MSE loss: 90.6117\n",
      "Iteration: 169500\n",
      "Gradient: [  -5.7346   -3.0392    4.5269 -124.1654 -138.0096]\n",
      "Weights: [-4.8183  0.8398 -1.3786  0.1962  0.13  ]\n",
      "MSE loss: 90.7475\n",
      "Iteration: 169600\n",
      "Gradient: [ -7.0308 -30.305  -19.2323  16.0032 -89.501 ]\n",
      "Weights: [-4.8136  0.8365 -1.3794  0.1966  0.1301]\n",
      "MSE loss: 90.5429\n",
      "Iteration: 169700\n",
      "Gradient: [   5.5852   12.8553   42.9904  -10.7477 -161.6032]\n",
      "Weights: [-4.7996  0.8422 -1.3818  0.1968  0.1302]\n",
      "MSE loss: 90.446\n",
      "Iteration: 169800\n",
      "Gradient: [-6.73440e+00 -5.76000e-02 -4.66242e+01 -6.12398e+01 -1.60616e+02]\n",
      "Weights: [-4.818   0.8451 -1.384   0.1973  0.1301]\n",
      "MSE loss: 90.6377\n",
      "Iteration: 169900\n",
      "Gradient: [   2.0296  -23.2775   33.6043  -47.0048 -194.9742]\n",
      "Weights: [-4.8053  0.8484 -1.3863  0.1969  0.1302]\n",
      "MSE loss: 90.3739\n",
      "Iteration: 170000\n",
      "Gradient: [ -0.0504 -12.4562 -21.6787  -0.7593  22.5924]\n",
      "Weights: [-4.8283  0.8599 -1.3876  0.1973  0.1303]\n",
      "MSE loss: 90.5062\n",
      "Iteration: 170100\n",
      "Gradient: [  4.3368 -13.9837  11.6683 -26.8563 -77.0723]\n",
      "Weights: [-4.8313  0.8682 -1.3886  0.1969  0.1302]\n",
      "MSE loss: 90.4649\n",
      "Iteration: 170200\n",
      "Gradient: [  -4.3834  -20.658    43.2139  -99.0506 -327.7295]\n",
      "Weights: [-4.8221  0.8624 -1.3895  0.1973  0.1302]\n",
      "MSE loss: 90.3929\n",
      "Iteration: 170300\n",
      "Gradient: [ -1.4624   0.3801 -16.7467  38.9622 -94.7438]\n",
      "Weights: [-4.8167  0.8706 -1.3906  0.1968  0.1302]\n",
      "MSE loss: 90.4285\n",
      "Iteration: 170400\n",
      "Gradient: [  -1.0765    3.0159  -10.5488  -52.553  -312.7886]\n",
      "Weights: [-4.814   0.8624 -1.3902  0.1971  0.1301]\n",
      "MSE loss: 90.4257\n",
      "Iteration: 170500\n",
      "Gradient: [ 2.457  10.4286 20.774   1.2516 57.336 ]\n",
      "Weights: [-4.804   0.8659 -1.3914  0.198   0.1301]\n",
      "MSE loss: 90.8475\n",
      "Iteration: 170600\n",
      "Gradient: [   4.8606   11.5892  -12.7525  -23.2755 -272.2324]\n",
      "Weights: [-4.8177  0.8612 -1.3908  0.1983  0.1301]\n",
      "MSE loss: 90.3252\n",
      "Iteration: 170700\n",
      "Gradient: [ -0.5867  -1.1412 -42.5449 -77.9784 -71.474 ]\n",
      "Weights: [-4.8253  0.8656 -1.3915  0.1983  0.13  ]\n",
      "MSE loss: 90.4007\n",
      "Iteration: 170800\n",
      "Gradient: [ -2.0939  -0.3153  10.1608  57.2825 -33.9109]\n",
      "Weights: [-4.8062  0.8571 -1.3907  0.1989  0.1301]\n",
      "MSE loss: 90.4826\n",
      "Iteration: 170900\n",
      "Gradient: [   3.7411   17.4589  -23.723   -64.465  -108.7428]\n",
      "Weights: [-4.8145  0.8501 -1.3888  0.1986  0.13  ]\n",
      "MSE loss: 90.4381\n",
      "Iteration: 171000\n",
      "Gradient: [  0.3129   0.9519 -10.4833  19.0613 246.334 ]\n",
      "Weights: [-4.8167  0.8631 -1.3917  0.1984  0.1299]\n",
      "MSE loss: 90.3593\n",
      "Iteration: 171100\n",
      "Gradient: [-4.900000e-02 -7.974000e-01 -6.241100e+00  4.254400e+01  1.132972e+02]\n",
      "Weights: [-4.8149  0.8698 -1.3932  0.1982  0.1299]\n",
      "MSE loss: 90.3822\n",
      "Iteration: 171200\n",
      "Gradient: [   1.426   -12.8786   -4.8452 -102.6095  -21.258 ]\n",
      "Weights: [-4.8335  0.8762 -1.3932  0.199   0.1298]\n",
      "MSE loss: 90.4575\n",
      "Iteration: 171300\n",
      "Gradient: [  4.0121  -1.7789 -30.2172 -79.8755 354.8546]\n",
      "Weights: [-4.8408  0.8734 -1.3918  0.1991  0.1297]\n",
      "MSE loss: 90.64\n",
      "Iteration: 171400\n",
      "Gradient: [ -0.3335  -0.1654 -10.1241 -63.1483  13.3708]\n",
      "Weights: [-4.8182  0.8675 -1.3923  0.1996  0.1294]\n",
      "MSE loss: 90.339\n",
      "Iteration: 171500\n",
      "Gradient: [   8.5308    1.2165   24.3263   36.2745 -110.0852]\n",
      "Weights: [-4.8309  0.8692 -1.3931  0.1995  0.1295]\n",
      "MSE loss: 90.5919\n",
      "Iteration: 171600\n",
      "Gradient: [   3.6705    7.3002   18.9274  175.1306 -148.3231]\n",
      "Weights: [-4.8089  0.8709 -1.3945  0.1995  0.1298]\n",
      "MSE loss: 90.7318\n",
      "Iteration: 171700\n",
      "Gradient: [ -2.0111  -9.1254   7.3224  77.0328 -88.9308]\n",
      "Weights: [-4.8173  0.8688 -1.3962  0.2002  0.13  ]\n",
      "MSE loss: 90.5033\n",
      "Iteration: 171800\n",
      "Gradient: [  11.3946    4.3626  -54.1514  -66.9802 -208.7288]\n",
      "Weights: [-4.8126  0.8698 -1.3996  0.2004  0.1299]\n",
      "MSE loss: 90.3095\n",
      "Iteration: 171900\n",
      "Gradient: [  -8.0282   -3.209    12.0193  -84.5607 -204.3136]\n",
      "Weights: [-4.8087  0.8621 -1.3977  0.2004  0.1302]\n",
      "MSE loss: 90.402\n",
      "Iteration: 172000\n",
      "Gradient: [ -4.9367   3.2778 -22.2022 -49.7025 -62.3461]\n",
      "Weights: [-4.8067  0.8491 -1.3972  0.2011  0.1302]\n",
      "MSE loss: 90.4853\n",
      "Iteration: 172100\n",
      "Gradient: [  1.9885 -13.8858  -7.1831  64.2307 190.1684]\n",
      "Weights: [-4.805   0.8529 -1.3969  0.2008  0.1302]\n",
      "MSE loss: 90.3296\n",
      "Iteration: 172200\n",
      "Gradient: [  3.3917 -13.5906 -41.923  -24.0829 -87.3352]\n",
      "Weights: [-4.8125  0.8505 -1.3943  0.2007  0.1301]\n",
      "MSE loss: 90.4006\n",
      "Iteration: 172300\n",
      "Gradient: [   0.7466    7.1626  -19.1436  -33.4201 -109.9218]\n",
      "Weights: [-4.8152  0.8572 -1.3939  0.2002  0.1299]\n",
      "MSE loss: 90.3242\n",
      "Iteration: 172400\n",
      "Gradient: [   4.158    17.8339  -23.8225  -26.3967 -142.4507]\n",
      "Weights: [-4.8112  0.855  -1.3912  0.2005  0.1294]\n",
      "MSE loss: 90.2877\n",
      "Iteration: 172500\n",
      "Gradient: [  3.1958  26.8917  22.2848 187.7932  66.376 ]\n",
      "Weights: [-4.8018  0.8557 -1.3918  0.2     0.1295]\n",
      "MSE loss: 90.3926\n",
      "Iteration: 172600\n",
      "Gradient: [-2.229000e-01  1.893830e+01 -7.692800e+00  3.203400e+01  3.444323e+02]\n",
      "Weights: [-4.8095  0.855  -1.392   0.2     0.1299]\n",
      "MSE loss: 90.3226\n",
      "Iteration: 172700\n",
      "Gradient: [  5.6978  23.0835   1.7533 -20.0339 -33.7999]\n",
      "Weights: [-4.8109  0.8627 -1.3922  0.1995  0.1297]\n",
      "MSE loss: 90.3991\n",
      "Iteration: 172800\n",
      "Gradient: [   6.4579    8.6443    3.0152   69.0576 -296.6325]\n",
      "Weights: [-4.8162  0.8578 -1.389   0.1994  0.1296]\n",
      "MSE loss: 90.3133\n",
      "Iteration: 172900\n",
      "Gradient: [  3.3356  -8.2599  56.5131 119.4304 -12.4424]\n",
      "Weights: [-4.8068  0.8504 -1.3883  0.1998  0.1296]\n",
      "MSE loss: 90.3364\n",
      "Iteration: 173000\n",
      "Gradient: [ -11.2357  -11.3023  -16.6334   15.3571 -128.1257]\n",
      "Weights: [-4.8052  0.8542 -1.3913  0.2004  0.1294]\n",
      "MSE loss: 90.3243\n",
      "Iteration: 173100\n",
      "Gradient: [   5.0448   11.1727   19.2805   19.8032 -146.1828]\n",
      "Weights: [-4.8142  0.8512 -1.3909  0.2017  0.1295]\n",
      "MSE loss: 90.3897\n",
      "Iteration: 173200\n",
      "Gradient: [  4.8746 -13.6228 -17.3692  54.0259  -1.3448]\n",
      "Weights: [-4.7996  0.8513 -1.3957  0.2013  0.1295]\n",
      "MSE loss: 90.4546\n",
      "Iteration: 173300\n",
      "Gradient: [  0.8474  -3.4744  -9.3179 -84.9866 141.4638]\n",
      "Weights: [-4.814   0.8461 -1.3909  0.2017  0.1295]\n",
      "MSE loss: 90.3888\n",
      "Iteration: 173400\n",
      "Gradient: [  2.6001   8.9297  -7.1242 -72.5565 294.5411]\n",
      "Weights: [-4.8252  0.8449 -1.393   0.2023  0.1296]\n",
      "MSE loss: 91.115\n",
      "Iteration: 173500\n",
      "Gradient: [  -8.2163    8.178   -16.4506   89.1346 -505.8785]\n",
      "Weights: [-4.8093  0.849  -1.3902  0.2021  0.129 ]\n",
      "MSE loss: 90.2638\n",
      "Iteration: 173600\n",
      "Gradient: [-16.7662  20.3777 -14.9001  60.1085 114.9015]\n",
      "Weights: [-4.8228  0.8534 -1.3931  0.2027  0.1292]\n",
      "MSE loss: 90.4572\n",
      "Iteration: 173700\n",
      "Gradient: [ 7.2759  0.7015  6.1114  9.7913 92.0283]\n",
      "Weights: [-4.8136  0.864  -1.3938  0.2019  0.129 ]\n",
      "MSE loss: 90.3489\n",
      "Iteration: 173800\n",
      "Gradient: [ -2.1312  -9.7568 -20.2608  -9.7819 -60.0346]\n",
      "Weights: [-4.828   0.8546 -1.3928  0.203   0.129 ]\n",
      "MSE loss: 90.5949\n",
      "Iteration: 173900\n",
      "Gradient: [  -8.6889  -10.2374  -58.2352   -3.6091 -128.177 ]\n",
      "Weights: [-4.8245  0.85   -1.3913  0.2028  0.129 ]\n",
      "MSE loss: 90.6566\n",
      "Iteration: 174000\n",
      "Gradient: [ -0.9932 -11.3971 -51.7767  -8.6473  -5.0198]\n",
      "Weights: [-4.8048  0.8373 -1.3916  0.2039  0.1289]\n",
      "MSE loss: 90.3741\n",
      "Iteration: 174100\n",
      "Gradient: [ -6.6841   8.6171 -19.0188  98.5844 -98.0107]\n",
      "Weights: [-4.8116  0.8547 -1.3954  0.2035  0.1291]\n",
      "MSE loss: 90.2497\n",
      "Iteration: 174200\n",
      "Gradient: [  7.0956   3.7244 -11.4414 -74.0759 -27.4566]\n",
      "Weights: [-4.8082  0.8571 -1.3947  0.204   0.1289]\n",
      "MSE loss: 90.5204\n",
      "Iteration: 174300\n",
      "Gradient: [ -2.7533  -5.0646 -10.7743 -12.3815 287.4989]\n",
      "Weights: [-4.8096  0.857  -1.3923  0.2032  0.1287]\n",
      "MSE loss: 90.3498\n",
      "Iteration: 174400\n",
      "Gradient: [   3.9756   12.0949    3.1111  -28.8342 -122.8425]\n",
      "Weights: [-4.8017  0.8443 -1.3933  0.204   0.1287]\n",
      "MSE loss: 90.3209\n",
      "Iteration: 174500\n",
      "Gradient: [-9.032300e+00  4.382600e+00  1.366000e-01 -6.322000e-01 -2.818116e+02]\n",
      "Weights: [-4.806   0.8424 -1.3901  0.204   0.1285]\n",
      "MSE loss: 90.2542\n",
      "Iteration: 174600\n",
      "Gradient: [ -13.8213   -9.3459  -17.7832   26.3088 -212.7222]\n",
      "Weights: [-4.8126  0.8475 -1.3917  0.2042  0.1284]\n",
      "MSE loss: 90.3146\n",
      "Iteration: 174700\n",
      "Gradient: [  -3.5086    0.1738   -8.1555   59.6076 -141.9059]\n",
      "Weights: [-4.8203  0.8601 -1.3936  0.2038  0.1283]\n",
      "MSE loss: 90.363\n",
      "Iteration: 174800\n",
      "Gradient: [  13.7204  -11.8748   55.2172   13.6229 -457.6754]\n",
      "Weights: [-4.8231  0.8639 -1.3948  0.2059  0.1281]\n",
      "MSE loss: 90.4563\n",
      "Iteration: 174900\n",
      "Gradient: [  1.5845 -19.9632  -5.787  -76.1599 -60.1803]\n",
      "Weights: [-4.8127  0.8559 -1.3963  0.2058  0.1278]\n",
      "MSE loss: 90.5247\n",
      "Iteration: 175000\n",
      "Gradient: [ -9.7456   1.3128  -0.6865 -68.5236 264.9324]\n",
      "Weights: [-4.8265  0.8548 -1.3928  0.2066  0.1281]\n",
      "MSE loss: 90.4771\n",
      "Iteration: 175100\n",
      "Gradient: [ -1.3499  -4.6887 -11.627   89.8821   9.1537]\n",
      "Weights: [-4.8283  0.8618 -1.3971  0.2068  0.1279]\n",
      "MSE loss: 90.3569\n",
      "Iteration: 175200\n",
      "Gradient: [  5.7079   0.5744  19.6548  78.8427 -23.636 ]\n",
      "Weights: [-4.8004  0.8565 -1.3975  0.2064  0.128 ]\n",
      "MSE loss: 90.4203\n",
      "Iteration: 175300\n",
      "Gradient: [  4.8884   6.3384   2.677  123.3133 -46.7579]\n",
      "Weights: [-4.7957  0.8551 -1.3972  0.2059  0.1282]\n",
      "MSE loss: 90.5675\n",
      "Iteration: 175400\n",
      "Gradient: [ -6.4599  -4.8661  24.3253 -85.027  169.8796]\n",
      "Weights: [-4.7973  0.8351 -1.3958  0.2066  0.1287]\n",
      "MSE loss: 90.3017\n",
      "Iteration: 175500\n",
      "Gradient: [  -0.3815   -1.5504   30.0772  -10.8709 -334.3071]\n",
      "Weights: [-4.7969  0.8388 -1.3958  0.2054  0.1287]\n",
      "MSE loss: 90.3206\n",
      "Iteration: 175600\n",
      "Gradient: [  8.7693   8.5745   5.3657  91.5811 206.4677]\n",
      "Weights: [-4.8007  0.8488 -1.3971  0.2059  0.1286]\n",
      "MSE loss: 90.3162\n",
      "Iteration: 175700\n",
      "Gradient: [   2.2513   12.8875    6.237   -38.9512 -134.6717]\n",
      "Weights: [-4.8406  0.8634 -1.3986  0.2058  0.1285]\n",
      "MSE loss: 91.0373\n",
      "Iteration: 175800\n",
      "Gradient: [ -8.431    6.1308 -31.5525  55.7269 -30.823 ]\n",
      "Weights: [-4.8427  0.8738 -1.399   0.2054  0.1281]\n",
      "MSE loss: 90.7809\n",
      "Iteration: 175900\n",
      "Gradient: [ -8.0261  11.2513 -31.1266  -7.2837  12.943 ]\n",
      "Weights: [-4.8288  0.8708 -1.3988  0.2046  0.1283]\n",
      "MSE loss: 90.4195\n",
      "Iteration: 176000\n",
      "Gradient: [  -7.4433  -12.3471   -9.4503 -114.131   122.2106]\n",
      "Weights: [-4.814   0.8692 -1.3976  0.2045  0.1282]\n",
      "MSE loss: 90.3642\n",
      "Iteration: 176100\n",
      "Gradient: [ 12.6258  16.9058  -7.1504  32.0147 -99.2181]\n",
      "Weights: [-4.8133  0.8688 -1.3984  0.2052  0.1282]\n",
      "MSE loss: 90.4129\n",
      "Iteration: 176200\n",
      "Gradient: [ -7.3824  -3.0022   8.851   25.6686 -83.6939]\n",
      "Weights: [-4.8282  0.8514 -1.3954  0.2062  0.1285]\n",
      "MSE loss: 90.6244\n",
      "Iteration: 176300\n",
      "Gradient: [ -3.357  -12.7746  -6.7578  -5.1049 -70.5928]\n",
      "Weights: [-4.8301  0.8626 -1.3984  0.206   0.1281]\n",
      "MSE loss: 90.6055\n",
      "Iteration: 176400\n",
      "Gradient: [  -3.9189  -14.9964   43.2732 -140.4369 -193.5043]\n",
      "Weights: [-4.8213  0.8746 -1.4022  0.2064  0.128 ]\n",
      "MSE loss: 90.2739\n",
      "Iteration: 176500\n",
      "Gradient: [-18.9626   0.8396   5.7046   1.9357  64.7445]\n",
      "Weights: [-4.823   0.8598 -1.4017  0.2073  0.1282]\n",
      "MSE loss: 90.4116\n",
      "Iteration: 176600\n",
      "Gradient: [  -3.1231   -2.5573   -9.9313  -21.0582 -187.9681]\n",
      "Weights: [-4.8261  0.8644 -1.4008  0.2073  0.1281]\n",
      "MSE loss: 90.3029\n",
      "Iteration: 176700\n",
      "Gradient: [   0.7884    8.0367   22.0029  -58.86   -158.7179]\n",
      "Weights: [-4.8159  0.8599 -1.3972  0.2066  0.128 ]\n",
      "MSE loss: 90.2326\n",
      "Iteration: 176800\n",
      "Gradient: [-16.411   12.5931  11.8057  51.9616 264.9743]\n",
      "Weights: [-4.8264  0.8511 -1.3968  0.2065  0.1284]\n",
      "MSE loss: 90.6858\n",
      "Iteration: 176900\n",
      "Gradient: [  17.9801   -7.1754  -53.4416  -94.9838 -342.4063]\n",
      "Weights: [-4.8176  0.8678 -1.3986  0.2054  0.1281]\n",
      "MSE loss: 90.2658\n",
      "Iteration: 177000\n",
      "Gradient: [  -9.614   -26.1066  -22.466  -100.1081 -276.4819]\n",
      "Weights: [-4.8309  0.8556 -1.3959  0.2068  0.128 ]\n",
      "MSE loss: 90.633\n",
      "Iteration: 177100\n",
      "Gradient: [ -0.7483 -20.2881 -34.7972 -74.6404 -97.9459]\n",
      "Weights: [-4.821   0.8624 -1.3998  0.2066  0.1281]\n",
      "MSE loss: 90.2877\n",
      "Iteration: 177200\n",
      "Gradient: [-11.9236  15.2514  27.3428 -66.2605  27.6693]\n",
      "Weights: [-4.806   0.8548 -1.401   0.2073  0.1281]\n",
      "MSE loss: 90.2337\n",
      "Iteration: 177300\n",
      "Gradient: [-14.1338  -7.9362 -71.5953  74.4527 -27.3808]\n",
      "Weights: [-4.8154  0.8519 -1.3993  0.208   0.1282]\n",
      "MSE loss: 90.2799\n",
      "Iteration: 177400\n",
      "Gradient: [ -14.8777  -19.6469  -55.016   -75.1063 -400.7344]\n",
      "Weights: [-4.8222  0.8486 -1.3994  0.2086  0.128 ]\n",
      "MSE loss: 90.6445\n",
      "Iteration: 177500\n",
      "Gradient: [   1.9582    9.6675  -23.2767  -25.8707 -190.1502]\n",
      "Weights: [-4.8091  0.8504 -1.397   0.2084  0.1276]\n",
      "MSE loss: 90.1802\n",
      "Iteration: 177600\n",
      "Gradient: [-12.7843   8.3089 -58.7188  29.6263 -17.5854]\n",
      "Weights: [-4.8045  0.8424 -1.3971  0.2088  0.1276]\n",
      "MSE loss: 90.2502\n",
      "Iteration: 177700\n",
      "Gradient: [ -1.6775  -0.3253  11.4384 102.9993   7.4665]\n",
      "Weights: [-4.8184  0.8546 -1.3981  0.2087  0.1273]\n",
      "MSE loss: 90.3786\n",
      "Iteration: 177800\n",
      "Gradient: [-7.1178 13.3737  1.7264  7.2179 43.9418]\n",
      "Weights: [-4.7974  0.8462 -1.3973  0.2091  0.1272]\n",
      "MSE loss: 90.2671\n",
      "Iteration: 177900\n",
      "Gradient: [-1.53800e-01  6.34470e+00  1.32720e+00 -5.06098e+01 -2.00819e+02]\n",
      "Weights: [-4.82    0.8472 -1.3958  0.2096  0.1271]\n",
      "MSE loss: 90.4435\n",
      "Iteration: 178000\n",
      "Gradient: [  2.1114  20.7262   3.6489  45.4018 165.3391]\n",
      "Weights: [-4.8098  0.8452 -1.3964  0.2101  0.1271]\n",
      "MSE loss: 90.1698\n",
      "Iteration: 178100\n",
      "Gradient: [  5.9171  -2.9675 -12.2227  48.0394 231.4902]\n",
      "Weights: [-4.8274  0.856  -1.3974  0.2104  0.1271]\n",
      "MSE loss: 90.3753\n",
      "Iteration: 178200\n",
      "Gradient: [ -2.5505  -6.5024 -17.6441 -14.5233 365.53  ]\n",
      "Weights: [-4.8205  0.8531 -1.3968  0.2094  0.1271]\n",
      "MSE loss: 90.2821\n",
      "Iteration: 178300\n",
      "Gradient: [  -3.5706   -1.6385   51.1974  -40.754  -257.1592]\n",
      "Weights: [-4.8151  0.8532 -1.3952  0.2099  0.1269]\n",
      "MSE loss: 90.2432\n",
      "Iteration: 178400\n",
      "Gradient: [ 3.8041 11.6987 25.0683 14.7107 28.783 ]\n",
      "Weights: [-4.8107  0.8542 -1.3954  0.2102  0.127 ]\n",
      "MSE loss: 90.6857\n",
      "Iteration: 178500\n",
      "Gradient: [ -8.4465  -2.2806  13.255    4.6031 211.8005]\n",
      "Weights: [-4.8079  0.8502 -1.3971  0.2105  0.1271]\n",
      "MSE loss: 90.3543\n",
      "Iteration: 178600\n",
      "Gradient: [  -2.5966    0.3056   34.3712  -13.7742 -124.0884]\n",
      "Weights: [-4.7903  0.8429 -1.4007  0.2115  0.1272]\n",
      "MSE loss: 90.4221\n",
      "Iteration: 178700\n",
      "Gradient: [ 4.508   2.2471  9.2685 61.0775 69.2184]\n",
      "Weights: [-4.8102  0.8484 -1.4018  0.2112  0.1274]\n",
      "MSE loss: 90.1741\n",
      "Iteration: 178800\n",
      "Gradient: [   2.9822   -4.3818   25.4999 -101.1581  196.4689]\n",
      "Weights: [-4.8008  0.8514 -1.4043  0.2115  0.1272]\n",
      "MSE loss: 90.1658\n",
      "Iteration: 178900\n",
      "Gradient: [   6.5528   -7.9219   29.125     4.889  -292.6598]\n",
      "Weights: [-4.8146  0.8578 -1.4038  0.2109  0.1273]\n",
      "MSE loss: 90.1529\n",
      "Iteration: 179000\n",
      "Gradient: [  -3.3831  -13.2449    0.9628   69.5975 -358.1212]\n",
      "Weights: [-4.8198  0.8582 -1.4026  0.2107  0.1272]\n",
      "MSE loss: 90.2263\n",
      "Iteration: 179100\n",
      "Gradient: [-17.2055  -7.8647 -44.9184  -5.1976 -86.2385]\n",
      "Weights: [-4.8018  0.8567 -1.4036  0.2104  0.1272]\n",
      "MSE loss: 90.2364\n",
      "Iteration: 179200\n",
      "Gradient: [  3.5576  -0.4951  -6.3517  53.3634 159.7195]\n",
      "Weights: [-4.7884  0.843  -1.4045  0.2117  0.1274]\n",
      "MSE loss: 90.2937\n",
      "Iteration: 179300\n",
      "Gradient: [  0.8959   5.4247 -26.5477  68.3656  22.877 ]\n",
      "Weights: [-4.7853  0.8259 -1.4001  0.212   0.1273]\n",
      "MSE loss: 90.3267\n",
      "Iteration: 179400\n",
      "Gradient: [  5.4793  -4.0473  -4.3882 -58.7935 -31.4563]\n",
      "Weights: [-4.7985  0.8295 -1.3971  0.2118  0.1274]\n",
      "MSE loss: 90.2967\n",
      "Iteration: 179500\n",
      "Gradient: [   2.9819   12.4389  -30.9732  -68.5637 -185.6074]\n",
      "Weights: [-4.7934  0.8261 -1.3967  0.2109  0.1273]\n",
      "MSE loss: 90.3914\n",
      "Iteration: 179600\n",
      "Gradient: [  3.9211  -5.7589  -1.8173  23.8196 240.3057]\n",
      "Weights: [-4.799   0.8398 -1.3951  0.2105  0.127 ]\n",
      "MSE loss: 90.2076\n",
      "Iteration: 179700\n",
      "Gradient: [ -5.3005 -14.8593  32.8881 -75.4687   2.111 ]\n",
      "Weights: [-4.8141  0.8382 -1.3925  0.2103  0.1268]\n",
      "MSE loss: 90.3101\n",
      "Iteration: 179800\n",
      "Gradient: [ -3.5266  21.7373  27.8212 -31.3418 107.4285]\n",
      "Weights: [-4.789   0.8309 -1.3929  0.2099  0.1271]\n",
      "MSE loss: 90.2705\n",
      "Iteration: 179900\n",
      "Gradient: [  -3.3589  -21.7265  -12.789   -63.312  -435.7994]\n",
      "Weights: [-4.7872  0.8275 -1.3941  0.21    0.1273]\n",
      "MSE loss: 90.2768\n",
      "Iteration: 180000\n",
      "Gradient: [-8.1314  9.421  57.8735 68.2714 81.5784]\n",
      "Weights: [-4.7954  0.831  -1.3908  0.2094  0.1272]\n",
      "MSE loss: 90.2426\n",
      "Iteration: 180100\n",
      "Gradient: [   1.9833   11.3022  -10.758    57.5564 -278.3217]\n",
      "Weights: [-4.7968  0.8365 -1.3913  0.2091  0.1273]\n",
      "MSE loss: 90.3979\n",
      "Iteration: 180200\n",
      "Gradient: [ -5.2617   4.7238 -16.1489  21.3625  49.4716]\n",
      "Weights: [-4.823   0.8359 -1.3921  0.2084  0.1277]\n",
      "MSE loss: 90.9376\n",
      "Iteration: 180300\n",
      "Gradient: [ 8.9838 40.9043  7.2167 29.2731 84.3646]\n",
      "Weights: [-4.7989  0.8371 -1.3925  0.2084  0.1276]\n",
      "MSE loss: 90.2566\n",
      "Iteration: 180400\n",
      "Gradient: [ 0.322  -4.4629 22.6533 97.0772 74.354 ]\n",
      "Weights: [-4.7899  0.838  -1.3907  0.2082  0.1275]\n",
      "MSE loss: 90.8558\n",
      "Iteration: 180500\n",
      "Gradient: [-14.275    9.1767   9.2502 -20.327   55.2011]\n",
      "Weights: [-4.8003  0.8393 -1.3926  0.2083  0.1276]\n",
      "MSE loss: 90.2344\n",
      "Iteration: 180600\n",
      "Gradient: [  1.8429  -8.4731 -68.8186  49.1553  30.8204]\n",
      "Weights: [-4.816   0.8439 -1.394   0.2075  0.1279]\n",
      "MSE loss: 90.3362\n",
      "Iteration: 180700\n",
      "Gradient: [  7.6185  -2.9226  28.3444  46.8943 -28.323 ]\n",
      "Weights: [-4.8027  0.8444 -1.393   0.2079  0.1276]\n",
      "MSE loss: 90.3251\n",
      "Iteration: 180800\n",
      "Gradient: [   3.3029   -1.4804   -4.8401   73.2132 -143.0062]\n",
      "Weights: [-4.814   0.8441 -1.3923  0.2082  0.1275]\n",
      "MSE loss: 90.2344\n",
      "Iteration: 180900\n",
      "Gradient: [ 3.3009  1.1013 11.9906 66.5293 26.0889]\n",
      "Weights: [-4.794   0.8326 -1.392   0.2088  0.1274]\n",
      "MSE loss: 90.2255\n",
      "Iteration: 181000\n",
      "Gradient: [  0.796   11.7101 -26.4675  19.7619 240.4472]\n",
      "Weights: [-4.8003  0.83   -1.3879  0.2086  0.1273]\n",
      "MSE loss: 90.2206\n",
      "Iteration: 181100\n",
      "Gradient: [  0.6059   2.7645  43.3524  21.3851 168.9287]\n",
      "Weights: [-4.7904  0.8253 -1.3873  0.2085  0.1274]\n",
      "MSE loss: 90.3342\n",
      "Iteration: 181200\n",
      "Gradient: [   1.1321    8.7033   -2.0751   89.673  -109.3507]\n",
      "Weights: [-4.7912  0.8195 -1.3864  0.2093  0.127 ]\n",
      "MSE loss: 90.2198\n",
      "Iteration: 181300\n",
      "Gradient: [   0.9417  -15.9098  -22.895    44.5248 -117.4689]\n",
      "Weights: [-4.7988  0.828  -1.3876  0.2099  0.1269]\n",
      "MSE loss: 90.2646\n",
      "Iteration: 181400\n",
      "Gradient: [   2.1292   14.686    -5.5128  -54.733  -269.7877]\n",
      "Weights: [-4.8045  0.8286 -1.3867  0.2098  0.1269]\n",
      "MSE loss: 90.2629\n",
      "Iteration: 181500\n",
      "Gradient: [  8.7528  21.3546 -33.324  -16.0606 -14.7272]\n",
      "Weights: [-4.793   0.8295 -1.3871  0.2098  0.1268]\n",
      "MSE loss: 90.5227\n",
      "Iteration: 181600\n",
      "Gradient: [   4.3025   16.121   -14.7885 -129.4478 -402.3842]\n",
      "Weights: [-4.8038  0.8247 -1.3843  0.2094  0.1268]\n",
      "MSE loss: 90.2386\n",
      "Iteration: 181700\n",
      "Gradient: [  -1.2749   -5.6276  -21.83     86.2151 -130.1196]\n",
      "Weights: [-4.7916  0.8205 -1.3864  0.2094  0.1268]\n",
      "MSE loss: 90.2401\n",
      "Iteration: 181800\n",
      "Gradient: [  0.8413   9.042   27.8854  70.2796 283.627 ]\n",
      "Weights: [-4.8018  0.8336 -1.3872  0.2096  0.1269]\n",
      "MSE loss: 90.5277\n",
      "Iteration: 181900\n",
      "Gradient: [ -3.3411  -5.3753   9.1267 -12.2818   1.1499]\n",
      "Weights: [-4.8053  0.8318 -1.3877  0.2102  0.1267]\n",
      "MSE loss: 90.257\n",
      "Iteration: 182000\n",
      "Gradient: [ 13.2881   1.2969   5.5255  20.572  257.5627]\n",
      "Weights: [-4.7896  0.831  -1.3917  0.2102  0.1268]\n",
      "MSE loss: 90.2771\n",
      "Iteration: 182100\n",
      "Gradient: [ -6.0238  -8.5166   9.2792  84.1546 -39.8253]\n",
      "Weights: [-4.8103  0.8353 -1.3919  0.2107  0.1268]\n",
      "MSE loss: 90.2284\n",
      "Iteration: 182200\n",
      "Gradient: [  -4.4776    6.6485   23.3331   46.971  -145.1883]\n",
      "Weights: [-4.8105  0.8374 -1.393   0.2107  0.1268]\n",
      "MSE loss: 90.2176\n",
      "Iteration: 182300\n",
      "Gradient: [ 7.9977 10.4141 66.308   3.0327 17.6914]\n",
      "Weights: [-4.8187  0.8383 -1.3922  0.2111  0.1268]\n",
      "MSE loss: 90.3565\n",
      "Iteration: 182400\n",
      "Gradient: [  -7.7303    7.4503    4.0284  -79.0584 -207.0719]\n",
      "Weights: [-4.7996  0.8265 -1.3907  0.2112  0.1267]\n",
      "MSE loss: 90.1848\n",
      "Iteration: 182500\n",
      "Gradient: [ -9.2953  24.3869  36.9677  59.8158 130.709 ]\n",
      "Weights: [-4.8121  0.8453 -1.3914  0.2117  0.1262]\n",
      "MSE loss: 90.4141\n",
      "Iteration: 182600\n",
      "Gradient: [   5.7311  -11.0848   18.8192 -117.5445 -346.6007]\n",
      "Weights: [-4.8047  0.837  -1.3943  0.2122  0.1262]\n",
      "MSE loss: 90.2052\n",
      "Iteration: 182700\n",
      "Gradient: [   0.5177  -10.9964   39.9292  -50.4087 -165.5336]\n",
      "Weights: [-4.8103  0.8302 -1.392   0.2126  0.1262]\n",
      "MSE loss: 90.3259\n",
      "Iteration: 182800\n",
      "Gradient: [ -5.4074   9.1196 -21.4332  47.9747 413.4172]\n",
      "Weights: [-4.8068  0.8427 -1.3965  0.2125  0.1263]\n",
      "MSE loss: 90.1358\n",
      "Iteration: 182900\n",
      "Gradient: [  4.9802 -16.9089  -4.9674 -15.3346  23.8301]\n",
      "Weights: [-4.7858  0.8283 -1.396   0.2129  0.1264]\n",
      "MSE loss: 90.256\n",
      "Iteration: 183000\n",
      "Gradient: [-12.3601 -22.9936 -10.7427 -33.4993   2.8894]\n",
      "Weights: [-4.7874  0.8259 -1.395   0.2131  0.1263]\n",
      "MSE loss: 90.2271\n",
      "Iteration: 183100\n",
      "Gradient: [-1.1624 26.5545  9.9653 -3.7665 57.4313]\n",
      "Weights: [-4.7883  0.8269 -1.3963  0.2144  0.1261]\n",
      "MSE loss: 90.1929\n",
      "Iteration: 183200\n",
      "Gradient: [ -9.2315   1.1126  21.9462  52.9959 -45.8403]\n",
      "Weights: [-4.7921  0.8272 -1.3956  0.2145  0.1261]\n",
      "MSE loss: 90.167\n",
      "Iteration: 183300\n",
      "Gradient: [-11.3804   5.0473 -42.1063 -29.858  -50.6439]\n",
      "Weights: [-4.8026  0.8224 -1.3961  0.2141  0.1262]\n",
      "MSE loss: 90.7345\n",
      "Iteration: 183400\n",
      "Gradient: [  7.7856   1.577   32.8046  34.9729 -62.4064]\n",
      "Weights: [-4.7808  0.8244 -1.3979  0.2151  0.1262]\n",
      "MSE loss: 90.3266\n",
      "Iteration: 183500\n",
      "Gradient: [  8.803   -6.5096  -7.7403  52.7365 127.3139]\n",
      "Weights: [-4.7731  0.8138 -1.3967  0.2153  0.1263]\n",
      "MSE loss: 90.4288\n",
      "Iteration: 183600\n",
      "Gradient: [ -1.0699   6.0455   1.9296 102.6292  47.6093]\n",
      "Weights: [-4.7933  0.8217 -1.3965  0.2151  0.1261]\n",
      "MSE loss: 90.1933\n",
      "Iteration: 183700\n",
      "Gradient: [  -9.7713   -1.8067  -28.5306 -116.3152 -217.2852]\n",
      "Weights: [-4.8046  0.8216 -1.3944  0.2151  0.1258]\n",
      "MSE loss: 90.6119\n",
      "Iteration: 183800\n",
      "Gradient: [ -5.3413  24.3397  28.4526  -7.6073 201.7182]\n",
      "Weights: [-4.7981  0.8238 -1.3956  0.2157  0.1259]\n",
      "MSE loss: 90.1555\n",
      "Iteration: 183900\n",
      "Gradient: [ 18.1073  17.9737   0.8884  47.1235 -39.7912]\n",
      "Weights: [-4.7873  0.8234 -1.3959  0.2155  0.1259]\n",
      "MSE loss: 90.2125\n",
      "Iteration: 184000\n",
      "Gradient: [   6.0055   13.5618  -12.7642   85.1082 -273.4266]\n",
      "Weights: [-4.7935  0.836  -1.3996  0.2149  0.1258]\n",
      "MSE loss: 90.2288\n",
      "Iteration: 184100\n",
      "Gradient: [   7.8358   15.3172  -36.857   -66.4361 -128.3228]\n",
      "Weights: [-4.8034  0.845  -1.3993  0.2153  0.1255]\n",
      "MSE loss: 90.1599\n",
      "Iteration: 184200\n",
      "Gradient: [  4.0159 -12.1304  62.9266  45.3747 488.4274]\n",
      "Weights: [-4.8072  0.8405 -1.3995  0.2158  0.1256]\n",
      "MSE loss: 90.1047\n",
      "Iteration: 184300\n",
      "Gradient: [2.067700e+00 4.232000e-01 3.835870e+01 5.136120e+01 4.245786e+02]\n",
      "Weights: [-4.7917  0.8328 -1.4001  0.216   0.1257]\n",
      "MSE loss: 90.1527\n",
      "Iteration: 184400\n",
      "Gradient: [  4.3712  14.7979 -22.7153  37.0732 -52.904 ]\n",
      "Weights: [-4.7952  0.842  -1.4021  0.2162  0.1257]\n",
      "MSE loss: 90.213\n",
      "Iteration: 184500\n",
      "Gradient: [  8.3934  -6.0017   1.0933  34.7318 114.2662]\n",
      "Weights: [-4.8027  0.8346 -1.4008  0.2169  0.1258]\n",
      "MSE loss: 90.1348\n",
      "Iteration: 184600\n",
      "Gradient: [ 2.1432  3.0077  9.7336 67.8021 28.0406]\n",
      "Weights: [-4.7955  0.8378 -1.4006  0.2156  0.1258]\n",
      "MSE loss: 90.1409\n",
      "Iteration: 184700\n",
      "Gradient: [ -6.5633  -6.0064  19.9319  -7.0711 214.3152]\n",
      "Weights: [-4.832   0.8479 -1.4003  0.2159  0.1259]\n",
      "MSE loss: 90.6438\n",
      "Iteration: 184800\n",
      "Gradient: [-2.2302  6.5832 -8.6759 26.4939 61.9175]\n",
      "Weights: [-4.8092  0.8483 -1.4009  0.216   0.1258]\n",
      "MSE loss: 90.3176\n",
      "Iteration: 184900\n",
      "Gradient: [-13.2419  11.67   -41.4668  51.1817  16.6529]\n",
      "Weights: [-4.8064  0.8455 -1.4013  0.2147  0.1259]\n",
      "MSE loss: 90.159\n",
      "Iteration: 185000\n",
      "Gradient: [   2.9355   13.1325   11.7649  102.209  -175.871 ]\n",
      "Weights: [-4.7935  0.8441 -1.4033  0.2149  0.126 ]\n",
      "MSE loss: 90.2251\n",
      "Iteration: 185100\n",
      "Gradient: [  -0.2434  -17.0782   51.6971  -72.0424 -179.4123]\n",
      "Weights: [-4.8004  0.8365 -1.4013  0.2151  0.1263]\n",
      "MSE loss: 90.1197\n",
      "Iteration: 185200\n",
      "Gradient: [ 11.6702  -2.9913  18.0129 -80.9884 -78.8201]\n",
      "Weights: [-4.7868  0.8309 -1.4022  0.2154  0.1264]\n",
      "MSE loss: 90.2122\n",
      "Iteration: 185300\n",
      "Gradient: [  5.1371  25.2478 -11.4435  70.8606 135.2465]\n",
      "Weights: [-4.812   0.834  -1.4014  0.2155  0.1266]\n",
      "MSE loss: 90.4138\n",
      "Iteration: 185400\n",
      "Gradient: [   0.4884   12.9355  -40.6372  115.3945 -201.2904]\n",
      "Weights: [-4.8086  0.8397 -1.4031  0.2149  0.1265]\n",
      "MSE loss: 90.2151\n",
      "Iteration: 185500\n",
      "Gradient: [ -5.6371 -20.976   -1.7186 -40.5446  60.0715]\n",
      "Weights: [-4.8068  0.8389 -1.4036  0.2148  0.1265]\n",
      "MSE loss: 90.2609\n",
      "Iteration: 185600\n",
      "Gradient: [ -3.205    3.6915 -15.074  -50.9593 -51.778 ]\n",
      "Weights: [-4.8045  0.8391 -1.4029  0.2154  0.1263]\n",
      "MSE loss: 90.1294\n",
      "Iteration: 185700\n",
      "Gradient: [ 8.7125 -3.003  32.0862  5.2901 24.2739]\n",
      "Weights: [-4.7986  0.8454 -1.4035  0.2145  0.1263]\n",
      "MSE loss: 90.14\n",
      "Iteration: 185800\n",
      "Gradient: [  7.0034 -12.4454  44.6358  -5.5108  61.5953]\n",
      "Weights: [-4.8132  0.8519 -1.405   0.2153  0.1262]\n",
      "MSE loss: 90.1128\n",
      "Iteration: 185900\n",
      "Gradient: [ -3.2175  -3.0106 -12.4278  19.139   77.9198]\n",
      "Weights: [-4.8126  0.8533 -1.4079  0.2154  0.1262]\n",
      "MSE loss: 90.2126\n",
      "Iteration: 186000\n",
      "Gradient: [ -7.7635 -15.3796   3.3595  83.0743  89.9814]\n",
      "Weights: [-4.814   0.8579 -1.4083  0.2161  0.126 ]\n",
      "MSE loss: 90.0882\n",
      "Iteration: 186100\n",
      "Gradient: [  -4.2368  -12.615   -11.0927   -9.7543 -365.973 ]\n",
      "Weights: [-4.8315  0.8701 -1.4115  0.2165  0.126 ]\n",
      "MSE loss: 90.2688\n",
      "Iteration: 186200\n",
      "Gradient: [  -7.7517   -8.2195  -17.0257  -38.3428 -103.7718]\n",
      "Weights: [-4.8178  0.8663 -1.4124  0.2164  0.1259]\n",
      "MSE loss: 90.1198\n",
      "Iteration: 186300\n",
      "Gradient: [  3.7427 -17.6966  -4.7873 -16.7405 -16.4607]\n",
      "Weights: [-4.8166  0.864  -1.4124  0.2164  0.1262]\n",
      "MSE loss: 90.0829\n",
      "Iteration: 186400\n",
      "Gradient: [  -4.2992   -2.5683    2.2841   96.84   -130.0158]\n",
      "Weights: [-4.7995  0.8546 -1.4149  0.2168  0.1264]\n",
      "MSE loss: 90.1145\n",
      "Iteration: 186500\n",
      "Gradient: [ -12.7563    3.9181    5.0951  -54.0221 -117.6923]\n",
      "Weights: [-4.7998  0.8468 -1.4108  0.2167  0.1264]\n",
      "MSE loss: 90.1018\n",
      "Iteration: 186600\n",
      "Gradient: [   4.706    -6.1753    0.7065 -105.021  -166.8866]\n",
      "Weights: [-4.8027  0.8469 -1.4082  0.2164  0.1262]\n",
      "MSE loss: 90.0801\n",
      "Iteration: 186700\n",
      "Gradient: [  -3.9285    6.8416    2.2922  -91.4743 -366.4315]\n",
      "Weights: [-4.8101  0.8471 -1.4075  0.2168  0.1262]\n",
      "MSE loss: 90.12\n",
      "Iteration: 186800\n",
      "Gradient: [  -4.14      7.3534  -13.6829  -29.1358 -276.1724]\n",
      "Weights: [-4.818   0.8498 -1.4077  0.2166  0.1261]\n",
      "MSE loss: 90.2704\n",
      "Iteration: 186900\n",
      "Gradient: [   2.6905   -0.3526   21.114  -103.3852 -197.8845]\n",
      "Weights: [-4.8116  0.8627 -1.4099  0.2162  0.126 ]\n",
      "MSE loss: 90.1505\n",
      "Iteration: 187000\n",
      "Gradient: [ -5.7469  10.2523   4.8762 -48.2014 155.4947]\n",
      "Weights: [-4.8203  0.8513 -1.4089  0.2167  0.1261]\n",
      "MSE loss: 90.4647\n",
      "Iteration: 187100\n",
      "Gradient: [  -5.2748   -5.624   -36.93   -226.9682 -312.5637]\n",
      "Weights: [-4.7944  0.8437 -1.4102  0.2166  0.1264]\n",
      "MSE loss: 90.1245\n",
      "Iteration: 187200\n",
      "Gradient: [  -1.8455  -16.9963    8.5911 -145.1674  -56.1086]\n",
      "Weights: [-4.8005  0.8402 -1.4052  0.2157  0.1265]\n",
      "MSE loss: 90.1327\n",
      "Iteration: 187300\n",
      "Gradient: [  2.2197  -4.9883  34.9471 -59.6846 325.0664]\n",
      "Weights: [-4.8164  0.8623 -1.4076  0.2156  0.1262]\n",
      "MSE loss: 90.2516\n",
      "Iteration: 187400\n",
      "Gradient: [   2.7186  -17.517    -6.9488   40.1278 -163.0708]\n",
      "Weights: [-4.8113  0.8599 -1.4085  0.2152  0.1261]\n",
      "MSE loss: 90.1193\n",
      "Iteration: 187500\n",
      "Gradient: [ -4.0475  14.077   11.346  -67.7856 313.3429]\n",
      "Weights: [-4.8286  0.8646 -1.408   0.2148  0.1262]\n",
      "MSE loss: 90.3176\n",
      "Iteration: 187600\n",
      "Gradient: [   3.655     0.9056   39.6811  105.937  -118.9039]\n",
      "Weights: [-4.8101  0.8654 -1.4091  0.2144  0.1261]\n",
      "MSE loss: 90.2032\n",
      "Iteration: 187700\n",
      "Gradient: [   4.1257   22.9331   19.4581   51.452  -174.7069]\n",
      "Weights: [-4.815   0.8697 -1.4114  0.2151  0.1262]\n",
      "MSE loss: 90.1441\n",
      "Iteration: 187800\n",
      "Gradient: [  3.8864   7.0891  46.0233 -56.2348 -34.7993]\n",
      "Weights: [-4.8165  0.8717 -1.4139  0.2157  0.1262]\n",
      "MSE loss: 90.1259\n",
      "Iteration: 187900\n",
      "Gradient: [  1.4733  -2.9096  15.7764 -42.7798 -55.9761]\n",
      "Weights: [-4.8126  0.8667 -1.415   0.2162  0.1263]\n",
      "MSE loss: 90.0866\n",
      "Iteration: 188000\n",
      "Gradient: [ -3.7015   3.7237  -1.1098  81.7687 -65.9754]\n",
      "Weights: [-4.8091  0.8647 -1.4137  0.2167  0.1262]\n",
      "MSE loss: 90.1098\n",
      "Iteration: 188100\n",
      "Gradient: [  -7.0874   -6.0608  -25.0124 -123.2947 -173.0017]\n",
      "Weights: [-4.8184  0.8622 -1.4139  0.217   0.1259]\n",
      "MSE loss: 90.3239\n",
      "Iteration: 188200\n",
      "Gradient: [-2.7595  1.7549 22.4902 94.9844  6.7066]\n",
      "Weights: [-4.7958  0.8547 -1.4121  0.2174  0.1259]\n",
      "MSE loss: 90.2218\n",
      "Iteration: 188300\n",
      "Gradient: [   0.7367    2.436   -15.316    13.4659 -266.8202]\n",
      "Weights: [-4.7958  0.8492 -1.4104  0.2179  0.1259]\n",
      "MSE loss: 90.2127\n",
      "Iteration: 188400\n",
      "Gradient: [  -3.2991  -16.848     2.5881  -54.4349 -151.8014]\n",
      "Weights: [-4.8088  0.8505 -1.4121  0.2176  0.1261]\n",
      "MSE loss: 90.1488\n",
      "Iteration: 188500\n",
      "Gradient: [  -8.1026  -13.3004   -5.5896 -146.2467 -122.2769]\n",
      "Weights: [-4.8062  0.8492 -1.4155  0.2184  0.1262]\n",
      "MSE loss: 90.2123\n",
      "Iteration: 188600\n",
      "Gradient: [   5.3597  -22.5395   37.6785   -1.4583 -218.704 ]\n",
      "Weights: [-4.7913  0.8546 -1.418   0.2191  0.126 ]\n",
      "MSE loss: 90.2018\n",
      "Iteration: 188700\n",
      "Gradient: [ 3.204  10.9746 34.9962 91.3315 37.9179]\n",
      "Weights: [-4.8166  0.8705 -1.4198  0.2184  0.1261]\n",
      "MSE loss: 90.0471\n",
      "Iteration: 188800\n",
      "Gradient: [ -4.0713 -10.0917   9.2191 -34.3943 -22.2636]\n",
      "Weights: [-4.8147  0.8735 -1.4176  0.2185  0.1259]\n",
      "MSE loss: 90.2636\n",
      "Iteration: 188900\n",
      "Gradient: [-15.4553  -8.6707  24.8979  73.4071   6.3248]\n",
      "Weights: [-4.8138  0.8746 -1.4194  0.2181  0.126 ]\n",
      "MSE loss: 90.0926\n",
      "Iteration: 189000\n",
      "Gradient: [  -0.6591   -4.9026  -42.409  -106.5812 -153.1192]\n",
      "Weights: [-4.8156  0.8741 -1.4194  0.2177  0.126 ]\n",
      "MSE loss: 90.1029\n",
      "Iteration: 189100\n",
      "Gradient: [ -0.8671 -10.5337 -23.0107 -78.6565  53.857 ]\n",
      "Weights: [-4.831   0.8815 -1.4225  0.2187  0.1261]\n",
      "MSE loss: 90.1708\n",
      "Iteration: 189200\n",
      "Gradient: [  2.8486 -11.6535 -15.0854  14.6661 129.4621]\n",
      "Weights: [-4.8229  0.8798 -1.4248  0.2198  0.1258]\n",
      "MSE loss: 90.0777\n",
      "Iteration: 189300\n",
      "Gradient: [ -2.1884  -7.0179   0.7654 -92.3386  -9.6099]\n",
      "Weights: [-4.8086  0.8739 -1.4248  0.2209  0.126 ]\n",
      "MSE loss: 90.3649\n",
      "Iteration: 189400\n",
      "Gradient: [ -1.6796  -0.5226 -12.0856  57.6626 167.4353]\n",
      "Weights: [-4.8194  0.8663 -1.4232  0.2211  0.1259]\n",
      "MSE loss: 90.1245\n",
      "Iteration: 189500\n",
      "Gradient: [  13.3182  -24.4312    7.7684 -155.0248 -184.5159]\n",
      "Weights: [-4.8124  0.8607 -1.4207  0.2208  0.1259]\n",
      "MSE loss: 90.0782\n",
      "Iteration: 189600\n",
      "Gradient: [   7.9194   16.0619   12.7128   70.0763 -142.768 ]\n",
      "Weights: [-4.8099  0.8593 -1.4186  0.2207  0.1259]\n",
      "MSE loss: 90.1802\n",
      "Iteration: 189700\n",
      "Gradient: [ 11.9374  18.7312  75.557   99.8294 -42.7176]\n",
      "Weights: [-4.7979  0.8634 -1.4195  0.2208  0.1257]\n",
      "MSE loss: 90.597\n",
      "Iteration: 189800\n",
      "Gradient: [   3.8851    9.6787  -39.8393    5.1827 -162.0532]\n",
      "Weights: [-4.7968  0.8523 -1.4186  0.2208  0.1257]\n",
      "MSE loss: 90.0775\n",
      "Iteration: 189900\n",
      "Gradient: [  -7.057    -6.0622   35.5797  -78.8705 -279.7091]\n",
      "Weights: [-4.8148  0.8462 -1.4155  0.2208  0.1256]\n",
      "MSE loss: 90.5528\n",
      "Iteration: 190000\n",
      "Gradient: [11.5647 12.3842 33.5729 -2.1792 -4.0622]\n",
      "Weights: [-4.8075  0.8609 -1.4163  0.2202  0.1254]\n",
      "MSE loss: 90.0501\n",
      "Iteration: 190100\n",
      "Gradient: [ -6.4532 -11.9677 -44.897   99.7687 -87.8372]\n",
      "Weights: [-4.818   0.8618 -1.4155  0.2195  0.1256]\n",
      "MSE loss: 90.0774\n",
      "Iteration: 190200\n",
      "Gradient: [ 6.7837 -1.984  -8.0155 25.9002 25.4378]\n",
      "Weights: [-4.8048  0.8555 -1.415   0.2194  0.1256]\n",
      "MSE loss: 90.0356\n",
      "Iteration: 190300\n",
      "Gradient: [  10.9165    7.8388   41.9513  162.5975 -550.7339]\n",
      "Weights: [-4.7989  0.858  -1.4151  0.2201  0.1255]\n",
      "MSE loss: 90.3782\n",
      "Iteration: 190400\n",
      "Gradient: [ -9.5765 -13.0786 -60.3174 -85.7606 -65.0166]\n",
      "Weights: [-4.819   0.8529 -1.4144  0.2205  0.1254]\n",
      "MSE loss: 90.2879\n",
      "Iteration: 190500\n",
      "Gradient: [  -6.6225   19.9724   40.856    91.8391 -114.7174]\n",
      "Weights: [-4.8292  0.8685 -1.4147  0.22    0.1254]\n",
      "MSE loss: 90.2455\n",
      "Iteration: 190600\n",
      "Gradient: [ -0.5123   0.2791   5.8697  34.469  138.3715]\n",
      "Weights: [-4.8085  0.8666 -1.415   0.2191  0.1252]\n",
      "MSE loss: 90.1416\n",
      "Iteration: 190700\n",
      "Gradient: [  -0.6953  -22.2937   33.8195   37.6842 -166.3323]\n",
      "Weights: [-4.8248  0.8747 -1.417   0.2195  0.1252]\n",
      "MSE loss: 90.119\n",
      "Iteration: 190800\n",
      "Gradient: [   4.0352    4.0718   -0.4715    3.0338 -233.4789]\n",
      "Weights: [-4.823   0.8693 -1.4182  0.2197  0.1252]\n",
      "MSE loss: 90.3416\n",
      "Iteration: 190900\n",
      "Gradient: [ -4.8642  25.7162   4.4903  -9.627  126.3012]\n",
      "Weights: [-4.8051  0.8606 -1.4163  0.2202  0.1253]\n",
      "MSE loss: 90.065\n",
      "Iteration: 191000\n",
      "Gradient: [   8.4228    1.8367   14.1111    7.628  -432.667 ]\n",
      "Weights: [-4.8074  0.8581 -1.4152  0.2194  0.1256]\n",
      "MSE loss: 90.0326\n",
      "Iteration: 191100\n",
      "Gradient: [  10.0019   -5.2679  -12.72   -150.5248  -76.3937]\n",
      "Weights: [-4.7946  0.8419 -1.4133  0.2196  0.1258]\n",
      "MSE loss: 90.0931\n",
      "Iteration: 191200\n",
      "Gradient: [ 2.2097  7.719  13.6475 -8.0951 11.7974]\n",
      "Weights: [-4.8047  0.845  -1.4109  0.2195  0.1256]\n",
      "MSE loss: 90.0597\n",
      "Iteration: 191300\n",
      "Gradient: [  7.3789   2.8158  55.798  -47.9042  73.883 ]\n",
      "Weights: [-4.8108  0.8533 -1.4098  0.2192  0.1253]\n",
      "MSE loss: 90.061\n",
      "Iteration: 191400\n",
      "Gradient: [  -3.932     0.5961   19.1356   81.5929 -149.0483]\n",
      "Weights: [-4.8084  0.8544 -1.4123  0.219   0.1255]\n",
      "MSE loss: 90.0387\n",
      "Iteration: 191500\n",
      "Gradient: [  -7.8757   -3.3913  -10.1689  -68.7336 -530.3209]\n",
      "Weights: [-4.7945  0.8396 -1.4112  0.2184  0.1257]\n",
      "MSE loss: 90.3821\n",
      "Iteration: 191600\n",
      "Gradient: [  1.571   -9.2975 -10.7807 -45.4101 -16.7183]\n",
      "Weights: [-4.8188  0.8628 -1.4125  0.2186  0.1256]\n",
      "MSE loss: 90.0987\n",
      "Iteration: 191700\n",
      "Gradient: [  -0.8003   -4.334    66.5892 -168.191  -159.5409]\n",
      "Weights: [-4.8009  0.8654 -1.4171  0.2187  0.1255]\n",
      "MSE loss: 90.2403\n",
      "Iteration: 191800\n",
      "Gradient: [   1.7744    5.6236   31.0692   77.4059 -289.2194]\n",
      "Weights: [-4.8009  0.8606 -1.419   0.2203  0.1255]\n",
      "MSE loss: 90.0746\n",
      "Iteration: 191900\n",
      "Gradient: [  5.6137   2.2933 -59.1052 -62.0002  10.6494]\n",
      "Weights: [-4.8069  0.8625 -1.4215  0.2213  0.1254]\n",
      "MSE loss: 90.0414\n",
      "Iteration: 192000\n",
      "Gradient: [ -1.8553  -7.2394   0.2775  47.6319 -95.0733]\n",
      "Weights: [-4.807   0.8601 -1.4208  0.2222  0.1252]\n",
      "MSE loss: 89.9919\n",
      "Iteration: 192100\n",
      "Gradient: [  3.5036  -0.8735  -5.6645 -24.2703 397.5251]\n",
      "Weights: [-4.813   0.8474 -1.417   0.2222  0.1253]\n",
      "MSE loss: 90.2849\n",
      "Iteration: 192200\n",
      "Gradient: [   4.4492   -8.7239  -18.2578  -46.2901 -153.311 ]\n",
      "Weights: [-4.8063  0.8585 -1.4186  0.2223  0.125 ]\n",
      "MSE loss: 90.0112\n",
      "Iteration: 192300\n",
      "Gradient: [ 12.9961  -4.361   40.3077  -4.1749 241.8024]\n",
      "Weights: [-4.8118  0.8634 -1.4202  0.2218  0.1251]\n",
      "MSE loss: 90.0192\n",
      "Iteration: 192400\n",
      "Gradient: [ 19.4928 -25.1433  19.0063  71.6933 156.9078]\n",
      "Weights: [-4.8028  0.8651 -1.4211  0.2218  0.1252]\n",
      "MSE loss: 90.1361\n",
      "Iteration: 192500\n",
      "Gradient: [13.4567 -9.0639 59.8023 -5.2814 61.9349]\n",
      "Weights: [-4.8102  0.8635 -1.4226  0.2225  0.1254]\n",
      "MSE loss: 90.0161\n",
      "Iteration: 192600\n",
      "Gradient: [  -3.3231  -11.041   -61.5101  -97.8957 -337.0594]\n",
      "Weights: [-4.8144  0.8668 -1.4208  0.2219  0.1251]\n",
      "MSE loss: 90.0114\n",
      "Iteration: 192700\n",
      "Gradient: [  7.2521   7.5557  13.4434 -37.0892 -37.8332]\n",
      "Weights: [-4.7883  0.8499 -1.4194  0.2225  0.1253]\n",
      "MSE loss: 90.2601\n",
      "Iteration: 192800\n",
      "Gradient: [   1.5397   -3.8584   25.3574   53.6508 -217.3564]\n",
      "Weights: [-4.8045  0.8574 -1.4204  0.2226  0.1254]\n",
      "MSE loss: 90.1577\n",
      "Iteration: 192900\n",
      "Gradient: [  2.0635   8.5169 -45.998   27.9114 121.1924]\n",
      "Weights: [-4.8152  0.8534 -1.4186  0.2227  0.1253]\n",
      "MSE loss: 90.1508\n",
      "Iteration: 193000\n",
      "Gradient: [ -2.1339  -4.5839  -0.7916  98.425  -96.6495]\n",
      "Weights: [-4.7968  0.8546 -1.4169  0.2223  0.1252]\n",
      "MSE loss: 90.5561\n",
      "Iteration: 193100\n",
      "Gradient: [ 10.5426  15.4134  25.6188  50.1063 357.0135]\n",
      "Weights: [-4.81    0.8685 -1.4186  0.2219  0.125 ]\n",
      "MSE loss: 90.3147\n",
      "Iteration: 193200\n",
      "Gradient: [ -2.9082  29.127  -11.9776 -40.669  442.4356]\n",
      "Weights: [-4.8155  0.8623 -1.422   0.2225  0.1251]\n",
      "MSE loss: 90.1152\n",
      "Iteration: 193300\n",
      "Gradient: [  1.3457  -2.0004 -54.6788 -83.8925 130.1317]\n",
      "Weights: [-4.8127  0.8595 -1.4208  0.2226  0.1252]\n",
      "MSE loss: 90.0279\n",
      "Iteration: 193400\n",
      "Gradient: [  5.5834   6.6589  17.6783 -13.4596 254.4326]\n",
      "Weights: [-4.8121  0.8508 -1.4175  0.2232  0.125 ]\n",
      "MSE loss: 90.0848\n",
      "Iteration: 193500\n",
      "Gradient: [ -0.3493  16.295  -30.742  127.9043  63.373 ]\n",
      "Weights: [-4.7984  0.8539 -1.4197  0.2231  0.1248]\n",
      "MSE loss: 90.0394\n",
      "Iteration: 193600\n",
      "Gradient: [  0.2622  18.1601  13.8444  62.5861 -37.5882]\n",
      "Weights: [-4.7874  0.8431 -1.4204  0.2238  0.1251]\n",
      "MSE loss: 90.101\n",
      "Iteration: 193700\n",
      "Gradient: [-9.930000e-02  3.538600e+00  3.816580e+01 -1.375550e+01 -2.569564e+02]\n",
      "Weights: [-4.7967  0.8505 -1.422   0.2247  0.1249]\n",
      "MSE loss: 90.0375\n",
      "Iteration: 193800\n",
      "Gradient: [ -1.0307  -8.2558   1.1623  29.3794 231.9514]\n",
      "Weights: [-4.786   0.8424 -1.423   0.2248  0.1248]\n",
      "MSE loss: 90.2676\n",
      "Iteration: 193900\n",
      "Gradient: [ 10.1433   2.2104 -47.4689 117.4695 135.5778]\n",
      "Weights: [-4.7807  0.8409 -1.4202  0.2246  0.125 ]\n",
      "MSE loss: 90.3577\n",
      "Iteration: 194000\n",
      "Gradient: [ -2.9692 -18.4662 -25.1725 100.0797  30.9419]\n",
      "Weights: [-4.8133  0.8481 -1.4183  0.2237  0.1247]\n",
      "MSE loss: 90.4129\n",
      "Iteration: 194100\n",
      "Gradient: [-12.4986  -1.8335   5.6224 200.1145  43.6226]\n",
      "Weights: [-4.809   0.8446 -1.415   0.2238  0.125 ]\n",
      "MSE loss: 90.2028\n",
      "Iteration: 194200\n",
      "Gradient: [   8.6507   20.3447  -18.3455 -122.7091  -84.717 ]\n",
      "Weights: [-4.7938  0.8408 -1.4151  0.2239  0.1246]\n",
      "MSE loss: 90.0242\n",
      "Iteration: 194300\n",
      "Gradient: [  -7.2731  -16.963   -19.884    32.605  -223.474 ]\n",
      "Weights: [-4.7931  0.8454 -1.4181  0.2235  0.1249]\n",
      "MSE loss: 90.0418\n",
      "Iteration: 194400\n",
      "Gradient: [   2.3146   -3.8353   13.1324   34.2668 -132.4123]\n",
      "Weights: [-4.7937  0.8352 -1.412   0.2234  0.1247]\n",
      "MSE loss: 90.0344\n",
      "Iteration: 194500\n",
      "Gradient: [  -0.6638   -2.7447    9.546    33.6215 -291.0622]\n",
      "Weights: [-4.8018  0.8347 -1.4107  0.2236  0.1246]\n",
      "MSE loss: 90.0654\n",
      "Iteration: 194600\n",
      "Gradient: [  -4.4896   16.2084   -3.7184  123.7039 -295.2026]\n",
      "Weights: [-4.8014  0.8375 -1.4137  0.224   0.1244]\n",
      "MSE loss: 90.1324\n",
      "Iteration: 194700\n",
      "Gradient: [ 3.7493 -3.6671  4.9546 48.3563  9.265 ]\n",
      "Weights: [-4.803   0.8419 -1.4126  0.2242  0.1242]\n",
      "MSE loss: 89.9975\n",
      "Iteration: 194800\n",
      "Gradient: [  0.5871 -11.1634  38.8418 -85.4725 169.3645]\n",
      "Weights: [-4.796   0.8315 -1.4095  0.2246  0.1243]\n",
      "MSE loss: 90.0999\n",
      "Iteration: 194900\n",
      "Gradient: [ -5.9676  -8.3723  12.2036 -87.0719  53.8981]\n",
      "Weights: [-4.7863  0.8284 -1.4101  0.224   0.1242]\n",
      "MSE loss: 90.1088\n",
      "Iteration: 195000\n",
      "Gradient: [ -0.533    0.5356 -21.069   37.5326 -19.3833]\n",
      "Weights: [-4.7971  0.8407 -1.4127  0.2234  0.1245]\n",
      "MSE loss: 90.0215\n",
      "Iteration: 195100\n",
      "Gradient: [   2.5291   -1.2154   30.5723   66.3763 -122.9471]\n",
      "Weights: [-4.7984  0.8475 -1.4108  0.2228  0.1242]\n",
      "MSE loss: 90.1374\n",
      "Iteration: 195200\n",
      "Gradient: [ -0.7297  12.6052  45.9294 -15.494   21.3347]\n",
      "Weights: [-4.8219  0.8501 -1.4112  0.2231  0.1242]\n",
      "MSE loss: 90.2473\n",
      "Iteration: 195300\n",
      "Gradient: [  1.9863  16.4888  21.6789  93.3247 270.3944]\n",
      "Weights: [-4.7888  0.8479 -1.4122  0.2233  0.1242]\n",
      "MSE loss: 90.6138\n",
      "Iteration: 195400\n",
      "Gradient: [  7.2997   6.5966   3.1095  32.8043 130.5204]\n",
      "Weights: [-4.7988  0.8395 -1.4136  0.2239  0.1247]\n",
      "MSE loss: 90.0814\n",
      "Iteration: 195500\n",
      "Gradient: [   2.9372    5.3989  -14.2724  -29.6215 -262.9721]\n",
      "Weights: [-4.7993  0.8446 -1.4157  0.2233  0.1246]\n",
      "MSE loss: 90.0267\n",
      "Iteration: 195600\n",
      "Gradient: [  7.6928  16.042   21.5029  75.1364 267.4427]\n",
      "Weights: [-4.7916  0.8442 -1.4154  0.2231  0.1248]\n",
      "MSE loss: 90.1053\n",
      "Iteration: 195700\n",
      "Gradient: [-1.045420e+01  4.561500e+00  2.912000e-01  8.791110e+01 -2.974331e+02]\n",
      "Weights: [-4.8222  0.8491 -1.4146  0.2235  0.1247]\n",
      "MSE loss: 90.3601\n",
      "Iteration: 195800\n",
      "Gradient: [ -2.6543  -3.3881  22.7767 -14.2602 -46.8669]\n",
      "Weights: [-4.8074  0.837  -1.4129  0.2237  0.1247]\n",
      "MSE loss: 90.1996\n",
      "Iteration: 195900\n",
      "Gradient: [-4.6475 -2.4297  3.9947 84.8764 36.8183]\n",
      "Weights: [-4.811   0.849  -1.419   0.2243  0.1246]\n",
      "MSE loss: 90.1974\n",
      "Iteration: 196000\n",
      "Gradient: [  6.2709  10.7938 -41.5028 -41.3875 -99.559 ]\n",
      "Weights: [-4.8087  0.8654 -1.4195  0.2235  0.1245]\n",
      "MSE loss: 90.1225\n",
      "Iteration: 196100\n",
      "Gradient: [  -3.4431   -1.9162   -5.4818   20.6952 -497.7783]\n",
      "Weights: [-4.8004  0.8454 -1.4237  0.2253  0.1248]\n",
      "MSE loss: 90.298\n",
      "Iteration: 196200\n",
      "Gradient: [   7.6775    5.6794   35.0636    7.9232 -243.1657]\n",
      "Weights: [-4.7917  0.8351 -1.4186  0.2259  0.1247]\n",
      "MSE loss: 90.0705\n",
      "Iteration: 196300\n",
      "Gradient: [  -4.2519  -11.166    -5.8622   41.8602 -261.5254]\n",
      "Weights: [-4.7953  0.841  -1.4174  0.2246  0.1244]\n",
      "MSE loss: 90.0969\n",
      "Iteration: 196400\n",
      "Gradient: [  -8.472    -5.6592   18.6622 -138.6504  128.2173]\n",
      "Weights: [-4.7946  0.8372 -1.416   0.2247  0.1245]\n",
      "MSE loss: 90.045\n",
      "Iteration: 196500\n",
      "Gradient: [ -9.252   -4.6235  -6.6671  22.6286 -10.2768]\n",
      "Weights: [-4.798   0.8375 -1.4112  0.2237  0.1244]\n",
      "MSE loss: 90.0102\n",
      "Iteration: 196600\n",
      "Gradient: [   4.4458   19.8347   -7.5892  -33.8779 -263.9122]\n",
      "Weights: [-4.7934  0.8337 -1.4094  0.223   0.1245]\n",
      "MSE loss: 90.0412\n",
      "Iteration: 196700\n",
      "Gradient: [ -3.449  -20.3034  32.9671  19.2412  48.5958]\n",
      "Weights: [-4.8045  0.8394 -1.4125  0.2233  0.1246]\n",
      "MSE loss: 90.0767\n",
      "Iteration: 196800\n",
      "Gradient: [   1.7802   16.2762  -67.0878  -93.0437 -104.0574]\n",
      "Weights: [-4.7932  0.8462 -1.4146  0.2228  0.1246]\n",
      "MSE loss: 90.0957\n",
      "Iteration: 196900\n",
      "Gradient: [   0.2316   -3.381    18.1526   16.5914 -121.9702]\n",
      "Weights: [-4.7868  0.8424 -1.4153  0.224   0.1244]\n",
      "MSE loss: 90.2285\n",
      "Iteration: 197000\n",
      "Gradient: [  -4.2147    7.3297   -1.2975  -10.7379 -283.7362]\n",
      "Weights: [-4.7982  0.8409 -1.414   0.2247  0.1243]\n",
      "MSE loss: 90.0347\n",
      "Iteration: 197100\n",
      "Gradient: [  6.1057  -3.5936  46.4503 -84.0694 148.8415]\n",
      "Weights: [-4.7769  0.8348 -1.4138  0.224   0.1243]\n",
      "MSE loss: 90.3644\n",
      "Iteration: 197200\n",
      "Gradient: [ 0.2028 20.3811 -9.056  -5.8156 -5.9974]\n",
      "Weights: [-4.7995  0.838  -1.4112  0.2241  0.1244]\n",
      "MSE loss: 90.0561\n",
      "Iteration: 197300\n",
      "Gradient: [  -7.8139   -2.9951    6.706    31.6997 -255.5877]\n",
      "Weights: [-4.7955  0.8405 -1.4113  0.224   0.1242]\n",
      "MSE loss: 90.1071\n",
      "Iteration: 197400\n",
      "Gradient: [   4.6509   26.9804   51.6163  141.7791 -167.0639]\n",
      "Weights: [-4.7956  0.8495 -1.4152  0.2245  0.1243]\n",
      "MSE loss: 90.4113\n",
      "Iteration: 197500\n",
      "Gradient: [-12.0087   5.9277  -6.045   55.0629 130.7411]\n",
      "Weights: [-4.8139  0.8516 -1.4136  0.2243  0.1241]\n",
      "MSE loss: 90.0351\n",
      "Iteration: 197600\n",
      "Gradient: [  -2.8909   11.6691    7.6068  -58.8851 -214.2724]\n",
      "Weights: [-4.7976  0.8468 -1.4145  0.2243  0.1241]\n",
      "MSE loss: 90.053\n",
      "Iteration: 197700\n",
      "Gradient: [  -1.8602    6.4606  -20.0026 -112.0894 -165.2505]\n",
      "Weights: [-4.7934  0.8397 -1.4125  0.2251  0.1241]\n",
      "MSE loss: 90.2244\n",
      "Iteration: 197800\n",
      "Gradient: [ -9.2778  16.4713  19.6163  12.0237 219.3334]\n",
      "Weights: [-4.8012  0.8506 -1.4144  0.2246  0.1242]\n",
      "MSE loss: 90.2747\n",
      "Iteration: 197900\n",
      "Gradient: [  8.1228  17.1646  14.2965 177.9239 192.5918]\n",
      "Weights: [-4.8007  0.8542 -1.4167  0.2247  0.1242]\n",
      "MSE loss: 90.2506\n",
      "Iteration: 198000\n",
      "Gradient: [-11.5271 -15.5921  -4.8977  39.5571 -76.0733]\n",
      "Weights: [-4.8074  0.8552 -1.417   0.2247  0.1242]\n",
      "MSE loss: 90.0736\n",
      "Iteration: 198100\n",
      "Gradient: [ -1.8545  -4.3306  -1.7649  71.8165 199.133 ]\n",
      "Weights: [-4.7978  0.8498 -1.4139  0.2242  0.1241]\n",
      "MSE loss: 90.2074\n",
      "Iteration: 198200\n",
      "Gradient: [  -0.7439   -3.3719  -22.643  -111.4905 -480.48  ]\n",
      "Weights: [-4.8032  0.8517 -1.4142  0.224   0.1241]\n",
      "MSE loss: 90.0451\n",
      "Iteration: 198300\n",
      "Gradient: [   9.7589    8.2935   21.01     26.9501 -300.1003]\n",
      "Weights: [-4.7875  0.8341 -1.4105  0.224   0.1242]\n",
      "MSE loss: 90.1158\n",
      "Iteration: 198400\n",
      "Gradient: [  -5.7583   -4.3037   -3.5486  -65.0481 -178.3354]\n",
      "Weights: [-4.8024  0.8362 -1.4112  0.224   0.124 ]\n",
      "MSE loss: 90.2382\n",
      "Iteration: 198500\n",
      "Gradient: [  0.7695  12.9893   8.1917  86.9195 215.0428]\n",
      "Weights: [-4.8155  0.8471 -1.4119  0.2248  0.1241]\n",
      "MSE loss: 90.1867\n",
      "Iteration: 198600\n",
      "Gradient: [   1.8434   -6.2387    1.9296   41.2407 -444.8866]\n",
      "Weights: [-4.8049  0.8488 -1.4144  0.2249  0.124 ]\n",
      "MSE loss: 90.0148\n",
      "Iteration: 198700\n",
      "Gradient: [  4.0645 -20.3957   1.7322   8.006  137.6455]\n",
      "Weights: [-4.7975  0.8399 -1.4133  0.2246  0.124 ]\n",
      "MSE loss: 90.0401\n",
      "Iteration: 198800\n",
      "Gradient: [  -2.4578  -20.9813  -75.985     5.9107 -118.2469]\n",
      "Weights: [-4.8166  0.8473 -1.4135  0.2248  0.124 ]\n",
      "MSE loss: 90.1538\n",
      "Iteration: 198900\n",
      "Gradient: [ -3.1267 -22.2004  43.5046 -80.1851  17.0652]\n",
      "Weights: [-4.785   0.841  -1.4136  0.2243  0.1243]\n",
      "MSE loss: 90.4022\n",
      "Iteration: 199000\n",
      "Gradient: [   2.7114   10.3355   -1.9253  -75.0029 -114.1499]\n",
      "Weights: [-4.8057  0.8501 -1.4127  0.2238  0.124 ]\n",
      "MSE loss: 90.0191\n",
      "Iteration: 199100\n",
      "Gradient: [-12.0421  -4.5019  23.8713 -48.5483  27.5302]\n",
      "Weights: [-4.8241  0.8584 -1.4139  0.2229  0.1243]\n",
      "MSE loss: 90.1847\n",
      "Iteration: 199200\n",
      "Gradient: [  -3.9948   -5.8676   28.8721  -49.159  -253.8388]\n",
      "Weights: [-4.8411  0.8676 -1.4167  0.2234  0.1243]\n",
      "MSE loss: 90.6677\n",
      "Iteration: 199300\n",
      "Gradient: [  -3.9059   10.0794   -8.3083   -5.3844 -167.8542]\n",
      "Weights: [-4.8155  0.8566 -1.4184  0.2244  0.1244]\n",
      "MSE loss: 90.0368\n",
      "Iteration: 199400\n",
      "Gradient: [ -9.6463   6.157   28.2966 -60.569  211.4318]\n",
      "Weights: [-4.8098  0.8437 -1.4163  0.225   0.1244]\n",
      "MSE loss: 90.1142\n",
      "Iteration: 199500\n",
      "Gradient: [ 2.687   5.044   3.1495 33.4145 65.5913]\n",
      "Weights: [-4.81    0.8427 -1.4164  0.2256  0.1243]\n",
      "MSE loss: 90.1159\n",
      "Iteration: 199600\n",
      "Gradient: [  0.6531  -7.4888  20.4309  -2.4396 144.968 ]\n",
      "Weights: [-4.8131  0.8482 -1.4152  0.2248  0.1242]\n",
      "MSE loss: 90.0599\n",
      "Iteration: 199700\n",
      "Gradient: [ -2.556    6.4498 -10.431  -57.8883 -62.9514]\n",
      "Weights: [-4.8036  0.8456 -1.4165  0.2246  0.1241]\n",
      "MSE loss: 90.1637\n",
      "Iteration: 199800\n",
      "Gradient: [ -6.0644  -4.6381 -24.163  -89.7726 238.134 ]\n",
      "Weights: [-4.805   0.8459 -1.4156  0.2248  0.1243]\n",
      "MSE loss: 89.9876\n",
      "Iteration: 199900\n",
      "Gradient: [  5.1529  -3.8732   3.1623  63.9001 297.4288]\n",
      "Weights: [-4.8053  0.8407 -1.4153  0.2256  0.1242]\n",
      "MSE loss: 90.0358\n"
     ]
    }
   ],
   "source": [
    "# Обучение на случайных батчак, по 10% датасета.\n",
    "# lr - индивидуальный для каждого из параметров модели.\n",
    "weights_3, losses_3, iter_final_3, fit_time_3 = model_fit(X_train, y_train,\n",
    "                                                          learning_rate=[1e-3, 1e-4, 1e-5, 1e-6, 1e-7],\n",
    "                                                          tolerance=(0.2**2 * N_points),\n",
    "                                                          beta=0,\n",
    "                                                          batch_ratio=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2309aeb6",
   "metadata": {},
   "source": [
    "Теперь добавим фильтр градиента (momentum) по правилу:\n",
    "\n",
    "${y[n] = \\beta y[n-1] + (1 - \\beta)x[n]}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63ccc912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Gradient: [ 1653.8697  2208.3991  3991.4472  8502.208  15523.765 ]\n",
      "Weights: [-0.0002 -0.     -0.     -0.     -0.    ]\n",
      "MSE loss: 39327.4031\n",
      "Iteration: 100\n",
      "Gradient: [   -4.2848   -39.7197   -56.1211  -292.5246 -1658.0983]\n",
      "Weights: [-5.1164 -0.3682  0.1336  0.0748  0.0307]\n",
      "MSE loss: 719.4727\n",
      "Iteration: 200\n",
      "Gradient: [  14.3254   65.2956   86.9634 -621.0712  200.0639]\n",
      "Weights: [-4.8299 -0.6496  0.1016  0.0894  0.0408]\n",
      "MSE loss: 477.3759\n",
      "Iteration: 300\n",
      "Gradient: [  16.1042  -17.8151   13.7967 -110.1817 -924.2089]\n",
      "Weights: [-4.6185 -0.8768  0.0856  0.0968  0.048 ]\n",
      "MSE loss: 349.1848\n",
      "Iteration: 400\n",
      "Gradient: [-20.5589  60.2301 -66.8076 -45.8473 129.61  ]\n",
      "Weights: [-4.4938 -1.0242  0.0709  0.1038  0.0534]\n",
      "MSE loss: 283.1345\n",
      "Iteration: 500\n",
      "Gradient: [   6.8061   19.129    15.2614   20.268  -462.8656]\n",
      "Weights: [-4.4271 -1.0956  0.0561  0.107   0.0573]\n",
      "MSE loss: 252.8433\n",
      "Iteration: 600\n",
      "Gradient: [ -14.0985   14.5096   11.1932  -50.3558 -166.5598]\n",
      "Weights: [-4.347  -1.1598  0.0402  0.1094  0.0607]\n",
      "MSE loss: 231.2212\n",
      "Iteration: 700\n",
      "Gradient: [  16.234   -12.196   -76.1488  -73.403  -679.5248]\n",
      "Weights: [-4.2731 -1.2167  0.0227  0.1107  0.0635]\n",
      "MSE loss: 217.1372\n",
      "Iteration: 800\n",
      "Gradient: [ -0.4614  32.2829  87.2208  28.3121 358.9512]\n",
      "Weights: [-4.2528 -1.2316  0.0102  0.1126  0.0658]\n",
      "MSE loss: 209.5453\n",
      "Iteration: 900\n",
      "Gradient: [ -10.3554  -11.1235    3.1041   93.1041 -617.1794]\n",
      "Weights: [-4.2311e+00 -1.2480e+00  2.2000e-03  1.1230e-01  6.7300e-02]\n",
      "MSE loss: 205.2895\n",
      "Iteration: 1000\n",
      "Gradient: [   6.9576   -3.8592   33.0659 -200.4896   82.2141]\n",
      "Weights: [-4.2195 -1.259  -0.0071  0.1132  0.0685]\n",
      "MSE loss: 202.088\n",
      "Iteration: 1100\n",
      "Gradient: [ -9.5187 -13.6012  -0.7406 -28.2704 354.4059]\n",
      "Weights: [-4.1852 -1.2633 -0.0184  0.1135  0.0697]\n",
      "MSE loss: 199.505\n",
      "Iteration: 1200\n",
      "Gradient: [ -11.0538    3.6396   -8.9156   44.8137 -305.8082]\n",
      "Weights: [-4.1806 -1.2732 -0.0242  0.1129  0.071 ]\n",
      "MSE loss: 197.1243\n",
      "Iteration: 1300\n",
      "Gradient: [-14.3226  -9.3976 -38.6438   8.4417  66.4388]\n",
      "Weights: [-4.212  -1.2623 -0.0297  0.1124  0.0722]\n",
      "MSE loss: 195.7122\n",
      "Iteration: 1400\n",
      "Gradient: [  12.7935  -35.8863  -68.1435  -71.5151 -444.3091]\n",
      "Weights: [-4.1989 -1.2447 -0.0447  0.1112  0.073 ]\n",
      "MSE loss: 193.3892\n",
      "Iteration: 1500\n",
      "Gradient: [ -8.6874  -1.0611  29.8044 -66.6619 124.4599]\n",
      "Weights: [-4.1911 -1.2295 -0.0492  0.1104  0.0741]\n",
      "MSE loss: 191.5656\n",
      "Iteration: 1600\n",
      "Gradient: [   2.0426    9.1636  -17.055   102.8789 -105.316 ]\n",
      "Weights: [-4.1621 -1.247  -0.0582  0.111   0.0749]\n",
      "MSE loss: 190.0216\n",
      "Iteration: 1700\n",
      "Gradient: [  3.273  -11.8269  24.2931 106.5198  34.0396]\n",
      "Weights: [-4.1611 -1.2352 -0.0638  0.1106  0.0754]\n",
      "MSE loss: 189.1289\n",
      "Iteration: 1800\n",
      "Gradient: [   0.6263   -2.7504    5.7607 -117.1988  -73.4598]\n",
      "Weights: [-4.2088 -1.2002 -0.0744  0.1106  0.076 ]\n",
      "MSE loss: 186.6866\n",
      "Iteration: 1900\n",
      "Gradient: [  4.6982   1.6213  -4.1657 -51.7564 234.2218]\n",
      "Weights: [-4.1987 -1.1784 -0.0834  0.1084  0.0766]\n",
      "MSE loss: 184.9505\n",
      "Iteration: 2000\n",
      "Gradient: [ -4.5597  -2.8913  40.4386  -3.175  -42.7621]\n",
      "Weights: [-4.1868 -1.1782 -0.0949  0.1085  0.0775]\n",
      "MSE loss: 183.4477\n",
      "Iteration: 2100\n",
      "Gradient: [   9.461    14.9448   24.6224   -6.2394 -134.8316]\n",
      "Weights: [-4.2145 -1.1417 -0.1078  0.1083  0.0784]\n",
      "MSE loss: 180.6101\n",
      "Iteration: 2200\n",
      "Gradient: [   6.9466   -8.9967   34.0681 -133.0563  194.9825]\n",
      "Weights: [-4.2267 -1.1235 -0.118   0.1081  0.0789]\n",
      "MSE loss: 179.1883\n",
      "Iteration: 2300\n",
      "Gradient: [  1.5845 -20.8797 -20.5191 -21.8275  89.4697]\n",
      "Weights: [-4.2362 -1.1196 -0.1233  0.1077  0.0795]\n",
      "MSE loss: 178.7167\n",
      "Iteration: 2400\n",
      "Gradient: [ -0.447    5.3722  48.9998 -57.4374 259.4021]\n",
      "Weights: [-4.241  -1.0971 -0.1253  0.1068  0.08  ]\n",
      "MSE loss: 177.5406\n",
      "Iteration: 2500\n",
      "Gradient: [  -4.6902    5.1373   11.175   116.1175 -129.2586]\n",
      "Weights: [-4.2429 -1.085  -0.1352  0.1059  0.0807]\n",
      "MSE loss: 175.4648\n",
      "Iteration: 2600\n",
      "Gradient: [ -5.979    5.3614 -20.0928   3.0091 255.1166]\n",
      "Weights: [-4.2365 -1.0726 -0.1425  0.1047  0.0814]\n",
      "MSE loss: 174.07\n",
      "Iteration: 2700\n",
      "Gradient: [ -9.6583 -17.0274 -11.7178 -78.0031 -70.669 ]\n",
      "Weights: [-4.2548 -1.051  -0.1516  0.1036  0.0817]\n",
      "MSE loss: 173.0694\n",
      "Iteration: 2800\n",
      "Gradient: [ -3.7831  10.1901 -20.623   14.8772 163.7482]\n",
      "Weights: [-4.2659 -1.0352 -0.1596  0.1039  0.0825]\n",
      "MSE loss: 171.3308\n",
      "Iteration: 2900\n",
      "Gradient: [  -1.2849    0.8101   -6.7673   76.6583 -214.1809]\n",
      "Weights: [-4.2522 -1.0302 -0.1656  0.1031  0.0834]\n",
      "MSE loss: 169.9787\n",
      "Iteration: 3000\n",
      "Gradient: [  1.9016  -9.8745 -10.3675   2.6245 162.5051]\n",
      "Weights: [-4.2485 -1.0284 -0.1761  0.1035  0.0843]\n",
      "MSE loss: 168.4729\n",
      "Iteration: 3100\n",
      "Gradient: [ -8.5655  -3.563    8.5601 -75.1738  31.8527]\n",
      "Weights: [-4.2771 -1.0111 -0.1817  0.1034  0.0847]\n",
      "MSE loss: 167.979\n",
      "Iteration: 3200\n",
      "Gradient: [  -0.8253    0.4419    1.9794  130.2435 -123.9011]\n",
      "Weights: [-4.2534 -0.9966 -0.1886  0.102   0.0852]\n",
      "MSE loss: 166.2658\n",
      "Iteration: 3300\n",
      "Gradient: [-11.3831 -24.4209 -19.6112 -56.3371 -28.2191]\n",
      "Weights: [-4.2722 -0.9805 -0.1955  0.1016  0.0856]\n",
      "MSE loss: 165.1154\n",
      "Iteration: 3400\n",
      "Gradient: [  -0.2987  -15.5765   -5.0751   97.9932 -251.4912]\n",
      "Weights: [-4.27   -0.967  -0.2008  0.1013  0.086 ]\n",
      "MSE loss: 164.2711\n",
      "Iteration: 3500\n",
      "Gradient: [  13.2     -21.5369    8.9973   57.043  -375.0247]\n",
      "Weights: [-4.2777 -0.9573 -0.2064  0.1002  0.0864]\n",
      "MSE loss: 163.3699\n",
      "Iteration: 3600\n",
      "Gradient: [   1.5065   12.6095   -4.9677 -161.254  -262.0454]\n",
      "Weights: [-4.2668 -0.9444 -0.2184  0.0999  0.0874]\n",
      "MSE loss: 161.5618\n",
      "Iteration: 3700\n",
      "Gradient: [  2.5545   0.8883  16.1086 114.9161 425.6906]\n",
      "Weights: [-4.2902 -0.9273 -0.2237  0.1002  0.088 ]\n",
      "MSE loss: 160.5099\n",
      "Iteration: 3800\n",
      "Gradient: [-6.3082 -5.6146 26.816  -4.5164 76.7319]\n",
      "Weights: [-4.2686 -0.9301 -0.2316  0.1001  0.0888]\n",
      "MSE loss: 159.4523\n",
      "Iteration: 3900\n",
      "Gradient: [  -4.6595    7.2142   26.1847   34.167  -441.6809]\n",
      "Weights: [-4.2686 -0.9276 -0.2375  0.0989  0.0893]\n",
      "MSE loss: 158.7774\n",
      "Iteration: 4000\n",
      "Gradient: [ -10.7432    2.8027   50.1749  112.0306 -175.1359]\n",
      "Weights: [-4.2705 -0.9159 -0.2438  0.0988  0.0898]\n",
      "MSE loss: 157.6494\n",
      "Iteration: 4100\n",
      "Gradient: [  -7.4768  -11.363   -55.5722  -44.2489 -146.4537]\n",
      "Weights: [-4.2891 -0.9057 -0.2452  0.0978  0.0902]\n",
      "MSE loss: 157.1851\n",
      "Iteration: 4200\n",
      "Gradient: [  21.1551  -13.3826   20.9848  -22.3682 -118.34  ]\n",
      "Weights: [-4.2965 -0.8784 -0.2543  0.098   0.0905]\n",
      "MSE loss: 155.6271\n",
      "Iteration: 4300\n",
      "Gradient: [  -1.5841    6.093    23.2489   18.7779 -525.8716]\n",
      "Weights: [-4.3091 -0.8537 -0.2615  0.097   0.091 ]\n",
      "MSE loss: 154.5823\n",
      "Iteration: 4400\n",
      "Gradient: [   3.6453   -1.0737   16.7832   10.8358 -169.5867]\n",
      "Weights: [-4.3124 -0.8477 -0.2668  0.0956  0.0915]\n",
      "MSE loss: 153.5124\n",
      "Iteration: 4500\n",
      "Gradient: [   2.0381   -2.7417  -49.4349   59.2628 -235.0329]\n",
      "Weights: [-4.3024 -0.8497 -0.273   0.0956  0.0925]\n",
      "MSE loss: 152.4894\n",
      "Iteration: 4600\n",
      "Gradient: [  -7.9225  -12.634   -14.9078  -11.2598 -345.2672]\n",
      "Weights: [-4.3087 -0.8408 -0.2793  0.0946  0.0932]\n",
      "MSE loss: 151.6001\n",
      "Iteration: 4700\n",
      "Gradient: [ -18.3964  -25.9168  -32.44    -40.9661 -561.5189]\n",
      "Weights: [-4.3259 -0.8251 -0.2876  0.0937  0.0939]\n",
      "MSE loss: 151.1402\n",
      "Iteration: 4800\n",
      "Gradient: [ -11.6783  -11.4523    9.7247 -131.655   -88.39  ]\n",
      "Weights: [-4.3385 -0.8052 -0.2925  0.0936  0.0942]\n",
      "MSE loss: 149.7463\n",
      "Iteration: 4900\n",
      "Gradient: [-5.952300e+00  6.960000e-02  7.096900e+00  1.047860e+01 -1.392826e+02]\n",
      "Weights: [-4.3356 -0.7846 -0.3034  0.0933  0.0946]\n",
      "MSE loss: 148.1354\n",
      "Iteration: 5000\n",
      "Gradient: [ -3.5825  20.3357  35.3778  84.6644 179.5671]\n",
      "Weights: [-4.324  -0.766  -0.3114  0.0918  0.095 ]\n",
      "MSE loss: 147.0487\n",
      "Iteration: 5100\n",
      "Gradient: [ 3.7685 10.3403 -6.4344 -8.1642 -4.0007]\n",
      "Weights: [-4.3375 -0.7612 -0.3144  0.092   0.0959]\n",
      "MSE loss: 146.2634\n",
      "Iteration: 5200\n",
      "Gradient: [ -7.3412   4.0048 -60.5215 -63.8512  24.6671]\n",
      "Weights: [-4.3579 -0.7492 -0.3194  0.0908  0.0961]\n",
      "MSE loss: 146.1491\n",
      "Iteration: 5300\n",
      "Gradient: [-19.1392  -5.5095 -11.9576 199.6832 -48.3951]\n",
      "Weights: [-4.3365 -0.76   -0.3206  0.0902  0.0967]\n",
      "MSE loss: 145.4716\n",
      "Iteration: 5400\n",
      "Gradient: [ 28.1929  -5.838   41.515   37.6371 281.442 ]\n",
      "Weights: [-4.3233 -0.7499 -0.3262  0.0911  0.0969]\n",
      "MSE loss: 144.7522\n",
      "Iteration: 5500\n",
      "Gradient: [ 15.0065  19.5936  26.0196 103.1672 274.4129]\n",
      "Weights: [-4.309  -0.7432 -0.333   0.0911  0.0972]\n",
      "MSE loss: 144.6234\n",
      "Iteration: 5600\n",
      "Gradient: [  -4.0901  -18.2232    4.2298   65.2901 -225.396 ]\n",
      "Weights: [-4.3385 -0.7351 -0.3384  0.0914  0.098 ]\n",
      "MSE loss: 143.0725\n",
      "Iteration: 5700\n",
      "Gradient: [ -6.1263   1.8764  47.2871  94.7228 134.22  ]\n",
      "Weights: [-4.3233 -0.7251 -0.3428  0.0903  0.0983]\n",
      "MSE loss: 142.7899\n",
      "Iteration: 5800\n",
      "Gradient: [  5.7559  -2.3746   8.6096  73.5558 198.3859]\n",
      "Weights: [-4.3585 -0.708  -0.3496  0.0907  0.0988]\n",
      "MSE loss: 141.5987\n",
      "Iteration: 5900\n",
      "Gradient: [  1.0451  12.0235 -24.4723 -14.1863  46.8214]\n",
      "Weights: [-4.3486 -0.6985 -0.3573  0.0904  0.099 ]\n",
      "MSE loss: 140.6327\n",
      "Iteration: 6000\n",
      "Gradient: [  4.0598   5.7898  -2.7388  27.7181 -37.7109]\n",
      "Weights: [-4.363  -0.6794 -0.3625  0.0897  0.0994]\n",
      "MSE loss: 139.7863\n",
      "Iteration: 6100\n",
      "Gradient: [   7.3742   10.553   -42.5949  -53.9209 -201.5828]\n",
      "Weights: [-4.3621 -0.6816 -0.3639  0.089   0.0998]\n",
      "MSE loss: 139.5673\n",
      "Iteration: 6200\n",
      "Gradient: [ 4.6726 25.4664 17.2949 22.1536 39.3086]\n",
      "Weights: [-4.3532 -0.6682 -0.3691  0.0886  0.1001]\n",
      "MSE loss: 138.9279\n",
      "Iteration: 6300\n",
      "Gradient: [ 11.7834 -13.2739  10.2963 -23.1144 164.1501]\n",
      "Weights: [-4.3697 -0.6461 -0.378   0.088   0.1007]\n",
      "MSE loss: 137.6561\n",
      "Iteration: 6400\n",
      "Gradient: [ -6.7573 -10.6777 -37.2026  64.262   44.5034]\n",
      "Weights: [-4.3605 -0.6441 -0.3838  0.0883  0.1011]\n",
      "MSE loss: 137.133\n",
      "Iteration: 6500\n",
      "Gradient: [   3.2272  -38.1069   36.1673  -75.862  -255.5461]\n",
      "Weights: [-4.3829 -0.6284 -0.3882  0.0871  0.1015]\n",
      "MSE loss: 136.3598\n",
      "Iteration: 6600\n",
      "Gradient: [   4.6432   -2.473    38.6548 -106.0711  126.5072]\n",
      "Weights: [-4.3591 -0.6241 -0.3962  0.0876  0.1018]\n",
      "MSE loss: 135.8567\n",
      "Iteration: 6700\n",
      "Gradient: [   2.6278    9.0904  -17.0835  -36.6631 -164.4231]\n",
      "Weights: [-4.3716 -0.6095 -0.4025  0.087   0.1024]\n",
      "MSE loss: 134.7614\n",
      "Iteration: 6800\n",
      "Gradient: [  0.7788  -9.8864  -6.8688 -59.3    -97.5188]\n",
      "Weights: [-4.3932 -0.588  -0.4082  0.086   0.1028]\n",
      "MSE loss: 133.8678\n",
      "Iteration: 6900\n",
      "Gradient: [  4.6672 -23.0392 -15.4711 150.1964 -61.4437]\n",
      "Weights: [-4.3935 -0.5723 -0.4125  0.0843  0.1032]\n",
      "MSE loss: 133.2113\n",
      "Iteration: 7000\n",
      "Gradient: [  0.8604  -5.5639  15.3984 158.8784  -5.6088]\n",
      "Weights: [-4.4028 -0.5664 -0.4142  0.0825  0.104 ]\n",
      "MSE loss: 132.5123\n",
      "Iteration: 7100\n",
      "Gradient: [-12.6769  -1.3071  20.3753 -71.2245 -86.2428]\n",
      "Weights: [-4.4018 -0.5715 -0.417   0.0828  0.1045]\n",
      "MSE loss: 132.2792\n",
      "Iteration: 7200\n",
      "Gradient: [  -3.8306   -5.4885    4.3899  -58.571  -198.0898]\n",
      "Weights: [-4.4159 -0.5542 -0.4243  0.0826  0.105 ]\n",
      "MSE loss: 131.5388\n",
      "Iteration: 7300\n",
      "Gradient: [ -0.4509 -21.3531 -21.0824  57.3833 144.6217]\n",
      "Weights: [-4.4067 -0.5541 -0.4267  0.0821  0.1055]\n",
      "MSE loss: 130.9617\n",
      "Iteration: 7400\n",
      "Gradient: [-12.5397  -7.2829  74.3543  98.6298  11.2746]\n",
      "Weights: [-4.3949 -0.5495 -0.431   0.0815  0.1059]\n",
      "MSE loss: 130.422\n",
      "Iteration: 7500\n",
      "Gradient: [  16.3482    2.3133   39.111  -122.0163 -142.5304]\n",
      "Weights: [-4.409  -0.5386 -0.4384  0.0814  0.1064]\n",
      "MSE loss: 129.6527\n",
      "Iteration: 7600\n",
      "Gradient: [   2.1486  -15.2132    4.5144  -19.5222 -169.5698]\n",
      "Weights: [-4.4128 -0.5141 -0.4497  0.0817  0.1067]\n",
      "MSE loss: 128.5114\n",
      "Iteration: 7700\n",
      "Gradient: [ 16.8717   9.993  -14.1795  95.9015 -14.0713]\n",
      "Weights: [-4.3944 -0.515  -0.4538  0.0817  0.1071]\n",
      "MSE loss: 128.341\n",
      "Iteration: 7800\n",
      "Gradient: [  -0.7038  -18.8567   -8.8318  -40.8661 -141.8225]\n",
      "Weights: [-4.4042 -0.5111 -0.4586  0.0819  0.1073]\n",
      "MSE loss: 127.8981\n",
      "Iteration: 7900\n",
      "Gradient: [   5.39     -4.2518   31.4367  -53.8711 -165.4595]\n",
      "Weights: [-4.4181 -0.489  -0.4637  0.0811  0.1075]\n",
      "MSE loss: 127.0685\n",
      "Iteration: 8000\n",
      "Gradient: [  3.6219 -15.151   16.0475  38.4391 -35.1818]\n",
      "Weights: [-4.4259 -0.4845 -0.4678  0.0815  0.108 ]\n",
      "MSE loss: 126.5867\n",
      "Iteration: 8100\n",
      "Gradient: [ 10.5601 -11.7166 -12.8624 -52.2233 154.3821]\n",
      "Weights: [-4.4198 -0.4745 -0.4724  0.081   0.1082]\n",
      "MSE loss: 126.0989\n",
      "Iteration: 8200\n",
      "Gradient: [   1.034     3.9036   -9.5812   14.6067 -197.1013]\n",
      "Weights: [-4.4284 -0.4579 -0.4785  0.0805  0.1086]\n",
      "MSE loss: 125.4088\n",
      "Iteration: 8300\n",
      "Gradient: [  12.4553    6.5668   -7.8457   31.5687 -211.8537]\n",
      "Weights: [-4.4318 -0.4555 -0.4798  0.0798  0.1092]\n",
      "MSE loss: 125.0877\n",
      "Iteration: 8400\n",
      "Gradient: [ -0.4567  -6.2058  28.4663 177.0981 415.8385]\n",
      "Weights: [-4.4436 -0.4392 -0.4855  0.0793  0.1096]\n",
      "MSE loss: 124.483\n",
      "Iteration: 8500\n",
      "Gradient: [   3.7176    5.4403   10.6116  -82.9556 -118.4325]\n",
      "Weights: [-4.4255 -0.4398 -0.491   0.0783  0.1101]\n",
      "MSE loss: 123.8829\n",
      "Iteration: 8600\n",
      "Gradient: [ 13.1686  17.7561  -8.8515  99.6366 163.6696]\n",
      "Weights: [-4.4261 -0.4323 -0.4913  0.0777  0.1105]\n",
      "MSE loss: 124.28\n",
      "Iteration: 8700\n",
      "Gradient: [  -3.0687  -12.9675  -14.8486 -132.54   -584.246 ]\n",
      "Weights: [-4.4452 -0.4253 -0.4986  0.0776  0.1107]\n",
      "MSE loss: 123.2303\n",
      "Iteration: 8800\n",
      "Gradient: [  -4.4413   -8.417   -24.9256  -54.864  -146.0285]\n",
      "Weights: [-4.4615 -0.4136 -0.5015  0.0773  0.1113]\n",
      "MSE loss: 122.7376\n",
      "Iteration: 8900\n",
      "Gradient: [  11.5599   -6.4315  -11.1139  -38.6019 -141.574 ]\n",
      "Weights: [-4.4512 -0.4116 -0.5051  0.0771  0.1115]\n",
      "MSE loss: 122.1901\n",
      "Iteration: 9000\n",
      "Gradient: [  2.3936  -3.3633 -18.9969 169.7413 -43.722 ]\n",
      "Weights: [-4.4641 -0.3827 -0.5103  0.0762  0.1116]\n",
      "MSE loss: 121.702\n",
      "Iteration: 9100\n",
      "Gradient: [-15.4886 -27.6388  24.2068 -52.8258  26.5517]\n",
      "Weights: [-4.479  -0.3764 -0.5145  0.0756  0.1119]\n",
      "MSE loss: 121.4849\n",
      "Iteration: 9200\n",
      "Gradient: [ -17.6899   18.2462  -44.7577    0.6236 -247.431 ]\n",
      "Weights: [-4.4784 -0.3654 -0.5211  0.0758  0.1121]\n",
      "MSE loss: 120.8275\n",
      "Iteration: 9300\n",
      "Gradient: [ -8.444    1.8859  65.888  -43.3     48.1832]\n",
      "Weights: [-4.4501 -0.3659 -0.5294  0.0764  0.1126]\n",
      "MSE loss: 120.1024\n",
      "Iteration: 9400\n",
      "Gradient: [  -3.3257   -2.0437   15.1965  179.3825 -237.6001]\n",
      "Weights: [-4.4622 -0.3614 -0.5332  0.0762  0.1135]\n",
      "MSE loss: 119.4143\n",
      "Iteration: 9500\n",
      "Gradient: [ -5.3643 -15.9508  48.7649 110.8182 175.1295]\n",
      "Weights: [-4.4551 -0.3555 -0.5369  0.0755  0.1138]\n",
      "MSE loss: 119.0673\n",
      "Iteration: 9600\n",
      "Gradient: [  1.9851  -9.4294   5.6919 -13.8058 -84.0166]\n",
      "Weights: [-4.4646 -0.3481 -0.541   0.0753  0.114 ]\n",
      "MSE loss: 118.6419\n",
      "Iteration: 9700\n",
      "Gradient: [  -3.1308   -3.4371   56.3143  -43.7648 -170.8221]\n",
      "Weights: [-4.4522 -0.3484 -0.5456  0.0757  0.1145]\n",
      "MSE loss: 118.3489\n",
      "Iteration: 9800\n",
      "Gradient: [  10.4374    5.8966  -43.875    59.8069 -118.0828]\n",
      "Weights: [-4.4701 -0.3401 -0.5483  0.0761  0.1146]\n",
      "MSE loss: 118.0739\n",
      "Iteration: 9900\n",
      "Gradient: [ -8.9995  -9.142   44.7536 -12.6801 142.7346]\n",
      "Weights: [-4.4679 -0.332  -0.5502  0.0753  0.1148]\n",
      "MSE loss: 117.7911\n",
      "Iteration: 10000\n",
      "Gradient: [   7.2025   -1.8495    2.6195 -113.6392   88.3617]\n",
      "Weights: [-4.4572 -0.3281 -0.5575  0.0754  0.1151]\n",
      "MSE loss: 117.3668\n",
      "Iteration: 10100\n",
      "Gradient: [ 12.2983  -5.4318  13.9522 -77.6183 -65.4553]\n",
      "Weights: [-4.4727 -0.3104 -0.5613  0.0748  0.1154]\n",
      "MSE loss: 116.8028\n",
      "Iteration: 10200\n",
      "Gradient: [  4.9049 -21.4413 -23.2519  99.0402  47.0489]\n",
      "Weights: [-4.4817 -0.2999 -0.5714  0.0754  0.1161]\n",
      "MSE loss: 116.0229\n",
      "Iteration: 10300\n",
      "Gradient: [   2.0172    7.6606   -3.8751   80.394  -272.8488]\n",
      "Weights: [-4.4877 -0.2809 -0.5756  0.0742  0.1163]\n",
      "MSE loss: 115.4755\n",
      "Iteration: 10400\n",
      "Gradient: [ -7.0146  13.0645  45.4687 -32.139  133.6464]\n",
      "Weights: [-4.4862 -0.2685 -0.5797  0.0739  0.1165]\n",
      "MSE loss: 115.3168\n",
      "Iteration: 10500\n",
      "Gradient: [ 10.3425  -7.4987  31.8022 132.3441  -1.8284]\n",
      "Weights: [-4.4909 -0.2528 -0.5879  0.0738  0.117 ]\n",
      "MSE loss: 114.7208\n",
      "Iteration: 10600\n",
      "Gradient: [  5.372   -9.8262  -5.9157  28.1743 193.5861]\n",
      "Weights: [-4.4898 -0.2607 -0.5928  0.0743  0.1173]\n",
      "MSE loss: 114.2734\n",
      "Iteration: 10700\n",
      "Gradient: [ -2.4144  19.4771   7.4215 -17.5077 118.1896]\n",
      "Weights: [-4.4778 -0.2655 -0.5944  0.0749  0.1178]\n",
      "MSE loss: 114.1727\n",
      "Iteration: 10800\n",
      "Gradient: [  -8.3515  -12.1793   10.425   -60.6997 -169.4298]\n",
      "Weights: [-4.4937 -0.2475 -0.6004  0.0743  0.1179]\n",
      "MSE loss: 113.5351\n",
      "Iteration: 10900\n",
      "Gradient: [  6.6483  23.6455  31.0795 -79.7435  22.6579]\n",
      "Weights: [-4.4941 -0.2384 -0.6035  0.0739  0.1182]\n",
      "MSE loss: 113.2063\n",
      "Iteration: 11000\n",
      "Gradient: [  10.0237    6.3613   -3.4146  -48.622  -147.2054]\n",
      "Weights: [-4.501  -0.2439 -0.6051  0.0745  0.1186]\n",
      "MSE loss: 113.1887\n",
      "Iteration: 11100\n",
      "Gradient: [  17.2858   13.2945   26.2528  114.7806 -260.0069]\n",
      "Weights: [-4.4972 -0.2269 -0.6106  0.0745  0.1185]\n",
      "MSE loss: 112.7486\n",
      "Iteration: 11200\n",
      "Gradient: [ -3.0491 -22.2217 -41.4812  88.078  -67.0308]\n",
      "Weights: [-4.5078 -0.2132 -0.6159  0.074   0.1188]\n",
      "MSE loss: 112.2202\n",
      "Iteration: 11300\n",
      "Gradient: [  3.9126   0.5725  23.101  -80.5272 173.6449]\n",
      "Weights: [-4.5066 -0.2113 -0.6171  0.0743  0.1189]\n",
      "MSE loss: 112.1875\n",
      "Iteration: 11400\n",
      "Gradient: [  3.5613  -0.1925   5.9136 -39.6109 166.488 ]\n",
      "Weights: [-4.4896 -0.2114 -0.6242  0.0742  0.1193]\n",
      "MSE loss: 111.9136\n",
      "Iteration: 11500\n",
      "Gradient: [ -3.3862  -3.1986 -37.059   48.8318 311.5342]\n",
      "Weights: [-4.5271 -0.199  -0.6273  0.0743  0.1197]\n",
      "MSE loss: 111.8634\n",
      "Iteration: 11600\n",
      "Gradient: [ 3.9364 17.1149 22.6295 11.2222 -3.9398]\n",
      "Weights: [-4.5165 -0.1984 -0.6259  0.0733  0.1197]\n",
      "MSE loss: 111.4301\n",
      "Iteration: 11700\n",
      "Gradient: [   6.5478  -13.0426   25.702  -184.2086 -183.0745]\n",
      "Weights: [-4.5197 -0.1973 -0.6279  0.0723  0.1202]\n",
      "MSE loss: 111.464\n",
      "Iteration: 11800\n",
      "Gradient: [ -8.6867   0.4955 -43.8879 -84.9363 173.8476]\n",
      "Weights: [-4.5258 -0.1825 -0.6316  0.0716  0.1203]\n",
      "MSE loss: 111.038\n",
      "Iteration: 11900\n",
      "Gradient: [  0.309    3.2114  -6.0729  55.2468 217.2204]\n",
      "Weights: [-4.5176 -0.1715 -0.6351  0.0714  0.1206]\n",
      "MSE loss: 110.5616\n",
      "Iteration: 12000\n",
      "Gradient: [ -2.5448  13.8268  28.5659  94.9965 389.2743]\n",
      "Weights: [-4.5023 -0.1779 -0.6382  0.0715  0.1208]\n",
      "MSE loss: 110.4801\n",
      "Iteration: 12100\n",
      "Gradient: [ -4.9951 -11.1229  14.9897 -24.7497  65.4426]\n",
      "Weights: [-4.5339 -0.1711 -0.6377  0.0701  0.1213]\n",
      "MSE loss: 110.4165\n",
      "Iteration: 12200\n",
      "Gradient: [  6.8647  15.4975  30.9216  32.2455 -98.5504]\n",
      "Weights: [-4.5099 -0.1711 -0.642   0.0707  0.1215]\n",
      "MSE loss: 109.9242\n",
      "Iteration: 12300\n",
      "Gradient: [  8.8838  33.8107   1.5341 101.3386 511.7857]\n",
      "Weights: [-4.4962 -0.1755 -0.6451  0.0719  0.1219]\n",
      "MSE loss: 110.4254\n",
      "Iteration: 12400\n",
      "Gradient: [  7.4749  -2.5224  59.424  -90.9535 123.9213]\n",
      "Weights: [-4.5214 -0.1652 -0.6484  0.0706  0.1222]\n",
      "MSE loss: 109.4853\n",
      "Iteration: 12500\n",
      "Gradient: [ -0.2825   6.4233   6.1446 -46.859  -39.5361]\n",
      "Weights: [-4.5229 -0.155  -0.6523  0.0701  0.1224]\n",
      "MSE loss: 109.1429\n",
      "Iteration: 12600\n",
      "Gradient: [  -5.5578   -1.9983   19.1784 -136.053    12.9573]\n",
      "Weights: [-4.5415 -0.1416 -0.656   0.0694  0.1226]\n",
      "MSE loss: 109.2866\n",
      "Iteration: 12700\n",
      "Gradient: [  3.1477 -10.0684   0.4798   0.7835 214.1888]\n",
      "Weights: [-4.5159 -0.1447 -0.6587  0.0696  0.1229]\n",
      "MSE loss: 108.6286\n",
      "Iteration: 12800\n",
      "Gradient: [ -6.2526  -5.1285 -18.9478  21.0603 138.4241]\n",
      "Weights: [-4.5283 -0.1442 -0.6612  0.0702  0.1231]\n",
      "MSE loss: 108.5921\n",
      "Iteration: 12900\n",
      "Gradient: [  4.3566  31.3831   8.824   87.1344 100.4316]\n",
      "Weights: [-4.5306 -0.1172 -0.666   0.0696  0.1231]\n",
      "MSE loss: 108.2753\n",
      "Iteration: 13000\n",
      "Gradient: [ -5.2393   0.3548 -45.6062 -37.2372  47.759 ]\n",
      "Weights: [-4.5438 -0.1016 -0.6726  0.0685  0.1235]\n",
      "MSE loss: 107.5127\n",
      "Iteration: 13100\n",
      "Gradient: [   5.0778    7.6315  -22.898  -127.806  -355.2137]\n",
      "Weights: [-4.56   -0.0881 -0.6723  0.0677  0.1235]\n",
      "MSE loss: 107.5163\n",
      "Iteration: 13200\n",
      "Gradient: [ -9.4073  10.4115 -56.5453 130.6192 -71.2583]\n",
      "Weights: [-4.5552 -0.0821 -0.6763  0.0673  0.1236]\n",
      "MSE loss: 107.2514\n",
      "Iteration: 13300\n",
      "Gradient: [  -8.491   -18.4103   19.7898 -110.5894 -333.9838]\n",
      "Weights: [-4.5506 -0.1046 -0.6775  0.0687  0.1242]\n",
      "MSE loss: 107.5161\n",
      "Iteration: 13400\n",
      "Gradient: [ -3.4096  17.3085  14.1101 -18.5373 -78.6451]\n",
      "Weights: [-4.5391 -0.0968 -0.6824  0.0692  0.1244]\n",
      "MSE loss: 106.8937\n",
      "Iteration: 13500\n",
      "Gradient: [  0.6955  -5.4209 -10.4628  -2.2936  31.9387]\n",
      "Weights: [-4.5437 -0.0964 -0.6843  0.0692  0.1246]\n",
      "MSE loss: 106.7806\n",
      "Iteration: 13600\n",
      "Gradient: [  2.1557   5.6859 -12.6188  17.551  -88.4568]\n",
      "Weights: [-4.5334 -0.0906 -0.6882  0.0689  0.1247]\n",
      "MSE loss: 106.625\n",
      "Iteration: 13700\n",
      "Gradient: [ 11.9676  -4.8692   9.1458  61.3669 181.087 ]\n",
      "Weights: [-4.5532 -0.0749 -0.6894  0.0689  0.1248]\n",
      "MSE loss: 106.4439\n",
      "Iteration: 13800\n",
      "Gradient: [ 11.4031   0.7883  13.6558  59.6065 269.6414]\n",
      "Weights: [-4.5425 -0.0771 -0.6922  0.0691  0.1248]\n",
      "MSE loss: 106.2993\n",
      "Iteration: 13900\n",
      "Gradient: [ -11.2207  -11.6727   19.2045   -5.4577 -155.0582]\n",
      "Weights: [-4.5716 -0.0685 -0.6917  0.068   0.1249]\n",
      "MSE loss: 106.6604\n",
      "Iteration: 14000\n",
      "Gradient: [   7.4935    9.4682    5.0898   11.4142 -156.6107]\n",
      "Weights: [-4.5588 -0.0607 -0.6966  0.0682  0.1252]\n",
      "MSE loss: 105.8595\n",
      "Iteration: 14100\n",
      "Gradient: [  2.5197  17.2227 -33.8102  36.1457  77.3642]\n",
      "Weights: [-4.5545 -0.0513 -0.7035  0.068   0.1255]\n",
      "MSE loss: 105.5075\n",
      "Iteration: 14200\n",
      "Gradient: [-10.3011   6.9856   3.8632 101.0818  -4.4062]\n",
      "Weights: [-4.5649 -0.04   -0.7095  0.0685  0.1258]\n",
      "MSE loss: 105.0982\n",
      "Iteration: 14300\n",
      "Gradient: [ 5.10000e-02 -4.97810e+00 -1.75186e+01 -5.24318e+01 -8.57020e+01]\n",
      "Weights: [-4.5524 -0.0456 -0.7104  0.0683  0.1261]\n",
      "MSE loss: 105.0633\n",
      "Iteration: 14400\n",
      "Gradient: [  5.6511   2.5908   5.0568 -37.0302 -56.0773]\n",
      "Weights: [-4.5731 -0.0362 -0.7118  0.0678  0.1265]\n",
      "MSE loss: 104.9089\n",
      "Iteration: 14500\n",
      "Gradient: [  9.908   16.5562  33.8029 125.9049  19.7216]\n",
      "Weights: [-4.5487 -0.0306 -0.7151  0.0669  0.1268]\n",
      "MSE loss: 105.1811\n",
      "Iteration: 14600\n",
      "Gradient: [   0.5659   -5.5195    6.3225   10.7921 -222.0164]\n",
      "Weights: [-4.5622 -0.0323 -0.7192  0.0668  0.1271]\n",
      "MSE loss: 104.5627\n",
      "Iteration: 14700\n",
      "Gradient: [-11.9172  -4.1093 -17.8276  36.4628 151.0404]\n",
      "Weights: [-4.5601 -0.0235 -0.7216  0.0673  0.1271]\n",
      "MSE loss: 104.2762\n",
      "Iteration: 14800\n",
      "Gradient: [  -0.7554   -8.2043   26.3679   10.705  -266.6591]\n",
      "Weights: [-4.5478 -0.0308 -0.7246  0.068   0.1275]\n",
      "MSE loss: 104.3045\n",
      "Iteration: 14900\n",
      "Gradient: [ -3.9164  -4.3654  -3.121   45.1772 -51.5657]\n",
      "Weights: [-4.575  -0.0194 -0.7279  0.068   0.1275]\n",
      "MSE loss: 104.2277\n",
      "Iteration: 15000\n",
      "Gradient: [-11.5526 -11.0422 -21.5136 113.442  117.2208]\n",
      "Weights: [-4.5693 -0.0128 -0.7303  0.0679  0.1278]\n",
      "MSE loss: 103.7736\n",
      "Iteration: 15100\n",
      "Gradient: [  0.7887 -16.0434  47.3801 119.307  124.3284]\n",
      "Weights: [-4.5765  0.0076 -0.7367  0.0677  0.1279]\n",
      "MSE loss: 103.4609\n",
      "Iteration: 15200\n",
      "Gradient: [   5.5265   13.779   -30.6024   18.0933 -103.0599]\n",
      "Weights: [-4.5685  0.0106 -0.7416  0.0671  0.1281]\n",
      "MSE loss: 103.2477\n",
      "Iteration: 15300\n",
      "Gradient: [  9.1609  22.5487   8.2591  35.102  -87.308 ]\n",
      "Weights: [-4.5883  0.0243 -0.7456  0.0676  0.1285]\n",
      "MSE loss: 102.8905\n",
      "Iteration: 15400\n",
      "Gradient: [  3.2131  -0.8096 -37.4502  42.9805 171.7028]\n",
      "Weights: [-4.5979  0.0353 -0.7458  0.0673  0.1281]\n",
      "MSE loss: 102.926\n",
      "Iteration: 15500\n",
      "Gradient: [  5.8365  -1.7897  -4.9246 -70.2005 117.7788]\n",
      "Weights: [-4.585   0.0352 -0.7485  0.0672  0.1282]\n",
      "MSE loss: 102.8075\n",
      "Iteration: 15600\n",
      "Gradient: [  2.208    8.9566  70.1826 111.1292 308.8959]\n",
      "Weights: [-4.5899  0.0431 -0.7533  0.0681  0.1286]\n",
      "MSE loss: 102.7369\n",
      "Iteration: 15700\n",
      "Gradient: [ -5.5401   5.8707  97.5651  20.3145 282.108 ]\n",
      "Weights: [-4.5847  0.0481 -0.7579  0.0674  0.129 ]\n",
      "MSE loss: 102.4212\n",
      "Iteration: 15800\n",
      "Gradient: [ 7.691300e+00  4.700000e-03  2.928890e+01 -4.396100e+00 -2.216805e+02]\n",
      "Weights: [-4.5952  0.058  -0.7637  0.0676  0.1293]\n",
      "MSE loss: 101.9269\n",
      "Iteration: 15900\n",
      "Gradient: [-5.4773  6.7161 45.2062 -1.1986  4.5578]\n",
      "Weights: [-4.5934  0.0659 -0.7662  0.0669  0.1295]\n",
      "MSE loss: 101.8771\n",
      "Iteration: 16000\n",
      "Gradient: [  -0.1461  -11.2751  -22.5725 -109.4278  123.2222]\n",
      "Weights: [-4.6088  0.0662 -0.77    0.0668  0.1299]\n",
      "MSE loss: 101.8155\n",
      "Iteration: 16100\n",
      "Gradient: [ -8.176   -0.9236 -46.2526 105.2874 174.4288]\n",
      "Weights: [-4.5932  0.0649 -0.7714  0.0673  0.1299]\n",
      "MSE loss: 101.4915\n",
      "Iteration: 16200\n",
      "Gradient: [ -15.5026   -2.6578  -43.2131   66.1859 -324.0927]\n",
      "Weights: [-4.5957  0.0629 -0.774   0.0676  0.1303]\n",
      "MSE loss: 101.4126\n",
      "Iteration: 16300\n",
      "Gradient: [ -7.1607   9.3516  -2.5908 -30.5912 -72.0761]\n",
      "Weights: [-4.5958  0.0805 -0.7831  0.0675  0.1304]\n",
      "MSE loss: 101.0867\n",
      "Iteration: 16400\n",
      "Gradient: [ 17.7462 -17.2349 -24.2829  -9.4967  59.5232]\n",
      "Weights: [-4.5907  0.0793 -0.7851  0.0679  0.1306]\n",
      "MSE loss: 100.954\n",
      "Iteration: 16500\n",
      "Gradient: [  -6.023   -22.4807  -23.7561   97.2763 -121.7828]\n",
      "Weights: [-4.6115  0.0906 -0.7882  0.0678  0.1309]\n",
      "MSE loss: 100.8207\n",
      "Iteration: 16600\n",
      "Gradient: [  9.0438   9.8646  13.1016 -40.0848 214.0847]\n",
      "Weights: [-4.6146  0.0989 -0.7869  0.0668  0.1311]\n",
      "MSE loss: 100.679\n",
      "Iteration: 16700\n",
      "Gradient: [ -12.826    -1.4485  -16.1021 -161.1597   -4.8372]\n",
      "Weights: [-4.6065  0.1038 -0.7932  0.0659  0.1315]\n",
      "MSE loss: 100.371\n",
      "Iteration: 16800\n",
      "Gradient: [  2.5427 -12.1124  14.7557  -7.2672 279.2477]\n",
      "Weights: [-4.6142  0.1131 -0.798   0.0667  0.1319]\n",
      "MSE loss: 100.0699\n",
      "Iteration: 16900\n",
      "Gradient: [ -13.4654   13.7288   12.3381   30.3087 -160.2307]\n",
      "Weights: [-4.6123  0.1152 -0.8001  0.0671  0.1317]\n",
      "MSE loss: 100.0183\n",
      "Iteration: 17000\n",
      "Gradient: [ -10.7704    1.1981  -55.2338 -119.4536 -190.102 ]\n",
      "Weights: [-4.6115  0.1115 -0.8011  0.0675  0.1318]\n",
      "MSE loss: 100.0273\n",
      "Iteration: 17100\n",
      "Gradient: [ 10.265   21.4865 -23.599   17.7093 -14.7999]\n",
      "Weights: [-4.6117  0.1206 -0.8012  0.0664  0.132 ]\n",
      "MSE loss: 99.9255\n",
      "Iteration: 17200\n",
      "Gradient: [  12.8337   20.9652   26.2601   26.3809 -186.6683]\n",
      "Weights: [-4.6091  0.1256 -0.804   0.0663  0.1323]\n",
      "MSE loss: 99.941\n",
      "Iteration: 17300\n",
      "Gradient: [  2.4427 -17.2633 -27.9526  -6.1799  46.4797]\n",
      "Weights: [-4.5938  0.1199 -0.8088  0.0662  0.1325]\n",
      "MSE loss: 99.8359\n",
      "Iteration: 17400\n",
      "Gradient: [  0.9794  -2.3072 -23.7584 -35.7251 -24.899 ]\n",
      "Weights: [-4.6056  0.1193 -0.8089  0.0663  0.1328]\n",
      "MSE loss: 99.5956\n",
      "Iteration: 17500\n",
      "Gradient: [  -3.0605   25.0639   16.0135   -9.3935 -144.0767]\n",
      "Weights: [-4.5974  0.1228 -0.8101  0.0666  0.1327]\n",
      "MSE loss: 99.6628\n",
      "Iteration: 17600\n",
      "Gradient: [ -2.1377  -0.9415 -19.5802 -59.4055 -13.7166]\n",
      "Weights: [-4.611   0.1317 -0.8125  0.0672  0.1328]\n",
      "MSE loss: 99.4858\n",
      "Iteration: 17700\n",
      "Gradient: [   9.4949   -2.9483    6.5849   19.6477 -194.3294]\n",
      "Weights: [-4.6116  0.1393 -0.8161  0.0669  0.1326]\n",
      "MSE loss: 99.3754\n",
      "Iteration: 17800\n",
      "Gradient: [-9.189000e+00  1.774000e-01  1.089300e+01 -1.815000e+01 -2.030084e+02]\n",
      "Weights: [-4.6229  0.1572 -0.8191  0.0664  0.1326]\n",
      "MSE loss: 99.2012\n",
      "Iteration: 17900\n",
      "Gradient: [ -6.6998 -21.0467   1.9115 -64.685  -99.4681]\n",
      "Weights: [-4.6441  0.1656 -0.82    0.0656  0.1327]\n",
      "MSE loss: 99.429\n",
      "Iteration: 18000\n",
      "Gradient: [-0.2521  9.0563  5.5962 41.837  40.4549]\n",
      "Weights: [-4.6299  0.1777 -0.8267  0.0658  0.1332]\n",
      "MSE loss: 98.9962\n",
      "Iteration: 18100\n",
      "Gradient: [ 13.31     5.2674  39.7534 -66.6274 202.8358]\n",
      "Weights: [-4.6407  0.18   -0.8274  0.066   0.1335]\n",
      "MSE loss: 99.0241\n",
      "Iteration: 18200\n",
      "Gradient: [ 3.9444 18.0689  2.865  46.8068 85.3243]\n",
      "Weights: [-4.6481  0.1824 -0.8292  0.0664  0.1335]\n",
      "MSE loss: 98.8339\n",
      "Iteration: 18300\n",
      "Gradient: [ 0.5377  3.7078 31.4301 20.9179 15.6436]\n",
      "Weights: [-4.6393  0.1851 -0.8321  0.0666  0.1334]\n",
      "MSE loss: 98.6587\n",
      "Iteration: 18400\n",
      "Gradient: [  -3.2151  -16.0258  -18.7437  -87.0105 -263.7393]\n",
      "Weights: [-4.6326  0.1824 -0.8382  0.0671  0.1337]\n",
      "MSE loss: 98.3944\n",
      "Iteration: 18500\n",
      "Gradient: [ 11.7417  -9.2321   5.1777  48.4865 -19.8045]\n",
      "Weights: [-4.6217  0.1757 -0.838   0.0677  0.1338]\n",
      "MSE loss: 98.4057\n",
      "Iteration: 18600\n",
      "Gradient: [17.2211 21.2094 59.2828 23.2473 30.307 ]\n",
      "Weights: [-4.631   0.1852 -0.8402  0.0678  0.1338]\n",
      "MSE loss: 98.2896\n",
      "Iteration: 18700\n",
      "Gradient: [  7.7541  11.9075  -4.791   78.6777 217.7182]\n",
      "Weights: [-4.6161  0.1898 -0.8445  0.0681  0.134 ]\n",
      "MSE loss: 98.7294\n",
      "Iteration: 18800\n",
      "Gradient: [ -5.927   -3.3603 -30.2424 -37.9918 -71.7219]\n",
      "Weights: [-4.6184  0.1785 -0.8478  0.0688  0.1344]\n",
      "MSE loss: 98.118\n",
      "Iteration: 18900\n",
      "Gradient: [ -5.3486  21.9294 -11.8706  91.5918  72.8331]\n",
      "Weights: [-4.6327  0.1896 -0.8476  0.0684  0.1345]\n",
      "MSE loss: 98.014\n",
      "Iteration: 19000\n",
      "Gradient: [  5.5491   1.2205   4.4638 189.7682  -6.6867]\n",
      "Weights: [-4.6226  0.1815 -0.8481  0.0695  0.1346]\n",
      "MSE loss: 98.1912\n",
      "Iteration: 19100\n",
      "Gradient: [-7.652600e+00 -5.740000e-02  6.265640e+01  1.753203e+02 -2.215540e+02]\n",
      "Weights: [-4.6627  0.2026 -0.8519  0.0701  0.1344]\n",
      "MSE loss: 98.4639\n",
      "Iteration: 19200\n",
      "Gradient: [   0.8533    9.6001  -15.9979  -19.3535 -105.8196]\n",
      "Weights: [-4.6405  0.2118 -0.8572  0.0697  0.1344]\n",
      "MSE loss: 97.7324\n",
      "Iteration: 19300\n",
      "Gradient: [  2.3898  12.6754  -5.7689 -32.5078 136.017 ]\n",
      "Weights: [-4.6468  0.2157 -0.8596  0.069   0.1347]\n",
      "MSE loss: 97.5539\n",
      "Iteration: 19400\n",
      "Gradient: [ -8.6195 -21.0256   3.96    17.2166  -9.4438]\n",
      "Weights: [-4.6466  0.2194 -0.8607  0.0679  0.1348]\n",
      "MSE loss: 97.6237\n",
      "Iteration: 19500\n",
      "Gradient: [ -6.7402  -1.8038  13.1574  74.1549 330.6643]\n",
      "Weights: [-4.6391  0.2235 -0.8636  0.069   0.1347]\n",
      "MSE loss: 97.4915\n",
      "Iteration: 19600\n",
      "Gradient: [ 3.852  25.7894 17.8009 82.1616 87.2708]\n",
      "Weights: [-4.648   0.2347 -0.8642  0.0682  0.1348]\n",
      "MSE loss: 97.505\n",
      "Iteration: 19700\n",
      "Gradient: [  -3.0804    5.2308    5.2218   -0.8996 -132.9278]\n",
      "Weights: [-4.6496  0.2387 -0.8697  0.0685  0.1352]\n",
      "MSE loss: 97.1951\n",
      "Iteration: 19800\n",
      "Gradient: [ -1.4704   3.1749 -37.1918   9.9737  57.0771]\n",
      "Weights: [-4.656   0.2457 -0.8729  0.0687  0.1354]\n",
      "MSE loss: 97.0698\n",
      "Iteration: 19900\n",
      "Gradient: [   4.2193    9.823    -0.921    24.1689 -205.792 ]\n",
      "Weights: [-4.6697  0.2468 -0.8734  0.0686  0.1355]\n",
      "MSE loss: 97.2553\n",
      "Iteration: 20000\n",
      "Gradient: [ -1.353  -13.1302  20.9722  53.7354 103.1092]\n",
      "Weights: [-4.6585  0.2507 -0.8786  0.0686  0.1359]\n",
      "MSE loss: 96.8248\n",
      "Iteration: 20100\n",
      "Gradient: [ -4.0606 -11.7868 -38.5285 -78.1225 112.5378]\n",
      "Weights: [-4.6565  0.2499 -0.8798  0.0678  0.1363]\n",
      "MSE loss: 96.7285\n",
      "Iteration: 20200\n",
      "Gradient: [12.2277 27.7774 33.9502 65.3363 57.2411]\n",
      "Weights: [-4.6477  0.255  -0.8823  0.0685  0.1362]\n",
      "MSE loss: 96.7868\n",
      "Iteration: 20300\n",
      "Gradient: [  5.6344  10.8869   3.266  -53.0596 -62.3711]\n",
      "Weights: [-4.6514  0.2546 -0.8835  0.0684  0.1364]\n",
      "MSE loss: 96.6227\n",
      "Iteration: 20400\n",
      "Gradient: [ -5.2205   8.2711 -35.2707  42.711  -70.5077]\n",
      "Weights: [-4.6582  0.264  -0.8871  0.0676  0.1366]\n",
      "MSE loss: 96.504\n",
      "Iteration: 20500\n",
      "Gradient: [ -3.5015 -12.3971   8.3848 -16.5431 264.7456]\n",
      "Weights: [-4.6644  0.2749 -0.8901  0.0678  0.1368]\n",
      "MSE loss: 96.4089\n",
      "Iteration: 20600\n",
      "Gradient: [-10.6908  -1.7642 -63.6998  24.3491 -21.0053]\n",
      "Weights: [-4.6556  0.2684 -0.892   0.0681  0.1372]\n",
      "MSE loss: 96.3281\n",
      "Iteration: 20700\n",
      "Gradient: [  9.6414   2.8425  44.7532  43.0386 -53.6798]\n",
      "Weights: [-4.6474  0.2813 -0.892   0.0671  0.137 ]\n",
      "MSE loss: 97.3678\n",
      "Iteration: 20800\n",
      "Gradient: [ -4.1311  -0.0528  -0.1318  11.6879 -17.6447]\n",
      "Weights: [-4.667   0.2887 -0.893   0.0664  0.137 ]\n",
      "MSE loss: 96.3466\n",
      "Iteration: 20900\n",
      "Gradient: [  2.4737  -8.7954   4.884  -34.9904 110.4663]\n",
      "Weights: [-4.6733  0.2892 -0.8961  0.0662  0.1372]\n",
      "MSE loss: 96.3139\n",
      "Iteration: 21000\n",
      "Gradient: [ 6.81520e+00 -5.62000e-02  2.91624e+01  8.76892e+01 -9.11095e+01]\n",
      "Weights: [-4.6764  0.2947 -0.8979  0.0659  0.1378]\n",
      "MSE loss: 96.059\n",
      "Iteration: 21100\n",
      "Gradient: [ -12.9831    6.8198  -53.4447 -108.7057   40.8315]\n",
      "Weights: [-4.6888  0.2907 -0.8972  0.0659  0.1378]\n",
      "MSE loss: 96.5289\n",
      "Iteration: 21200\n",
      "Gradient: [  6.3683  -9.2693  -3.1565 114.7806   1.6249]\n",
      "Weights: [-4.6609  0.2918 -0.8992  0.0658  0.1378]\n",
      "MSE loss: 96.0767\n",
      "Iteration: 21300\n",
      "Gradient: [  13.8396    6.0765   24.6378   58.2384 -133.2685]\n",
      "Weights: [-4.6627  0.2926 -0.9015  0.0664  0.1379]\n",
      "MSE loss: 95.9749\n",
      "Iteration: 21400\n",
      "Gradient: [  1.3639   4.4607  42.4662  35.8814 325.4148]\n",
      "Weights: [-4.6751  0.3048 -0.9028  0.0657  0.138 ]\n",
      "MSE loss: 95.929\n",
      "Iteration: 21500\n",
      "Gradient: [   7.2196  -14.0268  -46.756   -55.8526 -116.6477]\n",
      "Weights: [-4.6557  0.2913 -0.9071  0.0663  0.1381]\n",
      "MSE loss: 96.1209\n",
      "Iteration: 21600\n",
      "Gradient: [  -7.0433  -15.3203  -30.9548  -60.6304 -586.1924]\n",
      "Weights: [-4.671   0.2919 -0.9036  0.0663  0.1381]\n",
      "MSE loss: 95.9414\n",
      "Iteration: 21700\n",
      "Gradient: [  -4.8934    6.5496  -18.8284  -22.9914 -110.6212]\n",
      "Weights: [-4.676   0.2977 -0.9044  0.0655  0.1383]\n",
      "MSE loss: 95.9635\n",
      "Iteration: 21800\n",
      "Gradient: [  -1.3134    7.173   -11.9807  -37.1262 -186.0182]\n",
      "Weights: [-4.6862  0.3072 -0.905   0.0656  0.1383]\n",
      "MSE loss: 95.8648\n",
      "Iteration: 21900\n",
      "Gradient: [ 10.6468  -3.9293  -6.9424 -92.5766 197.2756]\n",
      "Weights: [-4.6818  0.3013 -0.9057  0.065   0.1384]\n",
      "MSE loss: 96.1016\n",
      "Iteration: 22000\n",
      "Gradient: [ -9.0627  20.7095 -36.6254 110.2986 191.2788]\n",
      "Weights: [-4.6719  0.3024 -0.9067  0.0652  0.1387]\n",
      "MSE loss: 95.6641\n",
      "Iteration: 22100\n",
      "Gradient: [ 4.4383 -1.3337  6.5146 82.996  60.2241]\n",
      "Weights: [-4.6679  0.2971 -0.9073  0.0663  0.1387]\n",
      "MSE loss: 95.7467\n",
      "Iteration: 22200\n",
      "Gradient: [ -5.1566  -8.7364   5.83   -27.8978  14.0898]\n",
      "Weights: [-4.6619  0.3034 -0.9105  0.0656  0.1387]\n",
      "MSE loss: 95.6367\n",
      "Iteration: 22300\n",
      "Gradient: [ -3.46     4.5615 -24.5327 102.2399 -89.7829]\n",
      "Weights: [-4.665   0.3024 -0.9098  0.0658  0.1389]\n",
      "MSE loss: 95.6212\n",
      "Iteration: 22400\n",
      "Gradient: [ -0.7355  -9.69    21.667   94.9391 -91.739 ]\n",
      "Weights: [-4.6915  0.3307 -0.9121  0.0648  0.1386]\n",
      "MSE loss: 95.6897\n",
      "Iteration: 22500\n",
      "Gradient: [ -14.1182  -10.8045   -6.4261  -29.785  -472.3452]\n",
      "Weights: [-4.6973  0.3258 -0.9156  0.0667  0.1385]\n",
      "MSE loss: 95.7259\n",
      "Iteration: 22600\n",
      "Gradient: [ -6.5613  -5.9867  -2.9762  57.2207 248.4375]\n",
      "Weights: [-4.6822  0.3234 -0.9152  0.066   0.1389]\n",
      "MSE loss: 95.5888\n",
      "Iteration: 22700\n",
      "Gradient: [ 11.1777  -4.6485 -15.6615  69.1869  56.0976]\n",
      "Weights: [-4.6825  0.3286 -0.9184  0.0655  0.139 ]\n",
      "MSE loss: 95.3588\n",
      "Iteration: 22800\n",
      "Gradient: [-8.5407  3.1418 40.3017 82.1694 16.6481]\n",
      "Weights: [-4.6776  0.3159 -0.9168  0.0659  0.1393]\n",
      "MSE loss: 95.4019\n",
      "Iteration: 22900\n",
      "Gradient: [ -3.2144   1.2351  12.3731 -33.5724  65.7357]\n",
      "Weights: [-4.668   0.3113 -0.918   0.0658  0.1395]\n",
      "MSE loss: 95.3679\n",
      "Iteration: 23000\n",
      "Gradient: [ 10.8992  10.5072  18.7464  67.0645 290.4131]\n",
      "Weights: [-4.6461  0.307  -0.9209  0.0661  0.1397]\n",
      "MSE loss: 95.6876\n",
      "Iteration: 23100\n",
      "Gradient: [10.2285  8.9743 50.9265 69.1132 12.0361]\n",
      "Weights: [-4.6462  0.3098 -0.9228  0.067   0.14  ]\n",
      "MSE loss: 96.4013\n",
      "Iteration: 23200\n",
      "Gradient: [  -5.5793   17.4612   24.0538  -92.9653 -125.8695]\n",
      "Weights: [-4.6708  0.3158 -0.9268  0.0672  0.1398]\n",
      "MSE loss: 95.32\n",
      "Iteration: 23300\n",
      "Gradient: [  5.4948  -0.1515   0.954  -76.1639  96.7161]\n",
      "Weights: [-4.6523  0.3096 -0.928   0.0675  0.1401]\n",
      "MSE loss: 95.3308\n",
      "Iteration: 23400\n",
      "Gradient: [  -1.8552  -20.7885  -26.6058  100.0079 -198.9738]\n",
      "Weights: [-4.6818  0.3269 -0.9318  0.0677  0.14  ]\n",
      "MSE loss: 95.2925\n",
      "Iteration: 23500\n",
      "Gradient: [-10.5399 -20.8918 -32.5449 -12.8286 -76.5819]\n",
      "Weights: [-4.6774  0.3312 -0.9328  0.0674  0.1397]\n",
      "MSE loss: 95.2029\n",
      "Iteration: 23600\n",
      "Gradient: [   6.7763    7.6531   25.0923  -14.2045 -211.6609]\n",
      "Weights: [-4.6759  0.3457 -0.9388  0.0676  0.14  ]\n",
      "MSE loss: 94.8423\n",
      "Iteration: 23700\n",
      "Gradient: [-10.6197   8.2258  44.5885  73.79    98.5605]\n",
      "Weights: [-4.6764  0.3509 -0.9401  0.0679  0.14  ]\n",
      "MSE loss: 94.9068\n",
      "Iteration: 23800\n",
      "Gradient: [ -0.4819  -2.8607   5.5138 -55.2993 265.4207]\n",
      "Weights: [-4.6979  0.3605 -0.9403  0.0664  0.1401]\n",
      "MSE loss: 94.8693\n",
      "Iteration: 23900\n",
      "Gradient: [ -8.9119   6.3878   7.8025 -38.537   39.0947]\n",
      "Weights: [-4.7002  0.3657 -0.942   0.067   0.14  ]\n",
      "MSE loss: 94.8441\n",
      "Iteration: 24000\n",
      "Gradient: [  -6.4145   26.0534   13.1093  -58.8323 -106.4904]\n",
      "Weights: [-4.6867  0.3688 -0.9426  0.0669  0.1399]\n",
      "MSE loss: 94.8954\n",
      "Iteration: 24100\n",
      "Gradient: [  -2.7908    8.5205  -29.3507  137.3216 -187.7631]\n",
      "Weights: [-4.69    0.3615 -0.9422  0.0667  0.1403]\n",
      "MSE loss: 94.6985\n",
      "Iteration: 24200\n",
      "Gradient: [   1.2355  -28.6515   18.2206   38.073  -109.3497]\n",
      "Weights: [-4.6748  0.3584 -0.9455  0.0674  0.1404]\n",
      "MSE loss: 94.7438\n",
      "Iteration: 24300\n",
      "Gradient: [  3.2105   1.2632 -55.854  -93.1853 -33.0462]\n",
      "Weights: [-4.6841  0.3466 -0.9409  0.0667  0.1406]\n",
      "MSE loss: 94.8809\n",
      "Iteration: 24400\n",
      "Gradient: [  2.8906  13.3292 -18.6109   5.6425  35.9933]\n",
      "Weights: [-4.6628  0.3488 -0.9431  0.0672  0.1406]\n",
      "MSE loss: 95.0718\n",
      "Iteration: 24500\n",
      "Gradient: [  -4.8557  -10.0499  -37.7951  -49.551  -373.0844]\n",
      "Weights: [-4.6781  0.3505 -0.9455  0.0674  0.1407]\n",
      "MSE loss: 94.7125\n",
      "Iteration: 24600\n",
      "Gradient: [ -0.3056  -5.8245 -14.2768 -53.7217  97.7864]\n",
      "Weights: [-4.6695  0.3521 -0.9475  0.067   0.1408]\n",
      "MSE loss: 94.7472\n",
      "Iteration: 24700\n",
      "Gradient: [ -1.2738  -4.9327  13.2562 -42.0963 -15.1758]\n",
      "Weights: [-4.6828  0.3574 -0.9469  0.0667  0.141 ]\n",
      "MSE loss: 94.5956\n",
      "Iteration: 24800\n",
      "Gradient: [   3.0772   -6.3134   26.841   -26.629  -149.3487]\n",
      "Weights: [-4.688   0.3701 -0.9481  0.0657  0.141 ]\n",
      "MSE loss: 94.5128\n",
      "Iteration: 24900\n",
      "Gradient: [  1.7853   3.0375  23.9942  37.9327 130.5838]\n",
      "Weights: [-4.6773  0.3712 -0.9513  0.0662  0.1411]\n",
      "MSE loss: 94.6466\n",
      "Iteration: 25000\n",
      "Gradient: [ -0.2866 -10.4996 -16.1688 -79.5095 268.6459]\n",
      "Weights: [-4.6867  0.3781 -0.9563  0.0664  0.1414]\n",
      "MSE loss: 94.3395\n",
      "Iteration: 25100\n",
      "Gradient: [-10.3252   3.3575  19.3917   0.7793 220.2396]\n",
      "Weights: [-4.6827  0.3751 -0.9613  0.0675  0.1416]\n",
      "MSE loss: 94.3432\n",
      "Iteration: 25200\n",
      "Gradient: [   3.3328    9.5172  -29.0531  -30.7572 -194.7683]\n",
      "Weights: [-4.6828  0.3757 -0.9616  0.068   0.1416]\n",
      "MSE loss: 94.296\n",
      "Iteration: 25300\n",
      "Gradient: [  -7.8878    0.9911   -1.5903  -41.4764 -251.9064]\n",
      "Weights: [-4.696   0.3813 -0.9636  0.0677  0.1417]\n",
      "MSE loss: 94.4274\n",
      "Iteration: 25400\n",
      "Gradient: [ -1.7484   7.7204 -20.347  -93.7694  10.8127]\n",
      "Weights: [-4.7169  0.4023 -0.9656  0.0674  0.1417]\n",
      "MSE loss: 94.3523\n",
      "Iteration: 25500\n",
      "Gradient: [ -12.507   -13.0584  -38.9789 -171.2437 -222.2354]\n",
      "Weights: [-4.7025  0.4056 -0.9689  0.0666  0.1416]\n",
      "MSE loss: 94.2362\n",
      "Iteration: 25600\n",
      "Gradient: [  -0.7251   -3.1157  -10.0602   46.8047 -135.7836]\n",
      "Weights: [-4.7009  0.4011 -0.9707  0.0682  0.1418]\n",
      "MSE loss: 94.0441\n",
      "Iteration: 25700\n",
      "Gradient: [  0.1524 -11.5198 -54.2035  82.5998 -26.9571]\n",
      "Weights: [-4.6981  0.4064 -0.9718  0.0678  0.1418]\n",
      "MSE loss: 94.0112\n",
      "Iteration: 25800\n",
      "Gradient: [  -2.1308  -14.0991   12.6822    1.6267 -121.0242]\n",
      "Weights: [-4.7043  0.4151 -0.9734  0.067   0.1418]\n",
      "MSE loss: 94.0246\n",
      "Iteration: 25900\n",
      "Gradient: [  8.0978   3.9726  34.4386 -18.2171 -69.5294]\n",
      "Weights: [-4.6998  0.4097 -0.9744  0.067   0.142 ]\n",
      "MSE loss: 94.0437\n",
      "Iteration: 26000\n",
      "Gradient: [   1.4476   -3.5949    7.5052 -103.5602  -83.3771]\n",
      "Weights: [-4.6936  0.3996 -0.9729  0.0674  0.1422]\n",
      "MSE loss: 94.0174\n",
      "Iteration: 26100\n",
      "Gradient: [-12.4936 -10.4875 -18.9753 -29.9075  81.5833]\n",
      "Weights: [-4.6944  0.4035 -0.9724  0.0672  0.1419]\n",
      "MSE loss: 94.0741\n",
      "Iteration: 26200\n",
      "Gradient: [  10.2485   -4.4228  -17.1136   28.9727 -286.7186]\n",
      "Weights: [-4.7063  0.4092 -0.9731  0.0675  0.1419]\n",
      "MSE loss: 94.0167\n",
      "Iteration: 26300\n",
      "Gradient: [  7.956   20.0859  73.7337  16.978  102.7399]\n",
      "Weights: [-4.7018  0.4187 -0.9743  0.0677  0.142 ]\n",
      "MSE loss: 94.3342\n",
      "Iteration: 26400\n",
      "Gradient: [ -2.8884   5.4038  19.6192 -12.2904 -73.3289]\n",
      "Weights: [-4.7122  0.434  -0.9773  0.0666  0.1419]\n",
      "MSE loss: 94.0518\n",
      "Iteration: 26500\n",
      "Gradient: [ -10.0944  -23.0028    8.8654  -12.8445 -205.8701]\n",
      "Weights: [-4.7231  0.4236 -0.9764  0.0672  0.1422]\n",
      "MSE loss: 94.0615\n",
      "Iteration: 26600\n",
      "Gradient: [   7.7035    5.5646   -1.5058   -2.0462 -219.071 ]\n",
      "Weights: [-4.7118  0.4251 -0.9801  0.0672  0.1422]\n",
      "MSE loss: 93.9563\n",
      "Iteration: 26700\n",
      "Gradient: [   3.7411    0.2692   -7.338  -110.1882  -45.0554]\n",
      "Weights: [-4.7086  0.4321 -0.9804  0.0659  0.1423]\n",
      "MSE loss: 93.9245\n",
      "Iteration: 26800\n",
      "Gradient: [ 10.1858  21.7818   6.4005  47.1941 -55.7233]\n",
      "Weights: [-4.7009  0.4244 -0.9797  0.0672  0.1425]\n",
      "MSE loss: 94.0793\n",
      "Iteration: 26900\n",
      "Gradient: [ 10.557   15.1413  10.8959  -8.231  171.6283]\n",
      "Weights: [-4.7011  0.4246 -0.9837  0.0672  0.1426]\n",
      "MSE loss: 93.8075\n",
      "Iteration: 27000\n",
      "Gradient: [ -1.3034   5.01     4.8503  96.3194 142.4611]\n",
      "Weights: [-4.7006  0.4235 -0.9816  0.0672  0.1427]\n",
      "MSE loss: 93.8756\n",
      "Iteration: 27100\n",
      "Gradient: [   4.688    -4.3438    8.2684  -46.6608 -174.4749]\n",
      "Weights: [-4.7017  0.4213 -0.9835  0.0677  0.1426]\n",
      "MSE loss: 93.802\n",
      "Iteration: 27200\n",
      "Gradient: [ -11.5154   -7.6943  -30.9278   77.9601 -111.6183]\n",
      "Weights: [-4.7192  0.4379 -0.9862  0.0677  0.1426]\n",
      "MSE loss: 93.7259\n",
      "Iteration: 27300\n",
      "Gradient: [   8.3745    8.2928  -19.8626  -52.6417 -127.4297]\n",
      "Weights: [-4.7156  0.4425 -0.9868  0.0667  0.1425]\n",
      "MSE loss: 93.7975\n",
      "Iteration: 27400\n",
      "Gradient: [  0.3307  -5.1326  -1.496  101.1015 143.4062]\n",
      "Weights: [-4.7168  0.4471 -0.9889  0.068   0.1427]\n",
      "MSE loss: 93.9668\n",
      "Iteration: 27500\n",
      "Gradient: [  2.1922   2.4168  -7.4645  53.0222 -30.4745]\n",
      "Weights: [-4.707   0.4476 -0.9934  0.0674  0.1427]\n",
      "MSE loss: 93.7826\n",
      "Iteration: 27600\n",
      "Gradient: [  8.8735  14.1065  18.9399 121.3988 326.4163]\n",
      "Weights: [-4.7085  0.449  -0.995   0.0677  0.1431]\n",
      "MSE loss: 93.6015\n",
      "Iteration: 27700\n",
      "Gradient: [  -1.0136   -5.8719  -10.3235  -31.6101 -125.384 ]\n",
      "Weights: [-4.7222  0.4545 -0.9964  0.068   0.1432]\n",
      "MSE loss: 93.5377\n",
      "Iteration: 27800\n",
      "Gradient: [   2.2383   -7.8998   -4.8733   -3.6922 -297.0702]\n",
      "Weights: [-4.7189  0.4535 -0.9978  0.068   0.1431]\n",
      "MSE loss: 93.5066\n",
      "Iteration: 27900\n",
      "Gradient: [   1.4901   15.179    -3.3301  -32.878  -164.2719]\n",
      "Weights: [-4.7202  0.4563 -0.9977  0.0678  0.1431]\n",
      "MSE loss: 93.5103\n",
      "Iteration: 28000\n",
      "Gradient: [ -9.5316  -6.1755 -18.0601   9.9867 -13.4878]\n",
      "Weights: [-4.728   0.4555 -0.9985  0.0685  0.1431]\n",
      "MSE loss: 93.6446\n",
      "Iteration: 28100\n",
      "Gradient: [  -1.861   -10.6537  -27.1001   25.6836 -657.4094]\n",
      "Weights: [-4.7097  0.4515 -1.0004  0.0678  0.1434]\n",
      "MSE loss: 93.4659\n",
      "Iteration: 28200\n",
      "Gradient: [ 8.4202  1.2849 22.8989 65.3804 -4.7759]\n",
      "Weights: [-4.7141  0.449  -0.9997  0.0684  0.1435]\n",
      "MSE loss: 93.4451\n",
      "Iteration: 28300\n",
      "Gradient: [ -0.8808 -23.8772  26.476  -13.9852 149.3778]\n",
      "Weights: [-4.7107  0.4536 -1.0001  0.0684  0.1434]\n",
      "MSE loss: 93.4779\n",
      "Iteration: 28400\n",
      "Gradient: [ -4.9181 -21.8949  -6.3156 135.9868 125.8497]\n",
      "Weights: [-4.7188  0.4599 -1.0014  0.0681  0.1435]\n",
      "MSE loss: 93.4171\n",
      "Iteration: 28500\n",
      "Gradient: [ -2.425   -4.3159  13.067  -48.7697  34.6963]\n",
      "Weights: [-4.7011  0.4508 -1.0048  0.0684  0.1438]\n",
      "MSE loss: 93.4421\n",
      "Iteration: 28600\n",
      "Gradient: [  -8.493    -2.442   -17.5145    1.2168 -180.3965]\n",
      "Weights: [-4.6973  0.4428 -1.0057  0.0692  0.1438]\n",
      "MSE loss: 93.5767\n",
      "Iteration: 28700\n",
      "Gradient: [ 9.3111 11.7422 -7.3408 13.2681 54.8125]\n",
      "Weights: [-4.7084  0.4437 -1.0044  0.0704  0.1437]\n",
      "MSE loss: 93.5012\n",
      "Iteration: 28800\n",
      "Gradient: [   1.2498   -2.5846  -23.8436   34.3086 -239.2794]\n",
      "Weights: [-4.7151  0.4486 -1.0065  0.07    0.1437]\n",
      "MSE loss: 93.5987\n",
      "Iteration: 28900\n",
      "Gradient: [  2.6633  -9.8594  41.5786  72.2669 331.0999]\n",
      "Weights: [-4.7061  0.4602 -1.0089  0.0702  0.1435]\n",
      "MSE loss: 93.4247\n",
      "Iteration: 29000\n",
      "Gradient: [-10.9268  10.1672  -1.8171  65.8633 -62.225 ]\n",
      "Weights: [-4.731   0.4686 -1.0105  0.0708  0.1436]\n",
      "MSE loss: 93.4355\n",
      "Iteration: 29100\n",
      "Gradient: [-6.3077  2.0128  9.01   74.4185 74.7997]\n",
      "Weights: [-4.7236  0.4746 -1.0132  0.0705  0.1435]\n",
      "MSE loss: 93.2396\n",
      "Iteration: 29200\n",
      "Gradient: [  -1.3271   -5.0809    2.424    13.6884 -140.7614]\n",
      "Weights: [-4.7381  0.4731 -1.0131  0.0707  0.1434]\n",
      "MSE loss: 93.7948\n",
      "Iteration: 29300\n",
      "Gradient: [ -0.4548  10.0839   4.3749 105.9187 264.7638]\n",
      "Weights: [-4.7112  0.4782 -1.0155  0.0702  0.1437]\n",
      "MSE loss: 93.4832\n",
      "Iteration: 29400\n",
      "Gradient: [ -3.5674   9.919    4.0805 -68.7944  90.5975]\n",
      "Weights: [-4.7193  0.4681 -1.016   0.0709  0.1437]\n",
      "MSE loss: 93.3098\n",
      "Iteration: 29500\n",
      "Gradient: [ -8.8718   5.5319 -23.3189  26.6572 224.7556]\n",
      "Weights: [-4.7215  0.485  -1.0215  0.0703  0.1439]\n",
      "MSE loss: 93.1809\n",
      "Iteration: 29600\n",
      "Gradient: [-5.9037 -8.9998 -3.9945 20.3988 83.2975]\n",
      "Weights: [-4.7174  0.482  -1.0226  0.0711  0.1438]\n",
      "MSE loss: 93.1728\n",
      "Iteration: 29700\n",
      "Gradient: [  -2.815     8.062   -48.486    26.6176 -151.9276]\n",
      "Weights: [-4.7294  0.488  -1.0213  0.0702  0.144 ]\n",
      "MSE loss: 93.114\n",
      "Iteration: 29800\n",
      "Gradient: [  5.2259  12.6312 -10.4484  23.8523  56.0627]\n",
      "Weights: [-4.7303  0.4934 -1.021   0.0701  0.1441]\n",
      "MSE loss: 93.2644\n",
      "Iteration: 29900\n",
      "Gradient: [ -1.1043   5.7884  -0.4371   5.2656 143.38  ]\n",
      "Weights: [-4.7217  0.4946 -1.0245  0.0699  0.1442]\n",
      "MSE loss: 93.1288\n",
      "Iteration: 30000\n",
      "Gradient: [ -0.2842  -0.0959  -6.6484 -63.899  -60.5851]\n",
      "Weights: [-4.7147  0.4855 -1.0243  0.0707  0.1442]\n",
      "MSE loss: 93.1277\n",
      "Iteration: 30100\n",
      "Gradient: [ -8.8676   7.8171 -59.3214 -25.5231 111.8251]\n",
      "Weights: [-4.7292  0.4837 -1.0248  0.0714  0.1443]\n",
      "MSE loss: 93.1778\n",
      "Iteration: 30200\n",
      "Gradient: [   4.9368    7.1432   15.9291 -127.675  -116.6761]\n",
      "Weights: [-4.7236  0.4851 -1.0244  0.0716  0.1442]\n",
      "MSE loss: 93.1075\n",
      "Iteration: 30300\n",
      "Gradient: [ -11.5926    9.4431  -21.5177  -63.137  -112.5152]\n",
      "Weights: [-4.741   0.4929 -1.0275  0.0709  0.1445]\n",
      "MSE loss: 93.4011\n",
      "Iteration: 30400\n",
      "Gradient: [  -6.3494    5.528     5.747    -7.5286 -308.1573]\n",
      "Weights: [-4.7241  0.4935 -1.0307  0.0715  0.1444]\n",
      "MSE loss: 92.9979\n",
      "Iteration: 30500\n",
      "Gradient: [  1.6984  22.2034  -4.5175 132.7508  18.9961]\n",
      "Weights: [-4.7208  0.5004 -1.0291  0.0706  0.1446]\n",
      "MSE loss: 93.3207\n",
      "Iteration: 30600\n",
      "Gradient: [ -2.3215  -5.7114  -9.6056   0.4939 110.6153]\n",
      "Weights: [-4.7188  0.4913 -1.0291  0.0708  0.1445]\n",
      "MSE loss: 93.0179\n",
      "Iteration: 30700\n",
      "Gradient: [  0.2132  -6.8649 -15.538  -33.0385 125.8368]\n",
      "Weights: [-4.7232  0.5031 -1.0292  0.07    0.1445]\n",
      "MSE loss: 93.1261\n",
      "Iteration: 30800\n",
      "Gradient: [   6.1928   -3.0938   -2.1113  -15.6841 -271.4836]\n",
      "Weights: [-4.727   0.5053 -1.0323  0.0699  0.1448]\n",
      "MSE loss: 92.9467\n",
      "Iteration: 30900\n",
      "Gradient: [ -2.6732  17.1922 -28.2605 -35.3247 -42.1996]\n",
      "Weights: [-4.7326  0.504  -1.0331  0.0699  0.1451]\n",
      "MSE loss: 92.9347\n",
      "Iteration: 31000\n",
      "Gradient: [   2.9244   -0.221    -5.3406   42.0091 -103.073 ]\n",
      "Weights: [-4.7281  0.4991 -1.0299  0.0691  0.1451]\n",
      "MSE loss: 92.9542\n",
      "Iteration: 31100\n",
      "Gradient: [  8.0429  16.2907  30.6832  61.7112 -37.69  ]\n",
      "Weights: [-4.716   0.4979 -1.0307  0.0686  0.1455]\n",
      "MSE loss: 93.1364\n",
      "Iteration: 31200\n",
      "Gradient: [  -4.9901   -7.3131    9.1466   51.7016 -183.787 ]\n",
      "Weights: [-4.7368  0.5078 -1.0339  0.0691  0.1454]\n",
      "MSE loss: 92.9365\n",
      "Iteration: 31300\n",
      "Gradient: [   6.1461   -8.4757  -56.4862 -137.4851 -212.9482]\n",
      "Weights: [-4.7434  0.5164 -1.0364  0.068   0.1455]\n",
      "MSE loss: 93.1542\n",
      "Iteration: 31400\n",
      "Gradient: [-4.52510e+00 -3.93090e+00  2.35000e-02 -3.53966e+01  4.20703e+01]\n",
      "Weights: [-4.7414  0.524  -1.0392  0.0675  0.1459]\n",
      "MSE loss: 92.8182\n",
      "Iteration: 31500\n",
      "Gradient: [   2.1155    2.4177    6.3877 -159.7882 -103.7162]\n",
      "Weights: [-4.732   0.5224 -1.0416  0.068   0.1459]\n",
      "MSE loss: 92.811\n",
      "Iteration: 31600\n",
      "Gradient: [  7.0592  -1.3602 -12.8809 -32.1639 -94.7547]\n",
      "Weights: [-4.7386  0.5243 -1.0415  0.0678  0.1458]\n",
      "MSE loss: 92.8758\n",
      "Iteration: 31700\n",
      "Gradient: [   3.1127    3.8056  -13.679    24.7359 -174.7137]\n",
      "Weights: [-4.7396  0.5227 -1.0405  0.0678  0.1458]\n",
      "MSE loss: 92.8719\n",
      "Iteration: 31800\n",
      "Gradient: [ -10.0192  -10.1441  -14.599  -119.2435  -53.7542]\n",
      "Weights: [-4.7459  0.5222 -1.0425  0.0681  0.1461]\n",
      "MSE loss: 93.0848\n",
      "Iteration: 31900\n",
      "Gradient: [  1.5453   5.5912 -56.13    10.8416 378.6062]\n",
      "Weights: [-4.7338  0.5236 -1.0421  0.0683  0.1459]\n",
      "MSE loss: 92.7844\n",
      "Iteration: 32000\n",
      "Gradient: [   5.9212    8.7507   35.7255   11.7069 -159.1316]\n",
      "Weights: [-4.7295  0.5167 -1.0368  0.0685  0.1457]\n",
      "MSE loss: 93.0432\n",
      "Iteration: 32100\n",
      "Gradient: [   4.9185   12.5875   46.8216   13.0938 -299.2673]\n",
      "Weights: [-4.7373  0.5158 -1.0369  0.0685  0.1456]\n",
      "MSE loss: 92.8556\n",
      "Iteration: 32200\n",
      "Gradient: [-10.6522 -11.985   27.4648   5.3341 -62.3968]\n",
      "Weights: [-4.7401  0.5147 -1.0374  0.0686  0.1457]\n",
      "MSE loss: 92.9025\n",
      "Iteration: 32300\n",
      "Gradient: [ -3.3976  10.6896  -6.4463 -77.1078 241.4452]\n",
      "Weights: [-4.7289  0.5051 -1.0375  0.0694  0.1458]\n",
      "MSE loss: 92.9053\n",
      "Iteration: 32400\n",
      "Gradient: [  -2.343     1.7129  -38.7372  -13.8486 -244.3796]\n",
      "Weights: [-4.7207  0.5072 -1.0394  0.0693  0.1457]\n",
      "MSE loss: 92.9033\n",
      "Iteration: 32500\n",
      "Gradient: [-6.0325 -4.0821  6.6601 32.8218 31.8041]\n",
      "Weights: [-4.7317  0.5071 -1.0357  0.0688  0.1457]\n",
      "MSE loss: 92.8931\n",
      "Iteration: 32600\n",
      "Gradient: [  3.5232 -10.5352   3.6199 -18.9971 162.074 ]\n",
      "Weights: [-4.7172  0.5108 -1.0372  0.0688  0.1455]\n",
      "MSE loss: 93.0891\n",
      "Iteration: 32700\n",
      "Gradient: [ -10.5568   -7.7473   -3.7392  -32.3856 -223.0797]\n",
      "Weights: [-4.7273  0.5021 -1.0366  0.069   0.1457]\n",
      "MSE loss: 93.0062\n",
      "Iteration: 32800\n",
      "Gradient: [  -3.7643   15.122   -10.6233  -49.757  -124.7285]\n",
      "Weights: [-4.7149  0.495  -1.0372  0.0691  0.1459]\n",
      "MSE loss: 93.079\n",
      "Iteration: 32900\n",
      "Gradient: [ -2.3013  -0.9176  28.374  104.8202 295.829 ]\n",
      "Weights: [-4.7274  0.5031 -1.0342  0.0693  0.1456]\n",
      "MSE loss: 92.9692\n",
      "Iteration: 33000\n",
      "Gradient: [ 6.870000e-02 -1.334200e+00  1.036190e+01 -6.807300e+00 -2.647562e+02]\n",
      "Weights: [-4.7326  0.5053 -1.0352  0.0686  0.1456]\n",
      "MSE loss: 92.9731\n",
      "Iteration: 33100\n",
      "Gradient: [   1.4771    4.823   -30.6993   43.9239 -249.1933]\n",
      "Weights: [-4.7533  0.5246 -1.0404  0.0686  0.1456]\n",
      "MSE loss: 93.1887\n",
      "Iteration: 33200\n",
      "Gradient: [  1.1291  -2.8875   0.9475   3.0267 111.32  ]\n",
      "Weights: [-4.7439  0.5358 -1.0443  0.0679  0.1458]\n",
      "MSE loss: 92.772\n",
      "Iteration: 33300\n",
      "Gradient: [ 10.8979   1.1836   6.8392  91.8708 112.356 ]\n",
      "Weights: [-4.7317  0.5392 -1.0505  0.068   0.1462]\n",
      "MSE loss: 92.8196\n",
      "Iteration: 33400\n",
      "Gradient: [-12.4679 -15.1139 -19.2341 -30.5491 -71.9388]\n",
      "Weights: [-4.7616  0.5418 -1.0498  0.068   0.1464]\n",
      "MSE loss: 93.2071\n",
      "Iteration: 33500\n",
      "Gradient: [  4.4136  -3.937   11.3661  79.9201 106.6819]\n",
      "Weights: [-4.7483  0.5432 -1.0507  0.0682  0.1465]\n",
      "MSE loss: 92.7086\n",
      "Iteration: 33600\n",
      "Gradient: [  2.2254   9.525   75.1978  17.5035 -65.2419]\n",
      "Weights: [-4.7355  0.5491 -1.0532  0.0682  0.1463]\n",
      "MSE loss: 92.8812\n",
      "Iteration: 33700\n",
      "Gradient: [  -5.8138   10.3386   13.7528   79.5304 -105.0108]\n",
      "Weights: [-4.7375  0.547  -1.0529  0.069   0.1462]\n",
      "MSE loss: 92.8519\n",
      "Iteration: 33800\n",
      "Gradient: [   1.8912    8.9716  -48.7178 -137.7992  215.0107]\n",
      "Weights: [-4.7534  0.5607 -1.0552  0.0685  0.1463]\n",
      "MSE loss: 92.7805\n",
      "Iteration: 33900\n",
      "Gradient: [ -2.427   12.1597 -31.5052 -61.4682 351.7985]\n",
      "Weights: [-4.7483  0.5585 -1.0561  0.068   0.1465]\n",
      "MSE loss: 92.6809\n",
      "Iteration: 34000\n",
      "Gradient: [ -3.3096  -5.2777  -8.0652 -35.9543 304.411 ]\n",
      "Weights: [-4.7383  0.5489 -1.0587  0.069   0.1467]\n",
      "MSE loss: 92.6206\n",
      "Iteration: 34100\n",
      "Gradient: [ -17.7919   -2.0653   13.1445   34.7812 -333.5456]\n",
      "Weights: [-4.7441  0.5472 -1.0584  0.0694  0.1466]\n",
      "MSE loss: 92.6253\n",
      "Iteration: 34200\n",
      "Gradient: [ -1.4435  12.1413   3.7054  47.719  237.8007]\n",
      "Weights: [-4.7566  0.5524 -1.0604  0.0695  0.1471]\n",
      "MSE loss: 92.7764\n",
      "Iteration: 34300\n",
      "Gradient: [   7.7469   16.0751   -5.5109  -27.9677 -126.9272]\n",
      "Weights: [-4.7501  0.5503 -1.0616  0.0696  0.1469]\n",
      "MSE loss: 92.6996\n",
      "Iteration: 34400\n",
      "Gradient: [   6.3441   25.1454   17.9971  103.6215 -329.2588]\n",
      "Weights: [-4.7316  0.5645 -1.0652  0.0684  0.147 ]\n",
      "MSE loss: 92.9747\n",
      "Iteration: 34500\n",
      "Gradient: [  -4.0914  -14.0702  -49.0805 -103.8819 -207.7275]\n",
      "Weights: [-4.7678  0.5709 -1.0673  0.069   0.1472]\n",
      "MSE loss: 92.8189\n",
      "Iteration: 34600\n",
      "Gradient: [ -3.1924   5.78    65.2125  59.1009 130.8613]\n",
      "Weights: [-4.7656  0.5862 -1.0693  0.0687  0.147 ]\n",
      "MSE loss: 92.5976\n",
      "Iteration: 34700\n",
      "Gradient: [  -1.532    -9.4777  -10.9608  -54.456  -175.1308]\n",
      "Weights: [-4.75    0.5777 -1.0702  0.0687  0.1471]\n",
      "MSE loss: 92.5611\n",
      "Iteration: 34800\n",
      "Gradient: [   0.5156  -19.8543  -14.0442 -121.1453    5.5657]\n",
      "Weights: [-4.7465  0.5754 -1.0736  0.0692  0.1471]\n",
      "MSE loss: 92.6681\n",
      "Iteration: 34900\n",
      "Gradient: [   0.7847  -18.46      1.3894 -117.732  -175.8775]\n",
      "Weights: [-4.7625  0.5826 -1.0768  0.0711  0.1471]\n",
      "MSE loss: 92.4757\n",
      "Iteration: 35000\n",
      "Gradient: [   3.5565   15.1472   28.3172  -19.2704 -119.7024]\n",
      "Weights: [-4.7578  0.583  -1.0774  0.0715  0.1471]\n",
      "MSE loss: 92.4715\n",
      "Iteration: 35100\n",
      "Gradient: [ -2.0795  22.154   17.2751  72.4167 264.5169]\n",
      "Weights: [-4.7683  0.5961 -1.0803  0.0717  0.1471]\n",
      "MSE loss: 92.5694\n",
      "Iteration: 35200\n",
      "Gradient: [  1.9322  -9.4574   7.4743  27.9364 135.3815]\n",
      "Weights: [-4.7656  0.5898 -1.0808  0.072   0.1471]\n",
      "MSE loss: 92.4483\n",
      "Iteration: 35300\n",
      "Gradient: [  -3.371   -28.3401  -14.1129  -73.8897 -107.3416]\n",
      "Weights: [-4.7655  0.5868 -1.0833  0.0728  0.147 ]\n",
      "MSE loss: 92.6391\n",
      "Iteration: 35400\n",
      "Gradient: [  3.411   -0.075  -19.2656  24.0239  55.2695]\n",
      "Weights: [-4.7483  0.5861 -1.0815  0.072   0.147 ]\n",
      "MSE loss: 92.4762\n",
      "Iteration: 35500\n",
      "Gradient: [   5.1521    2.9303   36.5634 -108.0285  240.4347]\n",
      "Weights: [-4.7507  0.576  -1.0757  0.0715  0.1471]\n",
      "MSE loss: 92.4531\n",
      "Iteration: 35600\n",
      "Gradient: [ -0.4034   9.3007  -1.5782 -65.4493 282.9353]\n",
      "Weights: [-4.7603  0.5869 -1.0787  0.0723  0.147 ]\n",
      "MSE loss: 92.6214\n",
      "Iteration: 35700\n",
      "Gradient: [  8.6599  -6.0322 -44.4624 107.5174  86.6287]\n",
      "Weights: [-4.751   0.5835 -1.0793  0.0714  0.1471]\n",
      "MSE loss: 92.4409\n",
      "Iteration: 35800\n",
      "Gradient: [  -3.1455  -16.4642   -0.8246  -87.7997 -237.5998]\n",
      "Weights: [-4.7489  0.5767 -1.079   0.0715  0.1472]\n",
      "MSE loss: 92.4831\n",
      "Iteration: 35900\n",
      "Gradient: [  -9.3052   -6.72      7.3998    1.0184 -217.1399]\n",
      "Weights: [-4.7641  0.5841 -1.08    0.0727  0.1472]\n",
      "MSE loss: 92.5652\n",
      "Iteration: 36000\n",
      "Gradient: [  4.3994  -2.5543   7.7261  52.5332 -59.7835]\n",
      "Weights: [-4.767   0.594  -1.0841  0.0723  0.1472]\n",
      "MSE loss: 92.438\n",
      "Iteration: 36100\n",
      "Gradient: [  4.2752   9.7156  -5.9144 -74.8445 119.4114]\n",
      "Weights: [-4.7467  0.5946 -1.0859  0.0715  0.1473]\n",
      "MSE loss: 92.5503\n",
      "Iteration: 36200\n",
      "Gradient: [   1.2169   -6.508     7.0425  -49.9392 -467.479 ]\n",
      "Weights: [-4.771   0.5985 -1.0855  0.0717  0.1473]\n",
      "MSE loss: 92.5133\n",
      "Iteration: 36300\n",
      "Gradient: [ -8.4454  -2.01    -4.4942 -29.1885 -56.9329]\n",
      "Weights: [-4.7658  0.6072 -1.0894  0.0713  0.1474]\n",
      "MSE loss: 92.4555\n",
      "Iteration: 36400\n",
      "Gradient: [  6.5402  -6.9482 -13.7      9.9156 107.4933]\n",
      "Weights: [-4.7709  0.6041 -1.0889  0.0719  0.1475]\n",
      "MSE loss: 92.422\n",
      "Iteration: 36500\n",
      "Gradient: [ -1.1164   8.0022 -21.5025  14.7231  23.7932]\n",
      "Weights: [-4.7506  0.6035 -1.0911  0.0719  0.1476]\n",
      "MSE loss: 92.5193\n",
      "Iteration: 36600\n",
      "Gradient: [ 2.433300e+00 -1.014000e-01 -2.222960e+01 -2.536500e+00 -1.119676e+02]\n",
      "Weights: [-4.7709  0.6116 -1.0904  0.0717  0.1476]\n",
      "MSE loss: 92.4114\n",
      "Iteration: 36700\n",
      "Gradient: [ -3.0074   2.3091 -24.7157  54.8173  34.1893]\n",
      "Weights: [-4.7577  0.5985 -1.0891  0.0718  0.1476]\n",
      "MSE loss: 92.3506\n",
      "Iteration: 36800\n",
      "Gradient: [ -3.7162  -3.6059   8.0793  39.5165 108.6223]\n",
      "Weights: [-4.755   0.5913 -1.0843  0.0711  0.1476]\n",
      "MSE loss: 92.3815\n",
      "Iteration: 36900\n",
      "Gradient: [-11.3946  -0.1301  27.6905  31.9619 -59.6472]\n",
      "Weights: [-4.7511  0.5908 -1.0848  0.0705  0.1479]\n",
      "MSE loss: 92.416\n",
      "Iteration: 37000\n",
      "Gradient: [  1.7711   1.2098 -24.32    21.531  217.4983]\n",
      "Weights: [-4.7419  0.5782 -1.0837  0.0713  0.1478]\n",
      "MSE loss: 92.4523\n",
      "Iteration: 37100\n",
      "Gradient: [ -1.9502  24.5889  14.1149  74.59   155.1702]\n",
      "Weights: [-4.7592  0.5789 -1.0843  0.0722  0.1477]\n",
      "MSE loss: 92.7182\n",
      "Iteration: 37200\n",
      "Gradient: [ -2.81   -11.0687 -11.6591 -16.6729  87.3562]\n",
      "Weights: [-4.7489  0.5735 -1.0848  0.0723  0.1478]\n",
      "MSE loss: 92.7082\n",
      "Iteration: 37300\n",
      "Gradient: [ -2.0482   1.7142 -12.5983 -82.3682 310.5837]\n",
      "Weights: [-4.7607  0.5819 -1.0823  0.0716  0.1478]\n",
      "MSE loss: 92.5197\n",
      "Iteration: 37400\n",
      "Gradient: [  -3.7883    5.3566  -14.2589   -5.745  -124.112 ]\n",
      "Weights: [-4.7423  0.5836 -1.0834  0.0718  0.1475]\n",
      "MSE loss: 92.5402\n",
      "Iteration: 37500\n",
      "Gradient: [   1.4251    0.384    27.6234  -49.183  -219.1283]\n",
      "Weights: [-4.7654  0.5972 -1.0849  0.0712  0.1477]\n",
      "MSE loss: 92.4096\n",
      "Iteration: 37600\n",
      "Gradient: [  3.6516 -17.7597   7.5552 -80.0473 -40.9008]\n",
      "Weights: [-4.7569  0.5948 -1.0853  0.0704  0.1477]\n",
      "MSE loss: 92.4114\n",
      "Iteration: 37700\n",
      "Gradient: [ -8.1989  -1.1355  -8.1272 -72.2551 -55.8972]\n",
      "Weights: [-4.7714  0.5987 -1.082   0.0705  0.1474]\n",
      "MSE loss: 92.4724\n",
      "Iteration: 37800\n",
      "Gradient: [   3.7331    3.3329   -6.3741   42.2972 -233.6398]\n",
      "Weights: [-4.7647  0.5926 -1.0833  0.0713  0.1474]\n",
      "MSE loss: 92.4451\n",
      "Iteration: 37900\n",
      "Gradient: [  2.1259   1.7021 -15.4121 -78.2024  55.9637]\n",
      "Weights: [-4.7496  0.5888 -1.0839  0.0718  0.1473]\n",
      "MSE loss: 92.4294\n",
      "Iteration: 38000\n",
      "Gradient: [  -4.1805    9.3665  -10.4706    0.9193 -198.4232]\n",
      "Weights: [-4.7505  0.5698 -1.0812  0.0733  0.1475]\n",
      "MSE loss: 92.5414\n",
      "Iteration: 38100\n",
      "Gradient: [ -5.9147  12.5493   9.3242  71.4583 129.1198]\n",
      "Weights: [-4.7414  0.566  -1.081   0.0733  0.1474]\n",
      "MSE loss: 92.4944\n",
      "Iteration: 38200\n",
      "Gradient: [  -6.9055   -9.7395   12.3897  -78.7147 -159.6431]\n",
      "Weights: [-4.7539  0.5715 -1.0804  0.0731  0.1473]\n",
      "MSE loss: 92.5394\n",
      "Iteration: 38300\n",
      "Gradient: [ 12.3338  -4.1644   3.3525 -54.1569 -79.5485]\n",
      "Weights: [-4.7378  0.5796 -1.0817  0.0726  0.147 ]\n",
      "MSE loss: 92.6064\n",
      "Iteration: 38400\n",
      "Gradient: [-4.63000e-02  5.55140e+00 -4.33800e+01  6.52507e+01  5.82455e+01]\n",
      "Weights: [-4.747   0.5794 -1.0816  0.0726  0.1474]\n",
      "MSE loss: 92.5453\n",
      "Iteration: 38500\n",
      "Gradient: [  -8.6288   -2.4133  -23.5547   47.9936 -138.168 ]\n",
      "Weights: [-4.7758  0.5893 -1.081   0.0723  0.1472]\n",
      "MSE loss: 92.7135\n",
      "Iteration: 38600\n",
      "Gradient: [   2.4004   -0.7363  -29.4687  -89.2435 -275.4917]\n",
      "Weights: [-4.7551  0.5839 -1.0823  0.0722  0.1472]\n",
      "MSE loss: 92.4055\n",
      "Iteration: 38700\n",
      "Gradient: [ -2.3057   6.4967  11.6122  75.5672 186.8562]\n",
      "Weights: [-4.7559  0.5858 -1.0845  0.0728  0.1473]\n",
      "MSE loss: 92.3816\n",
      "Iteration: 38800\n",
      "Gradient: [  3.8894   3.2563   8.5231  44.737  109.1494]\n",
      "Weights: [-4.7551  0.5952 -1.0876  0.0737  0.1472]\n",
      "MSE loss: 92.5348\n",
      "Iteration: 38900\n",
      "Gradient: [  -7.5575   -9.9027    2.4226 -116.2084  258.4927]\n",
      "Weights: [-4.7589  0.5869 -1.0861  0.0733  0.1472]\n",
      "MSE loss: 92.4369\n",
      "Iteration: 39000\n",
      "Gradient: [  1.477    4.044    9.7456  76.234  148.635 ]\n",
      "Weights: [-4.7519  0.5829 -1.0862  0.0738  0.1472]\n",
      "MSE loss: 92.378\n",
      "Iteration: 39100\n",
      "Gradient: [ -2.9719   1.4022  11.4835 -16.1295   0.0403]\n",
      "Weights: [-4.7519  0.5835 -1.0861  0.0734  0.1473]\n",
      "MSE loss: 92.3861\n",
      "Iteration: 39200\n",
      "Gradient: [  -9.1001   -7.9024   -7.5378 -104.4353 -200.4343]\n",
      "Weights: [-4.7524  0.5712 -1.0831  0.0734  0.1472]\n",
      "MSE loss: 92.8657\n",
      "Iteration: 39300\n",
      "Gradient: [ -4.72    -5.7947  -7.5253  14.8076 201.0513]\n",
      "Weights: [-4.7467  0.5769 -1.084   0.074   0.1473]\n",
      "MSE loss: 92.503\n",
      "Iteration: 39400\n",
      "Gradient: [   2.8563  -27.2468  -17.3705  -53.7626 -140.0984]\n",
      "Weights: [-4.7381  0.5696 -1.082   0.074   0.1473]\n",
      "MSE loss: 92.5567\n",
      "Iteration: 39500\n",
      "Gradient: [ 14.4165 -16.872   22.2271 -83.89    82.8598]\n",
      "Weights: [-4.7534  0.5807 -1.0817  0.0734  0.1471]\n",
      "MSE loss: 92.4334\n",
      "Iteration: 39600\n",
      "Gradient: [   1.8341  -11.7743    5.5502  -92.0282 -136.6156]\n",
      "Weights: [-4.753   0.5666 -1.0827  0.0747  0.1471]\n",
      "MSE loss: 92.8159\n",
      "Iteration: 39700\n",
      "Gradient: [ -1.3824   1.3618  22.256  -49.2518 -75.8844]\n",
      "Weights: [-4.7487  0.5774 -1.0836  0.0742  0.1469]\n",
      "MSE loss: 92.3969\n",
      "Iteration: 39800\n",
      "Gradient: [  12.8498   11.1242    9.6774 -128.321   -30.1055]\n",
      "Weights: [-4.7618  0.5935 -1.0861  0.0739  0.1468]\n",
      "MSE loss: 92.3713\n",
      "Iteration: 39900\n",
      "Gradient: [  3.1434   2.9376   3.8435  76.8473 122.5273]\n",
      "Weights: [-4.7503  0.5999 -1.0876  0.0738  0.1469]\n",
      "MSE loss: 92.9035\n",
      "Iteration: 40000\n",
      "Gradient: [  5.1258  12.1829  -7.6095  69.332  235.0951]\n",
      "Weights: [-4.7844  0.6174 -1.0887  0.0735  0.1468]\n",
      "MSE loss: 92.6716\n",
      "Iteration: 40100\n",
      "Gradient: [  9.7016  -1.3732 -10.9106 -49.0425  32.8252]\n",
      "Weights: [-4.7794  0.6136 -1.0895  0.0729  0.1469]\n",
      "MSE loss: 92.4877\n",
      "Iteration: 40200\n",
      "Gradient: [  -2.9472    8.5425  -18.8503   42.5832 -347.4955]\n",
      "Weights: [-4.7719  0.6063 -1.0906  0.0727  0.1473]\n",
      "MSE loss: 92.4485\n",
      "Iteration: 40300\n",
      "Gradient: [ -3.1288 -22.6394  -0.2231  73.6588  -2.1781]\n",
      "Weights: [-4.7737  0.6097 -1.0926  0.0732  0.1473]\n",
      "MSE loss: 92.4113\n",
      "Iteration: 40400\n",
      "Gradient: [ 14.8145  -8.7121 -13.4546 -71.7587 -70.8398]\n",
      "Weights: [-4.7658  0.6011 -1.0914  0.0735  0.1473]\n",
      "MSE loss: 92.3686\n",
      "Iteration: 40500\n",
      "Gradient: [-12.2901  -7.5036  14.554   -0.4976 116.5244]\n",
      "Weights: [-4.7596  0.5961 -1.0915  0.0737  0.1473]\n",
      "MSE loss: 92.3612\n",
      "Iteration: 40600\n",
      "Gradient: [  7.2895  -3.6503  45.944  -47.0027 292.4347]\n",
      "Weights: [-4.7498  0.5919 -1.0911  0.0742  0.1474]\n",
      "MSE loss: 92.3899\n",
      "Iteration: 40700\n",
      "Gradient: [  3.771    2.1339  26.0229   7.8246 166.6977]\n",
      "Weights: [-4.7466  0.5948 -1.0902  0.0743  0.147 ]\n",
      "MSE loss: 92.5166\n",
      "Iteration: 40800\n",
      "Gradient: [  -8.8558    4.0299  -33.7128 -212.3768  143.6132]\n",
      "Weights: [-4.7701  0.5994 -1.0916  0.0747  0.1469]\n",
      "MSE loss: 92.5413\n",
      "Iteration: 40900\n",
      "Gradient: [-4.55000e-02  5.22980e+00  7.41480e+00  2.58180e+01  5.54545e+01]\n",
      "Weights: [-4.7665  0.6041 -1.0947  0.0751  0.1471]\n",
      "MSE loss: 92.3221\n",
      "Iteration: 41000\n",
      "Gradient: [  7.938   18.6384  39.4163  16.4711 160.2955]\n",
      "Weights: [-4.7484  0.5932 -1.0943  0.0757  0.1471]\n",
      "MSE loss: 92.3382\n",
      "Iteration: 41100\n",
      "Gradient: [  1.0545  -8.6495  -5.56    99.2769 -53.1976]\n",
      "Weights: [-4.7492  0.593  -1.0952  0.0758  0.1471]\n",
      "MSE loss: 92.3297\n",
      "Iteration: 41200\n",
      "Gradient: [ -4.8178  16.994   24.6672 -58.7693  -4.9124]\n",
      "Weights: [-4.7601  0.5986 -1.0959  0.0765  0.1468]\n",
      "MSE loss: 92.3051\n",
      "Iteration: 41300\n",
      "Gradient: [ 10.5317  12.1188  -1.5492  19.4199 234.3551]\n",
      "Weights: [-4.7553  0.5983 -1.0933  0.0758  0.1471]\n",
      "MSE loss: 92.5022\n",
      "Iteration: 41400\n",
      "Gradient: [  6.0305  12.0013  -3.4702 115.6384 347.8439]\n",
      "Weights: [-4.7388  0.5955 -1.0942  0.0758  0.1471]\n",
      "MSE loss: 92.9442\n",
      "Iteration: 41500\n",
      "Gradient: [  -1.1131    8.2822   32.6912   -2.114  -126.7839]\n",
      "Weights: [-4.7417  0.581  -1.0924  0.0759  0.1473]\n",
      "MSE loss: 92.3994\n",
      "Iteration: 41600\n",
      "Gradient: [  2.7256  -4.3014  11.7622 -15.4374 167.0259]\n",
      "Weights: [-4.7324  0.5826 -1.0957  0.0761  0.1473]\n",
      "MSE loss: 92.5213\n",
      "Iteration: 41700\n",
      "Gradient: [ -4.5924  -9.1886 -17.6516  31.4175 198.7418]\n",
      "Weights: [-4.7651  0.5919 -1.0935  0.076   0.1475]\n",
      "MSE loss: 92.5806\n",
      "Iteration: 41800\n",
      "Gradient: [ -1.0199   9.585  -17.7258  30.991   91.5894]\n",
      "Weights: [-4.7492  0.5826 -1.096   0.0767  0.1473]\n",
      "MSE loss: 92.5229\n",
      "Iteration: 41900\n",
      "Gradient: [ -3.6669 -13.045  -18.4929 -46.6741   1.172 ]\n",
      "Weights: [-4.7624  0.5941 -1.0967  0.0761  0.1475]\n",
      "MSE loss: 92.4552\n",
      "Iteration: 42000\n",
      "Gradient: [   0.3706   -1.5164  -11.5499  103.9152 -173.5796]\n",
      "Weights: [-4.7535  0.5975 -1.0976  0.075   0.1475]\n",
      "MSE loss: 92.3317\n",
      "Iteration: 42100\n",
      "Gradient: [  1.7366  -4.4782  19.1162  26.7127 -72.2187]\n",
      "Weights: [-4.7509  0.5969 -1.0959  0.0748  0.1475]\n",
      "MSE loss: 92.3167\n",
      "Iteration: 42200\n",
      "Gradient: [  7.5367  -5.5461  11.8776 -27.5182 -94.5004]\n",
      "Weights: [-4.7527  0.5977 -1.0965  0.0755  0.1474]\n",
      "MSE loss: 92.3341\n",
      "Iteration: 42300\n",
      "Gradient: [ 9.9426 -8.0105 14.3453 69.7251 89.6287]\n",
      "Weights: [-4.7476  0.6008 -1.1009  0.0764  0.1474]\n",
      "MSE loss: 92.356\n",
      "Iteration: 42400\n",
      "Gradient: [ -5.0638  -3.4269  38.3111  41.7335 244.8389]\n",
      "Weights: [-4.7568  0.6061 -1.1013  0.0766  0.1472]\n",
      "MSE loss: 92.2561\n",
      "Iteration: 42500\n",
      "Gradient: [ -7.4752   0.5583   2.5466   9.7895 -36.8242]\n",
      "Weights: [-4.7676  0.6077 -1.1028  0.077   0.1474]\n",
      "MSE loss: 92.3375\n",
      "Iteration: 42600\n",
      "Gradient: [  -4.3985    5.6093   -7.359   -70.4252 -132.0802]\n",
      "Weights: [-4.7626  0.6075 -1.1036  0.0767  0.1475]\n",
      "MSE loss: 92.2781\n",
      "Iteration: 42700\n",
      "Gradient: [ -2.7295  -2.7133   4.6861  24.1926 172.1708]\n",
      "Weights: [-4.7616  0.6072 -1.105   0.0773  0.1475]\n",
      "MSE loss: 92.2888\n",
      "Iteration: 42800\n",
      "Gradient: [  -6.7375    1.7411   -2.7754   77.3214 -136.1591]\n",
      "Weights: [-4.7611  0.6077 -1.1031  0.0765  0.1475]\n",
      "MSE loss: 92.2653\n",
      "Iteration: 42900\n",
      "Gradient: [ -3.4337 -12.5345 -11.1857 -97.8283 307.4959]\n",
      "Weights: [-4.7725  0.6057 -1.0999  0.0764  0.1473]\n",
      "MSE loss: 92.4954\n",
      "Iteration: 43000\n",
      "Gradient: [  4.1902  12.1888  -4.428  -76.482  -25.8056]\n",
      "Weights: [-4.7508  0.6048 -1.1017  0.0772  0.147 ]\n",
      "MSE loss: 92.3085\n",
      "Iteration: 43100\n",
      "Gradient: [  8.2513 -17.2061  -9.9253 110.3251 137.6569]\n",
      "Weights: [-4.7742  0.6135 -1.0988  0.0767  0.147 ]\n",
      "MSE loss: 92.4391\n",
      "Iteration: 43200\n",
      "Gradient: [  6.0046 -23.8634 -38.5508 -69.686  -69.6838]\n",
      "Weights: [-4.7636  0.6027 -1.1026  0.0767  0.1473]\n",
      "MSE loss: 92.5773\n",
      "Iteration: 43300\n",
      "Gradient: [  4.7488   8.3551   3.7428 -13.7874 184.0627]\n",
      "Weights: [-4.7507  0.6077 -1.1035  0.0772  0.1473]\n",
      "MSE loss: 92.4394\n",
      "Iteration: 43400\n",
      "Gradient: [ -1.3178   5.7602 -14.9959  -0.3667 -58.316 ]\n",
      "Weights: [-4.7753  0.6095 -1.1017  0.0775  0.147 ]\n",
      "MSE loss: 92.4899\n",
      "Iteration: 43500\n",
      "Gradient: [-11.814    5.2997 -40.7423 -48.1806  46.3623]\n",
      "Weights: [-4.761   0.6094 -1.1021  0.077   0.147 ]\n",
      "MSE loss: 92.2582\n",
      "Iteration: 43600\n",
      "Gradient: [ -5.9024  -2.3167 -11.8092 -87.8272  45.5056]\n",
      "Weights: [-4.7728  0.6136 -1.1012  0.0769  0.1471]\n",
      "MSE loss: 92.3418\n",
      "Iteration: 43700\n",
      "Gradient: [ -6.9719 -10.9861   1.6157  13.3172  28.5296]\n",
      "Weights: [-4.7529  0.5977 -1.0992  0.0769  0.1472]\n",
      "MSE loss: 92.2826\n",
      "Iteration: 43800\n",
      "Gradient: [ -2.371   29.1725  21.4306   9.054  190.9981]\n",
      "Weights: [-4.7552  0.5985 -1.0968  0.0768  0.1471]\n",
      "MSE loss: 92.4076\n",
      "Iteration: 43900\n",
      "Gradient: [   4.2094   -8.5047   20.8617  -62.5762 -373.4435]\n",
      "Weights: [-4.7489  0.5984 -1.0973  0.0765  0.1472]\n",
      "MSE loss: 92.5178\n",
      "Iteration: 44000\n",
      "Gradient: [ 1.4777 -2.9178  7.3958 12.8835 60.3022]\n",
      "Weights: [-4.7482  0.5861 -1.0956  0.0767  0.1472]\n",
      "MSE loss: 92.3601\n",
      "Iteration: 44100\n",
      "Gradient: [-10.9403   3.6108 -19.5669 -53.0849 -66.6648]\n",
      "Weights: [-4.7606  0.587  -1.0918  0.0764  0.1468]\n",
      "MSE loss: 92.5239\n",
      "Iteration: 44200\n",
      "Gradient: [  -1.1969   -7.833     5.279    62.8505 -261.4584]\n",
      "Weights: [-4.764   0.595  -1.0965  0.0766  0.147 ]\n",
      "MSE loss: 92.5565\n",
      "Iteration: 44300\n",
      "Gradient: [  -9.6205  -13.6011    5.5728   44.8487 -294.6616]\n",
      "Weights: [-4.7642  0.6079 -1.0987  0.0756  0.147 ]\n",
      "MSE loss: 92.3423\n",
      "Iteration: 44400\n",
      "Gradient: [   7.3984    6.0336  -31.7217  145.6593 -113.5354]\n",
      "Weights: [-4.7657  0.6087 -1.0962  0.0747  0.1475]\n",
      "MSE loss: 92.4457\n",
      "Iteration: 44500\n",
      "Gradient: [ -4.5898  -5.6569  23.3828 -84.0872 189.1872]\n",
      "Weights: [-4.7559  0.6073 -1.0971  0.0744  0.1475]\n",
      "MSE loss: 92.4064\n",
      "Iteration: 44600\n",
      "Gradient: [ -2.9207   0.3284  14.6674 -32.4075  58.4529]\n",
      "Weights: [-4.7572  0.5994 -1.0961  0.0749  0.1474]\n",
      "MSE loss: 92.2974\n",
      "Iteration: 44700\n",
      "Gradient: [ -7.2425   9.5287 -15.881  147.9727 101.7928]\n",
      "Weights: [-4.7606  0.6019 -1.0964  0.075   0.1473]\n",
      "MSE loss: 92.3059\n",
      "Iteration: 44800\n",
      "Gradient: [ -2.0393   7.4732  60.344  115.4547   5.9413]\n",
      "Weights: [-4.7584  0.6025 -1.0953  0.0755  0.1471]\n",
      "MSE loss: 92.3371\n",
      "Iteration: 44900\n",
      "Gradient: [  -4.3921  -11.5083  -47.4583    8.4655 -206.2063]\n",
      "Weights: [-4.765   0.6032 -1.0957  0.0745  0.1472]\n",
      "MSE loss: 92.4967\n",
      "Iteration: 45000\n",
      "Gradient: [-1.24518e+01  2.22000e-02  1.01903e+01 -3.52491e+01 -8.91326e+01]\n",
      "Weights: [-4.7667  0.6035 -1.0952  0.074   0.1475]\n",
      "MSE loss: 92.3779\n",
      "Iteration: 45100\n",
      "Gradient: [ -2.2337   1.2985  33.0382  15.9544 157.8091]\n",
      "Weights: [-4.7691  0.6175 -1.0968  0.0736  0.1475]\n",
      "MSE loss: 92.4182\n",
      "Iteration: 45200\n",
      "Gradient: [  -8.3447   -5.2256  -48.4652 -161.0065 -138.9816]\n",
      "Weights: [-4.7734  0.6095 -1.0983  0.0733  0.1476]\n",
      "MSE loss: 92.9767\n",
      "Iteration: 45300\n",
      "Gradient: [ -2.5291  -4.7068  16.6309 113.5534 295.1142]\n",
      "Weights: [-4.7597  0.6086 -1.0968  0.0734  0.1477]\n",
      "MSE loss: 92.3072\n",
      "Iteration: 45400\n",
      "Gradient: [  -7.6261  -18.6004  -14.8996   -5.7488 -225.4925]\n",
      "Weights: [-4.7633  0.6037 -1.0953  0.0731  0.1478]\n",
      "MSE loss: 92.3591\n",
      "Iteration: 45500\n",
      "Gradient: [  -4.7966   -7.1484   24.1141  -72.8214 -173.5966]\n",
      "Weights: [-4.7632  0.6062 -1.0919  0.0721  0.1474]\n",
      "MSE loss: 92.4322\n",
      "Iteration: 45600\n",
      "Gradient: [ 9.5979 -6.6986 37.1983 15.9759 63.5382]\n",
      "Weights: [-4.7623  0.6074 -1.0949  0.0727  0.1475]\n",
      "MSE loss: 92.4424\n",
      "Iteration: 45700\n",
      "Gradient: [ 3.438000e-01 -2.600000e-03 -2.430960e+01  4.756970e+01  1.903873e+02]\n",
      "Weights: [-4.767   0.6206 -1.0991  0.0736  0.1476]\n",
      "MSE loss: 92.3576\n",
      "Iteration: 45800\n",
      "Gradient: [   5.1388  -19.6693  -35.2751   28.9996 -179.6638]\n",
      "Weights: [-4.7771  0.626  -1.1024  0.0735  0.1476]\n",
      "MSE loss: 92.3779\n",
      "Iteration: 45900\n",
      "Gradient: [ -4.2164   8.3043 -28.9777 -58.6448 251.5887]\n",
      "Weights: [-4.7689  0.6222 -1.1031  0.0745  0.1474]\n",
      "MSE loss: 92.3212\n",
      "Iteration: 46000\n",
      "Gradient: [ -12.8104    1.1978    9.489   116.6915 -311.5086]\n",
      "Weights: [-4.7722  0.6292 -1.1052  0.0748  0.1476]\n",
      "MSE loss: 92.3028\n",
      "Iteration: 46100\n",
      "Gradient: [ -4.8682  -9.4536  -9.0659 -17.7077 -53.255 ]\n",
      "Weights: [-4.7633  0.6133 -1.1018  0.0743  0.1478]\n",
      "MSE loss: 92.2676\n",
      "Iteration: 46200\n",
      "Gradient: [ -8.5449 -10.5443  30.273   30.3789  70.3191]\n",
      "Weights: [-4.7722  0.619  -1.1008  0.0738  0.1478]\n",
      "MSE loss: 92.2922\n",
      "Iteration: 46300\n",
      "Gradient: [ -12.1559    2.613    -2.8909   68.6257 -139.8265]\n",
      "Weights: [-4.7773  0.6226 -1.1045  0.0744  0.1477]\n",
      "MSE loss: 92.434\n",
      "Iteration: 46400\n",
      "Gradient: [   4.8369   -1.4151  -21.6462   77.2586 -170.7911]\n",
      "Weights: [-4.7706  0.6189 -1.1026  0.0746  0.148 ]\n",
      "MSE loss: 92.3967\n",
      "Iteration: 46500\n",
      "Gradient: [ -5.4479  10.1663   8.0977 -87.6758 -61.7478]\n",
      "Weights: [-4.7605  0.6186 -1.1063  0.0747  0.148 ]\n",
      "MSE loss: 92.2456\n",
      "Iteration: 46600\n",
      "Gradient: [ 11.2527   1.2625 -31.4542 -67.8141   9.3219]\n",
      "Weights: [-4.7243  0.6035 -1.1057  0.0744  0.1482]\n",
      "MSE loss: 93.1353\n",
      "Iteration: 46700\n",
      "Gradient: [ -3.6132  -6.5978  43.3283  77.1038 101.6481]\n",
      "Weights: [-4.7614  0.6247 -1.1071  0.0748  0.148 ]\n",
      "MSE loss: 92.3486\n",
      "Iteration: 46800\n",
      "Gradient: [ -3.7745  -1.6139 -39.7253  19.7676 -56.2893]\n",
      "Weights: [-4.7542  0.614  -1.1057  0.0751  0.1479]\n",
      "MSE loss: 92.2843\n",
      "Iteration: 46900\n",
      "Gradient: [  -8.7359    4.9996  -79.5889   89.7682 -190.164 ]\n",
      "Weights: [-4.771   0.6128 -1.1047  0.0751  0.1481]\n",
      "MSE loss: 92.4141\n",
      "Iteration: 47000\n",
      "Gradient: [ -5.5038   9.7664   2.3714 -78.3226  -6.9263]\n",
      "Weights: [-4.7684  0.6225 -1.1043  0.0733  0.1481]\n",
      "MSE loss: 92.2823\n",
      "Iteration: 47100\n",
      "Gradient: [ 10.7908   5.7115  14.1827 102.3126 -25.0745]\n",
      "Weights: [-4.7583  0.6327 -1.1052  0.0727  0.1482]\n",
      "MSE loss: 92.9179\n",
      "Iteration: 47200\n",
      "Gradient: [ -10.3303    1.7902    9.3554 -123.8262   63.9593]\n",
      "Weights: [-4.7622  0.6193 -1.1045  0.0729  0.1482]\n",
      "MSE loss: 92.3484\n",
      "Iteration: 47300\n",
      "Gradient: [   3.3854    7.0989    5.1632   76.8029 -162.4282]\n",
      "Weights: [-4.7661  0.627  -1.1035  0.073   0.148 ]\n",
      "MSE loss: 92.3327\n",
      "Iteration: 47400\n",
      "Gradient: [  1.0565   2.4497  78.2871  44.6456 154.9137]\n",
      "Weights: [-4.769   0.6304 -1.1075  0.0733  0.1484]\n",
      "MSE loss: 92.3057\n",
      "Iteration: 47500\n",
      "Gradient: [  1.8012   1.9534 -23.7954  25.4794 143.7504]\n",
      "Weights: [-4.7791  0.6359 -1.1091  0.0737  0.1483]\n",
      "MSE loss: 92.2682\n",
      "Iteration: 47600\n",
      "Gradient: [  -2.068   -10.7723    2.5952  -22.0726 -253.4006]\n",
      "Weights: [-4.7635  0.6262 -1.1072  0.0724  0.1483]\n",
      "MSE loss: 92.3957\n",
      "Iteration: 47700\n",
      "Gradient: [ -6.1051   4.5496   6.3953  30.8595 354.1761]\n",
      "Weights: [-4.7683  0.6309 -1.1085  0.0734  0.1486]\n",
      "MSE loss: 92.3832\n",
      "Iteration: 47800\n",
      "Gradient: [  5.245   12.7109  -0.6614  68.8823 508.3102]\n",
      "Weights: [-4.7641  0.6295 -1.1103  0.074   0.1486]\n",
      "MSE loss: 92.4593\n",
      "Iteration: 47900\n",
      "Gradient: [  7.6085   7.9848  15.5339  38.9781 -41.1148]\n",
      "Weights: [-4.7616  0.6335 -1.109   0.0725  0.1487]\n",
      "MSE loss: 92.4804\n",
      "Iteration: 48000\n",
      "Gradient: [-3.2579  7.2501  0.2593 63.5681 79.801 ]\n",
      "Weights: [-4.7707  0.6489 -1.1181  0.073   0.1487]\n",
      "MSE loss: 92.2796\n",
      "Iteration: 48100\n",
      "Gradient: [ 8.20000e-03 -3.18350e+00  3.45981e+01 -5.12470e+00 -2.83796e+01]\n",
      "Weights: [-4.7702  0.6392 -1.1162  0.0736  0.1487]\n",
      "MSE loss: 92.2258\n",
      "Iteration: 48200\n",
      "Gradient: [   3.3763  -18.7304   36.8136 -121.5103 -345.8456]\n",
      "Weights: [-4.7843  0.6447 -1.1176  0.0741  0.1487]\n",
      "MSE loss: 92.3478\n",
      "Iteration: 48300\n",
      "Gradient: [  3.9165 -11.4475 -16.2371  24.3532 173.4817]\n",
      "Weights: [-4.7821  0.6679 -1.1242  0.0729  0.1489]\n",
      "MSE loss: 92.2906\n",
      "Iteration: 48400\n",
      "Gradient: [  -3.        0.886    30.5469  -44.3241 -333.6543]\n",
      "Weights: [-4.7886  0.6671 -1.1253  0.0727  0.1492]\n",
      "MSE loss: 92.2814\n",
      "Iteration: 48500\n",
      "Gradient: [ -4.4843   8.0572  31.8306  54.6807 125.6357]\n",
      "Weights: [-4.7862  0.6659 -1.1243  0.0728  0.1492]\n",
      "MSE loss: 92.226\n",
      "Iteration: 48600\n",
      "Gradient: [  -2.1069  -19.2904   16.3433   -6.3632 -262.1188]\n",
      "Weights: [-4.78    0.6571 -1.1258  0.0731  0.1493]\n",
      "MSE loss: 92.3936\n",
      "Iteration: 48700\n",
      "Gradient: [ -5.7433   8.1746  18.7671  -3.0356 229.306 ]\n",
      "Weights: [-4.7728  0.6405 -1.1219  0.0742  0.1494]\n",
      "MSE loss: 92.2704\n",
      "Iteration: 48800\n",
      "Gradient: [ -8.7688  -4.3963  16.7966  30.8544 -91.3415]\n",
      "Weights: [-4.7707  0.6339 -1.121   0.0745  0.1493]\n",
      "MSE loss: 92.433\n",
      "Iteration: 48900\n",
      "Gradient: [ -2.6568   6.1803 -31.822  -87.766   54.2517]\n",
      "Weights: [-4.7761  0.64   -1.1193  0.0741  0.1492]\n",
      "MSE loss: 92.2776\n",
      "Iteration: 49000\n",
      "Gradient: [  8.3309  -1.1048 -42.0984 -26.9727 -36.5615]\n",
      "Weights: [-4.7773  0.6458 -1.1174  0.0738  0.1488]\n",
      "MSE loss: 92.2067\n",
      "Iteration: 49100\n",
      "Gradient: [   3.7743   -3.0974   25.6998  135.2847 -206.4667]\n",
      "Weights: [-4.7654  0.6379 -1.1184  0.0746  0.1489]\n",
      "MSE loss: 92.2092\n",
      "Iteration: 49200\n",
      "Gradient: [  5.8373 -21.1784  10.8819  37.6184 -29.0516]\n",
      "Weights: [-4.7581  0.6264 -1.1187  0.0748  0.149 ]\n",
      "MSE loss: 92.3489\n",
      "Iteration: 49300\n",
      "Gradient: [ -7.3508 -10.6724  29.476   33.6402 129.7083]\n",
      "Weights: [-4.7595  0.6308 -1.1183  0.0747  0.149 ]\n",
      "MSE loss: 92.24\n",
      "Iteration: 49400\n",
      "Gradient: [  4.3403  12.5548  14.4676 -38.2275 274.1555]\n",
      "Weights: [-4.7657  0.6393 -1.1205  0.075   0.1492]\n",
      "MSE loss: 92.357\n",
      "Iteration: 49500\n",
      "Gradient: [   6.4384   -0.29    -27.1107 -114.287   -12.0264]\n",
      "Weights: [-4.7621  0.6368 -1.1193  0.0743  0.1488]\n",
      "MSE loss: 92.3166\n",
      "Iteration: 49600\n",
      "Gradient: [  -7.9768   16.9151   37.4335  -53.9055 -115.2416]\n",
      "Weights: [-4.7708  0.6429 -1.116   0.0735  0.1487]\n",
      "MSE loss: 92.2173\n",
      "Iteration: 49700\n",
      "Gradient: [ -1.0106 -15.4799 -53.0339 -75.1492 -69.6374]\n",
      "Weights: [-4.7783  0.6407 -1.119   0.0743  0.1489]\n",
      "MSE loss: 92.384\n",
      "Iteration: 49800\n",
      "Gradient: [-10.9113   3.2245 -40.0402 -50.2883  75.7058]\n",
      "Weights: [-4.7761  0.6413 -1.1208  0.0747  0.1489]\n",
      "MSE loss: 92.3267\n",
      "Iteration: 49900\n",
      "Gradient: [  8.2965  -1.8553  24.6253 -39.8894 -15.8709]\n",
      "Weights: [-4.7669  0.648  -1.1214  0.0742  0.149 ]\n",
      "MSE loss: 92.2979\n",
      "Iteration: 50000\n",
      "Gradient: [  -4.5096  -17.6464   13.1632  -66.9893 -311.0371]\n",
      "Weights: [-4.7762  0.6457 -1.1242  0.0748  0.149 ]\n",
      "MSE loss: 92.3725\n",
      "Iteration: 50100\n",
      "Gradient: [-10.7833  -1.3243 -48.9211  27.0143 190.2346]\n",
      "Weights: [-4.7729  0.6535 -1.1265  0.0749  0.149 ]\n",
      "MSE loss: 92.1783\n",
      "Iteration: 50200\n",
      "Gradient: [ -2.7147  -7.9195   4.552  -85.3137  52.7729]\n",
      "Weights: [-4.7869  0.6575 -1.1259  0.0752  0.1489]\n",
      "MSE loss: 92.258\n",
      "Iteration: 50300\n",
      "Gradient: [   8.3335   -2.0571   39.1988    0.8368 -373.296 ]\n",
      "Weights: [-4.7675  0.645  -1.1237  0.0757  0.149 ]\n",
      "MSE loss: 92.2321\n",
      "Iteration: 50400\n",
      "Gradient: [ -4.6399 -14.3632 -22.2065 -98.6793  85.1692]\n",
      "Weights: [-4.7673  0.6449 -1.1224  0.0749  0.1488]\n",
      "MSE loss: 92.2127\n",
      "Iteration: 50500\n",
      "Gradient: [   2.21      1.9266   10.3349   43.6458 -326.6198]\n",
      "Weights: [-4.7692  0.644  -1.1209  0.0752  0.1487]\n",
      "MSE loss: 92.1802\n",
      "Iteration: 50600\n",
      "Gradient: [   8.6568    7.1181  -20.7813   64.7304 -315.7306]\n",
      "Weights: [-4.7654  0.6347 -1.1186  0.0758  0.1486]\n",
      "MSE loss: 92.1866\n",
      "Iteration: 50700\n",
      "Gradient: [ 2.30000e-03  1.21088e+01  2.91156e+01  2.39437e+01 -8.70200e-01]\n",
      "Weights: [-4.7709  0.6283 -1.1173  0.0772  0.1483]\n",
      "MSE loss: 92.2694\n",
      "Iteration: 50800\n",
      "Gradient: [  3.8681 -15.0061   8.1413  15.4194  24.7569]\n",
      "Weights: [-4.7593  0.6283 -1.1198  0.0779  0.1481]\n",
      "MSE loss: 92.2071\n",
      "Iteration: 50900\n",
      "Gradient: [ -1.2755  -0.4296 -47.5896  39.2713 -60.1519]\n",
      "Weights: [-4.7679  0.6342 -1.1196  0.0776  0.1481]\n",
      "MSE loss: 92.1665\n",
      "Iteration: 51000\n",
      "Gradient: [  5.3438   5.5666  11.6822  21.5839 -81.9973]\n",
      "Weights: [-4.7634  0.6363 -1.1208  0.0776  0.148 ]\n",
      "MSE loss: 92.1713\n",
      "Iteration: 51100\n",
      "Gradient: [  -1.6369    6.2505   -5.4333  -91.3555 -107.2571]\n",
      "Weights: [-4.7697  0.6382 -1.1246  0.0779  0.1482]\n",
      "MSE loss: 92.2927\n",
      "Iteration: 51200\n",
      "Gradient: [-15.7387  -6.2109 -35.4217   0.82   121.8454]\n",
      "Weights: [-4.7872  0.637  -1.1256  0.0786  0.1483]\n",
      "MSE loss: 93.3172\n",
      "Iteration: 51300\n",
      "Gradient: [-15.0741   4.3824   0.3969 -89.0928 -97.9372]\n",
      "Weights: [-4.769   0.6347 -1.1239  0.0783  0.1484]\n",
      "MSE loss: 92.2282\n",
      "Iteration: 51400\n",
      "Gradient: [  -5.6799  -17.5467  -37.6682  -29.8735 -195.3659]\n",
      "Weights: [-4.767   0.6371 -1.1238  0.0775  0.1484]\n",
      "MSE loss: 92.2545\n",
      "Iteration: 51500\n",
      "Gradient: [ -3.5443   8.2485  13.4335  28.7732 163.8094]\n",
      "Weights: [-4.7756  0.6436 -1.1238  0.0785  0.1481]\n",
      "MSE loss: 92.1569\n",
      "Iteration: 51600\n",
      "Gradient: [ -0.8728  -9.6282 -39.025  -40.1229 172.2301]\n",
      "Weights: [-4.7593  0.6411 -1.1256  0.0779  0.1482]\n",
      "MSE loss: 92.2211\n",
      "Iteration: 51700\n",
      "Gradient: [   1.6473   11.7275  -11.1322  -25.7993 -244.8676]\n",
      "Weights: [-4.7675  0.647  -1.1258  0.0774  0.1482]\n",
      "MSE loss: 92.2102\n",
      "Iteration: 51800\n",
      "Gradient: [-2.0654  2.124  16.3249 50.4693 80.3672]\n",
      "Weights: [-4.7891  0.6591 -1.126   0.0782  0.1481]\n",
      "MSE loss: 92.3198\n",
      "Iteration: 51900\n",
      "Gradient: [ -1.718   -3.6572  19.0239 -56.9656  11.2179]\n",
      "Weights: [-4.7769  0.652  -1.1272  0.0784  0.1481]\n",
      "MSE loss: 92.1247\n",
      "Iteration: 52000\n",
      "Gradient: [  -0.6132  -17.1044  -23.2674 -137.0072 -441.8513]\n",
      "Weights: [-4.7765  0.6509 -1.127   0.0773  0.1481]\n",
      "MSE loss: 92.4081\n",
      "Iteration: 52100\n",
      "Gradient: [ 10.445   27.0839  37.2528 -60.3156  94.8796]\n",
      "Weights: [-4.786   0.6608 -1.1273  0.0781  0.148 ]\n",
      "MSE loss: 92.1683\n",
      "Iteration: 52200\n",
      "Gradient: [  -1.815   -19.2658  -20.6608  145.9819 -161.5529]\n",
      "Weights: [-4.7914  0.6631 -1.1289  0.0784  0.1479]\n",
      "MSE loss: 92.245\n",
      "Iteration: 52300\n",
      "Gradient: [ -4.3662 -11.7341 -23.8084 -55.8279  50.874 ]\n",
      "Weights: [-4.7902  0.6642 -1.1292  0.0775  0.1481]\n",
      "MSE loss: 92.3256\n",
      "Iteration: 52400\n",
      "Gradient: [ -5.4701  15.9441 -24.1239 -76.1296  52.4975]\n",
      "Weights: [-4.7993  0.6733 -1.1283  0.0766  0.1481]\n",
      "MSE loss: 92.3613\n",
      "Iteration: 52500\n",
      "Gradient: [  3.6127  14.8914  31.6575 143.3268  65.9217]\n",
      "Weights: [-4.7865  0.6706 -1.1294  0.0772  0.1483]\n",
      "MSE loss: 92.378\n",
      "Iteration: 52600\n",
      "Gradient: [  5.858   -1.6661  19.3475 -91.0976 -18.3056]\n",
      "Weights: [-4.7917  0.6724 -1.1309  0.0764  0.1485]\n",
      "MSE loss: 92.2063\n",
      "Iteration: 52700\n",
      "Gradient: [ -1.8788   7.2605 -22.8376  83.2049 -91.1353]\n",
      "Weights: [-4.7797  0.6653 -1.1305  0.0769  0.1486]\n",
      "MSE loss: 92.1689\n",
      "Iteration: 52800\n",
      "Gradient: [ 4.056200e+00  2.698900e+00 -1.475000e-01 -2.695850e+01  1.815428e+02]\n",
      "Weights: [-4.7833  0.6565 -1.1333  0.0781  0.1486]\n",
      "MSE loss: 92.513\n",
      "Iteration: 52900\n",
      "Gradient: [ 8.564   1.8245 25.19   10.678  16.1308]\n",
      "Weights: [-4.7711  0.6548 -1.1311  0.0782  0.1486]\n",
      "MSE loss: 92.173\n",
      "Iteration: 53000\n",
      "Gradient: [  0.523  -10.282   19.9923 -26.3269 -45.0497]\n",
      "Weights: [-4.7703  0.6545 -1.1317  0.0781  0.1484]\n",
      "MSE loss: 92.1574\n",
      "Iteration: 53100\n",
      "Gradient: [   3.1892   -9.1749  -14.0042  -21.3614 -337.9003]\n",
      "Weights: [-4.766   0.6578 -1.1315  0.0774  0.1486]\n",
      "MSE loss: 92.2563\n",
      "Iteration: 53200\n",
      "Gradient: [  0.1548 -10.5079 -11.5608 -41.4578 -69.1819]\n",
      "Weights: [-4.768   0.6615 -1.134   0.0774  0.1486]\n",
      "MSE loss: 92.2115\n",
      "Iteration: 53300\n",
      "Gradient: [ -0.2652   0.203   10.9869   0.6793 143.7076]\n",
      "Weights: [-4.7807  0.6565 -1.1328  0.0786  0.1488]\n",
      "MSE loss: 92.2337\n",
      "Iteration: 53400\n",
      "Gradient: [  9.3534 -21.8029  49.429   70.0678   5.2809]\n",
      "Weights: [-4.7748  0.6598 -1.1357  0.079   0.1487]\n",
      "MSE loss: 92.1309\n",
      "Iteration: 53500\n",
      "Gradient: [   8.5032   -9.3473   16.707   -86.9456 -227.2784]\n",
      "Weights: [-4.7728  0.654  -1.1348  0.0789  0.1485]\n",
      "MSE loss: 92.2453\n",
      "Iteration: 53600\n",
      "Gradient: [   0.626     8.5817    7.1525 -109.7701  159.823 ]\n",
      "Weights: [-4.7725  0.6582 -1.1329  0.079   0.1483]\n",
      "MSE loss: 92.1156\n",
      "Iteration: 53700\n",
      "Gradient: [-1.490000e-01  8.756700e+00 -3.475150e+01 -4.351540e+01  1.888159e+02]\n",
      "Weights: [-4.7767  0.653  -1.1344  0.0803  0.1484]\n",
      "MSE loss: 92.1394\n",
      "Iteration: 53800\n",
      "Gradient: [-11.7751 -10.6122 -33.3747  -1.5068 105.0992]\n",
      "Weights: [-4.7916  0.6604 -1.1349  0.0801  0.1481]\n",
      "MSE loss: 92.4822\n",
      "Iteration: 53900\n",
      "Gradient: [ -2.4392  -0.6343  -8.1591  87.7231 365.8384]\n",
      "Weights: [-4.7795  0.6582 -1.1367  0.0814  0.148 ]\n",
      "MSE loss: 92.1017\n",
      "Iteration: 54000\n",
      "Gradient: [  0.1059   4.3852 -28.1404  81.0685 -14.4703]\n",
      "Weights: [-4.7834  0.664  -1.137   0.0822  0.1479]\n",
      "MSE loss: 92.2581\n",
      "Iteration: 54100\n",
      "Gradient: [  5.5336  17.8732  -9.7751 -52.9603 402.6243]\n",
      "Weights: [-4.7735  0.6533 -1.1351  0.0826  0.1476]\n",
      "MSE loss: 92.0619\n",
      "Iteration: 54200\n",
      "Gradient: [ -0.2703   4.0669   3.2985 -38.6079 -21.132 ]\n",
      "Weights: [-4.7702  0.6466 -1.1351  0.0835  0.1476]\n",
      "MSE loss: 92.1315\n",
      "Iteration: 54300\n",
      "Gradient: [  -6.5771  -17.6535   -9.4505  -12.7894 -284.9499]\n",
      "Weights: [-4.7815  0.6561 -1.1327  0.0826  0.1474]\n",
      "MSE loss: 92.1796\n",
      "Iteration: 54400\n",
      "Gradient: [  -2.1296  -17.4444   16.1044 -109.392   132.1181]\n",
      "Weights: [-4.7682  0.6495 -1.1351  0.0823  0.1475]\n",
      "MSE loss: 92.1508\n",
      "Iteration: 54500\n",
      "Gradient: [  6.9842   4.7693   4.7542 -53.9846 -22.9913]\n",
      "Weights: [-4.7651  0.6473 -1.1352  0.0821  0.1479]\n",
      "MSE loss: 92.1064\n",
      "Iteration: 54600\n",
      "Gradient: [  -3.6827    5.2386  -31.3943  -33.9565 -328.6488]\n",
      "Weights: [-4.7599  0.6457 -1.1381  0.0822  0.1481]\n",
      "MSE loss: 92.1702\n",
      "Iteration: 54700\n",
      "Gradient: [  -4.9483    0.2318  -19.6866   54.3666 -178.1666]\n",
      "Weights: [-4.7662  0.6471 -1.1378  0.083   0.148 ]\n",
      "MSE loss: 92.109\n",
      "Iteration: 54800\n",
      "Gradient: [  9.7483   3.6603  -6.2504 -79.9818 -88.718 ]\n",
      "Weights: [-4.7638  0.6448 -1.1351  0.0816  0.1481]\n",
      "MSE loss: 92.1244\n",
      "Iteration: 54900\n",
      "Gradient: [   8.964   -18.5412    8.5888  -66.7326 -219.3881]\n",
      "Weights: [-4.7614  0.6509 -1.1362  0.0815  0.1479]\n",
      "MSE loss: 92.1538\n",
      "Iteration: 55000\n",
      "Gradient: [  6.4552 -13.2483 -13.1317 -12.9227  47.9939]\n",
      "Weights: [-4.7639  0.6485 -1.1355  0.0814  0.1481]\n",
      "MSE loss: 92.1229\n",
      "Iteration: 55100\n",
      "Gradient: [  1.2262   7.4355 -36.4474  10.1734 202.2991]\n",
      "Weights: [-4.7659  0.6464 -1.1335  0.0813  0.1479]\n",
      "MSE loss: 92.1048\n",
      "Iteration: 55200\n",
      "Gradient: [  -8.0097   -5.0166   29.351    15.6923 -148.2481]\n",
      "Weights: [-4.7726  0.644  -1.1308  0.0811  0.1479]\n",
      "MSE loss: 92.1374\n",
      "Iteration: 55300\n",
      "Gradient: [ 2.0784 22.5683 17.5987 -9.6995 32.2331]\n",
      "Weights: [-4.768   0.6457 -1.1318  0.081   0.1478]\n",
      "MSE loss: 92.1322\n",
      "Iteration: 55400\n",
      "Gradient: [  9.4271   9.7611 -14.1585  20.3142 -39.1994]\n",
      "Weights: [-4.7808  0.6606 -1.1341  0.0805  0.1478]\n",
      "MSE loss: 92.1051\n",
      "Iteration: 55500\n",
      "Gradient: [  -5.8488    6.963   -14.027   -50.3613 -179.5368]\n",
      "Weights: [-4.7936  0.6544 -1.134   0.0803  0.1483]\n",
      "MSE loss: 92.7883\n",
      "Iteration: 55600\n",
      "Gradient: [ 4.7251 -9.0142 51.1952 15.5503 -6.7622]\n",
      "Weights: [-4.7729  0.653  -1.1324  0.0793  0.1484]\n",
      "MSE loss: 92.1173\n",
      "Iteration: 55700\n",
      "Gradient: [ 5.8818 12.1288 24.4129 31.4605 60.7065]\n",
      "Weights: [-4.7729  0.6436 -1.1328  0.0798  0.1487]\n",
      "MSE loss: 92.2885\n",
      "Iteration: 55800\n",
      "Gradient: [  7.0002   7.8381 -49.4093   7.749  -71.1752]\n",
      "Weights: [-4.7734  0.6531 -1.1324  0.0793  0.1483]\n",
      "MSE loss: 92.1099\n",
      "Iteration: 55900\n",
      "Gradient: [  2.3824   6.7348   8.5685 107.6378 298.0419]\n",
      "Weights: [-4.7473  0.6463 -1.134   0.0798  0.1486]\n",
      "MSE loss: 92.7656\n",
      "Iteration: 56000\n",
      "Gradient: [ -7.3456  20.0354  15.1844  61.268  307.9796]\n",
      "Weights: [-4.7639  0.6544 -1.1349  0.0802  0.1484]\n",
      "MSE loss: 92.2668\n",
      "Iteration: 56100\n",
      "Gradient: [-10.0057  -0.3391 -30.6762 -25.7234 -61.1783]\n",
      "Weights: [-4.7786  0.6526 -1.1335  0.0799  0.1482]\n",
      "MSE loss: 92.2808\n",
      "Iteration: 56200\n",
      "Gradient: [  -1.1925   -5.8052  -22.7588  -81.926  -136.4079]\n",
      "Weights: [-4.7737  0.6558 -1.1328  0.0801  0.148 ]\n",
      "MSE loss: 92.091\n",
      "Iteration: 56300\n",
      "Gradient: [-16.6957  -5.1217 -16.7511 -31.0095 -50.28  ]\n",
      "Weights: [-4.7894  0.6678 -1.1325  0.0797  0.1478]\n",
      "MSE loss: 92.1525\n",
      "Iteration: 56400\n",
      "Gradient: [  -7.8944   13.0577  -29.0738 -132.9224 -119.2231]\n",
      "Weights: [-4.7682  0.6533 -1.1335  0.0803  0.1477]\n",
      "MSE loss: 92.2789\n",
      "Iteration: 56500\n",
      "Gradient: [ -2.4485   2.2669  43.7361   3.4377 -61.3325]\n",
      "Weights: [-4.771   0.655  -1.1324  0.0804  0.1479]\n",
      "MSE loss: 92.1347\n",
      "Iteration: 56600\n",
      "Gradient: [18.1815 25.9161  7.0533 23.8266 58.561 ]\n",
      "Weights: [-4.7688  0.6535 -1.1321  0.0805  0.148 ]\n",
      "MSE loss: 92.2445\n",
      "Iteration: 56700\n",
      "Gradient: [ -3.2973   7.5531 -23.3011 -65.3508  90.6033]\n",
      "Weights: [-4.7597  0.6439 -1.1326  0.0801  0.1481]\n",
      "MSE loss: 92.2402\n",
      "Iteration: 56800\n",
      "Gradient: [ -4.5142  -1.9925 -62.4474  41.8738 141.7152]\n",
      "Weights: [-4.7748  0.645  -1.1291  0.0807  0.1479]\n",
      "MSE loss: 92.1392\n",
      "Iteration: 56900\n",
      "Gradient: [  3.4591  -3.6724  20.5754  79.9312 205.7817]\n",
      "Weights: [-4.768   0.6509 -1.1296  0.08    0.1479]\n",
      "MSE loss: 92.2005\n",
      "Iteration: 57000\n",
      "Gradient: [   6.1429    4.0236   37.2281   54.566  -227.5196]\n",
      "Weights: [-4.7849  0.6568 -1.1289  0.0799  0.1478]\n",
      "MSE loss: 92.1795\n",
      "Iteration: 57100\n",
      "Gradient: [-12.3746  -4.702  -29.7726  92.8694 -39.0617]\n",
      "Weights: [-4.7762  0.6523 -1.1293  0.08    0.1478]\n",
      "MSE loss: 92.1015\n",
      "Iteration: 57200\n",
      "Gradient: [ -2.9375  -7.1267   6.432   36.2831 -40.1575]\n",
      "Weights: [-4.7696  0.6539 -1.1308  0.0802  0.1477]\n",
      "MSE loss: 92.1354\n",
      "Iteration: 57300\n",
      "Gradient: [  2.7108 -13.5572  46.5539 -52.8888 220.4203]\n",
      "Weights: [-4.7897  0.6591 -1.1319  0.0806  0.1478]\n",
      "MSE loss: 92.2229\n",
      "Iteration: 57400\n",
      "Gradient: [  2.7337   8.8599 -17.9073 -99.3717  88.2017]\n",
      "Weights: [-4.7916  0.6637 -1.1338  0.0808  0.1478]\n",
      "MSE loss: 92.2085\n",
      "Iteration: 57500\n",
      "Gradient: [ -1.9086  -1.5724  -1.6441 -31.4424 -65.8627]\n",
      "Weights: [-4.7963  0.6698 -1.1335  0.0804  0.1478]\n",
      "MSE loss: 92.2446\n",
      "Iteration: 57600\n",
      "Gradient: [  7.9138 -10.3481 -15.2727 208.9486 296.541 ]\n",
      "Weights: [-4.7623  0.6561 -1.1336  0.0809  0.1477]\n",
      "MSE loss: 92.3301\n",
      "Iteration: 57700\n",
      "Gradient: [  -2.5276  -11.7097  -40.3575 -107.4028   96.9289]\n",
      "Weights: [-4.7906  0.6577 -1.1328  0.0808  0.1478]\n",
      "MSE loss: 92.3476\n",
      "Iteration: 57800\n",
      "Gradient: [ -6.8371  -4.3706  -6.6906   4.5552 197.7764]\n",
      "Weights: [-4.7678  0.6511 -1.1327  0.0812  0.1477]\n",
      "MSE loss: 92.0989\n",
      "Iteration: 57900\n",
      "Gradient: [  -0.456    -9.4209   -2.8155  -64.7283 -278.7503]\n",
      "Weights: [-4.7966  0.659  -1.1326  0.0803  0.148 ]\n",
      "MSE loss: 92.5768\n",
      "Iteration: 58000\n",
      "Gradient: [   6.7817   14.3572  -17.5024   50.2206 -102.3347]\n",
      "Weights: [-4.7694  0.6538 -1.1326  0.0803  0.1479]\n",
      "MSE loss: 92.1073\n",
      "Iteration: 58100\n",
      "Gradient: [ 4.191  25.7138 -0.6577 -9.1925 35.2295]\n",
      "Weights: [-4.7763  0.6482 -1.1312  0.0808  0.1479]\n",
      "MSE loss: 92.156\n",
      "Iteration: 58200\n",
      "Gradient: [  8.7843 -29.0414   9.885  -52.5712 367.1977]\n",
      "Weights: [-4.7807  0.6628 -1.1321  0.0811  0.1477]\n",
      "MSE loss: 92.4035\n",
      "Iteration: 58300\n",
      "Gradient: [ -3.6479 -17.9302  -6.124  -27.3634  96.7225]\n",
      "Weights: [-4.7812  0.6562 -1.132   0.0816  0.1474]\n",
      "MSE loss: 92.1024\n",
      "Iteration: 58400\n",
      "Gradient: [ -1.4421 -17.4379  -6.2014  15.9818   7.5202]\n",
      "Weights: [-4.7802  0.6624 -1.1336  0.081   0.1474]\n",
      "MSE loss: 92.1098\n",
      "Iteration: 58500\n",
      "Gradient: [  -6.5121    4.0461  -10.6544   25.2071 -318.6445]\n",
      "Weights: [-4.7842  0.6737 -1.1334  0.0807  0.1474]\n",
      "MSE loss: 92.3726\n",
      "Iteration: 58600\n",
      "Gradient: [  4.6799   4.7525  11.5154  -7.6653 129.1827]\n",
      "Weights: [-4.7926  0.6718 -1.1344  0.0808  0.1473]\n",
      "MSE loss: 92.24\n",
      "Iteration: 58700\n",
      "Gradient: [  -4.2836    2.444     9.5463   23.2239 -182.2081]\n",
      "Weights: [-4.8017  0.6833 -1.1359  0.0802  0.1474]\n",
      "MSE loss: 92.2818\n",
      "Iteration: 58800\n",
      "Gradient: [-1.432800e+00  8.543200e+00  2.814000e-01 -2.341430e+01  4.398796e+02]\n",
      "Weights: [-4.7996  0.6874 -1.1375  0.0805  0.1475]\n",
      "MSE loss: 92.3306\n",
      "Iteration: 58900\n",
      "Gradient: [  -9.6854    6.4947   42.9635   57.7664 -209.6365]\n",
      "Weights: [-4.7954  0.6821 -1.1368  0.0809  0.1474]\n",
      "MSE loss: 92.2392\n",
      "Iteration: 59000\n",
      "Gradient: [  -4.6268   -3.5884    8.2111  -38.6587 -238.7737]\n",
      "Weights: [-4.8095  0.6779 -1.1363  0.0803  0.1477]\n",
      "MSE loss: 92.7003\n",
      "Iteration: 59100\n",
      "Gradient: [  6.5809  11.4982  14.9177  84.184  -24.177 ]\n",
      "Weights: [-4.7895  0.6674 -1.1345  0.0807  0.1476]\n",
      "MSE loss: 92.167\n",
      "Iteration: 59200\n",
      "Gradient: [ 11.6787 -18.7706  44.3804  25.5931 140.6996]\n",
      "Weights: [-4.7801  0.6608 -1.1348  0.0817  0.1475]\n",
      "MSE loss: 92.0731\n",
      "Iteration: 59300\n",
      "Gradient: [-13.5615 -13.2147  13.3793 136.6798 320.0997]\n",
      "Weights: [-4.7792  0.6664 -1.1376  0.0821  0.1474]\n",
      "MSE loss: 92.0838\n",
      "Iteration: 59400\n",
      "Gradient: [-10.435   -1.4383   9.91    20.6784 118.9618]\n",
      "Weights: [-4.7813  0.6627 -1.1361  0.0821  0.1474]\n",
      "MSE loss: 92.087\n",
      "Iteration: 59500\n",
      "Gradient: [  8.3316  -3.2963 -11.2912 -15.0531 240.7995]\n",
      "Weights: [-4.7649  0.6533 -1.137   0.0825  0.1476]\n",
      "MSE loss: 92.1023\n",
      "Iteration: 59600\n",
      "Gradient: [   4.2121    3.5182   23.5422  125.6919 -100.889 ]\n",
      "Weights: [-4.7667  0.6528 -1.1353  0.0821  0.1477]\n",
      "MSE loss: 92.1123\n",
      "Iteration: 59700\n",
      "Gradient: [ -19.8518    1.8185  -23.383   -42.2147 -398.8885]\n",
      "Weights: [-4.7949  0.6625 -1.136   0.0821  0.1476]\n",
      "MSE loss: 92.4208\n",
      "Iteration: 59800\n",
      "Gradient: [  3.0566 -13.0227   7.9743  90.6567 118.6129]\n",
      "Weights: [-4.7593  0.6526 -1.1384  0.0831  0.1477]\n",
      "MSE loss: 92.2165\n",
      "Iteration: 59900\n",
      "Gradient: [ -1.9595  -2.9453 -28.8048 -28.4205 -28.5878]\n",
      "Weights: [-4.7673  0.651  -1.1371  0.0833  0.1476]\n",
      "MSE loss: 92.0741\n",
      "Iteration: 60000\n",
      "Gradient: [ 12.1111  12.7134  18.1715  93.3562 -23.1085]\n",
      "Weights: [-4.7543  0.6346 -1.1352  0.0842  0.1477]\n",
      "MSE loss: 92.2015\n",
      "Iteration: 60100\n",
      "Gradient: [   6.4469  -23.4423  -49.9872   18.1421 -383.1002]\n",
      "Weights: [-4.7665  0.6366 -1.1348  0.0839  0.1475]\n",
      "MSE loss: 92.2747\n",
      "Iteration: 60200\n",
      "Gradient: [  -4.3254   -3.7908   31.1363   97.6791 -300.4472]\n",
      "Weights: [-4.7656  0.643  -1.1349  0.0844  0.1471]\n",
      "MSE loss: 92.0632\n",
      "Iteration: 60300\n",
      "Gradient: [-11.2636   4.2697 -47.2351   9.0431 105.2434]\n",
      "Weights: [-4.7566  0.6385 -1.1341  0.0849  0.1469]\n",
      "MSE loss: 92.1223\n",
      "Iteration: 60400\n",
      "Gradient: [ -2.8317   3.3531  13.1219 -37.0426 162.0359]\n",
      "Weights: [-4.7661  0.6491 -1.137   0.0853  0.1468]\n",
      "MSE loss: 92.0461\n",
      "Iteration: 60500\n",
      "Gradient: [ 0.9701  9.0256 86.4637  0.6498 15.3858]\n",
      "Weights: [-4.7758  0.6603 -1.1398  0.0856  0.1467]\n",
      "MSE loss: 92.022\n",
      "Iteration: 60600\n",
      "Gradient: [ 10.7421   1.6227   5.2417  38.6072 125.9417]\n",
      "Weights: [-4.7632  0.6485 -1.1404  0.0864  0.1469]\n",
      "MSE loss: 92.0397\n",
      "Iteration: 60700\n",
      "Gradient: [   5.5395   21.7778   44.9335  -22.9149 -104.77  ]\n",
      "Weights: [-4.7785  0.6615 -1.141   0.0859  0.1467]\n",
      "MSE loss: 92.0094\n",
      "Iteration: 60800\n",
      "Gradient: [ -2.8016   5.8822  -2.3778 -36.2809  69.2773]\n",
      "Weights: [-4.7805  0.6597 -1.1396  0.0853  0.1468]\n",
      "MSE loss: 92.0808\n",
      "Iteration: 60900\n",
      "Gradient: [  3.8101   2.6383 -34.2832 -39.8942 143.1583]\n",
      "Weights: [-4.7829  0.663  -1.1409  0.086   0.1465]\n",
      "MSE loss: 92.1276\n",
      "Iteration: 61000\n",
      "Gradient: [  4.0707  -0.458   16.343   -2.3154 180.6324]\n",
      "Weights: [-4.7709  0.6588 -1.1406  0.0863  0.1465]\n",
      "MSE loss: 92.0365\n",
      "Iteration: 61100\n",
      "Gradient: [ 10.809  -16.4901  -2.6168   7.9326 316.0434]\n",
      "Weights: [-4.7759  0.6593 -1.138   0.0863  0.1466]\n",
      "MSE loss: 92.2138\n",
      "Iteration: 61200\n",
      "Gradient: [  5.392   19.7167  18.4869 103.1416 201.6946]\n",
      "Weights: [-4.7688  0.6512 -1.1406  0.0875  0.1465]\n",
      "MSE loss: 91.9989\n",
      "Iteration: 61300\n",
      "Gradient: [  0.9853  -1.8343 -20.4441 -23.453   41.9644]\n",
      "Weights: [-4.7735  0.6504 -1.1372  0.0868  0.1464]\n",
      "MSE loss: 92.0118\n",
      "Iteration: 61400\n",
      "Gradient: [ -8.3125  -2.9719 -20.7484   5.8699 -16.2702]\n",
      "Weights: [-4.7746  0.6553 -1.1371  0.0867  0.1461]\n",
      "MSE loss: 92.0267\n",
      "Iteration: 61500\n",
      "Gradient: [  -2.4742   -4.5869  -41.7914   43.9821 -102.4593]\n",
      "Weights: [-4.769   0.6462 -1.136   0.0876  0.1461]\n",
      "MSE loss: 92.011\n",
      "Iteration: 61600\n",
      "Gradient: [ 12.4461  16.5046   3.9366  74.4342 370.2411]\n",
      "Weights: [-4.773   0.6486 -1.1334  0.0871  0.1461]\n",
      "MSE loss: 92.1528\n",
      "Iteration: 61700\n",
      "Gradient: [ -9.8841   8.7978  27.9063  46.4063 -64.8463]\n",
      "Weights: [-4.7796  0.6528 -1.1346  0.0865  0.1457]\n",
      "MSE loss: 92.2774\n",
      "Iteration: 61800\n",
      "Gradient: [ -1.1499 -15.0559   5.4115   8.4257 114.4652]\n",
      "Weights: [-4.7866  0.6572 -1.1344  0.0867  0.1457]\n",
      "MSE loss: 92.1716\n",
      "Iteration: 61900\n",
      "Gradient: [   0.8845  -24.4389   21.024    15.7089 -195.3472]\n",
      "Weights: [-4.7673  0.6465 -1.1328  0.0861  0.146 ]\n",
      "MSE loss: 92.0991\n",
      "Iteration: 62000\n",
      "Gradient: [   6.0228    4.2352    2.3621    4.9668 -132.0468]\n",
      "Weights: [-4.7661  0.6335 -1.13    0.087   0.1461]\n",
      "MSE loss: 92.051\n",
      "Iteration: 62100\n",
      "Gradient: [  3.2402  11.1904 -29.3849  42.2436 292.1388]\n",
      "Weights: [-4.7504  0.6242 -1.1291  0.087   0.1459]\n",
      "MSE loss: 92.2347\n",
      "Iteration: 62200\n",
      "Gradient: [  7.737   -3.2177   7.5065  43.6387 190.1153]\n",
      "Weights: [-4.7667  0.6331 -1.1283  0.087   0.1461]\n",
      "MSE loss: 92.1868\n",
      "Iteration: 62300\n",
      "Gradient: [   1.1129   10.321    15.1115  -29.3985 -200.8994]\n",
      "Weights: [-4.7848  0.6431 -1.1302  0.087   0.1459]\n",
      "MSE loss: 92.2239\n",
      "Iteration: 62400\n",
      "Gradient: [  -1.4637  -20.0002  -22.8049   93.6414 -176.5288]\n",
      "Weights: [-4.765   0.6318 -1.1323  0.0874  0.146 ]\n",
      "MSE loss: 92.2727\n",
      "Iteration: 62500\n",
      "Gradient: [  -5.7838    6.4965    1.9103   10.4957 -185.2668]\n",
      "Weights: [-4.771   0.6397 -1.1346  0.0872  0.1462]\n",
      "MSE loss: 92.1483\n",
      "Iteration: 62600\n",
      "Gradient: [  0.919  -20.5868  23.523  -17.743   99.3869]\n",
      "Weights: [-4.7583  0.6375 -1.1364  0.0877  0.1463]\n",
      "MSE loss: 92.0571\n",
      "Iteration: 62700\n",
      "Gradient: [ -4.5131  -4.6197  33.5074 -29.233  236.0705]\n",
      "Weights: [-4.759   0.6365 -1.1384  0.0879  0.1465]\n",
      "MSE loss: 92.0792\n",
      "Iteration: 62800\n",
      "Gradient: [  -4.1357   -9.7718   24.0181   42.5219 -124.7158]\n",
      "Weights: [-4.7766  0.6504 -1.1372  0.0873  0.1464]\n",
      "MSE loss: 92.0469\n",
      "Iteration: 62900\n",
      "Gradient: [-5.7391  4.2484  7.0269 77.1107 47.0592]\n",
      "Weights: [-4.7726  0.6451 -1.1352  0.0873  0.1464]\n",
      "MSE loss: 92.0991\n",
      "Iteration: 63000\n",
      "Gradient: [ -4.4073   7.9496 -39.83   -65.9096 273.3926]\n",
      "Weights: [-4.781   0.6512 -1.1363  0.0873  0.1463]\n",
      "MSE loss: 92.1065\n",
      "Iteration: 63100\n",
      "Gradient: [ -6.9393   8.4928  28.986  -13.2548 100.2968]\n",
      "Weights: [-4.777   0.6438 -1.1362  0.0877  0.1465]\n",
      "MSE loss: 92.1432\n",
      "Iteration: 63200\n",
      "Gradient: [ -12.2049   -2.3841  -45.0369 -147.4012  189.1567]\n",
      "Weights: [-4.7812  0.6548 -1.1376  0.0866  0.1463]\n",
      "MSE loss: 92.0787\n",
      "Iteration: 63300\n",
      "Gradient: [ -3.5214   5.7352  43.1625 116.882  172.2691]\n",
      "Weights: [-4.7804  0.6514 -1.1361  0.087   0.1463]\n",
      "MSE loss: 92.0606\n",
      "Iteration: 63400\n",
      "Gradient: [ 15.6136   5.8855  34.1464  34.757  392.1526]\n",
      "Weights: [-4.7505  0.6429 -1.1359  0.0867  0.1463]\n",
      "MSE loss: 92.3935\n",
      "Iteration: 63500\n",
      "Gradient: [-4.9951  6.2633 45.9133 90.4923 81.0034]\n",
      "Weights: [-4.7638  0.6428 -1.1365  0.0867  0.1464]\n",
      "MSE loss: 92.0549\n",
      "Iteration: 63600\n",
      "Gradient: [  3.6071   0.949   24.1414   0.5244 -63.1383]\n",
      "Weights: [-4.7764  0.6525 -1.1342  0.0864  0.1463]\n",
      "MSE loss: 92.1642\n",
      "Iteration: 63700\n",
      "Gradient: [  0.8098  13.601  -17.6949 -47.2787 111.8384]\n",
      "Weights: [-4.7711  0.6456 -1.135   0.0865  0.1465]\n",
      "MSE loss: 92.0329\n",
      "Iteration: 63800\n",
      "Gradient: [ -2.4519  18.831   -3.3545   4.4036 -38.6824]\n",
      "Weights: [-4.7727  0.6452 -1.135   0.0867  0.1466]\n",
      "MSE loss: 92.1023\n",
      "Iteration: 63900\n",
      "Gradient: [ -1.3609  -9.8326  18.8982 106.6999 -30.453 ]\n",
      "Weights: [-4.7755  0.6497 -1.136   0.0865  0.1466]\n",
      "MSE loss: 92.0701\n",
      "Iteration: 64000\n",
      "Gradient: [  3.479   -3.8382 -17.7452 177.0796 -17.5368]\n",
      "Weights: [-4.7863  0.6588 -1.1362  0.0863  0.1464]\n",
      "MSE loss: 92.1229\n",
      "Iteration: 64100\n",
      "Gradient: [ -13.7209  -11.3048   11.6181  -32.8135 -198.0654]\n",
      "Weights: [-4.7897  0.6616 -1.1377  0.0866  0.1463]\n",
      "MSE loss: 92.1315\n",
      "Iteration: 64200\n",
      "Gradient: [ -4.9199  -5.122    7.5667  43.1354 -25.6107]\n",
      "Weights: [-4.7839  0.6598 -1.1362  0.0861  0.1461]\n",
      "MSE loss: 92.0865\n",
      "Iteration: 64300\n",
      "Gradient: [ 1.060000e-01 -6.628500e+00 -3.746240e+01  6.539900e+00 -1.661412e+02]\n",
      "Weights: [-4.7956  0.6642 -1.135   0.085   0.1463]\n",
      "MSE loss: 92.2768\n",
      "Iteration: 64400\n",
      "Gradient: [ -2.1051   6.4347 -13.0047  39.0623 149.3499]\n",
      "Weights: [-4.7702  0.6451 -1.132   0.085   0.1466]\n",
      "MSE loss: 92.0491\n",
      "Iteration: 64500\n",
      "Gradient: [-12.3732   8.2086 -23.0273 -18.1817 211.7835]\n",
      "Weights: [-4.7694  0.6423 -1.1303  0.0858  0.1464]\n",
      "MSE loss: 92.1659\n",
      "Iteration: 64600\n",
      "Gradient: [  -4.9111    1.7713   -5.3002   27.4068 -183.711 ]\n",
      "Weights: [-4.7601  0.6385 -1.1292  0.0853  0.1463]\n",
      "MSE loss: 92.1656\n",
      "Iteration: 64700\n",
      "Gradient: [ -4.1996  11.0723  25.7885  62.3094 262.6532]\n",
      "Weights: [-4.7799  0.6469 -1.1315  0.0856  0.1464]\n",
      "MSE loss: 92.1038\n",
      "Iteration: 64800\n",
      "Gradient: [   9.6838    6.1422    6.9761  -32.0329 -324.1135]\n",
      "Weights: [-4.7822  0.6469 -1.1308  0.0847  0.1466]\n",
      "MSE loss: 92.1514\n",
      "Iteration: 64900\n",
      "Gradient: [  7.6955   9.4696   4.4443  83.4604 -11.5458]\n",
      "Weights: [-4.7532  0.6335 -1.1284  0.0839  0.1469]\n",
      "MSE loss: 92.2827\n",
      "Iteration: 65000\n",
      "Gradient: [-2.450000e-02 -1.910730e+01 -1.579180e+01 -1.220898e+02  3.056354e+02]\n",
      "Weights: [-4.7509  0.625  -1.1269  0.084   0.1468]\n",
      "MSE loss: 92.1798\n",
      "Iteration: 65100\n",
      "Gradient: [   1.9563    5.2989   15.985   129.3194 -152.6002]\n",
      "Weights: [-4.7475  0.6239 -1.1282  0.0842  0.1469]\n",
      "MSE loss: 92.2358\n",
      "Iteration: 65200\n",
      "Gradient: [  5.4354   4.7296 -20.3064  42.8315 103.4907]\n",
      "Weights: [-4.7481  0.6215 -1.1281  0.0848  0.147 ]\n",
      "MSE loss: 92.2317\n",
      "Iteration: 65300\n",
      "Gradient: [ -1.2954  11.5721 -18.6047 -37.1777 361.5666]\n",
      "Weights: [-4.7571  0.6305 -1.1258  0.0842  0.1467]\n",
      "MSE loss: 92.2042\n",
      "Iteration: 65400\n",
      "Gradient: [ -5.7945   8.1672 -12.5372   9.6484 -67.3895]\n",
      "Weights: [-4.7555  0.6317 -1.1296  0.0841  0.1468]\n",
      "MSE loss: 92.149\n",
      "Iteration: 65500\n",
      "Gradient: [   2.0643   -5.9743  -23.0017 -104.6397   81.6409]\n",
      "Weights: [-4.7729  0.6493 -1.1317  0.0832  0.1469]\n",
      "MSE loss: 92.0789\n",
      "Iteration: 65600\n",
      "Gradient: [ -5.2651   4.328  -14.8367  45.5149 -28.6114]\n",
      "Weights: [-4.7678  0.6526 -1.1354  0.0835  0.1471]\n",
      "MSE loss: 92.0795\n",
      "Iteration: 65700\n",
      "Gradient: [ 10.2778  23.3885  28.7733 -36.3616 -17.5964]\n",
      "Weights: [-4.7639  0.6481 -1.1365  0.0839  0.1476]\n",
      "MSE loss: 92.2484\n",
      "Iteration: 65800\n",
      "Gradient: [  5.637    7.9899 -18.1997   8.8145 -45.3298]\n",
      "Weights: [-4.7753  0.6557 -1.1388  0.0843  0.1473]\n",
      "MSE loss: 92.0406\n",
      "Iteration: 65900\n",
      "Gradient: [  0.9377  -6.2918 -31.0824   3.7398 -63.7458]\n",
      "Weights: [-4.7672  0.6574 -1.1391  0.0836  0.1472]\n",
      "MSE loss: 92.108\n",
      "Iteration: 66000\n",
      "Gradient: [  7.223    6.3475  72.2762 224.6305 385.9442]\n",
      "Weights: [-4.7701  0.6553 -1.1378  0.0845  0.1475]\n",
      "MSE loss: 92.5189\n",
      "Iteration: 66100\n",
      "Gradient: [  -8.0975   -4.025    -5.8752  -86.3258 -210.0251]\n",
      "Weights: [-4.7825  0.6558 -1.1393  0.0833  0.1476]\n",
      "MSE loss: 92.3443\n",
      "Iteration: 66200\n",
      "Gradient: [ -8.0672  20.2193 -22.8419 240.3348 318.1562]\n",
      "Weights: [-4.7659  0.6644 -1.1418  0.084   0.1479]\n",
      "MSE loss: 93.0755\n",
      "Iteration: 66300\n",
      "Gradient: [  10.7487   -9.729   -37.3223   94.11   -139.4073]\n",
      "Weights: [-4.7512  0.6614 -1.1464  0.0838  0.1477]\n",
      "MSE loss: 92.4678\n",
      "Iteration: 66400\n",
      "Gradient: [-2.8532  7.2523 19.0172 49.9213 51.9852]\n",
      "Weights: [-4.7747  0.6618 -1.143   0.0845  0.1476]\n",
      "MSE loss: 92.0762\n",
      "Iteration: 66500\n",
      "Gradient: [   0.4345  -16.5681    1.9948  -69.3922 -236.4277]\n",
      "Weights: [-4.774   0.6576 -1.1433  0.0849  0.1474]\n",
      "MSE loss: 92.1087\n",
      "Iteration: 66600\n",
      "Gradient: [   4.2952  -10.0106   -4.5188  169.057  -241.8523]\n",
      "Weights: [-4.7804  0.6673 -1.1445  0.0859  0.1473]\n",
      "MSE loss: 92.1995\n",
      "Iteration: 66700\n",
      "Gradient: [   0.9584   -0.34    -34.9348 -135.7468   78.6176]\n",
      "Weights: [-4.7925  0.6736 -1.1475  0.0847  0.1475]\n",
      "MSE loss: 92.2612\n",
      "Iteration: 66800\n",
      "Gradient: [  3.186  -14.5657  21.2759 -81.0425  32.545 ]\n",
      "Weights: [-4.7751  0.6598 -1.1431  0.0842  0.1474]\n",
      "MSE loss: 92.2453\n",
      "Iteration: 66900\n",
      "Gradient: [  -1.2504   -3.7609  -15.999   -92.9987 -194.3368]\n",
      "Weights: [-4.7675  0.6598 -1.145   0.0842  0.1476]\n",
      "MSE loss: 92.1198\n",
      "Iteration: 67000\n",
      "Gradient: [   2.8024    3.3155   -3.8661  -16.2406 -283.0245]\n",
      "Weights: [-4.7628  0.6481 -1.1431  0.0856  0.1475]\n",
      "MSE loss: 92.0859\n",
      "Iteration: 67100\n",
      "Gradient: [  0.1496   0.5531   3.295  -22.6163 -26.3264]\n",
      "Weights: [-4.788   0.6628 -1.1403  0.0852  0.1473]\n",
      "MSE loss: 92.2573\n",
      "Iteration: 67200\n",
      "Gradient: [   1.7732  -21.5684  -16.897  -215.1746   42.3328]\n",
      "Weights: [-4.7741  0.6573 -1.1412  0.0849  0.1471]\n",
      "MSE loss: 92.0812\n",
      "Iteration: 67300\n",
      "Gradient: [  -6.4295  -10.4505  -43.4054   56.6327 -165.9695]\n",
      "Weights: [-4.7843  0.6563 -1.1416  0.0856  0.1471]\n",
      "MSE loss: 92.3274\n",
      "Iteration: 67400\n",
      "Gradient: [ -2.4821  -4.1258  10.0201  -3.849  -47.9359]\n",
      "Weights: [-4.7821  0.6685 -1.1419  0.0839  0.1469]\n",
      "MSE loss: 92.3152\n",
      "Iteration: 67500\n",
      "Gradient: [  6.5337   1.6005  -5.6108  29.0654 375.2412]\n",
      "Weights: [-4.7931  0.6793 -1.1388  0.0834  0.147 ]\n",
      "MSE loss: 92.2928\n",
      "Iteration: 67600\n",
      "Gradient: [   1.8131   -2.7045   -9.1139   57.28   -185.1434]\n",
      "Weights: [-4.7667  0.6734 -1.142   0.0832  0.1471]\n",
      "MSE loss: 92.4636\n",
      "Iteration: 67700\n",
      "Gradient: [ 0.1685  4.8032 -2.7595 47.905  72.7517]\n",
      "Weights: [-4.7752  0.6686 -1.1456  0.0845  0.1475]\n",
      "MSE loss: 92.018\n",
      "Iteration: 67800\n",
      "Gradient: [  -6.0426   -3.7201   22.0822  -21.6319 -167.6197]\n",
      "Weights: [-4.7692  0.6641 -1.1475  0.0853  0.1475]\n",
      "MSE loss: 92.0266\n",
      "Iteration: 67900\n",
      "Gradient: [  3.7104 -15.6908 -42.3784 -15.8624  70.0524]\n",
      "Weights: [-4.7857  0.6777 -1.1482  0.0851  0.1474]\n",
      "MSE loss: 92.0192\n",
      "Iteration: 68000\n",
      "Gradient: [  -0.8957    3.9454   -1.8831 -162.301   122.2137]\n",
      "Weights: [-4.7776  0.6771 -1.1467  0.0843  0.1471]\n",
      "MSE loss: 92.1199\n",
      "Iteration: 68100\n",
      "Gradient: [ 6.774300e+00 -2.060000e-01 -3.874300e+00  2.773380e+01  3.889062e+02]\n",
      "Weights: [-4.7798  0.6689 -1.1454  0.0853  0.1474]\n",
      "MSE loss: 92.0563\n",
      "Iteration: 68200\n",
      "Gradient: [-13.3343  -5.1225 -28.5516 -49.6961 -56.6755]\n",
      "Weights: [-4.7765  0.6683 -1.1448  0.0849  0.1472]\n",
      "MSE loss: 92.0179\n",
      "Iteration: 68300\n",
      "Gradient: [  1.3149  16.6641  -3.5429  24.3708 150.8383]\n",
      "Weights: [-4.781   0.6778 -1.1445  0.0845  0.1473]\n",
      "MSE loss: 92.3521\n",
      "Iteration: 68400\n",
      "Gradient: [  -3.2066    7.3817    3.0074 -119.5626  531.7935]\n",
      "Weights: [-4.7938  0.6646 -1.1414  0.0853  0.1473]\n",
      "MSE loss: 92.3021\n",
      "Iteration: 68500\n",
      "Gradient: [  3.1051  -5.7945 -11.9969 -14.8208  -2.6811]\n",
      "Weights: [-4.7676  0.6572 -1.1408  0.0846  0.1472]\n",
      "MSE loss: 92.0521\n",
      "Iteration: 68600\n",
      "Gradient: [  -5.3249  -10.3844   -0.3432   96.9822 -150.4735]\n",
      "Weights: [-4.7796  0.6521 -1.1412  0.0856  0.1475]\n",
      "MSE loss: 92.1931\n",
      "Iteration: 68700\n",
      "Gradient: [ -1.8426 -24.7279  47.5558  33.6032 -81.1152]\n",
      "Weights: [-4.7851  0.6673 -1.1461  0.0858  0.1472]\n",
      "MSE loss: 92.0838\n",
      "Iteration: 68800\n",
      "Gradient: [ 0.2483  9.9036 22.0202 96.8839 25.2308]\n",
      "Weights: [-4.7813  0.6733 -1.1463  0.0863  0.147 ]\n",
      "MSE loss: 92.0839\n",
      "Iteration: 68900\n",
      "Gradient: [  -0.9558    8.9753  -50.8629 -105.4902   46.2406]\n",
      "Weights: [-4.7784  0.6605 -1.1451  0.0866  0.1468]\n",
      "MSE loss: 92.176\n",
      "Iteration: 69000\n",
      "Gradient: [ -0.0978  -2.0627   9.5116  36.4969 -38.3856]\n",
      "Weights: [-4.7885  0.6708 -1.1476  0.0875  0.1469]\n",
      "MSE loss: 92.0593\n",
      "Iteration: 69100\n",
      "Gradient: [  -0.9406   -7.6757    9.0644  -41.5016 -265.4439]\n",
      "Weights: [-4.7851  0.6622 -1.1472  0.0872  0.1471]\n",
      "MSE loss: 92.2318\n",
      "Iteration: 69200\n",
      "Gradient: [  5.386  -23.0307  -1.0882  20.2708 112.7487]\n",
      "Weights: [-4.7785  0.6719 -1.1458  0.0863  0.1468]\n",
      "MSE loss: 92.0386\n",
      "Iteration: 69300\n",
      "Gradient: [ 7.6405  5.1802  5.149  72.2643 56.6875]\n",
      "Weights: [-4.7824  0.6827 -1.1481  0.086   0.1468]\n",
      "MSE loss: 92.1807\n",
      "Iteration: 69400\n",
      "Gradient: [ -2.9547 -10.0803  19.9207 -77.9799  15.5499]\n",
      "Weights: [-4.7915  0.6781 -1.1465  0.0854  0.1471]\n",
      "MSE loss: 92.0558\n",
      "Iteration: 69500\n",
      "Gradient: [  -7.6925    1.7935   24.8274 -160.2097 -389.3435]\n",
      "Weights: [-4.7909  0.6803 -1.1482  0.0854  0.1472]\n",
      "MSE loss: 92.0393\n",
      "Iteration: 69600\n",
      "Gradient: [ -5.8146 -11.9934   2.7077   2.2732 126.9594]\n",
      "Weights: [-4.7758  0.6646 -1.1513  0.087   0.1474]\n",
      "MSE loss: 92.0966\n",
      "Iteration: 69700\n",
      "Gradient: [  0.5378   3.8872 -11.94   -62.1032 -99.8335]\n",
      "Weights: [-4.7606  0.6609 -1.1488  0.087   0.1473]\n",
      "MSE loss: 92.1612\n",
      "Iteration: 69800\n",
      "Gradient: [-10.4907  19.7539  35.7413 110.0092  48.9001]\n",
      "Weights: [-4.7641  0.6635 -1.148   0.0866  0.1472]\n",
      "MSE loss: 92.1252\n",
      "Iteration: 69900\n",
      "Gradient: [-14.995   11.9659  -8.9886  15.5923 -20.2781]\n",
      "Weights: [-4.8025  0.6746 -1.1482  0.087   0.1471]\n",
      "MSE loss: 92.4619\n",
      "Iteration: 70000\n",
      "Gradient: [ -0.179   -6.9049 -25.5232  72.3763 -53.1153]\n",
      "Weights: [-4.767   0.6723 -1.1508  0.0868  0.1468]\n",
      "MSE loss: 92.1835\n",
      "Iteration: 70100\n",
      "Gradient: [  -4.1921  -16.2469   21.1242   24.4842 -349.2035]\n",
      "Weights: [-4.7725  0.6613 -1.145   0.0862  0.147 ]\n",
      "MSE loss: 92.0247\n",
      "Iteration: 70200\n",
      "Gradient: [   0.6713   -5.0308    4.386     4.3049 -212.7758]\n",
      "Weights: [-4.7623  0.6578 -1.1455  0.087   0.1469]\n",
      "MSE loss: 92.0787\n",
      "Iteration: 70300\n",
      "Gradient: [  4.6305  26.8865 -34.1695  47.6213 -54.6669]\n",
      "Weights: [-4.7625  0.6538 -1.1472  0.0877  0.147 ]\n",
      "MSE loss: 92.0433\n",
      "Iteration: 70400\n",
      "Gradient: [  0.4132 -14.1463 -14.5183  15.9911  20.6059]\n",
      "Weights: [-4.7633  0.6606 -1.1472  0.087   0.147 ]\n",
      "MSE loss: 92.0709\n",
      "Iteration: 70500\n",
      "Gradient: [ 20.5593   8.0058  -6.5614 178.448  200.5531]\n",
      "Weights: [-4.7595  0.6571 -1.1506  0.088   0.1471]\n",
      "MSE loss: 92.0719\n",
      "Iteration: 70600\n",
      "Gradient: [  -3.0532  -11.63    -10.3708 -125.7442 -140.3553]\n",
      "Weights: [-4.7785  0.6626 -1.1504  0.0889  0.1469]\n",
      "MSE loss: 92.0215\n",
      "Iteration: 70700\n",
      "Gradient: [-13.0877  -2.7874  -2.0895 -37.9489  40.1172]\n",
      "Weights: [-4.7873  0.6692 -1.1517  0.0891  0.1469]\n",
      "MSE loss: 92.0916\n",
      "Iteration: 70800\n",
      "Gradient: [  5.8594 -23.2669 -47.0257  60.2316  76.324 ]\n",
      "Weights: [-4.7873  0.6869 -1.1567  0.0885  0.1466]\n",
      "MSE loss: 92.0068\n",
      "Iteration: 70900\n",
      "Gradient: [  0.3575 -13.3874 -28.7444 105.7694 -61.1837]\n",
      "Weights: [-4.7869  0.6904 -1.1582  0.0889  0.1467]\n",
      "MSE loss: 91.9684\n",
      "Iteration: 71000\n",
      "Gradient: [   5.0928    1.7147   27.3735   15.365  -119.3568]\n",
      "Weights: [-4.798   0.7024 -1.1596  0.0889  0.1466]\n",
      "MSE loss: 92.0856\n",
      "Iteration: 71100\n",
      "Gradient: [  5.1381  27.7078  -7.2483  26.626  161.4095]\n",
      "Weights: [-4.7862  0.6881 -1.1591  0.0891  0.147 ]\n",
      "MSE loss: 91.9651\n",
      "Iteration: 71200\n",
      "Gradient: [  -4.3895  -23.5694   21.0323   61.9642 -235.4634]\n",
      "Weights: [-4.7982  0.701  -1.1616  0.0883  0.1469]\n",
      "MSE loss: 92.0427\n",
      "Iteration: 71300\n",
      "Gradient: [ -2.9839   0.9789  39.0633 105.6064 -39.954 ]\n",
      "Weights: [-4.8087  0.7103 -1.1622  0.0884  0.1472]\n",
      "MSE loss: 92.4093\n",
      "Iteration: 71400\n",
      "Gradient: [  5.3012  -8.2212 -22.2633 109.599  -17.3462]\n",
      "Weights: [-4.7867  0.6935 -1.1621  0.0885  0.1473]\n",
      "MSE loss: 91.9509\n",
      "Iteration: 71500\n",
      "Gradient: [  7.8087 -18.9199  54.0914  22.8304 252.3318]\n",
      "Weights: [-4.7798  0.6866 -1.1603  0.0882  0.1471]\n",
      "MSE loss: 91.9778\n",
      "Iteration: 71600\n",
      "Gradient: [  3.2117  15.9391  10.7502 -23.8714 255.0136]\n",
      "Weights: [-4.7759  0.688  -1.161   0.0884  0.1474]\n",
      "MSE loss: 92.1438\n",
      "Iteration: 71700\n",
      "Gradient: [-1.164000e+00 -8.870000e-02 -9.650500e+00 -1.321307e+02  2.957605e+02]\n",
      "Weights: [-4.7881  0.686  -1.1594  0.0887  0.1471]\n",
      "MSE loss: 91.9668\n",
      "Iteration: 71800\n",
      "Gradient: [ -6.05    25.6602 -18.6108 -56.8328 174.7593]\n",
      "Weights: [-4.782   0.6887 -1.1605  0.088   0.1473]\n",
      "MSE loss: 91.959\n",
      "Iteration: 71900\n",
      "Gradient: [-9.0046 -5.8203 11.8434 16.5624  2.4493]\n",
      "Weights: [-4.7712  0.6763 -1.1609  0.0889  0.1474]\n",
      "MSE loss: 91.9776\n",
      "Iteration: 72000\n",
      "Gradient: [  9.7105 -31.0243  36.1746  40.6848 -76.594 ]\n",
      "Weights: [-4.7632  0.6822 -1.1646  0.0891  0.1474]\n",
      "MSE loss: 92.1359\n",
      "Iteration: 72100\n",
      "Gradient: [ 10.7781  10.4435  10.6327 -54.802  298.2385]\n",
      "Weights: [-4.7732  0.6931 -1.1662  0.0891  0.1475]\n",
      "MSE loss: 92.1061\n",
      "Iteration: 72200\n",
      "Gradient: [-4.6777 14.9892 38.043   5.6648 67.0378]\n",
      "Weights: [-4.7873  0.7037 -1.1699  0.0891  0.1476]\n",
      "MSE loss: 91.9343\n",
      "Iteration: 72300\n",
      "Gradient: [ -10.5327   -6.2996  -39.0978  -56.4751 -292.3536]\n",
      "Weights: [-4.7889  0.7096 -1.1746  0.0895  0.1474]\n",
      "MSE loss: 92.0806\n",
      "Iteration: 72400\n",
      "Gradient: [  -4.6826   -7.5493   -2.5635  -76.7196 -190.8735]\n",
      "Weights: [-4.7765  0.6974 -1.174   0.0905  0.1474]\n",
      "MSE loss: 92.112\n",
      "Iteration: 72500\n",
      "Gradient: [  2.1966 -20.6666  17.8835  67.304  127.1321]\n",
      "Weights: [-4.7842  0.6997 -1.1722  0.091   0.1474]\n",
      "MSE loss: 91.915\n",
      "Iteration: 72600\n",
      "Gradient: [  4.5895  -8.2716  14.4936 147.205   44.4768]\n",
      "Weights: [-4.7821  0.7031 -1.1708  0.0902  0.1474]\n",
      "MSE loss: 92.0774\n",
      "Iteration: 72700\n",
      "Gradient: [-13.757   18.4921 -14.7606 -38.0349 133.8047]\n",
      "Weights: [-4.7688  0.6856 -1.1671  0.0902  0.1474]\n",
      "MSE loss: 92.0302\n",
      "Iteration: 72800\n",
      "Gradient: [-11.1127  -0.593   62.7769  65.7146 162.8631]\n",
      "Weights: [-4.7977  0.6971 -1.1657  0.0897  0.1474]\n",
      "MSE loss: 92.0395\n",
      "Iteration: 72900\n",
      "Gradient: [  4.286    1.571    6.1036 -82.1823 125.1631]\n",
      "Weights: [-4.7679  0.6892 -1.1668  0.0888  0.1475]\n",
      "MSE loss: 92.1121\n",
      "Iteration: 73000\n",
      "Gradient: [-3.981   9.5604  9.141  79.7939 87.8697]\n",
      "Weights: [-4.7829  0.6958 -1.1693  0.0903  0.1476]\n",
      "MSE loss: 92.0024\n",
      "Iteration: 73100\n",
      "Gradient: [   5.5399  -11.2949    5.0532   15.3972 -110.4389]\n",
      "Weights: [-4.7927  0.7036 -1.1717  0.0897  0.1476]\n",
      "MSE loss: 91.9473\n",
      "Iteration: 73200\n",
      "Gradient: [  -2.373    -3.9411  -11.6185 -100.8409 -337.5189]\n",
      "Weights: [-4.7998  0.7097 -1.17    0.0892  0.1475]\n",
      "MSE loss: 91.9762\n",
      "Iteration: 73300\n",
      "Gradient: [ -12.8406   -5.0703   11.2921 -104.4006 -303.1967]\n",
      "Weights: [-4.7903  0.696  -1.1719  0.0896  0.1474]\n",
      "MSE loss: 92.8746\n",
      "Iteration: 73400\n",
      "Gradient: [ -2.9197  -1.4853  28.8323 -15.9721 117.063 ]\n",
      "Weights: [-4.7856  0.6969 -1.1673  0.0896  0.1473]\n",
      "MSE loss: 91.9196\n",
      "Iteration: 73500\n",
      "Gradient: [  -9.7443  -16.529   -29.4391  -49.1548 -153.0255]\n",
      "Weights: [-4.7751  0.6999 -1.1693  0.089   0.1474]\n",
      "MSE loss: 92.0878\n",
      "Iteration: 73600\n",
      "Gradient: [   5.5784    7.702    44.4041  -88.6045 -156.3482]\n",
      "Weights: [-4.7812  0.7022 -1.1709  0.089   0.1475]\n",
      "MSE loss: 92.03\n",
      "Iteration: 73700\n",
      "Gradient: [ 10.1408   7.173   -3.4147  67.6775 -16.2825]\n",
      "Weights: [-4.7888  0.7048 -1.1696  0.0893  0.1475]\n",
      "MSE loss: 91.9602\n",
      "Iteration: 73800\n",
      "Gradient: [ 0.7913  3.6449 25.6707 17.7136 29.1916]\n",
      "Weights: [-4.7897  0.6955 -1.1683  0.0895  0.1475]\n",
      "MSE loss: 92.0172\n",
      "Iteration: 73900\n",
      "Gradient: [ -0.9313  12.2811   0.7917  55.5936 -61.9683]\n",
      "Weights: [-4.7857  0.6978 -1.1666  0.0896  0.1474]\n",
      "MSE loss: 92.0317\n",
      "Iteration: 74000\n",
      "Gradient: [ -2.6982  -7.4573 -43.5771   4.6215 -66.2327]\n",
      "Weights: [-4.7786  0.6872 -1.1635  0.0889  0.1473]\n",
      "MSE loss: 91.9523\n",
      "Iteration: 74100\n",
      "Gradient: [  -8.4442   -2.6519  -24.5439  -24.7259 -257.1163]\n",
      "Weights: [-4.7841  0.6844 -1.1628  0.0899  0.1472]\n",
      "MSE loss: 91.9424\n",
      "Iteration: 74200\n",
      "Gradient: [ -2.7625 -10.7693 -14.1327 -64.1834 -97.9135]\n",
      "Weights: [-4.785   0.6916 -1.1648  0.0893  0.1474]\n",
      "MSE loss: 91.9437\n",
      "Iteration: 74300\n",
      "Gradient: [   0.4753   -5.9161   82.1493   22.6581 -205.4195]\n",
      "Weights: [-4.7807  0.6896 -1.1654  0.0897  0.1472]\n",
      "MSE loss: 91.9545\n",
      "Iteration: 74400\n",
      "Gradient: [ -2.6101  -6.1618 -23.4093 -44.4719  44.6879]\n",
      "Weights: [-4.7927  0.7012 -1.1677  0.0896  0.1474]\n",
      "MSE loss: 91.9446\n",
      "Iteration: 74500\n",
      "Gradient: [  -1.6392    8.3419   23.0119   32.1253 -162.3683]\n",
      "Weights: [-4.7758  0.6979 -1.1691  0.0896  0.1473]\n",
      "MSE loss: 92.0294\n",
      "Iteration: 74600\n",
      "Gradient: [ -18.7509  -16.6181  -51.574   -57.8456 -215.2325]\n",
      "Weights: [-4.7966  0.6949 -1.1686  0.0897  0.1475]\n",
      "MSE loss: 92.3108\n",
      "Iteration: 74700\n",
      "Gradient: [  5.0751  12.8266  25.3035 136.1806 102.7549]\n",
      "Weights: [-4.7781  0.6949 -1.1675  0.0898  0.1474]\n",
      "MSE loss: 92.0348\n",
      "Iteration: 74800\n",
      "Gradient: [ -5.4598  26.4317 -18.0199 147.8465 -12.3757]\n",
      "Weights: [-4.7971  0.7021 -1.169   0.0894  0.1474]\n",
      "MSE loss: 92.0564\n",
      "Iteration: 74900\n",
      "Gradient: [  -5.3009   -9.173    20.275  -114.7588  177.6671]\n",
      "Weights: [-4.7966  0.6972 -1.1667  0.0899  0.1472]\n",
      "MSE loss: 92.0496\n",
      "Iteration: 75000\n",
      "Gradient: [ -2.1698   7.2179 -12.0815 109.9194 -74.5394]\n",
      "Weights: [-4.786   0.7005 -1.1681  0.0907  0.1469]\n",
      "MSE loss: 91.9325\n",
      "Iteration: 75100\n",
      "Gradient: [  -0.4126    1.3319   14.5946  -26.6685 -112.266 ]\n",
      "Weights: [-4.8012  0.7089 -1.1695  0.0907  0.1469]\n",
      "MSE loss: 91.975\n",
      "Iteration: 75200\n",
      "Gradient: [  1.3708   7.4294 -10.8138 132.8879 226.971 ]\n",
      "Weights: [-4.793   0.7062 -1.1693  0.0914  0.1468]\n",
      "MSE loss: 92.0258\n",
      "Iteration: 75300\n",
      "Gradient: [ 2.800000e-03 -6.487100e+00 -2.246050e+01 -8.558250e+01  2.388868e+02]\n",
      "Weights: [-4.8002  0.707  -1.1718  0.0915  0.1468]\n",
      "MSE loss: 92.0366\n",
      "Iteration: 75400\n",
      "Gradient: [ 15.1471   8.8957   9.9057  57.8381 421.6329]\n",
      "Weights: [-4.7903  0.7051 -1.1674  0.0907  0.1466]\n",
      "MSE loss: 91.9789\n",
      "Iteration: 75500\n",
      "Gradient: [  -0.2204   13.2751   64.1421 -101.1533  193.399 ]\n",
      "Weights: [-4.7957  0.7076 -1.1659  0.0903  0.1467]\n",
      "MSE loss: 92.1196\n",
      "Iteration: 75600\n",
      "Gradient: [   0.9786  -11.4766   -3.9184  -62.6022 -378.9492]\n",
      "Weights: [-4.812   0.7112 -1.1697  0.0902  0.1468]\n",
      "MSE loss: 92.5308\n",
      "Iteration: 75700\n",
      "Gradient: [  7.2146  -6.0802 -72.9276 -33.5937  13.2832]\n",
      "Weights: [-4.7933  0.7166 -1.1725  0.0906  0.1466]\n",
      "MSE loss: 92.1148\n",
      "Iteration: 75800\n",
      "Gradient: [  -5.8424   -7.6939    1.5661   -5.401  -104.3007]\n",
      "Weights: [-4.7944  0.7156 -1.1739  0.0906  0.1468]\n",
      "MSE loss: 92.0702\n",
      "Iteration: 75900\n",
      "Gradient: [ -2.7287 -10.6704  34.5596  62.904  -24.0493]\n",
      "Weights: [-4.8131  0.7325 -1.1731  0.0904  0.1467]\n",
      "MSE loss: 92.2504\n",
      "Iteration: 76000\n",
      "Gradient: [  0.9871  -7.6817  18.9246  -6.9932 130.6342]\n",
      "Weights: [-4.7975  0.7268 -1.1754  0.0904  0.1469]\n",
      "MSE loss: 92.1612\n",
      "Iteration: 76100\n",
      "Gradient: [  2.7427 -12.193  -66.3086 -13.0671 -71.3815]\n",
      "Weights: [-4.7964  0.7178 -1.177   0.0912  0.1472]\n",
      "MSE loss: 91.9229\n",
      "Iteration: 76200\n",
      "Gradient: [-16.5757   6.8682 -14.2532 130.3556 -55.2288]\n",
      "Weights: [-4.819   0.7229 -1.1747  0.091   0.147 ]\n",
      "MSE loss: 92.2768\n",
      "Iteration: 76300\n",
      "Gradient: [ 1.6907  8.5452 -7.469   1.9509 10.6769]\n",
      "Weights: [-4.8013  0.7104 -1.1733  0.0915  0.1472]\n",
      "MSE loss: 91.9844\n",
      "Iteration: 76400\n",
      "Gradient: [ 4.6818  0.6155 41.2978 34.2977 58.6883]\n",
      "Weights: [-4.7877  0.7099 -1.1739  0.092   0.147 ]\n",
      "MSE loss: 92.1047\n",
      "Iteration: 76500\n",
      "Gradient: [-6.80000e-03  7.90290e+00 -9.98980e+00 -4.96374e+01  5.99012e+01]\n",
      "Weights: [-4.8004  0.7133 -1.1773  0.0923  0.147 ]\n",
      "MSE loss: 91.9608\n",
      "Iteration: 76600\n",
      "Gradient: [  -6.8376    2.0308   43.4355  -28.7261 -172.8343]\n",
      "Weights: [-4.7941  0.7114 -1.1759  0.0917  0.1471]\n",
      "MSE loss: 91.8961\n",
      "Iteration: 76700\n",
      "Gradient: [  -3.127   -13.641   -45.8109 -134.972   101.3309]\n",
      "Weights: [-4.7927  0.7024 -1.1775  0.0934  0.1473]\n",
      "MSE loss: 91.9609\n",
      "Iteration: 76800\n",
      "Gradient: [  3.6432  -4.49    16.3876 -31.1862 297.9239]\n",
      "Weights: [-4.7848  0.7021 -1.1752  0.0927  0.147 ]\n",
      "MSE loss: 91.8784\n",
      "Iteration: 76900\n",
      "Gradient: [  6.5093   4.5596  24.4644 -70.6229 335.6172]\n",
      "Weights: [-4.7941  0.7062 -1.1733  0.0925  0.1471]\n",
      "MSE loss: 92.0506\n",
      "Iteration: 77000\n",
      "Gradient: [ -0.4061   2.2088 -10.5035 -59.9538 105.0248]\n",
      "Weights: [-4.8028  0.7044 -1.1723  0.0925  0.1469]\n",
      "MSE loss: 92.0542\n",
      "Iteration: 77100\n",
      "Gradient: [-3.8491 -6.3123  8.6113 -8.905  16.9297]\n",
      "Weights: [-4.8012  0.7086 -1.1701  0.0911  0.147 ]\n",
      "MSE loss: 92.0073\n",
      "Iteration: 77200\n",
      "Gradient: [  -2.4605   -5.3512   -6.8832    9.6347 -168.3101]\n",
      "Weights: [-4.8042  0.7094 -1.1732  0.0915  0.1471]\n",
      "MSE loss: 92.0445\n",
      "Iteration: 77300\n",
      "Gradient: [  2.7265   0.7233  16.4643  10.0972 106.5196]\n",
      "Weights: [-4.775   0.707  -1.1745  0.0907  0.1472]\n",
      "MSE loss: 92.1607\n",
      "Iteration: 77400\n",
      "Gradient: [   7.1289   14.724    12.867  -101.8528  159.2353]\n",
      "Weights: [-4.7756  0.6986 -1.1738  0.0908  0.1474]\n",
      "MSE loss: 91.9688\n",
      "Iteration: 77500\n",
      "Gradient: [ -3.6909  -1.2345 -30.8039 -23.8588 -72.128 ]\n",
      "Weights: [-4.7762  0.6904 -1.1739  0.0912  0.1478]\n",
      "MSE loss: 91.9654\n",
      "Iteration: 77600\n",
      "Gradient: [-9.5881 15.1156  6.8088 88.5653 94.0243]\n",
      "Weights: [-4.787   0.6928 -1.1743  0.0921  0.1479]\n",
      "MSE loss: 92.1481\n",
      "Iteration: 77700\n",
      "Gradient: [ -15.7374  -11.0507   -5.3784  -55.3093 -217.8177]\n",
      "Weights: [-4.7887  0.6912 -1.1729  0.0905  0.1477]\n",
      "MSE loss: 92.4548\n",
      "Iteration: 77800\n",
      "Gradient: [  5.7361   9.828    9.808   57.0686 299.4575]\n",
      "Weights: [-4.7743  0.6799 -1.169   0.0907  0.1478]\n",
      "MSE loss: 92.037\n",
      "Iteration: 77900\n",
      "Gradient: [ 11.5151   9.7677 -12.5454   4.6681  16.1896]\n",
      "Weights: [-4.7859  0.6909 -1.1664  0.09    0.1474]\n",
      "MSE loss: 91.9399\n",
      "Iteration: 78000\n",
      "Gradient: [  7.7041 -19.7217  28.9199  30.3623 -93.5893]\n",
      "Weights: [-4.7834  0.6883 -1.1663  0.0909  0.1472]\n",
      "MSE loss: 91.9145\n",
      "Iteration: 78100\n",
      "Gradient: [ 4.7888  1.3422 38.9743 -5.6009 -9.0304]\n",
      "Weights: [-4.7729  0.6874 -1.1668  0.0911  0.147 ]\n",
      "MSE loss: 91.9726\n",
      "Iteration: 78200\n",
      "Gradient: [  4.4696   8.782  -16.6573  41.2736 168.9535]\n",
      "Weights: [-4.7827  0.6971 -1.1688  0.0916  0.1472]\n",
      "MSE loss: 92.3723\n",
      "Iteration: 78300\n",
      "Gradient: [  -7.0103    3.2989  -47.3168  -73.3978 -306.4987]\n",
      "Weights: [-4.784   0.6909 -1.1697  0.0912  0.1472]\n",
      "MSE loss: 91.9542\n",
      "Iteration: 78400\n",
      "Gradient: [ -1.0332  -0.9129   2.8471 100.6299 136.2846]\n",
      "Weights: [-4.7772  0.6819 -1.1655  0.0912  0.1472]\n",
      "MSE loss: 91.9405\n",
      "Iteration: 78500\n",
      "Gradient: [  6.793   15.5388 -23.161   11.8853 -54.8298]\n",
      "Weights: [-4.7743  0.6946 -1.1686  0.0904  0.1473]\n",
      "MSE loss: 92.0721\n",
      "Iteration: 78600\n",
      "Gradient: [  5.6102   0.8264 -25.668   22.834   38.3392]\n",
      "Weights: [-4.7938  0.7029 -1.1683  0.0912  0.147 ]\n",
      "MSE loss: 92.0216\n",
      "Iteration: 78700\n",
      "Gradient: [   5.8155    9.8976    7.6185 -118.5834   -0.3496]\n",
      "Weights: [-4.7856  0.693  -1.1671  0.091   0.1468]\n",
      "MSE loss: 91.9389\n",
      "Iteration: 78800\n",
      "Gradient: [ -8.0679  22.4142 -72.636  -32.9583 -35.7628]\n",
      "Weights: [-4.7817  0.6827 -1.1682  0.0918  0.147 ]\n",
      "MSE loss: 92.1644\n",
      "Iteration: 78900\n",
      "Gradient: [ -14.517     2.4304  -24.746    35.8269 -288.5698]\n",
      "Weights: [-4.7953  0.689  -1.1661  0.092   0.1469]\n",
      "MSE loss: 92.0803\n",
      "Iteration: 79000\n",
      "Gradient: [  -8.7544  -21.7891  -10.3859   43.7253 -223.7378]\n",
      "Weights: [-4.787   0.6915 -1.168   0.0911  0.1469]\n",
      "MSE loss: 92.0421\n",
      "Iteration: 79100\n",
      "Gradient: [ -7.5389   1.0254   2.0181 -29.0626 259.9124]\n",
      "Weights: [-4.7855  0.6901 -1.1678  0.0919  0.1471]\n",
      "MSE loss: 91.955\n",
      "Iteration: 79200\n",
      "Gradient: [ -2.264  -11.262   -8.2316  -6.6992  -2.1785]\n",
      "Weights: [-4.7728  0.6929 -1.1716  0.092   0.1471]\n",
      "MSE loss: 91.978\n",
      "Iteration: 79300\n",
      "Gradient: [ 3.879900e+00  6.900000e-02 -3.767860e+01  1.494040e+01 -1.562994e+02]\n",
      "Weights: [-4.7793  0.6982 -1.1716  0.0915  0.1468]\n",
      "MSE loss: 91.9711\n",
      "Iteration: 79400\n",
      "Gradient: [  -0.4745  -11.7461   19.3868   35.9642 -108.5333]\n",
      "Weights: [-4.795   0.6985 -1.1707  0.0918  0.1468]\n",
      "MSE loss: 92.1203\n",
      "Iteration: 79500\n",
      "Gradient: [   3.2539  -15.6193   14.7371   50.3537 -133.2192]\n",
      "Weights: [-4.8019  0.7062 -1.1719  0.0921  0.1469]\n",
      "MSE loss: 92.0058\n",
      "Iteration: 79600\n",
      "Gradient: [  10.8964   -1.3       3.5302   44.8934 -130.8652]\n",
      "Weights: [-4.7804  0.7049 -1.172   0.0927  0.1466]\n",
      "MSE loss: 92.2209\n",
      "Iteration: 79700\n",
      "Gradient: [ -0.9613   5.5583   4.2608  70.0487 295.0069]\n",
      "Weights: [-4.784   0.7062 -1.172   0.0919  0.1468]\n",
      "MSE loss: 92.0594\n",
      "Iteration: 79800\n",
      "Gradient: [   7.4142   -7.2945   -5.1873   36.4019 -185.9523]\n",
      "Weights: [-4.8034  0.7089 -1.1726  0.0916  0.147 ]\n",
      "MSE loss: 92.0214\n",
      "Iteration: 79900\n",
      "Gradient: [ -2.2704   1.3959  58.779  -65.8058  40.2212]\n",
      "Weights: [-4.7839  0.6978 -1.1724  0.0916  0.1471]\n",
      "MSE loss: 91.918\n",
      "Iteration: 80000\n",
      "Gradient: [ -8.6276   8.8061  24.2065 -17.6304 229.663 ]\n",
      "Weights: [-4.8016  0.7058 -1.1726  0.0923  0.147 ]\n",
      "MSE loss: 92.0181\n",
      "Iteration: 80100\n",
      "Gradient: [  -4.634    -6.8035   23.7793 -136.4393  -22.5748]\n",
      "Weights: [-4.7854  0.7008 -1.1744  0.093   0.1469]\n",
      "MSE loss: 91.8895\n",
      "Iteration: 80200\n",
      "Gradient: [ -1.9224   0.2232 -48.9974 -60.8097 -63.7793]\n",
      "Weights: [-4.7953  0.7121 -1.1775  0.0924  0.147 ]\n",
      "MSE loss: 91.9089\n",
      "Iteration: 80300\n",
      "Gradient: [  -6.2824   -6.2686   -8.0109  -28.1172 -171.256 ]\n",
      "Weights: [-4.7929  0.712  -1.1752  0.0919  0.1468]\n",
      "MSE loss: 91.9189\n",
      "Iteration: 80400\n",
      "Gradient: [  -2.4144  -16.6163    0.7707   47.0415 -233.7987]\n",
      "Weights: [-4.7758  0.7068 -1.1762  0.0924  0.1469]\n",
      "MSE loss: 92.1289\n",
      "Iteration: 80500\n",
      "Gradient: [ -8.4827 -21.0335   9.0734  59.9617  64.242 ]\n",
      "Weights: [-4.791   0.7003 -1.177   0.0933  0.1472]\n",
      "MSE loss: 91.9578\n",
      "Iteration: 80600\n",
      "Gradient: [-1.142000e-01 -8.183400e+00 -8.053000e+00 -1.049287e+02 -1.294969e+02]\n",
      "Weights: [-4.7949  0.6928 -1.1746  0.0931  0.1472]\n",
      "MSE loss: 92.3322\n",
      "Iteration: 80700\n",
      "Gradient: [ -9.9442  -6.049    8.4948 -76.0233 -34.8916]\n",
      "Weights: [-4.7778  0.6907 -1.1739  0.0936  0.1471]\n",
      "MSE loss: 91.9401\n",
      "Iteration: 80800\n",
      "Gradient: [ 6.2306 17.73   40.9297 90.8582 44.3848]\n",
      "Weights: [-4.7752  0.6965 -1.1753  0.0939  0.1468]\n",
      "MSE loss: 91.9824\n",
      "Iteration: 80900\n",
      "Gradient: [ -2.9067   5.3899   6.1676  37.2825 -97.4844]\n",
      "Weights: [-4.7929  0.7031 -1.1758  0.0938  0.1469]\n",
      "MSE loss: 91.8989\n",
      "Iteration: 81000\n",
      "Gradient: [-11.1691 -19.5661  22.639  -25.1998 142.0036]\n",
      "Weights: [-4.7904  0.701  -1.1758  0.0936  0.1468]\n",
      "MSE loss: 91.9101\n",
      "Iteration: 81100\n",
      "Gradient: [   5.2236   -6.2343   -9.5265  -82.3555 -223.4399]\n",
      "Weights: [-4.7801  0.6969 -1.1736  0.0932  0.1468]\n",
      "MSE loss: 91.8785\n",
      "Iteration: 81200\n",
      "Gradient: [  2.7976  -1.6485  41.2222 -65.7933 280.3072]\n",
      "Weights: [-4.7815  0.7064 -1.1772  0.0926  0.147 ]\n",
      "MSE loss: 91.9223\n",
      "Iteration: 81300\n",
      "Gradient: [   1.5661  -20.7697  -34.8378   56.3656 -115.8162]\n",
      "Weights: [-4.7875  0.7085 -1.1802  0.0931  0.147 ]\n",
      "MSE loss: 91.9325\n",
      "Iteration: 81400\n",
      "Gradient: [  7.8761 -13.7648   1.1435  -5.1159 -74.7014]\n",
      "Weights: [-4.7811  0.6986 -1.1756  0.0929  0.1471]\n",
      "MSE loss: 91.8795\n",
      "Iteration: 81500\n",
      "Gradient: [  3.4553  -8.1284 -12.1867  68.5926 -53.196 ]\n",
      "Weights: [-4.7722  0.6932 -1.1734  0.0922  0.1472]\n",
      "MSE loss: 91.9785\n",
      "Iteration: 81600\n",
      "Gradient: [  8.6499  -0.9159   1.1206 -27.864  289.4419]\n",
      "Weights: [-4.777   0.6899 -1.1738  0.0931  0.1473]\n",
      "MSE loss: 91.9445\n",
      "Iteration: 81700\n",
      "Gradient: [  2.6143  -1.2884  -8.3117 -31.3851 122.61  ]\n",
      "Weights: [-4.7783  0.6929 -1.1761  0.0934  0.1471]\n",
      "MSE loss: 91.9261\n",
      "Iteration: 81800\n",
      "Gradient: [  3.6642  17.6646  33.8594  69.8357 193.8218]\n",
      "Weights: [-4.7762  0.7022 -1.1756  0.0939  0.1466]\n",
      "MSE loss: 92.1071\n",
      "Iteration: 81900\n",
      "Gradient: [  -1.0094    5.3708  -54.1316 -105.6792 -170.5196]\n",
      "Weights: [-4.7832  0.6997 -1.1787  0.0944  0.1467]\n",
      "MSE loss: 91.9597\n",
      "Iteration: 82000\n",
      "Gradient: [-5.19810e+00  8.58000e-02 -2.38155e+01  4.43543e+01  8.77639e+01]\n",
      "Weights: [-4.788   0.6965 -1.1757  0.0946  0.1467]\n",
      "MSE loss: 91.8877\n",
      "Iteration: 82100\n",
      "Gradient: [ -7.6588  15.1027   0.7506  13.2262 -20.5668]\n",
      "Weights: [-4.7825  0.6923 -1.1742  0.0949  0.1467]\n",
      "MSE loss: 91.8952\n",
      "Iteration: 82200\n",
      "Gradient: [ -6.8637   5.1738  50.1862  98.141  195.8949]\n",
      "Weights: [-4.7921  0.7034 -1.1758  0.0954  0.1462]\n",
      "MSE loss: 91.854\n",
      "Iteration: 82300\n",
      "Gradient: [   7.9562  -17.5642   51.2004   33.0132 -123.8988]\n",
      "Weights: [-4.7862  0.7031 -1.1746  0.095   0.1461]\n",
      "MSE loss: 91.8844\n",
      "Iteration: 82400\n",
      "Gradient: [  1.102   17.1069   2.251   12.4833 -66.3432]\n",
      "Weights: [-4.7959  0.7077 -1.1744  0.0952  0.1461]\n",
      "MSE loss: 91.9811\n",
      "Iteration: 82500\n",
      "Gradient: [ -2.4575  -6.1759 -52.1385 -28.6992   6.2614]\n",
      "Weights: [-4.7817  0.6813 -1.1684  0.0952  0.146 ]\n",
      "MSE loss: 91.9001\n",
      "Iteration: 82600\n",
      "Gradient: [  -7.2525    8.2544   31.5931 -118.1999  -73.8464]\n",
      "Weights: [-4.7752  0.6706 -1.1687  0.0963  0.146 ]\n",
      "MSE loss: 92.0493\n",
      "Iteration: 82700\n",
      "Gradient: [  -4.3581  -21.9636   -5.4697  -92.2104 -120.9488]\n",
      "Weights: [-4.7705  0.6591 -1.1659  0.0971  0.1458]\n",
      "MSE loss: 92.1724\n",
      "Iteration: 82800\n",
      "Gradient: [  0.8679  12.3059  11.421  136.2502 251.6863]\n",
      "Weights: [-4.7539  0.6581 -1.1655  0.0977  0.1457]\n",
      "MSE loss: 92.0762\n",
      "Iteration: 82900\n",
      "Gradient: [  2.5008   6.5134  13.6578 -48.7453 -17.7675]\n",
      "Weights: [-4.7612  0.659  -1.1653  0.0975  0.1457]\n",
      "MSE loss: 91.9386\n",
      "Iteration: 83000\n",
      "Gradient: [ 11.3283   5.4467  26.7465 111.2638 457.8313]\n",
      "Weights: [-4.7559  0.6584 -1.1642  0.0975  0.1457]\n",
      "MSE loss: 92.141\n",
      "Iteration: 83100\n",
      "Gradient: [-15.6557   7.2221  15.4326 -24.1759  92.0474]\n",
      "Weights: [-4.7826  0.6695 -1.1665  0.0975  0.1457]\n",
      "MSE loss: 92.0511\n",
      "Iteration: 83200\n",
      "Gradient: [  0.1279  -9.576  -13.3459  53.268  -21.033 ]\n",
      "Weights: [-4.7636  0.6666 -1.1689  0.0967  0.1458]\n",
      "MSE loss: 92.0226\n",
      "Iteration: 83300\n",
      "Gradient: [  -1.2145  -21.4732  -62.1277   15.2764 -137.6598]\n",
      "Weights: [-4.7771  0.6826 -1.1713  0.0973  0.1456]\n",
      "MSE loss: 91.8175\n",
      "Iteration: 83400\n",
      "Gradient: [  2.0303 -25.6563  18.2572 -10.525  153.5508]\n",
      "Weights: [-4.7828  0.6877 -1.1671  0.0967  0.1452]\n",
      "MSE loss: 91.8699\n",
      "Iteration: 83500\n",
      "Gradient: [ -13.9295  -15.2401  -71.0539   -1.1903 -280.8429]\n",
      "Weights: [-4.8039  0.6954 -1.1686  0.0961  0.1452]\n",
      "MSE loss: 92.2974\n",
      "Iteration: 83600\n",
      "Gradient: [-11.298   -5.9019 -27.3548  19.9621   5.4124]\n",
      "Weights: [-4.7911  0.6969 -1.1687  0.0957  0.145 ]\n",
      "MSE loss: 92.0381\n",
      "Iteration: 83700\n",
      "Gradient: [-12.817   -1.9116 -33.1235 -81.5132  -4.9704]\n",
      "Weights: [-4.7963  0.6894 -1.168   0.0961  0.1451]\n",
      "MSE loss: 92.4586\n",
      "Iteration: 83800\n",
      "Gradient: [  6.3967  22.2509  26.077  -20.9518 490.2425]\n",
      "Weights: [-4.7664  0.6872 -1.167   0.0964  0.1451]\n",
      "MSE loss: 92.3793\n",
      "Iteration: 83900\n",
      "Gradient: [ -8.908  -12.9472 -17.0817  11.1317  35.661 ]\n",
      "Weights: [-4.7846  0.6889 -1.1678  0.0964  0.1451]\n",
      "MSE loss: 91.9165\n",
      "Iteration: 84000\n",
      "Gradient: [ -4.6248  15.6628  42.3092 139.9734  74.6056]\n",
      "Weights: [-4.7848  0.6973 -1.1679  0.0957  0.1454]\n",
      "MSE loss: 92.0981\n",
      "Iteration: 84100\n",
      "Gradient: [  0.7092 -17.3899   5.436    5.2486 -34.7447]\n",
      "Weights: [-4.7928  0.6834 -1.1692  0.0966  0.1454]\n",
      "MSE loss: 92.3632\n",
      "Iteration: 84200\n",
      "Gradient: [ 10.4332 -10.4176 -21.859  -26.9329 -99.8493]\n",
      "Weights: [-4.7854  0.6863 -1.1677  0.0967  0.1451]\n",
      "MSE loss: 91.8938\n",
      "Iteration: 84300\n",
      "Gradient: [  7.1288  -0.6641  -3.738  152.3169 320.7629]\n",
      "Weights: [-4.7889  0.6884 -1.1648  0.0967  0.1449]\n",
      "MSE loss: 91.8827\n",
      "Iteration: 84400\n",
      "Gradient: [ 9.8803 12.7891 35.7839 27.6624 84.1368]\n",
      "Weights: [-4.7794  0.6907 -1.1665  0.0972  0.1449]\n",
      "MSE loss: 92.1652\n",
      "Iteration: 84500\n",
      "Gradient: [-13.1049   6.1858  34.6464  21.8768   3.9236]\n",
      "Weights: [-4.7997  0.689  -1.1675  0.0978  0.1448]\n",
      "MSE loss: 92.1124\n",
      "Iteration: 84600\n",
      "Gradient: [ -1.9893   3.4241 -18.1704 104.3202 149.3065]\n",
      "Weights: [-4.7905  0.688  -1.1683  0.0984  0.1448]\n",
      "MSE loss: 91.8476\n",
      "Iteration: 84700\n",
      "Gradient: [   1.2815  -13.6935  -64.1527  -95.835  -378.261 ]\n",
      "Weights: [-4.7916  0.6789 -1.1672  0.0977  0.145 ]\n",
      "MSE loss: 92.2327\n",
      "Iteration: 84800\n",
      "Gradient: [ 12.5308  -3.5399 114.2797   3.2495 -30.6067]\n",
      "Weights: [-4.777   0.6929 -1.1707  0.0974  0.1452]\n",
      "MSE loss: 92.0653\n",
      "Iteration: 84900\n",
      "Gradient: [   1.3196    9.7195   -9.0611   -7.8596 -263.6256]\n",
      "Weights: [-4.7786  0.6828 -1.172   0.0977  0.1453]\n",
      "MSE loss: 91.9242\n",
      "Iteration: 85000\n",
      "Gradient: [  11.7307  -10.0533  -46.9076 -183.5443 -284.7408]\n",
      "Weights: [-4.7848  0.6805 -1.1723  0.098   0.1455]\n",
      "MSE loss: 92.0127\n",
      "Iteration: 85100\n",
      "Gradient: [ 11.14    14.6686  48.4322  80.7701 147.1204]\n",
      "Weights: [-4.7748  0.693  -1.1729  0.0974  0.1455]\n",
      "MSE loss: 92.087\n",
      "Iteration: 85200\n",
      "Gradient: [  9.5582  -0.0647  54.9325 -43.4104 -19.3942]\n",
      "Weights: [-4.7731  0.6987 -1.1745  0.097   0.1454]\n",
      "MSE loss: 92.1795\n",
      "Iteration: 85300\n",
      "Gradient: [  7.4366 -23.1761 -34.3689 -20.858  -11.3444]\n",
      "Weights: [-4.7832  0.6834 -1.1722  0.0975  0.1455]\n",
      "MSE loss: 91.9681\n",
      "Iteration: 85400\n",
      "Gradient: [ -4.1749 -16.9382  -3.692  -61.8703 -22.8824]\n",
      "Weights: [-4.7798  0.6857 -1.171   0.0969  0.1457]\n",
      "MSE loss: 91.8261\n",
      "Iteration: 85500\n",
      "Gradient: [ -6.232    0.1924 -40.5551 -16.915  105.0518]\n",
      "Weights: [-4.7821  0.6912 -1.1721  0.0959  0.1458]\n",
      "MSE loss: 91.8345\n",
      "Iteration: 85600\n",
      "Gradient: [ -13.8282   -8.2834   10.2871 -132.7289   48.4149]\n",
      "Weights: [-4.7827  0.6846 -1.1697  0.0959  0.1458]\n",
      "MSE loss: 91.876\n",
      "Iteration: 85700\n",
      "Gradient: [ -0.6267   9.0286 -27.5419 -12.6196 242.7624]\n",
      "Weights: [-4.7678  0.6732 -1.1692  0.0964  0.146 ]\n",
      "MSE loss: 91.8827\n",
      "Iteration: 85800\n",
      "Gradient: [   0.3195   16.9646  -20.4754  137.7002 -238.8146]\n",
      "Weights: [-4.7835  0.6919 -1.1727  0.0968  0.1459]\n",
      "MSE loss: 91.9933\n",
      "Iteration: 85900\n",
      "Gradient: [  1.6587 -17.2078  19.2798   8.3604 -41.6412]\n",
      "Weights: [-4.7981  0.6978 -1.1745  0.0965  0.1459]\n",
      "MSE loss: 91.9788\n",
      "Iteration: 86000\n",
      "Gradient: [  -1.5923    2.1054    7.4088  -44.5882 -511.8013]\n",
      "Weights: [-4.788   0.6911 -1.1753  0.0964  0.1462]\n",
      "MSE loss: 91.9343\n",
      "Iteration: 86100\n",
      "Gradient: [   0.6159  -14.2376   10.5311    5.8574 -166.7514]\n",
      "Weights: [-4.7819  0.6906 -1.1771  0.0969  0.1461]\n",
      "MSE loss: 91.8666\n",
      "Iteration: 86200\n",
      "Gradient: [-10.5888 -15.489  -22.5955 -47.0485 -77.4976]\n",
      "Weights: [-4.7823  0.6845 -1.177   0.0973  0.1462]\n",
      "MSE loss: 92.0723\n",
      "Iteration: 86300\n",
      "Gradient: [-11.3985  12.0452  12.0233  -8.1319   9.7204]\n",
      "Weights: [-4.7773  0.6868 -1.1774  0.0975  0.1462]\n",
      "MSE loss: 91.849\n",
      "Iteration: 86400\n",
      "Gradient: [  4.8378  -8.388   -0.6889 -48.4206 -23.366 ]\n",
      "Weights: [-4.7615  0.6811 -1.1796  0.0977  0.1463]\n",
      "MSE loss: 91.9784\n",
      "Iteration: 86500\n",
      "Gradient: [-10.4014 -17.7445  15.027  -44.435  -95.5568]\n",
      "Weights: [-4.7733  0.6822 -1.1783  0.0982  0.146 ]\n",
      "MSE loss: 91.9577\n",
      "Iteration: 86600\n",
      "Gradient: [ -6.7937  -2.9399   7.695  -29.9334  25.0847]\n",
      "Weights: [-4.7788  0.6778 -1.1752  0.0992  0.1457]\n",
      "MSE loss: 91.9343\n",
      "Iteration: 86700\n",
      "Gradient: [   8.154    -4.0298   -8.9023   36.8858 -178.8812]\n",
      "Weights: [-4.7722  0.6769 -1.1725  0.0991  0.1454]\n",
      "MSE loss: 91.8391\n",
      "Iteration: 86800\n",
      "Gradient: [ -7.7386 -16.7246   8.9214 -20.016  -32.7649]\n",
      "Weights: [-4.7955  0.6929 -1.1731  0.0986  0.1453]\n",
      "MSE loss: 91.946\n",
      "Iteration: 86900\n",
      "Gradient: [ -0.5803  -7.6811  -5.7963 -43.8121  60.4032]\n",
      "Weights: [-4.7718  0.6793 -1.1748  0.0994  0.1454]\n",
      "MSE loss: 91.8145\n",
      "Iteration: 87000\n",
      "Gradient: [ -5.5043  -0.3718 -32.0103 -83.7205 -66.5198]\n",
      "Weights: [-4.7839  0.6819 -1.1741  0.0991  0.1453]\n",
      "MSE loss: 91.9795\n",
      "Iteration: 87100\n",
      "Gradient: [ 1.098   9.7483 34.4164 -6.505  78.1498]\n",
      "Weights: [-4.7774  0.6697 -1.1682  0.0986  0.1452]\n",
      "MSE loss: 91.9439\n",
      "Iteration: 87200\n",
      "Gradient: [ -5.4056  -4.8844 -51.0271 -43.4615  59.6738]\n",
      "Weights: [-4.7676  0.6712 -1.1696  0.099   0.1452]\n",
      "MSE loss: 91.8349\n",
      "Iteration: 87300\n",
      "Gradient: [  6.5067 -25.2986 -49.8831  51.1842 -93.1651]\n",
      "Weights: [-4.7836  0.6855 -1.171   0.0992  0.1449]\n",
      "MSE loss: 91.7942\n",
      "Iteration: 87400\n",
      "Gradient: [-11.4394   0.5403  -9.2725  30.2868  96.4366]\n",
      "Weights: [-4.7754  0.6884 -1.1704  0.0981  0.1449]\n",
      "MSE loss: 91.9018\n",
      "Iteration: 87500\n",
      "Gradient: [  1.5949  19.8086  32.1534 136.2665 185.294 ]\n",
      "Weights: [-4.7827  0.6864 -1.1718  0.0988  0.145 ]\n",
      "MSE loss: 91.793\n",
      "Iteration: 87600\n",
      "Gradient: [  0.4373   0.3327  81.8181  51.5018 -42.5551]\n",
      "Weights: [-4.7703  0.6834 -1.1733  0.099   0.1452]\n",
      "MSE loss: 91.8811\n",
      "Iteration: 87700\n",
      "Gradient: [-3.3337  4.8904 10.6798  0.6654 67.7334]\n",
      "Weights: [-4.7908  0.6766 -1.1729  0.0996  0.1456]\n",
      "MSE loss: 92.2526\n",
      "Iteration: 87800\n",
      "Gradient: [-12.4107   2.1125 -52.2541 -64.9447 -96.9229]\n",
      "Weights: [-4.7841  0.6868 -1.1756  0.099   0.1453]\n",
      "MSE loss: 91.8998\n",
      "Iteration: 87900\n",
      "Gradient: [ -5.4964   6.1725  60.6873 -74.1342  52.5542]\n",
      "Weights: [-4.7889  0.6999 -1.1757  0.0988  0.1452]\n",
      "MSE loss: 91.8737\n",
      "Iteration: 88000\n",
      "Gradient: [  -0.9322   -3.5203  -15.1532 -107.3706  -35.1056]\n",
      "Weights: [-4.7851  0.6936 -1.1774  0.0992  0.1451]\n",
      "MSE loss: 91.8477\n",
      "Iteration: 88100\n",
      "Gradient: [   3.9956  -22.5598  -53.6699 -115.513  -227.9282]\n",
      "Weights: [-4.7811  0.6913 -1.179   0.1003  0.1451]\n",
      "MSE loss: 91.789\n",
      "Iteration: 88200\n",
      "Gradient: [   0.972    -5.454   -79.8652 -117.1383   53.6098]\n",
      "Weights: [-4.7946  0.6949 -1.1789  0.1002  0.1451]\n",
      "MSE loss: 91.9878\n",
      "Iteration: 88300\n",
      "Gradient: [   1.862    12.8154  -31.1118 -138.9396  318.5764]\n",
      "Weights: [-4.7883  0.6983 -1.1779  0.1     0.145 ]\n",
      "MSE loss: 91.78\n",
      "Iteration: 88400\n",
      "Gradient: [ -0.9639   1.1859  34.9709  36.8192 -46.1267]\n",
      "Weights: [-4.7751  0.6873 -1.1791  0.1014  0.145 ]\n",
      "MSE loss: 91.8106\n",
      "Iteration: 88500\n",
      "Gradient: [  8.1991  -1.1187  49.9039  23.6771 410.4694]\n",
      "Weights: [-4.7786  0.6924 -1.1776  0.101   0.1449]\n",
      "MSE loss: 91.8692\n",
      "Iteration: 88600\n",
      "Gradient: [-11.4332  -2.5273  41.3416  68.831   90.7697]\n",
      "Weights: [-4.7841  0.6938 -1.178   0.1005  0.1449]\n",
      "MSE loss: 91.7615\n",
      "Iteration: 88700\n",
      "Gradient: [  4.3297 -17.667  -36.2891  26.3121 -75.8864]\n",
      "Weights: [-4.7729  0.686  -1.1784  0.1009  0.1448]\n",
      "MSE loss: 91.8305\n",
      "Iteration: 88800\n",
      "Gradient: [  -0.2066   -5.402    40.7632  142.1552 -103.631 ]\n",
      "Weights: [-4.7876  0.6942 -1.1796  0.1015  0.1447]\n",
      "MSE loss: 91.7922\n",
      "Iteration: 88900\n",
      "Gradient: [ -0.5462 -11.585  -26.8511  13.1014 302.0846]\n",
      "Weights: [-4.79    0.709  -1.1781  0.1006  0.1443]\n",
      "MSE loss: 91.9893\n",
      "Iteration: 89000\n",
      "Gradient: [  -2.0501   10.3373  -16.8502  -79.6715 -108.52  ]\n",
      "Weights: [-4.8176  0.712  -1.1766  0.1004  0.1443]\n",
      "MSE loss: 92.3045\n",
      "Iteration: 89100\n",
      "Gradient: [  -3.3296    0.39     -3.1594 -117.4363 -115.084 ]\n",
      "Weights: [-4.8095  0.7149 -1.1784  0.1003  0.1442]\n",
      "MSE loss: 92.0397\n",
      "Iteration: 89200\n",
      "Gradient: [  -4.012    -4.4084  -10.4529 -102.2626  280.3149]\n",
      "Weights: [-4.8034  0.7094 -1.179   0.101   0.1443]\n",
      "MSE loss: 91.9011\n",
      "Iteration: 89300\n",
      "Gradient: [  6.0418  -7.7755  17.6008 -41.7247 -81.7967]\n",
      "Weights: [-4.7884  0.7058 -1.1811  0.102   0.1443]\n",
      "MSE loss: 91.8158\n",
      "Iteration: 89400\n",
      "Gradient: [   6.0642    8.3237    2.4664   -7.6067 -262.1005]\n",
      "Weights: [-4.7671  0.6944 -1.1822  0.102   0.1445]\n",
      "MSE loss: 91.9816\n",
      "Iteration: 89500\n",
      "Gradient: [   8.8142  -18.1127  -32.4358 -211.2091  105.2442]\n",
      "Weights: [-4.7784  0.6874 -1.1797  0.1014  0.1447]\n",
      "MSE loss: 91.9322\n",
      "Iteration: 89600\n",
      "Gradient: [   9.3455    4.8898   68.7153  -13.6809 -332.6511]\n",
      "Weights: [-4.7654  0.6876 -1.1834  0.1025  0.1449]\n",
      "MSE loss: 91.8517\n",
      "Iteration: 89700\n",
      "Gradient: [ -15.0064  -14.7384   -1.9583  -67.2515 -165.3879]\n",
      "Weights: [-4.7846  0.6908 -1.1824  0.1027  0.1449]\n",
      "MSE loss: 91.7794\n",
      "Iteration: 89800\n",
      "Gradient: [ -1.9147  -0.1415 -43.6194  -1.2401 -21.1353]\n",
      "Weights: [-4.7765  0.6985 -1.1845  0.1021  0.1447]\n",
      "MSE loss: 91.7918\n",
      "Iteration: 89900\n",
      "Gradient: [-12.3009  25.0082  21.1299  47.9445   9.6672]\n",
      "Weights: [-4.7778  0.6968 -1.1853  0.1031  0.1447]\n",
      "MSE loss: 91.7345\n",
      "Iteration: 90000\n",
      "Gradient: [  1.6426  10.9176  34.5471  11.2852 210.4801]\n",
      "Weights: [-4.779   0.6908 -1.1837  0.1038  0.1447]\n",
      "MSE loss: 91.7736\n",
      "Iteration: 90100\n",
      "Gradient: [  3.7351  18.2721  13.3638 -23.2198 329.8827]\n",
      "Weights: [-4.7617  0.6808 -1.1835  0.104   0.1448]\n",
      "MSE loss: 91.9496\n",
      "Iteration: 90200\n",
      "Gradient: [ -7.4564   2.1724 -13.2384  56.1946  70.9879]\n",
      "Weights: [-4.7754  0.6854 -1.1838  0.1033  0.1448]\n",
      "MSE loss: 91.7877\n",
      "Iteration: 90300\n",
      "Gradient: [  -7.4226   -0.3915  -24.3386   16.0117 -106.4183]\n",
      "Weights: [-4.7659  0.6811 -1.1852  0.1044  0.1447]\n",
      "MSE loss: 91.817\n",
      "Iteration: 90400\n",
      "Gradient: [ -1.4987  14.3933  -6.4935 103.7091 133.5762]\n",
      "Weights: [-4.7766  0.6796 -1.1821  0.1055  0.1444]\n",
      "MSE loss: 91.8087\n",
      "Iteration: 90500\n",
      "Gradient: [  -5.3386   -0.725    -7.0364 -128.9597  -61.3453]\n",
      "Weights: [-4.7717  0.671  -1.181   0.1051  0.1441]\n",
      "MSE loss: 92.2502\n",
      "Iteration: 90600\n",
      "Gradient: [ -3.8758  -7.3201  17.8254 104.4893 -62.86  ]\n",
      "Weights: [-4.7799  0.6802 -1.1811  0.1058  0.1441]\n",
      "MSE loss: 91.7828\n",
      "Iteration: 90700\n",
      "Gradient: [  -8.4356   12.9424    0.5945 -120.6112   69.9928]\n",
      "Weights: [-4.7755  0.6893 -1.1798  0.1047  0.1439]\n",
      "MSE loss: 91.8561\n",
      "Iteration: 90800\n",
      "Gradient: [ -2.6878  15.67   -11.3196  27.313  146.7782]\n",
      "Weights: [-4.7825  0.6906 -1.1808  0.1052  0.1438]\n",
      "MSE loss: 91.7419\n",
      "Iteration: 90900\n",
      "Gradient: [   1.9033   -3.0679    1.0073   -4.4055 -382.0642]\n",
      "Weights: [-4.7749  0.6864 -1.1796  0.1044  0.144 ]\n",
      "MSE loss: 91.749\n",
      "Iteration: 91000\n",
      "Gradient: [ 6.4664 14.4085 -8.3658 19.1578 62.8098]\n",
      "Weights: [-4.7682  0.6874 -1.1823  0.1044  0.1441]\n",
      "MSE loss: 91.8082\n",
      "Iteration: 91100\n",
      "Gradient: [  -4.579    19.5473  -38.3715    6.3188 -195.5094]\n",
      "Weights: [-4.7753  0.6836 -1.1818  0.1046  0.1442]\n",
      "MSE loss: 91.725\n",
      "Iteration: 91200\n",
      "Gradient: [ -2.6139  -1.023  -18.8773  -2.6591 -19.1151]\n",
      "Weights: [-4.7785  0.6857 -1.185   0.1046  0.1443]\n",
      "MSE loss: 92.0515\n",
      "Iteration: 91300\n",
      "Gradient: [  3.9557  -0.1934   8.9513  21.1521 -50.6527]\n",
      "Weights: [-4.7694  0.6878 -1.1863  0.1049  0.1447]\n",
      "MSE loss: 91.819\n",
      "Iteration: 91400\n",
      "Gradient: [  1.0753   0.5852 -45.0311  51.9373 103.699 ]\n",
      "Weights: [-4.7629  0.6885 -1.1864  0.1037  0.1447]\n",
      "MSE loss: 91.8844\n",
      "Iteration: 91500\n",
      "Gradient: [  2.0087  18.3911 -24.5643  36.1703 463.6011]\n",
      "Weights: [-4.7805  0.6936 -1.1834  0.1032  0.1448]\n",
      "MSE loss: 91.7731\n",
      "Iteration: 91600\n",
      "Gradient: [  0.4186  19.002   26.9996  14.1986 -44.9009]\n",
      "Weights: [-4.7945  0.7083 -1.188   0.1032  0.1447]\n",
      "MSE loss: 91.7554\n",
      "Iteration: 91700\n",
      "Gradient: [ -6.5655  13.8163  -0.868  110.0856 160.6556]\n",
      "Weights: [-4.784   0.7099 -1.1901  0.1029  0.145 ]\n",
      "MSE loss: 91.7641\n",
      "Iteration: 91800\n",
      "Gradient: [ -7.1379  -4.3753  20.3829 121.7713  16.2128]\n",
      "Weights: [-4.7706  0.7013 -1.1931  0.1035  0.1452]\n",
      "MSE loss: 91.8038\n",
      "Iteration: 91900\n",
      "Gradient: [ 2.98000e-02 -8.41630e+00  2.86858e+01 -6.93045e+01 -3.09256e+02]\n",
      "Weights: [-4.7658  0.6958 -1.1955  0.1046  0.1452]\n",
      "MSE loss: 91.879\n",
      "Iteration: 92000\n",
      "Gradient: [ 7.177300e+00 -1.600000e-02  2.208630e+01  6.161290e+01  2.207771e+02]\n",
      "Weights: [-4.7698  0.7005 -1.195   0.1045  0.1452]\n",
      "MSE loss: 91.7994\n",
      "Iteration: 92100\n",
      "Gradient: [ 2.13440e+00 -7.41000e-02  1.66167e+01  7.50714e+01 -6.37550e+00]\n",
      "Weights: [-4.7849  0.7129 -1.1975  0.1054  0.1449]\n",
      "MSE loss: 91.6888\n",
      "Iteration: 92200\n",
      "Gradient: [-7.214   1.4532  6.3339 15.1566 52.3392]\n",
      "Weights: [-4.7956  0.7208 -1.2006  0.1052  0.1451]\n",
      "MSE loss: 91.7211\n",
      "Iteration: 92300\n",
      "Gradient: [ -7.0463   3.2832 -17.7248 -32.0075 161.7857]\n",
      "Weights: [-4.8005  0.7375 -1.2055  0.1043  0.1452]\n",
      "MSE loss: 91.7425\n",
      "Iteration: 92400\n",
      "Gradient: [  -4.9355  -16.0057  -15.9439  -12.4258 -141.376 ]\n",
      "Weights: [-4.7974  0.7375 -1.2042  0.1046  0.145 ]\n",
      "MSE loss: 91.73\n",
      "Iteration: 92500\n",
      "Gradient: [-13.0406 -11.9891  -4.985  -11.218   27.5809]\n",
      "Weights: [-4.812   0.7384 -1.2059  0.1052  0.1451]\n",
      "MSE loss: 91.9215\n",
      "Iteration: 92600\n",
      "Gradient: [  4.8442  11.7633  45.9928 -23.3563 264.0064]\n",
      "Weights: [-4.8004  0.7424 -1.2078  0.1061  0.1449]\n",
      "MSE loss: 91.8052\n",
      "Iteration: 92700\n",
      "Gradient: [  9.7251   3.1939   1.3612  64.8459 157.1926]\n",
      "Weights: [-4.7895  0.7342 -1.2089  0.1069  0.145 ]\n",
      "MSE loss: 91.7353\n",
      "Iteration: 92800\n",
      "Gradient: [   3.6792   24.0803   32.02     50.3223 -185.4834]\n",
      "Weights: [-4.8043  0.7362 -1.2064  0.1069  0.1449]\n",
      "MSE loss: 91.7888\n",
      "Iteration: 92900\n",
      "Gradient: [ -4.6669  -1.9013 -20.2601  57.3472  98.5843]\n",
      "Weights: [-4.8018  0.7332 -1.2071  0.1071  0.1448]\n",
      "MSE loss: 91.6964\n",
      "Iteration: 93000\n",
      "Gradient: [-2.67000e-02 -2.44270e+00  3.43150e+01  7.03301e+01 -3.04552e+01]\n",
      "Weights: [-4.794   0.7281 -1.207   0.1069  0.1451]\n",
      "MSE loss: 91.6851\n",
      "Iteration: 93100\n",
      "Gradient: [  4.9656  31.4481  23.9816  -2.7595 108.9134]\n",
      "Weights: [-4.7846  0.7307 -1.2045  0.1066  0.1449]\n",
      "MSE loss: 92.3404\n",
      "Iteration: 93200\n",
      "Gradient: [   1.6805    3.1137  -24.2355   27.3537 -203.7195]\n",
      "Weights: [-4.7935  0.7352 -1.2099  0.1069  0.145 ]\n",
      "MSE loss: 91.6566\n",
      "Iteration: 93300\n",
      "Gradient: [  3.8819  -8.4744 -18.4837 119.779  172.5014]\n",
      "Weights: [-4.7909  0.7248 -1.2099  0.1083  0.145 ]\n",
      "MSE loss: 91.6695\n",
      "Iteration: 93400\n",
      "Gradient: [ -0.293    9.5374  -3.7358  -4.5715 225.7439]\n",
      "Weights: [-4.7825  0.7204 -1.2108  0.1086  0.145 ]\n",
      "MSE loss: 91.681\n",
      "Iteration: 93500\n",
      "Gradient: [   1.4993   13.1681  -28.723   -12.5362 -368.1805]\n",
      "Weights: [-4.7836  0.7328 -1.2138  0.1086  0.1449]\n",
      "MSE loss: 91.7211\n",
      "Iteration: 93600\n",
      "Gradient: [ -2.8121  -2.7291   1.6941 -92.9879  83.5789]\n",
      "Weights: [-4.7965  0.7246 -1.2119  0.109   0.1449]\n",
      "MSE loss: 91.8665\n",
      "Iteration: 93700\n",
      "Gradient: [ -13.7524  -31.2598  -19.3705   27.8686 -269.5075]\n",
      "Weights: [-4.7959  0.729  -1.215   0.1093  0.1449]\n",
      "MSE loss: 91.9018\n",
      "Iteration: 93800\n",
      "Gradient: [  -6.9914   -3.8321   16.7361   22.884  -100.4403]\n",
      "Weights: [-4.7945  0.7305 -1.2156  0.1098  0.1448]\n",
      "MSE loss: 91.7431\n",
      "Iteration: 93900\n",
      "Gradient: [ -2.604   -1.1194  10.9525  54.7615 372.4711]\n",
      "Weights: [-4.7985  0.7377 -1.2157  0.11    0.1447]\n",
      "MSE loss: 91.6761\n",
      "Iteration: 94000\n",
      "Gradient: [  0.8492  -4.9401 -23.4236 -39.4664 -22.0216]\n",
      "Weights: [-4.8103  0.7531 -1.2225  0.1095  0.1449]\n",
      "MSE loss: 91.8489\n",
      "Iteration: 94100\n",
      "Gradient: [  2.7698  -0.1546 -24.92   -65.0488  -5.6621]\n",
      "Weights: [-4.7973  0.7512 -1.2214  0.1096  0.1448]\n",
      "MSE loss: 91.6374\n",
      "Iteration: 94200\n",
      "Gradient: [  -5.4283    8.421     5.7371 -114.5551  232.5565]\n",
      "Weights: [-4.8096  0.7482 -1.2199  0.1093  0.1449]\n",
      "MSE loss: 91.8437\n",
      "Iteration: 94300\n",
      "Gradient: [   3.4257  -20.8652   13.1203 -164.4767   38.4109]\n",
      "Weights: [-4.7825  0.7345 -1.22    0.1104  0.1449]\n",
      "MSE loss: 91.6634\n",
      "Iteration: 94400\n",
      "Gradient: [ -4.3508  -2.704  -68.6426 -23.0876  -8.372 ]\n",
      "Weights: [-4.7812  0.7292 -1.2177  0.1101  0.145 ]\n",
      "MSE loss: 91.6763\n",
      "Iteration: 94500\n",
      "Gradient: [   6.9023  -14.6771    6.7528  -94.0834 -177.0336]\n",
      "Weights: [-4.8013  0.7379 -1.2176  0.1102  0.1448]\n",
      "MSE loss: 91.7074\n",
      "Iteration: 94600\n",
      "Gradient: [   5.9633   14.5895   10.683   -41.0798 -163.3437]\n",
      "Weights: [-4.8043  0.7499 -1.2207  0.1099  0.1448]\n",
      "MSE loss: 91.6432\n",
      "Iteration: 94700\n",
      "Gradient: [ 14.0912  21.3727 -24.1083 -14.7558 107.223 ]\n",
      "Weights: [-4.7865  0.7433 -1.2203  0.1097  0.1449]\n",
      "MSE loss: 91.7162\n",
      "Iteration: 94800\n",
      "Gradient: [  7.1926 -12.4306  -0.7875  52.7219 229.0992]\n",
      "Weights: [-4.7879  0.7473 -1.2216  0.1093  0.145 ]\n",
      "MSE loss: 91.7299\n",
      "Iteration: 94900\n",
      "Gradient: [  -1.1194  -11.0422  -22.8324  -86.2782 -218.5544]\n",
      "Weights: [-4.7799  0.7296 -1.2199  0.1097  0.1449]\n",
      "MSE loss: 91.9848\n",
      "Iteration: 95000\n",
      "Gradient: [ 10.1245  11.2026   6.4324 112.3448 -71.1712]\n",
      "Weights: [-4.7699  0.7241 -1.2172  0.1095  0.1451]\n",
      "MSE loss: 91.8333\n",
      "Iteration: 95100\n",
      "Gradient: [  -1.966    -9.4761   16.6544 -161.4435   65.5899]\n",
      "Weights: [-4.7963  0.7371 -1.2157  0.1089  0.1451]\n",
      "MSE loss: 91.6694\n",
      "Iteration: 95200\n",
      "Gradient: [  1.4683   3.7471  -7.1434  90.7778 -40.8493]\n",
      "Weights: [-4.7898  0.7301 -1.2136  0.1097  0.145 ]\n",
      "MSE loss: 91.8083\n",
      "Iteration: 95300\n",
      "Gradient: [  7.6535 -15.1696  10.4158 -38.1853 -49.579 ]\n",
      "Weights: [-4.7971  0.7295 -1.2124  0.1088  0.1448]\n",
      "MSE loss: 91.7804\n",
      "Iteration: 95400\n",
      "Gradient: [-17.4497  16.9353  16.1405   9.6998 182.6683]\n",
      "Weights: [-4.7941  0.7354 -1.2134  0.1084  0.145 ]\n",
      "MSE loss: 91.6542\n",
      "Iteration: 95500\n",
      "Gradient: [ -2.7032  -8.3919  -4.4679 -58.3998  43.8268]\n",
      "Weights: [-4.7936  0.7385 -1.2153  0.1087  0.1447]\n",
      "MSE loss: 91.6876\n",
      "Iteration: 95600\n",
      "Gradient: [ -1.2367  20.1415 -35.5353 -52.9371   7.2564]\n",
      "Weights: [-4.7983  0.7368 -1.2145  0.1093  0.1447]\n",
      "MSE loss: 91.6396\n",
      "Iteration: 95700\n",
      "Gradient: [ -4.4577  12.9036  11.1774 -65.5617 -65.8126]\n",
      "Weights: [-4.8038  0.7399 -1.2136  0.1091  0.1447]\n",
      "MSE loss: 91.6717\n",
      "Iteration: 95800\n",
      "Gradient: [  9.2639   1.6327  27.4892 -11.7267  51.303 ]\n",
      "Weights: [-4.8111  0.7464 -1.2146  0.1095  0.1446]\n",
      "MSE loss: 91.7661\n",
      "Iteration: 95900\n",
      "Gradient: [ -1.5349  -0.8492  17.8047  63.3572 129.9416]\n",
      "Weights: [-4.803   0.7411 -1.2126  0.1097  0.1444]\n",
      "MSE loss: 91.7065\n",
      "Iteration: 96000\n",
      "Gradient: [  2.6721  15.3173 -23.6315  71.0821  87.7722]\n",
      "Weights: [-4.8066  0.7428 -1.213   0.1097  0.1445]\n",
      "MSE loss: 91.8047\n",
      "Iteration: 96100\n",
      "Gradient: [  -7.8795  -11.2606    3.2592   53.2508 -182.8272]\n",
      "Weights: [-4.8025  0.7298 -1.2111  0.1095  0.1443]\n",
      "MSE loss: 91.9323\n",
      "Iteration: 96200\n",
      "Gradient: [-2.705  -3.3538 19.2443 58.531  -8.304 ]\n",
      "Weights: [-4.7851  0.724  -1.2102  0.1095  0.1444]\n",
      "MSE loss: 91.6226\n",
      "Iteration: 96300\n",
      "Gradient: [  1.6402 -35.5053  -2.5835  22.4894 284.0348]\n",
      "Weights: [-4.7926  0.7201 -1.2077  0.1093  0.1444]\n",
      "MSE loss: 91.7208\n",
      "Iteration: 96400\n",
      "Gradient: [  0.3589  -9.5478   4.9107 -73.2069  52.1147]\n",
      "Weights: [-4.7988  0.7239 -1.2058  0.1089  0.1442]\n",
      "MSE loss: 91.7908\n",
      "Iteration: 96500\n",
      "Gradient: [  0.9721   1.4529 -17.1758 -93.7833 228.7696]\n",
      "Weights: [-4.7861  0.7177 -1.2068  0.1094  0.1443]\n",
      "MSE loss: 91.6413\n",
      "Iteration: 96600\n",
      "Gradient: [  -0.7289   16.4247   -9.9603  -84.4694 -142.1517]\n",
      "Weights: [-4.7875  0.7164 -1.2041  0.1088  0.1443]\n",
      "MSE loss: 91.6463\n",
      "Iteration: 96700\n",
      "Gradient: [  -4.6831  -12.1936   -4.3986   -7.7471 -262.4489]\n",
      "Weights: [-4.7942  0.719  -1.2052  0.109   0.1443]\n",
      "MSE loss: 91.7199\n",
      "Iteration: 96800\n",
      "Gradient: [ 2.563000e-01  4.153600e+00  2.162190e+01 -2.809290e+01 -3.242846e+02]\n",
      "Weights: [-4.7857  0.7201 -1.2061  0.1092  0.1444]\n",
      "MSE loss: 91.6608\n",
      "Iteration: 96900\n",
      "Gradient: [  -0.4509    1.9837   10.9956 -102.0804  342.08  ]\n",
      "Weights: [-4.7945  0.7146 -1.2033  0.1095  0.1443]\n",
      "MSE loss: 91.7412\n",
      "Iteration: 97000\n",
      "Gradient: [  5.476    2.4603  54.9185 -18.4453 233.089 ]\n",
      "Weights: [-4.785   0.7169 -1.2052  0.1098  0.1441]\n",
      "MSE loss: 91.6245\n",
      "Iteration: 97100\n",
      "Gradient: [-3.292600e+00 -1.570000e-01  2.826210e+01 -1.711146e+02 -3.043733e+02]\n",
      "Weights: [-4.79    0.7269 -1.2062  0.1089  0.1444]\n",
      "MSE loss: 91.8183\n",
      "Iteration: 97200\n",
      "Gradient: [ -6.5238   4.1979   6.845  141.6218  23.8371]\n",
      "Weights: [-4.7879  0.7252 -1.205   0.1087  0.1442]\n",
      "MSE loss: 91.7144\n",
      "Iteration: 97300\n",
      "Gradient: [  0.4445   5.6313  11.7456 -59.7513  43.0322]\n",
      "Weights: [-4.8004  0.7347 -1.2059  0.1079  0.1444]\n",
      "MSE loss: 91.723\n",
      "Iteration: 97400\n",
      "Gradient: [  -6.5368    1.6147  -10.0921 -100.0853  365.1088]\n",
      "Weights: [-4.8016  0.7259 -1.2065  0.1087  0.1442]\n",
      "MSE loss: 91.8361\n",
      "Iteration: 97500\n",
      "Gradient: [  1.8262  10.8284  34.0259 -56.3605  88.7173]\n",
      "Weights: [-4.785   0.7149 -1.205   0.1092  0.1445]\n",
      "MSE loss: 91.6611\n",
      "Iteration: 97600\n",
      "Gradient: [-12.2216 -22.8797  13.3053 -45.2206 -35.3352]\n",
      "Weights: [-4.789   0.7158 -1.2072  0.1088  0.1444]\n",
      "MSE loss: 92.0221\n",
      "Iteration: 97700\n",
      "Gradient: [   2.052   -17.0896   19.0018   50.5798 -368.1977]\n",
      "Weights: [-4.7916  0.7191 -1.2071  0.1091  0.1445]\n",
      "MSE loss: 91.6862\n",
      "Iteration: 97800\n",
      "Gradient: [-14.5897   7.0417 -25.6546 -38.4828  57.15  ]\n",
      "Weights: [-4.8101  0.7263 -1.2088  0.1098  0.1444]\n",
      "MSE loss: 92.0989\n",
      "Iteration: 97900\n",
      "Gradient: [  2.1505  -6.6717  34.8006  80.3496 -58.6988]\n",
      "Weights: [-4.7918  0.7209 -1.2077  0.1098  0.1443]\n",
      "MSE loss: 91.6434\n",
      "Iteration: 98000\n",
      "Gradient: [  -5.4337  -26.8106    3.7111 -137.9983  142.5542]\n",
      "Weights: [-4.7908  0.7088 -1.2054  0.1099  0.1444]\n",
      "MSE loss: 91.947\n",
      "Iteration: 98100\n",
      "Gradient: [  14.071   -12.1949   -2.031   -38.7008 -212.6676]\n",
      "Weights: [-4.77    0.7025 -1.2078  0.1106  0.1444]\n",
      "MSE loss: 91.8242\n",
      "Iteration: 98200\n",
      "Gradient: [  -4.3778   -1.2248  -18.2328   67.184  -139.9226]\n",
      "Weights: [-4.7713  0.6984 -1.2042  0.1108  0.1444]\n",
      "MSE loss: 91.7654\n",
      "Iteration: 98300\n",
      "Gradient: [ -7.0487   6.7269   8.2147  93.503  188.1067]\n",
      "Weights: [-4.7892  0.7158 -1.205   0.1107  0.1441]\n",
      "MSE loss: 91.7224\n",
      "Iteration: 98400\n",
      "Gradient: [   6.2069   -2.6328   10.3244   12.9199 -155.759 ]\n",
      "Weights: [-4.8034  0.7243 -1.2054  0.11    0.1438]\n",
      "MSE loss: 91.7782\n",
      "Iteration: 98500\n",
      "Gradient: [ -7.9233  -1.6538 -33.4204  71.1638  54.2713]\n",
      "Weights: [-4.7872  0.7329 -1.2119  0.1104  0.1439]\n",
      "MSE loss: 91.6594\n",
      "Iteration: 98600\n",
      "Gradient: [ 2.886000e-01  9.808900e+00  1.134900e+00 -7.149670e+01 -4.739996e+02]\n",
      "Weights: [-4.794   0.733  -1.2134  0.1104  0.1438]\n",
      "MSE loss: 92.005\n",
      "Iteration: 98700\n",
      "Gradient: [  -8.0305  -21.2522    8.9175   20.0914 -312.5316]\n",
      "Weights: [-4.7952  0.7295 -1.2134  0.1115  0.1439]\n",
      "MSE loss: 91.6575\n",
      "Iteration: 98800\n",
      "Gradient: [ -5.8633  -3.3085  -3.3728  32.8675 -82.0696]\n",
      "Weights: [-4.7992  0.7305 -1.212   0.1115  0.144 ]\n",
      "MSE loss: 91.6289\n",
      "Iteration: 98900\n",
      "Gradient: [ -5.9924  -8.3285   1.4456 -17.9157 -63.2628]\n",
      "Weights: [-4.8079  0.732  -1.2116  0.1119  0.1438]\n",
      "MSE loss: 91.7772\n",
      "Iteration: 99000\n",
      "Gradient: [   3.9241    8.4906   35.8932   75.9071 -105.6417]\n",
      "Weights: [-4.7835  0.7398 -1.216   0.1113  0.1438]\n",
      "MSE loss: 91.8317\n",
      "Iteration: 99100\n",
      "Gradient: [  6.0395   8.9439   1.0763  11.489  221.4272]\n",
      "Weights: [-4.7992  0.7445 -1.2177  0.1124  0.1437]\n",
      "MSE loss: 91.5939\n",
      "Iteration: 99200\n",
      "Gradient: [ 2.8388 17.4591 10.0937 78.3166 52.8464]\n",
      "Weights: [-4.7831  0.7484 -1.2205  0.1124  0.1436]\n",
      "MSE loss: 92.0708\n",
      "Iteration: 99300\n",
      "Gradient: [  -2.9179  -11.6684  -29.1727  -84.0299 -245.9842]\n",
      "Weights: [-4.804   0.7497 -1.2214  0.1123  0.1437]\n",
      "MSE loss: 91.782\n",
      "Iteration: 99400\n",
      "Gradient: [   1.6937    6.0414   41.1309   48.0885 -276.1083]\n",
      "Weights: [-4.808   0.7451 -1.2205  0.1134  0.1438]\n",
      "MSE loss: 91.6519\n",
      "Iteration: 99500\n",
      "Gradient: [  2.5978 -15.7503 -11.0554 111.3982 -93.5994]\n",
      "Weights: [-4.7828  0.7391 -1.2209  0.1134  0.1436]\n",
      "MSE loss: 91.6931\n",
      "Iteration: 99600\n",
      "Gradient: [  0.6698  -6.7351  -9.1464 -44.1682 -31.3618]\n",
      "Weights: [-4.791   0.7332 -1.2195  0.1144  0.1435]\n",
      "MSE loss: 91.5567\n",
      "Iteration: 99700\n",
      "Gradient: [  -2.012   -11.9477  -12.4874  -20.8536 -103.6467]\n",
      "Weights: [-4.7888  0.7349 -1.2198  0.1144  0.1434]\n",
      "MSE loss: 91.5527\n",
      "Iteration: 99800\n",
      "Gradient: [   6.097    -7.0523   52.8999  -65.1015 -292.5402]\n",
      "Weights: [-4.7696  0.7255 -1.223   0.1155  0.1436]\n",
      "MSE loss: 91.7372\n",
      "Iteration: 99900\n",
      "Gradient: [  4.948   -2.5157   1.7634 -31.5757   4.9363]\n",
      "Weights: [-4.7969  0.7307 -1.2195  0.1152  0.1436]\n",
      "MSE loss: 91.6019\n",
      "Iteration: 100000\n",
      "Gradient: [ 2.4045 -0.2541 31.8254 37.4318 46.0217]\n",
      "Weights: [-4.7801  0.7351 -1.2209  0.1145  0.1435]\n",
      "MSE loss: 91.6891\n",
      "Iteration: 100100\n",
      "Gradient: [  -7.2771   -4.0259    8.7925  -12.8649 -119.741 ]\n",
      "Weights: [-4.8133  0.7447 -1.2218  0.1147  0.1437]\n",
      "MSE loss: 91.7997\n",
      "Iteration: 100200\n",
      "Gradient: [-3.367900e+00  1.349200e+00  1.064000e-01  8.835070e+01 -1.856997e+02]\n",
      "Weights: [-4.7995  0.7431 -1.2207  0.114   0.1435]\n",
      "MSE loss: 91.5522\n",
      "Iteration: 100300\n",
      "Gradient: [ -3.755   20.0201 -52.4017 -77.3486 144.2303]\n",
      "Weights: [-4.8     0.7456 -1.2215  0.1146  0.1433]\n",
      "MSE loss: 91.5528\n",
      "Iteration: 100400\n",
      "Gradient: [ -4.4015   6.5006 -63.0568 -77.913  -13.3681]\n",
      "Weights: [-4.8094  0.7395 -1.2233  0.1153  0.1435]\n",
      "MSE loss: 91.9819\n",
      "Iteration: 100500\n",
      "Gradient: [  3.0581   8.7456  21.2937  41.3023 344.149 ]\n",
      "Weights: [-4.7886  0.7395 -1.2225  0.1159  0.1433]\n",
      "MSE loss: 91.6334\n",
      "Iteration: 100600\n",
      "Gradient: [  1.917    7.7722  40.0474 -55.5469 -21.0304]\n",
      "Weights: [-4.7817  0.7201 -1.218   0.1166  0.1431]\n",
      "MSE loss: 91.5299\n",
      "Iteration: 100700\n",
      "Gradient: [   6.7236   -2.4292   -8.7403   24.2626 -124.9688]\n",
      "Weights: [-4.7678  0.7079 -1.217   0.1167  0.1431]\n",
      "MSE loss: 91.6804\n",
      "Iteration: 100800\n",
      "Gradient: [ 14.6994   4.9927   9.3901 -86.5547 109.4598]\n",
      "Weights: [-4.7593  0.7103 -1.2175  0.1167  0.1433]\n",
      "MSE loss: 91.9785\n",
      "Iteration: 100900\n",
      "Gradient: [   1.3468   -7.5451    2.1854   42.1517 -235.7002]\n",
      "Weights: [-4.7792  0.7064 -1.212   0.1159  0.1431]\n",
      "MSE loss: 91.6049\n",
      "Iteration: 101000\n",
      "Gradient: [  6.4625   0.1135  -7.4208 -47.2211  82.3814]\n",
      "Weights: [-4.7775  0.7063 -1.2098  0.116   0.1433]\n",
      "MSE loss: 91.9246\n",
      "Iteration: 101100\n",
      "Gradient: [  14.3927    6.8906  -14.6082   91.375  -114.8965]\n",
      "Weights: [-4.7867  0.7167 -1.2115  0.1153  0.1429]\n",
      "MSE loss: 91.533\n",
      "Iteration: 101200\n",
      "Gradient: [   3.9876  -20.0515   -3.9457   -5.4448 -160.797 ]\n",
      "Weights: [-4.7837  0.7089 -1.2095  0.115   0.1431]\n",
      "MSE loss: 91.5919\n",
      "Iteration: 101300\n",
      "Gradient: [  3.224   -9.614   18.7374  -0.1804 166.7669]\n",
      "Weights: [-4.7987  0.715  -1.208   0.1142  0.1432]\n",
      "MSE loss: 91.7537\n",
      "Iteration: 101400\n",
      "Gradient: [  -8.339    -2.5607  -34.7539   -0.2493 -196.8159]\n",
      "Weights: [-4.8083  0.7335 -1.2099  0.1127  0.1431]\n",
      "MSE loss: 91.7157\n",
      "Iteration: 101500\n",
      "Gradient: [  8.0496   0.3928  27.8224  74.9166 223.7226]\n",
      "Weights: [-4.7805  0.7222 -1.2081  0.1127  0.1433]\n",
      "MSE loss: 91.8799\n",
      "Iteration: 101600\n",
      "Gradient: [  -7.7483  -15.9639    9.0367  -81.4465 -182.9169]\n",
      "Weights: [-4.7887  0.7218 -1.2082  0.1132  0.143 ]\n",
      "MSE loss: 91.5653\n",
      "Iteration: 101700\n",
      "Gradient: [-11.3717  -9.1521   0.0622  29.9305 -54.8496]\n",
      "Weights: [-4.7902  0.7165 -1.21    0.1143  0.1431]\n",
      "MSE loss: 91.5744\n",
      "Iteration: 101800\n",
      "Gradient: [ 9.214300e+00 -2.456000e-01  1.625820e+01  4.803290e+01  2.466241e+02]\n",
      "Weights: [-4.7811  0.717  -1.2097  0.1146  0.143 ]\n",
      "MSE loss: 91.6966\n",
      "Iteration: 101900\n",
      "Gradient: [ -3.719   11.9404  54.6295 111.7393   5.4932]\n",
      "Weights: [-4.7828  0.7134 -1.2087  0.1146  0.1428]\n",
      "MSE loss: 91.5456\n",
      "Iteration: 102000\n",
      "Gradient: [  2.4258  16.0978 -14.7248 -34.3643 215.8382]\n",
      "Weights: [-4.7634  0.712  -1.2105  0.114   0.1429]\n",
      "MSE loss: 91.9203\n",
      "Iteration: 102100\n",
      "Gradient: [  -2.1948  -16.4247  -13.6001  -13.1414 -174.6461]\n",
      "Weights: [-4.7782  0.6981 -1.2061  0.1148  0.143 ]\n",
      "MSE loss: 91.6822\n",
      "Iteration: 102200\n",
      "Gradient: [  9.0883  21.0615  35.3154  28.7707 304.1513]\n",
      "Weights: [-4.7847  0.7077 -1.2038  0.1148  0.1428]\n",
      "MSE loss: 91.7385\n",
      "Iteration: 102300\n",
      "Gradient: [  2.8122   7.2414  30.2358 -79.0217  15.0669]\n",
      "Weights: [-4.7812  0.7064 -1.2043  0.1149  0.1426]\n",
      "MSE loss: 91.5745\n",
      "Iteration: 102400\n",
      "Gradient: [-1.522000e-01  1.572180e+01 -2.591680e+01 -5.917290e+01 -3.000895e+02]\n",
      "Weights: [-4.7854  0.7065 -1.2034  0.115   0.1423]\n",
      "MSE loss: 91.5809\n",
      "Iteration: 102500\n",
      "Gradient: [ -4.1278 -11.3185  -8.6682  84.5301 119.4969]\n",
      "Weights: [-4.8002  0.7107 -1.1983  0.1145  0.1422]\n",
      "MSE loss: 91.7925\n",
      "Iteration: 102600\n",
      "Gradient: [  0.4609  -7.1747  -7.134  -36.1075 227.5447]\n",
      "Weights: [-4.785   0.7185 -1.2021  0.1142  0.142 ]\n",
      "MSE loss: 91.7845\n",
      "Iteration: 102700\n",
      "Gradient: [  -3.4644  -14.2314   22.2414 -134.3804  173.4923]\n",
      "Weights: [-4.7857  0.7163 -1.2007  0.1139  0.142 ]\n",
      "MSE loss: 91.7192\n",
      "Iteration: 102800\n",
      "Gradient: [ -12.6711   -7.9119  -65.053    15.8866 -273.7253]\n",
      "Weights: [-4.7947  0.7041 -1.1986  0.1141  0.1421]\n",
      "MSE loss: 91.8057\n",
      "Iteration: 102900\n",
      "Gradient: [  9.0499  -6.6521  -1.2632 -69.982  231.7916]\n",
      "Weights: [-4.7896  0.6943 -1.195   0.1149  0.1419]\n",
      "MSE loss: 91.6791\n",
      "Iteration: 103000\n",
      "Gradient: [ -5.575  -12.7009 -18.3529 -33.5966  95.8065]\n",
      "Weights: [-4.7718  0.6907 -1.1932  0.114   0.1418]\n",
      "MSE loss: 91.669\n",
      "Iteration: 103100\n",
      "Gradient: [  6.2666  -1.1047  20.4493  20.7449 251.0645]\n",
      "Weights: [-4.7862  0.6992 -1.1954  0.1142  0.142 ]\n",
      "MSE loss: 91.5861\n",
      "Iteration: 103200\n",
      "Gradient: [ -5.0752  21.0265 -10.7906  54.0033  28.7472]\n",
      "Weights: [-4.7714  0.7002 -1.1975  0.1149  0.142 ]\n",
      "MSE loss: 91.8852\n",
      "Iteration: 103300\n",
      "Gradient: [  16.3079    5.3255   45.0329   19.7478 -103.7019]\n",
      "Weights: [-4.7718  0.6953 -1.1962  0.1153  0.1419]\n",
      "MSE loss: 91.8465\n",
      "Iteration: 103400\n",
      "Gradient: [   3.8402  -18.0537   -2.552    40.0819 -266.032 ]\n",
      "Weights: [-4.7792  0.6936 -1.1982  0.1152  0.142 ]\n",
      "MSE loss: 91.6287\n",
      "Iteration: 103500\n",
      "Gradient: [  -7.3423  -17.401    -7.5615 -118.5188  127.6161]\n",
      "Weights: [-4.7899  0.6973 -1.1989  0.1153  0.142 ]\n",
      "MSE loss: 91.746\n",
      "Iteration: 103600\n",
      "Gradient: [  -3.3129  -10.9941   13.279   -41.064  -208.4594]\n",
      "Weights: [-4.7943  0.6894 -1.1918  0.1148  0.142 ]\n",
      "MSE loss: 91.8442\n",
      "Iteration: 103700\n",
      "Gradient: [ -0.858    9.316    7.7602 -48.0048 -45.836 ]\n",
      "Weights: [-4.7792  0.6894 -1.1941  0.1137  0.1422]\n",
      "MSE loss: 91.6744\n",
      "Iteration: 103800\n",
      "Gradient: [   4.8871    2.0513   -5.3579 -142.3363  102.2364]\n",
      "Weights: [-4.7854  0.6912 -1.1928  0.1138  0.142 ]\n",
      "MSE loss: 91.688\n",
      "Iteration: 103900\n",
      "Gradient: [  -7.1855   22.5975   21.5433 -111.6124  -87.7568]\n",
      "Weights: [-4.7793  0.6878 -1.1914  0.1141  0.1418]\n",
      "MSE loss: 91.6199\n",
      "Iteration: 104000\n",
      "Gradient: [   7.2649   -3.0652    2.0116 -101.8962  166.8346]\n",
      "Weights: [-4.7807  0.689  -1.1918  0.1147  0.1418]\n",
      "MSE loss: 91.5913\n",
      "Iteration: 104100\n",
      "Gradient: [  -4.91     -4.0887    6.1978  -81.6215 -177.8269]\n",
      "Weights: [-4.7867  0.694  -1.1966  0.1144  0.142 ]\n",
      "MSE loss: 91.8517\n",
      "Iteration: 104200\n",
      "Gradient: [  2.6857   8.1194 -20.9312  56.1941 232.919 ]\n",
      "Weights: [-4.774   0.6987 -1.1981  0.1146  0.1422]\n",
      "MSE loss: 91.7123\n",
      "Iteration: 104300\n",
      "Gradient: [ 2.8345 21.8795 12.6582 75.2878 59.4229]\n",
      "Weights: [-4.7644  0.6917 -1.1991  0.1155  0.142 ]\n",
      "MSE loss: 91.6995\n",
      "Iteration: 104400\n",
      "Gradient: [ -8.4202   1.4638 -35.9789   6.9509  55.7001]\n",
      "Weights: [-4.7981  0.7032 -1.1996  0.1157  0.1419]\n",
      "MSE loss: 91.7871\n",
      "Iteration: 104500\n",
      "Gradient: [ -6.4778   5.4686  49.644   22.085  -40.4877]\n",
      "Weights: [-4.7881  0.7078 -1.2003  0.1152  0.1419]\n",
      "MSE loss: 91.5654\n",
      "Iteration: 104600\n",
      "Gradient: [  -3.851     4.1315  -16.681   -24.0201 -309.011 ]\n",
      "Weights: [-4.7875  0.7039 -1.201   0.1153  0.1421]\n",
      "MSE loss: 91.5611\n",
      "Iteration: 104700\n",
      "Gradient: [ 14.666  -11.5173  39.123   -3.642  245.0846]\n",
      "Weights: [-4.7684  0.7001 -1.1995  0.1155  0.1421]\n",
      "MSE loss: 92.0319\n",
      "Iteration: 104800\n",
      "Gradient: [  3.244   13.6328 -52.3538 -57.6212  31.4445]\n",
      "Weights: [-4.7817  0.6999 -1.202   0.1153  0.1424]\n",
      "MSE loss: 91.5561\n",
      "Iteration: 104900\n",
      "Gradient: [   4.6879  -11.937    16.7804  -56.1141 -134.3403]\n",
      "Weights: [-4.7802  0.6982 -1.1996  0.1149  0.142 ]\n",
      "MSE loss: 91.6415\n",
      "Iteration: 105000\n",
      "Gradient: [  7.9197  -0.9888  -9.8099  -7.9944 -88.175 ]\n",
      "Weights: [-4.7815  0.6962 -1.2006  0.1148  0.1421]\n",
      "MSE loss: 91.8925\n",
      "Iteration: 105100\n",
      "Gradient: [   1.9     -11.6917  -52.7503   59.2899 -106.4849]\n",
      "Weights: [-4.7746  0.6921 -1.2015  0.1157  0.1423]\n",
      "MSE loss: 91.6135\n",
      "Iteration: 105200\n",
      "Gradient: [  -3.5544  -15.5793  -32.5493  112.3353 -243.1057]\n",
      "Weights: [-4.773   0.6911 -1.1987  0.1157  0.1422]\n",
      "MSE loss: 91.6031\n",
      "Iteration: 105300\n",
      "Gradient: [ 1.9636 -0.7566 19.4834 -2.0886 17.382 ]\n",
      "Weights: [-4.7698  0.698  -1.2011  0.1152  0.1422]\n",
      "MSE loss: 91.6517\n",
      "Iteration: 105400\n",
      "Gradient: [  0.7721  -1.4389   6.7812  23.0908 -30.6524]\n",
      "Weights: [-4.7715  0.6962 -1.2005  0.1164  0.142 ]\n",
      "MSE loss: 91.6804\n",
      "Iteration: 105500\n",
      "Gradient: [  2.7986 -11.6299 -44.5755  27.5323 162.9168]\n",
      "Weights: [-4.7813  0.6879 -1.1999  0.1169  0.142 ]\n",
      "MSE loss: 91.7296\n",
      "Iteration: 105600\n",
      "Gradient: [ 15.6196  17.5838 -14.2302  58.5153  -3.4298]\n",
      "Weights: [-4.7698  0.6841 -1.1953  0.1169  0.1418]\n",
      "MSE loss: 91.7439\n",
      "Iteration: 105700\n",
      "Gradient: [  -6.0774  -13.6255  -40.6576  -78.7652 -122.4849]\n",
      "Weights: [-4.7823  0.6861 -1.1988  0.1172  0.1417]\n",
      "MSE loss: 91.8403\n",
      "Iteration: 105800\n",
      "Gradient: [ 6.7344 14.7542 -4.1659 21.6611 49.9505]\n",
      "Weights: [-4.7667  0.6918 -1.1996  0.1168  0.1417]\n",
      "MSE loss: 91.6858\n",
      "Iteration: 105900\n",
      "Gradient: [  -6.4408  -22.4705   -1.2041   12.3047 -208.6548]\n",
      "Weights: [-4.7881  0.6912 -1.1993  0.1162  0.142 ]\n",
      "MSE loss: 91.9367\n",
      "Iteration: 106000\n",
      "Gradient: [ -9.0982  -5.3283 -34.8976 -44.0784 129.5767]\n",
      "Weights: [-4.7721  0.6953 -1.1996  0.1159  0.1417]\n",
      "MSE loss: 91.6521\n",
      "Iteration: 106100\n",
      "Gradient: [ -4.3472  25.3401  19.3642  59.2903 263.5394]\n",
      "Weights: [-4.7825  0.6965 -1.1959  0.1166  0.1416]\n",
      "MSE loss: 91.7246\n",
      "Iteration: 106200\n",
      "Gradient: [-2.706000e-01  2.125700e+00 -1.397810e+01  5.281080e+01  3.129343e+02]\n",
      "Weights: [-4.7808  0.699  -1.1942  0.1158  0.1414]\n",
      "MSE loss: 91.7803\n",
      "Iteration: 106300\n",
      "Gradient: [  0.4084 -11.9778  19.1619  62.8598  73.3708]\n",
      "Weights: [-4.7827  0.6973 -1.1959  0.1164  0.1412]\n",
      "MSE loss: 91.5835\n",
      "Iteration: 106400\n",
      "Gradient: [  2.3672  15.3191 -13.2069 -21.8917 196.8355]\n",
      "Weights: [-4.7928  0.6996 -1.1949  0.1168  0.1412]\n",
      "MSE loss: 91.6393\n",
      "Iteration: 106500\n",
      "Gradient: [ -4.7799   8.869  -21.0406 -22.4385 -17.6434]\n",
      "Weights: [-4.7925  0.6969 -1.1969  0.1175  0.141 ]\n",
      "MSE loss: 91.7147\n",
      "Iteration: 106600\n",
      "Gradient: [  -6.922    22.5976   53.6632   60.7314 -269.2682]\n",
      "Weights: [-4.7711  0.691  -1.1964  0.1171  0.1412]\n",
      "MSE loss: 91.6178\n",
      "Iteration: 106700\n",
      "Gradient: [  -8.7736   -5.8002    3.1402   10.1042 -172.7057]\n",
      "Weights: [-4.7847  0.6855 -1.1964  0.1172  0.1416]\n",
      "MSE loss: 91.7613\n",
      "Iteration: 106800\n",
      "Gradient: [-12.9129   4.9854  13.7725 -26.0608 -92.0204]\n",
      "Weights: [-4.7676  0.675  -1.1936  0.1166  0.1418]\n",
      "MSE loss: 91.6097\n",
      "Iteration: 106900\n",
      "Gradient: [   0.784    23.7236   31.6225   -4.0791 -509.1629]\n",
      "Weights: [-4.7588  0.67   -1.1944  0.1172  0.1418]\n",
      "MSE loss: 91.6651\n",
      "Iteration: 107000\n",
      "Gradient: [   5.5465   11.3407   70.0391 -119.4167 -102.0813]\n",
      "Weights: [-4.7602  0.6757 -1.1969  0.1168  0.142 ]\n",
      "MSE loss: 91.6716\n",
      "Iteration: 107100\n",
      "Gradient: [   6.4873   -5.9925  -13.2689  -70.326  -214.8574]\n",
      "Weights: [-4.7674  0.6716 -1.1959  0.1171  0.1419]\n",
      "MSE loss: 91.7928\n",
      "Iteration: 107200\n",
      "Gradient: [ -8.0534   5.11   -25.241  -91.9747  96.9733]\n",
      "Weights: [-4.7692  0.6872 -1.1978  0.1168  0.1416]\n",
      "MSE loss: 91.5771\n",
      "Iteration: 107300\n",
      "Gradient: [   4.1478  -11.5562   -3.318     8.3391 -186.7573]\n",
      "Weights: [-4.792   0.6963 -1.1994  0.116   0.1418]\n",
      "MSE loss: 91.9096\n",
      "Iteration: 107400\n",
      "Gradient: [  2.1164 -27.5762   2.3595 -18.5307  -3.8339]\n",
      "Weights: [-4.7748  0.6922 -1.198   0.116   0.1419]\n",
      "MSE loss: 91.5781\n",
      "Iteration: 107500\n",
      "Gradient: [  5.9519   2.4264   9.4513  12.7248 106.0565]\n",
      "Weights: [-4.7673  0.6814 -1.1966  0.1171  0.1418]\n",
      "MSE loss: 91.6239\n",
      "Iteration: 107600\n",
      "Gradient: [   2.4768  -34.1515  -23.1281   19.8974 -236.3615]\n",
      "Weights: [-4.7767  0.6848 -1.1981  0.1176  0.1415]\n",
      "MSE loss: 91.645\n",
      "Iteration: 107700\n",
      "Gradient: [  -2.6972  -12.3036   23.8083 -111.6915  -46.0012]\n",
      "Weights: [-4.7912  0.6972 -1.2002  0.118   0.1415]\n",
      "MSE loss: 91.6422\n",
      "Iteration: 107800\n",
      "Gradient: [ -0.7655  -6.0184  61.1419  27.6867 168.9664]\n",
      "Weights: [-4.7616  0.6898 -1.2     0.1173  0.1418]\n",
      "MSE loss: 91.8491\n",
      "Iteration: 107900\n",
      "Gradient: [ -2.0931  -8.1149  10.7114 121.18   243.8442]\n",
      "Weights: [-4.7819  0.6913 -1.2033  0.1184  0.1417]\n",
      "MSE loss: 91.6634\n",
      "Iteration: 108000\n",
      "Gradient: [  2.0282  -7.6565 -31.1624 -73.3343 106.3288]\n",
      "Weights: [-4.7772  0.6899 -1.2015  0.1185  0.1415]\n",
      "MSE loss: 91.5513\n",
      "Iteration: 108100\n",
      "Gradient: [   2.3795   16.7518   49.2792   98.1321 -100.277 ]\n",
      "Weights: [-4.7554  0.6847 -1.1993  0.1183  0.1415]\n",
      "MSE loss: 92.0922\n",
      "Iteration: 108200\n",
      "Gradient: [  2.7891   3.5756 -17.9823  54.7597 174.054 ]\n",
      "Weights: [-4.7647  0.6817 -1.1994  0.1182  0.1417]\n",
      "MSE loss: 91.6156\n",
      "Iteration: 108300\n",
      "Gradient: [  2.6263  13.8744  11.917   36.2078 -89.6326]\n",
      "Weights: [-4.768   0.6923 -1.2022  0.1181  0.1415]\n",
      "MSE loss: 91.5857\n",
      "Iteration: 108400\n",
      "Gradient: [  3.6819   2.2251 -19.6026 -10.0008 110.4998]\n",
      "Weights: [-4.7719  0.6924 -1.1998  0.118   0.1413]\n",
      "MSE loss: 91.5656\n",
      "Iteration: 108500\n",
      "Gradient: [  8.2509  11.7107  27.4563  22.7008 158.455 ]\n",
      "Weights: [-4.7737  0.696  -1.2003  0.118   0.1414]\n",
      "MSE loss: 91.6576\n",
      "Iteration: 108600\n",
      "Gradient: [ -2.4918 -19.7126 -58.5662  65.8485 -41.5049]\n",
      "Weights: [-4.7704  0.6967 -1.2015  0.117   0.1414]\n",
      "MSE loss: 91.7014\n",
      "Iteration: 108700\n",
      "Gradient: [  3.5119  12.1629 -43.6934 -16.9986 331.1906]\n",
      "Weights: [-4.7833  0.705  -1.1996  0.1165  0.1417]\n",
      "MSE loss: 91.7257\n",
      "Iteration: 108800\n",
      "Gradient: [ 3.5209 -3.178   1.6668 26.016  57.8465]\n",
      "Weights: [-4.7875  0.702  -1.2014  0.1165  0.1417]\n",
      "MSE loss: 91.5946\n",
      "Iteration: 108900\n",
      "Gradient: [ -2.3981  12.8484  21.8889 -47.8802 252.1681]\n",
      "Weights: [-4.7749  0.7016 -1.2024  0.1168  0.1417]\n",
      "MSE loss: 91.5952\n",
      "Iteration: 109000\n",
      "Gradient: [  -3.06    -12.9408   19.649    71.9353 -318.2992]\n",
      "Weights: [-4.7676  0.693  -1.203   0.1179  0.1417]\n",
      "MSE loss: 91.5895\n",
      "Iteration: 109100\n",
      "Gradient: [-12.1344 -25.868   -6.6631  -4.4057 119.7843]\n",
      "Weights: [-4.7836  0.6849 -1.2013  0.1184  0.1419]\n",
      "MSE loss: 91.8531\n",
      "Iteration: 109200\n",
      "Gradient: [   4.2949   -9.4799  -19.4072  -26.8252 -195.4613]\n",
      "Weights: [-4.7631  0.6873 -1.2033  0.1188  0.1415]\n",
      "MSE loss: 91.6074\n",
      "Iteration: 109300\n",
      "Gradient: [-4.1481 -5.1262 -5.5906 78.8187 69.7492]\n",
      "Weights: [-4.7845  0.6891 -1.2     0.1187  0.1414]\n",
      "MSE loss: 91.6518\n",
      "Iteration: 109400\n",
      "Gradient: [ -2.6653  19.6889  -2.8138  55.2482 105.221 ]\n",
      "Weights: [-4.7746  0.6906 -1.1992  0.1189  0.1412]\n",
      "MSE loss: 91.5855\n",
      "Iteration: 109500\n",
      "Gradient: [-23.2667   2.0503 -15.0967 -68.9629  10.8741]\n",
      "Weights: [-4.8     0.6974 -1.2001  0.1201  0.141 ]\n",
      "MSE loss: 91.8626\n",
      "Iteration: 109600\n",
      "Gradient: [ -6.572   -2.2023  30.6064 -90.8303 110.5659]\n",
      "Weights: [-4.7859  0.6959 -1.202   0.1202  0.141 ]\n",
      "MSE loss: 91.5621\n",
      "Iteration: 109700\n",
      "Gradient: [ -2.9094   7.309  -13.5232 -53.3906 -74.5102]\n",
      "Weights: [-4.7824  0.6946 -1.203   0.1205  0.1408]\n",
      "MSE loss: 91.5121\n",
      "Iteration: 109800\n",
      "Gradient: [  1.4346  -0.3858   4.48   -51.5805 170.4996]\n",
      "Weights: [-4.7856  0.6996 -1.2025  0.1206  0.1407]\n",
      "MSE loss: 91.532\n",
      "Iteration: 109900\n",
      "Gradient: [ -3.8826 -20.6595 -10.2919   2.3986 -37.208 ]\n",
      "Weights: [-4.7814  0.6942 -1.202   0.1202  0.1407]\n",
      "MSE loss: 91.5887\n",
      "Iteration: 110000\n",
      "Gradient: [ -4.8385  -6.9029  15.1002 -35.0979 -79.9488]\n",
      "Weights: [-4.7685  0.6872 -1.2001  0.1197  0.1406]\n",
      "MSE loss: 91.6772\n",
      "Iteration: 110100\n",
      "Gradient: [   3.292     5.9961  -14.3087  -23.5626 -106.1286]\n",
      "Weights: [-4.7735  0.6903 -1.2007  0.1198  0.1408]\n",
      "MSE loss: 91.5275\n",
      "Iteration: 110200\n",
      "Gradient: [  4.0852  16.8746  54.3174 135.4651 207.8013]\n",
      "Weights: [-4.7717  0.6938 -1.1989  0.1199  0.1406]\n",
      "MSE loss: 91.7714\n",
      "Iteration: 110300\n",
      "Gradient: [  1.5465 -20.1143 -23.9746 -64.1807 126.4586]\n",
      "Weights: [-4.7842  0.6992 -1.2039  0.1208  0.1408]\n",
      "MSE loss: 91.5178\n",
      "Iteration: 110400\n",
      "Gradient: [ -2.4864   3.7691  41.1406 -29.2566  46.9056]\n",
      "Weights: [-4.7821  0.7028 -1.2052  0.1202  0.1411]\n",
      "MSE loss: 91.5791\n",
      "Iteration: 110500\n",
      "Gradient: [   1.3205   18.3272   13.9008   29.823  -221.1329]\n",
      "Weights: [-4.7886  0.7111 -1.2071  0.1208  0.141 ]\n",
      "MSE loss: 91.7446\n",
      "Iteration: 110600\n",
      "Gradient: [  5.6835  -7.492   21.3951 -38.7174 129.4377]\n",
      "Weights: [-4.7857  0.7124 -1.2073  0.1201  0.141 ]\n",
      "MSE loss: 91.6103\n",
      "Iteration: 110700\n",
      "Gradient: [  5.5959  -4.5649 -14.5989 -26.5543 143.5461]\n",
      "Weights: [-4.7943  0.7177 -1.2085  0.1201  0.1409]\n",
      "MSE loss: 91.5505\n",
      "Iteration: 110800\n",
      "Gradient: [  2.7217 -12.3982 -27.8813   6.2972 194.5945]\n",
      "Weights: [-4.7938  0.7106 -1.2078  0.12    0.141 ]\n",
      "MSE loss: 91.5536\n",
      "Iteration: 110900\n",
      "Gradient: [  -2.9688  -12.5676  -23.1172 -129.9271 -150.4477]\n",
      "Weights: [-4.7987  0.7032 -1.2031  0.1193  0.1411]\n",
      "MSE loss: 91.748\n",
      "Iteration: 111000\n",
      "Gradient: [ -1.0507 -23.7188  25.3518   7.0859  61.1842]\n",
      "Weights: [-4.787   0.7049 -1.2033  0.1188  0.141 ]\n",
      "MSE loss: 91.5411\n",
      "Iteration: 111100\n",
      "Gradient: [   4.9162   -0.9955   16.8213 -136.4072  175.8516]\n",
      "Weights: [-4.7743  0.6975 -1.2035  0.12    0.1409]\n",
      "MSE loss: 91.5452\n",
      "Iteration: 111200\n",
      "Gradient: [  2.7033   9.2704  12.2879 -17.8754 -59.1156]\n",
      "Weights: [-4.7674  0.6924 -1.203   0.1209  0.1408]\n",
      "MSE loss: 91.7626\n",
      "Iteration: 111300\n",
      "Gradient: [  6.0386   3.4775  -7.2661 -62.9699 -76.7588]\n",
      "Weights: [-4.7668  0.684  -1.2002  0.1206  0.1408]\n",
      "MSE loss: 91.5787\n",
      "Iteration: 111400\n",
      "Gradient: [ -8.162    5.59     7.0437 -51.6019 -58.5938]\n",
      "Weights: [-4.7851  0.6885 -1.2002  0.1207  0.1408]\n",
      "MSE loss: 91.6024\n",
      "Iteration: 111500\n",
      "Gradient: [ -4.8338  -8.1558 -20.5119  -2.3225  66.8079]\n",
      "Weights: [-4.7817  0.6941 -1.2001  0.1204  0.1406]\n",
      "MSE loss: 91.5355\n",
      "Iteration: 111600\n",
      "Gradient: [   2.7625  -13.3571  -36.5312   26.9715 -240.0447]\n",
      "Weights: [-4.7711  0.6904 -1.2032  0.1209  0.1407]\n",
      "MSE loss: 91.515\n",
      "Iteration: 111700\n",
      "Gradient: [  4.1703  13.4385  -2.9567  -3.3221 -46.3906]\n",
      "Weights: [-4.783   0.6974 -1.2046  0.121   0.1407]\n",
      "MSE loss: 91.5026\n",
      "Iteration: 111800\n",
      "Gradient: [   4.1954   12.7357   39.3722  -22.0667 -294.3653]\n",
      "Weights: [-4.7787  0.6942 -1.2036  0.1205  0.1408]\n",
      "MSE loss: 91.516\n",
      "Iteration: 111900\n",
      "Gradient: [ 5.40000e-03  1.52990e+01  1.59600e+00  3.87427e+01 -9.18749e+01]\n",
      "Weights: [-4.7709  0.6854 -1.2025  0.1207  0.1409]\n",
      "MSE loss: 91.5277\n",
      "Iteration: 112000\n",
      "Gradient: [   2.5659   11.7411   -6.7803  113.2509 -176.5372]\n",
      "Weights: [-4.7618  0.6807 -1.1992  0.12    0.1409]\n",
      "MSE loss: 91.6227\n",
      "Iteration: 112100\n",
      "Gradient: [ -0.0715  11.4203  10.7966  -6.4022 -30.2801]\n",
      "Weights: [-4.7673  0.6806 -1.1972  0.12    0.1406]\n",
      "MSE loss: 91.5511\n",
      "Iteration: 112200\n",
      "Gradient: [  1.8769   2.8766  19.4212 -84.4982 -34.2899]\n",
      "Weights: [-4.7813  0.6872 -1.1962  0.1195  0.1407]\n",
      "MSE loss: 91.5438\n",
      "Iteration: 112300\n",
      "Gradient: [  -4.6239    8.5787  -15.0914   73.5623 -274.9675]\n",
      "Weights: [-4.7845  0.6871 -1.1957  0.119   0.1406]\n",
      "MSE loss: 91.6667\n",
      "Iteration: 112400\n",
      "Gradient: [  -4.1539    9.8611  -28.7763  -65.3378 -239.2611]\n",
      "Weights: [-4.7848  0.6808 -1.1936  0.1192  0.1406]\n",
      "MSE loss: 91.7549\n",
      "Iteration: 112500\n",
      "Gradient: [ 17.5685   6.6692   2.5574  -3.9249 208.1164]\n",
      "Weights: [-4.7778  0.6801 -1.1943  0.1207  0.1407]\n",
      "MSE loss: 91.884\n",
      "Iteration: 112600\n",
      "Gradient: [ 10.3581  19.7007  35.7349  17.9955 -19.4569]\n",
      "Weights: [-4.7566  0.6695 -1.1947  0.1215  0.1404]\n",
      "MSE loss: 91.7567\n",
      "Iteration: 112700\n",
      "Gradient: [  7.1606   9.8459  17.585  -50.3714 173.0368]\n",
      "Weights: [-4.7707  0.6753 -1.1952  0.1216  0.1404]\n",
      "MSE loss: 91.6202\n",
      "Iteration: 112800\n",
      "Gradient: [  5.0947  -7.5314  12.0503  53.3192 187.902 ]\n",
      "Weights: [-4.7687  0.6747 -1.1958  0.1215  0.1403]\n",
      "MSE loss: 91.5309\n",
      "Iteration: 112900\n",
      "Gradient: [   4.3428    2.1517    4.5008   -0.2134 -128.9446]\n",
      "Weights: [-4.7738  0.6779 -1.1991  0.1215  0.1407]\n",
      "MSE loss: 91.5687\n",
      "Iteration: 113000\n",
      "Gradient: [  8.5047  -0.3634 -48.5444 -87.3963 -19.9133]\n",
      "Weights: [-4.7757  0.6778 -1.2004  0.122   0.1406]\n",
      "MSE loss: 91.6389\n",
      "Iteration: 113100\n",
      "Gradient: [ 6.3326  2.2152 40.5902 28.0945 60.2302]\n",
      "Weights: [-4.7754  0.6968 -1.2029  0.1219  0.1405]\n",
      "MSE loss: 91.7567\n",
      "Iteration: 113200\n",
      "Gradient: [ -1.4191  -9.4323 -15.0877 -28.6822 445.658 ]\n",
      "Weights: [-4.77    0.6924 -1.206   0.1224  0.1403]\n",
      "MSE loss: 91.528\n",
      "Iteration: 113300\n",
      "Gradient: [ -5.191    9.4434  17.7267  34.275  244.1464]\n",
      "Weights: [-4.7855  0.6927 -1.2053  0.1231  0.1403]\n",
      "MSE loss: 91.5801\n",
      "Iteration: 113400\n",
      "Gradient: [ 15.5834  -3.0533   3.0326 102.7111 -47.6231]\n",
      "Weights: [-4.7809  0.6973 -1.2052  0.1228  0.1402]\n",
      "MSE loss: 91.4769\n",
      "Iteration: 113500\n",
      "Gradient: [  18.3353    1.3392   -5.1681  -10.147  -318.8654]\n",
      "Weights: [-4.7796  0.6877 -1.2012  0.1226  0.14  ]\n",
      "MSE loss: 91.5406\n",
      "Iteration: 113600\n",
      "Gradient: [ -5.6528  -3.4349 -16.3381  62.0531 -39.1524]\n",
      "Weights: [-4.7704  0.6875 -1.2058  0.1239  0.1404]\n",
      "MSE loss: 91.5758\n",
      "Iteration: 113700\n",
      "Gradient: [-10.2636   8.0556  47.8371  59.4933  86.0454]\n",
      "Weights: [-4.7671  0.6841 -1.206   0.1239  0.1403]\n",
      "MSE loss: 91.491\n",
      "Iteration: 113800\n",
      "Gradient: [ -9.318  -34.8232  -8.0098 -98.3731 239.0465]\n",
      "Weights: [-4.77    0.6817 -1.2065  0.1245  0.1402]\n",
      "MSE loss: 91.5271\n",
      "Iteration: 113900\n",
      "Gradient: [ -8.3287  17.4096  -5.655  -35.208  155.3961]\n",
      "Weights: [-4.7714  0.679  -1.2067  0.1252  0.1402]\n",
      "MSE loss: 91.5542\n",
      "Iteration: 114000\n",
      "Gradient: [-0.1196  1.2631 -8.0304 67.7931 33.5598]\n",
      "Weights: [-4.7615  0.6813 -1.2093  0.1257  0.1402]\n",
      "MSE loss: 91.5396\n",
      "Iteration: 114100\n",
      "Gradient: [  -4.0202    3.007    -5.3459  123.7252 -116.3341]\n",
      "Weights: [-4.7725  0.6878 -1.2067  0.1246  0.1402]\n",
      "MSE loss: 91.483\n",
      "Iteration: 114200\n",
      "Gradient: [   2.5488    1.9221   38.7756   17.1676 -285.977 ]\n",
      "Weights: [-4.7789  0.7043 -1.209   0.1233  0.1402]\n",
      "MSE loss: 91.4996\n",
      "Iteration: 114300\n",
      "Gradient: [  -5.1836    1.49     22.2591   67.0664 -262.9371]\n",
      "Weights: [-4.7835  0.6976 -1.2072  0.1243  0.1402]\n",
      "MSE loss: 91.564\n",
      "Iteration: 114400\n",
      "Gradient: [   4.9314    4.3784   16.841    66.9505 -145.5819]\n",
      "Weights: [-4.7635  0.6858 -1.2105  0.1251  0.1404]\n",
      "MSE loss: 91.5136\n",
      "Iteration: 114500\n",
      "Gradient: [  0.9616  -4.6284 -21.7128 -31.3634 -39.7472]\n",
      "Weights: [-4.7683  0.693  -1.2129  0.1243  0.1404]\n",
      "MSE loss: 91.5637\n",
      "Iteration: 114600\n",
      "Gradient: [ -9.1348   5.6106  -7.8834  99.6324 -69.8159]\n",
      "Weights: [-4.765   0.6906 -1.2124  0.1242  0.1407]\n",
      "MSE loss: 91.5056\n",
      "Iteration: 114700\n",
      "Gradient: [   4.0844    0.5615   -5.0931  -86.4465 -177.4972]\n",
      "Weights: [-4.7599  0.6807 -1.2085  0.1241  0.1404]\n",
      "MSE loss: 91.5941\n",
      "Iteration: 114800\n",
      "Gradient: [ 2.5109 13.7561  1.6979 85.4612 41.8667]\n",
      "Weights: [-4.7641  0.6875 -1.2083  0.1243  0.1404]\n",
      "MSE loss: 91.566\n",
      "Iteration: 114900\n",
      "Gradient: [  -2.1523   11.7742   10.6538   50.2523 -209.3091]\n",
      "Weights: [-4.7766  0.696  -1.2073  0.1242  0.1402]\n",
      "MSE loss: 91.6303\n",
      "Iteration: 115000\n",
      "Gradient: [ -4.9862   7.4552  24.9676 -38.897  324.3346]\n",
      "Weights: [-4.7737  0.6881 -1.2048  0.1242  0.1403]\n",
      "MSE loss: 91.7377\n",
      "Iteration: 115100\n",
      "Gradient: [  -0.2983    3.1855    9.5985   35.2442 -219.9753]\n",
      "Weights: [-4.7666  0.6832 -1.2057  0.124   0.1403]\n",
      "MSE loss: 91.5023\n",
      "Iteration: 115200\n",
      "Gradient: [ -0.33     1.3657 -10.9627 -69.0815 103.6843]\n",
      "Weights: [-4.7775  0.6863 -1.2026  0.1231  0.1403]\n",
      "MSE loss: 91.503\n",
      "Iteration: 115300\n",
      "Gradient: [ -3.8092  -9.8239 -23.4438 -45.5391  19.6197]\n",
      "Weights: [-4.7712  0.6833 -1.2035  0.123   0.1401]\n",
      "MSE loss: 91.6043\n",
      "Iteration: 115400\n",
      "Gradient: [ 4.298  18.9549  4.4938 64.656  90.1554]\n",
      "Weights: [-4.7731  0.6897 -1.205   0.1237  0.1403]\n",
      "MSE loss: 91.5598\n",
      "Iteration: 115500\n",
      "Gradient: [   4.2784   -0.6364   14.9447  119.5319 -175.7024]\n",
      "Weights: [-4.7743  0.6961 -1.2075  0.1231  0.1404]\n",
      "MSE loss: 91.5125\n",
      "Iteration: 115600\n",
      "Gradient: [  1.1915   2.5751 -52.2202 -94.6389 -75.8894]\n",
      "Weights: [-4.7859  0.6963 -1.2093  0.1236  0.1404]\n",
      "MSE loss: 91.5971\n",
      "Iteration: 115700\n",
      "Gradient: [  -0.4065   23.7349  -64.9485 -111.1763    6.9479]\n",
      "Weights: [-4.7707  0.6979 -1.2089  0.1234  0.1404]\n",
      "MSE loss: 91.5616\n",
      "Iteration: 115800\n",
      "Gradient: [ -0.4945  16.8161  57.5715 -18.3245 139.6801]\n",
      "Weights: [-4.7489  0.6882 -1.2073  0.124   0.1402]\n",
      "MSE loss: 92.3914\n",
      "Iteration: 115900\n",
      "Gradient: [  6.8667   6.5741 -11.5176 -71.5336 -58.8716]\n",
      "Weights: [-4.7496  0.6843 -1.2078  0.1232  0.1404]\n",
      "MSE loss: 91.8819\n",
      "Iteration: 116000\n",
      "Gradient: [ -2.8703 -22.7386 -20.5826 -28.2867  42.9539]\n",
      "Weights: [-4.7819  0.6917 -1.2077  0.123   0.1404]\n",
      "MSE loss: 91.7417\n",
      "Iteration: 116100\n",
      "Gradient: [ -1.4673   6.9094   7.5693 -20.1641 -51.5808]\n",
      "Weights: [-4.7786  0.7042 -1.2108  0.1232  0.1407]\n",
      "MSE loss: 91.5964\n",
      "Iteration: 116200\n",
      "Gradient: [  1.0902 -14.7938 -19.4303  31.7105  -4.7948]\n",
      "Weights: [-4.7649  0.6916 -1.2106  0.1233  0.1406]\n",
      "MSE loss: 91.5318\n",
      "Iteration: 116300\n",
      "Gradient: [ -2.7192   5.4871  10.7412 -31.9328 215.5582]\n",
      "Weights: [-4.7924  0.7013 -1.2125  0.1239  0.1407]\n",
      "MSE loss: 91.7111\n",
      "Iteration: 116400\n",
      "Gradient: [ -6.6012   3.0972 -46.2338 -19.8717 290.1237]\n",
      "Weights: [-4.7846  0.7039 -1.2129  0.1236  0.1405]\n",
      "MSE loss: 91.5058\n",
      "Iteration: 116500\n",
      "Gradient: [  -2.4269  -14.225   -26.8293   49.0648 -109.3779]\n",
      "Weights: [-4.7641  0.6896 -1.2086  0.1223  0.1407]\n",
      "MSE loss: 91.5783\n",
      "Iteration: 116600\n",
      "Gradient: [  0.3668   5.4898  15.5026  29.4311 209.3272]\n",
      "Weights: [-4.7881  0.7094 -1.21    0.1225  0.1404]\n",
      "MSE loss: 91.4705\n",
      "Iteration: 116700\n",
      "Gradient: [  4.8043   2.9486  -7.3515 -59.7307 329.5333]\n",
      "Weights: [-4.7898  0.705  -1.2108  0.1228  0.1407]\n",
      "MSE loss: 91.5137\n",
      "Iteration: 116800\n",
      "Gradient: [  7.4467   7.413  -17.8296 -50.6509  37.7991]\n",
      "Weights: [-4.7892  0.7019 -1.2109  0.1228  0.1406]\n",
      "MSE loss: 91.6978\n",
      "Iteration: 116900\n",
      "Gradient: [  -2.0941  -15.2231  -14.6107    3.0784 -290.1988]\n",
      "Weights: [-4.7853  0.6954 -1.2088  0.1229  0.1406]\n",
      "MSE loss: 91.6723\n",
      "Iteration: 117000\n",
      "Gradient: [  -9.5033  -13.0437   -0.5402  -46.154  -233.883 ]\n",
      "Weights: [-4.7641  0.6998 -1.2098  0.1224  0.1405]\n",
      "MSE loss: 91.7022\n",
      "Iteration: 117100\n",
      "Gradient: [-10.5869 -11.2669  46.5632  -3.9509 -55.2181]\n",
      "Weights: [-4.7755  0.7049 -1.2109  0.1227  0.1405]\n",
      "MSE loss: 91.5011\n",
      "Iteration: 117200\n",
      "Gradient: [ 10.0557   3.6337  46.6331 -39.4021 178.6016]\n",
      "Weights: [-4.7729  0.7077 -1.2125  0.1232  0.1405]\n",
      "MSE loss: 91.627\n",
      "Iteration: 117300\n",
      "Gradient: [  1.9315  25.1434  51.8829  -2.8219 187.2901]\n",
      "Weights: [-4.7766  0.7112 -1.2148  0.1243  0.1405]\n",
      "MSE loss: 91.8725\n",
      "Iteration: 117400\n",
      "Gradient: [  6.3086  17.4     11.0792  19.2638 372.1144]\n",
      "Weights: [-4.7767  0.6989 -1.2126  0.1248  0.1405]\n",
      "MSE loss: 91.5529\n",
      "Iteration: 117500\n",
      "Gradient: [  4.4879  -4.2597  -6.8426 -75.1496 112.9291]\n",
      "Weights: [-4.79    0.7132 -1.2125  0.124   0.1401]\n",
      "MSE loss: 91.464\n",
      "Iteration: 117600\n",
      "Gradient: [ -2.4798   5.4403   8.9892  -4.6177 109.5468]\n",
      "Weights: [-4.7852  0.7135 -1.2115  0.1238  0.14  ]\n",
      "MSE loss: 91.54\n",
      "Iteration: 117700\n",
      "Gradient: [  0.4221  -4.7542  -3.2841  14.1531 188.8808]\n",
      "Weights: [-4.7891  0.7203 -1.2143  0.124   0.1401]\n",
      "MSE loss: 91.5597\n",
      "Iteration: 117800\n",
      "Gradient: [  3.3544 -11.6881 -62.0735 -59.6213  78.072 ]\n",
      "Weights: [-4.7948  0.7136 -1.2139  0.124   0.14  ]\n",
      "MSE loss: 91.6503\n",
      "Iteration: 117900\n",
      "Gradient: [   4.2661   26.443   -37.1448   92.6311 -349.1185]\n",
      "Weights: [-4.7903  0.7239 -1.2177  0.1244  0.1402]\n",
      "MSE loss: 91.4918\n",
      "Iteration: 118000\n",
      "Gradient: [   5.9632    3.5697   -8.4424   41.3939 -195.7291]\n",
      "Weights: [-4.7815  0.7093 -1.2166  0.1246  0.1402]\n",
      "MSE loss: 91.4949\n",
      "Iteration: 118100\n",
      "Gradient: [-8.5829  8.2238 16.646  77.7771 34.3893]\n",
      "Weights: [-4.7997  0.7204 -1.215   0.1246  0.14  ]\n",
      "MSE loss: 91.5239\n",
      "Iteration: 118200\n",
      "Gradient: [ -2.1131 -14.6449  20.5988  -1.8046 245.8167]\n",
      "Weights: [-4.7915  0.7235 -1.2154  0.1245  0.14  ]\n",
      "MSE loss: 91.6272\n",
      "Iteration: 118300\n",
      "Gradient: [-12.5535  -6.9655 -46.4273  85.514  -93.9478]\n",
      "Weights: [-4.8127  0.7227 -1.2161  0.1244  0.1402]\n",
      "MSE loss: 91.9428\n",
      "Iteration: 118400\n",
      "Gradient: [  -3.0326    5.4196   -8.6064 -102.1734   89.1468]\n",
      "Weights: [-4.7965  0.7182 -1.2177  0.125   0.1402]\n",
      "MSE loss: 91.5273\n",
      "Iteration: 118500\n",
      "Gradient: [  3.5554  14.7359  20.6259 -63.8771  60.8786]\n",
      "Weights: [-4.7837  0.7108 -1.2131  0.1249  0.1399]\n",
      "MSE loss: 91.4875\n",
      "Iteration: 118600\n",
      "Gradient: [ -11.6041    4.9968   54.5023  -68.1988 -209.6494]\n",
      "Weights: [-4.7855  0.7118 -1.217   0.1261  0.1401]\n",
      "MSE loss: 91.4564\n",
      "Iteration: 118700\n",
      "Gradient: [   0.407    -3.3271   21.087   -55.6043 -235.0285]\n",
      "Weights: [-4.7944  0.7182 -1.2184  0.1264  0.1402]\n",
      "MSE loss: 91.59\n",
      "Iteration: 118800\n",
      "Gradient: [  8.9867 -19.004   32.2553  18.0269  33.8438]\n",
      "Weights: [-4.7884  0.7162 -1.2194  0.1256  0.1402]\n",
      "MSE loss: 91.403\n",
      "Iteration: 118900\n",
      "Gradient: [  -6.6905   14.4588  -20.472   -80.2049 -165.2699]\n",
      "Weights: [-4.7923  0.7272 -1.2245  0.1249  0.1404]\n",
      "MSE loss: 91.5257\n",
      "Iteration: 119000\n",
      "Gradient: [ 12.0305  17.706   24.6982  57.5352 117.6795]\n",
      "Weights: [-4.7869  0.7288 -1.2262  0.1251  0.1406]\n",
      "MSE loss: 91.4078\n",
      "Iteration: 119100\n",
      "Gradient: [   4.2841   12.8406    1.0612  113.0223 -242.6233]\n",
      "Weights: [-4.7858  0.7164 -1.2223  0.1257  0.1409]\n",
      "MSE loss: 91.5373\n",
      "Iteration: 119200\n",
      "Gradient: [ -2.4564 -15.8474 -85.3212 -43.0788 203.0437]\n",
      "Weights: [-4.7888  0.7116 -1.2234  0.1253  0.1409]\n",
      "MSE loss: 91.6649\n",
      "Iteration: 119300\n",
      "Gradient: [   2.7327   -3.1755   28.347   -95.1069 -195.6498]\n",
      "Weights: [-4.7749  0.711  -1.2216  0.1253  0.1407]\n",
      "MSE loss: 91.4369\n",
      "Iteration: 119400\n",
      "Gradient: [  9.0652   8.2823  69.3084 107.0753 232.7987]\n",
      "Weights: [-4.7763  0.7156 -1.2219  0.1265  0.1404]\n",
      "MSE loss: 91.7029\n",
      "Iteration: 119500\n",
      "Gradient: [   2.4992    7.2845  -22.5273  -79.1037 -198.97  ]\n",
      "Weights: [-4.7904  0.7194 -1.2251  0.1266  0.1408]\n",
      "MSE loss: 91.511\n",
      "Iteration: 119600\n",
      "Gradient: [  -2.3038    2.3251   17.6751   95.0729 -137.8839]\n",
      "Weights: [-4.7835  0.7194 -1.2261  0.1272  0.1408]\n",
      "MSE loss: 91.7161\n",
      "Iteration: 119700\n",
      "Gradient: [ 11.7374  -9.537  -31.0007 -16.9541 -91.9103]\n",
      "Weights: [-4.7734  0.7144 -1.2278  0.1268  0.1407]\n",
      "MSE loss: 91.4188\n",
      "Iteration: 119800\n",
      "Gradient: [ 5.5028 11.441  -8.5248  5.2235 31.2705]\n",
      "Weights: [-4.7593  0.7091 -1.229   0.1267  0.141 ]\n",
      "MSE loss: 91.6464\n",
      "Iteration: 119900\n",
      "Gradient: [  -1.9571   11.6356  -12.4554 -110.9673   72.8467]\n",
      "Weights: [-4.7878  0.7297 -1.2317  0.1262  0.1409]\n",
      "MSE loss: 91.3715\n",
      "Iteration: 120000\n",
      "Gradient: [ -10.6607   -4.0488   -2.9775    1.7229 -159.834 ]\n",
      "Weights: [-4.7772  0.7266 -1.2294  0.1261  0.1407]\n",
      "MSE loss: 91.4667\n",
      "Iteration: 120100\n",
      "Gradient: [ -12.6908   -4.472   -33.2775  -93.2901 -268.3898]\n",
      "Weights: [-4.8073  0.7357 -1.2295  0.1259  0.1406]\n",
      "MSE loss: 91.7463\n",
      "Iteration: 120200\n",
      "Gradient: [  4.3773  -7.3983 -67.4917   4.3803 135.9688]\n",
      "Weights: [-4.7863  0.7368 -1.2314  0.1262  0.1405]\n",
      "MSE loss: 91.4612\n",
      "Iteration: 120300\n",
      "Gradient: [ 12.9236   6.8084  -5.7575 -24.2094 100.0657]\n",
      "Weights: [-4.763   0.7297 -1.2331  0.1267  0.1405]\n",
      "MSE loss: 92.0004\n",
      "Iteration: 120400\n",
      "Gradient: [  2.8831   1.08     2.7227 -65.5283  88.0772]\n",
      "Weights: [-4.7719  0.7208 -1.2329  0.1276  0.141 ]\n",
      "MSE loss: 91.4974\n",
      "Iteration: 120500\n",
      "Gradient: [ -4.255    1.4055 -24.4565  22.8886 -56.502 ]\n",
      "Weights: [-4.7709  0.726  -1.2367  0.1277  0.1409]\n",
      "MSE loss: 91.4832\n",
      "Iteration: 120600\n",
      "Gradient: [ -2.5421  -8.1689  29.0144 -33.3763 143.4312]\n",
      "Weights: [-4.786   0.7291 -1.2379  0.1281  0.1409]\n",
      "MSE loss: 91.4946\n",
      "Iteration: 120700\n",
      "Gradient: [ -1.7001  -9.7854 -15.4097  14.1144   8.3549]\n",
      "Weights: [-4.805   0.7405 -1.2382  0.1282  0.1408]\n",
      "MSE loss: 91.5552\n",
      "Iteration: 120800\n",
      "Gradient: [ 10.4722  -5.1535  -9.7394  16.6547 160.2632]\n",
      "Weights: [-4.7831  0.7409 -1.2394  0.1289  0.1409]\n",
      "MSE loss: 91.8288\n",
      "Iteration: 120900\n",
      "Gradient: [  7.5503   0.3812  19.0938 -34.4789 -57.9282]\n",
      "Weights: [-4.7956  0.7506 -1.2416  0.128   0.1408]\n",
      "MSE loss: 91.3442\n",
      "Iteration: 121000\n",
      "Gradient: [   1.9961    8.8443   13.2324   37.7218 -157.8841]\n",
      "Weights: [-4.8054  0.76   -1.2428  0.1285  0.1406]\n",
      "MSE loss: 91.4044\n",
      "Iteration: 121100\n",
      "Gradient: [  3.8859  20.1267 -31.8064 -88.8687 -46.7106]\n",
      "Weights: [-4.7842  0.7519 -1.2448  0.1286  0.1406]\n",
      "MSE loss: 91.498\n",
      "Iteration: 121200\n",
      "Gradient: [ -0.8741   7.6918 -21.8682  14.8211 233.0482]\n",
      "Weights: [-4.7914  0.7447 -1.2438  0.129   0.1408]\n",
      "MSE loss: 91.3688\n",
      "Iteration: 121300\n",
      "Gradient: [  6.6541   9.8637  25.8209  59.7622 206.1942]\n",
      "Weights: [-4.798   0.7511 -1.2431  0.1293  0.1407]\n",
      "MSE loss: 91.3895\n",
      "Iteration: 121400\n",
      "Gradient: [ -0.5008  21.7132   0.6066 103.9092 -14.1939]\n",
      "Weights: [-4.7891  0.7528 -1.2445  0.1294  0.1406]\n",
      "MSE loss: 91.4881\n",
      "Iteration: 121500\n",
      "Gradient: [ -4.7127  -8.5725 -42.8658 -64.7246  60.8823]\n",
      "Weights: [-4.8131  0.7534 -1.2407  0.1283  0.1407]\n",
      "MSE loss: 91.5398\n",
      "Iteration: 121600\n",
      "Gradient: [ 10.7818  11.0142  64.483   72.017  568.0198]\n",
      "Weights: [-4.7996  0.7635 -1.243   0.1278  0.1406]\n",
      "MSE loss: 91.5646\n",
      "Iteration: 121700\n",
      "Gradient: [  -0.3976   10.9559  -30.8666  -60.4008 -271.7158]\n",
      "Weights: [-4.799   0.7552 -1.2414  0.1277  0.1407]\n",
      "MSE loss: 91.3775\n",
      "Iteration: 121800\n",
      "Gradient: [ 13.3508  13.5033  -0.4684 133.4208 360.8322]\n",
      "Weights: [-4.7898  0.757  -1.2399  0.1271  0.1408]\n",
      "MSE loss: 91.9623\n",
      "Iteration: 121900\n",
      "Gradient: [ -0.7201 -15.3722 -18.7061  -2.6712 -40.778 ]\n",
      "Weights: [-4.8104  0.7594 -1.2419  0.1271  0.1407]\n",
      "MSE loss: 91.5456\n",
      "Iteration: 122000\n",
      "Gradient: [  13.2783   -7.7114    6.8664   37.6247 -183.9565]\n",
      "Weights: [-4.8154  0.7628 -1.2417  0.1275  0.1406]\n",
      "MSE loss: 91.5467\n",
      "Iteration: 122100\n",
      "Gradient: [-11.8785   2.5091 -43.6007  20.6691 -98.0604]\n",
      "Weights: [-4.8171  0.7664 -1.2397  0.1268  0.1407]\n",
      "MSE loss: 91.5562\n",
      "Iteration: 122200\n",
      "Gradient: [ -2.8439  -7.491  -14.206  -15.4076 -15.4819]\n",
      "Weights: [-4.8107  0.7592 -1.2393  0.1273  0.1407]\n",
      "MSE loss: 91.4783\n",
      "Iteration: 122300\n",
      "Gradient: [   0.5785   -8.3628   22.2953   -1.4191 -309.6165]\n",
      "Weights: [-4.7994  0.7558 -1.2385  0.1262  0.1407]\n",
      "MSE loss: 91.4658\n",
      "Iteration: 122400\n",
      "Gradient: [  -6.2011  -27.771   -19.1687 -106.4733  -42.5707]\n",
      "Weights: [-4.8141  0.7555 -1.2396  0.1269  0.1407]\n",
      "MSE loss: 91.7166\n",
      "Iteration: 122500\n",
      "Gradient: [-7.4435  9.572  24.8443 19.168  25.2207]\n",
      "Weights: [-4.7857  0.7499 -1.2392  0.1273  0.1407]\n",
      "MSE loss: 91.6566\n",
      "Iteration: 122600\n",
      "Gradient: [  -2.8355   -4.6287   31.9265  -59.4142 -240.7715]\n",
      "Weights: [-4.7988  0.7416 -1.2403  0.128   0.1409]\n",
      "MSE loss: 91.521\n",
      "Iteration: 122700\n",
      "Gradient: [ 2.0353  7.0042 39.7576 72.1023 82.7207]\n",
      "Weights: [-4.7832  0.7412 -1.2423  0.1281  0.1411]\n",
      "MSE loss: 91.3761\n",
      "Iteration: 122800\n",
      "Gradient: [ -4.7506 -20.2935 -15.3252 -64.5734 -51.0246]\n",
      "Weights: [-4.79    0.7422 -1.2406  0.127   0.1412]\n",
      "MSE loss: 91.4097\n",
      "Iteration: 122900\n",
      "Gradient: [  -6.1828  -14.9721  -39.2343  -84.4025 -297.1512]\n",
      "Weights: [-4.7853  0.7382 -1.239   0.1266  0.141 ]\n",
      "MSE loss: 91.5962\n",
      "Iteration: 123000\n",
      "Gradient: [  4.4244   4.7566  38.0005 114.4215 352.7875]\n",
      "Weights: [-4.7725  0.735  -1.2408  0.1273  0.1413]\n",
      "MSE loss: 91.5185\n",
      "Iteration: 123100\n",
      "Gradient: [  2.6527   6.6493  -0.9371 122.2837 192.9838]\n",
      "Weights: [-4.7882  0.7398 -1.2382  0.1273  0.1413]\n",
      "MSE loss: 91.4509\n",
      "Iteration: 123200\n",
      "Gradient: [  6.9253  -5.0997  27.7546  49.907  152.6453]\n",
      "Weights: [-4.7803  0.7373 -1.24    0.1268  0.1415]\n",
      "MSE loss: 91.4074\n",
      "Iteration: 123300\n",
      "Gradient: [ -11.9937   -9.7272    0.5529 -116.564  -361.1228]\n",
      "Weights: [-4.795   0.7291 -1.2369  0.1262  0.1416]\n",
      "MSE loss: 91.8444\n",
      "Iteration: 123400\n",
      "Gradient: [  -0.7708   17.7257   36.6244   53.9403 -141.9858]\n",
      "Weights: [-4.7912  0.7409 -1.2381  0.1263  0.1413]\n",
      "MSE loss: 91.3578\n",
      "Iteration: 123500\n",
      "Gradient: [  -0.3545    0.3042   43.5553  -31.8563 -168.3174]\n",
      "Weights: [-4.7808  0.7471 -1.2435  0.1264  0.1415]\n",
      "MSE loss: 91.4803\n",
      "Iteration: 123600\n",
      "Gradient: [   7.8383    1.1783   43.0829   45.745  -194.1948]\n",
      "Weights: [-4.7921  0.7517 -1.2462  0.1272  0.1413]\n",
      "MSE loss: 91.4567\n",
      "Iteration: 123700\n",
      "Gradient: [ 3.6766  5.743   3.2164  7.8612 -1.9191]\n",
      "Weights: [-4.7989  0.7585 -1.2461  0.1274  0.1414]\n",
      "MSE loss: 91.3546\n",
      "Iteration: 123800\n",
      "Gradient: [ -8.9549  -4.4126   7.8381 110.0424 279.1754]\n",
      "Weights: [-4.803   0.7633 -1.2469  0.1276  0.1413]\n",
      "MSE loss: 91.4223\n",
      "Iteration: 123900\n",
      "Gradient: [   5.8121  -11.335    -1.7206  -44.0568 -191.8469]\n",
      "Weights: [-4.8064  0.7602 -1.2471  0.1274  0.1413]\n",
      "MSE loss: 91.4293\n",
      "Iteration: 124000\n",
      "Gradient: [  9.1363  -6.502   35.9293 117.3348 372.1139]\n",
      "Weights: [-4.8071  0.7622 -1.2476  0.128   0.1414]\n",
      "MSE loss: 91.4367\n",
      "Iteration: 124100\n",
      "Gradient: [ 7.77130e+00 -4.31000e-02  1.21977e+01  8.98358e+01  1.21142e+01]\n",
      "Weights: [-4.8011  0.7583 -1.2474  0.1286  0.1413]\n",
      "MSE loss: 91.41\n",
      "Iteration: 124200\n",
      "Gradient: [  -3.4458   -4.1514  -37.6653 -107.8783  129.6373]\n",
      "Weights: [-4.8005  0.7579 -1.248   0.128   0.141 ]\n",
      "MSE loss: 91.5546\n",
      "Iteration: 124300\n",
      "Gradient: [ -6.0706  13.6912  29.8284 -71.5796 220.3547]\n",
      "Weights: [-4.7979  0.7581 -1.2482  0.1291  0.141 ]\n",
      "MSE loss: 91.3255\n",
      "Iteration: 124400\n",
      "Gradient: [ -5.4905  12.6571  13.0958 160.295  257.8548]\n",
      "Weights: [-4.7806  0.753  -1.2492  0.13    0.141 ]\n",
      "MSE loss: 91.7404\n",
      "Iteration: 124500\n",
      "Gradient: [  -1.5545   -6.26     21.5576   -4.0254 -127.6939]\n",
      "Weights: [-4.7896  0.7503 -1.2512  0.1305  0.1411]\n",
      "MSE loss: 91.3076\n",
      "Iteration: 124600\n",
      "Gradient: [-3.5211 27.0151 -6.2691  0.3008 51.2932]\n",
      "Weights: [-4.7991  0.7511 -1.2498  0.1305  0.1411]\n",
      "MSE loss: 91.3626\n",
      "Iteration: 124700\n",
      "Gradient: [  -3.0728    7.9534   23.0347   60.0754 -141.9293]\n",
      "Weights: [-4.7924  0.7474 -1.2493  0.1302  0.1412]\n",
      "MSE loss: 91.3328\n",
      "Iteration: 124800\n",
      "Gradient: [  1.1491  19.8635  56.8944   3.4891 257.3103]\n",
      "Weights: [-4.7868  0.7547 -1.2507  0.1307  0.1409]\n",
      "MSE loss: 91.4711\n",
      "Iteration: 124900\n",
      "Gradient: [  9.1465   8.4967   3.4362 -33.0803 228.533 ]\n",
      "Weights: [-4.7902  0.7571 -1.2509  0.1312  0.1408]\n",
      "MSE loss: 91.5598\n",
      "Iteration: 125000\n",
      "Gradient: [ -1.3635  11.199  -51.3818 -53.9966  75.2104]\n",
      "Weights: [-4.8162  0.7612 -1.2518  0.1313  0.1407]\n",
      "MSE loss: 91.6546\n",
      "Iteration: 125100\n",
      "Gradient: [  -9.195    11.3273   15.0961  -38.5166 -101.5146]\n",
      "Weights: [-4.7916  0.7459 -1.2475  0.131   0.1406]\n",
      "MSE loss: 91.306\n",
      "Iteration: 125200\n",
      "Gradient: [-10.4735   0.2727 -43.9306   8.4046 190.5712]\n",
      "Weights: [-4.7918  0.7395 -1.2484  0.1319  0.1406]\n",
      "MSE loss: 91.4411\n",
      "Iteration: 125300\n",
      "Gradient: [  -8.8345    7.4113  -50.4924   53.7335 -202.6133]\n",
      "Weights: [-4.7984  0.7502 -1.2505  0.1323  0.1404]\n",
      "MSE loss: 91.3428\n",
      "Iteration: 125400\n",
      "Gradient: [  1.7505  -9.2193 -39.3051  22.6415  69.8568]\n",
      "Weights: [-4.7844  0.7457 -1.2518  0.1322  0.1404]\n",
      "MSE loss: 91.3975\n",
      "Iteration: 125500\n",
      "Gradient: [  -5.264     3.3739  -10.6222  -76.8238 -169.5619]\n",
      "Weights: [-4.79    0.74   -1.2503  0.1329  0.1406]\n",
      "MSE loss: 91.3502\n",
      "Iteration: 125600\n",
      "Gradient: [ -8.5443  -1.9971  27.7956 -79.6706 -81.9924]\n",
      "Weights: [-4.7827  0.7432 -1.252   0.1326  0.1405]\n",
      "MSE loss: 91.326\n",
      "Iteration: 125700\n",
      "Gradient: [ -3.3989 -14.4648  -4.1894 -35.0484 -27.4952]\n",
      "Weights: [-4.7871  0.7433 -1.2526  0.1323  0.1407]\n",
      "MSE loss: 91.4617\n",
      "Iteration: 125800\n",
      "Gradient: [   3.0016   10.5139   12.9249  -92.6131 -112.0677]\n",
      "Weights: [-4.7931  0.7534 -1.2527  0.1325  0.1405]\n",
      "MSE loss: 91.2723\n",
      "Iteration: 125900\n",
      "Gradient: [  1.2238   6.8166 -18.1888 139.3348 193.3495]\n",
      "Weights: [-4.795   0.7495 -1.2521  0.1328  0.1407]\n",
      "MSE loss: 91.332\n",
      "Iteration: 126000\n",
      "Gradient: [ -4.6165 -19.4288  -8.3332 -52.059  -18.1882]\n",
      "Weights: [-4.8091  0.7477 -1.251   0.1328  0.1405]\n",
      "MSE loss: 91.844\n",
      "Iteration: 126100\n",
      "Gradient: [ 12.4119  -1.2206  18.3519 -30.6983 150.5419]\n",
      "Weights: [-4.7876  0.7535 -1.2513  0.1328  0.1402]\n",
      "MSE loss: 91.3541\n",
      "Iteration: 126200\n",
      "Gradient: [ -0.3106   2.6924   7.144   90.4851 122.6442]\n",
      "Weights: [-4.799   0.7618 -1.2507  0.1328  0.1401]\n",
      "MSE loss: 91.5776\n",
      "Iteration: 126300\n",
      "Gradient: [  2.277    7.32   -28.5318 -71.1579 242.695 ]\n",
      "Weights: [-4.7735  0.7528 -1.2532  0.1322  0.1401]\n",
      "MSE loss: 91.7314\n",
      "Iteration: 126400\n",
      "Gradient: [-11.0783  -1.4039   1.5042 126.4524  56.6798]\n",
      "Weights: [-4.8095  0.7633 -1.2512  0.1327  0.1402]\n",
      "MSE loss: 91.4069\n",
      "Iteration: 126500\n",
      "Gradient: [  0.2497   3.0383 -31.5055 -46.6843 231.5928]\n",
      "Weights: [-4.7954  0.7478 -1.2488  0.1323  0.1402]\n",
      "MSE loss: 91.3176\n",
      "Iteration: 126600\n",
      "Gradient: [  9.7521  -1.8812  -1.6263 -70.3759 278.7176]\n",
      "Weights: [-4.7961  0.7591 -1.2503  0.1313  0.1404]\n",
      "MSE loss: 91.3182\n",
      "Iteration: 126700\n",
      "Gradient: [ -1.5744  -0.6059 -30.7701 -69.3355 119.826 ]\n",
      "Weights: [-4.7989  0.7614 -1.2522  0.1319  0.1404]\n",
      "MSE loss: 91.2973\n",
      "Iteration: 126800\n",
      "Gradient: [ 12.9401  20.5711  41.3346 -22.5419 624.3138]\n",
      "Weights: [-4.8021  0.7634 -1.249   0.1316  0.1403]\n",
      "MSE loss: 91.5983\n",
      "Iteration: 126900\n",
      "Gradient: [ 16.417    3.8097  38.2409   2.8645 144.0648]\n",
      "Weights: [-4.7933  0.7562 -1.2504  0.1324  0.1403]\n",
      "MSE loss: 91.458\n",
      "Iteration: 127000\n",
      "Gradient: [  -4.8154    2.2303  -15.3005  -23.2034 -186.8944]\n",
      "Weights: [-4.802   0.7601 -1.2519  0.1318  0.1405]\n",
      "MSE loss: 91.3005\n",
      "Iteration: 127100\n",
      "Gradient: [  5.2546  -0.7833  -5.3181 -76.0062 192.4716]\n",
      "Weights: [-4.7939  0.7559 -1.2518  0.132   0.1404]\n",
      "MSE loss: 91.2748\n",
      "Iteration: 127200\n",
      "Gradient: [  4.4852  16.0801  32.0649 -46.2175  12.8394]\n",
      "Weights: [-4.7882  0.7529 -1.2516  0.1324  0.1407]\n",
      "MSE loss: 91.6063\n",
      "Iteration: 127300\n",
      "Gradient: [ 10.8586  -4.7228   1.8311  -3.0514 242.4378]\n",
      "Weights: [-4.7879  0.7543 -1.2507  0.1313  0.1406]\n",
      "MSE loss: 91.365\n",
      "Iteration: 127400\n",
      "Gradient: [  -9.3887   -9.2563   26.0353 -109.4004 -438.0694]\n",
      "Weights: [-4.7942  0.7519 -1.2484  0.1313  0.1406]\n",
      "MSE loss: 91.3268\n",
      "Iteration: 127500\n",
      "Gradient: [  7.0364   7.4745 -59.1785 -49.9536  45.5847]\n",
      "Weights: [-4.7937  0.7392 -1.2482  0.1326  0.1405]\n",
      "MSE loss: 91.4084\n",
      "Iteration: 127600\n",
      "Gradient: [  0.8202  -9.2208 -37.8456 -31.6381  78.2914]\n",
      "Weights: [-4.7987  0.7429 -1.2441  0.1318  0.1403]\n",
      "MSE loss: 91.3436\n",
      "Iteration: 127700\n",
      "Gradient: [ -1.4901  -2.4023  12.1904 -80.2022 -15.804 ]\n",
      "Weights: [-4.7953  0.7456 -1.2457  0.1319  0.1401]\n",
      "MSE loss: 91.2811\n",
      "Iteration: 127800\n",
      "Gradient: [ -14.0047  -15.4474  -11.1866 -101.8442 -503.8055]\n",
      "Weights: [-4.8044  0.7411 -1.2463  0.1324  0.14  ]\n",
      "MSE loss: 92.0585\n",
      "Iteration: 127900\n",
      "Gradient: [   0.6064   17.571    -0.5727 -135.5042   29.8823]\n",
      "Weights: [-4.797   0.7464 -1.2488  0.1326  0.1402]\n",
      "MSE loss: 91.3511\n",
      "Iteration: 128000\n",
      "Gradient: [ -5.1196   6.2603  16.609   85.6421 333.6371]\n",
      "Weights: [-4.7942  0.755  -1.25    0.1326  0.1399]\n",
      "MSE loss: 91.2842\n",
      "Iteration: 128100\n",
      "Gradient: [  5.8086  13.0188   1.9178 -67.9169 144.5614]\n",
      "Weights: [-4.7982  0.7586 -1.2506  0.1327  0.1401]\n",
      "MSE loss: 91.3292\n",
      "Iteration: 128200\n",
      "Gradient: [  2.7237   3.2346 -19.342  -13.8813 221.2613]\n",
      "Weights: [-4.7945  0.7547 -1.251   0.133   0.1402]\n",
      "MSE loss: 91.3081\n",
      "Iteration: 128300\n",
      "Gradient: [-10.3828 -10.6814 -41.8011 -11.6805  19.3526]\n",
      "Weights: [-4.803   0.7511 -1.2524  0.1331  0.1402]\n",
      "MSE loss: 91.6297\n",
      "Iteration: 128400\n",
      "Gradient: [ -3.4799 -15.5052 -31.5563 -56.0882 -27.0121]\n",
      "Weights: [-4.7901  0.7551 -1.2535  0.1333  0.14  ]\n",
      "MSE loss: 91.3043\n",
      "Iteration: 128500\n",
      "Gradient: [   2.6279    6.0561  -28.4954   20.6558 -349.3251]\n",
      "Weights: [-4.7869  0.7443 -1.2538  0.135   0.14  ]\n",
      "MSE loss: 91.2516\n",
      "Iteration: 128600\n",
      "Gradient: [-12.0372  -9.5427   9.7081  70.1967 -23.4352]\n",
      "Weights: [-4.7777  0.7402 -1.2504  0.1342  0.1399]\n",
      "MSE loss: 91.3249\n",
      "Iteration: 128700\n",
      "Gradient: [ -6.2838 -16.2433 -13.72    33.7729 413.2238]\n",
      "Weights: [-4.7988  0.7456 -1.2499  0.1343  0.1398]\n",
      "MSE loss: 91.3479\n",
      "Iteration: 128800\n",
      "Gradient: [ -9.1973 -17.2111 -19.3174 -94.0044  82.8751]\n",
      "Weights: [-4.816   0.7621 -1.2529  0.1345  0.1396]\n",
      "MSE loss: 91.5354\n",
      "Iteration: 128900\n",
      "Gradient: [ -8.294   -1.0456  -4.8643   2.8383 387.5554]\n",
      "Weights: [-4.8065  0.7564 -1.2517  0.1343  0.1396]\n",
      "MSE loss: 91.3532\n",
      "Iteration: 129000\n",
      "Gradient: [ -6.0205   2.1101   4.0645   8.3311 443.9131]\n",
      "Weights: [-4.8001  0.7523 -1.252   0.1345  0.1397]\n",
      "MSE loss: 91.2996\n",
      "Iteration: 129100\n",
      "Gradient: [  -0.2973   -3.227     4.8144   12.96   -106.5865]\n",
      "Weights: [-4.8105  0.7617 -1.2534  0.1347  0.1397]\n",
      "MSE loss: 91.3443\n",
      "Iteration: 129200\n",
      "Gradient: [  -8.1527   -7.6721  -11.5268  -10.8925 -220.1084]\n",
      "Weights: [-4.7959  0.7557 -1.2534  0.1348  0.1396]\n",
      "MSE loss: 91.2439\n",
      "Iteration: 129300\n",
      "Gradient: [  1.8722  12.2661 -15.3308  61.8463 178.8788]\n",
      "Weights: [-4.7922  0.7545 -1.2538  0.1352  0.1398]\n",
      "MSE loss: 91.3389\n",
      "Iteration: 129400\n",
      "Gradient: [  -8.9764  -10.6427  -16.2221  -49.7486 -416.4856]\n",
      "Weights: [-4.7873  0.7507 -1.2544  0.1356  0.1395]\n",
      "MSE loss: 91.2573\n",
      "Iteration: 129500\n",
      "Gradient: [  4.5548  25.9072   2.6527 115.0925  29.8277]\n",
      "Weights: [-4.7881  0.756  -1.2567  0.1365  0.1395]\n",
      "MSE loss: 91.3095\n",
      "Iteration: 129600\n",
      "Gradient: [   1.0529  -20.2501  -17.7822  -53.9493 -217.6055]\n",
      "Weights: [-4.781   0.7378 -1.2545  0.1367  0.1395]\n",
      "MSE loss: 91.3323\n",
      "Iteration: 129700\n",
      "Gradient: [  -5.3251   15.0626    9.7728  -98.5879 -187.7073]\n",
      "Weights: [-4.775   0.7351 -1.2538  0.1373  0.1394]\n",
      "MSE loss: 91.2863\n",
      "Iteration: 129800\n",
      "Gradient: [  3.5566   9.4118 -34.7048  93.691  -56.1813]\n",
      "Weights: [-4.786   0.739  -1.2514  0.1365  0.1394]\n",
      "MSE loss: 91.2229\n",
      "Iteration: 129900\n",
      "Gradient: [ -2.7894  -5.7821 -21.0733 -47.5948  20.9397]\n",
      "Weights: [-4.8023  0.7472 -1.25    0.1358  0.1393]\n",
      "MSE loss: 91.3175\n",
      "Iteration: 130000\n",
      "Gradient: [   6.723    29.4715   -3.1179   46.5079 -257.9333]\n",
      "Weights: [-4.7816  0.7411 -1.2498  0.1355  0.1395]\n",
      "MSE loss: 91.3385\n",
      "Iteration: 130100\n",
      "Gradient: [ 12.0798 -25.6966 -63.9508   6.8232  24.009 ]\n",
      "Weights: [-4.7967  0.7507 -1.2527  0.1357  0.1393]\n",
      "MSE loss: 91.2587\n",
      "Iteration: 130200\n",
      "Gradient: [ -9.3743   3.601    4.0947 -82.1155 -75.865 ]\n",
      "Weights: [-4.8022  0.7623 -1.2523  0.1351  0.1393]\n",
      "MSE loss: 91.3209\n",
      "Iteration: 130300\n",
      "Gradient: [  1.7021  -5.5068 -14.4315  96.0501  48.1717]\n",
      "Weights: [-4.7777  0.7508 -1.2547  0.1353  0.1393]\n",
      "MSE loss: 91.539\n",
      "Iteration: 130400\n",
      "Gradient: [  2.3917  -0.1734 -17.7447 -39.2945  19.0891]\n",
      "Weights: [-4.7904  0.7511 -1.2547  0.1367  0.1393]\n",
      "MSE loss: 91.2217\n",
      "Iteration: 130500\n",
      "Gradient: [  2.337   -5.0287  -4.1517  86.9813 104.062 ]\n",
      "Weights: [-4.7929  0.7448 -1.2531  0.1368  0.1393]\n",
      "MSE loss: 91.2224\n",
      "Iteration: 130600\n",
      "Gradient: [   5.9351    6.9596   -5.1355   57.8631 -434.504 ]\n",
      "Weights: [-4.7971  0.7598 -1.2554  0.1363  0.139 ]\n",
      "MSE loss: 91.2567\n",
      "Iteration: 130700\n",
      "Gradient: [   6.9321   -8.6992  -59.2603  -58.5599 -113.6208]\n",
      "Weights: [-4.8032  0.758  -1.2551  0.1363  0.139 ]\n",
      "MSE loss: 91.4131\n",
      "Iteration: 130800\n",
      "Gradient: [ 2.169000e-01  6.915800e+00  1.257830e+01 -1.236300e+01  2.585577e+02]\n",
      "Weights: [-4.8076  0.7663 -1.256   0.1368  0.1392]\n",
      "MSE loss: 91.3929\n",
      "Iteration: 130900\n",
      "Gradient: [ -4.7202  -1.6821 -49.2268  14.7171  27.4745]\n",
      "Weights: [-4.8133  0.7666 -1.2593  0.1364  0.1396]\n",
      "MSE loss: 91.3744\n",
      "Iteration: 131000\n",
      "Gradient: [  1.0995   5.8926  25.0371  -5.7637 -95.9115]\n",
      "Weights: [-4.8007  0.7696 -1.26    0.1362  0.1394]\n",
      "MSE loss: 91.2527\n",
      "Iteration: 131100\n",
      "Gradient: [  -3.4004   -7.2994   31.2511   25.2443 -195.5521]\n",
      "Weights: [-4.8051  0.7715 -1.261   0.1362  0.1394]\n",
      "MSE loss: 91.2433\n",
      "Iteration: 131200\n",
      "Gradient: [  -6.013   -13.1647  -15.9568  121.1738 -199.1659]\n",
      "Weights: [-4.8043  0.7756 -1.2658  0.1362  0.1397]\n",
      "MSE loss: 91.3183\n",
      "Iteration: 131300\n",
      "Gradient: [ -2.3119   9.1379 -23.6953  -9.474  -65.3918]\n",
      "Weights: [-4.8244  0.7774 -1.2677  0.1374  0.1399]\n",
      "MSE loss: 91.7459\n",
      "Iteration: 131400\n",
      "Gradient: [-4.800000e-03 -7.001200e+00  2.436280e+01  8.267840e+01  2.938818e+02]\n",
      "Weights: [-4.8006  0.7751 -1.2683  0.1376  0.1399]\n",
      "MSE loss: 91.2356\n",
      "Iteration: 131500\n",
      "Gradient: [ -1.4422  -0.2076  -0.4305 -44.8688  -1.0744]\n",
      "Weights: [-4.803   0.7752 -1.2683  0.1368  0.14  ]\n",
      "MSE loss: 91.2134\n",
      "Iteration: 131600\n",
      "Gradient: [-12.3587   7.0645  53.4934  11.1647 -45.0883]\n",
      "Weights: [-4.811   0.7784 -1.2695  0.1372  0.1401]\n",
      "MSE loss: 91.2713\n",
      "Iteration: 131700\n",
      "Gradient: [  1.8805 -14.9428  28.5985  71.8448 190.3507]\n",
      "Weights: [-4.7959  0.7676 -1.2701  0.1382  0.14  ]\n",
      "MSE loss: 91.2052\n",
      "Iteration: 131800\n",
      "Gradient: [ -2.4993  -3.2686 -14.8658  13.7914 267.5642]\n",
      "Weights: [-4.7946  0.7626 -1.2668  0.1378  0.14  ]\n",
      "MSE loss: 91.208\n",
      "Iteration: 131900\n",
      "Gradient: [ -1.4094  11.3979  35.0321  50.1474 259.4452]\n",
      "Weights: [-4.7905  0.7635 -1.2641  0.1382  0.1397]\n",
      "MSE loss: 91.4633\n",
      "Iteration: 132000\n",
      "Gradient: [  0.5276   2.0488  -4.9774  65.9466 249.2841]\n",
      "Weights: [-4.7878  0.7623 -1.2622  0.1377  0.1393]\n",
      "MSE loss: 91.291\n",
      "Iteration: 132100\n",
      "Gradient: [   3.493    -2.7914  -16.5383   58.3059 -115.2628]\n",
      "Weights: [-4.8015  0.7669 -1.2641  0.1385  0.1394]\n",
      "MSE loss: 91.2102\n",
      "Iteration: 132200\n",
      "Gradient: [   8.7085   -4.562    13.6911  116.2769 -205.3242]\n",
      "Weights: [-4.7805  0.7602 -1.2611  0.1379  0.1393]\n",
      "MSE loss: 91.6404\n",
      "Iteration: 132300\n",
      "Gradient: [ -2.8449   8.1607  -3.6418  -1.0002 -51.5178]\n",
      "Weights: [-4.7939  0.7658 -1.2644  0.1383  0.1392]\n",
      "MSE loss: 91.2114\n",
      "Iteration: 132400\n",
      "Gradient: [  -6.7286  -18.8234   -4.2698 -102.4358  -83.5604]\n",
      "Weights: [-4.7909  0.7624 -1.2672  0.1392  0.1392]\n",
      "MSE loss: 91.2852\n",
      "Iteration: 132500\n",
      "Gradient: [ -1.7301  -5.4209  17.276   -3.4393 238.1666]\n",
      "Weights: [-4.8015  0.7608 -1.267   0.1406  0.1394]\n",
      "MSE loss: 91.2886\n",
      "Iteration: 132600\n",
      "Gradient: [  17.729    25.4735  -37.1582   40.4752 -268.0801]\n",
      "Weights: [-4.782   0.7605 -1.269   0.1401  0.1395]\n",
      "MSE loss: 91.3083\n",
      "Iteration: 132700\n",
      "Gradient: [ 4.8833  4.5672 39.1367 85.7162 52.2695]\n",
      "Weights: [-4.807   0.7632 -1.2665  0.1406  0.1395]\n",
      "MSE loss: 91.4725\n",
      "Iteration: 132800\n",
      "Gradient: [ -3.0801 -12.3314  42.8759  57.2706 302.442 ]\n",
      "Weights: [-4.7995  0.7623 -1.2666  0.1405  0.1393]\n",
      "MSE loss: 91.2797\n",
      "Iteration: 132900\n",
      "Gradient: [-2.5338 -5.2886 14.2021 24.4789 48.275 ]\n",
      "Weights: [-4.8006  0.7705 -1.2664  0.1392  0.1392]\n",
      "MSE loss: 91.1895\n",
      "Iteration: 133000\n",
      "Gradient: [   8.7214   13.4434   -8.0463   64.9019 -210.592 ]\n",
      "Weights: [-4.779   0.7628 -1.2672  0.1394  0.1392]\n",
      "MSE loss: 91.4295\n",
      "Iteration: 133100\n",
      "Gradient: [   7.4334   21.6863    4.0511  142.0493 -395.4787]\n",
      "Weights: [-4.798   0.7687 -1.2652  0.1394  0.1391]\n",
      "MSE loss: 91.2782\n",
      "Iteration: 133200\n",
      "Gradient: [  7.2133  -5.7807  37.3193  -9.3556 -83.5983]\n",
      "Weights: [-4.8034  0.7611 -1.2641  0.1391  0.1392]\n",
      "MSE loss: 91.3163\n",
      "Iteration: 133300\n",
      "Gradient: [ -3.434   18.3779  28.3603  58.7393 -50.7912]\n",
      "Weights: [-4.7981  0.7655 -1.2663  0.1393  0.1394]\n",
      "MSE loss: 91.1661\n",
      "Iteration: 133400\n",
      "Gradient: [   3.3198    7.5094   28.012    -4.6261 -365.549 ]\n",
      "Weights: [-4.7983  0.7634 -1.2655  0.139   0.1393]\n",
      "MSE loss: 91.1793\n",
      "Iteration: 133500\n",
      "Gradient: [-9.5024  6.2843  0.2039 13.7298  1.2551]\n",
      "Weights: [-4.7809  0.7478 -1.2633  0.1399  0.1394]\n",
      "MSE loss: 91.2301\n",
      "Iteration: 133600\n",
      "Gradient: [  -6.0155    5.8472  -17.3102   -4.9845 -278.6139]\n",
      "Weights: [-4.7841  0.7304 -1.2615  0.1411  0.1392]\n",
      "MSE loss: 91.7923\n",
      "Iteration: 133700\n",
      "Gradient: [ -5.51     3.9333  46.8364 -27.3804 -88.9113]\n",
      "Weights: [-4.7963  0.7476 -1.2588  0.1404  0.1391]\n",
      "MSE loss: 91.4124\n",
      "Iteration: 133800\n",
      "Gradient: [-2.48700e-01 -1.08836e+01 -1.63975e+01 -6.00000e-03 -2.25793e+01]\n",
      "Weights: [-4.7914  0.7533 -1.2602  0.1396  0.139 ]\n",
      "MSE loss: 91.2209\n",
      "Iteration: 133900\n",
      "Gradient: [   1.5328   14.6777  -19.0148   35.6649 -167.5852]\n",
      "Weights: [-4.7952  0.7562 -1.2603  0.1395  0.1389]\n",
      "MSE loss: 91.1856\n",
      "Iteration: 134000\n",
      "Gradient: [  2.4236   3.0742   9.3369 -26.4548  39.6033]\n",
      "Weights: [-4.7737  0.743  -1.2619  0.1399  0.139 ]\n",
      "MSE loss: 91.3047\n",
      "Iteration: 134100\n",
      "Gradient: [  8.8617 -10.4463  18.6484 -52.8219 283.4699]\n",
      "Weights: [-4.7642  0.7389 -1.2592  0.1401  0.1389]\n",
      "MSE loss: 91.5942\n",
      "Iteration: 134200\n",
      "Gradient: [  -0.1528  -13.7951  -20.9566  -58.591  -149.5664]\n",
      "Weights: [-4.7816  0.7367 -1.2599  0.1407  0.1389]\n",
      "MSE loss: 91.2794\n",
      "Iteration: 134300\n",
      "Gradient: [  3.6384  11.6313  25.1867  78.4569 -42.4964]\n",
      "Weights: [-4.7878  0.7533 -1.2624  0.1397  0.139 ]\n",
      "MSE loss: 91.1688\n",
      "Iteration: 134400\n",
      "Gradient: [ -7.9021   2.5014   2.947  154.7906 377.078 ]\n",
      "Weights: [-4.8096  0.7635 -1.2615  0.1392  0.139 ]\n",
      "MSE loss: 91.2918\n",
      "Iteration: 134500\n",
      "Gradient: [   9.9201   20.9733   12.0278  160.9057 -262.6059]\n",
      "Weights: [-4.7827  0.7579 -1.2615  0.1391  0.139 ]\n",
      "MSE loss: 91.4836\n",
      "Iteration: 134600\n",
      "Gradient: [   3.7436  -14.6547   51.5685 -127.8858 -132.3694]\n",
      "Weights: [-4.7969  0.7626 -1.2625  0.1387  0.1391]\n",
      "MSE loss: 91.1735\n",
      "Iteration: 134700\n",
      "Gradient: [  1.19    14.3957  29.4656  36.0573 174.6821]\n",
      "Weights: [-4.8161  0.7671 -1.2633  0.1393  0.1392]\n",
      "MSE loss: 91.4366\n",
      "Iteration: 134800\n",
      "Gradient: [  0.8381  -6.6771 -44.2523 -26.3519 -62.4654]\n",
      "Weights: [-4.7982  0.7622 -1.2644  0.1392  0.1392]\n",
      "MSE loss: 91.1738\n",
      "Iteration: 134900\n",
      "Gradient: [  8.3421   6.7282 -45.6941  16.0239 125.8254]\n",
      "Weights: [-4.7721  0.7533 -1.2663  0.1398  0.1391]\n",
      "MSE loss: 91.4364\n",
      "Iteration: 135000\n",
      "Gradient: [  9.7678  23.3815   0.6942  81.102  239.3465]\n",
      "Weights: [-4.7968  0.7649 -1.2635  0.1394  0.1392]\n",
      "MSE loss: 91.4003\n",
      "Iteration: 135100\n",
      "Gradient: [  1.5094 -21.7213  29.9697 147.5008  30.3664]\n",
      "Weights: [-4.8078  0.7614 -1.2647  0.1399  0.1393]\n",
      "MSE loss: 91.3438\n",
      "Iteration: 135200\n",
      "Gradient: [   3.8189  -12.5391   -1.6044   77.9295 -214.4268]\n",
      "Weights: [-4.7993  0.7687 -1.2667  0.1393  0.1389]\n",
      "MSE loss: 91.2749\n",
      "Iteration: 135300\n",
      "Gradient: [ 1.2152  7.3916 30.4555 70.4798  7.1253]\n",
      "Weights: [-4.7861  0.7695 -1.2671  0.139   0.1391]\n",
      "MSE loss: 91.3906\n",
      "Iteration: 135400\n",
      "Gradient: [  3.9355  22.1799 -11.477  111.0219 136.873 ]\n",
      "Weights: [-4.8041  0.774  -1.2672  0.1401  0.139 ]\n",
      "MSE loss: 91.2725\n",
      "Iteration: 135500\n",
      "Gradient: [-7.8539 14.6097 21.5735 75.8444 67.906 ]\n",
      "Weights: [-4.793   0.7574 -1.2678  0.1411  0.1391]\n",
      "MSE loss: 91.1791\n",
      "Iteration: 135600\n",
      "Gradient: [ 12.2134  19.491   -2.3082 -24.4328 161.7635]\n",
      "Weights: [-4.7947  0.7546 -1.2671  0.1415  0.139 ]\n",
      "MSE loss: 91.2131\n",
      "Iteration: 135700\n",
      "Gradient: [  -6.3725   -2.6363   -6.1555   38.2442 -325.7017]\n",
      "Weights: [-4.806   0.7572 -1.2696  0.142   0.139 ]\n",
      "MSE loss: 91.7342\n",
      "Iteration: 135800\n",
      "Gradient: [ 1.439000e-01  3.803000e+00  1.394940e+01 -1.668365e+02 -2.200250e+01]\n",
      "Weights: [-4.7911  0.7681 -1.271   0.1415  0.1389]\n",
      "MSE loss: 91.1683\n",
      "Iteration: 135900\n",
      "Gradient: [-10.464   14.5971   2.9555 -55.6414  32.5998]\n",
      "Weights: [-4.7927  0.7687 -1.2707  0.1417  0.1389]\n",
      "MSE loss: 91.2057\n",
      "Iteration: 136000\n",
      "Gradient: [ -5.4638  -5.1878   3.9521 -54.5124 -52.847 ]\n",
      "Weights: [-4.7837  0.7672 -1.2741  0.142   0.1389]\n",
      "MSE loss: 91.2544\n",
      "Iteration: 136100\n",
      "Gradient: [ -6.4957  16.4271  14.704  -50.0303 469.9464]\n",
      "Weights: [-4.7969  0.7739 -1.2744  0.1431  0.1387]\n",
      "MSE loss: 91.1881\n",
      "Iteration: 136200\n",
      "Gradient: [ 15.2856  -0.8254 -32.0512 -22.4824 122.0334]\n",
      "Weights: [-4.7932  0.769  -1.2758  0.1432  0.1387]\n",
      "MSE loss: 91.1668\n",
      "Iteration: 136300\n",
      "Gradient: [   7.3778    5.0582  -30.6894    4.3343 -265.242 ]\n",
      "Weights: [-4.7919  0.7655 -1.2737  0.1428  0.1388]\n",
      "MSE loss: 91.1278\n",
      "Iteration: 136400\n",
      "Gradient: [ 20.2014  17.4137  57.5082 -14.6492  31.0245]\n",
      "Weights: [-4.7835  0.771  -1.2753  0.143   0.1388]\n",
      "MSE loss: 91.4018\n",
      "Iteration: 136500\n",
      "Gradient: [-19.0297  -3.9549   2.466  -72.3335  61.324 ]\n",
      "Weights: [-4.8025  0.7711 -1.2734  0.1427  0.139 ]\n",
      "MSE loss: 91.165\n",
      "Iteration: 136600\n",
      "Gradient: [  2.1849  -7.7514 -17.1351 -55.845  157.576 ]\n",
      "Weights: [-4.7783  0.7507 -1.2704  0.1435  0.1387]\n",
      "MSE loss: 91.1867\n",
      "Iteration: 136700\n",
      "Gradient: [ 13.895    7.9707   4.6116 153.8562 230.8974]\n",
      "Weights: [-4.7892  0.7584 -1.2679  0.1436  0.1384]\n",
      "MSE loss: 91.2318\n",
      "Iteration: 136800\n",
      "Gradient: [  6.0813  14.5048   6.9249  67.103  619.1224]\n",
      "Weights: [-4.7909  0.7677 -1.268   0.1431  0.1383]\n",
      "MSE loss: 91.6948\n",
      "Iteration: 136900\n",
      "Gradient: [   7.0125   -3.9309  -16.421   -27.7682 -360.8843]\n",
      "Weights: [-4.8038  0.7669 -1.2682  0.1424  0.1383]\n",
      "MSE loss: 91.1997\n",
      "Iteration: 137000\n",
      "Gradient: [ 12.1948   1.6478  30.0965 -79.0331 120.2859]\n",
      "Weights: [-4.7995  0.7687 -1.2727  0.1438  0.1384]\n",
      "MSE loss: 91.0966\n",
      "Iteration: 137100\n",
      "Gradient: [  3.29   -10.0863  68.4914  55.5906 199.8517]\n",
      "Weights: [-4.7772  0.7586 -1.2697  0.1445  0.1379]\n",
      "MSE loss: 91.4121\n",
      "Iteration: 137200\n",
      "Gradient: [  2.1134  -5.6062  28.6903  -7.3198 139.9847]\n",
      "Weights: [-4.7813  0.7552 -1.2691  0.1438  0.1381]\n",
      "MSE loss: 91.1958\n",
      "Iteration: 137300\n",
      "Gradient: [ -10.373    -5.3566    8.9645  104.5613 -232.0715]\n",
      "Weights: [-4.8056  0.7626 -1.2699  0.1445  0.138 ]\n",
      "MSE loss: 91.2616\n",
      "Iteration: 137400\n",
      "Gradient: [ -2.0578  20.3363 -16.9965 -21.5226 -13.478 ]\n",
      "Weights: [-4.7854  0.7671 -1.2742  0.1447  0.138 ]\n",
      "MSE loss: 91.214\n",
      "Iteration: 137500\n",
      "Gradient: [ 4.07650e+00 -4.43160e+00 -1.99045e+01 -2.70000e-03  5.59161e+01]\n",
      "Weights: [-4.79    0.7699 -1.2776  0.145   0.1382]\n",
      "MSE loss: 91.1395\n",
      "Iteration: 137600\n",
      "Gradient: [  0.1461 -11.144   -9.6664  18.256  -98.7595]\n",
      "Weights: [-4.8175  0.7857 -1.2791  0.1453  0.1382]\n",
      "MSE loss: 91.2148\n",
      "Iteration: 137700\n",
      "Gradient: [  4.9428  -0.3147 -11.6386 119.5505 -34.368 ]\n",
      "Weights: [-4.8035  0.7793 -1.2791  0.1452  0.1382]\n",
      "MSE loss: 91.0793\n",
      "Iteration: 137800\n",
      "Gradient: [  -7.5199  -25.2452   10.7165  -66.7009 -167.0626]\n",
      "Weights: [-4.8196  0.7871 -1.2848  0.1456  0.1382]\n",
      "MSE loss: 91.9381\n",
      "Iteration: 137900\n",
      "Gradient: [  -9.3558  -13.2951   24.2059 -130.1188  -25.363 ]\n",
      "Weights: [-4.8178  0.7911 -1.2863  0.1459  0.1383]\n",
      "MSE loss: 91.4529\n",
      "Iteration: 138000\n",
      "Gradient: [-2.918  15.5403 16.8398 47.7685 80.805 ]\n",
      "Weights: [-4.7996  0.789  -1.2852  0.1459  0.1385]\n",
      "MSE loss: 91.1977\n",
      "Iteration: 138100\n",
      "Gradient: [ -5.204   -7.9383 -37.9002 125.5511  44.0073]\n",
      "Weights: [-4.8059  0.784  -1.2836  0.146   0.1383]\n",
      "MSE loss: 91.1002\n",
      "Iteration: 138200\n",
      "Gradient: [  -1.4879   -4.829    -4.0605    5.1091 -142.2682]\n",
      "Weights: [-4.8167  0.7892 -1.285   0.1464  0.1382]\n",
      "MSE loss: 91.2786\n",
      "Iteration: 138300\n",
      "Gradient: [ 13.8686   7.8846  15.8029 -11.078   81.4287]\n",
      "Weights: [-4.7805  0.7789 -1.2854  0.1465  0.1381]\n",
      "MSE loss: 91.3845\n",
      "Iteration: 138400\n",
      "Gradient: [  1.5575  13.7296 -11.2751   7.9392 158.8757]\n",
      "Weights: [-4.7827  0.7688 -1.2837  0.1469  0.1384]\n",
      "MSE loss: 91.1434\n",
      "Iteration: 138500\n",
      "Gradient: [  2.4745 -14.0665  40.2318  55.1225  93.0347]\n",
      "Weights: [-4.7973  0.7787 -1.2826  0.1468  0.1382]\n",
      "MSE loss: 91.072\n",
      "Iteration: 138600\n",
      "Gradient: [  10.6732    7.6991  -42.4173   39.9983 -111.5591]\n",
      "Weights: [-4.8145  0.7871 -1.2817  0.1464  0.1381]\n",
      "MSE loss: 91.1662\n",
      "Iteration: 138700\n",
      "Gradient: [  -0.624     8.0311   44.4255 -155.8996 -108.5599]\n",
      "Weights: [-4.8063  0.7812 -1.2824  0.1466  0.138 ]\n",
      "MSE loss: 91.0979\n",
      "Iteration: 138800\n",
      "Gradient: [ -16.7755  -15.4178   22.8448  -23.7706 -160.3515]\n",
      "Weights: [-4.8195  0.7856 -1.2821  0.1467  0.138 ]\n",
      "MSE loss: 91.283\n",
      "Iteration: 138900\n",
      "Gradient: [   9.4784    2.7856  -34.3142  -10.5141 -104.4034]\n",
      "Weights: [-4.7928  0.7809 -1.2836  0.1461  0.1383]\n",
      "MSE loss: 91.1394\n",
      "Iteration: 139000\n",
      "Gradient: [ -4.0142   2.2632 -40.9914 -15.5767 429.339 ]\n",
      "Weights: [-4.8042  0.7728 -1.2838  0.1473  0.1385]\n",
      "MSE loss: 91.1946\n",
      "Iteration: 139100\n",
      "Gradient: [ -9.1845   4.9449   6.7649 -50.816  -28.5397]\n",
      "Weights: [-4.7967  0.7778 -1.2863  0.1474  0.1383]\n",
      "MSE loss: 91.0664\n",
      "Iteration: 139200\n",
      "Gradient: [  9.9511  -9.1118  -2.893  101.3533  84.6921]\n",
      "Weights: [-4.7924  0.7772 -1.2857  0.1473  0.1384]\n",
      "MSE loss: 91.0766\n",
      "Iteration: 139300\n",
      "Gradient: [  -0.7898   14.5974   -8.7988  148.8913 -137.4428]\n",
      "Weights: [-4.7883  0.7748 -1.2858  0.1474  0.1385]\n",
      "MSE loss: 91.128\n",
      "Iteration: 139400\n",
      "Gradient: [  1.6645  17.3892 -10.381    4.07   365.3856]\n",
      "Weights: [-4.7981  0.7757 -1.2862  0.1469  0.1384]\n",
      "MSE loss: 91.2281\n",
      "Iteration: 139500\n",
      "Gradient: [ -5.4491 -20.4343 -55.8782  -0.4235 377.2942]\n",
      "Weights: [-4.7965  0.7616 -1.2807  0.1466  0.1385]\n",
      "MSE loss: 91.3634\n",
      "Iteration: 139600\n",
      "Gradient: [  -5.6799   -3.45     84.6462   82.0569 -259.1052]\n",
      "Weights: [-4.7972  0.7693 -1.2788  0.1462  0.1384]\n",
      "MSE loss: 91.084\n",
      "Iteration: 139700\n",
      "Gradient: [  4.8412   3.305   22.9243 105.81    15.388 ]\n",
      "Weights: [-4.8076  0.7748 -1.2796  0.1463  0.1382]\n",
      "MSE loss: 91.1637\n",
      "Iteration: 139800\n",
      "Gradient: [  -2.0895   -9.6416    3.9279  -16.4592 -291.3678]\n",
      "Weights: [-4.7907  0.774  -1.2848  0.1464  0.1385]\n",
      "MSE loss: 91.1404\n",
      "Iteration: 139900\n",
      "Gradient: [   8.9726   -6.3261   21.7993   18.9433 -133.0229]\n",
      "Weights: [-4.789   0.7761 -1.2865  0.1466  0.1385]\n",
      "MSE loss: 91.1259\n",
      "Iteration: 140000\n",
      "Gradient: [   7.3262   17.119   -10.4233  -60.3098 -243.7584]\n",
      "Weights: [-4.7876  0.7778 -1.286   0.1466  0.1382]\n",
      "MSE loss: 91.2296\n",
      "Iteration: 140100\n",
      "Gradient: [ -3.958   10.612   -6.0203 -11.3592 529.8232]\n",
      "Weights: [-4.7986  0.7782 -1.2876  0.1474  0.1387]\n",
      "MSE loss: 91.0855\n",
      "Iteration: 140200\n",
      "Gradient: [ -4.7014  -5.6777 -19.1131  -7.7196 191.7782]\n",
      "Weights: [-4.8068  0.7807 -1.2881  0.1479  0.1384]\n",
      "MSE loss: 91.1867\n",
      "Iteration: 140300\n",
      "Gradient: [ -8.8073  21.4144  17.7163 -20.0692 104.698 ]\n",
      "Weights: [-4.8041  0.794  -1.2915  0.1479  0.1384]\n",
      "MSE loss: 91.1353\n",
      "Iteration: 140400\n",
      "Gradient: [  -8.8869   -0.934   -86.9915  -29.6205 -131.8477]\n",
      "Weights: [-4.8123  0.8061 -1.2962  0.1468  0.1386]\n",
      "MSE loss: 91.1211\n",
      "Iteration: 140500\n",
      "Gradient: [-5.0584  4.0493 43.4495 56.0912 47.0769]\n",
      "Weights: [-4.8165  0.8003 -1.2913  0.1465  0.1385]\n",
      "MSE loss: 91.176\n",
      "Iteration: 140600\n",
      "Gradient: [  2.6816 -12.1522 -15.5392  23.0193   2.5148]\n",
      "Weights: [-4.8272  0.8086 -1.2914  0.1456  0.1385]\n",
      "MSE loss: 91.3809\n",
      "Iteration: 140700\n",
      "Gradient: [  6.2835  15.1932 -12.5292  12.2306 193.3322]\n",
      "Weights: [-4.8175  0.8118 -1.292   0.1458  0.1386]\n",
      "MSE loss: 91.2247\n",
      "Iteration: 140800\n",
      "Gradient: [  10.4033    0.867    -4.9851    1.9183 -242.068 ]\n",
      "Weights: [-4.806   0.7978 -1.2905  0.1461  0.1386]\n",
      "MSE loss: 91.0836\n",
      "Iteration: 140900\n",
      "Gradient: [   9.7636  -12.5129  -12.0496 -126.4155  205.9773]\n",
      "Weights: [-4.8139  0.7972 -1.291   0.1462  0.1386]\n",
      "MSE loss: 91.1668\n",
      "Iteration: 141000\n",
      "Gradient: [ -3.8427  -3.1262  16.8078  69.5825 162.6523]\n",
      "Weights: [-4.818   0.7999 -1.2901  0.1467  0.1388]\n",
      "MSE loss: 91.3178\n",
      "Iteration: 141100\n",
      "Gradient: [  8.6906  -8.194  -32.8004 -32.382  245.0504]\n",
      "Weights: [-4.7988  0.7927 -1.2929  0.1469  0.1388]\n",
      "MSE loss: 91.0702\n",
      "Iteration: 141200\n",
      "Gradient: [ -8.3652  15.427  -16.404   42.4143  26.5235]\n",
      "Weights: [-4.8137  0.8025 -1.2949  0.1462  0.139 ]\n",
      "MSE loss: 91.1171\n",
      "Iteration: 141300\n",
      "Gradient: [ -8.5898  22.3948   0.254  -65.9251  62.6496]\n",
      "Weights: [-4.8153  0.8071 -1.2966  0.1461  0.1391]\n",
      "MSE loss: 91.113\n",
      "Iteration: 141400\n",
      "Gradient: [  6.9246   3.7915  13.1116  85.0812 -82.2589]\n",
      "Weights: [-4.7894  0.794  -1.2962  0.1465  0.1391]\n",
      "MSE loss: 91.2072\n",
      "Iteration: 141500\n",
      "Gradient: [ -0.7902  -7.62    22.5805 -28.1048  -0.419 ]\n",
      "Weights: [-4.8077  0.7909 -1.2961  0.1476  0.1391]\n",
      "MSE loss: 91.2252\n",
      "Iteration: 141600\n",
      "Gradient: [ 11.4433   0.7108  -1.6027  -3.555  -27.6698]\n",
      "Weights: [-4.8095  0.8041 -1.2983  0.1476  0.139 ]\n",
      "MSE loss: 91.0823\n",
      "Iteration: 141700\n",
      "Gradient: [ -7.2078   4.0646 -37.6257 -33.8916  74.1819]\n",
      "Weights: [-4.8074  0.8012 -1.2957  0.1481  0.1387]\n",
      "MSE loss: 91.1625\n",
      "Iteration: 141800\n",
      "Gradient: [  8.4619  -9.9128 -37.858   -8.8412 -85.7226]\n",
      "Weights: [-4.8016  0.7924 -1.2967  0.1479  0.1388]\n",
      "MSE loss: 91.1054\n",
      "Iteration: 141900\n",
      "Gradient: [ -5.5612 -23.1467 -34.5881  39.0883 278.6602]\n",
      "Weights: [-4.7968  0.7914 -1.2973  0.1482  0.1388]\n",
      "MSE loss: 91.0861\n",
      "Iteration: 142000\n",
      "Gradient: [ -0.8553   1.8445   1.6448 -82.6623 121.5189]\n",
      "Weights: [-4.8027  0.794  -1.2962  0.1475  0.1389]\n",
      "MSE loss: 91.0879\n",
      "Iteration: 142100\n",
      "Gradient: [  -8.8024   -1.7031  -22.7708  -40.0097 -284.7297]\n",
      "Weights: [-4.7994  0.7833 -1.2919  0.1473  0.1387]\n",
      "MSE loss: 91.2543\n",
      "Iteration: 142200\n",
      "Gradient: [  1.9637 -20.8169  37.6837  68.7099  74.0155]\n",
      "Weights: [-4.7998  0.7838 -1.2895  0.1467  0.1385]\n",
      "MSE loss: 91.2221\n",
      "Iteration: 142300\n",
      "Gradient: [  1.6607   9.5843 -48.3911  27.6838  70.1144]\n",
      "Weights: [-4.807   0.7962 -1.2902  0.1464  0.1385]\n",
      "MSE loss: 91.0689\n",
      "Iteration: 142400\n",
      "Gradient: [   3.4375   -6.7712  -15.2239 -134.8186   -8.7035]\n",
      "Weights: [-4.7917  0.7898 -1.2906  0.1469  0.1384]\n",
      "MSE loss: 91.1752\n",
      "Iteration: 142500\n",
      "Gradient: [  4.0619  17.0198  -9.7361 -47.1383 115.7655]\n",
      "Weights: [-4.7968  0.7972 -1.2926  0.1474  0.1384]\n",
      "MSE loss: 91.2546\n",
      "Iteration: 142600\n",
      "Gradient: [  5.9256   0.6867   7.3873 -49.9555 115.36  ]\n",
      "Weights: [-4.8104  0.8044 -1.2917  0.1466  0.1382]\n",
      "MSE loss: 91.1212\n",
      "Iteration: 142700\n",
      "Gradient: [ 12.8389  11.1101  10.5688  56.6058 236.0823]\n",
      "Weights: [-4.8022  0.8032 -1.2899  0.1464  0.1384]\n",
      "MSE loss: 91.4877\n",
      "Iteration: 142800\n",
      "Gradient: [  8.5701 -15.6675 -16.4601  13.3391  46.3207]\n",
      "Weights: [-4.8075  0.7967 -1.2895  0.147   0.1383]\n",
      "MSE loss: 91.1025\n",
      "Iteration: 142900\n",
      "Gradient: [ 11.8086   8.4019  13.0839  77.8815 148.0168]\n",
      "Weights: [-4.7962  0.7935 -1.288   0.1473  0.1381]\n",
      "MSE loss: 91.3876\n",
      "Iteration: 143000\n",
      "Gradient: [ -2.7661   2.1323  41.9278 128.2899   2.5912]\n",
      "Weights: [-4.7921  0.7783 -1.2844  0.1478  0.1382]\n",
      "MSE loss: 91.2503\n",
      "Iteration: 143100\n",
      "Gradient: [ -7.0251 -12.5543 -48.1135 -17.0673  84.4395]\n",
      "Weights: [-4.8055  0.7793 -1.2846  0.1465  0.1384]\n",
      "MSE loss: 91.231\n",
      "Iteration: 143200\n",
      "Gradient: [ -0.7792   3.185  -39.1915  -6.0479 107.8176]\n",
      "Weights: [-4.8099  0.7881 -1.2866  0.1466  0.1382]\n",
      "MSE loss: 91.1659\n",
      "Iteration: 143300\n",
      "Gradient: [  14.64     20.4103   -5.7252   46.0097 -139.3773]\n",
      "Weights: [-4.797   0.797  -1.2868  0.1465  0.1382]\n",
      "MSE loss: 91.7165\n",
      "Iteration: 143400\n",
      "Gradient: [ -7.6446  17.6314  20.3413  54.9662 216.8289]\n",
      "Weights: [-4.8242  0.7964 -1.2875  0.147   0.1383]\n",
      "MSE loss: 91.2985\n",
      "Iteration: 143500\n",
      "Gradient: [ -7.3082  -7.9449  61.9363 -48.2382 -58.9974]\n",
      "Weights: [-4.8174  0.7993 -1.2903  0.1467  0.1383]\n",
      "MSE loss: 91.2198\n",
      "Iteration: 143600\n",
      "Gradient: [   1.2419   -9.901   -20.6944   18.3478 -116.0438]\n",
      "Weights: [-4.8104  0.8008 -1.2938  0.1479  0.1382]\n",
      "MSE loss: 91.0594\n",
      "Iteration: 143700\n",
      "Gradient: [  -5.6084   16.7233    7.0964   90.4914 -280.0134]\n",
      "Weights: [-4.8132  0.8041 -1.2929  0.1472  0.1382]\n",
      "MSE loss: 91.1299\n",
      "Iteration: 143800\n",
      "Gradient: [ -2.1836  -5.026  -10.3073  81.0553  64.814 ]\n",
      "Weights: [-4.823   0.8011 -1.2911  0.1473  0.1384]\n",
      "MSE loss: 91.2256\n",
      "Iteration: 143900\n",
      "Gradient: [ -8.1548  12.068  -28.3914  13.1958  63.0162]\n",
      "Weights: [-4.8147  0.7876 -1.2866  0.1475  0.1383]\n",
      "MSE loss: 91.1763\n",
      "Iteration: 144000\n",
      "Gradient: [  0.1359   3.1035  14.5026  28.7538 101.2348]\n",
      "Weights: [-4.8112  0.7966 -1.2842  0.1455  0.1381]\n",
      "MSE loss: 91.1502\n",
      "Iteration: 144100\n",
      "Gradient: [ -13.2105  -16.2078   35.4413 -215.1664  270.2977]\n",
      "Weights: [-4.8347  0.8017 -1.2862  0.1462  0.1381]\n",
      "MSE loss: 91.6204\n",
      "Iteration: 144200\n",
      "Gradient: [15.5709  7.1685 -2.7607 96.5492 28.6235]\n",
      "Weights: [-4.8096  0.7915 -1.2855  0.1465  0.1382]\n",
      "MSE loss: 91.0888\n",
      "Iteration: 144300\n",
      "Gradient: [ 12.0299   2.2668  15.0889  69.4214 125.2305]\n",
      "Weights: [-4.7877  0.7785 -1.2844  0.1465  0.1384]\n",
      "MSE loss: 91.2477\n",
      "Iteration: 144400\n",
      "Gradient: [  -3.0784   -6.4982   14.2608   13.8508 -276.7471]\n",
      "Weights: [-4.7869  0.7747 -1.284   0.1463  0.1385]\n",
      "MSE loss: 91.1477\n",
      "Iteration: 144500\n",
      "Gradient: [  11.9737    0.8878    6.1788 -177.0469 -242.3884]\n",
      "Weights: [-4.7994  0.7698 -1.2827  0.1463  0.1386]\n",
      "MSE loss: 91.2685\n",
      "Iteration: 144600\n",
      "Gradient: [ -2.272    3.8307  11.7868  92.744  204.9799]\n",
      "Weights: [-4.7909  0.7674 -1.2807  0.1462  0.1385]\n",
      "MSE loss: 91.0836\n",
      "Iteration: 144700\n",
      "Gradient: [  1.6114  13.8856 -11.691    2.2255  63.3615]\n",
      "Weights: [-4.8055  0.7705 -1.279   0.1462  0.1384]\n",
      "MSE loss: 91.1852\n",
      "Iteration: 144800\n",
      "Gradient: [  -1.2574    1.8423  -22.1541  -67.3214 -372.049 ]\n",
      "Weights: [-4.7748  0.7561 -1.2777  0.1462  0.1382]\n",
      "MSE loss: 91.2335\n",
      "Iteration: 144900\n",
      "Gradient: [   1.6757    2.289   -37.2459   32.368  -188.2934]\n",
      "Weights: [-4.7843  0.7567 -1.2761  0.1465  0.1381]\n",
      "MSE loss: 91.1255\n",
      "Iteration: 145000\n",
      "Gradient: [  4.0176  -9.2364 -29.6914 -81.9303 324.1995]\n",
      "Weights: [-4.7798  0.7509 -1.2762  0.1471  0.138 ]\n",
      "MSE loss: 91.2137\n",
      "Iteration: 145100\n",
      "Gradient: [  1.2979   6.4702 -26.4619  83.7781 122.353 ]\n",
      "Weights: [-4.7838  0.7545 -1.2735  0.1472  0.1377]\n",
      "MSE loss: 91.0812\n",
      "Iteration: 145200\n",
      "Gradient: [ -5.0159  18.847   -2.6713 -46.5771  29.4787]\n",
      "Weights: [-4.7826  0.7489 -1.2734  0.1474  0.1379]\n",
      "MSE loss: 91.0998\n",
      "Iteration: 145300\n",
      "Gradient: [ -8.8358  -0.4164  11.0436  53.7107 -79.843 ]\n",
      "Weights: [-4.7697  0.7452 -1.2746  0.1464  0.1384]\n",
      "MSE loss: 91.2581\n",
      "Iteration: 145400\n",
      "Gradient: [  0.9722  -5.5499  50.7964 147.2914  72.2305]\n",
      "Weights: [-4.7819  0.7506 -1.277   0.1472  0.1382]\n",
      "MSE loss: 91.1764\n",
      "Iteration: 145500\n",
      "Gradient: [ -1.0718 -19.0167 -11.481   34.0254 -70.6624]\n",
      "Weights: [-4.7801  0.7473 -1.274   0.1469  0.1381]\n",
      "MSE loss: 91.1432\n",
      "Iteration: 145600\n",
      "Gradient: [  0.5047  32.1034   5.8526 152.9736  73.5952]\n",
      "Weights: [-4.784   0.7592 -1.2769  0.1475  0.1381]\n",
      "MSE loss: 91.2784\n",
      "Iteration: 145700\n",
      "Gradient: [-1.7601  2.1126 30.7128 93.3927 79.2412]\n",
      "Weights: [-4.7914  0.7703 -1.279   0.1466  0.1381]\n",
      "MSE loss: 91.0953\n",
      "Iteration: 145800\n",
      "Gradient: [   6.4475   -3.5561   -0.2779  -26.0194 -127.6759]\n",
      "Weights: [-4.8014  0.7731 -1.2789  0.1462  0.1378]\n",
      "MSE loss: 91.2535\n",
      "Iteration: 145900\n",
      "Gradient: [  -4.5712   -2.5806   54.638   -60.3183 -191.7302]\n",
      "Weights: [-4.7972  0.7707 -1.2787  0.1473  0.1377]\n",
      "MSE loss: 91.0609\n",
      "Iteration: 146000\n",
      "Gradient: [ -10.4458   -6.7405   -6.1675  -77.187  -143.2881]\n",
      "Weights: [-4.7974  0.7622 -1.2769  0.1479  0.1374]\n",
      "MSE loss: 91.2508\n",
      "Iteration: 146100\n",
      "Gradient: [ -2.6529  -2.6597  24.7659  53.3773 209.1417]\n",
      "Weights: [-4.7861  0.7563 -1.2768  0.1487  0.1375]\n",
      "MSE loss: 91.0519\n",
      "Iteration: 146200\n",
      "Gradient: [ -6.9076 -21.9785 -38.772  -41.6848 243.3123]\n",
      "Weights: [-4.8081  0.7713 -1.2765  0.1479  0.1372]\n",
      "MSE loss: 91.2071\n",
      "Iteration: 146300\n",
      "Gradient: [ -4.7964  -2.8026   9.4244  41.7677 -25.8286]\n",
      "Weights: [-4.8059  0.7709 -1.2763  0.1479  0.1372]\n",
      "MSE loss: 91.1206\n",
      "Iteration: 146400\n",
      "Gradient: [  4.9977   4.1107   9.6806   2.3318 174.2047]\n",
      "Weights: [-4.7859  0.762  -1.2718  0.1475  0.1372]\n",
      "MSE loss: 91.3016\n",
      "Iteration: 146500\n",
      "Gradient: [  6.616   -3.6575   8.8695  -1.1371 291.4769]\n",
      "Weights: [-4.784   0.7595 -1.2726  0.1478  0.1374]\n",
      "MSE loss: 91.3439\n",
      "Iteration: 146600\n",
      "Gradient: [ -11.2571    5.2638    0.8078  -39.426  -233.3587]\n",
      "Weights: [-4.807   0.7612 -1.2733  0.1482  0.1373]\n",
      "MSE loss: 91.2394\n",
      "Iteration: 146700\n",
      "Gradient: [   1.4491   18.0506    0.1152   23.0034 -100.7547]\n",
      "Weights: [-4.7843  0.7513 -1.2684  0.1481  0.137 ]\n",
      "MSE loss: 91.1934\n",
      "Iteration: 146800\n",
      "Gradient: [  3.6383 -25.0554 -26.1836  42.2374  78.4391]\n",
      "Weights: [-4.7558  0.7307 -1.2687  0.1485  0.1372]\n",
      "MSE loss: 91.4954\n",
      "Iteration: 146900\n",
      "Gradient: [  -7.5646   18.6109   12.7827 -102.4313   24.9653]\n",
      "Weights: [-4.7925  0.7409 -1.2669  0.1485  0.1371]\n",
      "MSE loss: 91.1875\n",
      "Iteration: 147000\n",
      "Gradient: [  -2.0411   -1.5032   14.1489    1.6017 -152.6771]\n",
      "Weights: [-4.7873  0.7445 -1.2689  0.1489  0.137 ]\n",
      "MSE loss: 91.0585\n",
      "Iteration: 147100\n",
      "Gradient: [ -9.1745  -5.6901 -30.8964  54.2084 -42.0332]\n",
      "Weights: [-4.7912  0.7486 -1.2685  0.1478  0.137 ]\n",
      "MSE loss: 91.1538\n",
      "Iteration: 147200\n",
      "Gradient: [  4.5899  10.3732  57.3434  37.1973 189.2997]\n",
      "Weights: [-4.7786  0.7446 -1.2711  0.1496  0.1369]\n",
      "MSE loss: 91.0876\n",
      "Iteration: 147300\n",
      "Gradient: [  -1.6809    2.3759   14.1314  -22.5931 -249.3368]\n",
      "Weights: [-4.8017  0.7536 -1.2734  0.1499  0.137 ]\n",
      "MSE loss: 91.2107\n",
      "Iteration: 147400\n",
      "Gradient: [  1.8994  18.1132  43.3536  37.9582 114.6309]\n",
      "Weights: [-4.7794  0.7514 -1.2706  0.1489  0.1371]\n",
      "MSE loss: 91.4813\n",
      "Iteration: 147500\n",
      "Gradient: [ -1.1235  -1.1448  -8.6827 -17.9013  29.6971]\n",
      "Weights: [-4.8066  0.7577 -1.273   0.1489  0.1372]\n",
      "MSE loss: 91.2719\n",
      "Iteration: 147600\n",
      "Gradient: [  7.6626   4.9393  20.9144  93.2172 129.2536]\n",
      "Weights: [-4.7861  0.7627 -1.2741  0.1486  0.1369]\n",
      "MSE loss: 91.169\n",
      "Iteration: 147700\n",
      "Gradient: [ 15.8395 -17.7882  -4.1915 -58.3292 -71.0227]\n",
      "Weights: [-4.7942  0.7616 -1.2732  0.1485  0.1369]\n",
      "MSE loss: 91.0535\n",
      "Iteration: 147800\n",
      "Gradient: [ -2.1734  10.894   14.4124  24.9296 -69.6048]\n",
      "Weights: [-4.7976  0.7648 -1.2716  0.1478  0.1371]\n",
      "MSE loss: 91.1257\n",
      "Iteration: 147900\n",
      "Gradient: [  -1.4942   -4.6877  -13.5239  -14.7925 -406.0994]\n",
      "Weights: [-4.7966  0.7557 -1.2725  0.1483  0.1371]\n",
      "MSE loss: 91.1317\n",
      "Iteration: 148000\n",
      "Gradient: [-12.1135   4.2081  -9.4161   0.6276  22.21  ]\n",
      "Weights: [-4.7956  0.753  -1.2724  0.1488  0.137 ]\n",
      "MSE loss: 91.144\n",
      "Iteration: 148100\n",
      "Gradient: [ -5.454    5.1681 -27.6634  66.0236 140.2979]\n",
      "Weights: [-4.7837  0.7494 -1.2715  0.1482  0.1372]\n",
      "MSE loss: 91.0734\n",
      "Iteration: 148200\n",
      "Gradient: [  0.1538   7.3255  14.6084 -18.5744   4.7671]\n",
      "Weights: [-4.7837  0.7477 -1.2711  0.1485  0.1373]\n",
      "MSE loss: 91.0893\n",
      "Iteration: 148300\n",
      "Gradient: [  2.4795  -5.177    1.8919   5.4099 -57.5159]\n",
      "Weights: [-4.7929  0.7574 -1.2725  0.1486  0.137 ]\n",
      "MSE loss: 91.0361\n",
      "Iteration: 148400\n",
      "Gradient: [  2.041   18.0821  53.8274 -58.4621 370.0831]\n",
      "Weights: [-4.7997  0.7595 -1.2715  0.1492  0.1369]\n",
      "MSE loss: 91.1226\n",
      "Iteration: 148500\n",
      "Gradient: [  -4.6812   -2.3842   11.418   -21.505  -135.6376]\n",
      "Weights: [-4.7912  0.7506 -1.2708  0.1492  0.1367]\n",
      "MSE loss: 91.0938\n",
      "Iteration: 148600\n",
      "Gradient: [  -8.6047   -3.1456   -3.2601  -34.6925 -298.4165]\n",
      "Weights: [-4.7977  0.7584 -1.2743  0.1496  0.1369]\n",
      "MSE loss: 91.0673\n",
      "Iteration: 148700\n",
      "Gradient: [ -6.8399   2.962  -27.7315 -79.9639 -61.7041]\n",
      "Weights: [-4.8031  0.7572 -1.2759  0.1503  0.1368]\n",
      "MSE loss: 91.3095\n",
      "Iteration: 148800\n",
      "Gradient: [ -8.0242  10.0146  -3.7716 -87.43   -84.7575]\n",
      "Weights: [-4.7972  0.7548 -1.2748  0.1501  0.1369]\n",
      "MSE loss: 91.1061\n",
      "Iteration: 148900\n",
      "Gradient: [ -7.7177  -8.9896 -18.7638  40.53    63.4804]\n",
      "Weights: [-4.7758  0.7478 -1.2735  0.1492  0.1368]\n",
      "MSE loss: 91.2441\n",
      "Iteration: 149000\n",
      "Gradient: [ -3.2699  -2.6052 -17.0315  57.3099  25.3577]\n",
      "Weights: [-4.7921  0.758  -1.2746  0.1494  0.1368]\n",
      "MSE loss: 91.0648\n",
      "Iteration: 149100\n",
      "Gradient: [  1.3821  -2.0952 -14.1904   4.3505 -36.7485]\n",
      "Weights: [-4.7889  0.7578 -1.2744  0.1495  0.1367]\n",
      "MSE loss: 91.052\n",
      "Iteration: 149200\n",
      "Gradient: [ 21.1972   2.5764 -13.9334  67.409  -74.3146]\n",
      "Weights: [-4.7885  0.7586 -1.2751  0.1492  0.1369]\n",
      "MSE loss: 91.0367\n",
      "Iteration: 149300\n",
      "Gradient: [ 3.876500e+00 -4.557800e+00  7.100000e-03  1.634800e+00  3.117487e+02]\n",
      "Weights: [-4.801   0.7692 -1.2756  0.1485  0.137 ]\n",
      "MSE loss: 91.0483\n",
      "Iteration: 149400\n",
      "Gradient: [ -2.8061   4.324   16.736    9.9946 -24.9897]\n",
      "Weights: [-4.7995  0.7717 -1.2763  0.1476  0.1372]\n",
      "MSE loss: 91.0726\n",
      "Iteration: 149500\n",
      "Gradient: [  2.8999  26.8323  27.9467  38.0748 175.8064]\n",
      "Weights: [-4.7887  0.7804 -1.2825  0.1481  0.1376]\n",
      "MSE loss: 91.4227\n",
      "Iteration: 149600\n",
      "Gradient: [-16.6781  -5.771  -24.8377 -95.8175 -21.2728]\n",
      "Weights: [-4.7944  0.7745 -1.2837  0.1485  0.1376]\n",
      "MSE loss: 91.0819\n",
      "Iteration: 149700\n",
      "Gradient: [  1.0468 -16.2583 -11.246  -96.3394 143.7573]\n",
      "Weights: [-4.7914  0.7748 -1.2816  0.1473  0.1376]\n",
      "MSE loss: 91.1756\n",
      "Iteration: 149800\n",
      "Gradient: [ -5.0058  13.8381 -24.9037 -95.4438  -6.4114]\n",
      "Weights: [-4.7996  0.7722 -1.28    0.148   0.1378]\n",
      "MSE loss: 91.0992\n",
      "Iteration: 149900\n",
      "Gradient: [  5.0491  -1.5151  -3.1119  71.9662 474.2977]\n",
      "Weights: [-4.778   0.7522 -1.276   0.1477  0.1379]\n",
      "MSE loss: 91.1297\n",
      "Iteration: 150000\n",
      "Gradient: [  -1.0049   -1.475   -31.9696  101.241  -109.081 ]\n",
      "Weights: [-4.7935  0.7693 -1.2767  0.1474  0.1375]\n",
      "MSE loss: 91.0776\n",
      "Iteration: 150100\n",
      "Gradient: [  6.1295   8.9321  33.7055  30.472  154.8798]\n",
      "Weights: [-4.7913  0.7633 -1.2748  0.1473  0.1377]\n",
      "MSE loss: 91.1432\n",
      "Iteration: 150200\n",
      "Gradient: [ -1.3377 -20.7357 -68.6752  16.7219 179.4177]\n",
      "Weights: [-4.7901  0.7563 -1.2775  0.1483  0.1376]\n",
      "MSE loss: 91.2011\n",
      "Iteration: 150300\n",
      "Gradient: [  -6.5243   -3.8088   27.5082 -151.301   149.4901]\n",
      "Weights: [-4.7838  0.7575 -1.2762  0.148   0.1374]\n",
      "MSE loss: 91.1\n",
      "Iteration: 150400\n",
      "Gradient: [   2.1041    9.8901    1.4344 -140.349  -115.9458]\n",
      "Weights: [-4.7941  0.7576 -1.2735  0.1481  0.1375]\n",
      "MSE loss: 91.0668\n",
      "Iteration: 150500\n",
      "Gradient: [  3.5527   0.4501  13.8209  66.8139 116.2812]\n",
      "Weights: [-4.7931  0.7692 -1.2767  0.1481  0.1373]\n",
      "MSE loss: 91.1054\n",
      "Iteration: 150600\n",
      "Gradient: [  9.4972   2.7373  49.0649 151.133   -6.9264]\n",
      "Weights: [-4.7923  0.7773 -1.2807  0.1486  0.1374]\n",
      "MSE loss: 91.3877\n",
      "Iteration: 150700\n",
      "Gradient: [ -2.7315  -3.8431  33.6615  21.6631 294.3783]\n",
      "Weights: [-4.8013  0.772  -1.2796  0.1499  0.1372]\n",
      "MSE loss: 91.1601\n",
      "Iteration: 150800\n",
      "Gradient: [ 14.311   -1.7151  -5.34    44.4346 -16.0585]\n",
      "Weights: [-4.8009  0.7724 -1.2782  0.1497  0.1371]\n",
      "MSE loss: 91.2003\n",
      "Iteration: 150900\n",
      "Gradient: [   4.2314    0.6452   44.1507  200.0465 -330.1045]\n",
      "Weights: [-4.7789  0.7554 -1.2763  0.1501  0.1369]\n",
      "MSE loss: 91.1609\n",
      "Iteration: 151000\n",
      "Gradient: [  -4.2974    1.1142   19.9047   -2.248  -450.075 ]\n",
      "Weights: [-4.7815  0.7515 -1.2743  0.1491  0.1371]\n",
      "MSE loss: 91.0746\n",
      "Iteration: 151100\n",
      "Gradient: [ -3.1864  -2.5827   9.6567 118.4558 -28.7034]\n",
      "Weights: [-4.7887  0.7485 -1.272   0.1502  0.1369]\n",
      "MSE loss: 91.0526\n",
      "Iteration: 151200\n",
      "Gradient: [   5.3053    1.0077  -22.6647   73.2539 -273.3049]\n",
      "Weights: [-4.78    0.7475 -1.2727  0.1497  0.1367]\n",
      "MSE loss: 91.107\n",
      "Iteration: 151300\n",
      "Gradient: [  -1.344   -12.2164  -13.1515 -102.6374 -237.6391]\n",
      "Weights: [-4.7772  0.7354 -1.269   0.1492  0.137 ]\n",
      "MSE loss: 91.1294\n",
      "Iteration: 151400\n",
      "Gradient: [ -4.0807  -5.7354  37.189  -26.5361 466.4536]\n",
      "Weights: [-4.7606  0.73   -1.2681  0.1492  0.1371]\n",
      "MSE loss: 91.3613\n",
      "Iteration: 151500\n",
      "Gradient: [ -4.564  -26.6926  -7.693   65.1596 -81.9523]\n",
      "Weights: [-4.78    0.7273 -1.2645  0.1497  0.1369]\n",
      "MSE loss: 91.1542\n",
      "Iteration: 151600\n",
      "Gradient: [  6.3221  -4.9512  38.1427   8.1208 -32.5877]\n",
      "Weights: [-4.7837  0.7388 -1.2645  0.149   0.1369]\n",
      "MSE loss: 91.1967\n",
      "Iteration: 151700\n",
      "Gradient: [-1.091820e+01 -1.207000e-01 -4.044780e+01 -8.726830e+01 -3.228963e+02]\n",
      "Weights: [-4.7826  0.7284 -1.2606  0.1476  0.1367]\n",
      "MSE loss: 91.3813\n",
      "Iteration: 151800\n",
      "Gradient: [  8.8419   7.5537 -14.544  114.1999 -94.2552]\n",
      "Weights: [-4.7631  0.7284 -1.2606  0.1476  0.1367]\n",
      "MSE loss: 91.3519\n",
      "Iteration: 151900\n",
      "Gradient: [ -0.3001 -13.1647   3.1234  36.0365 142.5883]\n",
      "Weights: [-4.7888  0.7285 -1.2569  0.1479  0.1367]\n",
      "MSE loss: 91.1677\n",
      "Iteration: 152000\n",
      "Gradient: [  6.5907  -7.2244 -12.9089  18.6607   5.114 ]\n",
      "Weights: [-4.783   0.7222 -1.2565  0.1485  0.1365]\n",
      "MSE loss: 91.1532\n",
      "Iteration: 152100\n",
      "Gradient: [ 4.489000e-01 -1.401420e+01  2.930000e-02  2.925720e+01 -1.284838e+02]\n",
      "Weights: [-4.7762  0.7287 -1.257   0.1479  0.1364]\n",
      "MSE loss: 91.1393\n",
      "Iteration: 152200\n",
      "Gradient: [ -5.2217   4.9569  24.1585 -77.337  129.0297]\n",
      "Weights: [-4.7801  0.7276 -1.2539  0.1474  0.1362]\n",
      "MSE loss: 91.114\n",
      "Iteration: 152300\n",
      "Gradient: [  -5.1329   12.3479   41.4015    4.4639 -168.2248]\n",
      "Weights: [-4.7759  0.7303 -1.2514  0.1463  0.1359]\n",
      "MSE loss: 91.3049\n",
      "Iteration: 152400\n",
      "Gradient: [  1.3881  -7.96     7.8867 -17.8827 168.354 ]\n",
      "Weights: [-4.7779  0.7238 -1.2508  0.1469  0.1361]\n",
      "MSE loss: 91.1484\n",
      "Iteration: 152500\n",
      "Gradient: [ -7.745  -10.4467  35.508  -55.7082 -87.3899]\n",
      "Weights: [-4.795   0.7292 -1.2531  0.1466  0.1363]\n",
      "MSE loss: 91.374\n",
      "Iteration: 152600\n",
      "Gradient: [ -11.8695  -11.4098  -31.9172  -38.5239 -324.8485]\n",
      "Weights: [-4.7914  0.7302 -1.254   0.1474  0.1361]\n",
      "MSE loss: 91.1968\n",
      "Iteration: 152700\n",
      "Gradient: [ -1.8231  -1.5344  19.4868  32.4706 181.1584]\n",
      "Weights: [-4.7795  0.722  -1.2519  0.1479  0.1362]\n",
      "MSE loss: 91.1976\n",
      "Iteration: 152800\n",
      "Gradient: [  11.4745    2.8706   25.1622  -44.0437 -155.1019]\n",
      "Weights: [-4.7751  0.7305 -1.2529  0.147   0.1363]\n",
      "MSE loss: 91.5072\n",
      "Iteration: 152900\n",
      "Gradient: [ -8.8615  -7.2796  -0.6531 -34.0585  80.7208]\n",
      "Weights: [-4.7905  0.7294 -1.2517  0.1459  0.1363]\n",
      "MSE loss: 91.2412\n",
      "Iteration: 153000\n",
      "Gradient: [ -3.354    7.3427  38.1291  92.9824 316.2155]\n",
      "Weights: [-4.7747  0.7336 -1.2553  0.147   0.1362]\n",
      "MSE loss: 91.3069\n",
      "Iteration: 153100\n",
      "Gradient: [  -5.8226   -9.2316  -48.6987  -52.2712 -189.4784]\n",
      "Weights: [-4.7914  0.7291 -1.2545  0.1465  0.1362]\n",
      "MSE loss: 91.7282\n",
      "Iteration: 153200\n",
      "Gradient: [ 12.5904   4.57    59.7364  64.2606 172.7513]\n",
      "Weights: [-4.7937  0.7338 -1.2523  0.1473  0.1364]\n",
      "MSE loss: 91.4517\n",
      "Iteration: 153300\n",
      "Gradient: [  3.544   -2.554   -2.7205  76.8627 -33.4488]\n",
      "Weights: [-4.789   0.737  -1.2527  0.1469  0.1362]\n",
      "MSE loss: 91.3944\n",
      "Iteration: 153400\n",
      "Gradient: [-10.8923  -4.3565  26.3924   5.0001 388.9082]\n",
      "Weights: [-4.7896  0.7351 -1.2535  0.1469  0.1363]\n",
      "MSE loss: 91.1559\n",
      "Iteration: 153500\n",
      "Gradient: [ -5.5247  -6.7484  37.1896  45.0797 -56.3902]\n",
      "Weights: [-4.7979  0.7397 -1.2553  0.1473  0.1362]\n",
      "MSE loss: 91.1748\n",
      "Iteration: 153600\n",
      "Gradient: [  7.6158  15.4496  13.0038  74.8988 148.9771]\n",
      "Weights: [-4.7985  0.7507 -1.2577  0.1476  0.1362]\n",
      "MSE loss: 91.4813\n",
      "Iteration: 153700\n",
      "Gradient: [ -3.7753 -10.7752   1.0371 -18.2723 -84.8141]\n",
      "Weights: [-4.8013  0.7444 -1.2577  0.1475  0.1362]\n",
      "MSE loss: 91.1746\n",
      "Iteration: 153800\n",
      "Gradient: [   0.4711  -18.5826  -31.963  -108.619   231.7297]\n",
      "Weights: [-4.7868  0.7427 -1.2596  0.1477  0.136 ]\n",
      "MSE loss: 91.186\n",
      "Iteration: 153900\n",
      "Gradient: [   9.3736   12.0233    5.3283  103.3274 -332.2353]\n",
      "Weights: [-4.7787  0.7391 -1.2594  0.1475  0.1361]\n",
      "MSE loss: 91.2451\n",
      "Iteration: 154000\n",
      "Gradient: [ -1.0162  -5.4716 -27.4    -15.2452 297.3572]\n",
      "Weights: [-4.7952  0.7391 -1.2586  0.1481  0.1361]\n",
      "MSE loss: 91.19\n",
      "Iteration: 154100\n",
      "Gradient: [   8.5904    8.7079  -10.5349 -110.5794 -147.9376]\n",
      "Weights: [-4.7946  0.739  -1.2568  0.1477  0.136 ]\n",
      "MSE loss: 91.1586\n",
      "Iteration: 154200\n",
      "Gradient: [ -9.0101  18.4138 -32.9355 -84.8372 -21.663 ]\n",
      "Weights: [-4.7889  0.7374 -1.2561  0.1478  0.1361]\n",
      "MSE loss: 91.1193\n",
      "Iteration: 154300\n",
      "Gradient: [  1.6538 -21.1259  10.5932 -21.9294  93.2706]\n",
      "Weights: [-4.7863  0.7331 -1.2595  0.1479  0.1362]\n",
      "MSE loss: 91.4341\n",
      "Iteration: 154400\n",
      "Gradient: [  -1.0628   10.6356   27.4928  -49.0447 -133.5719]\n",
      "Weights: [-4.7872  0.7291 -1.2614  0.1486  0.1368]\n",
      "MSE loss: 91.2126\n",
      "Iteration: 154500\n",
      "Gradient: [   4.907     2.8442  -20.0205 -107.981   187.7855]\n",
      "Weights: [-4.7888  0.7351 -1.2618  0.1478  0.1366]\n",
      "MSE loss: 91.3017\n",
      "Iteration: 154600\n",
      "Gradient: [ -0.3855  16.4601 -59.1982  27.5285 133.0591]\n",
      "Weights: [-4.7961  0.7422 -1.2562  0.1473  0.1362]\n",
      "MSE loss: 91.1582\n",
      "Iteration: 154700\n",
      "Gradient: [  5.0273 -15.902   23.1102   9.7569   9.6217]\n",
      "Weights: [-4.7658  0.7249 -1.2554  0.1477  0.1362]\n",
      "MSE loss: 91.3415\n",
      "Iteration: 154800\n",
      "Gradient: [-11.2875   0.5837  11.4416 155.2364 321.8113]\n",
      "Weights: [-4.7863  0.7259 -1.2545  0.1487  0.1361]\n",
      "MSE loss: 91.1262\n",
      "Iteration: 154900\n",
      "Gradient: [  8.7462  -1.8637 -22.3628  23.142  378.6332]\n",
      "Weights: [-4.789   0.718  -1.2524  0.1487  0.1361]\n",
      "MSE loss: 91.297\n",
      "Iteration: 155000\n",
      "Gradient: [ 13.4099   1.1097   3.9376  -4.1273 -99.3448]\n",
      "Weights: [-4.7716  0.7233 -1.252   0.1482  0.1361]\n",
      "MSE loss: 91.5085\n",
      "Iteration: 155100\n",
      "Gradient: [  2.6769   1.0752  56.2013 -16.7831  50.0004]\n",
      "Weights: [-4.7534  0.7067 -1.2511  0.1478  0.1363]\n",
      "MSE loss: 91.4421\n",
      "Iteration: 155200\n",
      "Gradient: [  2.2392   3.3461  55.9518 -49.3537 -14.6731]\n",
      "Weights: [-4.7878  0.7221 -1.252   0.1484  0.136 ]\n",
      "MSE loss: 91.1556\n",
      "Iteration: 155300\n",
      "Gradient: [  -2.563     2.1624  -23.0373   69.3558 -154.4234]\n",
      "Weights: [-4.7779  0.7142 -1.2508  0.1492  0.1357]\n",
      "MSE loss: 91.1169\n",
      "Iteration: 155400\n",
      "Gradient: [  -3.3145  -11.2435   36.3228 -104.0276 -307.503 ]\n",
      "Weights: [-4.7757  0.7181 -1.2518  0.1487  0.1357]\n",
      "MSE loss: 91.1376\n",
      "Iteration: 155500\n",
      "Gradient: [ -7.946  -14.9913 -37.7088 -16.596   95.8659]\n",
      "Weights: [-4.7707  0.7129 -1.2549  0.1495  0.1357]\n",
      "MSE loss: 91.4666\n",
      "Iteration: 155600\n",
      "Gradient: [ -6.1275   9.293   55.369   22.1565 143.0331]\n",
      "Weights: [-4.7804  0.722  -1.2533  0.149   0.1358]\n",
      "MSE loss: 91.1026\n",
      "Iteration: 155700\n",
      "Gradient: [   4.8985    9.537    17.76   -101.279    18.6243]\n",
      "Weights: [-4.7688  0.7252 -1.2539  0.149   0.1358]\n",
      "MSE loss: 91.5039\n",
      "Iteration: 155800\n",
      "Gradient: [ -8.8503  -7.9144 -21.9994  87.4657  68.2182]\n",
      "Weights: [-4.7856  0.7269 -1.2575  0.1494  0.136 ]\n",
      "MSE loss: 91.125\n",
      "Iteration: 155900\n",
      "Gradient: [  0.8342 -14.9045  -7.6731 -48.1936 206.0219]\n",
      "Weights: [-4.7763  0.7199 -1.2566  0.1499  0.136 ]\n",
      "MSE loss: 91.0922\n",
      "Iteration: 156000\n",
      "Gradient: [ -0.716    2.6714  -7.1952  17.4766 485.1433]\n",
      "Weights: [-4.7943  0.7391 -1.2611  0.1496  0.1362]\n",
      "MSE loss: 91.1198\n",
      "Iteration: 156100\n",
      "Gradient: [  1.9328   5.1048 -12.2396  92.8685 487.3391]\n",
      "Weights: [-4.8004  0.7457 -1.2626  0.1505  0.1361]\n",
      "MSE loss: 91.3113\n",
      "Iteration: 156200\n",
      "Gradient: [   6.6822    8.7986   39.0897 -152.8475  -62.9356]\n",
      "Weights: [-4.7757  0.7393 -1.2638  0.1497  0.1361]\n",
      "MSE loss: 91.2233\n",
      "Iteration: 156300\n",
      "Gradient: [ -6.2195   6.6127 -20.973   51.8995  47.1158]\n",
      "Weights: [-4.7651  0.7317 -1.2664  0.1498  0.1364]\n",
      "MSE loss: 91.2603\n",
      "Iteration: 156400\n",
      "Gradient: [  -6.5608   -4.3565  -54.8856  -28.788  -274.26  ]\n",
      "Weights: [-4.7879  0.7335 -1.2676  0.1501  0.1365]\n",
      "MSE loss: 91.6535\n",
      "Iteration: 156500\n",
      "Gradient: [   0.7973   10.7857  -25.8767  -64.7184 -292.5433]\n",
      "Weights: [-4.7808  0.7318 -1.2636  0.1498  0.1363]\n",
      "MSE loss: 91.0833\n",
      "Iteration: 156600\n",
      "Gradient: [  -6.8196   11.0782    6.3847   20.1829 -257.3487]\n",
      "Weights: [-4.7873  0.737  -1.2686  0.1501  0.1366]\n",
      "MSE loss: 91.2979\n",
      "Iteration: 156700\n",
      "Gradient: [  8.5965  -7.847   -7.7803 -27.0947 -37.1711]\n",
      "Weights: [-4.7912  0.7551 -1.2733  0.1506  0.1366]\n",
      "MSE loss: 91.026\n",
      "Iteration: 156800\n",
      "Gradient: [ -8.355    0.4184  -5.3758  42.2075 -47.0696]\n",
      "Weights: [-4.8024  0.7546 -1.272   0.1498  0.1367]\n",
      "MSE loss: 91.2038\n",
      "Iteration: 156900\n",
      "Gradient: [  -2.0182    8.4207   12.3527  -52.2513 -128.1384]\n",
      "Weights: [-4.7983  0.7653 -1.2747  0.1507  0.1364]\n",
      "MSE loss: 91.1235\n",
      "Iteration: 157000\n",
      "Gradient: [ -11.5491  -15.0018  -48.4455 -231.4357 -210.5315]\n",
      "Weights: [-4.8014  0.7572 -1.2742  0.1499  0.1365]\n",
      "MSE loss: 91.47\n",
      "Iteration: 157100\n",
      "Gradient: [  -3.752    16.5227   16.7397  -69.0779 -104.8805]\n",
      "Weights: [-4.7998  0.75   -1.2732  0.1507  0.1367]\n",
      "MSE loss: 91.2535\n",
      "Iteration: 157200\n",
      "Gradient: [   3.3526   -5.1913  -14.949  -156.0579  215.2255]\n",
      "Weights: [-4.7815  0.7349 -1.2706  0.1506  0.1367]\n",
      "MSE loss: 91.2989\n",
      "Iteration: 157300\n",
      "Gradient: [  4.9882  -8.3202   5.2608  92.5458 134.2495]\n",
      "Weights: [-4.7744  0.7401 -1.2723  0.1514  0.1366]\n",
      "MSE loss: 91.0986\n",
      "Iteration: 157400\n",
      "Gradient: [-11.7078  12.5524 -55.3734 -43.8396  93.5961]\n",
      "Weights: [-4.7875  0.7393 -1.2734  0.151   0.1369]\n",
      "MSE loss: 91.3069\n",
      "Iteration: 157500\n",
      "Gradient: [  1.2763  -4.9695 -13.0984  58.1343 116.5549]\n",
      "Weights: [-4.7656  0.7368 -1.2757  0.1517  0.1367]\n",
      "MSE loss: 91.2174\n",
      "Iteration: 157600\n",
      "Gradient: [ 6.0544  2.3612  1.8602 41.123  61.3128]\n",
      "Weights: [-4.7905  0.75   -1.273   0.1517  0.1364]\n",
      "MSE loss: 91.0388\n",
      "Iteration: 157700\n",
      "Gradient: [ -7.3964   9.5497  -1.2392 -34.6763  59.0775]\n",
      "Weights: [-4.7797  0.7489 -1.273   0.1513  0.1363]\n",
      "MSE loss: 91.0797\n",
      "Iteration: 157800\n",
      "Gradient: [  0.291   -0.8674   0.3947  14.9562 102.7776]\n",
      "Weights: [-4.7962  0.757  -1.2736  0.1504  0.1364]\n",
      "MSE loss: 91.0681\n",
      "Iteration: 157900\n",
      "Gradient: [ 1.0978  5.6505 10.3528  5.198  -7.6296]\n",
      "Weights: [-4.7896  0.7597 -1.2734  0.1508  0.1363]\n",
      "MSE loss: 91.1142\n",
      "Iteration: 158000\n",
      "Gradient: [  2.8629  -3.6283 -13.7644 -67.4271 -84.167 ]\n",
      "Weights: [-4.7861  0.7613 -1.2742  0.1511  0.136 ]\n",
      "MSE loss: 91.1694\n",
      "Iteration: 158100\n",
      "Gradient: [  5.3104  -7.7733 -20.1963 -60.5712  71.4375]\n",
      "Weights: [-4.8023  0.7617 -1.2731  0.1507  0.136 ]\n",
      "MSE loss: 91.1744\n",
      "Iteration: 158200\n",
      "Gradient: [   2.9002  -35.1828  -10.5412   31.7519 -465.2754]\n",
      "Weights: [-4.7895  0.7632 -1.2759  0.1502  0.1361]\n",
      "MSE loss: 91.3898\n",
      "Iteration: 158300\n",
      "Gradient: [  0.3906  -7.153   19.0306 -22.6579 204.8646]\n",
      "Weights: [-4.8073  0.7857 -1.2778  0.1502  0.1362]\n",
      "MSE loss: 91.3697\n",
      "Iteration: 158400\n",
      "Gradient: [  -8.6895   10.2043   -6.7705   51.7728 -102.7686]\n",
      "Weights: [-4.8122  0.7893 -1.2822  0.1508  0.1363]\n",
      "MSE loss: 91.161\n",
      "Iteration: 158500\n",
      "Gradient: [  9.8058 -15.0408 -35.559  -83.082   37.4125]\n",
      "Weights: [-4.7969  0.7663 -1.2802  0.1509  0.1366]\n",
      "MSE loss: 91.1139\n",
      "Iteration: 158600\n",
      "Gradient: [ -4.9675 -18.0626   6.9019 -17.8582 -11.4307]\n",
      "Weights: [-4.7865  0.7686 -1.2814  0.1507  0.1366]\n",
      "MSE loss: 91.1381\n",
      "Iteration: 158700\n",
      "Gradient: [  -6.0276  -10.7336  -17.2636  -98.1921 -107.2432]\n",
      "Weights: [-4.8074  0.7759 -1.2818  0.1517  0.1362]\n",
      "MSE loss: 91.1634\n",
      "Iteration: 158800\n",
      "Gradient: [-4.4357 -0.3155 33.446  60.7593 74.191 ]\n",
      "Weights: [-4.8005  0.7765 -1.284   0.1521  0.1364]\n",
      "MSE loss: 91.0109\n",
      "Iteration: 158900\n",
      "Gradient: [   0.2044   19.7966   21.4611   27.2677 -156.8295]\n",
      "Weights: [-4.8005  0.775  -1.2847  0.1521  0.1367]\n",
      "MSE loss: 90.9822\n",
      "Iteration: 159000\n",
      "Gradient: [ 4.7565 14.7058  6.3828 20.7509 27.1609]\n",
      "Weights: [-4.8106  0.7771 -1.2844  0.1519  0.1369]\n",
      "MSE loss: 91.1054\n",
      "Iteration: 159100\n",
      "Gradient: [  -9.5071   -4.6367  -39.2819  -98.6112 -173.6611]\n",
      "Weights: [-4.7911  0.7669 -1.2869  0.1522  0.1369]\n",
      "MSE loss: 91.0739\n",
      "Iteration: 159200\n",
      "Gradient: [  4.632   16.6816  40.5372  84.756  259.8051]\n",
      "Weights: [-4.7964  0.7777 -1.2868  0.1525  0.1368]\n",
      "MSE loss: 91.1168\n",
      "Iteration: 159300\n",
      "Gradient: [ -0.9824  22.7409  36.9843  80.9741 318.7436]\n",
      "Weights: [-4.786   0.7724 -1.2853  0.153   0.1366]\n",
      "MSE loss: 91.3513\n",
      "Iteration: 159400\n",
      "Gradient: [ 8.804600e+00  1.585000e-01 -2.417380e+01  6.475590e+01 -3.303001e+02]\n",
      "Weights: [-4.8057  0.7748 -1.287   0.1532  0.1365]\n",
      "MSE loss: 91.0851\n",
      "Iteration: 159500\n",
      "Gradient: [  -4.5386   12.1505   42.8698   43.6351 -165.2221]\n",
      "Weights: [-4.7987  0.7819 -1.2898  0.1529  0.1366]\n",
      "MSE loss: 90.9796\n",
      "Iteration: 159600\n",
      "Gradient: [ -2.1595   1.5526  63.6489  20.8938 452.07  ]\n",
      "Weights: [-4.8048  0.7892 -1.2948  0.1534  0.1369]\n",
      "MSE loss: 90.9583\n",
      "Iteration: 159700\n",
      "Gradient: [  -5.6367  -11.728   -14.2075 -104.1982   39.9919]\n",
      "Weights: [-4.8072  0.7811 -1.2957  0.1536  0.1371]\n",
      "MSE loss: 91.3113\n",
      "Iteration: 159800\n",
      "Gradient: [   6.6232    1.3121   31.3993   54.3665 -225.2649]\n",
      "Weights: [-4.7777  0.7675 -1.2936  0.1541  0.1372]\n",
      "MSE loss: 91.1151\n",
      "Iteration: 159900\n",
      "Gradient: [ -1.7558  20.1624  31.6666 -23.7243 131.6008]\n",
      "Weights: [-4.7836  0.7659 -1.2939  0.1553  0.137 ]\n",
      "MSE loss: 91.0328\n",
      "Iteration: 160000\n",
      "Gradient: [   5.3808    7.3932    6.2191   73.2968 -161.1714]\n",
      "Weights: [-4.7749  0.768  -1.2955  0.1548  0.1369]\n",
      "MSE loss: 91.1318\n",
      "Iteration: 160100\n",
      "Gradient: [  8.3157   8.7317 -17.0304 -68.7888 137.4723]\n",
      "Weights: [-4.7777  0.7605 -1.2931  0.1545  0.1369]\n",
      "MSE loss: 91.2299\n",
      "Iteration: 160200\n",
      "Gradient: [  17.2636   18.0072  -28.1135   88.7499 -142.6553]\n",
      "Weights: [-4.7762  0.761  -1.2919  0.1552  0.1369]\n",
      "MSE loss: 91.1312\n",
      "Iteration: 160300\n",
      "Gradient: [  9.584    4.9388 -14.2202  34.3681 -24.8921]\n",
      "Weights: [-4.777   0.7533 -1.2895  0.1551  0.137 ]\n",
      "MSE loss: 91.0851\n",
      "Iteration: 160400\n",
      "Gradient: [   6.018    -9.9243    5.3261  -54.2307 -254.971 ]\n",
      "Weights: [-4.763   0.736  -1.285   0.1551  0.1368]\n",
      "MSE loss: 91.2703\n",
      "Iteration: 160500\n",
      "Gradient: [ -18.4091   -9.5155   13.6542 -166.1476  -98.0603]\n",
      "Weights: [-4.7836  0.7452 -1.2834  0.1544  0.1364]\n",
      "MSE loss: 91.4515\n",
      "Iteration: 160600\n",
      "Gradient: [  1.3856  -6.4645 -10.4333  -5.2552 133.352 ]\n",
      "Weights: [-4.778   0.7497 -1.2855  0.1548  0.1367]\n",
      "MSE loss: 91.0457\n",
      "Iteration: 160700\n",
      "Gradient: [   6.8342  -12.1436    6.1365 -113.0193  146.4704]\n",
      "Weights: [-4.7765  0.7353 -1.281   0.1557  0.1367]\n",
      "MSE loss: 91.3046\n",
      "Iteration: 160800\n",
      "Gradient: [-3.9732 -4.8367  1.6837  8.2032 74.2764]\n",
      "Weights: [-4.7658  0.7389 -1.2805  0.1551  0.1362]\n",
      "MSE loss: 91.1543\n",
      "Iteration: 160900\n",
      "Gradient: [ 8.11000e-02  1.41317e+01 -1.12276e+01 -4.65412e+01 -9.51360e+01]\n",
      "Weights: [-4.7832  0.7453 -1.2804  0.1554  0.1362]\n",
      "MSE loss: 91.0237\n",
      "Iteration: 161000\n",
      "Gradient: [ 11.727   15.526  -13.7798 140.8554  50.3364]\n",
      "Weights: [-4.7669  0.7408 -1.2811  0.1557  0.1361]\n",
      "MSE loss: 91.1942\n",
      "Iteration: 161100\n",
      "Gradient: [ -1.4212 -14.8162 -40.2425 113.9861 -25.4878]\n",
      "Weights: [-4.7706  0.7397 -1.28    0.1563  0.136 ]\n",
      "MSE loss: 91.1933\n",
      "Iteration: 161200\n",
      "Gradient: [   2.2874   18.1419   -4.1366   97.79   -140.3099]\n",
      "Weights: [-4.7752  0.7482 -1.2826  0.1564  0.136 ]\n",
      "MSE loss: 91.2135\n",
      "Iteration: 161300\n",
      "Gradient: [  -2.5111   -5.6219   47.1637  115.8542 -170.0958]\n",
      "Weights: [-4.7771  0.7497 -1.2825  0.1559  0.1358]\n",
      "MSE loss: 91.0075\n",
      "Iteration: 161400\n",
      "Gradient: [  -4.7053   -2.8836   51.7946 -127.1144  -54.7071]\n",
      "Weights: [-4.7976  0.7611 -1.2845  0.1564  0.1357]\n",
      "MSE loss: 90.974\n",
      "Iteration: 161500\n",
      "Gradient: [  6.4997   2.1822  16.8801  46.3352 -99.6227]\n",
      "Weights: [-4.7822  0.7635 -1.2911  0.1575  0.1358]\n",
      "MSE loss: 90.9579\n",
      "Iteration: 161600\n",
      "Gradient: [  -5.6939   11.8832   -9.5785   47.9299 -114.9247]\n",
      "Weights: [-4.7891  0.7704 -1.2914  0.1577  0.1356]\n",
      "MSE loss: 90.9422\n",
      "Iteration: 161700\n",
      "Gradient: [  -6.535   -20.83      0.2934 -137.6679 -155.0605]\n",
      "Weights: [-4.8137  0.7829 -1.2926  0.1565  0.1355]\n",
      "MSE loss: 91.47\n",
      "Iteration: 161800\n",
      "Gradient: [   1.5143  -11.4566  -24.4601 -186.0048  -41.6751]\n",
      "Weights: [-4.8067  0.7862 -1.2929  0.1577  0.1354]\n",
      "MSE loss: 90.9846\n",
      "Iteration: 161900\n",
      "Gradient: [-2.0106  7.4047  3.1561 50.7048 51.8553]\n",
      "Weights: [-4.801   0.7899 -1.2962  0.1579  0.1352]\n",
      "MSE loss: 90.9987\n",
      "Iteration: 162000\n",
      "Gradient: [ -3.9836 -10.0517  -1.214  -94.9481 -24.3134]\n",
      "Weights: [-4.7944  0.7787 -1.2962  0.1585  0.1353]\n",
      "MSE loss: 90.9873\n",
      "Iteration: 162100\n",
      "Gradient: [  -8.956    27.8557  -37.0069 -116.0638   52.9487]\n",
      "Weights: [-4.8112  0.7908 -1.297   0.1583  0.1353]\n",
      "MSE loss: 90.9866\n",
      "Iteration: 162200\n",
      "Gradient: [  1.5824  -8.3497  20.5126  33.9136 240.7028]\n",
      "Weights: [-4.79    0.7788 -1.2979  0.1587  0.1355]\n",
      "MSE loss: 90.943\n",
      "Iteration: 162300\n",
      "Gradient: [   4.4546   11.4101   13.4317   13.893  -150.0405]\n",
      "Weights: [-4.7898  0.7733 -1.3006  0.1599  0.1355]\n",
      "MSE loss: 91.1366\n",
      "Iteration: 162400\n",
      "Gradient: [ -11.456    16.4496  -11.0133   39.5446 -301.7804]\n",
      "Weights: [-4.8034  0.7765 -1.299   0.1597  0.1358]\n",
      "MSE loss: 90.9829\n",
      "Iteration: 162500\n",
      "Gradient: [  18.3645   -1.7316   17.5079   81.397  -171.2202]\n",
      "Weights: [-4.7765  0.7672 -1.2973  0.1597  0.1356]\n",
      "MSE loss: 91.0526\n",
      "Iteration: 162600\n",
      "Gradient: [  -4.7793  -23.3504   -3.9728 -181.3717 -273.6528]\n",
      "Weights: [-4.7995  0.7688 -1.2952  0.1598  0.1354]\n",
      "MSE loss: 91.016\n",
      "Iteration: 162700\n",
      "Gradient: [ -6.2026 -28.9706 -11.514   -4.0165  41.9388]\n",
      "Weights: [-4.7922  0.7676 -1.2947  0.1599  0.1354]\n",
      "MSE loss: 90.8839\n",
      "Iteration: 162800\n",
      "Gradient: [  7.4477  -8.3334 -23.5909 -33.631   38.1264]\n",
      "Weights: [-4.7955  0.7685 -1.2958  0.1602  0.1352]\n",
      "MSE loss: 90.9692\n",
      "Iteration: 162900\n",
      "Gradient: [ -0.2925 -17.6433  -3.1353 -22.967   24.6965]\n",
      "Weights: [-4.7943  0.7602 -1.292   0.1603  0.1352]\n",
      "MSE loss: 90.97\n",
      "Iteration: 163000\n",
      "Gradient: [ -0.1388 -20.1987  26.9292 119.4414 134.1096]\n",
      "Weights: [-4.7791  0.7635 -1.2958  0.16    0.1354]\n",
      "MSE loss: 90.9507\n",
      "Iteration: 163100\n",
      "Gradient: [  -4.1686  -11.5006   17.9012  158.727  -110.9596]\n",
      "Weights: [-4.8035  0.7803 -1.2963  0.1584  0.1355]\n",
      "MSE loss: 90.9928\n",
      "Iteration: 163200\n",
      "Gradient: [ -6.6177 -11.3543  20.1147 -68.629  120.7121]\n",
      "Weights: [-4.8031  0.7775 -1.2954  0.1575  0.1358]\n",
      "MSE loss: 91.0401\n",
      "Iteration: 163300\n",
      "Gradient: [  4.9231  -2.4743  30.9657 -29.1504 142.8116]\n",
      "Weights: [-4.7956  0.7848 -1.2963  0.1572  0.1359]\n",
      "MSE loss: 91.0142\n",
      "Iteration: 163400\n",
      "Gradient: [ -7.8781 -12.1262 -25.5072 -45.4855 136.3161]\n",
      "Weights: [-4.8053  0.7774 -1.2952  0.157   0.1362]\n",
      "MSE loss: 91.0151\n",
      "Iteration: 163500\n",
      "Gradient: [  0.3859 -11.7718  19.8908 -27.2743  94.2233]\n",
      "Weights: [-4.7977  0.7796 -1.296   0.1561  0.1363]\n",
      "MSE loss: 90.9541\n",
      "Iteration: 163600\n",
      "Gradient: [   5.055    -9.046    10.214    52.9241 -254.8974]\n",
      "Weights: [-4.8007  0.7886 -1.298   0.157   0.1362]\n",
      "MSE loss: 91.0016\n",
      "Iteration: 163700\n",
      "Gradient: [  -3.639     2.373     4.6724  -12.3516 -239.2026]\n",
      "Weights: [-4.7974  0.7882 -1.298   0.1566  0.1363]\n",
      "MSE loss: 91.0128\n",
      "Iteration: 163800\n",
      "Gradient: [ -8.4247 -16.6791 -23.2463   6.0079 -24.4778]\n",
      "Weights: [-4.8034  0.7768 -1.2966  0.157   0.1363]\n",
      "MSE loss: 91.0539\n",
      "Iteration: 163900\n",
      "Gradient: [ 1.9173  0.2967  5.9456 14.9953 83.0743]\n",
      "Weights: [-4.7888  0.7766 -1.2961  0.1569  0.1365]\n",
      "MSE loss: 91.1476\n",
      "Iteration: 164000\n",
      "Gradient: [ -0.8139  -9.3975  23.3138 126.3181  34.8537]\n",
      "Weights: [-4.8002  0.7827 -1.2964  0.1567  0.1363]\n",
      "MSE loss: 90.9365\n",
      "Iteration: 164100\n",
      "Gradient: [  1.4728 -15.0768  26.1625   9.131  265.5006]\n",
      "Weights: [-4.7899  0.7775 -1.2973  0.1564  0.1363]\n",
      "MSE loss: 90.9498\n",
      "Iteration: 164200\n",
      "Gradient: [  2.7076  -5.4598   4.6918  17.871  392.2143]\n",
      "Weights: [-4.8014  0.7823 -1.2957  0.1571  0.1362]\n",
      "MSE loss: 90.9669\n",
      "Iteration: 164300\n",
      "Gradient: [  -7.1565  -14.9486   10.8183   59.1958 -214.4175]\n",
      "Weights: [-4.7892  0.778  -1.2979  0.1572  0.1362]\n",
      "MSE loss: 90.9293\n",
      "Iteration: 164400\n",
      "Gradient: [   6.3646  -11.4848   16.4744   36.8916 -137.2505]\n",
      "Weights: [-4.7917  0.7786 -1.2997  0.1584  0.1362]\n",
      "MSE loss: 90.9292\n",
      "Iteration: 164500\n",
      "Gradient: [ 10.5159  27.1489  47.8395  44.4507 553.702 ]\n",
      "Weights: [-4.7939  0.787  -1.2984  0.1583  0.1359]\n",
      "MSE loss: 91.3669\n",
      "Iteration: 164600\n",
      "Gradient: [  5.4239  -3.0986  -0.7297 -29.4552  16.754 ]\n",
      "Weights: [-4.8003  0.7829 -1.2979  0.1578  0.136 ]\n",
      "MSE loss: 90.8927\n",
      "Iteration: 164700\n",
      "Gradient: [ -13.0689  -12.1235  -21.8387 -125.396  -396.5641]\n",
      "Weights: [-4.8052  0.7844 -1.2989  0.1578  0.1359]\n",
      "MSE loss: 91.0095\n",
      "Iteration: 164800\n",
      "Gradient: [ -3.053    5.7865   1.1463 177.6832 399.299 ]\n",
      "Weights: [-4.814   0.7919 -1.2972  0.1579  0.1359]\n",
      "MSE loss: 91.0876\n",
      "Iteration: 164900\n",
      "Gradient: [   1.2617  -16.6643   13.5163  114.4624 -250.2705]\n",
      "Weights: [-4.8034  0.7925 -1.2985  0.1575  0.1358]\n",
      "MSE loss: 90.9536\n",
      "Iteration: 165000\n",
      "Gradient: [  -3.9531   -6.3268   25.9914  102.3997 -172.2501]\n",
      "Weights: [-4.8005  0.7751 -1.2948  0.158   0.1357]\n",
      "MSE loss: 90.9629\n",
      "Iteration: 165100\n",
      "Gradient: [ -4.5518  -5.617   34.2438 169.9652 220.9437]\n",
      "Weights: [-4.7817  0.764  -1.292   0.158   0.1357]\n",
      "MSE loss: 90.9575\n",
      "Iteration: 165200\n",
      "Gradient: [ -12.2533   -0.2027  -22.2872 -150.4005  -18.5016]\n",
      "Weights: [-4.7909  0.7599 -1.293   0.1586  0.1357]\n",
      "MSE loss: 91.1266\n",
      "Iteration: 165300\n",
      "Gradient: [-16.4683   9.08     5.1875 -90.6462  52.2938]\n",
      "Weights: [-4.784   0.757  -1.2923  0.1591  0.1357]\n",
      "MSE loss: 90.9431\n",
      "Iteration: 165400\n",
      "Gradient: [ -1.0523   7.3859 -13.3828  46.8642 124.4935]\n",
      "Weights: [-4.7935  0.7645 -1.2943  0.1593  0.1359]\n",
      "MSE loss: 90.9829\n",
      "Iteration: 165500\n",
      "Gradient: [  -5.4096   12.4952  -12.8759  -55.1124 -435.3366]\n",
      "Weights: [-4.7917  0.7679 -1.2951  0.1587  0.1356]\n",
      "MSE loss: 90.9674\n",
      "Iteration: 165600\n",
      "Gradient: [  2.094   14.869   17.6765  22.2784 113.1443]\n",
      "Weights: [-4.7986  0.776  -1.2936  0.1596  0.1355]\n",
      "MSE loss: 91.2543\n",
      "Iteration: 165700\n",
      "Gradient: [  5.5042  -9.8919 -23.7323 -68.9709 -27.8947]\n",
      "Weights: [-4.7874  0.7683 -1.2907  0.1586  0.1353]\n",
      "MSE loss: 90.9722\n",
      "Iteration: 165800\n",
      "Gradient: [14.6187 -8.745  -6.9033 58.4702 37.4655]\n",
      "Weights: [-4.7825  0.7596 -1.2898  0.1595  0.1353]\n",
      "MSE loss: 91.0172\n",
      "Iteration: 165900\n",
      "Gradient: [  4.9714 -12.6828 -16.6309  19.7178  19.2329]\n",
      "Weights: [-4.791   0.7646 -1.2902  0.1589  0.1352]\n",
      "MSE loss: 90.8929\n",
      "Iteration: 166000\n",
      "Gradient: [  -5.8901   14.753    39.3162  -15.0209 -126.4912]\n",
      "Weights: [-4.7741  0.7603 -1.292   0.1592  0.1353]\n",
      "MSE loss: 91.0704\n",
      "Iteration: 166100\n",
      "Gradient: [ -3.891  -12.4451  22.3334 -17.3319 361.727 ]\n",
      "Weights: [-4.8121  0.7803 -1.2929  0.1595  0.1352]\n",
      "MSE loss: 91.0381\n",
      "Iteration: 166200\n",
      "Gradient: [   1.2596   -9.1273  -44.3469  -46.064  -241.4303]\n",
      "Weights: [-4.8118  0.7869 -1.2972  0.16    0.1349]\n",
      "MSE loss: 90.9739\n",
      "Iteration: 166300\n",
      "Gradient: [  8.8938  -4.0269  13.9224 -88.2843 -85.8294]\n",
      "Weights: [-4.7862  0.7791 -1.3005  0.1607  0.135 ]\n",
      "MSE loss: 90.9811\n",
      "Iteration: 166400\n",
      "Gradient: [ -9.4288  -9.3513  32.1398  45.0112 160.3711]\n",
      "Weights: [-4.7935  0.779  -1.2997  0.1615  0.135 ]\n",
      "MSE loss: 90.8721\n",
      "Iteration: 166500\n",
      "Gradient: [  1.3712   9.1693  19.3036 -42.4369 124.6444]\n",
      "Weights: [-4.7996  0.7863 -1.301   0.1613  0.1349]\n",
      "MSE loss: 90.8705\n",
      "Iteration: 166600\n",
      "Gradient: [-11.7481 -12.8812  -3.4037 -73.316  -50.2244]\n",
      "Weights: [-4.8057  0.7832 -1.301   0.1615  0.1349]\n",
      "MSE loss: 90.9577\n",
      "Iteration: 166700\n",
      "Gradient: [ -3.5575  13.2797  -7.2147 -99.5399 258.9632]\n",
      "Weights: [-4.8116  0.7958 -1.3025  0.1608  0.135 ]\n",
      "MSE loss: 90.9108\n",
      "Iteration: 166800\n",
      "Gradient: [  3.5633  18.8883  36.0374 -68.5745  62.9703]\n",
      "Weights: [-4.8047  0.7943 -1.3064  0.1612  0.1351]\n",
      "MSE loss: 90.9082\n",
      "Iteration: 166900\n",
      "Gradient: [  2.5831  23.0736 -56.8856  84.3249  55.8922]\n",
      "Weights: [-4.8118  0.7991 -1.3065  0.1616  0.1353]\n",
      "MSE loss: 90.9213\n",
      "Iteration: 167000\n",
      "Gradient: [   5.0709  -13.3969   20.4176  -21.3961 -181.6824]\n",
      "Weights: [-4.8032  0.7924 -1.3066  0.1614  0.1352]\n",
      "MSE loss: 90.9123\n",
      "Iteration: 167100\n",
      "Gradient: [   4.757     4.206    43.0147   -2.577  -128.8703]\n",
      "Weights: [-4.8019  0.7867 -1.3045  0.1614  0.1353]\n",
      "MSE loss: 90.8636\n",
      "Iteration: 167200\n",
      "Gradient: [  7.0543  10.7564  57.2211 -26.162   88.8486]\n",
      "Weights: [-4.7947  0.7831 -1.3049  0.162   0.1355]\n",
      "MSE loss: 90.9349\n",
      "Iteration: 167300\n",
      "Gradient: [ -17.3039  -29.6671  -52.9623  -63.6219 -176.9328]\n",
      "Weights: [-4.8034  0.7802 -1.3055  0.1619  0.1353]\n",
      "MSE loss: 91.2224\n",
      "Iteration: 167400\n",
      "Gradient: [-3.9291 12.9919 21.8509 60.3468 52.0581]\n",
      "Weights: [-4.8042  0.7839 -1.305   0.1624  0.1351]\n",
      "MSE loss: 90.925\n",
      "Iteration: 167500\n",
      "Gradient: [ 12.7521 -11.2888  17.0878 -23.0956 219.6838]\n",
      "Weights: [-4.7928  0.7842 -1.3046  0.1622  0.1351]\n",
      "MSE loss: 90.8748\n",
      "Iteration: 167600\n",
      "Gradient: [  4.6251   2.6755  44.6317 -48.6932 233.7939]\n",
      "Weights: [-4.7938  0.7835 -1.3034  0.1615  0.1352]\n",
      "MSE loss: 90.8542\n",
      "Iteration: 167700\n",
      "Gradient: [  1.6904   4.8045 -10.8735 141.691  169.1515]\n",
      "Weights: [-4.7828  0.7863 -1.3055  0.1618  0.1354]\n",
      "MSE loss: 91.3783\n",
      "Iteration: 167800\n",
      "Gradient: [  8.9473  -8.895   20.1914 -28.1252 -84.0664]\n",
      "Weights: [-4.7948  0.7895 -1.3052  0.1611  0.1353]\n",
      "MSE loss: 90.8878\n",
      "Iteration: 167900\n",
      "Gradient: [  0.3975 -30.0853 -16.5998  12.5872  67.8834]\n",
      "Weights: [-4.7925  0.7853 -1.307   0.1618  0.1353]\n",
      "MSE loss: 90.8757\n",
      "Iteration: 168000\n",
      "Gradient: [-13.1798   8.0776 -12.1322  -7.358  461.9419]\n",
      "Weights: [-4.7975  0.7858 -1.306   0.1625  0.1351]\n",
      "MSE loss: 90.8224\n",
      "Iteration: 168100\n",
      "Gradient: [ -7.4919 -13.2616 -14.5486 -50.2285 -43.3163]\n",
      "Weights: [-4.8052  0.7881 -1.3083  0.163   0.1353]\n",
      "MSE loss: 90.8808\n",
      "Iteration: 168200\n",
      "Gradient: [  -2.6231  -27.8308   28.6666   48.714  -158.5024]\n",
      "Weights: [-4.8016  0.7915 -1.3087  0.1628  0.1352]\n",
      "MSE loss: 90.8265\n",
      "Iteration: 168300\n",
      "Gradient: [  9.6938  31.0991   7.2106 -26.8464 152.2865]\n",
      "Weights: [-4.7949  0.7916 -1.3094  0.163   0.1355]\n",
      "MSE loss: 91.1851\n",
      "Iteration: 168400\n",
      "Gradient: [  7.2133  -0.8097  30.9752   3.3855 162.791 ]\n",
      "Weights: [-4.7887  0.78   -1.3098  0.1631  0.1358]\n",
      "MSE loss: 90.9477\n",
      "Iteration: 168500\n",
      "Gradient: [ -3.964  -10.4408 -56.0799 -47.3376 215.0862]\n",
      "Weights: [-4.8084  0.7924 -1.3113  0.1625  0.1354]\n",
      "MSE loss: 91.1076\n",
      "Iteration: 168600\n",
      "Gradient: [  -2.8654    0.39    -18.3424    4.8491 -151.7006]\n",
      "Weights: [-4.7935  0.799  -1.3133  0.1618  0.1355]\n",
      "MSE loss: 90.9261\n",
      "Iteration: 168700\n",
      "Gradient: [-11.0217  -4.4393 -25.9646  34.9159 -43.301 ]\n",
      "Weights: [-4.7982  0.7928 -1.3136  0.1628  0.1357]\n",
      "MSE loss: 90.8281\n",
      "Iteration: 168800\n",
      "Gradient: [-10.6938  -3.8483 -25.5447  68.1699 -13.7052]\n",
      "Weights: [-4.8004  0.8016 -1.3148  0.1624  0.1356]\n",
      "MSE loss: 90.8315\n",
      "Iteration: 168900\n",
      "Gradient: [   1.9299   -1.2278   -9.9066    5.418  -149.5365]\n",
      "Weights: [-4.7924  0.791  -1.3141  0.1636  0.1356]\n",
      "MSE loss: 90.847\n",
      "Iteration: 169000\n",
      "Gradient: [ 1.067540e+01  4.192000e-01 -3.568000e-01  2.345450e+01  4.209131e+02]\n",
      "Weights: [-4.7995  0.7929 -1.3128  0.1635  0.1353]\n",
      "MSE loss: 90.8105\n",
      "Iteration: 169100\n",
      "Gradient: [  1.8314   0.3616   9.4756 116.7653 -11.4619]\n",
      "Weights: [-4.7965  0.7933 -1.3125  0.1635  0.1353]\n",
      "MSE loss: 90.8198\n",
      "Iteration: 169200\n",
      "Gradient: [  14.0679   12.3839   54.3133   49.2548 -138.8937]\n",
      "Weights: [-4.7825  0.7874 -1.3128  0.1639  0.1355]\n",
      "MSE loss: 91.1285\n",
      "Iteration: 169300\n",
      "Gradient: [  -0.4765    7.6556  -35.4713  -85.8538 -152.9838]\n",
      "Weights: [-4.8079  0.7933 -1.3123  0.1637  0.1354]\n",
      "MSE loss: 90.8938\n",
      "Iteration: 169400\n",
      "Gradient: [-1.043000e-01  8.147700e+00 -2.362700e+01  3.255330e+01  1.444256e+02]\n",
      "Weights: [-4.7974  0.7918 -1.313   0.1641  0.1353]\n",
      "MSE loss: 90.8327\n",
      "Iteration: 169500\n",
      "Gradient: [  7.492  -13.5649 -25.7457 -73.5661 283.4029]\n",
      "Weights: [-4.7964  0.791  -1.3111  0.1638  0.1349]\n",
      "MSE loss: 90.8423\n",
      "Iteration: 169600\n",
      "Gradient: [   2.8718   33.3323    8.7205  -65.618  -102.4064]\n",
      "Weights: [-4.7924  0.7861 -1.3077  0.164   0.135 ]\n",
      "MSE loss: 91.0064\n",
      "Iteration: 169700\n",
      "Gradient: [  1.304    5.9332  30.3424 -23.2001 218.3728]\n",
      "Weights: [-4.7993  0.7803 -1.3086  0.1648  0.1349]\n",
      "MSE loss: 90.8696\n",
      "Iteration: 169800\n",
      "Gradient: [ 4.410700e+00 -1.263000e-01 -1.501650e+01 -6.777600e+00 -3.044807e+02]\n",
      "Weights: [-4.7865  0.7739 -1.3088  0.1648  0.1348]\n",
      "MSE loss: 90.9605\n",
      "Iteration: 169900\n",
      "Gradient: [-15.1792  26.477    6.6039 -57.1365 158.8266]\n",
      "Weights: [-4.782   0.7729 -1.3067  0.1653  0.135 ]\n",
      "MSE loss: 91.1706\n",
      "Iteration: 170000\n",
      "Gradient: [  -2.3091  -11.5302  -13.9481  -27.9878 -139.9169]\n",
      "Weights: [-4.7786  0.7638 -1.3103  0.1662  0.1348]\n",
      "MSE loss: 91.185\n",
      "Iteration: 170100\n",
      "Gradient: [10.0113  3.8178 41.2352 77.5282 12.321 ]\n",
      "Weights: [-4.7844  0.7741 -1.3136  0.1664  0.1352]\n",
      "MSE loss: 90.8846\n",
      "Iteration: 170200\n",
      "Gradient: [ -3.8714 -18.9374  19.2805  18.1421 -48.3774]\n",
      "Weights: [-4.8039  0.7895 -1.3119  0.1654  0.1349]\n",
      "MSE loss: 90.8578\n",
      "Iteration: 170300\n",
      "Gradient: [  -9.1137   21.4491  -34.0311 -105.1384   77.272 ]\n",
      "Weights: [-4.8038  0.7861 -1.3113  0.1648  0.1349]\n",
      "MSE loss: 90.9658\n",
      "Iteration: 170400\n",
      "Gradient: [  6.8495 -12.1329  25.8846 -14.3588  49.1056]\n",
      "Weights: [-4.7864  0.788  -1.3104  0.1642  0.135 ]\n",
      "MSE loss: 91.0596\n",
      "Iteration: 170500\n",
      "Gradient: [  -8.7411    0.2796   10.1313   53.4467 -247.9952]\n",
      "Weights: [-4.786   0.7708 -1.3082  0.1654  0.135 ]\n",
      "MSE loss: 90.8689\n",
      "Iteration: 170600\n",
      "Gradient: [  -3.2872   -4.4593    0.9846  -60.324  -265.9663]\n",
      "Weights: [-4.8113  0.7889 -1.3065  0.1637  0.1348]\n",
      "MSE loss: 90.9318\n",
      "Iteration: 170700\n",
      "Gradient: [  6.9037  -5.3219  39.0013 161.3771 170.9215]\n",
      "Weights: [-4.7858  0.7799 -1.3069  0.1644  0.135 ]\n",
      "MSE loss: 91.0817\n",
      "Iteration: 170800\n",
      "Gradient: [ -6.1118  -5.7535 -29.8847 -64.9692  92.7878]\n",
      "Weights: [-4.7926  0.782  -1.3097  0.1641  0.135 ]\n",
      "MSE loss: 90.8357\n",
      "Iteration: 170900\n",
      "Gradient: [   9.8754    4.0715   15.0266  -43.8803 -123.8814]\n",
      "Weights: [-4.7992  0.7879 -1.3104  0.1644  0.135 ]\n",
      "MSE loss: 90.8158\n",
      "Iteration: 171000\n",
      "Gradient: [ -0.149   -2.6793 -36.3945 -66.8081 144.8482]\n",
      "Weights: [-4.8034  0.7883 -1.3118  0.165   0.1349]\n",
      "MSE loss: 90.8633\n",
      "Iteration: 171100\n",
      "Gradient: [ -3.603    0.1587 -34.6832 -21.4185 -93.7322]\n",
      "Weights: [-4.8049  0.7932 -1.3135  0.1659  0.1348]\n",
      "MSE loss: 90.8388\n",
      "Iteration: 171200\n",
      "Gradient: [-12.5576 -18.871  -84.9732 -98.7389 -87.7145]\n",
      "Weights: [-4.8063  0.7854 -1.3138  0.1654  0.1349]\n",
      "MSE loss: 91.3306\n",
      "Iteration: 171300\n",
      "Gradient: [-3.6213 -4.3222 13.4504 43.5835 53.8091]\n",
      "Weights: [-4.8021  0.7879 -1.313   0.166   0.1349]\n",
      "MSE loss: 90.8259\n",
      "Iteration: 171400\n",
      "Gradient: [  6.5576   7.8205  10.167   10.5427 -28.9345]\n",
      "Weights: [-4.7862  0.7872 -1.3148  0.1658  0.1348]\n",
      "MSE loss: 90.8534\n",
      "Iteration: 171500\n",
      "Gradient: [  1.5971  15.0585  -7.53   -35.4895 632.4262]\n",
      "Weights: [-4.8029  0.7973 -1.3162  0.1667  0.1349]\n",
      "MSE loss: 91.0139\n",
      "Iteration: 171600\n",
      "Gradient: [  -3.3221   13.6041   18.0476   23.4379 -150.196 ]\n",
      "Weights: [-4.7917  0.7876 -1.3139  0.1656  0.1345]\n",
      "MSE loss: 90.9523\n",
      "Iteration: 171700\n",
      "Gradient: [  0.3263 -14.4154 -49.4416  69.2362   2.3781]\n",
      "Weights: [-4.7965  0.7968 -1.3159  0.1664  0.1345]\n",
      "MSE loss: 90.8163\n",
      "Iteration: 171800\n",
      "Gradient: [ -4.9787  -8.9904  37.0443  62.6868 170.328 ]\n",
      "Weights: [-4.8012  0.7941 -1.3137  0.1665  0.1342]\n",
      "MSE loss: 90.7998\n",
      "Iteration: 171900\n",
      "Gradient: [  3.9531  13.3316  12.3775 -36.8935 -15.4721]\n",
      "Weights: [-4.7919  0.7782 -1.3096  0.1676  0.1342]\n",
      "MSE loss: 90.8228\n",
      "Iteration: 172000\n",
      "Gradient: [ -7.9265 -10.7713 -28.1527 113.7596  27.8479]\n",
      "Weights: [-4.8143  0.7835 -1.3093  0.168   0.1341]\n",
      "MSE loss: 91.0859\n",
      "Iteration: 172100\n",
      "Gradient: [  5.0305 -11.3496  40.6706  28.7999 401.6465]\n",
      "Weights: [-4.7922  0.7837 -1.3106  0.1679  0.134 ]\n",
      "MSE loss: 90.8533\n",
      "Iteration: 172200\n",
      "Gradient: [  -8.2416  -16.9653  -45.2837   -5.9753 -275.2755]\n",
      "Weights: [-4.7893  0.7726 -1.3097  0.1685  0.1337]\n",
      "MSE loss: 90.8536\n",
      "Iteration: 172300\n",
      "Gradient: [  5.5097   1.9573   0.5359 -26.19   251.9558]\n",
      "Weights: [-4.7803  0.7755 -1.3107  0.1693  0.1335]\n",
      "MSE loss: 90.893\n",
      "Iteration: 172400\n",
      "Gradient: [  2.8137  -7.7053  16.1469   4.8668 429.6435]\n",
      "Weights: [-4.7976  0.779  -1.3128  0.1711  0.1335]\n",
      "MSE loss: 90.8272\n",
      "Iteration: 172500\n",
      "Gradient: [ -7.127   -2.2396 -32.1836  50.4249 -19.8583]\n",
      "Weights: [-4.7923  0.7797 -1.3149  0.1706  0.1333]\n",
      "MSE loss: 90.8678\n",
      "Iteration: 172600\n",
      "Gradient: [   7.6096   -2.8075   13.0808  -10.9782 -129.6279]\n",
      "Weights: [-4.7864  0.7677 -1.3128  0.1721  0.1331]\n",
      "MSE loss: 90.801\n",
      "Iteration: 172700\n",
      "Gradient: [ -7.3273  -7.3297  30.942   55.4997 245.8562]\n",
      "Weights: [-4.7838  0.7764 -1.3152  0.1722  0.1331]\n",
      "MSE loss: 90.7698\n",
      "Iteration: 172800\n",
      "Gradient: [ -1.9861  12.8359  26.3854 -46.2591 -54.6817]\n",
      "Weights: [-4.801   0.772  -1.3125  0.1726  0.133 ]\n",
      "MSE loss: 90.9106\n",
      "Iteration: 172900\n",
      "Gradient: [  9.4513  15.5036  43.7918 -37.5644  90.3211]\n",
      "Weights: [-4.7931  0.7828 -1.3122  0.1718  0.1328]\n",
      "MSE loss: 90.8055\n",
      "Iteration: 173000\n",
      "Gradient: [  2.0724  10.3985  12.9115 -10.4617 586.1925]\n",
      "Weights: [-4.7851  0.7678 -1.3081  0.1719  0.1327]\n",
      "MSE loss: 90.7574\n",
      "Iteration: 173100\n",
      "Gradient: [  -1.9063   11.5776  -44.8216  -49.6958 -142.6914]\n",
      "Weights: [-4.7834  0.762  -1.3082  0.1718  0.1329]\n",
      "MSE loss: 90.7725\n",
      "Iteration: 173200\n",
      "Gradient: [-12.6224  -8.706   38.0289  85.1101  38.2966]\n",
      "Weights: [-4.8007  0.7653 -1.3062  0.1717  0.1331]\n",
      "MSE loss: 90.9652\n",
      "Iteration: 173300\n",
      "Gradient: [  3.6684   8.1277  15.462   76.7895 218.9815]\n",
      "Weights: [-4.7824  0.7666 -1.3096  0.171   0.1333]\n",
      "MSE loss: 90.7652\n",
      "Iteration: 173400\n",
      "Gradient: [-10.8817  -8.9865 -13.2141 -61.127   76.8852]\n",
      "Weights: [-4.7917  0.7651 -1.3094  0.1708  0.1332]\n",
      "MSE loss: 90.9525\n",
      "Iteration: 173500\n",
      "Gradient: [ -7.5755  10.8421  -1.6961  53.4574 116.0279]\n",
      "Weights: [-4.7905  0.7676 -1.3096  0.1716  0.1331]\n",
      "MSE loss: 90.7551\n",
      "Iteration: 173600\n",
      "Gradient: [  -8.6966   -9.5835  -19.9728   20.8726 -397.9927]\n",
      "Weights: [-4.7936  0.7635 -1.3104  0.1713  0.1331]\n",
      "MSE loss: 91.3008\n",
      "Iteration: 173700\n",
      "Gradient: [  0.4659  15.659  -10.71    49.4776   8.0622]\n",
      "Weights: [-4.7768  0.7665 -1.3109  0.1713  0.1332]\n",
      "MSE loss: 90.8177\n",
      "Iteration: 173800\n",
      "Gradient: [  -2.5066  -27.9371  -49.9256  -49.0918 -279.3027]\n",
      "Weights: [-4.7922  0.7757 -1.3159  0.1715  0.1333]\n",
      "MSE loss: 90.9363\n",
      "Iteration: 173900\n",
      "Gradient: [ -3.9286   6.3104 -11.5171   1.5706 348.6382]\n",
      "Weights: [-4.7788  0.7747 -1.3163  0.1713  0.1335]\n",
      "MSE loss: 90.8107\n",
      "Iteration: 174000\n",
      "Gradient: [  -9.4717   13.6192  -18.4456   80.2858 -207.9167]\n",
      "Weights: [-4.8019  0.7813 -1.3151  0.1709  0.1336]\n",
      "MSE loss: 90.8172\n",
      "Iteration: 174100\n",
      "Gradient: [ -13.9829    3.5472   13.2626  -19.4282 -318.8706]\n",
      "Weights: [-4.7869  0.7743 -1.3149  0.1705  0.1334]\n",
      "MSE loss: 91.087\n",
      "Iteration: 174200\n",
      "Gradient: [ -3.8483  20.2927   1.2491 104.8947 128.1499]\n",
      "Weights: [-4.7905  0.7764 -1.3138  0.1709  0.1335]\n",
      "MSE loss: 90.7428\n",
      "Iteration: 174300\n",
      "Gradient: [  8.1884   2.7583  31.6872  22.7489 101.9722]\n",
      "Weights: [-4.8009  0.793  -1.3152  0.1703  0.1334]\n",
      "MSE loss: 90.8623\n",
      "Iteration: 174400\n",
      "Gradient: [  2.8503  -4.8803 -19.7429  72.7931 141.9434]\n",
      "Weights: [-4.8033  0.7904 -1.3153  0.17    0.1334]\n",
      "MSE loss: 90.7656\n",
      "Iteration: 174500\n",
      "Gradient: [-3.5608 -6.1014 17.5514 40.9816 54.4509]\n",
      "Weights: [-4.8062  0.7891 -1.3148  0.1709  0.1333]\n",
      "MSE loss: 90.7787\n",
      "Iteration: 174600\n",
      "Gradient: [ 10.2887  22.4138 -47.9465  -1.6646 395.8419]\n",
      "Weights: [-4.7947  0.7842 -1.3136  0.1708  0.1333]\n",
      "MSE loss: 90.7578\n",
      "Iteration: 174700\n",
      "Gradient: [  -5.5055  -13.7591  -16.7598 -112.3611  -40.8229]\n",
      "Weights: [-4.8087  0.7874 -1.3103  0.1697  0.1329]\n",
      "MSE loss: 90.943\n",
      "Iteration: 174800\n",
      "Gradient: [  8.4924   7.1156 -12.7148  51.0519  75.0752]\n",
      "Weights: [-4.8115  0.8053 -1.3146  0.1698  0.1329]\n",
      "MSE loss: 90.8788\n",
      "Iteration: 174900\n",
      "Gradient: [  -0.3409    0.4778  -27.3675  -64.9482 -135.2538]\n",
      "Weights: [-4.8079  0.7974 -1.3143  0.1691  0.133 ]\n",
      "MSE loss: 91.1597\n",
      "Iteration: 175000\n",
      "Gradient: [  6.7084  14.6558  64.7831  13.6648 220.6253]\n",
      "Weights: [-4.8056  0.7986 -1.3124  0.1692  0.1333]\n",
      "MSE loss: 91.0103\n",
      "Iteration: 175100\n",
      "Gradient: [-10.4167  -1.9323 -44.685   38.3281  27.4343]\n",
      "Weights: [-4.8204  0.8029 -1.3157  0.1692  0.1334]\n",
      "MSE loss: 90.949\n",
      "Iteration: 175200\n",
      "Gradient: [ -5.1151   8.6575 -27.3035 -12.5456  79.7502]\n",
      "Weights: [-4.8079  0.8044 -1.3183  0.1688  0.1336]\n",
      "MSE loss: 90.793\n",
      "Iteration: 175300\n",
      "Gradient: [  -5.7472   -2.8418  -13.908   -82.4575 -105.2815]\n",
      "Weights: [-4.8045  0.7946 -1.3178  0.1693  0.1337]\n",
      "MSE loss: 90.8076\n",
      "Iteration: 175400\n",
      "Gradient: [ 6.4205  2.9427 27.9667 19.2504 67.4788]\n",
      "Weights: [-4.7812  0.7823 -1.3172  0.1694  0.134 ]\n",
      "MSE loss: 90.8613\n",
      "Iteration: 175500\n",
      "Gradient: [  2.2037  16.6957  12.354   73.2755 -25.8943]\n",
      "Weights: [-4.7881  0.786  -1.3177  0.1697  0.1338]\n",
      "MSE loss: 90.7874\n",
      "Iteration: 175600\n",
      "Gradient: [  -0.7674   -7.2642  -30.1682  -87.0582 -429.472 ]\n",
      "Weights: [-4.8059  0.7859 -1.316   0.1697  0.1338]\n",
      "MSE loss: 90.9606\n",
      "Iteration: 175700\n",
      "Gradient: [  -3.9652   -7.9851   14.7265  -50.5903 -293.0638]\n",
      "Weights: [-4.8068  0.7873 -1.3155  0.1702  0.1338]\n",
      "MSE loss: 90.8361\n",
      "Iteration: 175800\n",
      "Gradient: [  14.0395    0.9328   11.2359 -153.3079  304.5019]\n",
      "Weights: [-4.7947  0.7911 -1.3189  0.1705  0.1334]\n",
      "MSE loss: 90.7826\n",
      "Iteration: 175900\n",
      "Gradient: [ -3.112   12.0764 -13.4172 143.9872 -57.9653]\n",
      "Weights: [-4.8163  0.8122 -1.3231  0.1709  0.1334]\n",
      "MSE loss: 90.7848\n",
      "Iteration: 176000\n",
      "Gradient: [  -8.8152   -8.9626   13.9919 -120.4969  -74.4039]\n",
      "Weights: [-4.8173  0.8048 -1.3222  0.1724  0.1332]\n",
      "MSE loss: 90.8389\n",
      "Iteration: 176100\n",
      "Gradient: [ -3.8778  -3.1543 -27.9786  66.7406 128.956 ]\n",
      "Weights: [-4.8209  0.8116 -1.3233  0.1724  0.1329]\n",
      "MSE loss: 90.8509\n",
      "Iteration: 176200\n",
      "Gradient: [ 11.0303   9.5005  44.6727 -85.1926 -14.7579]\n",
      "Weights: [-4.8052  0.8121 -1.3212  0.171   0.1328]\n",
      "MSE loss: 90.9413\n",
      "Iteration: 176300\n",
      "Gradient: [ 12.0859  -9.9315 -36.118   84.5355 176.9925]\n",
      "Weights: [-4.7918  0.7912 -1.3181  0.1718  0.1329]\n",
      "MSE loss: 90.7936\n",
      "Iteration: 176400\n",
      "Gradient: [-1.528000e-01 -2.666560e+01  3.462850e+01  4.784130e+01 -2.284208e+02]\n",
      "Weights: [-4.7963  0.7877 -1.3186  0.1724  0.1328]\n",
      "MSE loss: 90.7709\n",
      "Iteration: 176500\n",
      "Gradient: [   0.7838   -6.0018  -15.4692  -45.6304 -427.5169]\n",
      "Weights: [-4.8098  0.7941 -1.3193  0.1723  0.133 ]\n",
      "MSE loss: 90.8351\n",
      "Iteration: 176600\n",
      "Gradient: [ -5.9299  15.338   25.7548 137.1517  16.6932]\n",
      "Weights: [-4.8056  0.7962 -1.3188  0.1731  0.1329]\n",
      "MSE loss: 90.904\n",
      "Iteration: 176700\n",
      "Gradient: [  1.1492  -5.275   -6.4412 -20.8786 198.8209]\n",
      "Weights: [-4.7936  0.8029 -1.3245  0.1733  0.133 ]\n",
      "MSE loss: 91.0013\n",
      "Iteration: 176800\n",
      "Gradient: [ 1.9244  1.7311 18.5557 54.1882 73.0693]\n",
      "Weights: [-4.8083  0.8134 -1.328   0.1732  0.1331]\n",
      "MSE loss: 90.7553\n",
      "Iteration: 176900\n",
      "Gradient: [   5.8487    9.1837   11.7484 -133.887  -457.5659]\n",
      "Weights: [-4.8124  0.8079 -1.3307  0.1753  0.1328]\n",
      "MSE loss: 90.7494\n",
      "Iteration: 177000\n",
      "Gradient: [   5.0525    3.7739  -52.2631  -86.3516 -340.6283]\n",
      "Weights: [-4.797   0.8004 -1.334   0.1762  0.1329]\n",
      "MSE loss: 90.7365\n",
      "Iteration: 177100\n",
      "Gradient: [  4.1691 -17.3449   1.8751  14.1023 280.8145]\n",
      "Weights: [-4.7996  0.7967 -1.3296  0.1767  0.1326]\n",
      "MSE loss: 90.6296\n",
      "Iteration: 177200\n",
      "Gradient: [ -0.0813  -4.0751  25.2591  58.2305 -19.3745]\n",
      "Weights: [-4.7858  0.7973 -1.3305  0.1778  0.1324]\n",
      "MSE loss: 91.0663\n",
      "Iteration: 177300\n",
      "Gradient: [ 2.043000e-01 -4.525000e+00  4.544020e+01  1.718603e+02  3.483651e+02]\n",
      "Weights: [-4.7848  0.785  -1.3294  0.1776  0.1327]\n",
      "MSE loss: 90.6925\n",
      "Iteration: 177400\n",
      "Gradient: [   3.3785   -3.9591  -11.736    33.1819 -210.8372]\n",
      "Weights: [-4.7937  0.7831 -1.3264  0.1769  0.1327]\n",
      "MSE loss: 90.6874\n",
      "Iteration: 177500\n",
      "Gradient: [   3.2501  -11.8971  -27.542   -52.8021 -128.1229]\n",
      "Weights: [-4.7726  0.7729 -1.3245  0.1763  0.1327]\n",
      "MSE loss: 90.8203\n",
      "Iteration: 177600\n",
      "Gradient: [-10.1021 -26.6587  10.3425   4.7466  22.6682]\n",
      "Weights: [-4.7999  0.7687 -1.3197  0.1769  0.1327]\n",
      "MSE loss: 91.0781\n",
      "Iteration: 177700\n",
      "Gradient: [ -1.1531  20.7464 -25.9247 106.3979 400.1972]\n",
      "Weights: [-4.7999  0.7818 -1.3188  0.1755  0.1324]\n",
      "MSE loss: 90.7015\n",
      "Iteration: 177800\n",
      "Gradient: [ -1.4946 -12.5142 -22.6736 -52.7259 -88.8572]\n",
      "Weights: [-4.8056  0.7826 -1.3166  0.1751  0.1322]\n",
      "MSE loss: 90.7713\n",
      "Iteration: 177900\n",
      "Gradient: [  1.989   -5.2979 -30.943  -42.558   -9.5157]\n",
      "Weights: [-4.8022  0.7842 -1.3171  0.1745  0.132 ]\n",
      "MSE loss: 90.967\n",
      "Iteration: 178000\n",
      "Gradient: [ 4.0648 18.5197 -2.5884  8.0427  7.9155]\n",
      "Weights: [-4.7905  0.7781 -1.3158  0.1745  0.1324]\n",
      "MSE loss: 90.7019\n",
      "Iteration: 178100\n",
      "Gradient: [-15.2344   1.3765 -26.9074  44.9738 -23.501 ]\n",
      "Weights: [-4.7855  0.7719 -1.3158  0.1749  0.1324]\n",
      "MSE loss: 90.6907\n",
      "Iteration: 178200\n",
      "Gradient: [  -4.7154  -16.748   -45.054  -127.6002 -183.3974]\n",
      "Weights: [-4.7865  0.769  -1.316   0.1751  0.1322]\n",
      "MSE loss: 90.8408\n",
      "Iteration: 178300\n",
      "Gradient: [-3.4645  1.6002 -9.8734 63.6608 68.9934]\n",
      "Weights: [-4.7941  0.7734 -1.3124  0.174   0.1324]\n",
      "MSE loss: 90.7077\n",
      "Iteration: 178400\n",
      "Gradient: [ -1.9776  11.0252 -28.247   69.6604  43.6039]\n",
      "Weights: [-4.8     0.7811 -1.3167  0.1739  0.1325]\n",
      "MSE loss: 90.8178\n",
      "Iteration: 178500\n",
      "Gradient: [   9.5028   21.1274   18.1072   61.0473 -118.873 ]\n",
      "Weights: [-4.7785  0.7724 -1.3164  0.1747  0.1324]\n",
      "MSE loss: 90.8029\n",
      "Iteration: 178600\n",
      "Gradient: [ -1.6057   1.3285   5.1528 -52.6627 185.28  ]\n",
      "Weights: [-4.7781  0.7657 -1.3158  0.1755  0.1325]\n",
      "MSE loss: 90.7576\n",
      "Iteration: 178700\n",
      "Gradient: [ 3.5155 14.0311 15.6873 48.2289 87.4112]\n",
      "Weights: [-4.779   0.7672 -1.3156  0.1754  0.1323]\n",
      "MSE loss: 90.731\n",
      "Iteration: 178800\n",
      "Gradient: [  5.8512  16.1766   3.1966 -30.2991 343.6419]\n",
      "Weights: [-4.7819  0.7666 -1.3138  0.1757  0.1323]\n",
      "MSE loss: 90.8716\n",
      "Iteration: 178900\n",
      "Gradient: [2.070700e+00 4.093370e+01 3.181520e+01 9.210000e-02 2.942592e+02]\n",
      "Weights: [-4.7795  0.7674 -1.3145  0.1749  0.1322]\n",
      "MSE loss: 90.7518\n",
      "Iteration: 179000\n",
      "Gradient: [  -1.8833   -3.4925  -46.98    -18.6977 -123.3288]\n",
      "Weights: [-4.8023  0.7815 -1.3157  0.1753  0.1321]\n",
      "MSE loss: 90.7254\n",
      "Iteration: 179100\n",
      "Gradient: [  1.3967  10.0323 -64.3199  -1.1129 -54.3966]\n",
      "Weights: [-4.7784  0.7665 -1.3176  0.1756  0.1321]\n",
      "MSE loss: 90.9488\n",
      "Iteration: 179200\n",
      "Gradient: [  5.768   -2.8657 -27.0282   4.1098 239.6634]\n",
      "Weights: [-4.7924  0.7772 -1.3175  0.1752  0.1323]\n",
      "MSE loss: 90.6838\n",
      "Iteration: 179300\n",
      "Gradient: [ 10.3756  15.6526   4.3024  25.0616 247.2414]\n",
      "Weights: [-4.7841  0.7717 -1.3142  0.1752  0.1322]\n",
      "MSE loss: 90.8085\n",
      "Iteration: 179400\n",
      "Gradient: [  2.0297  -0.5793 -14.1754 -72.3912  36.0423]\n",
      "Weights: [-4.7846  0.7665 -1.3141  0.1756  0.132 ]\n",
      "MSE loss: 90.7197\n",
      "Iteration: 179500\n",
      "Gradient: [ -0.565   12.5117  -4.8533  66.2626 115.7212]\n",
      "Weights: [-4.8035  0.7696 -1.3096  0.1748  0.132 ]\n",
      "MSE loss: 90.8627\n",
      "Iteration: 179600\n",
      "Gradient: [  5.7881  12.9889   1.6443 -43.7258  23.9332]\n",
      "Weights: [-4.7911  0.7802 -1.3109  0.1746  0.1317]\n",
      "MSE loss: 90.9281\n",
      "Iteration: 179700\n",
      "Gradient: [  -3.1088  -33.0822   28.6693   -6.5971 -258.3254]\n",
      "Weights: [-4.7869  0.7705 -1.3106  0.1743  0.1318]\n",
      "MSE loss: 90.7528\n",
      "Iteration: 179800\n",
      "Gradient: [  2.7337  -1.2427 -11.4571  13.9592  37.2265]\n",
      "Weights: [-4.7929  0.7768 -1.3092  0.1743  0.1316]\n",
      "MSE loss: 90.789\n",
      "Iteration: 179900\n",
      "Gradient: [   0.607     3.9133   13.303   -85.6723 -205.5869]\n",
      "Weights: [-4.7855  0.7797 -1.3117  0.1736  0.1316]\n",
      "MSE loss: 90.9879\n",
      "Iteration: 180000\n",
      "Gradient: [-8.4971  8.9795 -4.1388 41.991  86.5058]\n",
      "Weights: [-4.788   0.769  -1.3122  0.1758  0.1317]\n",
      "MSE loss: 90.6965\n",
      "Iteration: 180100\n",
      "Gradient: [-10.1912 -11.2193 -19.1965 -42.694   30.7591]\n",
      "Weights: [-4.7955  0.7676 -1.3117  0.1756  0.1318]\n",
      "MSE loss: 90.8192\n",
      "Iteration: 180200\n",
      "Gradient: [  0.1729  -3.9111 -64.0994 -27.5541  52.694 ]\n",
      "Weights: [-4.799   0.7688 -1.3092  0.1751  0.1316]\n",
      "MSE loss: 90.8142\n",
      "Iteration: 180300\n",
      "Gradient: [ -4.5279 -12.3042  18.4238 -67.4822  20.7007]\n",
      "Weights: [-4.8011  0.7697 -1.3073  0.1748  0.1314]\n",
      "MSE loss: 90.8757\n",
      "Iteration: 180400\n",
      "Gradient: [  -5.1113   -8.9397   12.0269  -57.3511 -250.9817]\n",
      "Weights: [-4.7829  0.7666 -1.312   0.1763  0.1315]\n",
      "MSE loss: 90.7195\n",
      "Iteration: 180500\n",
      "Gradient: [  4.9122  -1.2025 -23.254  -17.7598 402.8877]\n",
      "Weights: [-4.7931  0.7705 -1.3133  0.1764  0.1316]\n",
      "MSE loss: 90.6837\n",
      "Iteration: 180600\n",
      "Gradient: [ 12.9326   7.1151   8.5185 102.9318 215.8427]\n",
      "Weights: [-4.7909  0.7707 -1.3146  0.1772  0.1316]\n",
      "MSE loss: 90.6878\n",
      "Iteration: 180700\n",
      "Gradient: [  -5.7169    1.6361   -2.8042  121.1456 -195.7959]\n",
      "Weights: [-4.7874  0.7732 -1.3152  0.1771  0.1315]\n",
      "MSE loss: 90.6871\n",
      "Iteration: 180800\n",
      "Gradient: [  4.4868   9.1025  -1.2423 -88.8748 453.4023]\n",
      "Weights: [-4.7993  0.7764 -1.316   0.1779  0.1315]\n",
      "MSE loss: 90.7073\n",
      "Iteration: 180900\n",
      "Gradient: [  10.2184    2.6029    2.5104 -214.0127 -127.8018]\n",
      "Weights: [-4.8022  0.7791 -1.3166  0.1784  0.1314]\n",
      "MSE loss: 90.837\n",
      "Iteration: 181000\n",
      "Gradient: [  8.3803  14.5687  59.2294 -79.323  209.9184]\n",
      "Weights: [-4.7951  0.7815 -1.3201  0.1786  0.1312]\n",
      "MSE loss: 90.6441\n",
      "Iteration: 181100\n",
      "Gradient: [-4.4522  4.5387 14.1916 -4.3742 23.3394]\n",
      "Weights: [-4.7891  0.7657 -1.3165  0.1797  0.131 ]\n",
      "MSE loss: 90.664\n",
      "Iteration: 181200\n",
      "Gradient: [  0.357  -12.2308  30.3766  20.3729  -9.3474]\n",
      "Weights: [-4.7787  0.7655 -1.3179  0.1798  0.1311]\n",
      "MSE loss: 90.6886\n",
      "Iteration: 181300\n",
      "Gradient: [-7.5486  7.3216 19.5631 57.8964 48.3752]\n",
      "Weights: [-4.79    0.7637 -1.3175  0.1799  0.1312]\n",
      "MSE loss: 90.7251\n",
      "Iteration: 181400\n",
      "Gradient: [ -3.7986  -1.7517  -1.5676  75.4123 -91.0217]\n",
      "Weights: [-4.7913  0.7684 -1.3181  0.1795  0.1313]\n",
      "MSE loss: 90.6637\n",
      "Iteration: 181500\n",
      "Gradient: [-0.953   0.7369 -8.6194 60.6792 35.2147]\n",
      "Weights: [-4.7852  0.7693 -1.3187  0.1798  0.1311]\n",
      "MSE loss: 90.6404\n",
      "Iteration: 181600\n",
      "Gradient: [  9.38    -8.1735  64.4223  39.2789 107.1326]\n",
      "Weights: [-4.77    0.7697 -1.3209  0.1803  0.1311]\n",
      "MSE loss: 91.0504\n",
      "Iteration: 181700\n",
      "Gradient: [  6.8501   4.4812  30.3643 -31.5226 -96.5554]\n",
      "Weights: [-4.7919  0.776  -1.3188  0.1796  0.1311]\n",
      "MSE loss: 90.6595\n",
      "Iteration: 181800\n",
      "Gradient: [  5.1408  -3.2484   6.6558  45.9778 232.629 ]\n",
      "Weights: [-4.7906  0.7877 -1.324   0.18    0.131 ]\n",
      "MSE loss: 90.7271\n",
      "Iteration: 181900\n",
      "Gradient: [ -6.1057   7.704   13.4214 103.586    6.7977]\n",
      "Weights: [-4.7993  0.7909 -1.3232  0.18    0.131 ]\n",
      "MSE loss: 90.7324\n",
      "Iteration: 182000\n",
      "Gradient: [-8.4491  4.1906 32.7468 45.1789 -9.7089]\n",
      "Weights: [-4.7872  0.7856 -1.3214  0.1793  0.131 ]\n",
      "MSE loss: 90.8775\n",
      "Iteration: 182100\n",
      "Gradient: [   9.0437    0.7678  -18.6957 -129.1628 -193.4273]\n",
      "Weights: [-4.8007  0.7874 -1.3209  0.1792  0.131 ]\n",
      "MSE loss: 90.6612\n",
      "Iteration: 182200\n",
      "Gradient: [   6.5059  -34.9816   25.4065  -22.1253 -325.381 ]\n",
      "Weights: [-4.8002  0.7794 -1.3209  0.1797  0.131 ]\n",
      "MSE loss: 90.7412\n",
      "Iteration: 182300\n",
      "Gradient: [  -8.7219  -13.2297  -15.2655  -99.7457 -397.8003]\n",
      "Weights: [-4.7816  0.7621 -1.3186  0.1803  0.131 ]\n",
      "MSE loss: 90.7476\n",
      "Iteration: 182400\n",
      "Gradient: [  -6.1009  -17.6389  -19.0284  -77.0452 -184.7912]\n",
      "Weights: [-4.7919  0.7632 -1.3171  0.1803  0.1306]\n",
      "MSE loss: 91.1157\n",
      "Iteration: 182500\n",
      "Gradient: [ -5.5954  -5.6263 -62.5795 -46.0872   7.6816]\n",
      "Weights: [-4.8074  0.7715 -1.3145  0.1807  0.1305]\n",
      "MSE loss: 90.8653\n",
      "Iteration: 182600\n",
      "Gradient: [  4.7085 -25.3783  26.0478  -3.858  -62.1007]\n",
      "Weights: [-4.7896  0.7671 -1.316   0.1806  0.1307]\n",
      "MSE loss: 90.6395\n",
      "Iteration: 182700\n",
      "Gradient: [  -4.8473   15.5921    0.9846  -15.2472 -275.9538]\n",
      "Weights: [-4.7888  0.7752 -1.3175  0.1795  0.1307]\n",
      "MSE loss: 90.6812\n",
      "Iteration: 182800\n",
      "Gradient: [  4.5598   7.5117  27.0573 -14.2505  -6.9492]\n",
      "Weights: [-4.7878  0.7676 -1.3163  0.1804  0.1307]\n",
      "MSE loss: 90.6382\n",
      "Iteration: 182900\n",
      "Gradient: [  4.6774   5.2739  -9.4367  18.2388 389.5491]\n",
      "Weights: [-4.7923  0.7621 -1.3149  0.1807  0.1307]\n",
      "MSE loss: 90.7085\n",
      "Iteration: 183000\n",
      "Gradient: [  -2.0195    3.6357  -10.481   -61.9355 -418.323 ]\n",
      "Weights: [-4.797   0.7698 -1.3189  0.1813  0.1304]\n",
      "MSE loss: 90.8879\n",
      "Iteration: 183100\n",
      "Gradient: [ -14.6572  -11.9813   -3.8152   33.5544 -433.6569]\n",
      "Weights: [-4.8003  0.775  -1.3201  0.1813  0.1306]\n",
      "MSE loss: 90.7527\n",
      "Iteration: 183200\n",
      "Gradient: [ -6.7376  -5.8133 -14.0117  37.8532 177.5488]\n",
      "Weights: [-4.7853  0.7717 -1.32    0.1813  0.1306]\n",
      "MSE loss: 90.6414\n",
      "Iteration: 183300\n",
      "Gradient: [  -8.9634  -15.9027   29.7696   68.4805 -178.5016]\n",
      "Weights: [-4.8048  0.7752 -1.3192  0.1814  0.1305]\n",
      "MSE loss: 90.7965\n",
      "Iteration: 183400\n",
      "Gradient: [  -7.1285   15.3805  -15.6997 -119.5556  183.0216]\n",
      "Weights: [-4.7895  0.7798 -1.3202  0.1814  0.1307]\n",
      "MSE loss: 91.0359\n",
      "Iteration: 183500\n",
      "Gradient: [   2.2536   20.826    34.1888   27.9497 -131.6633]\n",
      "Weights: [-4.7793  0.7711 -1.3213  0.1812  0.1308]\n",
      "MSE loss: 90.7139\n",
      "Iteration: 183600\n",
      "Gradient: [ -3.7326  -4.783   28.9808  79.1547 133.4001]\n",
      "Weights: [-4.8002  0.7742 -1.323   0.1825  0.1309]\n",
      "MSE loss: 90.7693\n",
      "Iteration: 183700\n",
      "Gradient: [ -2.2517  21.9745 -34.3137  66.4999 -52.9584]\n",
      "Weights: [-4.8036  0.7909 -1.3268  0.1822  0.1306]\n",
      "MSE loss: 90.6213\n",
      "Iteration: 183800\n",
      "Gradient: [ 4.900000e-02  1.520810e+01 -4.338700e+00  2.213860e+01 -1.363526e+02]\n",
      "Weights: [-4.791   0.7947 -1.329   0.1832  0.1303]\n",
      "MSE loss: 90.9631\n",
      "Iteration: 183900\n",
      "Gradient: [ -1.5549  -3.0612 -32.345  -31.3541 203.0261]\n",
      "Weights: [-4.8011  0.7896 -1.3296  0.1832  0.1306]\n",
      "MSE loss: 90.5857\n",
      "Iteration: 184000\n",
      "Gradient: [ -14.7009   -4.0573   23.4524   -8.9228 -320.4055]\n",
      "Weights: [-4.7957  0.7807 -1.3276  0.1829  0.1306]\n",
      "MSE loss: 90.6532\n",
      "Iteration: 184100\n",
      "Gradient: [ -12.0754  -17.8029  -16.5224   93.7994 -212.9563]\n",
      "Weights: [-4.793   0.7846 -1.328   0.1822  0.1307]\n",
      "MSE loss: 90.6139\n",
      "Iteration: 184200\n",
      "Gradient: [ -0.3932   6.6388   4.578  177.7999 204.2394]\n",
      "Weights: [-4.8065  0.7929 -1.328   0.1826  0.1306]\n",
      "MSE loss: 90.6619\n",
      "Iteration: 184300\n",
      "Gradient: [   4.3159   -0.9855  -34.9602  -10.7696 -457.0026]\n",
      "Weights: [-4.8102  0.7911 -1.3303  0.1833  0.1304]\n",
      "MSE loss: 90.9802\n",
      "Iteration: 184400\n",
      "Gradient: [  2.5157  -9.8811  13.7856 -51.8455 -77.6173]\n",
      "Weights: [-4.7962  0.7937 -1.331   0.1827  0.1303]\n",
      "MSE loss: 90.8363\n",
      "Iteration: 184500\n",
      "Gradient: [ -7.4935  -1.7062  22.7942  64.5025 201.1308]\n",
      "Weights: [-4.8021  0.7966 -1.331   0.1841  0.1305]\n",
      "MSE loss: 90.9317\n",
      "Iteration: 184600\n",
      "Gradient: [ -0.3507 -10.8943  -9.0005  23.3062 -57.0484]\n",
      "Weights: [-4.8144  0.8081 -1.334   0.1844  0.1303]\n",
      "MSE loss: 90.7873\n",
      "Iteration: 184700\n",
      "Gradient: [  -8.5203  -11.5342  -33.7798  114.7239 -145.1262]\n",
      "Weights: [-4.8052  0.7967 -1.3328  0.1841  0.1303]\n",
      "MSE loss: 90.6182\n",
      "Iteration: 184800\n",
      "Gradient: [  -2.6454    6.5655    2.2042   -8.5751 -218.9312]\n",
      "Weights: [-4.8009  0.7929 -1.3323  0.1838  0.1304]\n",
      "MSE loss: 90.6043\n",
      "Iteration: 184900\n",
      "Gradient: [   0.214   -21.4975  -28.2224   37.4204 -157.6325]\n",
      "Weights: [-4.8271  0.7999 -1.3276  0.1834  0.1303]\n",
      "MSE loss: 91.0623\n",
      "Iteration: 185000\n",
      "Gradient: [ -0.8068  -3.2141 -19.4944 -76.8254  29.7872]\n",
      "Weights: [-4.8054  0.7977 -1.3306  0.1833  0.1303]\n",
      "MSE loss: 90.6217\n",
      "Iteration: 185100\n",
      "Gradient: [ 11.4922   4.8068  -4.6574   9.0306 279.5914]\n",
      "Weights: [-4.7953  0.7882 -1.3296  0.1839  0.1302]\n",
      "MSE loss: 90.5901\n",
      "Iteration: 185200\n",
      "Gradient: [  9.6818  20.9094  -4.9353  -8.8623 196.0455]\n",
      "Weights: [-4.78    0.7815 -1.3314  0.1848  0.1304]\n",
      "MSE loss: 90.7197\n",
      "Iteration: 185300\n",
      "Gradient: [  -4.3602  -25.1801   26.7235  -29.5775 -229.4196]\n",
      "Weights: [-4.7952  0.7824 -1.329   0.1836  0.1305]\n",
      "MSE loss: 90.5906\n",
      "Iteration: 185400\n",
      "Gradient: [ -14.5781   -4.2342  -17.9098  -78.7337 -390.0307]\n",
      "Weights: [-4.7927  0.783  -1.3302  0.1839  0.1302]\n",
      "MSE loss: 90.8519\n",
      "Iteration: 185500\n",
      "Gradient: [ -14.5258   14.1619   -0.6138  -66.8416 -316.1118]\n",
      "Weights: [-4.8155  0.7986 -1.3309  0.1836  0.1304]\n",
      "MSE loss: 90.7484\n",
      "Iteration: 185600\n",
      "Gradient: [  -5.884    -3.1816  -13.9164   33.2325 -116.1432]\n",
      "Weights: [-4.7912  0.7823 -1.3322  0.1847  0.1305]\n",
      "MSE loss: 90.5747\n",
      "Iteration: 185700\n",
      "Gradient: [-13.2714  10.6929  15.024   55.5302 110.1032]\n",
      "Weights: [-4.809   0.7991 -1.3331  0.1857  0.1302]\n",
      "MSE loss: 90.9039\n",
      "Iteration: 185800\n",
      "Gradient: [  -2.5101   -2.576   -18.9863  -12.449  -115.7337]\n",
      "Weights: [-4.7936  0.7953 -1.3342  0.1849  0.1303]\n",
      "MSE loss: 90.6318\n",
      "Iteration: 185900\n",
      "Gradient: [  9.7817 -10.9455   3.4434  63.8173 202.4712]\n",
      "Weights: [-4.7921  0.791  -1.3375  0.1864  0.1302]\n",
      "MSE loss: 90.5207\n",
      "Iteration: 186000\n",
      "Gradient: [ -5.7564 -24.059  -11.1422 -62.8269 167.2386]\n",
      "Weights: [-4.7903  0.7807 -1.3374  0.1869  0.1302]\n",
      "MSE loss: 90.7599\n",
      "Iteration: 186100\n",
      "Gradient: [ -14.4771   -4.9145   13.3574  -79.8629 -232.5108]\n",
      "Weights: [-4.8067  0.7878 -1.3387  0.188   0.1301]\n",
      "MSE loss: 90.8674\n",
      "Iteration: 186200\n",
      "Gradient: [ -5.17     0.8843  -2.1065 187.8694 283.4008]\n",
      "Weights: [-4.792   0.7884 -1.3399  0.1878  0.1303]\n",
      "MSE loss: 90.5232\n",
      "Iteration: 186300\n",
      "Gradient: [  5.0177  10.4571   4.6394   8.898  -29.7662]\n",
      "Weights: [-4.7991  0.7957 -1.3414  0.1876  0.1302]\n",
      "MSE loss: 90.5052\n",
      "Iteration: 186400\n",
      "Gradient: [  -6.3829   -1.8744   -8.9945   15.9818 -416.7051]\n",
      "Weights: [-4.7862  0.7926 -1.3418  0.1868  0.1304]\n",
      "MSE loss: 90.5563\n",
      "Iteration: 186500\n",
      "Gradient: [ -0.6886 -27.2289  38.5007  60.045   36.8788]\n",
      "Weights: [-4.7969  0.7908 -1.3406  0.187   0.1306]\n",
      "MSE loss: 90.5444\n",
      "Iteration: 186600\n",
      "Gradient: [ -9.7761  12.2593 -25.3787  19.605   27.0467]\n",
      "Weights: [-4.8066  0.8063 -1.3408  0.1854  0.1305]\n",
      "MSE loss: 90.5565\n",
      "Iteration: 186700\n",
      "Gradient: [-2.763000e-01  1.134790e+01  2.075020e+01 -1.980400e+00 -2.891009e+02]\n",
      "Weights: [-4.7904  0.7925 -1.3395  0.1858  0.1306]\n",
      "MSE loss: 90.5334\n",
      "Iteration: 186800\n",
      "Gradient: [   2.0698  -20.9699  -28.6084  -98.6993 -174.6167]\n",
      "Weights: [-4.8019  0.7961 -1.3419  0.1864  0.1306]\n",
      "MSE loss: 90.6323\n",
      "Iteration: 186900\n",
      "Gradient: [ -4.4089 -15.5069  -1.3265  19.952   88.0183]\n",
      "Weights: [-4.8192  0.8027 -1.3412  0.1866  0.1305]\n",
      "MSE loss: 90.9081\n",
      "Iteration: 187000\n",
      "Gradient: [  -1.8956  -11.936    12.7967   41.9257 -139.0029]\n",
      "Weights: [-4.801   0.7944 -1.3428  0.1872  0.1305]\n",
      "MSE loss: 90.6748\n",
      "Iteration: 187100\n",
      "Gradient: [  1.8893  -1.9994   1.443   44.4532 446.4879]\n",
      "Weights: [-4.7843  0.7908 -1.3389  0.1869  0.1306]\n",
      "MSE loss: 91.0354\n",
      "Iteration: 187200\n",
      "Gradient: [ -0.0795 -16.7006 -49.9495 -28.3229  16.4108]\n",
      "Weights: [-4.7999  0.7914 -1.3405  0.1876  0.1304]\n",
      "MSE loss: 90.5674\n",
      "Iteration: 187300\n",
      "Gradient: [ -2.257   12.1691 -11.4284 -29.3979 -89.4963]\n",
      "Weights: [-4.8188  0.8077 -1.341   0.1864  0.1302]\n",
      "MSE loss: 90.7938\n",
      "Iteration: 187400\n",
      "Gradient: [ 10.7395 -30.9139  12.5489 -54.1029 106.5067]\n",
      "Weights: [-4.8086  0.8053 -1.3417  0.1869  0.1302]\n",
      "MSE loss: 90.551\n",
      "Iteration: 187500\n",
      "Gradient: [   4.3136  -11.4799  -27.3451 -123.8378   85.569 ]\n",
      "Weights: [-4.8022  0.7959 -1.3405  0.1873  0.1301]\n",
      "MSE loss: 90.59\n",
      "Iteration: 187600\n",
      "Gradient: [  8.3242  17.5902  -4.8909 -65.8982 295.4983]\n",
      "Weights: [-4.7954  0.7972 -1.3422  0.1877  0.1303]\n",
      "MSE loss: 90.5207\n",
      "Iteration: 187700\n",
      "Gradient: [ -15.3611   -8.3845  -25.144  -133.7306  131.2168]\n",
      "Weights: [-4.7774  0.7827 -1.3384  0.1864  0.1303]\n",
      "MSE loss: 90.7218\n",
      "Iteration: 187800\n",
      "Gradient: [ 6.1257  0.9493  2.1696 93.8693 44.692 ]\n",
      "Weights: [-4.7793  0.7806 -1.3366  0.1871  0.1303]\n",
      "MSE loss: 90.622\n",
      "Iteration: 187900\n",
      "Gradient: [ -16.3153   -9.9649   37.5006  123.2786 -128.483 ]\n",
      "Weights: [-4.7795  0.773  -1.335   0.1875  0.1303]\n",
      "MSE loss: 90.5953\n",
      "Iteration: 188000\n",
      "Gradient: [  -5.6276    2.6911   -3.6551 -107.9192   22.0591]\n",
      "Weights: [-4.7849  0.7755 -1.3327  0.1865  0.1302]\n",
      "MSE loss: 90.5565\n",
      "Iteration: 188100\n",
      "Gradient: [  8.8458   4.0023  31.0546 -11.2467  66.445 ]\n",
      "Weights: [-4.7815  0.7899 -1.3364  0.1869  0.1298]\n",
      "MSE loss: 90.7448\n",
      "Iteration: 188200\n",
      "Gradient: [-11.596   15.7786  34.1697  11.7216 -15.9666]\n",
      "Weights: [-4.7825  0.7852 -1.3356  0.1875  0.1299]\n",
      "MSE loss: 90.6852\n",
      "Iteration: 188300\n",
      "Gradient: [ -2.1136  -1.096   -3.5638 -70.5498 179.7378]\n",
      "Weights: [-4.7895  0.7862 -1.3371  0.1881  0.13  ]\n",
      "MSE loss: 90.5666\n",
      "Iteration: 188400\n",
      "Gradient: [  1.9954 -28.5631 -63.6255 -10.3    243.8902]\n",
      "Weights: [-4.7965  0.7882 -1.337   0.1869  0.13  ]\n",
      "MSE loss: 90.6215\n",
      "Iteration: 188500\n",
      "Gradient: [-3.842000e-01 -2.302600e+00 -6.889700e+00 -1.078133e+02 -3.907201e+02]\n",
      "Weights: [-4.7952  0.7817 -1.3338  0.1866  0.1301]\n",
      "MSE loss: 90.5913\n",
      "Iteration: 188600\n",
      "Gradient: [ -2.0694  14.4906  21.1007 -89.3323 136.7944]\n",
      "Weights: [-4.794   0.7862 -1.3393  0.188   0.1303]\n",
      "MSE loss: 90.5386\n",
      "Iteration: 188700\n",
      "Gradient: [ -0.8579  -0.9561   0.1553 -86.0036  53.078 ]\n",
      "Weights: [-4.7824  0.7833 -1.3376  0.1871  0.1304]\n",
      "MSE loss: 90.5917\n",
      "Iteration: 188800\n",
      "Gradient: [-5.8152 -6.7737 13.0781 40.8003 82.1304]\n",
      "Weights: [-4.7956  0.7891 -1.3394  0.1867  0.1304]\n",
      "MSE loss: 90.571\n",
      "Iteration: 188900\n",
      "Gradient: [ -6.5453   6.9404  48.2705 -38.8374 433.9505]\n",
      "Weights: [-4.7817  0.7917 -1.3394  0.1865  0.1303]\n",
      "MSE loss: 90.6847\n",
      "Iteration: 189000\n",
      "Gradient: [   7.6782    0.6281  -30.2826   45.956  -229.6228]\n",
      "Weights: [-4.7944  0.7842 -1.3384  0.187   0.1306]\n",
      "MSE loss: 90.5982\n",
      "Iteration: 189100\n",
      "Gradient: [   1.2315  -17.1928   16.7003  -55.924  -164.742 ]\n",
      "Weights: [-4.7772  0.7888 -1.3387  0.1859  0.1306]\n",
      "MSE loss: 90.7993\n",
      "Iteration: 189200\n",
      "Gradient: [ 19.2906  18.352   65.551    1.9816 168.3027]\n",
      "Weights: [-4.7746  0.7893 -1.3422  0.1871  0.1307]\n",
      "MSE loss: 90.9934\n",
      "Iteration: 189300\n",
      "Gradient: [ -7.5863   4.195  -30.2996  98.5597 -50.4919]\n",
      "Weights: [-4.7909  0.7901 -1.3448  0.1878  0.1309]\n",
      "MSE loss: 90.5595\n",
      "Iteration: 189400\n",
      "Gradient: [ -2.5355   0.6945 -15.9109  27.5945 113.5165]\n",
      "Weights: [-4.7923  0.7862 -1.3454  0.1879  0.131 ]\n",
      "MSE loss: 90.6853\n",
      "Iteration: 189500\n",
      "Gradient: [  3.0367  -3.054   21.6073 -16.5942 166.1804]\n",
      "Weights: [-4.7766  0.7938 -1.345   0.1869  0.1308]\n",
      "MSE loss: 90.7523\n",
      "Iteration: 189600\n",
      "Gradient: [  0.7495   0.6657 -37.0421  -6.0705   4.5385]\n",
      "Weights: [-4.7922  0.8023 -1.3477  0.1875  0.1307]\n",
      "MSE loss: 90.4937\n",
      "Iteration: 189700\n",
      "Gradient: [ 7.7108  3.9525 -5.2095 55.1359 99.1526]\n",
      "Weights: [-4.7896  0.8022 -1.3481  0.1881  0.1306]\n",
      "MSE loss: 90.5762\n",
      "Iteration: 189800\n",
      "Gradient: [15.7225  8.7063 11.7214 62.5605 40.43  ]\n",
      "Weights: [-4.7898  0.7994 -1.3497  0.1882  0.1308]\n",
      "MSE loss: 90.4997\n",
      "Iteration: 189900\n",
      "Gradient: [ -8.78   -10.9793  18.2199  -5.3885 103.975 ]\n",
      "Weights: [-4.7831  0.8032 -1.3534  0.1888  0.1309]\n",
      "MSE loss: 90.5981\n",
      "Iteration: 190000\n",
      "Gradient: [ -6.7333 -13.3947 -44.5395 -10.9272 -86.8313]\n",
      "Weights: [-4.807   0.8094 -1.3548  0.1885  0.1308]\n",
      "MSE loss: 90.8765\n",
      "Iteration: 190100\n",
      "Gradient: [  4.535  -13.8092 -14.6568 -60.0061  12.0264]\n",
      "Weights: [-4.7998  0.8137 -1.3538  0.1884  0.131 ]\n",
      "MSE loss: 90.562\n",
      "Iteration: 190200\n",
      "Gradient: [-10.1261 -25.4392  -7.1109 -77.8707  17.1654]\n",
      "Weights: [-4.8063  0.8083 -1.353   0.1888  0.1307]\n",
      "MSE loss: 90.64\n",
      "Iteration: 190300\n",
      "Gradient: [  -2.1508  -11.8926   21.7663  -52.1675 -112.0901]\n",
      "Weights: [-4.803   0.8194 -1.353   0.188   0.1307]\n",
      "MSE loss: 90.5011\n",
      "Iteration: 190400\n",
      "Gradient: [ 1.435000e-01  8.124200e+00  3.047260e+01 -2.227390e+01  1.823509e+02]\n",
      "Weights: [-4.7912  0.8153 -1.3534  0.187   0.131 ]\n",
      "MSE loss: 90.587\n",
      "Iteration: 190500\n",
      "Gradient: [ -1.3222   5.4459 -27.3982 113.1914  95.0048]\n",
      "Weights: [-4.7995  0.8117 -1.3507  0.1876  0.1309]\n",
      "MSE loss: 90.5366\n",
      "Iteration: 190600\n",
      "Gradient: [   4.8865    1.9917    6.0269   67.8316 -245.013 ]\n",
      "Weights: [-4.8015  0.812  -1.3498  0.1871  0.1308]\n",
      "MSE loss: 90.4817\n",
      "Iteration: 190700\n",
      "Gradient: [ 3.4177 11.0422  2.3893 25.1035 89.2793]\n",
      "Weights: [-4.8001  0.8189 -1.3503  0.1865  0.1311]\n",
      "MSE loss: 90.7858\n",
      "Iteration: 190800\n",
      "Gradient: [  7.2122  -5.8105 -22.6215 -60.0542 -85.4984]\n",
      "Weights: [-4.7989  0.8161 -1.3523  0.1861  0.1309]\n",
      "MSE loss: 90.6826\n",
      "Iteration: 190900\n",
      "Gradient: [  0.1982   9.2637 -11.6761  13.7181  34.4573]\n",
      "Weights: [-4.8113  0.829  -1.3565  0.187   0.1311]\n",
      "MSE loss: 90.4897\n",
      "Iteration: 191000\n",
      "Gradient: [ 12.9375  28.2423  40.6963  60.9085 208.6679]\n",
      "Weights: [-4.7912  0.8214 -1.3543  0.1862  0.1312]\n",
      "MSE loss: 90.7575\n",
      "Iteration: 191100\n",
      "Gradient: [ -2.403    5.3365  19.4938  50.2014 254.5831]\n",
      "Weights: [-4.783   0.8127 -1.3551  0.1865  0.1314]\n",
      "MSE loss: 90.7221\n",
      "Iteration: 191200\n",
      "Gradient: [ -2.259    1.2784 -17.4391 -26.1571 176.3822]\n",
      "Weights: [-4.8024  0.8102 -1.3525  0.1872  0.1314]\n",
      "MSE loss: 90.5533\n",
      "Iteration: 191300\n",
      "Gradient: [  -2.9622   22.5363  -62.706   -55.8814 -124.0585]\n",
      "Weights: [-4.8015  0.808  -1.3507  0.1864  0.1314]\n",
      "MSE loss: 90.5325\n",
      "Iteration: 191400\n",
      "Gradient: [ -3.4387   8.8344 -41.7444  22.9013 157.2327]\n",
      "Weights: [-4.7898  0.8033 -1.3472  0.1856  0.1314]\n",
      "MSE loss: 90.5916\n",
      "Iteration: 191500\n",
      "Gradient: [  1.6023   2.1927  38.9466  80.0069 432.4   ]\n",
      "Weights: [-4.7973  0.8085 -1.3436  0.1842  0.1312]\n",
      "MSE loss: 90.5742\n",
      "Iteration: 191600\n",
      "Gradient: [  8.5774  28.9823   4.3269  63.0183 309.7084]\n",
      "Weights: [-4.8006  0.8098 -1.3448  0.1851  0.131 ]\n",
      "MSE loss: 90.5324\n",
      "Iteration: 191700\n",
      "Gradient: [ 2.9077  3.4167  3.4952 60.4252 -4.0964]\n",
      "Weights: [-4.7897  0.8068 -1.3417  0.1844  0.1308]\n",
      "MSE loss: 90.7647\n",
      "Iteration: 191800\n",
      "Gradient: [ -7.5067  -3.8719   1.6628 -28.6209 -81.5267]\n",
      "Weights: [-4.8013  0.7998 -1.3403  0.1845  0.1309]\n",
      "MSE loss: 90.5769\n",
      "Iteration: 191900\n",
      "Gradient: [ 1.910000e-02 -1.032080e+01 -4.762860e+01  5.020960e+01 -3.173624e+02]\n",
      "Weights: [-4.7973  0.8025 -1.3419  0.185   0.1308]\n",
      "MSE loss: 90.5411\n",
      "Iteration: 192000\n",
      "Gradient: [ -9.6472 -20.7339  -8.1645  -7.2812  23.7574]\n",
      "Weights: [-4.8161  0.8103 -1.3437  0.1853  0.131 ]\n",
      "MSE loss: 90.6862\n",
      "Iteration: 192100\n",
      "Gradient: [ 6.9263 -5.1387  4.7265 74.3347 61.5966]\n",
      "Weights: [-4.7908  0.8035 -1.3459  0.1854  0.1311]\n",
      "MSE loss: 90.542\n",
      "Iteration: 192200\n",
      "Gradient: [ -6.8669   1.5022 -12.6206  85.1717 209.8133]\n",
      "Weights: [-4.7948  0.7941 -1.3454  0.1847  0.1316]\n",
      "MSE loss: 90.7702\n",
      "Iteration: 192300\n",
      "Gradient: [ -1.2325  11.3491 -29.8544  33.6296 436.0508]\n",
      "Weights: [-4.7908  0.7941 -1.345   0.1852  0.1317]\n",
      "MSE loss: 90.5973\n",
      "Iteration: 192400\n",
      "Gradient: [  8.182   -1.4874  -5.822   47.984  247.289 ]\n",
      "Weights: [-4.7976  0.8063 -1.3441  0.1848  0.1312]\n",
      "MSE loss: 90.5397\n",
      "Iteration: 192500\n",
      "Gradient: [ -0.4308   8.2435  16.6784  27.4397 115.3702]\n",
      "Weights: [-4.7786  0.7959 -1.3429  0.1846  0.1312]\n",
      "MSE loss: 90.7409\n",
      "Iteration: 192600\n",
      "Gradient: [  -6.7169   21.6607   32.1733  191.15   -117.7133]\n",
      "Weights: [-4.7869  0.8031 -1.3469  0.1853  0.1313]\n",
      "MSE loss: 90.5949\n",
      "Iteration: 192700\n",
      "Gradient: [   8.0937   28.8357   10.7252   15.7684 -221.1748]\n",
      "Weights: [-4.7961  0.8063 -1.3472  0.1855  0.1313]\n",
      "MSE loss: 90.525\n",
      "Iteration: 192800\n",
      "Gradient: [  -5.5148  -14.0568  -43.7538  -40.001  -299.966 ]\n",
      "Weights: [-4.8085  0.8024 -1.3448  0.1854  0.1313]\n",
      "MSE loss: 90.7163\n",
      "Iteration: 192900\n",
      "Gradient: [  -1.9115   -4.7322   31.2655 -157.6072  126.4622]\n",
      "Weights: [-4.7988  0.8022 -1.3462  0.1849  0.1315]\n",
      "MSE loss: 90.5608\n",
      "Iteration: 193000\n",
      "Gradient: [ -13.4486   -5.4408   37.1124   14.0261 -378.3537]\n",
      "Weights: [-4.8033  0.8112 -1.347   0.1847  0.1315]\n",
      "MSE loss: 90.5484\n",
      "Iteration: 193100\n",
      "Gradient: [   2.9483  -12.4274   -4.1521   36.6146 -191.1043]\n",
      "Weights: [-4.8028  0.8062 -1.3445  0.1843  0.1315]\n",
      "MSE loss: 90.5448\n",
      "Iteration: 193200\n",
      "Gradient: [ -2.8538   2.5947  10.8542 -28.5297  91.1048]\n",
      "Weights: [-4.8049  0.811  -1.3434  0.1833  0.1314]\n",
      "MSE loss: 90.5357\n",
      "Iteration: 193300\n",
      "Gradient: [-2.27900e-01 -7.71300e+00 -2.20976e+01  9.37740e+00  3.61659e+02]\n",
      "Weights: [-4.8041  0.8098 -1.3436  0.1837  0.1315]\n",
      "MSE loss: 90.571\n",
      "Iteration: 193400\n",
      "Gradient: [ -5.2061   7.8766  34.952    9.7943 -94.2736]\n",
      "Weights: [-4.8033  0.8169 -1.3459  0.1832  0.1315]\n",
      "MSE loss: 90.5441\n",
      "Iteration: 193500\n",
      "Gradient: [  0.8537  -9.5741   4.1094  72.6391 103.9657]\n",
      "Weights: [-4.8022  0.8107 -1.3439  0.1838  0.1315]\n",
      "MSE loss: 90.5822\n",
      "Iteration: 193600\n",
      "Gradient: [   0.9634  -11.0474  -21.7709  -16.0132 -332.547 ]\n",
      "Weights: [-4.8059  0.8159 -1.3439  0.1836  0.1314]\n",
      "MSE loss: 90.6216\n",
      "Iteration: 193700\n",
      "Gradient: [ -2.6214  12.83   -50.4159 -28.4174 197.3852]\n",
      "Weights: [-4.8093  0.8105 -1.3443  0.1841  0.1312]\n",
      "MSE loss: 90.6127\n",
      "Iteration: 193800\n",
      "Gradient: [ -4.6408  -6.7478  39.0639 -25.2712 177.7763]\n",
      "Weights: [-4.8068  0.8114 -1.3451  0.1843  0.1311]\n",
      "MSE loss: 90.5777\n",
      "Iteration: 193900\n",
      "Gradient: [ 1.4823 -1.1398 -4.5212 83.1757  4.5141]\n",
      "Weights: [-4.821   0.8242 -1.3459  0.1834  0.1315]\n",
      "MSE loss: 90.7149\n",
      "Iteration: 194000\n",
      "Gradient: [  4.9843  -0.2456  36.6889  36.3377 214.2193]\n",
      "Weights: [-4.8102  0.8252 -1.3481  0.1832  0.1315]\n",
      "MSE loss: 90.5586\n",
      "Iteration: 194100\n",
      "Gradient: [ -4.7978  -7.5541  12.7986   5.8094 -63.075 ]\n",
      "Weights: [-4.8176  0.8341 -1.3489  0.1828  0.1313]\n",
      "MSE loss: 90.6332\n",
      "Iteration: 194200\n",
      "Gradient: [  0.2749  -7.8407 -38.0919 -14.9994 -12.7653]\n",
      "Weights: [-4.811   0.8333 -1.3496  0.1835  0.1311]\n",
      "MSE loss: 90.667\n",
      "Iteration: 194300\n",
      "Gradient: [   3.1458    7.8706   37.1968   18.7202 -297.4481]\n",
      "Weights: [-4.791   0.8112 -1.3465  0.1832  0.1313]\n",
      "MSE loss: 90.7823\n",
      "Iteration: 194400\n",
      "Gradient: [   2.3581   18.3009  -36.5493    2.3542 -221.3258]\n",
      "Weights: [-4.8034  0.8132 -1.3456  0.1844  0.1313]\n",
      "MSE loss: 90.5383\n",
      "Iteration: 194500\n",
      "Gradient: [ 5.5885  6.5101 -2.8701 27.1305 52.8042]\n",
      "Weights: [-4.8047  0.8203 -1.3455  0.1832  0.1315]\n",
      "MSE loss: 90.6887\n",
      "Iteration: 194600\n",
      "Gradient: [   0.2301   12.2042   22.597   -39.5093 -132.1461]\n",
      "Weights: [-4.8102  0.818  -1.3454  0.1833  0.1314]\n",
      "MSE loss: 90.5503\n",
      "Iteration: 194700\n",
      "Gradient: [   2.8648   -5.9829   -7.5205   13.5469 -337.8078]\n",
      "Weights: [-4.8186  0.8245 -1.3464  0.1824  0.1315]\n",
      "MSE loss: 90.711\n",
      "Iteration: 194800\n",
      "Gradient: [-10.2191   1.3945 -34.8767 -82.3662 174.2819]\n",
      "Weights: [-4.8122  0.819  -1.3474  0.1829  0.1319]\n",
      "MSE loss: 90.5793\n",
      "Iteration: 194900\n",
      "Gradient: [ -5.0898  13.563  -33.9515  70.1557 -63.105 ]\n",
      "Weights: [-4.8071  0.816  -1.3483  0.1831  0.1319]\n",
      "MSE loss: 90.555\n",
      "Iteration: 195000\n",
      "Gradient: [  9.0764  15.1257  69.5578  30.2101 -81.4433]\n",
      "Weights: [-4.8007  0.8217 -1.3473  0.1826  0.132 ]\n",
      "MSE loss: 90.9473\n",
      "Iteration: 195100\n",
      "Gradient: [ -0.7023 -19.4946 -17.2926 -18.9687 -51.9015]\n",
      "Weights: [-4.8085  0.8286 -1.3499  0.1825  0.1319]\n",
      "MSE loss: 90.5959\n",
      "Iteration: 195200\n",
      "Gradient: [ -3.1937   8.4348  34.5105 -36.777   72.6885]\n",
      "Weights: [-4.811   0.834  -1.3493  0.1819  0.1321]\n",
      "MSE loss: 90.904\n",
      "Iteration: 195300\n",
      "Gradient: [-14.1521  37.1635  18.6241 -88.9101 -15.1832]\n",
      "Weights: [-4.8145  0.8312 -1.3495  0.1813  0.1322]\n",
      "MSE loss: 90.591\n",
      "Iteration: 195400\n",
      "Gradient: [ 7.29690e+00 -2.05000e-02  1.49268e+01  1.41609e+01 -6.51514e+01]\n",
      "Weights: [-4.8014  0.8139 -1.3499  0.1826  0.1323]\n",
      "MSE loss: 90.5609\n",
      "Iteration: 195500\n",
      "Gradient: [ -9.5095  23.6176 -22.8785 -44.0132  41.1126]\n",
      "Weights: [-4.7985  0.814  -1.3504  0.1831  0.1323]\n",
      "MSE loss: 90.5595\n",
      "Iteration: 195600\n",
      "Gradient: [   3.8269  -27.3487   23.9516   25.473  -179.37  ]\n",
      "Weights: [-4.7966  0.8063 -1.3484  0.1824  0.1324]\n",
      "MSE loss: 90.6631\n",
      "Iteration: 195700\n",
      "Gradient: [ -1.0861   2.2325  -3.5679 -13.1229 249.555 ]\n",
      "Weights: [-4.8181  0.822  -1.3493  0.1826  0.1322]\n",
      "MSE loss: 90.6965\n",
      "Iteration: 195800\n",
      "Gradient: [  3.431    3.1425 -16.8231  70.885  153.9337]\n",
      "Weights: [-4.8077  0.821  -1.3492  0.1824  0.1321]\n",
      "MSE loss: 90.548\n",
      "Iteration: 195900\n",
      "Gradient: [  9.2282  -9.5016  20.2197  -7.1495 -17.8935]\n",
      "Weights: [-4.8035  0.8155 -1.351   0.1833  0.1323]\n",
      "MSE loss: 90.5669\n",
      "Iteration: 196000\n",
      "Gradient: [ -2.2158  25.7474  23.4177 -14.0092 274.1626]\n",
      "Weights: [-4.7977  0.8213 -1.3507  0.183   0.1322]\n",
      "MSE loss: 90.7365\n",
      "Iteration: 196100\n",
      "Gradient: [  3.7717  -2.5328 -34.969   -2.4269 -84.9722]\n",
      "Weights: [-4.809   0.8329 -1.3524  0.182   0.132 ]\n",
      "MSE loss: 90.5804\n",
      "Iteration: 196200\n",
      "Gradient: [  4.1027   4.4392  24.5811 -53.2034  68.9806]\n",
      "Weights: [-4.7982  0.8341 -1.3531  0.1816  0.1322]\n",
      "MSE loss: 90.8884\n",
      "Iteration: 196300\n",
      "Gradient: [  3.257   10.7379 -11.3425 145.9326 140.0615]\n",
      "Weights: [-4.804   0.8281 -1.3514  0.1817  0.1324]\n",
      "MSE loss: 90.6379\n",
      "Iteration: 196400\n",
      "Gradient: [ 10.3418  19.7734  50.2907  70.3848 359.8038]\n",
      "Weights: [-4.7884  0.8236 -1.351   0.1822  0.1322]\n",
      "MSE loss: 91.1044\n",
      "Iteration: 196500\n",
      "Gradient: [ -1.3654  11.5594  72.7611 230.6978 -73.4064]\n",
      "Weights: [-4.8085  0.8294 -1.3542  0.1827  0.1323]\n",
      "MSE loss: 90.537\n",
      "Iteration: 196600\n",
      "Gradient: [  -1.8138  -14.07     -6.8349   -6.8462 -309.5962]\n",
      "Weights: [-4.803   0.8235 -1.3545  0.1825  0.1325]\n",
      "MSE loss: 90.5514\n",
      "Iteration: 196700\n",
      "Gradient: [  7.3348  -3.4779  59.0005   1.5282 146.5884]\n",
      "Weights: [-4.7878  0.8219 -1.3535  0.1824  0.1326]\n",
      "MSE loss: 91.0147\n",
      "Iteration: 196800\n",
      "Gradient: [-4.14   -7.0547 18.2126 -2.2682 93.2365]\n",
      "Weights: [-4.8128  0.8252 -1.355   0.1825  0.1328]\n",
      "MSE loss: 90.6571\n",
      "Iteration: 196900\n",
      "Gradient: [ -2.414    7.3027 -11.2355 -40.4111  49.4592]\n",
      "Weights: [-4.788   0.8154 -1.3547  0.1827  0.1329]\n",
      "MSE loss: 90.7852\n",
      "Iteration: 197000\n",
      "Gradient: [  0.5789   6.39     8.1114 -99.9924  22.0261]\n",
      "Weights: [-4.7993  0.8149 -1.3565  0.1834  0.1329]\n",
      "MSE loss: 90.6573\n",
      "Iteration: 197100\n",
      "Gradient: [   7.8271   11.9123    9.7501  166.7487 -211.0195]\n",
      "Weights: [-4.7864  0.8065 -1.3515  0.1836  0.1326]\n",
      "MSE loss: 90.7709\n",
      "Iteration: 197200\n",
      "Gradient: [  0.2283   6.5023  19.9836 -83.4076 -17.1592]\n",
      "Weights: [-4.7911  0.8172 -1.3497  0.1826  0.1324]\n",
      "MSE loss: 91.0124\n",
      "Iteration: 197300\n",
      "Gradient: [ 10.4695   3.8481   6.0846 -52.0745 -88.5283]\n",
      "Weights: [-4.8122  0.8205 -1.3513  0.1831  0.1324]\n",
      "MSE loss: 90.6474\n",
      "Iteration: 197400\n",
      "Gradient: [ -1.2242 -13.1521 -35.9377  51.4207 -28.7218]\n",
      "Weights: [-4.7986  0.8062 -1.349   0.1834  0.1323]\n",
      "MSE loss: 90.608\n",
      "Iteration: 197500\n",
      "Gradient: [  7.6616 -12.6641  22.4272   7.2009  91.4902]\n",
      "Weights: [-4.7958  0.8118 -1.3494  0.1832  0.1321]\n",
      "MSE loss: 90.5427\n",
      "Iteration: 197600\n",
      "Gradient: [   0.4781    0.3239   10.2432  -58.559  -188.1553]\n",
      "Weights: [-4.7969  0.8047 -1.3478  0.1839  0.132 ]\n",
      "MSE loss: 90.5702\n",
      "Iteration: 197700\n",
      "Gradient: [  -1.2573   -4.3737  -38.8309   43.1439 -155.7067]\n",
      "Weights: [-4.8247  0.8181 -1.3482  0.1839  0.1319]\n",
      "MSE loss: 90.9719\n",
      "Iteration: 197800\n",
      "Gradient: [   8.0343  -12.7379  -25.2439   73.7012 -247.5975]\n",
      "Weights: [-4.8033  0.8137 -1.3504  0.1839  0.1318]\n",
      "MSE loss: 90.5912\n",
      "Iteration: 197900\n",
      "Gradient: [ -1.7384  22.4815  -6.7383 -26.125   78.4188]\n",
      "Weights: [-4.8057  0.8213 -1.3515  0.1841  0.1319]\n",
      "MSE loss: 90.5379\n",
      "Iteration: 198000\n",
      "Gradient: [ -2.4741  -4.7967  49.5833 -72.8571 178.4576]\n",
      "Weights: [-4.8044  0.8262 -1.3518  0.1842  0.1315]\n",
      "MSE loss: 90.5559\n",
      "Iteration: 198100\n",
      "Gradient: [ -0.9103   2.5247   7.4359 -71.4721 124.2568]\n",
      "Weights: [-4.8026  0.8177 -1.3543  0.1851  0.1316]\n",
      "MSE loss: 90.634\n",
      "Iteration: 198200\n",
      "Gradient: [  9.9277  11.7804  40.9497  60.622  191.2474]\n",
      "Weights: [-4.7986  0.8115 -1.3507  0.1862  0.1314]\n",
      "MSE loss: 90.552\n",
      "Iteration: 198300\n",
      "Gradient: [  -3.8264   -4.4732   53.0296 -145.6847   -7.7248]\n",
      "Weights: [-4.784   0.8086 -1.351   0.1865  0.131 ]\n",
      "MSE loss: 90.6867\n",
      "Iteration: 198400\n",
      "Gradient: [  10.6382    8.7663  -29.563    14.4625 -162.3804]\n",
      "Weights: [-4.7998  0.814  -1.3516  0.1861  0.1313]\n",
      "MSE loss: 90.4877\n",
      "Iteration: 198500\n",
      "Gradient: [  -1.3271   -7.3785   -1.5874  -38.3213 -115.243 ]\n",
      "Weights: [-4.7942  0.8035 -1.3505  0.1857  0.1314]\n",
      "MSE loss: 90.6759\n",
      "Iteration: 198600\n",
      "Gradient: [   9.2345   -5.0013   40.1404  -79.3338 -222.0132]\n",
      "Weights: [-4.7993  0.8212 -1.351   0.185   0.1314]\n",
      "MSE loss: 90.703\n",
      "Iteration: 198700\n",
      "Gradient: [ -12.6517  -14.8209  -32.3216  -54.81   -406.799 ]\n",
      "Weights: [-4.8226  0.8235 -1.3549  0.1852  0.1316]\n",
      "MSE loss: 91.2357\n",
      "Iteration: 198800\n",
      "Gradient: [   6.4479   -8.3888  -56.8571  -99.0344 -213.5802]\n",
      "Weights: [-4.8044  0.8265 -1.3566  0.1847  0.1318]\n",
      "MSE loss: 90.5315\n",
      "Iteration: 198900\n",
      "Gradient: [  0.7548  10.1989  -5.1841 -53.4771 168.232 ]\n",
      "Weights: [-4.8137  0.828  -1.3563  0.1853  0.1318]\n",
      "MSE loss: 90.5494\n",
      "Iteration: 199000\n",
      "Gradient: [ -8.3687   8.1826 -28.0359 -14.2377 182.5472]\n",
      "Weights: [-4.8049  0.8239 -1.3554  0.1849  0.1318]\n",
      "MSE loss: 90.5026\n",
      "Iteration: 199100\n",
      "Gradient: [ -7.9049   1.4652  27.0072 -10.7097 -21.3947]\n",
      "Weights: [-4.8086  0.8216 -1.3517  0.1846  0.1317]\n",
      "MSE loss: 90.5365\n",
      "Iteration: 199200\n",
      "Gradient: [  1.2989  -1.1779 -16.7247 -44.1527  72.3441]\n",
      "Weights: [-4.8139  0.8216 -1.3516  0.1848  0.1317]\n",
      "MSE loss: 90.6039\n",
      "Iteration: 199300\n",
      "Gradient: [  6.3054  -4.2984 -35.3275  17.0935 -15.7279]\n",
      "Weights: [-4.8116  0.8235 -1.3516  0.1837  0.1319]\n",
      "MSE loss: 90.5523\n",
      "Iteration: 199400\n",
      "Gradient: [   1.771   -12.3397  -43.4817  -64.9018 -145.4208]\n",
      "Weights: [-4.7967  0.8118 -1.3515  0.1837  0.132 ]\n",
      "MSE loss: 90.6183\n",
      "Iteration: 199500\n",
      "Gradient: [-4.4853 12.5531 -7.8343 12.5689 55.4602]\n",
      "Weights: [-4.8144  0.8222 -1.3495  0.1839  0.1319]\n",
      "MSE loss: 90.7028\n",
      "Iteration: 199600\n",
      "Gradient: [   3.2351   -6.8124  -57.5113 -187.859   100.7816]\n",
      "Weights: [-4.7988  0.8207 -1.3544  0.1838  0.1319]\n",
      "MSE loss: 90.6462\n",
      "Iteration: 199700\n",
      "Gradient: [ -1.4078 -12.2225 -11.2188  -6.3937 130.9638]\n",
      "Weights: [-4.8009  0.8104 -1.3511  0.1844  0.132 ]\n",
      "MSE loss: 90.5674\n",
      "Iteration: 199800\n",
      "Gradient: [-14.2671   8.2884  -9.344  -20.4592   5.1867]\n",
      "Weights: [-4.7975  0.809  -1.3498  0.1845  0.1317]\n",
      "MSE loss: 90.5513\n",
      "Iteration: 199900\n",
      "Gradient: [  7.9721   7.9833 -14.5067 105.9075 184.387 ]\n",
      "Weights: [-4.8043  0.8171 -1.3496  0.1846  0.1316]\n",
      "MSE loss: 90.5347\n"
     ]
    }
   ],
   "source": [
    "# Обучение на случайных батчак, по 10% датасета.\n",
    "# lr - индивидуальный для каждого из параметров модели.\n",
    "# Фильтрация градиента (beta=0.8).\n",
    "weights_4, losses_4, iter_final_4, fit_time_4 = model_fit(X_train, y_train,\n",
    "                                                          learning_rate=[1e-3, 1e-4, 1e-5, 1e-6, 1e-7],\n",
    "                                                          tolerance=(0.2**2 * N_points),\n",
    "                                                          beta=0.8,\n",
    "                                                          batch_ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b09af430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Gradient: [ 1661.8269  1940.6748  3944.2258  7347.65   16442.8078]\n",
      "Weights: [-0.0006 -0.0001 -0.     -0.     -0.    ]\n",
      "MSE loss: 39319.7065\n",
      "Iteration: 100\n",
      "Gradient: [  -16.8079    64.7484   -16.3544  -708.4154 -1386.9012]\n",
      "Weights: [-5.0835 -0.4052  0.1263  0.0777  0.0321]\n",
      "MSE loss: 680.4919\n",
      "Iteration: 200\n",
      "Gradient: [  -8.9434   28.0478  102.5649 -233.1932 -188.3906]\n",
      "Weights: [-4.8072 -0.7005  0.1002  0.092   0.0416]\n",
      "MSE loss: 451.2732\n",
      "Iteration: 300\n",
      "Gradient: [    3.6288   -26.344     31.7973  -280.5392 -1045.9457]\n",
      "Weights: [-4.6272 -0.9039  0.0814  0.1036  0.048 ]\n",
      "MSE loss: 337.5957\n",
      "Iteration: 400\n",
      "Gradient: [ -12.5757    3.2383   29.8561   31.0318 -457.7536]\n",
      "Weights: [-4.4762 -1.0403  0.0578  0.1109  0.0528]\n",
      "MSE loss: 276.9013\n",
      "Iteration: 500\n",
      "Gradient: [   5.8685   -5.4504   50.5289    0.7166 -282.038 ]\n",
      "Weights: [-4.3788 -1.1381  0.0432  0.1159  0.0568]\n",
      "MSE loss: 243.4831\n",
      "Iteration: 600\n",
      "Gradient: [  17.6008   29.8737   -1.1829  -51.5116 -183.6645]\n",
      "Weights: [-4.2858 -1.1994  0.0274  0.1192  0.0598]\n",
      "MSE loss: 226.5777\n",
      "Iteration: 700\n",
      "Gradient: [-4.550000e-02 -1.275650e+01 -9.100900e+00 -2.364033e+02 -2.248479e+02]\n",
      "Weights: [-4.2702 -1.2276  0.0142  0.1198  0.0622]\n",
      "MSE loss: 216.4887\n",
      "Iteration: 800\n",
      "Gradient: [ 1.585000e-01 -4.017000e-01  2.661110e+01  1.768691e+02  2.210413e+02]\n",
      "Weights: [-4.2402e+00 -1.2417e+00 -3.0000e-03  1.2150e-01  6.4000e-02]\n",
      "MSE loss: 209.9008\n",
      "Iteration: 900\n",
      "Gradient: [-10.5215  10.427  -19.1212 165.5322 -33.5038]\n",
      "Weights: [-4.2333 -1.2443 -0.0122  0.1219  0.0653]\n",
      "MSE loss: 206.1601\n",
      "Iteration: 1000\n",
      "Gradient: [-1.307020e+01  4.037000e-01 -5.032680e+01 -6.445610e+01 -4.062126e+02]\n",
      "Weights: [-4.2117 -1.2658 -0.0209  0.1222  0.0674]\n",
      "MSE loss: 201.7108\n",
      "Iteration: 1100\n",
      "Gradient: [ 16.5296  10.9795 -24.0904  31.3585 133.843 ]\n",
      "Weights: [-4.2009 -1.2583 -0.0302  0.1219  0.0686]\n",
      "MSE loss: 199.37\n",
      "Iteration: 1200\n",
      "Gradient: [ 12.1832 -10.793   79.976   57.2823 281.6761]\n",
      "Weights: [-4.2009 -1.2551 -0.0386  0.122   0.0698]\n",
      "MSE loss: 197.3926\n",
      "Iteration: 1300\n",
      "Gradient: [  2.8978   5.1861  19.8681 -28.4798  90.9545]\n",
      "Weights: [-4.1782 -1.2631 -0.0442  0.1206  0.0706]\n",
      "MSE loss: 195.5715\n",
      "Iteration: 1400\n",
      "Gradient: [ -5.411   -4.4228  42.1201 -28.6471 135.5172]\n",
      "Weights: [-4.1826 -1.2447 -0.0505  0.1194  0.0713]\n",
      "MSE loss: 194.3011\n",
      "Iteration: 1500\n",
      "Gradient: [  -1.1557    7.2926   73.9149  -80.6837 -246.5867]\n",
      "Weights: [-4.1928 -1.2255 -0.0623  0.1189  0.0721]\n",
      "MSE loss: 191.9365\n",
      "Iteration: 1600\n",
      "Gradient: [-1.759800e+00 -2.480000e-02  2.169450e+01  1.169570e+01  2.910234e+02]\n",
      "Weights: [-4.2024 -1.2087 -0.0681  0.1187  0.0725]\n",
      "MSE loss: 190.976\n",
      "Iteration: 1700\n",
      "Gradient: [-4.430000e-02 -2.885390e+01  3.217620e+01 -1.505045e+02 -3.805628e+02]\n",
      "Weights: [-4.2023 -1.2083 -0.0794  0.1185  0.0735]\n",
      "MSE loss: 188.9818\n",
      "Iteration: 1800\n",
      "Gradient: [  2.2006  -4.5768  13.4712 -37.2125 -52.2045]\n",
      "Weights: [-4.1889 -1.184  -0.0862  0.1173  0.0739]\n",
      "MSE loss: 188.0436\n",
      "Iteration: 1900\n",
      "Gradient: [ -2.5325  26.4266  59.1461 -30.1607 160.1496]\n",
      "Weights: [-4.2043 -1.1706 -0.0965  0.1165  0.075 ]\n",
      "MSE loss: 185.2792\n",
      "Iteration: 2000\n",
      "Gradient: [   3.7233   21.5253   21.7525  -79.2614 -486.391 ]\n",
      "Weights: [-4.2108 -1.159  -0.1014  0.1155  0.0759]\n",
      "MSE loss: 183.9865\n",
      "Iteration: 2100\n",
      "Gradient: [  -0.5693  -23.4596   31.2589  109.7047 -237.6234]\n",
      "Weights: [-4.2221 -1.1453 -0.1126  0.1151  0.0765]\n",
      "MSE loss: 182.2921\n",
      "Iteration: 2200\n",
      "Gradient: [-20.678    8.7083  68.193   -6.7559 368.9361]\n",
      "Weights: [-4.2244 -1.1303 -0.1173  0.1138  0.0774]\n",
      "MSE loss: 180.7138\n",
      "Iteration: 2300\n",
      "Gradient: [   2.1167   14.9689   35.2695   90.3716 -155.211 ]\n",
      "Weights: [-4.1975 -1.1238 -0.1244  0.1134  0.0779]\n",
      "MSE loss: 180.6212\n",
      "Iteration: 2400\n",
      "Gradient: [  5.0216 -11.5303  27.0841 -48.9973 -79.3509]\n",
      "Weights: [-4.2037 -1.1211 -0.1336  0.1134  0.0789]\n",
      "MSE loss: 178.1345\n",
      "Iteration: 2500\n",
      "Gradient: [  0.6843   3.7801 -16.2433  48.2769  31.1112]\n",
      "Weights: [-4.228  -1.0987 -0.1367  0.1117  0.0796]\n",
      "MSE loss: 177.0247\n",
      "Iteration: 2600\n",
      "Gradient: [-12.7484  -9.6438 -18.673  -29.0609 -71.4782]\n",
      "Weights: [-4.2355 -1.0766 -0.1477  0.1111  0.08  ]\n",
      "MSE loss: 175.1036\n",
      "Iteration: 2700\n",
      "Gradient: [  13.905   -14.4747  -18.6966   82.135  -299.7764]\n",
      "Weights: [-4.2154 -1.08   -0.1595  0.1117  0.0811]\n",
      "MSE loss: 173.4983\n",
      "Iteration: 2800\n",
      "Gradient: [   4.8569    5.1758   17.078   118.4098 -408.6446]\n",
      "Weights: [-4.2351 -1.0665 -0.1647  0.1116  0.0816]\n",
      "MSE loss: 172.3513\n",
      "Iteration: 2900\n",
      "Gradient: [  8.4968  -9.0058  31.4968 -22.694  316.391 ]\n",
      "Weights: [-4.2334 -1.0492 -0.1727  0.1102  0.0825]\n",
      "MSE loss: 170.8349\n",
      "Iteration: 3000\n",
      "Gradient: [ 14.8968 -13.5255 -18.3292 140.3229 282.1737]\n",
      "Weights: [-4.2241 -1.029  -0.1819  0.1094  0.0828]\n",
      "MSE loss: 169.9959\n",
      "Iteration: 3100\n",
      "Gradient: [  -6.2336    6.9935   60.2499  143.4503 -164.1624]\n",
      "Weights: [-4.2342 -1.0097 -0.1913  0.1084  0.0838]\n",
      "MSE loss: 168.1887\n",
      "Iteration: 3200\n",
      "Gradient: [  -0.8416    1.5647  -29.5586 -151.6258  -39.2073]\n",
      "Weights: [-4.2543 -0.9932 -0.1996  0.1068  0.0845]\n",
      "MSE loss: 166.0885\n",
      "Iteration: 3300\n",
      "Gradient: [  18.5305  -10.4249   58.4531  101.1633 -174.462 ]\n",
      "Weights: [-4.2619 -0.9859 -0.2028  0.1068  0.085 ]\n",
      "MSE loss: 165.2828\n",
      "Iteration: 3400\n",
      "Gradient: [  4.013   -5.5664 -41.3723 134.333  357.1537]\n",
      "Weights: [-4.2667 -0.9551 -0.2164  0.1058  0.0855]\n",
      "MSE loss: 163.3549\n",
      "Iteration: 3500\n",
      "Gradient: [  -8.484     7.9148  -22.5273   16.2229 -234.6105]\n",
      "Weights: [-4.2785 -0.9451 -0.2231  0.1055  0.0863]\n",
      "MSE loss: 162.0781\n",
      "Iteration: 3600\n",
      "Gradient: [  13.7376   18.2193   -8.0316  169.3144 -154.1593]\n",
      "Weights: [-4.25   -0.9474 -0.2286  0.1056  0.0871]\n",
      "MSE loss: 161.9683\n",
      "Iteration: 3700\n",
      "Gradient: [ 2.51600e-01  1.99490e+00  4.41469e+01  8.23880e+00 -3.85377e+02]\n",
      "Weights: [-4.2706 -0.9385 -0.2336  0.1052  0.0876]\n",
      "MSE loss: 160.3771\n",
      "Iteration: 3800\n",
      "Gradient: [  16.7988   10.0724   27.793   -51.8602 -189.9077]\n",
      "Weights: [-4.2635 -0.9238 -0.2445  0.1047  0.0883]\n",
      "MSE loss: 158.9451\n",
      "Iteration: 3900\n",
      "Gradient: [ 1.54197e+01  1.44556e+01 -1.77700e-01  1.00220e+02 -3.30829e+02]\n",
      "Weights: [-4.2737 -0.9018 -0.2518  0.1032  0.0888]\n",
      "MSE loss: 157.6067\n",
      "Iteration: 4000\n",
      "Gradient: [ -8.8111 -10.4362   7.7176 -11.9485  44.8211]\n",
      "Weights: [-4.2895 -0.8902 -0.2561  0.1026  0.0895]\n",
      "MSE loss: 156.5473\n",
      "Iteration: 4100\n",
      "Gradient: [  -5.4021    4.499    45.9172  -71.2491 -402.6662]\n",
      "Weights: [-4.2894 -0.8789 -0.266   0.1036  0.0896]\n",
      "MSE loss: 155.6259\n",
      "Iteration: 4200\n",
      "Gradient: [  12.0774  -11.2257  -12.65    -10.983  -151.5353]\n",
      "Weights: [-4.2723 -0.8758 -0.2704  0.1023  0.0902]\n",
      "MSE loss: 155.121\n",
      "Iteration: 4300\n",
      "Gradient: [  5.9825   2.9972  51.6138 103.4452 -62.9152]\n",
      "Weights: [-4.2963 -0.8607 -0.2731  0.102   0.0906]\n",
      "MSE loss: 154.1054\n",
      "Iteration: 4400\n",
      "Gradient: [-16.1864  20.7523  13.0851   8.2495 -34.1095]\n",
      "Weights: [-4.2976 -0.8441 -0.2782  0.1003  0.0911]\n",
      "MSE loss: 153.1806\n",
      "Iteration: 4500\n",
      "Gradient: [  -8.6168   -1.4401   15.1505  -10.768  -458.8065]\n",
      "Weights: [-4.303  -0.8317 -0.2806  0.0983  0.0916]\n",
      "MSE loss: 152.4351\n",
      "Iteration: 4600\n",
      "Gradient: [ 4.831  -8.2407 15.6382 82.4525 35.8642]\n",
      "Weights: [-4.2901 -0.8445 -0.2836  0.0991  0.0924]\n",
      "MSE loss: 152.1482\n",
      "Iteration: 4700\n",
      "Gradient: [  5.204   -8.3987  14.3417 -26.2535 164.6392]\n",
      "Weights: [-4.2971 -0.8455 -0.2919  0.0992  0.0933]\n",
      "MSE loss: 151.1997\n",
      "Iteration: 4800\n",
      "Gradient: [  -1.2101  -18.7368  -33.7      -6.1353 -171.6084]\n",
      "Weights: [-4.318  -0.8245 -0.3     0.0988  0.0936]\n",
      "MSE loss: 150.6188\n",
      "Iteration: 4900\n",
      "Gradient: [   0.6945   -7.0808  -17.7841   53.3822 -241.3011]\n",
      "Weights: [-4.3124 -0.8072 -0.3052  0.0983  0.0939]\n",
      "MSE loss: 148.9583\n",
      "Iteration: 5000\n",
      "Gradient: [ -11.4316  -10.4819  -38.0893  -98.9045 -443.5185]\n",
      "Weights: [-4.333  -0.7991 -0.3083  0.0973  0.0945]\n",
      "MSE loss: 148.8395\n",
      "Iteration: 5100\n",
      "Gradient: [   1.4131   21.7881   21.3222  130.7646 -151.5215]\n",
      "Weights: [-4.3157 -0.7845 -0.3135  0.0969  0.0947]\n",
      "MSE loss: 147.591\n",
      "Iteration: 5200\n",
      "Gradient: [ -0.1324   0.1791  -7.2805 -27.9671 -87.6899]\n",
      "Weights: [-4.314  -0.773  -0.317   0.0952  0.0949]\n",
      "MSE loss: 146.989\n",
      "Iteration: 5300\n",
      "Gradient: [   3.8277  -14.8864  -30.292    23.6691 -198.4966]\n",
      "Weights: [-4.3209 -0.7616 -0.322   0.0949  0.0953]\n",
      "MSE loss: 146.185\n",
      "Iteration: 5400\n",
      "Gradient: [   2.8423    5.5792   11.0125  -73.3481 -378.9504]\n",
      "Weights: [-4.337  -0.7454 -0.3285  0.0945  0.0959]\n",
      "MSE loss: 145.073\n",
      "Iteration: 5500\n",
      "Gradient: [ 10.6036 -32.4677  16.593   49.3252 -92.599 ]\n",
      "Weights: [-4.3446 -0.7397 -0.3334  0.0948  0.0962]\n",
      "MSE loss: 144.5059\n",
      "Iteration: 5600\n",
      "Gradient: [ -2.1398  32.5766  36.1311 -87.8355 -23.5838]\n",
      "Weights: [-4.3368 -0.7197 -0.3396  0.094   0.0966]\n",
      "MSE loss: 144.0782\n",
      "Iteration: 5700\n",
      "Gradient: [  10.1137   -5.5918  -14.3941  -32.6643 -443.2572]\n",
      "Weights: [-4.3765 -0.6923 -0.3485  0.0929  0.0971]\n",
      "MSE loss: 142.4983\n",
      "Iteration: 5800\n",
      "Gradient: [   5.5823    4.022    36.3964 -103.7958  122.6771]\n",
      "Weights: [-4.3721 -0.6854 -0.3546  0.0924  0.0978]\n",
      "MSE loss: 141.4413\n",
      "Iteration: 5900\n",
      "Gradient: [  14.1758   -5.1792   22.3342   20.1229 -286.0844]\n",
      "Weights: [-4.3608 -0.6698 -0.3645  0.0927  0.0979]\n",
      "MSE loss: 140.5534\n",
      "Iteration: 6000\n",
      "Gradient: [ 14.7506  -5.6561  12.7482 -71.8855 -22.026 ]\n",
      "Weights: [-4.3817 -0.654  -0.3693  0.0915  0.0985]\n",
      "MSE loss: 139.6785\n",
      "Iteration: 6100\n",
      "Gradient: [  -4.4044   -2.3269   23.7891    2.0746 -202.2701]\n",
      "Weights: [-4.3776 -0.638  -0.3767  0.0908  0.0992]\n",
      "MSE loss: 138.6955\n",
      "Iteration: 6200\n",
      "Gradient: [ -2.3308   2.0326 -14.583   29.2763 -33.8679]\n",
      "Weights: [-4.3808 -0.6441 -0.3802  0.0907  0.0997]\n",
      "MSE loss: 138.233\n",
      "Iteration: 6300\n",
      "Gradient: [-6.7269 -6.6549 24.7021 -0.1696 70.97  ]\n",
      "Weights: [-4.3692 -0.6535 -0.3826  0.091   0.1002]\n",
      "MSE loss: 137.9012\n",
      "Iteration: 6400\n",
      "Gradient: [  -7.5722    6.9058   -9.5185   69.125  -241.0937]\n",
      "Weights: [-4.3624 -0.6454 -0.3877  0.0907  0.1009]\n",
      "MSE loss: 137.1032\n",
      "Iteration: 6500\n",
      "Gradient: [ -1.7347 -19.9266 -19.1603  13.7629  94.6329]\n",
      "Weights: [-4.3626 -0.6515 -0.3938  0.0918  0.1012]\n",
      "MSE loss: 136.9345\n",
      "Iteration: 6600\n",
      "Gradient: [  -7.348    10.0734   40.1884  124.8948 -227.0737]\n",
      "Weights: [-4.3723 -0.6266 -0.4017  0.0908  0.1017]\n",
      "MSE loss: 135.5516\n",
      "Iteration: 6700\n",
      "Gradient: [   3.5895  -12.3008  -30.6733   16.0705 -154.4707]\n",
      "Weights: [-4.3641 -0.62   -0.4045  0.0897  0.1022]\n",
      "MSE loss: 135.0567\n",
      "Iteration: 6800\n",
      "Gradient: [  -5.9156  -10.9532  -15.9226 -145.1344 -342.6655]\n",
      "Weights: [-4.3935 -0.5947 -0.4121  0.0894  0.1026]\n",
      "MSE loss: 134.0194\n",
      "Iteration: 6900\n",
      "Gradient: [  -4.0348   18.188    29.5783   40.0227 -156.9535]\n",
      "Weights: [-4.3985 -0.5798 -0.4226  0.0896  0.1029]\n",
      "MSE loss: 133.2321\n",
      "Iteration: 7000\n",
      "Gradient: [   3.9455    2.4459   30.3167  -55.4564 -158.7318]\n",
      "Weights: [-4.3912 -0.5636 -0.4275  0.089   0.1033]\n",
      "MSE loss: 132.3715\n",
      "Iteration: 7100\n",
      "Gradient: [  -4.1484   18.2181  -17.6232 -123.6186   21.4178]\n",
      "Weights: [-4.3882 -0.5586 -0.4307  0.0881  0.1038]\n",
      "MSE loss: 131.9256\n",
      "Iteration: 7200\n",
      "Gradient: [  -2.9027    8.708     9.4615   66.5144 -164.6709]\n",
      "Weights: [-4.3913 -0.5476 -0.4382  0.0883  0.1042]\n",
      "MSE loss: 131.1408\n",
      "Iteration: 7300\n",
      "Gradient: [ 14.9135 -16.1874   5.753  200.9771 -90.7139]\n",
      "Weights: [-4.3926 -0.5478 -0.4381  0.0878  0.1045]\n",
      "MSE loss: 131.0221\n",
      "Iteration: 7400\n",
      "Gradient: [  -8.7711  -18.088    12.353   -37.914  -381.8833]\n",
      "Weights: [-4.3969 -0.5445 -0.4431  0.0878  0.1047]\n",
      "MSE loss: 130.4504\n",
      "Iteration: 7500\n",
      "Gradient: [  9.742   -3.0302   0.7522 -18.0901 194.3206]\n",
      "Weights: [-4.4154 -0.5264 -0.449   0.0871  0.1053]\n",
      "MSE loss: 129.629\n",
      "Iteration: 7600\n",
      "Gradient: [  -0.3515   -0.2763  -33.1218  -83.6115 -164.3095]\n",
      "Weights: [-4.4133 -0.5148 -0.457   0.0868  0.1055]\n",
      "MSE loss: 129.1761\n",
      "Iteration: 7700\n",
      "Gradient: [ -4.5182 -11.7986  10.5189 -77.2669 297.2142]\n",
      "Weights: [-4.4224 -0.5054 -0.4577  0.0864  0.106 ]\n",
      "MSE loss: 128.5303\n",
      "Iteration: 7800\n",
      "Gradient: [ 17.0666   5.2863 -16.182   18.2883 134.0649]\n",
      "Weights: [-4.4112 -0.4947 -0.4688  0.0865  0.1066]\n",
      "MSE loss: 127.4562\n",
      "Iteration: 7900\n",
      "Gradient: [ -0.979    3.9462  -7.7599 167.706  177.8563]\n",
      "Weights: [-4.4189 -0.4843 -0.4738  0.0863  0.1072]\n",
      "MSE loss: 126.8663\n",
      "Iteration: 8000\n",
      "Gradient: [ -11.4014  -15.5457    5.1062  -30.5597 -593.5061]\n",
      "Weights: [-4.4433 -0.4796 -0.4758  0.0853  0.1074]\n",
      "MSE loss: 127.452\n",
      "Iteration: 8100\n",
      "Gradient: [  3.5803  -5.3389  75.4249 -44.6839 273.4122]\n",
      "Weights: [-4.4265 -0.4687 -0.4795  0.0848  0.1078]\n",
      "MSE loss: 126.0085\n",
      "Iteration: 8200\n",
      "Gradient: [ 2.640000e-02  1.237600e+01  2.695460e+01 -4.627700e+00  2.345811e+02]\n",
      "Weights: [-4.4296 -0.4572 -0.4865  0.0845  0.1081]\n",
      "MSE loss: 125.3205\n",
      "Iteration: 8300\n",
      "Gradient: [-14.3758   6.0733 -10.8252 -88.9845 285.9731]\n",
      "Weights: [-4.4274 -0.4566 -0.4887  0.0839  0.1084]\n",
      "MSE loss: 125.1429\n",
      "Iteration: 8400\n",
      "Gradient: [  13.075     2.5451  -12.7329   70.6667 -205.0859]\n",
      "Weights: [-4.4257 -0.444  -0.4949  0.0829  0.1089]\n",
      "MSE loss: 124.3837\n",
      "Iteration: 8500\n",
      "Gradient: [  13.1138   15.3464   50.2847  112.9876 -161.8187]\n",
      "Weights: [-4.438  -0.4394 -0.4962  0.0825  0.1096]\n",
      "MSE loss: 123.9322\n",
      "Iteration: 8600\n",
      "Gradient: [ -11.5845  -17.8386  -16.1136   11.2188 -164.0012]\n",
      "Weights: [-4.4257 -0.4452 -0.4988  0.0818  0.1098]\n",
      "MSE loss: 123.994\n",
      "Iteration: 8700\n",
      "Gradient: [ 1.62000e-02 -3.53390e+00  7.63110e+00 -5.03103e+01 -2.60776e+01]\n",
      "Weights: [-4.4399 -0.4348 -0.5039  0.0829  0.11  ]\n",
      "MSE loss: 123.501\n",
      "Iteration: 8800\n",
      "Gradient: [  -2.8662  -14.9052   -1.0981  -47.0425 -183.9141]\n",
      "Weights: [-4.4584 -0.4055 -0.5087  0.082   0.1101]\n",
      "MSE loss: 122.7188\n",
      "Iteration: 8900\n",
      "Gradient: [   9.0284   11.1367  -10.9443  160.1919 -436.9675]\n",
      "Weights: [-4.4368 -0.4047 -0.514   0.0819  0.1105]\n",
      "MSE loss: 122.3092\n",
      "Iteration: 9000\n",
      "Gradient: [   7.6245   -8.9436   38.6979 -132.8669   21.3044]\n",
      "Weights: [-4.4314 -0.4113 -0.5169  0.0817  0.1109]\n",
      "MSE loss: 122.0671\n",
      "Iteration: 9100\n",
      "Gradient: [-14.2078  -2.5941  74.9132 -24.3584 150.4166]\n",
      "Weights: [-4.4315 -0.3978 -0.5232  0.081   0.1115]\n",
      "MSE loss: 121.347\n",
      "Iteration: 9200\n",
      "Gradient: [  -3.8664    3.9882   21.1005 -186.1605  400.6901]\n",
      "Weights: [-4.4596 -0.3886 -0.5255  0.081   0.1118]\n",
      "MSE loss: 121.0615\n",
      "Iteration: 9300\n",
      "Gradient: [  4.9904   4.9004  41.5314   8.9908 259.1887]\n",
      "Weights: [-4.45   -0.3725 -0.5275  0.0805  0.1118]\n",
      "MSE loss: 121.2173\n",
      "Iteration: 9400\n",
      "Gradient: [  -5.5494   20.1081   29.831   -19.3309 -185.3761]\n",
      "Weights: [-4.4699 -0.3495 -0.5356  0.079   0.1121]\n",
      "MSE loss: 119.7966\n",
      "Iteration: 9500\n",
      "Gradient: [-1.1924 15.698  12.4733 25.4503 61.8224]\n",
      "Weights: [-4.4666 -0.3481 -0.539   0.0781  0.1127]\n",
      "MSE loss: 119.3262\n",
      "Iteration: 9600\n",
      "Gradient: [  -2.2889    4.7466   36.3974   13.4731 -145.7205]\n",
      "Weights: [-4.4641 -0.3502 -0.5446  0.079   0.1132]\n",
      "MSE loss: 118.9525\n",
      "Iteration: 9700\n",
      "Gradient: [  -2.203    -2.0736   24.6462 -149.795   -56.5768]\n",
      "Weights: [-4.4576 -0.3392 -0.5503  0.0784  0.1135]\n",
      "MSE loss: 118.4433\n",
      "Iteration: 9800\n",
      "Gradient: [  5.5862 -12.4709 -22.8393  19.2552  15.2057]\n",
      "Weights: [-4.4564 -0.3393 -0.5539  0.0787  0.114 ]\n",
      "MSE loss: 118.0929\n",
      "Iteration: 9900\n",
      "Gradient: [  7.283   28.3662  24.9596 -79.0429  31.8055]\n",
      "Weights: [-4.4539 -0.3327 -0.5587  0.0785  0.1144]\n",
      "MSE loss: 117.7031\n",
      "Iteration: 10000\n",
      "Gradient: [  4.098   -9.2644   3.9662 -26.1878  -3.3693]\n",
      "Weights: [-4.4727 -0.3222 -0.5613  0.0779  0.1146]\n",
      "MSE loss: 117.3425\n",
      "Iteration: 10100\n",
      "Gradient: [  1.1902  25.8731  -8.1059 -14.903  269.4923]\n",
      "Weights: [-4.4742 -0.3023 -0.563   0.0759  0.1148]\n",
      "MSE loss: 116.9664\n",
      "Iteration: 10200\n",
      "Gradient: [ 5.7763 -0.2078 28.3671 53.5645 58.6359]\n",
      "Weights: [-4.4757 -0.2888 -0.5697  0.0758  0.1154]\n",
      "MSE loss: 116.6475\n",
      "Iteration: 10300\n",
      "Gradient: [  5.9284 -10.9059  52.0269  21.3312 -67.7922]\n",
      "Weights: [-4.4866 -0.2794 -0.5756  0.075   0.1157]\n",
      "MSE loss: 115.7342\n",
      "Iteration: 10400\n",
      "Gradient: [ -19.3502  -21.1469  -38.9246  -51.2588 -157.2683]\n",
      "Weights: [-4.4756 -0.2891 -0.5815  0.0751  0.1163]\n",
      "MSE loss: 115.9458\n",
      "Iteration: 10500\n",
      "Gradient: [   1.2721  -20.193     6.9007  -66.4666 -165.7648]\n",
      "Weights: [-4.4899 -0.2756 -0.5839  0.0752  0.1166]\n",
      "MSE loss: 115.0611\n",
      "Iteration: 10600\n",
      "Gradient: [ 10.2288  19.0155 -10.1281  69.3138 216.2972]\n",
      "Weights: [-4.4944 -0.2606 -0.587   0.075   0.1168]\n",
      "MSE loss: 114.6923\n",
      "Iteration: 10700\n",
      "Gradient: [ -3.4395 -10.8745  18.3924  99.7279  28.8919]\n",
      "Weights: [-4.4987 -0.2527 -0.5915  0.0745  0.117 ]\n",
      "MSE loss: 114.2961\n",
      "Iteration: 10800\n",
      "Gradient: [-1.9309 15.3646 53.9259 -0.7429  7.8175]\n",
      "Weights: [-4.5071 -0.2373 -0.5945  0.0739  0.1172]\n",
      "MSE loss: 113.9794\n",
      "Iteration: 10900\n",
      "Gradient: [ -2.4999  -7.3018  -5.7935 104.8518 119.7747]\n",
      "Weights: [-4.5088 -0.2456 -0.5978  0.0742  0.1177]\n",
      "MSE loss: 113.9256\n",
      "Iteration: 11000\n",
      "Gradient: [  0.2591  -0.5355 -13.8268 -19.915   45.9347]\n",
      "Weights: [-4.4961 -0.2338 -0.6014  0.0732  0.118 ]\n",
      "MSE loss: 113.3687\n",
      "Iteration: 11100\n",
      "Gradient: [   2.9923   -8.8238  -26.9699   15.572  -185.8646]\n",
      "Weights: [-4.4905 -0.2444 -0.6008  0.0731  0.1182]\n",
      "MSE loss: 113.3799\n",
      "Iteration: 11200\n",
      "Gradient: [   1.31     -7.2472   23.1438   41.7738 -223.5067]\n",
      "Weights: [-4.5122 -0.2118 -0.6078  0.0716  0.1185]\n",
      "MSE loss: 112.6448\n",
      "Iteration: 11300\n",
      "Gradient: [   0.5706    8.7951   24.9129    9.7228 -343.5491]\n",
      "Weights: [-4.5158 -0.2017 -0.616   0.0713  0.1191]\n",
      "MSE loss: 112.0651\n",
      "Iteration: 11400\n",
      "Gradient: [ -0.9801  -6.2619  30.36    -4.3172 101.1522]\n",
      "Weights: [-4.5133 -0.2014 -0.62    0.0716  0.1196]\n",
      "MSE loss: 111.6724\n",
      "Iteration: 11500\n",
      "Gradient: [ 14.523   -0.7168  -0.5051  52.7361 369.0011]\n",
      "Weights: [-4.5185 -0.1844 -0.6245  0.0711  0.12  ]\n",
      "MSE loss: 111.297\n",
      "Iteration: 11600\n",
      "Gradient: [  9.2513  -2.0608  47.8536  30.0941 -99.56  ]\n",
      "Weights: [-4.5279 -0.1799 -0.6289  0.0707  0.1204]\n",
      "MSE loss: 110.8767\n",
      "Iteration: 11700\n",
      "Gradient: [ 8.553800e+00 -1.174000e-01  5.872000e-01  9.683420e+01  2.087185e+02]\n",
      "Weights: [-4.5301 -0.1633 -0.632   0.0699  0.1206]\n",
      "MSE loss: 110.6497\n",
      "Iteration: 11800\n",
      "Gradient: [  2.4766   4.8885 -16.8493 -80.3601 376.5398]\n",
      "Weights: [-4.5041 -0.1767 -0.6349  0.0698  0.121 ]\n",
      "MSE loss: 110.5488\n",
      "Iteration: 11900\n",
      "Gradient: [   1.2187   11.8868   -6.7737   29.3344 -245.2903]\n",
      "Weights: [-4.5258 -0.1531 -0.6381  0.069   0.121 ]\n",
      "MSE loss: 110.1596\n",
      "Iteration: 12000\n",
      "Gradient: [   3.425    -3.6375  -25.1143  -99.7003 -160.9276]\n",
      "Weights: [-4.5374 -0.1538 -0.639   0.0685  0.1211]\n",
      "MSE loss: 110.1982\n",
      "Iteration: 12100\n",
      "Gradient: [ -14.685     4.9378  -37.9303  -25.7587 -235.1279]\n",
      "Weights: [-4.5462 -0.1501 -0.6421  0.0693  0.1213]\n",
      "MSE loss: 110.0751\n",
      "Iteration: 12200\n",
      "Gradient: [ -3.0838  20.3289  34.7644 -63.5013 164.6714]\n",
      "Weights: [-4.5291 -0.1489 -0.6484  0.0695  0.1217]\n",
      "MSE loss: 109.4297\n",
      "Iteration: 12300\n",
      "Gradient: [   2.7819    1.125   -11.2148   23.398  -326.2387]\n",
      "Weights: [-4.5191 -0.1467 -0.6522  0.0693  0.1221]\n",
      "MSE loss: 109.2045\n",
      "Iteration: 12400\n",
      "Gradient: [ -7.1867  11.5371  30.095   31.9063 -56.1684]\n",
      "Weights: [-4.5369 -0.1395 -0.6517  0.0694  0.1224]\n",
      "MSE loss: 109.0202\n",
      "Iteration: 12500\n",
      "Gradient: [ 10.9968  12.5785  -6.9955 175.0221 250.1358]\n",
      "Weights: [-4.5298 -0.1391 -0.6582  0.069   0.1231]\n",
      "MSE loss: 108.4904\n",
      "Iteration: 12600\n",
      "Gradient: [  10.6962  -10.651     7.8496   32.4442 -249.5781]\n",
      "Weights: [-4.5403 -0.124  -0.6605  0.0674  0.1232]\n",
      "MSE loss: 108.2397\n",
      "Iteration: 12700\n",
      "Gradient: [  -3.0223    3.7465   50.1422  128.0385 -442.2627]\n",
      "Weights: [-4.5386 -0.1166 -0.6621  0.0661  0.1235]\n",
      "MSE loss: 107.9647\n",
      "Iteration: 12800\n",
      "Gradient: [  -1.4541   -2.5721   43.5523   63.5453 -140.4798]\n",
      "Weights: [-4.5312 -0.1078 -0.6681  0.0663  0.124 ]\n",
      "MSE loss: 107.7558\n",
      "Iteration: 12900\n",
      "Gradient: [-5.2042 19.0855 -3.4429 77.5601 52.3131]\n",
      "Weights: [-4.5518 -0.0982 -0.6676  0.0653  0.1241]\n",
      "MSE loss: 107.4678\n",
      "Iteration: 13000\n",
      "Gradient: [ -1.4857 -10.8995 -48.4096 -50.639  -49.5182]\n",
      "Weights: [-4.5523 -0.0979 -0.6694  0.0641  0.1244]\n",
      "MSE loss: 107.4223\n",
      "Iteration: 13100\n",
      "Gradient: [   9.0564    2.7731   -5.6668   -2.2058 -121.4326]\n",
      "Weights: [-4.5527 -0.0861 -0.6727  0.0645  0.1247]\n",
      "MSE loss: 107.1865\n",
      "Iteration: 13200\n",
      "Gradient: [  1.1033  14.9113  20.9534 -83.9838 -19.1215]\n",
      "Weights: [-4.5429 -0.09   -0.675   0.0639  0.125 ]\n",
      "MSE loss: 106.8624\n",
      "Iteration: 13300\n",
      "Gradient: [  -7.4843   -2.1553  -37.1953  -31.4415 -148.8183]\n",
      "Weights: [-4.5529 -0.0941 -0.6772  0.0636  0.1255]\n",
      "MSE loss: 106.835\n",
      "Iteration: 13400\n",
      "Gradient: [   1.0129    9.5124   46.7909   46.0702 -243.9162]\n",
      "Weights: [-4.543  -0.0847 -0.6793  0.0634  0.1257]\n",
      "MSE loss: 106.5392\n",
      "Iteration: 13500\n",
      "Gradient: [ -2.7887   3.2593 -13.9274 -21.0643 147.1423]\n",
      "Weights: [-4.5625 -0.0709 -0.6848  0.0633  0.1261]\n",
      "MSE loss: 106.0941\n",
      "Iteration: 13600\n",
      "Gradient: [ -4.7483 -23.6    -68.786  -10.4892 174.4637]\n",
      "Weights: [-4.5537 -0.0744 -0.6899  0.0631  0.1263]\n",
      "MSE loss: 106.1066\n",
      "Iteration: 13700\n",
      "Gradient: [  1.2803  -4.333  -29.083  -37.1874 -24.8025]\n",
      "Weights: [-4.55   -0.0736 -0.6904  0.0629  0.1267]\n",
      "MSE loss: 105.6831\n",
      "Iteration: 13800\n",
      "Gradient: [  4.1551   4.0167  -7.647   20.221  -68.8231]\n",
      "Weights: [-4.5488 -0.0568 -0.6937  0.0617  0.1269]\n",
      "MSE loss: 105.5001\n",
      "Iteration: 13900\n",
      "Gradient: [ -7.2431   8.0108  23.7469   9.9851 212.4042]\n",
      "Weights: [-4.5491 -0.0596 -0.6955  0.0623  0.1272]\n",
      "MSE loss: 105.3886\n",
      "Iteration: 14000\n",
      "Gradient: [  3.2951  12.3519 -50.5625 -50.0435 -16.7429]\n",
      "Weights: [-4.5413 -0.06   -0.7007  0.0631  0.1272]\n",
      "MSE loss: 105.2133\n",
      "Iteration: 14100\n",
      "Gradient: [-10.8251   8.4073 -14.5192  51.0433 396.6488]\n",
      "Weights: [-4.5526 -0.0549 -0.7036  0.0627  0.1275]\n",
      "MSE loss: 104.9808\n",
      "Iteration: 14200\n",
      "Gradient: [  5.1849  13.6548  44.0375 100.6251 269.1382]\n",
      "Weights: [-4.5562 -0.0298 -0.7089  0.0623  0.1279]\n",
      "MSE loss: 105.109\n",
      "Iteration: 14300\n",
      "Gradient: [ 5.67400e-01 -1.45000e-02  4.03230e+00 -2.16675e+01  8.57030e+00]\n",
      "Weights: [-4.5631 -0.0299 -0.7158  0.0623  0.1282]\n",
      "MSE loss: 104.237\n",
      "Iteration: 14400\n",
      "Gradient: [ -7.1343 -19.1117 -83.433  -84.6864 102.1955]\n",
      "Weights: [-4.5764 -0.0327 -0.7149  0.0622  0.1283]\n",
      "MSE loss: 104.9366\n",
      "Iteration: 14500\n",
      "Gradient: [  -1.5148    9.1779   17.0523 -196.7018  121.9817]\n",
      "Weights: [-4.571  -0.0114 -0.7171  0.0616  0.1281]\n",
      "MSE loss: 103.9895\n",
      "Iteration: 14600\n",
      "Gradient: [   3.1989  -12.477    14.6413   94.8886 -112.8154]\n",
      "Weights: [-4.5846e+00 -4.3000e-03 -7.1730e-01  6.0700e-02  1.2830e-01]\n",
      "MSE loss: 103.8863\n",
      "Iteration: 14700\n",
      "Gradient: [  3.905   16.8107  52.8784  35.5145 491.4077]\n",
      "Weights: [-4.582   0.015  -0.7224  0.0594  0.1288]\n",
      "MSE loss: 103.86\n",
      "Iteration: 14800\n",
      "Gradient: [ -1.3296  -4.7953  18.5389 -49.3797 295.7271]\n",
      "Weights: [-4.5855  0.0163 -0.7285  0.0592  0.1292]\n",
      "MSE loss: 103.1777\n",
      "Iteration: 14900\n",
      "Gradient: [  -0.9458   13.5495  -29.7632 -101.9392 -233.6339]\n",
      "Weights: [-4.6059  0.027  -0.7315  0.0586  0.1296]\n",
      "MSE loss: 103.2591\n",
      "Iteration: 15000\n",
      "Gradient: [  3.5805  -5.7363   3.0404  79.3733 -47.2293]\n",
      "Weights: [-4.578   0.0253 -0.7353  0.0594  0.1298]\n",
      "MSE loss: 102.877\n",
      "Iteration: 15100\n",
      "Gradient: [  9.8465 -21.1759  18.2965 -26.9374 101.7608]\n",
      "Weights: [-4.5931  0.0238 -0.7368  0.0595  0.13  ]\n",
      "MSE loss: 102.7405\n",
      "Iteration: 15200\n",
      "Gradient: [  -1.2737  -11.0886  -28.0833  -23.8828 -233.6721]\n",
      "Weights: [-4.5755  0.0252 -0.7415  0.0584  0.1305]\n",
      "MSE loss: 102.5772\n",
      "Iteration: 15300\n",
      "Gradient: [  -3.9802  -14.8791  -15.3682   81.4588 -238.1235]\n",
      "Weights: [-4.6063  0.0254 -0.7395  0.0591  0.1307]\n",
      "MSE loss: 103.0735\n",
      "Iteration: 15400\n",
      "Gradient: [  -8.0397    8.2166   41.5362 -102.7967   13.7039]\n",
      "Weights: [-4.5825  0.0481 -0.7466  0.058   0.1307]\n",
      "MSE loss: 102.2413\n",
      "Iteration: 15500\n",
      "Gradient: [ -0.5852   8.9836  12.5362  44.2897 -42.1756]\n",
      "Weights: [-4.5771  0.0497 -0.7481  0.0573  0.131 ]\n",
      "MSE loss: 102.281\n",
      "Iteration: 15600\n",
      "Gradient: [   0.9913    6.7982   24.4609   69.1636 -116.6198]\n",
      "Weights: [-4.5878  0.054  -0.7482  0.057   0.1313]\n",
      "MSE loss: 102.0518\n",
      "Iteration: 15700\n",
      "Gradient: [ -4.1768 -10.3609 -50.4382 -16.3257  91.5203]\n",
      "Weights: [-4.5872  0.0465 -0.7512  0.0569  0.1316]\n",
      "MSE loss: 101.7813\n",
      "Iteration: 15800\n",
      "Gradient: [  5.2342  -1.8355   1.2186 -80.5845 -23.3168]\n",
      "Weights: [-4.5859  0.0482 -0.752   0.057   0.1318]\n",
      "MSE loss: 101.5994\n",
      "Iteration: 15900\n",
      "Gradient: [ -0.2447  -0.2406 -17.7368 -73.2088 -58.5075]\n",
      "Weights: [-4.5872  0.0591 -0.7548  0.0566  0.132 ]\n",
      "MSE loss: 101.5539\n",
      "Iteration: 16000\n",
      "Gradient: [  16.5322   -6.459    17.3521 -120.1901 -104.9048]\n",
      "Weights: [-4.5908  0.0584 -0.7555  0.0564  0.1319]\n",
      "MSE loss: 101.4423\n",
      "Iteration: 16100\n",
      "Gradient: [ 6.968  -2.9585 17.964   9.6258 83.3621]\n",
      "Weights: [-4.5973  0.0694 -0.7576  0.0569  0.1317]\n",
      "MSE loss: 101.3862\n",
      "Iteration: 16200\n",
      "Gradient: [  0.3849 -10.1436  32.0011  18.8533 -26.507 ]\n",
      "Weights: [-4.6075  0.0659 -0.7595  0.0577  0.1319]\n",
      "MSE loss: 101.401\n",
      "Iteration: 16300\n",
      "Gradient: [ -0.8294   6.7459  11.9651  76.1381 171.9325]\n",
      "Weights: [-4.6015  0.0742 -0.7635  0.0576  0.1322]\n",
      "MSE loss: 101.035\n",
      "Iteration: 16400\n",
      "Gradient: [ -1.0756   5.989   34.8685  62.9293 -63.4466]\n",
      "Weights: [-4.6007  0.083  -0.7669  0.0579  0.1323]\n",
      "MSE loss: 101.1273\n",
      "Iteration: 16500\n",
      "Gradient: [  5.5648 -12.3264 -12.1457  -2.0703 -75.3203]\n",
      "Weights: [-4.6175  0.0895 -0.771   0.0583  0.1324]\n",
      "MSE loss: 100.8091\n",
      "Iteration: 16600\n",
      "Gradient: [  2.7387  -8.5036  50.843   39.8496 175.9142]\n",
      "Weights: [-4.6068  0.1001 -0.7722  0.0578  0.1323]\n",
      "MSE loss: 100.9642\n",
      "Iteration: 16700\n",
      "Gradient: [  -9.4539   -7.2445    4.248   -87.7504 -286.1833]\n",
      "Weights: [-4.6192  0.0967 -0.775   0.0584  0.1325]\n",
      "MSE loss: 100.6774\n",
      "Iteration: 16800\n",
      "Gradient: [  3.4617   7.494   54.6454  69.0994 -60.9908]\n",
      "Weights: [-4.6233  0.1159 -0.7796  0.0576  0.1327]\n",
      "MSE loss: 100.3765\n",
      "Iteration: 16900\n",
      "Gradient: [   0.8706   20.985   -41.7002   62.6235 -157.7772]\n",
      "Weights: [-4.6091  0.1086 -0.7855  0.0584  0.133 ]\n",
      "MSE loss: 100.1504\n",
      "Iteration: 17000\n",
      "Gradient: [   2.0384   -6.02     50.1866  -44.917  -477.387 ]\n",
      "Weights: [-4.6078  0.1065 -0.7863  0.0593  0.1331]\n",
      "MSE loss: 100.0608\n",
      "Iteration: 17100\n",
      "Gradient: [  -6.2563   -0.3075  -29.7664  -71.1128 -144.8253]\n",
      "Weights: [-4.613   0.1113 -0.7898  0.0587  0.1333]\n",
      "MSE loss: 100.1185\n",
      "Iteration: 17200\n",
      "Gradient: [  -3.8049    6.6314   12.2423  -58.6305 -232.3694]\n",
      "Weights: [-4.6157  0.1192 -0.7935  0.0589  0.1338]\n",
      "MSE loss: 99.6761\n",
      "Iteration: 17300\n",
      "Gradient: [  9.6243  -4.3976 -25.9654  60.2662  15.4107]\n",
      "Weights: [-4.6184  0.1251 -0.796   0.0585  0.1339]\n",
      "MSE loss: 99.5548\n",
      "Iteration: 17400\n",
      "Gradient: [-16.4194 -18.3973  -6.9614  -0.8561 214.2809]\n",
      "Weights: [-4.6378  0.1317 -0.7958  0.0576  0.1341]\n",
      "MSE loss: 99.8922\n",
      "Iteration: 17500\n",
      "Gradient: [ -3.0478   8.2749  36.1833   6.4557 192.1227]\n",
      "Weights: [-4.6167  0.1342 -0.7981  0.0578  0.1343]\n",
      "MSE loss: 99.5654\n",
      "Iteration: 17600\n",
      "Gradient: [ -4.5996  -0.6472   8.0905 -17.4614 182.5648]\n",
      "Weights: [-4.6408  0.1398 -0.8     0.0579  0.1343]\n",
      "MSE loss: 99.6839\n",
      "Iteration: 17700\n",
      "Gradient: [  4.2705   7.0389 -32.9103 -16.908   64.0492]\n",
      "Weights: [-4.6334  0.1426 -0.8002  0.0574  0.1342]\n",
      "MSE loss: 99.4248\n",
      "Iteration: 17800\n",
      "Gradient: [   0.8959   21.048    14.9836    3.727  -222.18  ]\n",
      "Weights: [-4.6192  0.1415 -0.7996  0.0574  0.1341]\n",
      "MSE loss: 99.4162\n",
      "Iteration: 17900\n",
      "Gradient: [   5.6783  -20.0006  -33.8663  -33.6412 -342.8038]\n",
      "Weights: [-4.5971  0.1298 -0.8022  0.057   0.1344]\n",
      "MSE loss: 99.6842\n",
      "Iteration: 18000\n",
      "Gradient: [   3.5762   -1.0056  -19.4993  -13.5158 -299.7247]\n",
      "Weights: [-4.6201  0.147  -0.8064  0.0576  0.1347]\n",
      "MSE loss: 99.0432\n",
      "Iteration: 18100\n",
      "Gradient: [   2.3649   -2.4545    1.9333   54.5362 -244.7749]\n",
      "Weights: [-4.6266  0.1532 -0.81    0.0572  0.135 ]\n",
      "MSE loss: 98.8409\n",
      "Iteration: 18200\n",
      "Gradient: [ 15.644  -17.8263  31.0739  68.5738 -31.6288]\n",
      "Weights: [-4.6183  0.1532 -0.8087  0.0567  0.1351]\n",
      "MSE loss: 98.9934\n",
      "Iteration: 18300\n",
      "Gradient: [  7.3888   3.0259  46.5539  87.4207 -74.7059]\n",
      "Weights: [-4.6297  0.1558 -0.8085  0.0558  0.1354]\n",
      "MSE loss: 98.7971\n",
      "Iteration: 18400\n",
      "Gradient: [ 0.9087  0.4952 32.0276 17.1251 10.1448]\n",
      "Weights: [-4.6327  0.1578 -0.8078  0.0547  0.1356]\n",
      "MSE loss: 98.7598\n",
      "Iteration: 18500\n",
      "Gradient: [  5.9669   5.284  -28.7164 116.7397  75.835 ]\n",
      "Weights: [-4.6297  0.1514 -0.8092  0.0553  0.1359]\n",
      "MSE loss: 98.7509\n",
      "Iteration: 18600\n",
      "Gradient: [  -7.3711  -14.2177  -38.7222  -29.4066 -115.0758]\n",
      "Weights: [-4.6328  0.1549 -0.8122  0.0545  0.1361]\n",
      "MSE loss: 98.8454\n",
      "Iteration: 18700\n",
      "Gradient: [ -1.4975  15.2893   3.0156 -84.8761  49.9742]\n",
      "Weights: [-4.6265  0.1671 -0.8155  0.0534  0.1363]\n",
      "MSE loss: 98.44\n",
      "Iteration: 18800\n",
      "Gradient: [  2.3916   1.9297 -15.0749 -20.8758 139.272 ]\n",
      "Weights: [-4.648   0.1678 -0.8132  0.0541  0.1365]\n",
      "MSE loss: 98.7291\n",
      "Iteration: 18900\n",
      "Gradient: [ -5.8272  -2.306   32.0167  19.3703 134.3753]\n",
      "Weights: [-4.6359  0.165  -0.8161  0.0539  0.1367]\n",
      "MSE loss: 98.4118\n",
      "Iteration: 19000\n",
      "Gradient: [  4.538    4.523   27.0048  39.4265 280.9941]\n",
      "Weights: [-4.6233  0.1683 -0.8163  0.053   0.1369]\n",
      "MSE loss: 98.3908\n",
      "Iteration: 19100\n",
      "Gradient: [ -12.0004    9.2998   33.1447  -49.7239 -184.0804]\n",
      "Weights: [-4.6392  0.1811 -0.8214  0.0532  0.1369]\n",
      "MSE loss: 98.1082\n",
      "Iteration: 19200\n",
      "Gradient: [ -3.6314 -11.7364  40.5787 -36.9023  73.0917]\n",
      "Weights: [-4.6306  0.1775 -0.8212  0.0529  0.1369]\n",
      "MSE loss: 98.1068\n",
      "Iteration: 19300\n",
      "Gradient: [   0.979    10.4891  -28.0776 -154.6436 -133.5381]\n",
      "Weights: [-4.6531  0.1884 -0.8225  0.052   0.1372]\n",
      "MSE loss: 98.3189\n",
      "Iteration: 19400\n",
      "Gradient: [   2.5971   -6.7854  -20.2174  -16.3795 -146.0787]\n",
      "Weights: [-4.6458  0.1967 -0.8242  0.0515  0.1374]\n",
      "MSE loss: 97.8915\n",
      "Iteration: 19500\n",
      "Gradient: [ 7.183  -8.3433 -3.6729 38.527  47.2599]\n",
      "Weights: [-4.6355  0.1923 -0.8276  0.0516  0.1377]\n",
      "MSE loss: 97.746\n",
      "Iteration: 19600\n",
      "Gradient: [ -4.7408   6.5258  27.3899  37.9576 -69.718 ]\n",
      "Weights: [-4.6373  0.1962 -0.8284  0.0512  0.1379]\n",
      "MSE loss: 97.6916\n",
      "Iteration: 19700\n",
      "Gradient: [  -7.9287   -5.395   -23.081     2.9945 -278.8212]\n",
      "Weights: [-4.6497  0.2013 -0.8308  0.0508  0.1381]\n",
      "MSE loss: 97.7013\n",
      "Iteration: 19800\n",
      "Gradient: [  -4.9174   -6.3239    1.9523   36.5966 -243.8923]\n",
      "Weights: [-4.6498  0.212  -0.8328  0.0497  0.1383]\n",
      "MSE loss: 97.4878\n",
      "Iteration: 19900\n",
      "Gradient: [ 4.29800e-01  3.35860e+00  2.33020e+00  5.75981e+01 -5.41000e-02]\n",
      "Weights: [-4.6503  0.2126 -0.8351  0.0503  0.1384]\n",
      "MSE loss: 97.3928\n",
      "Iteration: 20000\n",
      "Gradient: [  5.5723  10.1385  31.4579 -35.2186  11.8751]\n",
      "Weights: [-4.6544  0.2258 -0.8393  0.0502  0.1385]\n",
      "MSE loss: 97.253\n",
      "Iteration: 20100\n",
      "Gradient: [  -0.8372   25.9787   -2.1253  -27.9631 -181.3337]\n",
      "Weights: [-4.6525  0.2337 -0.8454  0.0503  0.139 ]\n",
      "MSE loss: 97.076\n",
      "Iteration: 20200\n",
      "Gradient: [-13.4416 -10.5949  26.5405  50.6691 356.9126]\n",
      "Weights: [-4.6557  0.2265 -0.8475  0.05    0.1393]\n",
      "MSE loss: 97.2775\n",
      "Iteration: 20300\n",
      "Gradient: [ 4.084800e+00  7.650000e-02 -1.528610e+01  1.140199e+02 -3.421497e+02]\n",
      "Weights: [-4.6486  0.2375 -0.8508  0.0509  0.1393]\n",
      "MSE loss: 96.9517\n",
      "Iteration: 20400\n",
      "Gradient: [  14.3082   16.3758   -1.4076  -11.9437 -107.5224]\n",
      "Weights: [-4.6355  0.2251 -0.8514  0.051   0.1397]\n",
      "MSE loss: 96.9095\n",
      "Iteration: 20500\n",
      "Gradient: [ -2.2206   6.4744 -17.0279  39.5971 170.9407]\n",
      "Weights: [-4.6396  0.2282 -0.8539  0.0516  0.1394]\n",
      "MSE loss: 96.8482\n",
      "Iteration: 20600\n",
      "Gradient: [   8.6642    2.8459   58.1793  -30.0748 -206.3235]\n",
      "Weights: [-4.6542  0.2364 -0.8555  0.0514  0.1398]\n",
      "MSE loss: 96.7027\n",
      "Iteration: 20700\n",
      "Gradient: [ -4.8943 -25.5698  -1.8527 -20.2436 200.3493]\n",
      "Weights: [-4.6468  0.2435 -0.8586  0.052   0.1395]\n",
      "MSE loss: 96.6562\n",
      "Iteration: 20800\n",
      "Gradient: [  -1.0444   20.9767   61.1258 -116.023   255.0693]\n",
      "Weights: [-4.6573  0.2476 -0.8576  0.0511  0.1397]\n",
      "MSE loss: 96.5969\n",
      "Iteration: 20900\n",
      "Gradient: [   6.4759    0.3964  -28.4943  -64.1103 -167.0966]\n",
      "Weights: [-4.6472  0.2498 -0.8611  0.0505  0.14  ]\n",
      "MSE loss: 96.5155\n",
      "Iteration: 21000\n",
      "Gradient: [ -9.0034  -5.9347 -10.0747  17.031    1.6167]\n",
      "Weights: [-4.6558  0.2504 -0.8611  0.051   0.14  ]\n",
      "MSE loss: 96.4624\n",
      "Iteration: 21100\n",
      "Gradient: [  -8.3388    7.566    40.4506   38.9653 -144.6865]\n",
      "Weights: [-4.6575  0.2527 -0.8611  0.0509  0.14  ]\n",
      "MSE loss: 96.4749\n",
      "Iteration: 21200\n",
      "Gradient: [   2.63     -0.2759   43.8045  -11.0776 -211.5878]\n",
      "Weights: [-4.672   0.2623 -0.8632  0.0503  0.1401]\n",
      "MSE loss: 96.4934\n",
      "Iteration: 21300\n",
      "Gradient: [ 13.2726   8.3812  40.7313 -76.3201  37.1108]\n",
      "Weights: [-4.6509  0.2734 -0.8674  0.0493  0.1404]\n",
      "MSE loss: 96.6925\n",
      "Iteration: 21400\n",
      "Gradient: [ -6.481  -11.1417  -8.4119  25.0841 -95.1924]\n",
      "Weights: [-4.6813  0.276  -0.8688  0.0498  0.1406]\n",
      "MSE loss: 96.3422\n",
      "Iteration: 21500\n",
      "Gradient: [  -3.2238   -2.6148   -6.129    61.8115 -178.1476]\n",
      "Weights: [-4.6789  0.2756 -0.8716  0.0501  0.1408]\n",
      "MSE loss: 96.2846\n",
      "Iteration: 21600\n",
      "Gradient: [  -5.8944   -2.1494  -12.0343 -135.919    18.6528]\n",
      "Weights: [-4.6653  0.2647 -0.8703  0.0499  0.141 ]\n",
      "MSE loss: 96.1766\n",
      "Iteration: 21700\n",
      "Gradient: [  6.77   -12.9381  20.5331 123.4021 191.1393]\n",
      "Weights: [-4.647   0.2678 -0.8739  0.0493  0.1414]\n",
      "MSE loss: 96.1105\n",
      "Iteration: 21800\n",
      "Gradient: [   4.3485    2.8918   17.0665  -20.2229 -242.9512]\n",
      "Weights: [-4.6536  0.2626 -0.875   0.0499  0.1417]\n",
      "MSE loss: 96.0187\n",
      "Iteration: 21900\n",
      "Gradient: [  3.341  -17.7108   5.5098  23.7442 240.1462]\n",
      "Weights: [-4.6424  0.2663 -0.8771  0.0503  0.1416]\n",
      "MSE loss: 96.1887\n",
      "Iteration: 22000\n",
      "Gradient: [ -7.2858  -1.6445  15.9697 -32.856  -77.2374]\n",
      "Weights: [-4.6563  0.2734 -0.8805  0.0505  0.1418]\n",
      "MSE loss: 95.852\n",
      "Iteration: 22100\n",
      "Gradient: [  -6.7404  -14.8397   24.1626   41.1924 -327.2987]\n",
      "Weights: [-4.6482  0.2691 -0.8828  0.051   0.1418]\n",
      "MSE loss: 95.9106\n",
      "Iteration: 22200\n",
      "Gradient: [ 2.139   9.9105 65.4277  2.1591 42.5628]\n",
      "Weights: [-4.6533  0.2738 -0.8842  0.0514  0.1417]\n",
      "MSE loss: 95.8273\n",
      "Iteration: 22300\n",
      "Gradient: [  0.4648  -7.3791  31.4512  11.8879 105.7495]\n",
      "Weights: [-4.6628  0.2818 -0.8844  0.0514  0.1419]\n",
      "MSE loss: 95.8791\n",
      "Iteration: 22400\n",
      "Gradient: [   2.991     7.9132   -6.7097 -115.7341  -85.6683]\n",
      "Weights: [-4.6597  0.2857 -0.8876  0.0512  0.1419]\n",
      "MSE loss: 95.6558\n",
      "Iteration: 22500\n",
      "Gradient: [  3.7539   5.6017  21.7803 -15.2409 244.885 ]\n",
      "Weights: [-4.6539  0.2943 -0.8863  0.0507  0.1417]\n",
      "MSE loss: 96.3185\n",
      "Iteration: 22600\n",
      "Gradient: [  4.6627 -12.6708  32.2682 -70.209  -91.2009]\n",
      "Weights: [-4.6687  0.3007 -0.8916  0.0507  0.1418]\n",
      "MSE loss: 95.543\n",
      "Iteration: 22700\n",
      "Gradient: [ -2.1826   4.9807  27.4275   1.7506 154.9387]\n",
      "Weights: [-4.6726  0.3067 -0.8926  0.0506  0.1419]\n",
      "MSE loss: 95.4536\n",
      "Iteration: 22800\n",
      "Gradient: [ -8.9462  -3.5575 -35.127   32.1135 -48.5946]\n",
      "Weights: [-4.6695  0.3103 -0.8928  0.0497  0.1417]\n",
      "MSE loss: 95.5818\n",
      "Iteration: 22900\n",
      "Gradient: [  -4.001     6.8576   -7.2707   56.3368 -216.3779]\n",
      "Weights: [-4.6852  0.3069 -0.8915  0.0504  0.142 ]\n",
      "MSE loss: 95.6032\n",
      "Iteration: 23000\n",
      "Gradient: [ 3.6904 -5.846  -5.4141 31.7316 26.6891]\n",
      "Weights: [-4.687   0.3126 -0.8942  0.05    0.142 ]\n",
      "MSE loss: 95.6707\n",
      "Iteration: 23100\n",
      "Gradient: [  2.2356  -6.8148   9.9865   4.8291 -34.9379]\n",
      "Weights: [-4.6878  0.3052 -0.8901  0.0502  0.142 ]\n",
      "MSE loss: 95.7002\n",
      "Iteration: 23200\n",
      "Gradient: [ -8.8191  -4.7305 -53.0381  56.9718 228.019 ]\n",
      "Weights: [-4.6695  0.2983 -0.8934  0.05    0.1423]\n",
      "MSE loss: 95.7021\n",
      "Iteration: 23300\n",
      "Gradient: [  -4.1075   -0.833    63.6782  -44.6317 -126.8549]\n",
      "Weights: [-4.6631  0.3025 -0.8905  0.0494  0.1423]\n",
      "MSE loss: 95.6239\n",
      "Iteration: 23400\n",
      "Gradient: [  15.9754   17.6006   -7.4014  -10.015  -279.0658]\n",
      "Weights: [-4.6704  0.3172 -0.8967  0.0486  0.1425]\n",
      "MSE loss: 95.3409\n",
      "Iteration: 23500\n",
      "Gradient: [  -5.173   -15.1923  -28.1021 -119.7512  -70.5192]\n",
      "Weights: [-4.7015  0.3166 -0.898   0.0493  0.1428]\n",
      "MSE loss: 96.2735\n",
      "Iteration: 23600\n",
      "Gradient: [ -12.7376   -7.5353  -15.3452  -74.6239 -233.6169]\n",
      "Weights: [-4.6935  0.3255 -0.9013  0.0496  0.1429]\n",
      "MSE loss: 95.3264\n",
      "Iteration: 23700\n",
      "Gradient: [ -3.6333  11.181   35.1163  16.9884 101.5298]\n",
      "Weights: [-4.6758  0.3213 -0.9034  0.0498  0.1431]\n",
      "MSE loss: 95.1447\n",
      "Iteration: 23800\n",
      "Gradient: [ -19.7579   -4.3013    2.9737   28.1865 -126.0579]\n",
      "Weights: [-4.6775  0.323  -0.9055  0.0493  0.1435]\n",
      "MSE loss: 95.0582\n",
      "Iteration: 23900\n",
      "Gradient: [-17.5538  -4.6937  16.4961 -71.792  153.8981]\n",
      "Weights: [-4.6794  0.323  -0.9052  0.0489  0.1433]\n",
      "MSE loss: 95.1906\n",
      "Iteration: 24000\n",
      "Gradient: [  4.6812  11.1864 -11.8252 -68.0062  33.0608]\n",
      "Weights: [-4.6854  0.3329 -0.906   0.0486  0.1434]\n",
      "MSE loss: 95.0\n",
      "Iteration: 24100\n",
      "Gradient: [   6.5838   -4.1718    5.4411   11.8614 -157.1226]\n",
      "Weights: [-4.6735  0.3204 -0.905   0.0485  0.1436]\n",
      "MSE loss: 95.0764\n",
      "Iteration: 24200\n",
      "Gradient: [  0.9072 -20.3414  19.8562  34.8879  -5.1617]\n",
      "Weights: [-4.6948  0.3311 -0.9039  0.0476  0.1434]\n",
      "MSE loss: 95.3523\n",
      "Iteration: 24300\n",
      "Gradient: [  -6.4194   -6.209   -25.1371 -206.4027 -104.3633]\n",
      "Weights: [-4.6892  0.3389 -0.9085  0.0477  0.1435]\n",
      "MSE loss: 95.0865\n",
      "Iteration: 24400\n",
      "Gradient: [ 16.4372  11.1806  33.7774 154.7864  69.3468]\n",
      "Weights: [-4.6897  0.345  -0.9076  0.0478  0.1438]\n",
      "MSE loss: 95.2564\n",
      "Iteration: 24500\n",
      "Gradient: [  7.6627  19.457   32.158  128.1376 303.7559]\n",
      "Weights: [-4.6732  0.3482 -0.9107  0.0471  0.144 ]\n",
      "MSE loss: 95.6493\n",
      "Iteration: 24600\n",
      "Gradient: [  -3.8819   -6.3111    5.6761  131.0911 -409.9517]\n",
      "Weights: [-4.6824  0.334  -0.9093  0.0468  0.1442]\n",
      "MSE loss: 94.9347\n",
      "Iteration: 24700\n",
      "Gradient: [-4.04000e-02  1.25920e+01 -1.56957e+01 -8.95516e+01  7.01970e+00]\n",
      "Weights: [-4.6776  0.3339 -0.9081  0.0465  0.1443]\n",
      "MSE loss: 94.8799\n",
      "Iteration: 24800\n",
      "Gradient: [ -4.3916  13.5777   5.765   -5.1564 -64.6681]\n",
      "Weights: [-4.6816  0.3373 -0.9074  0.0463  0.1442]\n",
      "MSE loss: 94.8862\n",
      "Iteration: 24900\n",
      "Gradient: [ -0.9636  10.1659  38.3599  18.9052 278.6115]\n",
      "Weights: [-4.6897  0.3433 -0.9051  0.0451  0.1442]\n",
      "MSE loss: 94.9355\n",
      "Iteration: 25000\n",
      "Gradient: [  10.5189  -20.0825  -34.1306   77.2203 -244.5722]\n",
      "Weights: [-4.682   0.3313 -0.9066  0.0453  0.1443]\n",
      "MSE loss: 95.2716\n",
      "Iteration: 25100\n",
      "Gradient: [  -1.2711   22.1136    9.1691   85.8135 -228.13  ]\n",
      "Weights: [-4.6864  0.3333 -0.9053  0.0459  0.1443]\n",
      "MSE loss: 94.9509\n",
      "Iteration: 25200\n",
      "Gradient: [ -6.787    3.872  -48.6456 -49.746  226.8608]\n",
      "Weights: [-4.6936  0.3415 -0.9081  0.0458  0.1441]\n",
      "MSE loss: 95.0593\n",
      "Iteration: 25300\n",
      "Gradient: [ -8.7709   4.2964  17.6702 -81.9657 122.2521]\n",
      "Weights: [-4.6943  0.3423 -0.9076  0.0449  0.1443]\n",
      "MSE loss: 95.1211\n",
      "Iteration: 25400\n",
      "Gradient: [  -2.9464   12.5899  -49.6223 -105.2694 -127.7129]\n",
      "Weights: [-4.6937  0.349  -0.9093  0.0449  0.1445]\n",
      "MSE loss: 94.8047\n",
      "Iteration: 25500\n",
      "Gradient: [   2.8181   -7.2012   14.9719   39.0227 -104.3511]\n",
      "Weights: [-4.6852  0.3581 -0.9103  0.0446  0.1443]\n",
      "MSE loss: 95.0662\n",
      "Iteration: 25600\n",
      "Gradient: [  -2.6024   -6.7952  -30.5128 -170.9965    4.2757]\n",
      "Weights: [-4.6912  0.3558 -0.9134  0.0445  0.1446]\n",
      "MSE loss: 94.7822\n",
      "Iteration: 25700\n",
      "Gradient: [  -2.0646   -2.257   -34.0228 -135.9879 -148.2768]\n",
      "Weights: [-4.6855  0.3459 -0.9125  0.0452  0.1447]\n",
      "MSE loss: 94.7734\n",
      "Iteration: 25800\n",
      "Gradient: [  1.6239  -2.1037 -18.8299  47.2937 -28.0148]\n",
      "Weights: [-4.6967  0.3512 -0.912   0.0452  0.1444]\n",
      "MSE loss: 94.9879\n",
      "Iteration: 25900\n",
      "Gradient: [  3.2919   7.3987 -29.6878  40.3962  75.3386]\n",
      "Weights: [-4.7062  0.3481 -0.9092  0.0449  0.1447]\n",
      "MSE loss: 95.1356\n",
      "Iteration: 26000\n",
      "Gradient: [ 15.4881   6.257   52.6484 -34.0541 276.8614]\n",
      "Weights: [-4.6775  0.3497 -0.9119  0.0454  0.1449]\n",
      "MSE loss: 95.4119\n",
      "Iteration: 26100\n",
      "Gradient: [  -2.1777    6.9413   18.6794   -8.9048 -161.3052]\n",
      "Weights: [-4.6926  0.3569 -0.9146  0.045   0.1449]\n",
      "MSE loss: 94.6633\n",
      "Iteration: 26200\n",
      "Gradient: [ -8.3125  15.9384  -6.6119  60.7506 239.8767]\n",
      "Weights: [-4.6831  0.3538 -0.9175  0.0453  0.145 ]\n",
      "MSE loss: 94.6105\n",
      "Iteration: 26300\n",
      "Gradient: [  6.8254   4.0545  19.4319  95.3016 321.3564]\n",
      "Weights: [-4.673   0.3537 -0.9166  0.0446  0.1451]\n",
      "MSE loss: 94.8573\n",
      "Iteration: 26400\n",
      "Gradient: [  -4.6268   -6.3081   -5.2116   34.0452 -168.3201]\n",
      "Weights: [-4.6908  0.3642 -0.9204  0.0454  0.1453]\n",
      "MSE loss: 94.6183\n",
      "Iteration: 26500\n",
      "Gradient: [  2.9599 -10.0647  40.0045  28.9538 123.4529]\n",
      "Weights: [-4.6887  0.3578 -0.9225  0.0453  0.1454]\n",
      "MSE loss: 94.6248\n",
      "Iteration: 26600\n",
      "Gradient: [ -9.8839  -5.2888   4.7715  73.1771 -17.5156]\n",
      "Weights: [-4.6992  0.3658 -0.9226  0.0457  0.1455]\n",
      "MSE loss: 94.6151\n",
      "Iteration: 26700\n",
      "Gradient: [ 15.9312   2.0595  33.6721 -71.2795 410.2426]\n",
      "Weights: [-4.6757  0.3723 -0.9279  0.0461  0.1453]\n",
      "MSE loss: 94.8978\n",
      "Iteration: 26800\n",
      "Gradient: [  7.4597   6.2297  30.3916  76.7008 557.7271]\n",
      "Weights: [-4.6835  0.365  -0.9247  0.0463  0.1454]\n",
      "MSE loss: 94.6436\n",
      "Iteration: 26900\n",
      "Gradient: [   3.8581   -5.3414    3.9503  -31.9297 -145.2119]\n",
      "Weights: [-4.6758  0.3677 -0.9271  0.0454  0.1456]\n",
      "MSE loss: 94.6337\n",
      "Iteration: 27000\n",
      "Gradient: [  7.7747   6.8068  26.3063  73.0138 -86.0288]\n",
      "Weights: [-4.68    0.3863 -0.9315  0.0443  0.1458]\n",
      "MSE loss: 94.9207\n",
      "Iteration: 27100\n",
      "Gradient: [  -7.1837    4.231     6.4095  -84.8805 -340.9656]\n",
      "Weights: [-4.7128  0.3936 -0.9321  0.0436  0.146 ]\n",
      "MSE loss: 94.3608\n",
      "Iteration: 27200\n",
      "Gradient: [ -6.5698  18.5413  17.5972 -84.9473 324.3027]\n",
      "Weights: [-4.6922  0.394  -0.936   0.0437  0.1464]\n",
      "MSE loss: 94.2919\n",
      "Iteration: 27300\n",
      "Gradient: [  -9.7962  -12.1359    6.8995 -139.242  -104.6685]\n",
      "Weights: [-4.7077  0.3935 -0.9392  0.0438  0.1466]\n",
      "MSE loss: 94.4763\n",
      "Iteration: 27400\n",
      "Gradient: [  -6.9644   -0.893     5.008    94.3731 -280.3313]\n",
      "Weights: [-4.6967  0.3935 -0.9406  0.0439  0.1469]\n",
      "MSE loss: 94.0773\n",
      "Iteration: 27500\n",
      "Gradient: [5.780000e-02 3.750000e-01 8.132300e+00 9.150800e+01 1.110303e+02]\n",
      "Weights: [-4.6767  0.3871 -0.9434  0.0445  0.1473]\n",
      "MSE loss: 94.4112\n",
      "Iteration: 27600\n",
      "Gradient: [ -2.9688  -3.9177 -20.4513  -8.5057  10.5653]\n",
      "Weights: [-4.6927  0.4023 -0.9467  0.0429  0.1475]\n",
      "MSE loss: 93.9976\n",
      "Iteration: 27700\n",
      "Gradient: [  -1.0071   -8.1038  -19.4996   24.7821 -226.7144]\n",
      "Weights: [-4.7027  0.4068 -0.9474  0.0435  0.1474]\n",
      "MSE loss: 93.9126\n",
      "Iteration: 27800\n",
      "Gradient: [  -8.141    -1.5842  -17.5052  -76.3224 -123.7701]\n",
      "Weights: [-4.7082  0.4159 -0.9478  0.043   0.1475]\n",
      "MSE loss: 93.9215\n",
      "Iteration: 27900\n",
      "Gradient: [-13.3434   5.7611 -32.4227  -0.5886 -19.4282]\n",
      "Weights: [-4.7169  0.4166 -0.9483  0.0431  0.1475]\n",
      "MSE loss: 93.9332\n",
      "Iteration: 28000\n",
      "Gradient: [  -1.8234    6.7801   -0.2204   58.2472 -215.6765]\n",
      "Weights: [-4.7268  0.4363 -0.95    0.0422  0.1473]\n",
      "MSE loss: 93.9439\n",
      "Iteration: 28100\n",
      "Gradient: [  -2.554   -13.8311   -7.1395  -46.1299 -209.7233]\n",
      "Weights: [-4.7284  0.4326 -0.952   0.0423  0.1474]\n",
      "MSE loss: 94.1279\n",
      "Iteration: 28200\n",
      "Gradient: [   5.1538    5.2547    5.1261  -48.5127 -188.8393]\n",
      "Weights: [-4.7142  0.4356 -0.9547  0.0417  0.1477]\n",
      "MSE loss: 93.8015\n",
      "Iteration: 28300\n",
      "Gradient: [  1.1987 -11.6198 -21.1175 -24.4787   3.2363]\n",
      "Weights: [-4.7015  0.4261 -0.9542  0.0414  0.1479]\n",
      "MSE loss: 93.9586\n",
      "Iteration: 28400\n",
      "Gradient: [13.7193  5.2775 -7.5695  4.0682 31.9949]\n",
      "Weights: [-4.7016  0.421  -0.958   0.0425  0.1483]\n",
      "MSE loss: 93.8992\n",
      "Iteration: 28500\n",
      "Gradient: [   4.8974  -17.6192   -6.3878   71.3779 -124.4727]\n",
      "Weights: [-4.7045  0.4226 -0.9598  0.0432  0.1484]\n",
      "MSE loss: 93.7293\n",
      "Iteration: 28600\n",
      "Gradient: [ -0.7944   9.8868 -17.9993 -55.8167 284.6385]\n",
      "Weights: [-4.7158  0.4254 -0.9574  0.0429  0.1485]\n",
      "MSE loss: 93.821\n",
      "Iteration: 28700\n",
      "Gradient: [  2.9019  -1.2684   9.6994 135.4646 181.0808]\n",
      "Weights: [-4.6862  0.4189 -0.9586  0.043   0.1488]\n",
      "MSE loss: 94.5805\n",
      "Iteration: 28800\n",
      "Gradient: [ 4.7308 -7.7819  5.9755 82.416  41.6553]\n",
      "Weights: [-4.6937  0.4252 -0.9597  0.0419  0.1488]\n",
      "MSE loss: 93.9208\n",
      "Iteration: 28900\n",
      "Gradient: [  -3.134   -14.9035   -6.2044 -108.2887  -26.485 ]\n",
      "Weights: [-4.6983  0.4235 -0.9587  0.0417  0.1488]\n",
      "MSE loss: 93.7604\n",
      "Iteration: 29000\n",
      "Gradient: [  -2.1719  -14.2632    7.8737   72.5083 -330.2771]\n",
      "Weights: [-4.7134  0.4288 -0.9594  0.0419  0.1488]\n",
      "MSE loss: 93.7097\n",
      "Iteration: 29100\n",
      "Gradient: [-0.3819  0.5944  1.5251 12.3088 41.7269]\n",
      "Weights: [-4.7225  0.4365 -0.9616  0.0415  0.1489]\n",
      "MSE loss: 93.7775\n",
      "Iteration: 29200\n",
      "Gradient: [ 12.6201   5.5506   0.2814 -18.9677  57.4711]\n",
      "Weights: [-4.702   0.4308 -0.9603  0.0417  0.1486]\n",
      "MSE loss: 93.7126\n",
      "Iteration: 29300\n",
      "Gradient: [ -1.4778 -10.9033  28.8729 -22.6848 -67.2674]\n",
      "Weights: [-4.7299  0.4477 -0.9608  0.0413  0.1484]\n",
      "MSE loss: 93.7113\n",
      "Iteration: 29400\n",
      "Gradient: [  -3.8167    2.6767  -12.8583  -74.8417 -112.9643]\n",
      "Weights: [-4.7145  0.4432 -0.9612  0.0411  0.1486]\n",
      "MSE loss: 93.6345\n",
      "Iteration: 29500\n",
      "Gradient: [ -7.9151 -15.4087 -11.7704 -72.8493 191.3398]\n",
      "Weights: [-4.7254  0.4429 -0.9586  0.0405  0.1486]\n",
      "MSE loss: 93.686\n",
      "Iteration: 29600\n",
      "Gradient: [  -8.634    -7.4036   -2.8212   -8.6759 -289.4991]\n",
      "Weights: [-4.7147  0.4473 -0.9624  0.0406  0.1485]\n",
      "MSE loss: 93.6883\n",
      "Iteration: 29700\n",
      "Gradient: [  5.1269   7.6648   7.8488  13.1964 -11.7508]\n",
      "Weights: [-4.7235  0.4577 -0.9632  0.0406  0.1486]\n",
      "MSE loss: 93.6966\n",
      "Iteration: 29800\n",
      "Gradient: [ -1.9334  -0.2799 -21.2669  75.9579 265.2595]\n",
      "Weights: [-4.7149  0.4569 -0.9672  0.0408  0.1487]\n",
      "MSE loss: 93.6592\n",
      "Iteration: 29900\n",
      "Gradient: [   3.7481  -17.6231   19.2055   41.0969 -121.6439]\n",
      "Weights: [-4.7247  0.4613 -0.967   0.0415  0.1486]\n",
      "MSE loss: 93.6135\n",
      "Iteration: 30000\n",
      "Gradient: [  -2.7553    7.7282    5.6863   83.8391 -239.3025]\n",
      "Weights: [-4.7164  0.4704 -0.9745  0.0424  0.1487]\n",
      "MSE loss: 93.782\n",
      "Iteration: 30100\n",
      "Gradient: [   6.2181    1.5776  -12.5401   77.4385 -237.1512]\n",
      "Weights: [-4.7292  0.4672 -0.9768  0.0426  0.1491]\n",
      "MSE loss: 93.4468\n",
      "Iteration: 30200\n",
      "Gradient: [ -3.6312   8.6583  13.9039  87.1491 121.5862]\n",
      "Weights: [-4.7258  0.4637 -0.9787  0.0434  0.1495]\n",
      "MSE loss: 93.4567\n",
      "Iteration: 30300\n",
      "Gradient: [   0.3759   -3.1951   32.4549   27.4937 -330.0201]\n",
      "Weights: [-4.7297  0.4738 -0.9785  0.0423  0.1493]\n",
      "MSE loss: 93.369\n",
      "Iteration: 30400\n",
      "Gradient: [  -8.113   -21.191    -2.2646  -92.1954 -188.8831]\n",
      "Weights: [-4.7268  0.4749 -0.9806  0.0427  0.1491]\n",
      "MSE loss: 93.4154\n",
      "Iteration: 30500\n",
      "Gradient: [  8.3841  16.5542 -51.9177   5.9537  54.804 ]\n",
      "Weights: [-4.7373  0.4828 -0.9819  0.0436  0.149 ]\n",
      "MSE loss: 93.382\n",
      "Iteration: 30600\n",
      "Gradient: [  3.7597   2.3831  10.6333 -32.217   48.9523]\n",
      "Weights: [-4.7345  0.4836 -0.9827  0.0433  0.149 ]\n",
      "MSE loss: 93.3517\n",
      "Iteration: 30700\n",
      "Gradient: [ -11.1111    6.0531  -39.552    -3.6363 -190.0097]\n",
      "Weights: [-4.7196  0.4618 -0.9813  0.0435  0.1494]\n",
      "MSE loss: 93.4935\n",
      "Iteration: 30800\n",
      "Gradient: [ -4.5721  -8.2986 -13.3389  -7.1006 328.4651]\n",
      "Weights: [-4.7369  0.4681 -0.981   0.0431  0.1493]\n",
      "MSE loss: 93.8594\n",
      "Iteration: 30900\n",
      "Gradient: [ -9.346   16.5272  21.6604 -34.9018 -89.2785]\n",
      "Weights: [-4.711   0.4541 -0.9776  0.0429  0.1494]\n",
      "MSE loss: 93.5066\n",
      "Iteration: 31000\n",
      "Gradient: [-12.0216  -0.9055 -14.393  -30.6411 -90.6566]\n",
      "Weights: [-4.7272  0.4667 -0.9803  0.0438  0.1493]\n",
      "MSE loss: 93.373\n",
      "Iteration: 31100\n",
      "Gradient: [   2.5979    1.0176   -0.2968 -105.1224   68.9932]\n",
      "Weights: [-4.7311  0.4708 -0.9803  0.0432  0.1492]\n",
      "MSE loss: 93.4075\n",
      "Iteration: 31200\n",
      "Gradient: [ -6.7151  -3.3507 -10.8348 -30.1329 -76.1407]\n",
      "Weights: [-4.7128  0.4538 -0.9765  0.043   0.1492]\n",
      "MSE loss: 93.5159\n",
      "Iteration: 31300\n",
      "Gradient: [ -3.9684 -17.4613  -9.4957 -92.6614 123.8752]\n",
      "Weights: [-4.7203  0.4488 -0.9749  0.0428  0.1492]\n",
      "MSE loss: 93.9998\n",
      "Iteration: 31400\n",
      "Gradient: [-8.930000e-02 -3.443600e+00  3.711410e+01  7.937530e+01 -1.196067e+02]\n",
      "Weights: [-4.711   0.4555 -0.9742  0.0426  0.1494]\n",
      "MSE loss: 93.576\n",
      "Iteration: 31500\n",
      "Gradient: [   6.1695  -14.4567   23.5751  -43.7551 -145.3322]\n",
      "Weights: [-4.7225  0.47   -0.978   0.0418  0.1492]\n",
      "MSE loss: 93.4565\n",
      "Iteration: 31600\n",
      "Gradient: [  3.6427   8.4822  68.6439  99.1887 127.9621]\n",
      "Weights: [-4.7213  0.4783 -0.9805  0.0417  0.1495]\n",
      "MSE loss: 93.492\n",
      "Iteration: 31700\n",
      "Gradient: [  -8.9159    8.5446  -47.081   -86.1395 -213.6271]\n",
      "Weights: [-4.7453  0.4817 -0.9794  0.041   0.1496]\n",
      "MSE loss: 93.5358\n",
      "Iteration: 31800\n",
      "Gradient: [ -1.9629   2.0421   2.4497  66.7887 283.0948]\n",
      "Weights: [-4.7454  0.4929 -0.982   0.0396  0.1497]\n",
      "MSE loss: 93.5595\n",
      "Iteration: 31900\n",
      "Gradient: [  0.4452   8.587    6.997  148.8699  23.2522]\n",
      "Weights: [-4.7296  0.4865 -0.9824  0.0397  0.1501]\n",
      "MSE loss: 93.3326\n",
      "Iteration: 32000\n",
      "Gradient: [  2.5836  15.49    -5.0159 -97.5104  69.5397]\n",
      "Weights: [-4.7307  0.4859 -0.9795  0.0394  0.1498]\n",
      "MSE loss: 93.4082\n",
      "Iteration: 32100\n",
      "Gradient: [  6.8412 -19.2431  -8.4071  20.4184  19.8618]\n",
      "Weights: [-4.7301  0.4902 -0.9819  0.0395  0.15  ]\n",
      "MSE loss: 93.4312\n",
      "Iteration: 32200\n",
      "Gradient: [  5.7559  12.9191  11.3094 -19.0653 -94.2691]\n",
      "Weights: [-4.7483  0.4954 -0.9804  0.0389  0.1498]\n",
      "MSE loss: 93.4793\n",
      "Iteration: 32300\n",
      "Gradient: [ -2.9825  -3.2897   5.1177  10.9251 -29.5873]\n",
      "Weights: [-4.7227  0.4861 -0.9826  0.039   0.1501]\n",
      "MSE loss: 93.4794\n",
      "Iteration: 32400\n",
      "Gradient: [   0.3921    5.2028  -53.2728  -20.102  -280.5546]\n",
      "Weights: [-4.7352  0.4809 -0.9784  0.0397  0.15  ]\n",
      "MSE loss: 93.3558\n",
      "Iteration: 32500\n",
      "Gradient: [ -1.2956   0.3495  30.0815 -86.6604 -15.8942]\n",
      "Weights: [-4.7379  0.4817 -0.9774  0.0397  0.1497]\n",
      "MSE loss: 93.3919\n",
      "Iteration: 32600\n",
      "Gradient: [ -12.3634   -0.3379   25.1474 -117.6435   74.594 ]\n",
      "Weights: [-4.7587  0.4875 -0.9799  0.0398  0.1497]\n",
      "MSE loss: 94.3162\n",
      "Iteration: 32700\n",
      "Gradient: [  9.2725  12.0175  13.071  -12.8864 -31.5869]\n",
      "Weights: [-4.7264  0.4805 -0.9805  0.0399  0.15  ]\n",
      "MSE loss: 93.3483\n",
      "Iteration: 32800\n",
      "Gradient: [  1.9707 -11.8649 -18.5601 116.1437   0.4929]\n",
      "Weights: [-4.7342  0.479  -0.9807  0.0405  0.1502]\n",
      "MSE loss: 93.364\n",
      "Iteration: 32900\n",
      "Gradient: [ 4.1104  6.277  24.6563 12.177  77.6676]\n",
      "Weights: [-4.716   0.4712 -0.9802  0.0402  0.1503]\n",
      "MSE loss: 93.4304\n",
      "Iteration: 33000\n",
      "Gradient: [  0.1705 -16.8714 -15.9918 -33.3806  -1.4446]\n",
      "Weights: [-4.74    0.4844 -0.9839  0.0397  0.1503]\n",
      "MSE loss: 93.5387\n",
      "Iteration: 33100\n",
      "Gradient: [ -4.5444   5.975    6.4472  29.5561 101.4615]\n",
      "Weights: [-4.7364  0.5022 -0.9894  0.0398  0.1504]\n",
      "MSE loss: 93.2785\n",
      "Iteration: 33200\n",
      "Gradient: [-10.73     0.5781  17.3587 113.2588  36.5714]\n",
      "Weights: [-4.7302  0.4917 -0.9869  0.0396  0.1504]\n",
      "MSE loss: 93.2628\n",
      "Iteration: 33300\n",
      "Gradient: [ 11.0668   9.147   14.9656 -89.4831 -69.5395]\n",
      "Weights: [-4.7153  0.4797 -0.9836  0.0391  0.1507]\n",
      "MSE loss: 93.4579\n",
      "Iteration: 33400\n",
      "Gradient: [   6.0134   -3.6375   -4.2678  -51.9035 -416.8984]\n",
      "Weights: [-4.7244  0.4738 -0.9819  0.0393  0.1507]\n",
      "MSE loss: 93.3082\n",
      "Iteration: 33500\n",
      "Gradient: [  6.5785   5.8814 -12.7324  75.8566  -1.8947]\n",
      "Weights: [-4.7181  0.4693 -0.9805  0.0394  0.1504]\n",
      "MSE loss: 93.4019\n",
      "Iteration: 33600\n",
      "Gradient: [  0.5686  -1.7643  10.0333 -26.4066 -82.2707]\n",
      "Weights: [-4.7281  0.4652 -0.9797  0.0406  0.1505]\n",
      "MSE loss: 93.4613\n",
      "Iteration: 33700\n",
      "Gradient: [  1.4493 -10.0013 -53.8821  53.3281 146.5949]\n",
      "Weights: [-4.7152  0.4655 -0.9805  0.0406  0.1502]\n",
      "MSE loss: 93.3754\n",
      "Iteration: 33800\n",
      "Gradient: [  0.2861  25.8523   9.465   44.9994 111.5072]\n",
      "Weights: [-4.7135  0.4643 -0.9808  0.0406  0.1503]\n",
      "MSE loss: 93.3808\n",
      "Iteration: 33900\n",
      "Gradient: [ -5.879   -9.5833 -14.0833 -42.3547 216.5696]\n",
      "Weights: [-4.7249  0.469  -0.9804  0.0404  0.1502]\n",
      "MSE loss: 93.4348\n",
      "Iteration: 34000\n",
      "Gradient: [  7.4098   8.0828  -7.216   15.7592 175.9889]\n",
      "Weights: [-4.731   0.4854 -0.9805  0.0399  0.15  ]\n",
      "MSE loss: 93.4194\n",
      "Iteration: 34100\n",
      "Gradient: [  4.4302  -4.1703 -30.8588   3.4696  -1.1653]\n",
      "Weights: [-4.7455  0.4871 -0.981   0.0409  0.1497]\n",
      "MSE loss: 93.4177\n",
      "Iteration: 34200\n",
      "Gradient: [-16.7439   5.9415 -51.2254  14.7034  42.5237]\n",
      "Weights: [-4.7513  0.4933 -0.9812  0.0405  0.1496]\n",
      "MSE loss: 93.4792\n",
      "Iteration: 34300\n",
      "Gradient: [ -7.285   -8.2435 -53.2393 -65.0827 104.1015]\n",
      "Weights: [-4.7336  0.483  -0.9802  0.0401  0.1498]\n",
      "MSE loss: 93.3429\n",
      "Iteration: 34400\n",
      "Gradient: [ -4.6444  -7.4631 -15.2681 -51.4201 179.0349]\n",
      "Weights: [-4.7465  0.4867 -0.9803  0.04    0.1499]\n",
      "MSE loss: 93.45\n",
      "Iteration: 34500\n",
      "Gradient: [  1.4019  -6.5807   5.8031 -55.9269 329.0872]\n",
      "Weights: [-4.7375  0.4853 -0.9818  0.0394  0.1501]\n",
      "MSE loss: 93.397\n",
      "Iteration: 34600\n",
      "Gradient: [  5.2052   2.9957   1.5641 -18.2776  68.2616]\n",
      "Weights: [-4.7162  0.4721 -0.9827  0.0403  0.1502]\n",
      "MSE loss: 93.404\n",
      "Iteration: 34700\n",
      "Gradient: [  4.4101  -7.7073  47.2402 -90.9432  99.9262]\n",
      "Weights: [-4.7141  0.4658 -0.9802  0.0408  0.1501]\n",
      "MSE loss: 93.3932\n",
      "Iteration: 34800\n",
      "Gradient: [  -5.6918   -2.9185  -25.4435   96.3905 -141.6465]\n",
      "Weights: [-4.72    0.4614 -0.9792  0.0417  0.1501]\n",
      "MSE loss: 93.3901\n",
      "Iteration: 34900\n",
      "Gradient: [ 13.0589 -17.8771  12.5382 111.263  -40.3613]\n",
      "Weights: [-4.6969  0.4582 -0.9786  0.0416  0.1499]\n",
      "MSE loss: 93.8692\n",
      "Iteration: 35000\n",
      "Gradient: [  -6.7981   -0.9502  -22.1462  -78.5368 -260.786 ]\n",
      "Weights: [-4.724   0.4665 -0.9802  0.0415  0.15  ]\n",
      "MSE loss: 93.3572\n",
      "Iteration: 35100\n",
      "Gradient: [ 7.0905 14.2451 26.0693 27.7727 33.55  ]\n",
      "Weights: [-4.7114  0.4611 -0.9786  0.0422  0.1498]\n",
      "MSE loss: 93.5337\n",
      "Iteration: 35200\n",
      "Gradient: [   6.7717   -0.6216   -9.3356   48.3604 -123.813 ]\n",
      "Weights: [-4.7096  0.4699 -0.9796  0.0417  0.1495]\n",
      "MSE loss: 93.6597\n",
      "Iteration: 35300\n",
      "Gradient: [  -4.6493    4.1953   -3.0366    7.6305 -108.296 ]\n",
      "Weights: [-4.742   0.4818 -0.9795  0.0417  0.1493]\n",
      "MSE loss: 93.4725\n",
      "Iteration: 35400\n",
      "Gradient: [   6.1059   -4.2794   -2.6734  -76.2558 -161.5136]\n",
      "Weights: [-4.7291  0.4771 -0.9827  0.0421  0.1494]\n",
      "MSE loss: 93.4269\n",
      "Iteration: 35500\n",
      "Gradient: [  -6.0068   -5.7113   20.2126  -30.6217 -352.1206]\n",
      "Weights: [-4.7422  0.489  -0.9818  0.0419  0.1493]\n",
      "MSE loss: 93.3843\n",
      "Iteration: 35600\n",
      "Gradient: [-14.3951 -16.8834  13.9347 -69.1798  65.1205]\n",
      "Weights: [-4.728   0.4843 -0.9853  0.0426  0.1496]\n",
      "MSE loss: 93.3433\n",
      "Iteration: 35700\n",
      "Gradient: [   1.5471  -11.5762    5.2938 -156.3662   13.8744]\n",
      "Weights: [-4.7583  0.4953 -0.9856  0.0418  0.1497]\n",
      "MSE loss: 93.6912\n",
      "Iteration: 35800\n",
      "Gradient: [-5.356900e+00 -7.884600e+00 -8.906900e+00 -1.049000e-01 -1.394755e+02]\n",
      "Weights: [-4.7216  0.4817 -0.9848  0.0422  0.1495]\n",
      "MSE loss: 93.3817\n",
      "Iteration: 35900\n",
      "Gradient: [  0.3212   1.1257 -21.3422   9.3641 -84.8521]\n",
      "Weights: [-4.7208  0.4804 -0.9866  0.043   0.1498]\n",
      "MSE loss: 93.3915\n",
      "Iteration: 36000\n",
      "Gradient: [-1.824000e-01 -8.246600e+00 -1.933350e+01 -1.895700e+01  2.530325e+02]\n",
      "Weights: [-4.7309  0.4815 -0.9885  0.0436  0.1496]\n",
      "MSE loss: 93.2886\n",
      "Iteration: 36100\n",
      "Gradient: [  -4.622    -8.5261    3.5567   74.8171 -217.0773]\n",
      "Weights: [-4.7178  0.4739 -0.9896  0.0439  0.1499]\n",
      "MSE loss: 93.2968\n",
      "Iteration: 36200\n",
      "Gradient: [   4.5829   -4.8371  -15.8176 -148.016   137.3159]\n",
      "Weights: [-4.7287  0.4754 -0.9903  0.0437  0.15  ]\n",
      "MSE loss: 93.5175\n",
      "Iteration: 36300\n",
      "Gradient: [  -5.8116    5.9153  -44.5497 -108.1314   -8.1612]\n",
      "Weights: [-4.7302  0.4833 -0.9928  0.0439  0.1501]\n",
      "MSE loss: 93.237\n",
      "Iteration: 36400\n",
      "Gradient: [  -1.0241    0.4681    1.8564  -90.4124 -395.8208]\n",
      "Weights: [-4.7281  0.4772 -0.9923  0.0438  0.1504]\n",
      "MSE loss: 93.3262\n",
      "Iteration: 36500\n",
      "Gradient: [ -13.5999   -4.41      7.1028  -93.9098 -189.64  ]\n",
      "Weights: [-4.7288  0.4766 -0.9935  0.0439  0.1505]\n",
      "MSE loss: 93.4159\n",
      "Iteration: 36600\n",
      "Gradient: [  9.2384  -7.1258 -34.7267 -40.1438 115.0181]\n",
      "Weights: [-4.717   0.475  -0.991   0.0435  0.1504]\n",
      "MSE loss: 93.3486\n",
      "Iteration: 36700\n",
      "Gradient: [ -11.0356  -15.0487  -46.1406 -137.6861 -171.4448]\n",
      "Weights: [-4.7302  0.4776 -0.9932  0.0433  0.1505]\n",
      "MSE loss: 93.5594\n",
      "Iteration: 36800\n",
      "Gradient: [   7.9557   12.7553   -8.5205   34.4863 -117.244 ]\n",
      "Weights: [-4.7131  0.4782 -0.9909  0.0428  0.1503]\n",
      "MSE loss: 93.3383\n",
      "Iteration: 36900\n",
      "Gradient: [ 1.213  21.2926 31.0537 28.2657  8.9445]\n",
      "Weights: [-4.7184  0.4774 -0.9939  0.0442  0.1505]\n",
      "MSE loss: 93.3476\n",
      "Iteration: 37000\n",
      "Gradient: [ -9.8174  -5.0376  16.6418 -55.132  -93.0101]\n",
      "Weights: [-4.729   0.479  -0.9931  0.0433  0.1505]\n",
      "MSE loss: 93.3631\n",
      "Iteration: 37100\n",
      "Gradient: [  -4.8634   -2.0899  -25.7346 -123.988  -354.4541]\n",
      "Weights: [-4.7246  0.4763 -0.9934  0.0437  0.1503]\n",
      "MSE loss: 93.4254\n",
      "Iteration: 37200\n",
      "Gradient: [  1.1497  15.9578 -15.0511  -4.1385 301.4528]\n",
      "Weights: [-4.74    0.4984 -0.9951  0.0434  0.15  ]\n",
      "MSE loss: 93.1719\n",
      "Iteration: 37300\n",
      "Gradient: [ -4.2442 -22.9216  -3.3061  -4.5332   3.6648]\n",
      "Weights: [-4.7343  0.4968 -0.9948  0.0426  0.1501]\n",
      "MSE loss: 93.1838\n",
      "Iteration: 37400\n",
      "Gradient: [  3.7443   4.5638  23.7384 -71.988  255.0402]\n",
      "Weights: [-4.7366  0.5033 -0.9963  0.0423  0.1501]\n",
      "MSE loss: 93.1981\n",
      "Iteration: 37500\n",
      "Gradient: [-11.8457  -6.9967 -15.8335 -73.6243 -38.6721]\n",
      "Weights: [-4.7453  0.5054 -0.9995  0.0429  0.1506]\n",
      "MSE loss: 93.1698\n",
      "Iteration: 37600\n",
      "Gradient: [  4.0779  -9.566   -7.0871 -19.639  109.953 ]\n",
      "Weights: [-4.7496  0.5137 -1.0049  0.0438  0.1505]\n",
      "MSE loss: 93.1777\n",
      "Iteration: 37700\n",
      "Gradient: [  7.6348   1.4392 -46.8245  75.1451  97.6929]\n",
      "Weights: [-4.728   0.5028 -1.0067  0.0443  0.1506]\n",
      "MSE loss: 93.2226\n",
      "Iteration: 37800\n",
      "Gradient: [14.3889 10.0808 20.0713 53.9725 -1.4307]\n",
      "Weights: [-4.719   0.5003 -1.0077  0.0455  0.1503]\n",
      "MSE loss: 93.1912\n",
      "Iteration: 37900\n",
      "Gradient: [  6.6855   5.9322 -27.0305 -20.3707 248.4099]\n",
      "Weights: [-4.7191  0.5034 -1.0085  0.0457  0.1505]\n",
      "MSE loss: 93.2188\n",
      "Iteration: 38000\n",
      "Gradient: [-3.11000e-02  9.29100e+00 -1.85744e+01  4.44330e+01 -5.46077e+01]\n",
      "Weights: [-4.7367  0.5149 -1.0075  0.0447  0.1504]\n",
      "MSE loss: 93.0342\n",
      "Iteration: 38100\n",
      "Gradient: [  7.8423  13.7774  -7.6616  66.9437 201.1433]\n",
      "Weights: [-4.7491  0.5285 -1.0117  0.045   0.1505]\n",
      "MSE loss: 93.0273\n",
      "Iteration: 38200\n",
      "Gradient: [-5.043000e+00 -8.643300e+00 -1.168000e-01  4.377040e+01 -1.229496e+02]\n",
      "Weights: [-4.7438  0.5117 -1.0094  0.0449  0.1507]\n",
      "MSE loss: 93.1652\n",
      "Iteration: 38300\n",
      "Gradient: [-12.5861   1.6639   2.9035 -53.5467  25.8853]\n",
      "Weights: [-4.7428  0.5073 -1.0088  0.0454  0.1507]\n",
      "MSE loss: 93.1798\n",
      "Iteration: 38400\n",
      "Gradient: [   4.7096  -14.4279   -1.142   -63.5129 -329.2618]\n",
      "Weights: [-4.7254  0.5038 -1.0121  0.0456  0.1508]\n",
      "MSE loss: 93.1886\n",
      "Iteration: 38500\n",
      "Gradient: [   0.7016   -0.8614    8.7348 -103.0783 -166.4906]\n",
      "Weights: [-4.7261  0.5054 -1.0096  0.0457  0.1506]\n",
      "MSE loss: 93.054\n",
      "Iteration: 38600\n",
      "Gradient: [  4.9794  -3.8553   8.4088  36.8967 -22.1792]\n",
      "Weights: [-4.728   0.5157 -1.0119  0.046   0.1505]\n",
      "MSE loss: 93.1936\n",
      "Iteration: 38700\n",
      "Gradient: [ 10.8568   5.6363 -26.5032  61.3048 118.8021]\n",
      "Weights: [-4.7297  0.5061 -1.0087  0.0456  0.1505]\n",
      "MSE loss: 93.0366\n",
      "Iteration: 38800\n",
      "Gradient: [ -4.7873  -2.6096 -15.0697 -10.6985  36.357 ]\n",
      "Weights: [-4.7315  0.5095 -1.0062  0.0447  0.1505]\n",
      "MSE loss: 93.0778\n",
      "Iteration: 38900\n",
      "Gradient: [  8.2329 -16.7578  -9.963  -92.6964  46.2937]\n",
      "Weights: [-4.7365  0.5154 -1.0083  0.0444  0.1505]\n",
      "MSE loss: 93.0229\n",
      "Iteration: 39000\n",
      "Gradient: [  1.3766   4.3497  23.5319 -24.6324  89.8673]\n",
      "Weights: [-4.7404  0.5147 -1.0087  0.0445  0.1506]\n",
      "MSE loss: 93.03\n",
      "Iteration: 39100\n",
      "Gradient: [-1.9849  1.2691 13.131  37.5537 75.3718]\n",
      "Weights: [-4.738   0.5108 -1.0089  0.0456  0.1506]\n",
      "MSE loss: 93.0291\n",
      "Iteration: 39200\n",
      "Gradient: [ -8.0219 -16.88   -60.7962  43.1174  14.7891]\n",
      "Weights: [-4.7302  0.5048 -1.01    0.0454  0.1508]\n",
      "MSE loss: 93.0569\n",
      "Iteration: 39300\n",
      "Gradient: [ 7.965500e+00 -4.330000e-02  4.283900e+00 -3.328380e+01  2.261737e+02]\n",
      "Weights: [-4.7277  0.5104 -1.0093  0.0444  0.1507]\n",
      "MSE loss: 93.0614\n",
      "Iteration: 39400\n",
      "Gradient: [  -5.7765    6.4335   10.6486  -14.5141 -201.8343]\n",
      "Weights: [-4.7369  0.5096 -1.008   0.0441  0.1506]\n",
      "MSE loss: 93.242\n",
      "Iteration: 39500\n",
      "Gradient: [  -9.8279    1.1416  -27.7649   89.7841 -325.755 ]\n",
      "Weights: [-4.7269  0.5046 -1.0092  0.0451  0.1506]\n",
      "MSE loss: 93.093\n",
      "Iteration: 39600\n",
      "Gradient: [   3.6314   -6.6934    2.351    26.41   -159.3636]\n",
      "Weights: [-4.7228  0.4995 -1.0081  0.0459  0.1506]\n",
      "MSE loss: 93.0929\n",
      "Iteration: 39700\n",
      "Gradient: [  1.9223   1.1413 -15.8352 -87.0608  63.459 ]\n",
      "Weights: [-4.7244  0.4991 -1.0084  0.0459  0.1505]\n",
      "MSE loss: 93.1151\n",
      "Iteration: 39800\n",
      "Gradient: [   3.5511   -4.1803  -25.7324   40.5438 -138.8911]\n",
      "Weights: [-4.7189  0.498  -1.0089  0.0462  0.1504]\n",
      "MSE loss: 93.1877\n",
      "Iteration: 39900\n",
      "Gradient: [ -9.2403  10.7287 -10.4199  69.6474  15.8857]\n",
      "Weights: [-4.7352  0.4992 -1.0125  0.047   0.1507]\n",
      "MSE loss: 93.5024\n",
      "Iteration: 40000\n",
      "Gradient: [ -2.2049 -23.6999  -2.9776 -68.4803 244.9409]\n",
      "Weights: [-4.7326  0.5033 -1.0122  0.0469  0.1506]\n",
      "MSE loss: 93.1421\n",
      "Iteration: 40100\n",
      "Gradient: [ -0.8815  -1.7735   2.5721  82.9305 234.7461]\n",
      "Weights: [-4.7476  0.5212 -1.017   0.047   0.1507]\n",
      "MSE loss: 93.0475\n",
      "Iteration: 40200\n",
      "Gradient: [  3.3     -3.2151 -27.8935 -76.3535 347.5345]\n",
      "Weights: [-4.7246  0.5103 -1.016   0.046   0.1507]\n",
      "MSE loss: 93.2684\n",
      "Iteration: 40300\n",
      "Gradient: [ -5.392   -8.3129  26.9232 102.2767 144.6853]\n",
      "Weights: [-4.7393  0.5142 -1.0136  0.0471  0.1507]\n",
      "MSE loss: 93.1453\n",
      "Iteration: 40400\n",
      "Gradient: [  -2.2454    4.0589  -19.1868   32.1751 -144.8066]\n",
      "Weights: [-4.7391  0.5158 -1.0153  0.0467  0.1506]\n",
      "MSE loss: 93.0117\n",
      "Iteration: 40500\n",
      "Gradient: [ -1.5556   2.1928 -17.0202 -68.8277 175.0461]\n",
      "Weights: [-4.7441  0.529  -1.0164  0.0464  0.1506]\n",
      "MSE loss: 93.0483\n",
      "Iteration: 40600\n",
      "Gradient: [ -4.9739 -10.7708 -16.9929  75.1303   1.8712]\n",
      "Weights: [-4.7501  0.5247 -1.0164  0.047   0.1503]\n",
      "MSE loss: 93.1421\n",
      "Iteration: 40700\n",
      "Gradient: [-10.0792  10.4776  35.7898 -15.7773 -16.397 ]\n",
      "Weights: [-4.7547  0.5348 -1.0182  0.0466  0.1502]\n",
      "MSE loss: 93.1242\n",
      "Iteration: 40800\n",
      "Gradient: [ -8.4443   4.5146  68.4118 158.8373 -53.2062]\n",
      "Weights: [-4.7492  0.5304 -1.0182  0.047   0.1504]\n",
      "MSE loss: 92.9442\n",
      "Iteration: 40900\n",
      "Gradient: [  0.6692   6.4765 -29.9162 -68.7104 127.2411]\n",
      "Weights: [-4.7409  0.5294 -1.0194  0.0471  0.1507]\n",
      "MSE loss: 93.0248\n",
      "Iteration: 41000\n",
      "Gradient: [   1.6914   21.8674   -0.9398  -28.247  -169.4109]\n",
      "Weights: [-4.7481  0.5389 -1.0202  0.0459  0.1507]\n",
      "MSE loss: 92.925\n",
      "Iteration: 41100\n",
      "Gradient: [ -0.9498   8.3303  29.7785 128.3642 -93.4974]\n",
      "Weights: [-4.739   0.5326 -1.0191  0.0463  0.1507]\n",
      "MSE loss: 93.0189\n",
      "Iteration: 41200\n",
      "Gradient: [  -3.3366  -14.1931   -7.6324  137.5805 -128.8488]\n",
      "Weights: [-4.7672  0.5444 -1.0204  0.0462  0.1506]\n",
      "MSE loss: 93.1236\n",
      "Iteration: 41300\n",
      "Gradient: [ -0.627   24.3599  67.6163  38.3219 100.3972]\n",
      "Weights: [-4.7511  0.5403 -1.0218  0.0465  0.1508]\n",
      "MSE loss: 92.9243\n",
      "Iteration: 41400\n",
      "Gradient: [ -6.6036   1.581  -22.5784 -84.4904 -37.5931]\n",
      "Weights: [-4.7529  0.5392 -1.0218  0.0463  0.1508]\n",
      "MSE loss: 92.9113\n",
      "Iteration: 41500\n",
      "Gradient: [ -7.2144  16.0567 -19.4194  31.5077 136.8498]\n",
      "Weights: [-4.7379  0.5361 -1.0228  0.0457  0.1508]\n",
      "MSE loss: 93.036\n",
      "Iteration: 41600\n",
      "Gradient: [  6.9311   2.4552  -7.2721 -63.9329 156.1815]\n",
      "Weights: [-4.7553  0.5413 -1.0225  0.0463  0.1509]\n",
      "MSE loss: 92.919\n",
      "Iteration: 41700\n",
      "Gradient: [  -3.1668   -2.5766   52.286   -46.2199 -507.5674]\n",
      "Weights: [-4.7638  0.5454 -1.0237  0.0468  0.1508]\n",
      "MSE loss: 93.0309\n",
      "Iteration: 41800\n",
      "Gradient: [  1.58    -1.7865   7.6318  43.2556 -36.6218]\n",
      "Weights: [-4.7525  0.5467 -1.0255  0.0466  0.1508]\n",
      "MSE loss: 92.8683\n",
      "Iteration: 41900\n",
      "Gradient: [  -9.2707   11.004    -4.1592 -104.9161 -185.0658]\n",
      "Weights: [-4.7519  0.5433 -1.0246  0.0465  0.1509]\n",
      "MSE loss: 92.8729\n",
      "Iteration: 42000\n",
      "Gradient: [11.4457 13.2516 27.6162 41.9128 63.0839]\n",
      "Weights: [-4.7301  0.5298 -1.0247  0.0461  0.1512]\n",
      "MSE loss: 92.9955\n",
      "Iteration: 42100\n",
      "Gradient: [  -3.3471   -2.9167  -54.2505  -37.4564 -361.9513]\n",
      "Weights: [-4.7405  0.5425 -1.0245  0.0453  0.1513]\n",
      "MSE loss: 93.0012\n",
      "Iteration: 42200\n",
      "Gradient: [   0.7738    7.9044  -36.4825 -123.7549  164.4703]\n",
      "Weights: [-4.7562  0.5452 -1.0254  0.046   0.1513]\n",
      "MSE loss: 92.9422\n",
      "Iteration: 42300\n",
      "Gradient: [ -8.7241   4.2916   8.0503 -10.1386 121.4032]\n",
      "Weights: [-4.7762  0.5591 -1.0272  0.0458  0.1511]\n",
      "MSE loss: 93.1933\n",
      "Iteration: 42400\n",
      "Gradient: [  -0.3809  -12.6488  -28.1477 -184.5252  -60.702 ]\n",
      "Weights: [-4.7619  0.5567 -1.0296  0.046   0.1512]\n",
      "MSE loss: 92.8976\n",
      "Iteration: 42500\n",
      "Gradient: [  7.7495   3.3395 -10.8935  25.4981  -5.8269]\n",
      "Weights: [-4.753   0.5605 -1.0319  0.0472  0.1508]\n",
      "MSE loss: 92.8961\n",
      "Iteration: 42600\n",
      "Gradient: [ -8.6175   2.6967   2.9102 -42.952   -5.2435]\n",
      "Weights: [-4.7576  0.5597 -1.0304  0.0476  0.1506]\n",
      "MSE loss: 92.8988\n",
      "Iteration: 42700\n",
      "Gradient: [-13.8826  -2.5511  43.2469 107.1955  49.4081]\n",
      "Weights: [-4.7435  0.5467 -1.0307  0.0482  0.1507]\n",
      "MSE loss: 92.8462\n",
      "Iteration: 42800\n",
      "Gradient: [ 1.09640e+01 -6.88000e+00 -1.17600e+01 -3.80000e-02  4.80063e+01]\n",
      "Weights: [-4.7503  0.5428 -1.0289  0.0482  0.1509]\n",
      "MSE loss: 92.8508\n",
      "Iteration: 42900\n",
      "Gradient: [  4.4575  -8.5332  37.0572 -15.1847 -18.2186]\n",
      "Weights: [-4.7374  0.545  -1.031   0.0472  0.1511]\n",
      "MSE loss: 92.8938\n",
      "Iteration: 43000\n",
      "Gradient: [  -2.2293    6.9696    4.7754  -53.335  -162.3769]\n",
      "Weights: [-4.7524  0.5443 -1.0333  0.0479  0.1512]\n",
      "MSE loss: 93.0783\n",
      "Iteration: 43100\n",
      "Gradient: [  4.8013  19.9417  59.0728  -7.2728 218.263 ]\n",
      "Weights: [-4.7401  0.556  -1.0343  0.0479  0.1512]\n",
      "MSE loss: 93.2614\n",
      "Iteration: 43200\n",
      "Gradient: [   1.2211   11.1617   49.368   211.4635 -123.0476]\n",
      "Weights: [-4.7379  0.5644 -1.0372  0.0479  0.151 ]\n",
      "MSE loss: 93.4822\n",
      "Iteration: 43300\n",
      "Gradient: [  -5.2529   -5.2792  -19.7625   18.5849 -181.5558]\n",
      "Weights: [-4.745   0.5584 -1.0361  0.0475  0.1513]\n",
      "MSE loss: 92.9048\n",
      "Iteration: 43400\n",
      "Gradient: [   4.319     0.3199   18.7043  -49.1073 -158.0704]\n",
      "Weights: [-4.7498  0.5562 -1.0357  0.047   0.1513]\n",
      "MSE loss: 92.8387\n",
      "Iteration: 43500\n",
      "Gradient: [  4.5706  -5.3609 -17.2422 -77.2998 145.9328]\n",
      "Weights: [-4.742   0.5528 -1.0376  0.0474  0.1515]\n",
      "MSE loss: 92.8431\n",
      "Iteration: 43600\n",
      "Gradient: [  -2.1128   10.9626  -17.2221  -31.0972 -185.9796]\n",
      "Weights: [-4.7525  0.5656 -1.0393  0.0467  0.1516]\n",
      "MSE loss: 92.7643\n",
      "Iteration: 43700\n",
      "Gradient: [  13.1102   18.3388   -9.8534  -39.7831 -251.3419]\n",
      "Weights: [-4.7573  0.5656 -1.0402  0.0466  0.1518]\n",
      "MSE loss: 92.8103\n",
      "Iteration: 43800\n",
      "Gradient: [  1.571    2.3252 -54.0175  -5.0311  93.5542]\n",
      "Weights: [-4.7618  0.5715 -1.0428  0.046   0.1521]\n",
      "MSE loss: 92.8173\n",
      "Iteration: 43900\n",
      "Gradient: [   4.8918   10.4692   13.8752   19.6055 -182.6324]\n",
      "Weights: [-4.7715  0.5726 -1.0426  0.0474  0.152 ]\n",
      "MSE loss: 92.9398\n",
      "Iteration: 44000\n",
      "Gradient: [ -2.25     9.7612 -42.0715 -16.793   45.9765]\n",
      "Weights: [-4.7682  0.5763 -1.0456  0.047   0.1519]\n",
      "MSE loss: 92.9932\n",
      "Iteration: 44100\n",
      "Gradient: [ -2.0078  -0.9581 -72.1539 -73.0733 -38.968 ]\n",
      "Weights: [-4.7727  0.5783 -1.0462  0.0465  0.1523]\n",
      "MSE loss: 92.9831\n",
      "Iteration: 44200\n",
      "Gradient: [ -1.5683 -16.7523   8.8194  -8.3567  28.4343]\n",
      "Weights: [-4.7483  0.579  -1.0458  0.0464  0.1522]\n",
      "MSE loss: 93.0461\n",
      "Iteration: 44300\n",
      "Gradient: [  -0.8925   -6.4459   -2.8646   22.3919 -160.5004]\n",
      "Weights: [-4.757   0.5731 -1.0452  0.046   0.1524]\n",
      "MSE loss: 92.7417\n",
      "Iteration: 44400\n",
      "Gradient: [ 12.3396  -3.1116 -36.4413 -29.0851  46.6124]\n",
      "Weights: [-4.7475  0.5747 -1.0483  0.0462  0.1525]\n",
      "MSE loss: 92.7604\n",
      "Iteration: 44500\n",
      "Gradient: [  5.738    5.4353  26.9287 100.5725 352.8114]\n",
      "Weights: [-4.7642  0.5748 -1.0441  0.0462  0.1525]\n",
      "MSE loss: 92.8667\n",
      "Iteration: 44600\n",
      "Gradient: [  1.3555  25.1436  10.7481  20.9552 188.0189]\n",
      "Weights: [-4.7571  0.5759 -1.0455  0.0464  0.1522]\n",
      "MSE loss: 92.7218\n",
      "Iteration: 44700\n",
      "Gradient: [  2.0354 -12.957   15.0932 107.2127 142.3812]\n",
      "Weights: [-4.7609  0.5808 -1.0441  0.046   0.1522]\n",
      "MSE loss: 92.8232\n",
      "Iteration: 44800\n",
      "Gradient: [  -0.2568   -3.192   -46.5353 -153.3293 -223.8725]\n",
      "Weights: [-4.7624  0.5752 -1.0446  0.0454  0.1523]\n",
      "MSE loss: 92.8988\n",
      "Iteration: 44900\n",
      "Gradient: [  0.3364  -5.3029  11.63    21.9715 -49.0667]\n",
      "Weights: [-4.7477  0.5677 -1.0441  0.0461  0.1523]\n",
      "MSE loss: 92.7606\n",
      "Iteration: 45000\n",
      "Gradient: [  4.2383  -0.9416 -18.4962   4.6857 -83.9221]\n",
      "Weights: [-4.7409  0.5638 -1.0441  0.0461  0.1525]\n",
      "MSE loss: 92.8201\n",
      "Iteration: 45100\n",
      "Gradient: [   3.3336   -6.9068    5.5585 -113.2532 -141.5382]\n",
      "Weights: [-4.7477  0.565  -1.045   0.0458  0.1525]\n",
      "MSE loss: 92.8636\n",
      "Iteration: 45200\n",
      "Gradient: [ -2.313   12.2657  -0.9206 -49.1468 277.0531]\n",
      "Weights: [-4.761   0.5682 -1.0428  0.0457  0.1528]\n",
      "MSE loss: 92.8524\n",
      "Iteration: 45300\n",
      "Gradient: [  5.2078 -22.8549 -22.6488 -42.5378 -28.4444]\n",
      "Weights: [-4.7596  0.5708 -1.0426  0.045   0.1527]\n",
      "MSE loss: 92.7796\n",
      "Iteration: 45400\n",
      "Gradient: [-13.8144 -34.9499 -19.1446 -79.6715   3.3337]\n",
      "Weights: [-4.7665  0.5758 -1.0448  0.0447  0.1526]\n",
      "MSE loss: 93.022\n",
      "Iteration: 45500\n",
      "Gradient: [  6.8336 -21.8585   9.9768  46.8491 217.2464]\n",
      "Weights: [-4.7626  0.5913 -1.048   0.045   0.1527]\n",
      "MSE loss: 92.9852\n",
      "Iteration: 45600\n",
      "Gradient: [  -3.338   -18.5557    3.6942   88.1166 -112.2171]\n",
      "Weights: [-4.778   0.6011 -1.0493  0.0445  0.1526]\n",
      "MSE loss: 92.8199\n",
      "Iteration: 45700\n",
      "Gradient: [ -3.4273  13.6566   7.3736 -39.8418 -13.499 ]\n",
      "Weights: [-4.7631  0.5907 -1.0499  0.0449  0.153 ]\n",
      "MSE loss: 92.8101\n",
      "Iteration: 45800\n",
      "Gradient: [ -9.939    1.3615  -2.627  -99.4014 -75.352 ]\n",
      "Weights: [-4.7523  0.5795 -1.051   0.0453  0.153 ]\n",
      "MSE loss: 92.7407\n",
      "Iteration: 45900\n",
      "Gradient: [   5.096   -12.5952   -3.4031  -99.1594 -175.03  ]\n",
      "Weights: [-4.7516  0.5858 -1.0512  0.0447  0.153 ]\n",
      "MSE loss: 92.7756\n",
      "Iteration: 46000\n",
      "Gradient: [  3.8724   2.1696   7.6303  45.2279 185.7151]\n",
      "Weights: [-4.7565  0.5805 -1.0469  0.045   0.1528]\n",
      "MSE loss: 92.7479\n",
      "Iteration: 46100\n",
      "Gradient: [-11.1565 -18.0598 -17.5101 -51.4334   5.9309]\n",
      "Weights: [-4.7666  0.5778 -1.0456  0.0449  0.1528]\n",
      "MSE loss: 92.8166\n",
      "Iteration: 46200\n",
      "Gradient: [ 2.5358 -3.1509  6.8892 21.8203 22.809 ]\n",
      "Weights: [-4.7579  0.5818 -1.0447  0.0441  0.1526]\n",
      "MSE loss: 92.7569\n",
      "Iteration: 46300\n",
      "Gradient: [ -3.3374  15.544   19.9553 -88.2185  40.2076]\n",
      "Weights: [-4.7503  0.5856 -1.0473  0.0446  0.1526]\n",
      "MSE loss: 92.9706\n",
      "Iteration: 46400\n",
      "Gradient: [   6.5092   18.7606   17.4583 -114.3732  372.5334]\n",
      "Weights: [-4.777   0.6013 -1.0534  0.0458  0.1526]\n",
      "MSE loss: 92.7487\n",
      "Iteration: 46500\n",
      "Gradient: [   3.4958   10.8045   16.2579 -138.8761    8.2754]\n",
      "Weights: [-4.7755  0.6024 -1.0534  0.0455  0.1526]\n",
      "MSE loss: 92.7394\n",
      "Iteration: 46600\n",
      "Gradient: [  -1.8635  -12.2335  -26.4555   58.3046 -156.385 ]\n",
      "Weights: [-4.7629  0.5856 -1.0536  0.046   0.1529]\n",
      "MSE loss: 92.7856\n",
      "Iteration: 46700\n",
      "Gradient: [ -8.6637  -3.2161 -19.9659 -54.9545 -47.0195]\n",
      "Weights: [-4.7567  0.582  -1.0542  0.0468  0.153 ]\n",
      "MSE loss: 92.6974\n",
      "Iteration: 46800\n",
      "Gradient: [ -5.6544   7.1086  78.6805 103.3091  26.2174]\n",
      "Weights: [-4.7588  0.5872 -1.0543  0.0461  0.153 ]\n",
      "MSE loss: 92.682\n",
      "Iteration: 46900\n",
      "Gradient: [  -7.5053   -0.5305   -9.666   -81.9887 -136.6935]\n",
      "Weights: [-4.7791  0.5955 -1.0558  0.0455  0.1531]\n",
      "MSE loss: 93.0838\n",
      "Iteration: 47000\n",
      "Gradient: [  -5.9472   -7.9017  -20.3693   94.3452 -120.7318]\n",
      "Weights: [-4.766   0.596  -1.0562  0.0456  0.1531]\n",
      "MSE loss: 92.6725\n",
      "Iteration: 47100\n",
      "Gradient: [  3.6709   1.1648 -35.8601 -14.422   63.8577]\n",
      "Weights: [-4.7649  0.591  -1.0556  0.0452  0.1529]\n",
      "MSE loss: 93.0912\n",
      "Iteration: 47200\n",
      "Gradient: [ -10.0372  -15.0721    4.9059    5.416  -178.6897]\n",
      "Weights: [-4.7721  0.5869 -1.0521  0.046   0.153 ]\n",
      "MSE loss: 92.8387\n",
      "Iteration: 47300\n",
      "Gradient: [  -1.1227    0.9637  -41.0688    2.4082 -165.7738]\n",
      "Weights: [-4.7494  0.5761 -1.0509  0.046   0.1528]\n",
      "MSE loss: 92.7508\n",
      "Iteration: 47400\n",
      "Gradient: [  -3.585   -10.4062  -24.4143   59.4598 -379.1414]\n",
      "Weights: [-4.7576  0.5683 -1.0493  0.0465  0.153 ]\n",
      "MSE loss: 92.942\n",
      "Iteration: 47500\n",
      "Gradient: [  -5.4332   -5.2148  -61.7001  -66.9227 -352.5435]\n",
      "Weights: [-4.775   0.5764 -1.0492  0.0469  0.1526]\n",
      "MSE loss: 93.3911\n",
      "Iteration: 47600\n",
      "Gradient: [ 1.650000e-01 -3.511200e+00 -1.748670e+01  1.726370e+01 -2.133752e+02]\n",
      "Weights: [-4.7631  0.5852 -1.0504  0.0465  0.1526]\n",
      "MSE loss: 92.7283\n",
      "Iteration: 47700\n",
      "Gradient: [ -3.8342   0.3394   1.376    9.816  -17.9214]\n",
      "Weights: [-4.7659  0.5851 -1.0513  0.0465  0.1525]\n",
      "MSE loss: 92.7808\n",
      "Iteration: 47800\n",
      "Gradient: [  -3.901    -1.4328   -0.3206   31.8656 -199.4995]\n",
      "Weights: [-4.7634  0.5744 -1.0491  0.0475  0.1525]\n",
      "MSE loss: 92.811\n",
      "Iteration: 47900\n",
      "Gradient: [ -1.512    6.8549  15.9877  12.2758 281.2235]\n",
      "Weights: [-4.7502  0.5811 -1.0519  0.0474  0.1524]\n",
      "MSE loss: 92.7608\n",
      "Iteration: 48000\n",
      "Gradient: [  -5.535     2.4726  -22.3689   62.3182 -249.4624]\n",
      "Weights: [-4.7758  0.5903 -1.0536  0.047   0.1525]\n",
      "MSE loss: 92.9354\n",
      "Iteration: 48100\n",
      "Gradient: [  4.9375  -2.7727   4.5482  -2.2575 373.4438]\n",
      "Weights: [-4.7664  0.6019 -1.0547  0.0464  0.1523]\n",
      "MSE loss: 92.7638\n",
      "Iteration: 48200\n",
      "Gradient: [ -6.2136 -33.5876   6.2728 -52.3872 -83.6003]\n",
      "Weights: [-4.7639  0.6078 -1.0586  0.0469  0.1522]\n",
      "MSE loss: 92.8615\n",
      "Iteration: 48300\n",
      "Gradient: [   7.8866    4.7245   29.4369 -107.6462  265.1764]\n",
      "Weights: [-4.7607  0.6141 -1.0633  0.0482  0.1524]\n",
      "MSE loss: 93.1794\n",
      "Iteration: 48400\n",
      "Gradient: [-13.3353 -16.4701 -20.0212 -59.7376 -42.396 ]\n",
      "Weights: [-4.7774  0.5944 -1.0617  0.0489  0.1524]\n",
      "MSE loss: 93.4721\n",
      "Iteration: 48500\n",
      "Gradient: [  3.7323 -13.55   -13.2299  -6.3072 -40.6699]\n",
      "Weights: [-4.7426  0.5923 -1.0654  0.0498  0.1525]\n",
      "MSE loss: 92.8531\n",
      "Iteration: 48600\n",
      "Gradient: [  5.2913   8.7537   4.5322   5.8162 162.1141]\n",
      "Weights: [-4.7636  0.593  -1.068   0.0505  0.1528]\n",
      "MSE loss: 92.8884\n",
      "Iteration: 48700\n",
      "Gradient: [  -3.4069   12.0188   -4.8503 -213.8127  239.1357]\n",
      "Weights: [-4.7687  0.6034 -1.0679  0.0504  0.1526]\n",
      "MSE loss: 92.6204\n",
      "Iteration: 48800\n",
      "Gradient: [   7.621    -6.0734   20.6104  -29.1922 -164.4555]\n",
      "Weights: [-4.7558  0.6121 -1.0712  0.0505  0.1524]\n",
      "MSE loss: 92.7816\n",
      "Iteration: 48900\n",
      "Gradient: [  -8.6656   -8.5497  -19.2359   -6.0033 -181.6813]\n",
      "Weights: [-4.7844  0.6192 -1.0725  0.0505  0.1527]\n",
      "MSE loss: 92.7269\n",
      "Iteration: 49000\n",
      "Gradient: [ -1.4874   3.7441 -31.1042 112.7243 192.0906]\n",
      "Weights: [-4.756   0.6064 -1.0711  0.0509  0.1526]\n",
      "MSE loss: 92.6779\n",
      "Iteration: 49100\n",
      "Gradient: [  -6.6123    5.8147  -44.1656   42.8113 -381.1035]\n",
      "Weights: [-4.7657  0.5961 -1.0685  0.0517  0.1524]\n",
      "MSE loss: 92.6811\n",
      "Iteration: 49200\n",
      "Gradient: [10.4461 30.5929 34.4024 20.7326 -8.6047]\n",
      "Weights: [-4.7583  0.5997 -1.0697  0.052   0.1525]\n",
      "MSE loss: 92.6818\n",
      "Iteration: 49300\n",
      "Gradient: [ -6.3338  -2.4658   1.5008  45.4036 207.6977]\n",
      "Weights: [-4.7687  0.6097 -1.0727  0.0518  0.1524]\n",
      "MSE loss: 92.576\n",
      "Iteration: 49400\n",
      "Gradient: [ -5.4883  -3.0351  10.761   63.0531 141.1668]\n",
      "Weights: [-4.7591  0.602  -1.07    0.052   0.1523]\n",
      "MSE loss: 92.6171\n",
      "Iteration: 49500\n",
      "Gradient: [  2.1909   3.589   57.1286 -55.539  259.7092]\n",
      "Weights: [-4.7674  0.6022 -1.0692  0.0526  0.1522]\n",
      "MSE loss: 92.6122\n",
      "Iteration: 49600\n",
      "Gradient: [   1.0644  -27.2377  -13.2651   15.584  -221.6175]\n",
      "Weights: [-4.796   0.6232 -1.071   0.0526  0.1517]\n",
      "MSE loss: 92.9116\n",
      "Iteration: 49700\n",
      "Gradient: [ -5.4168   7.638   17.8004  32.3333 435.8044]\n",
      "Weights: [-4.7831  0.6228 -1.0727  0.0519  0.152 ]\n",
      "MSE loss: 92.6326\n",
      "Iteration: 49800\n",
      "Gradient: [ -8.2665 -24.331   -3.2766 -15.688    7.737 ]\n",
      "Weights: [-4.7876  0.6107 -1.0697  0.0529  0.1517]\n",
      "MSE loss: 93.0047\n",
      "Iteration: 49900\n",
      "Gradient: [  4.5025  -7.2258  44.764   23.0484 -62.7508]\n",
      "Weights: [-4.7651  0.603  -1.0683  0.0539  0.1518]\n",
      "MSE loss: 92.8474\n",
      "Iteration: 50000\n",
      "Gradient: [  6.2127  -8.4379  27.3164 117.9563 -92.9382]\n",
      "Weights: [-4.7578  0.5923 -1.0686  0.0534  0.1518]\n",
      "MSE loss: 92.616\n",
      "Iteration: 50100\n",
      "Gradient: [  6.6662  -8.6895 -60.8522 -73.2873 -29.367 ]\n",
      "Weights: [-4.7664  0.6028 -1.0712  0.0535  0.1521]\n",
      "MSE loss: 92.5937\n",
      "Iteration: 50200\n",
      "Gradient: [  -1.3881   -7.0732   16.5521  -73.4634 -100.4436]\n",
      "Weights: [-4.7641  0.5994 -1.0722  0.0531  0.1522]\n",
      "MSE loss: 92.634\n",
      "Iteration: 50300\n",
      "Gradient: [   2.4446  -16.8839  -33.2906  -88.9068 -333.1916]\n",
      "Weights: [-4.7649  0.6061 -1.0721  0.0528  0.1522]\n",
      "MSE loss: 92.5688\n",
      "Iteration: 50400\n",
      "Gradient: [  -3.9351   24.9864   -7.1221  -90.7458 -246.1641]\n",
      "Weights: [-4.796   0.6203 -1.0719  0.0523  0.152 ]\n",
      "MSE loss: 93.064\n",
      "Iteration: 50500\n",
      "Gradient: [ 2.2072  7.3746 35.3448 -1.5712 54.2528]\n",
      "Weights: [-4.7718  0.6168 -1.0741  0.0522  0.1522]\n",
      "MSE loss: 92.5533\n",
      "Iteration: 50600\n",
      "Gradient: [-4.7594  1.381  22.812  15.2241 29.3319]\n",
      "Weights: [-4.7599  0.6122 -1.0711  0.0515  0.1522]\n",
      "MSE loss: 92.7324\n",
      "Iteration: 50700\n",
      "Gradient: [  9.2407  14.85    37.1167 -44.4342 229.6829]\n",
      "Weights: [-4.7728  0.6137 -1.0735  0.0512  0.1525]\n",
      "MSE loss: 92.6335\n",
      "Iteration: 50800\n",
      "Gradient: [  -5.75    -13.0863  -19.3489   -1.5939 -360.3249]\n",
      "Weights: [-4.7638  0.6142 -1.0747  0.0517  0.1522]\n",
      "MSE loss: 92.6217\n",
      "Iteration: 50900\n",
      "Gradient: [ -7.9455   8.0689 -11.5803 -57.4024  17.6701]\n",
      "Weights: [-4.7728  0.6199 -1.0745  0.0515  0.1523]\n",
      "MSE loss: 92.5784\n",
      "Iteration: 51000\n",
      "Gradient: [  -1.3133  -15.2796  -49.4966   30.4648 -122.2397]\n",
      "Weights: [-4.7633  0.6069 -1.0738  0.051   0.1527]\n",
      "MSE loss: 92.6585\n",
      "Iteration: 51100\n",
      "Gradient: [ -4.1753   4.7125  -7.6488  74.1317 157.0361]\n",
      "Weights: [-4.7649  0.6025 -1.0717  0.0516  0.1526]\n",
      "MSE loss: 92.6314\n",
      "Iteration: 51200\n",
      "Gradient: [  0.6551   3.4867  15.4939  43.0384 210.8473]\n",
      "Weights: [-4.7616  0.6073 -1.0724  0.0514  0.1525]\n",
      "MSE loss: 92.5757\n",
      "Iteration: 51300\n",
      "Gradient: [   0.7837   19.5102   14.028    58.2263 -110.9862]\n",
      "Weights: [-4.7648  0.6096 -1.0751  0.0513  0.1528]\n",
      "MSE loss: 92.5756\n",
      "Iteration: 51400\n",
      "Gradient: [  1.4486  -1.0089  28.1661 -34.2143 225.0841]\n",
      "Weights: [-4.7309  0.598  -1.0776  0.0521  0.1529]\n",
      "MSE loss: 93.172\n",
      "Iteration: 51500\n",
      "Gradient: [ -4.1871   0.4559  32.7287  42.1091 -16.2963]\n",
      "Weights: [-4.7589  0.5993 -1.077   0.0532  0.1527]\n",
      "MSE loss: 92.7303\n",
      "Iteration: 51600\n",
      "Gradient: [  -2.6165   -4.2648  -25.9838  -87.4312 -177.9184]\n",
      "Weights: [-4.7598  0.6043 -1.0753  0.0526  0.1524]\n",
      "MSE loss: 92.6149\n",
      "Iteration: 51700\n",
      "Gradient: [ -1.6504 -17.1975  22.0349  16.5295 -50.3145]\n",
      "Weights: [-4.7624  0.6182 -1.0785  0.0531  0.1522]\n",
      "MSE loss: 92.5845\n",
      "Iteration: 51800\n",
      "Gradient: [-12.4007  13.2046 -18.0216   8.6398  19.0289]\n",
      "Weights: [-4.7845  0.6295 -1.0818  0.0532  0.1523]\n",
      "MSE loss: 92.6498\n",
      "Iteration: 51900\n",
      "Gradient: [ 14.261   -8.7414  22.8651  35.0202 377.4876]\n",
      "Weights: [-4.7658  0.6212 -1.0814  0.0531  0.1526]\n",
      "MSE loss: 92.5431\n",
      "Iteration: 52000\n",
      "Gradient: [ 0.7989  4.8617 27.2286 65.1292  6.1512]\n",
      "Weights: [-4.7617  0.6282 -1.0848  0.0533  0.1526]\n",
      "MSE loss: 92.6679\n",
      "Iteration: 52100\n",
      "Gradient: [ 2.000000e-02 -2.466800e+00 -3.115590e+01 -1.150486e+02  9.771930e+01]\n",
      "Weights: [-4.7672  0.6192 -1.0818  0.0532  0.1526]\n",
      "MSE loss: 92.5235\n",
      "Iteration: 52200\n",
      "Gradient: [  -8.0734  -10.8981    5.7848  -31.2159 -405.1707]\n",
      "Weights: [-4.7633  0.6164 -1.0836  0.0532  0.1528]\n",
      "MSE loss: 92.6213\n",
      "Iteration: 52300\n",
      "Gradient: [  1.4432   1.5544 -32.8377 -44.7428  71.1271]\n",
      "Weights: [-4.7649  0.6058 -1.0798  0.0538  0.1528]\n",
      "MSE loss: 92.6536\n",
      "Iteration: 52400\n",
      "Gradient: [   3.9121  -18.3115    9.6554   43.2472 -117.5077]\n",
      "Weights: [-4.7463  0.5935 -1.0774  0.053   0.1528]\n",
      "MSE loss: 92.8242\n",
      "Iteration: 52500\n",
      "Gradient: [  6.057   -9.945  -13.3706 110.7314   1.9577]\n",
      "Weights: [-4.7528  0.5983 -1.0767  0.0533  0.1528]\n",
      "MSE loss: 92.6407\n",
      "Iteration: 52600\n",
      "Gradient: [ 10.9027   7.2023  -3.0608 -44.9067  71.0485]\n",
      "Weights: [-4.7675  0.6035 -1.0766  0.0536  0.1526]\n",
      "MSE loss: 92.673\n",
      "Iteration: 52700\n",
      "Gradient: [   2.1162    2.3452   19.3505 -100.3794 -340.5965]\n",
      "Weights: [-4.7717  0.6075 -1.0759  0.0525  0.1525]\n",
      "MSE loss: 92.8307\n",
      "Iteration: 52800\n",
      "Gradient: [  4.848    4.9331  10.6    102.1642   7.738 ]\n",
      "Weights: [-4.7729  0.6272 -1.0795  0.0519  0.1522]\n",
      "MSE loss: 92.6917\n",
      "Iteration: 52900\n",
      "Gradient: [-0.3244 24.0205 16.1951 21.2495  2.9587]\n",
      "Weights: [-4.7649  0.6168 -1.0793  0.0533  0.1525]\n",
      "MSE loss: 92.5659\n",
      "Iteration: 53000\n",
      "Gradient: [  -3.5071    9.6157  -11.2375    0.3717 -297.3628]\n",
      "Weights: [-4.7676  0.62   -1.0822  0.0529  0.1525]\n",
      "MSE loss: 92.7377\n",
      "Iteration: 53100\n",
      "Gradient: [  -9.1394   -0.5732  -12.5964  -19.501  -133.8551]\n",
      "Weights: [-4.7836  0.6228 -1.0804  0.0534  0.1524]\n",
      "MSE loss: 92.7248\n",
      "Iteration: 53200\n",
      "Gradient: [   7.374    -1.4605   10.5709  -36.5695 -109.03  ]\n",
      "Weights: [-4.7722  0.6327 -1.079   0.0529  0.1521]\n",
      "MSE loss: 92.9276\n",
      "Iteration: 53300\n",
      "Gradient: [ -9.018    3.9818  25.8679 -20.7063 -69.267 ]\n",
      "Weights: [-4.792   0.6376 -1.0804  0.0532  0.1519]\n",
      "MSE loss: 92.6703\n",
      "Iteration: 53400\n",
      "Gradient: [   6.3857   11.866   -33.3567  -42.7144 -114.0003]\n",
      "Weights: [-4.7711  0.6238 -1.0807  0.0535  0.1521]\n",
      "MSE loss: 92.5281\n",
      "Iteration: 53500\n",
      "Gradient: [ -2.9541  -3.6871  24.4316  78.476  401.4004]\n",
      "Weights: [-4.777   0.6222 -1.08    0.0548  0.1519]\n",
      "MSE loss: 92.5391\n",
      "Iteration: 53600\n",
      "Gradient: [  8.1619  -9.9103 -68.44   -24.7088 -98.2276]\n",
      "Weights: [-4.7514  0.6065 -1.0816  0.0547  0.152 ]\n",
      "MSE loss: 92.8206\n",
      "Iteration: 53700\n",
      "Gradient: [  9.3221  29.5627  -6.1378  55.2358 199.8936]\n",
      "Weights: [-4.7458  0.601  -1.0779  0.0549  0.152 ]\n",
      "MSE loss: 92.6845\n",
      "Iteration: 53800\n",
      "Gradient: [  4.6902  11.3431 -14.618   31.8082 191.2422]\n",
      "Weights: [-4.7407  0.5986 -1.0767  0.0551  0.1523]\n",
      "MSE loss: 93.2659\n",
      "Iteration: 53900\n",
      "Gradient: [   9.493    -0.417   -16.8537   63.6464 -204.0104]\n",
      "Weights: [-4.7361  0.6049 -1.0801  0.0542  0.1523]\n",
      "MSE loss: 93.1031\n",
      "Iteration: 54000\n",
      "Gradient: [  2.4052   1.6974  39.0134  32.1007 -32.3123]\n",
      "Weights: [-4.7307  0.6007 -1.0832  0.0555  0.1526]\n",
      "MSE loss: 93.377\n",
      "Iteration: 54100\n",
      "Gradient: [ -5.3893  26.7955 -10.3812 146.9574  12.2453]\n",
      "Weights: [-4.748   0.6085 -1.0841  0.0558  0.1525]\n",
      "MSE loss: 92.8287\n",
      "Iteration: 54200\n",
      "Gradient: [  2.7821  21.9817  34.0403 -49.7584 290.5298]\n",
      "Weights: [-4.7653  0.616  -1.0879  0.0571  0.1522]\n",
      "MSE loss: 92.5107\n",
      "Iteration: 54300\n",
      "Gradient: [  0.1446  14.222   47.2083  91.4628 104.6351]\n",
      "Weights: [-4.7689  0.6357 -1.0909  0.0571  0.1521]\n",
      "MSE loss: 92.8517\n",
      "Iteration: 54400\n",
      "Gradient: [ -3.3063  -9.3322  79.088  109.6762   5.7757]\n",
      "Weights: [-4.773   0.6358 -1.0962  0.0576  0.1523]\n",
      "MSE loss: 92.4412\n",
      "Iteration: 54500\n",
      "Gradient: [ -3.4787  -5.41    -9.7501 -68.3818 -35.3167]\n",
      "Weights: [-4.778   0.6365 -1.0998  0.0583  0.1525]\n",
      "MSE loss: 92.5601\n",
      "Iteration: 54600\n",
      "Gradient: [  2.1006  -4.9521 -13.5046  82.5709 129.9422]\n",
      "Weights: [-4.762   0.6304 -1.0966  0.0583  0.1522]\n",
      "MSE loss: 92.4874\n",
      "Iteration: 54700\n",
      "Gradient: [  -2.4758   -7.9256  -29.1898 -152.2125 -124.0152]\n",
      "Weights: [-4.7934  0.6386 -1.0991  0.0586  0.1523]\n",
      "MSE loss: 93.0827\n",
      "Iteration: 54800\n",
      "Gradient: [  4.5538  16.7995  -7.8381 -47.7388  12.4711]\n",
      "Weights: [-4.7657  0.6221 -1.0973  0.0591  0.1526]\n",
      "MSE loss: 92.5776\n",
      "Iteration: 54900\n",
      "Gradient: [ -7.4778  14.3673 -37.2496  29.8045  10.0515]\n",
      "Weights: [-4.7752  0.6298 -1.0959  0.0579  0.1524]\n",
      "MSE loss: 92.5497\n",
      "Iteration: 55000\n",
      "Gradient: [   4.892    16.3479   50.5632 -133.677  -147.0483]\n",
      "Weights: [-4.743   0.6241 -1.0986  0.0586  0.1525]\n",
      "MSE loss: 92.9738\n",
      "Iteration: 55100\n",
      "Gradient: [  -2.9807  -27.7791  -37.4566  -69.0419 -209.1613]\n",
      "Weights: [-4.7553  0.6144 -1.0986  0.0592  0.1526]\n",
      "MSE loss: 92.7244\n",
      "Iteration: 55200\n",
      "Gradient: [ -3.8554  10.7407 -65.19    71.398  -76.832 ]\n",
      "Weights: [-4.7604  0.6218 -1.0962  0.0589  0.1525]\n",
      "MSE loss: 92.5677\n",
      "Iteration: 55300\n",
      "Gradient: [  -3.385   -16.2783   13.7276  110.3069 -131.8378]\n",
      "Weights: [-4.7748  0.6271 -1.0971  0.0588  0.1524]\n",
      "MSE loss: 92.6246\n",
      "Iteration: 55400\n",
      "Gradient: [  2.2799   7.1263  -0.2553 106.9036  28.7551]\n",
      "Weights: [-4.7643  0.6333 -1.0965  0.0579  0.1523]\n",
      "MSE loss: 92.4883\n",
      "Iteration: 55500\n",
      "Gradient: [  -8.2551   -8.6623   13.9469  -19.9893 -247.5529]\n",
      "Weights: [-4.7588  0.6196 -1.0942  0.0582  0.1523]\n",
      "MSE loss: 92.527\n",
      "Iteration: 55600\n",
      "Gradient: [ -4.1628  21.2732  -0.3273 -80.2234 -48.5898]\n",
      "Weights: [-4.7673  0.6238 -1.0947  0.0587  0.1523]\n",
      "MSE loss: 92.5126\n",
      "Iteration: 55700\n",
      "Gradient: [  2.9212 -10.8407 -13.0956 -51.6093  47.1053]\n",
      "Weights: [-4.7585  0.6236 -1.0982  0.0581  0.1523]\n",
      "MSE loss: 92.8578\n",
      "Iteration: 55800\n",
      "Gradient: [   1.2616   -7.23     18.1676  124.0968 -113.3946]\n",
      "Weights: [-4.7647  0.6203 -1.0952  0.0585  0.1524]\n",
      "MSE loss: 92.56\n",
      "Iteration: 55900\n",
      "Gradient: [-1.757000e-01  1.061570e+01 -2.967100e+01  4.907970e+01 -2.332076e+02]\n",
      "Weights: [-4.7741  0.6353 -1.0979  0.0582  0.1524]\n",
      "MSE loss: 92.4582\n",
      "Iteration: 56000\n",
      "Gradient: [   6.1747   -5.7749   11.5053  -16.1527 -180.248 ]\n",
      "Weights: [-4.7801  0.6345 -1.0942  0.058   0.152 ]\n",
      "MSE loss: 92.4968\n",
      "Iteration: 56100\n",
      "Gradient: [ -6.3663   4.9384  14.4414 -21.0214  69.8085]\n",
      "Weights: [-4.79    0.6516 -1.0981  0.0572  0.1521]\n",
      "MSE loss: 92.5224\n",
      "Iteration: 56200\n",
      "Gradient: [9.240000e-02 5.034200e+00 6.670020e+01 6.620130e+01 1.744023e+02]\n",
      "Weights: [-4.79    0.6493 -1.0993  0.0581  0.1523]\n",
      "MSE loss: 92.5546\n",
      "Iteration: 56300\n",
      "Gradient: [ -7.7385   2.5103  14.6449 -53.607   74.1067]\n",
      "Weights: [-4.7693  0.6465 -1.0999  0.0577  0.1523]\n",
      "MSE loss: 92.5656\n",
      "Iteration: 56400\n",
      "Gradient: [-7.0907 -3.4644 -4.1358 64.1725 58.7659]\n",
      "Weights: [-4.7826  0.6443 -1.1004  0.0581  0.1525]\n",
      "MSE loss: 92.4849\n",
      "Iteration: 56500\n",
      "Gradient: [-5.0971 10.2256 28.7507 19.6553 46.556 ]\n",
      "Weights: [-4.7701  0.6353 -1.1009  0.0592  0.1523]\n",
      "MSE loss: 92.4442\n",
      "Iteration: 56600\n",
      "Gradient: [  2.0678 -12.0193 -10.6825 132.4219 267.1789]\n",
      "Weights: [-4.7608  0.6397 -1.1012  0.0594  0.1521]\n",
      "MSE loss: 92.6772\n",
      "Iteration: 56700\n",
      "Gradient: [ 8.9766 19.9017  7.6716 21.1072 46.7931]\n",
      "Weights: [-4.7698  0.6511 -1.1023  0.059   0.1519]\n",
      "MSE loss: 92.6021\n",
      "Iteration: 56800\n",
      "Gradient: [  -1.553   -10.5349  -44.7865  -66.5747 -241.7964]\n",
      "Weights: [-4.787   0.6611 -1.1068  0.0589  0.1519]\n",
      "MSE loss: 92.5741\n",
      "Iteration: 56900\n",
      "Gradient: [  0.7681 -14.3955  15.975  -43.3483  84.8855]\n",
      "Weights: [-4.7683  0.6593 -1.1099  0.0588  0.1524]\n",
      "MSE loss: 92.5819\n",
      "Iteration: 57000\n",
      "Gradient: [ -0.8073  13.3268  32.1141  93.9816 145.668 ]\n",
      "Weights: [-4.7822  0.6606 -1.1047  0.0583  0.1523]\n",
      "MSE loss: 92.5978\n",
      "Iteration: 57100\n",
      "Gradient: [-3.327900e+00 -1.031880e+01 -1.760000e+00 -1.126000e-01 -2.503606e+02]\n",
      "Weights: [-4.7933  0.6493 -1.1032  0.0585  0.1522]\n",
      "MSE loss: 93.0364\n",
      "Iteration: 57200\n",
      "Gradient: [ -4.3713   2.08    23.5498  80.5779 103.5196]\n",
      "Weights: [-4.7733  0.6475 -1.1056  0.0593  0.1522]\n",
      "MSE loss: 92.4234\n",
      "Iteration: 57300\n",
      "Gradient: [  6.4924  14.2645 -23.5052  59.0522 -23.0282]\n",
      "Weights: [-4.7697  0.6498 -1.1053  0.059   0.1524]\n",
      "MSE loss: 92.5142\n",
      "Iteration: 57400\n",
      "Gradient: [  -3.6849    6.0597    8.0455   18.0262 -133.1132]\n",
      "Weights: [-4.7772  0.6531 -1.1068  0.058   0.1526]\n",
      "MSE loss: 92.4581\n",
      "Iteration: 57500\n",
      "Gradient: [  2.9883   0.3314  -0.8588 -47.0605 -81.2994]\n",
      "Weights: [-4.7721  0.6465 -1.1048  0.0582  0.1528]\n",
      "MSE loss: 92.4878\n",
      "Iteration: 57600\n",
      "Gradient: [ -4.2666   7.8477 -39.3918  17.7238 -79.1901]\n",
      "Weights: [-4.7888  0.6518 -1.1054  0.0579  0.1527]\n",
      "MSE loss: 92.7135\n",
      "Iteration: 57700\n",
      "Gradient: [  5.9642  -3.3174 -55.184   30.8176  70.2925]\n",
      "Weights: [-4.7677  0.6403 -1.1022  0.0587  0.1525]\n",
      "MSE loss: 92.5109\n",
      "Iteration: 57800\n",
      "Gradient: [  -6.708    -8.5796  -11.9477  -44.4944 -120.9584]\n",
      "Weights: [-4.7683  0.6457 -1.1064  0.0587  0.1526]\n",
      "MSE loss: 92.4641\n",
      "Iteration: 57900\n",
      "Gradient: [  5.7435   8.1765 -10.5049  66.3318 149.9077]\n",
      "Weights: [-4.7693  0.6437 -1.1068  0.0591  0.1529]\n",
      "MSE loss: 92.5\n",
      "Iteration: 58000\n",
      "Gradient: [  -3.8608    9.2754    2.4766  -72.8133 -155.4133]\n",
      "Weights: [-4.7583  0.6323 -1.1086  0.0596  0.153 ]\n",
      "MSE loss: 92.6352\n",
      "Iteration: 58100\n",
      "Gradient: [ 12.5843   2.7241   4.0263  -3.4065 217.1724]\n",
      "Weights: [-4.7625  0.6324 -1.1061  0.0601  0.1528]\n",
      "MSE loss: 92.5596\n",
      "Iteration: 58200\n",
      "Gradient: [  -8.9425   -7.4886  -43.2112   15.271  -134.5726]\n",
      "Weights: [-4.7722  0.6396 -1.1074  0.0601  0.1524]\n",
      "MSE loss: 92.6704\n",
      "Iteration: 58300\n",
      "Gradient: [ -3.4295   3.9776 -14.3693 -24.2166 -83.0981]\n",
      "Weights: [-4.7465  0.6325 -1.1085  0.0603  0.1528]\n",
      "MSE loss: 92.7982\n",
      "Iteration: 58400\n",
      "Gradient: [  -6.0097   -0.7974  -31.0855  -51.6062 -128.9767]\n",
      "Weights: [-4.7612  0.624  -1.1106  0.0619  0.1529]\n",
      "MSE loss: 92.9611\n",
      "Iteration: 58500\n",
      "Gradient: [   2.4697   -5.5469  -15.0569   61.7969 -202.5275]\n",
      "Weights: [-4.7567  0.6321 -1.1078  0.0611  0.1526]\n",
      "MSE loss: 92.5494\n",
      "Iteration: 58600\n",
      "Gradient: [ 2.6488 -6.6987 35.7728 -3.2061 27.6338]\n",
      "Weights: [-4.7577  0.6405 -1.1105  0.0607  0.1526]\n",
      "MSE loss: 92.5321\n",
      "Iteration: 58700\n",
      "Gradient: [   6.5492  -13.6224   17.1691   11.0981 -200.3319]\n",
      "Weights: [-4.7739  0.6446 -1.1106  0.0614  0.1524]\n",
      "MSE loss: 92.4556\n",
      "Iteration: 58800\n",
      "Gradient: [ -4.4557  -5.0605 -55.2634 -22.9509  67.167 ]\n",
      "Weights: [-4.7748  0.6371 -1.1101  0.0615  0.1524]\n",
      "MSE loss: 92.851\n",
      "Iteration: 58900\n",
      "Gradient: [  2.3095   2.4039 -38.4121 -55.833  -70.8649]\n",
      "Weights: [-4.7654  0.6381 -1.106   0.0599  0.1523]\n",
      "MSE loss: 92.501\n",
      "Iteration: 59000\n",
      "Gradient: [ -5.5051  -3.444  -23.0624  -5.0336 309.1456]\n",
      "Weights: [-4.7703  0.6526 -1.1104  0.0601  0.1522]\n",
      "MSE loss: 92.467\n",
      "Iteration: 59100\n",
      "Gradient: [  2.5897 -11.5419  23.3361 -56.7633 -21.3458]\n",
      "Weights: [-4.776   0.6606 -1.1113  0.0601  0.1523]\n",
      "MSE loss: 92.4545\n",
      "Iteration: 59200\n",
      "Gradient: [ -8.9396  -9.8662 -10.6963  39.7329 363.6969]\n",
      "Weights: [-4.7687  0.6632 -1.1155  0.0608  0.1526]\n",
      "MSE loss: 92.7169\n",
      "Iteration: 59300\n",
      "Gradient: [  3.5528  -4.8782   2.1745  43.0782 174.4119]\n",
      "Weights: [-4.7762  0.6606 -1.1141  0.0601  0.1526]\n",
      "MSE loss: 92.4009\n",
      "Iteration: 59400\n",
      "Gradient: [ -1.3285   2.1342  60.1136 -54.5968  22.6801]\n",
      "Weights: [-4.7953  0.6701 -1.1126  0.0593  0.1525]\n",
      "MSE loss: 92.4895\n",
      "Iteration: 59500\n",
      "Gradient: [ -6.1282   0.6666 -34.391  -26.9115   6.3321]\n",
      "Weights: [-4.7907  0.6736 -1.1144  0.06    0.1524]\n",
      "MSE loss: 92.4453\n",
      "Iteration: 59600\n",
      "Gradient: [  2.2625   1.748  -34.977  -25.8949  36.4445]\n",
      "Weights: [-4.7693  0.6599 -1.1136  0.0599  0.1526]\n",
      "MSE loss: 92.5063\n",
      "Iteration: 59700\n",
      "Gradient: [ -2.2826  -4.3891  30.4299  43.1909 125.8565]\n",
      "Weights: [-4.7914  0.6674 -1.1154  0.0604  0.1527]\n",
      "MSE loss: 92.471\n",
      "Iteration: 59800\n",
      "Gradient: [  1.4173 -16.7653 -30.049  -40.2471 257.8571]\n",
      "Weights: [-4.7672  0.6597 -1.115   0.0599  0.1526]\n",
      "MSE loss: 92.5114\n",
      "Iteration: 59900\n",
      "Gradient: [  -9.1254   12.4435   -2.5955   44.9119 -260.9652]\n",
      "Weights: [-4.7908  0.6592 -1.1146  0.0608  0.1526]\n",
      "MSE loss: 92.6826\n",
      "Iteration: 60000\n",
      "Gradient: [   4.2382  -20.0956  -36.7186  -46.1697 -331.2339]\n",
      "Weights: [-4.7632  0.6531 -1.1136  0.0602  0.1524]\n",
      "MSE loss: 92.5854\n",
      "Iteration: 60100\n",
      "Gradient: [ -0.1775  -0.3148   5.8561  12.4805 -48.9779]\n",
      "Weights: [-4.7859  0.6598 -1.1116  0.0605  0.1522]\n",
      "MSE loss: 92.4294\n",
      "Iteration: 60200\n",
      "Gradient: [  0.4466 -11.1712  29.4129  93.1521 189.9902]\n",
      "Weights: [-4.7842  0.6641 -1.1123  0.0608  0.152 ]\n",
      "MSE loss: 92.3925\n",
      "Iteration: 60300\n",
      "Gradient: [  1.4462  -9.0931 -18.6079 -37.907  -47.6162]\n",
      "Weights: [-4.782   0.6602 -1.1116  0.061   0.152 ]\n",
      "MSE loss: 92.376\n",
      "Iteration: 60400\n",
      "Gradient: [  -2.2103    8.2286   18.5498   31.9111 -235.9036]\n",
      "Weights: [-4.7933  0.6674 -1.1131  0.0611  0.152 ]\n",
      "MSE loss: 92.4452\n",
      "Iteration: 60500\n",
      "Gradient: [  1.7685  -2.0939 -41.7969  88.0176 156.1407]\n",
      "Weights: [-4.7954  0.668  -1.1142  0.0614  0.1519]\n",
      "MSE loss: 92.5615\n",
      "Iteration: 60600\n",
      "Gradient: [-11.9495 -10.7392  21.6473  33.5498   8.5168]\n",
      "Weights: [-4.7864  0.6668 -1.1142  0.0612  0.1519]\n",
      "MSE loss: 92.4255\n",
      "Iteration: 60700\n",
      "Gradient: [  -5.6908  -20.8512  -40.0395  -56.808  -125.7844]\n",
      "Weights: [-4.7895  0.6709 -1.1175  0.0617  0.1518]\n",
      "MSE loss: 92.6083\n",
      "Iteration: 60800\n",
      "Gradient: [ 3.376000e-01 -1.093860e+01 -2.301950e+01 -8.060400e+00  5.949203e+02]\n",
      "Weights: [-4.7945  0.6745 -1.1189  0.0631  0.1517]\n",
      "MSE loss: 92.3964\n",
      "Iteration: 60900\n",
      "Gradient: [ -6.4009   8.1739 -30.1248 -43.5584   8.2721]\n",
      "Weights: [-4.7881  0.676  -1.1214  0.0622  0.1519]\n",
      "MSE loss: 92.6071\n",
      "Iteration: 61000\n",
      "Gradient: [  5.6204  19.2609 -56.9473 -17.3822 -36.7669]\n",
      "Weights: [-4.7895  0.6827 -1.123   0.0629  0.1518]\n",
      "MSE loss: 92.3915\n",
      "Iteration: 61100\n",
      "Gradient: [  -4.371     1.1936  -18.1855   -8.3063 -242.0341]\n",
      "Weights: [-4.8019  0.6859 -1.1235  0.0634  0.1516]\n",
      "MSE loss: 92.4945\n",
      "Iteration: 61200\n",
      "Gradient: [   4.7993    8.9383   14.3405   29.6345 -117.9895]\n",
      "Weights: [-4.7827  0.6799 -1.1245  0.0638  0.1519]\n",
      "MSE loss: 92.4243\n",
      "Iteration: 61300\n",
      "Gradient: [ 10.6923   6.5662  62.6263  93.9324 157.0698]\n",
      "Weights: [-4.771   0.6789 -1.1279  0.0643  0.1522]\n",
      "MSE loss: 92.8062\n",
      "Iteration: 61400\n",
      "Gradient: [-1.573600e+00  5.322200e+00 -8.930000e-02  4.596800e+00 -4.343267e+02]\n",
      "Weights: [-4.7816  0.6713 -1.1294  0.0655  0.1522]\n",
      "MSE loss: 92.373\n",
      "Iteration: 61500\n",
      "Gradient: [  6.2167  11.1047 -14.401   90.7469  37.1407]\n",
      "Weights: [-4.7876  0.6719 -1.1301  0.0661  0.1523]\n",
      "MSE loss: 92.4497\n",
      "Iteration: 61600\n",
      "Gradient: [  -7.2843   -9.6281   -6.3742  -79.1748 -206.1868]\n",
      "Weights: [-4.7939  0.6828 -1.1312  0.0659  0.1521]\n",
      "MSE loss: 92.3757\n",
      "Iteration: 61700\n",
      "Gradient: [ -6.0273  -5.23   -24.1731 117.1931 121.6448]\n",
      "Weights: [-4.7849  0.6796 -1.1311  0.0653  0.152 ]\n",
      "MSE loss: 92.3752\n",
      "Iteration: 61800\n",
      "Gradient: [  1.575    7.4385   9.4037 -24.6217 101.1577]\n",
      "Weights: [-4.7582  0.664  -1.1308  0.0654  0.1523]\n",
      "MSE loss: 92.5939\n",
      "Iteration: 61900\n",
      "Gradient: [ -5.2662   1.2609   1.1689 -84.6152 523.976 ]\n",
      "Weights: [-4.7704  0.6711 -1.132   0.0668  0.1521]\n",
      "MSE loss: 92.4264\n",
      "Iteration: 62000\n",
      "Gradient: [ -9.0102  -5.6897 -28.7214  59.7314  70.4476]\n",
      "Weights: [-4.7877  0.6866 -1.1334  0.0664  0.1518]\n",
      "MSE loss: 92.2955\n",
      "Iteration: 62100\n",
      "Gradient: [ -9.7191   2.4171   9.7421  25.9639 -94.0697]\n",
      "Weights: [-4.7732  0.6879 -1.1317  0.0666  0.1515]\n",
      "MSE loss: 93.049\n",
      "Iteration: 62200\n",
      "Gradient: [  -0.3784   -0.4181  -13.3642 -107.1912   62.166 ]\n",
      "Weights: [-4.8068  0.6964 -1.1341  0.0668  0.1516]\n",
      "MSE loss: 92.4413\n",
      "Iteration: 62300\n",
      "Gradient: [ -2.2891   4.9897  32.3003 -25.17    73.5141]\n",
      "Weights: [-4.7837  0.6916 -1.1363  0.0669  0.1518]\n",
      "MSE loss: 92.396\n",
      "Iteration: 62400\n",
      "Gradient: [ -0.24     9.4865 -36.3544 109.8134 130.4567]\n",
      "Weights: [-4.7944  0.6936 -1.1371  0.067   0.1518]\n",
      "MSE loss: 92.3028\n",
      "Iteration: 62500\n",
      "Gradient: [ -9.5644  -2.3747  28.7353  73.7939 -82.3174]\n",
      "Weights: [-4.7991  0.7003 -1.1327  0.0656  0.1519]\n",
      "MSE loss: 92.5925\n",
      "Iteration: 62600\n",
      "Gradient: [ -2.7136  17.7074 -16.2351 -72.5592 206.5385]\n",
      "Weights: [-4.7858  0.688  -1.1311  0.0654  0.1519]\n",
      "MSE loss: 92.3938\n",
      "Iteration: 62700\n",
      "Gradient: [  3.0593  -3.2007  11.8055 -54.4113 116.1875]\n",
      "Weights: [-4.8041  0.7047 -1.1326  0.0655  0.1516]\n",
      "MSE loss: 92.5226\n",
      "Iteration: 62800\n",
      "Gradient: [  9.6964   4.662   15.8742 -52.3338 249.2897]\n",
      "Weights: [-4.7999  0.6981 -1.1334  0.0664  0.1516]\n",
      "MSE loss: 92.4291\n",
      "Iteration: 62900\n",
      "Gradient: [  -1.6118    4.7052  -24.1544 -153.8652 -269.2495]\n",
      "Weights: [-4.7991  0.6822 -1.1344  0.0667  0.1518]\n",
      "MSE loss: 92.9583\n",
      "Iteration: 63000\n",
      "Gradient: [  3.2564  -4.7274 -14.8279 -43.3514  60.8092]\n",
      "Weights: [-4.7809  0.6804 -1.1319  0.0666  0.1518]\n",
      "MSE loss: 92.3317\n",
      "Iteration: 63100\n",
      "Gradient: [ 4.0154 11.0002 -1.5322 18.0388 23.3286]\n",
      "Weights: [-4.7769  0.6845 -1.132   0.0675  0.1515]\n",
      "MSE loss: 92.7457\n",
      "Iteration: 63200\n",
      "Gradient: [  8.4049  -3.3324  41.8044 -58.4926 147.4737]\n",
      "Weights: [-4.7839  0.6791 -1.1307  0.0666  0.1516]\n",
      "MSE loss: 92.2887\n",
      "Iteration: 63300\n",
      "Gradient: [  -4.0143   -7.7841   18.2094  -42.1811 -386.7442]\n",
      "Weights: [-4.7828  0.6822 -1.133   0.0665  0.1517]\n",
      "MSE loss: 92.3205\n",
      "Iteration: 63400\n",
      "Gradient: [ -2.1018  10.3602 -25.3898 -37.9489 285.5286]\n",
      "Weights: [-4.7865  0.6855 -1.1348  0.0673  0.1516]\n",
      "MSE loss: 92.2799\n",
      "Iteration: 63500\n",
      "Gradient: [  1.6828 -11.1946 -24.8775   6.0105  61.6094]\n",
      "Weights: [-4.7903  0.6909 -1.1306  0.0663  0.1514]\n",
      "MSE loss: 92.4078\n",
      "Iteration: 63600\n",
      "Gradient: [ -0.7947  22.4319  -2.5243 -79.0856   4.0436]\n",
      "Weights: [-4.7913  0.6893 -1.1323  0.0674  0.1513]\n",
      "MSE loss: 92.3287\n",
      "Iteration: 63700\n",
      "Gradient: [ -22.1638    5.0513    8.2924 -154.4299   84.3376]\n",
      "Weights: [-4.7958  0.6757 -1.129   0.0676  0.1512]\n",
      "MSE loss: 92.6043\n",
      "Iteration: 63800\n",
      "Gradient: [ 10.6544  -6.2333  31.6397  45.2187 309.0404]\n",
      "Weights: [-4.7769  0.6658 -1.1293  0.0674  0.1515]\n",
      "MSE loss: 92.3949\n",
      "Iteration: 63900\n",
      "Gradient: [  2.6691   3.294  -10.1013  38.8359 -42.7849]\n",
      "Weights: [-4.7851  0.6843 -1.1338  0.0676  0.1515]\n",
      "MSE loss: 92.2829\n",
      "Iteration: 64000\n",
      "Gradient: [ 0.691  -1.1357 32.1289  1.333   7.2455]\n",
      "Weights: [-4.7829  0.6816 -1.1318  0.0678  0.1514]\n",
      "MSE loss: 92.3329\n",
      "Iteration: 64100\n",
      "Gradient: [  1.8751  -5.1992  -0.7768  30.0622 198.1483]\n",
      "Weights: [-4.78    0.6749 -1.1313  0.068   0.1513]\n",
      "MSE loss: 92.2785\n",
      "Iteration: 64200\n",
      "Gradient: [  -3.3927   -1.1234    1.457   -52.6481 -369.6049]\n",
      "Weights: [-4.7751  0.6717 -1.1313  0.0691  0.1512]\n",
      "MSE loss: 92.3452\n",
      "Iteration: 64300\n",
      "Gradient: [  0.7189   7.5891 -30.4028 -75.4227 203.8708]\n",
      "Weights: [-4.7925  0.6738 -1.1308  0.0697  0.1511]\n",
      "MSE loss: 92.3739\n",
      "Iteration: 64400\n",
      "Gradient: [ 9.5275 21.9102 47.4129 68.3147 47.7013]\n",
      "Weights: [-4.7822  0.6692 -1.1302  0.0696  0.1511]\n",
      "MSE loss: 92.2684\n",
      "Iteration: 64500\n",
      "Gradient: [  10.5322   10.8829   26.686  -119.2052 -255.7423]\n",
      "Weights: [-4.7667  0.6686 -1.1316  0.0703  0.151 ]\n",
      "MSE loss: 92.6586\n",
      "Iteration: 64600\n",
      "Gradient: [  -1.0694   12.1951  -12.2671  -56.864  -196.1677]\n",
      "Weights: [-4.7742  0.6567 -1.1303  0.0708  0.1509]\n",
      "MSE loss: 92.3615\n",
      "Iteration: 64700\n",
      "Gradient: [  7.0487  14.4079 -17.5838  23.6302  56.1661]\n",
      "Weights: [-4.7645  0.6606 -1.1301  0.0708  0.151 ]\n",
      "MSE loss: 92.5534\n",
      "Iteration: 64800\n",
      "Gradient: [ -5.4588   0.8971 -34.0717 -37.9343   8.3999]\n",
      "Weights: [-4.7716  0.6623 -1.1315  0.0701  0.151 ]\n",
      "MSE loss: 92.3334\n",
      "Iteration: 64900\n",
      "Gradient: [ 2.6767  2.5728 34.6187 37.3216 86.9825]\n",
      "Weights: [-4.7755  0.6704 -1.1315  0.0695  0.1507]\n",
      "MSE loss: 92.3532\n",
      "Iteration: 65000\n",
      "Gradient: [  -3.7139   -2.403     2.1947   -9.9025 -167.8401]\n",
      "Weights: [-4.7882  0.6719 -1.1321  0.0697  0.1508]\n",
      "MSE loss: 92.5266\n",
      "Iteration: 65100\n",
      "Gradient: [ -1.8796  11.345    5.3703  17.7979 -65.4109]\n",
      "Weights: [-4.7732  0.6605 -1.1288  0.0699  0.1509]\n",
      "MSE loss: 92.2646\n",
      "Iteration: 65200\n",
      "Gradient: [ -4.152   -7.242    7.737   33.2061 286.0139]\n",
      "Weights: [-4.7666  0.6536 -1.1252  0.0702  0.1508]\n",
      "MSE loss: 92.377\n",
      "Iteration: 65300\n",
      "Gradient: [ -6.1237  -5.4676  -7.9244 -46.2631 267.9213]\n",
      "Weights: [-4.7693  0.6475 -1.1232  0.07    0.1508]\n",
      "MSE loss: 92.3036\n",
      "Iteration: 65400\n",
      "Gradient: [  -6.0942   15.7557   29.7583   -9.6987 -305.729 ]\n",
      "Weights: [-4.7699  0.6481 -1.1237  0.0702  0.1506]\n",
      "MSE loss: 92.3184\n",
      "Iteration: 65500\n",
      "Gradient: [ 8.1013 28.7616  5.7224  9.7846 51.7159]\n",
      "Weights: [-4.7484  0.6467 -1.1221  0.0695  0.1506]\n",
      "MSE loss: 92.7993\n",
      "Iteration: 65600\n",
      "Gradient: [ -5.5732  -9.6929 -13.4792  34.4773 187.26  ]\n",
      "Weights: [-4.7897  0.6625 -1.1207  0.0692  0.1503]\n",
      "MSE loss: 92.316\n",
      "Iteration: 65700\n",
      "Gradient: [  -6.8392  -11.3548  -17.0997  -90.9632 -337.1578]\n",
      "Weights: [-4.7842  0.6717 -1.1244  0.0688  0.1502]\n",
      "MSE loss: 92.2976\n",
      "Iteration: 65800\n",
      "Gradient: [ -1.2994  -9.0712   9.5675  66.7455 320.9458]\n",
      "Weights: [-4.7775  0.6658 -1.1233  0.0695  0.1502]\n",
      "MSE loss: 92.3063\n",
      "Iteration: 65900\n",
      "Gradient: [  6.2341   7.7708 -33.373  -32.446   89.5394]\n",
      "Weights: [-4.7792  0.677  -1.1252  0.0697  0.1501]\n",
      "MSE loss: 92.6537\n",
      "Iteration: 66000\n",
      "Gradient: [   0.809   -17.8039  -41.8477  -69.2724 -299.8754]\n",
      "Weights: [-4.7923  0.6734 -1.1237  0.0697  0.1502]\n",
      "MSE loss: 92.3388\n",
      "Iteration: 66100\n",
      "Gradient: [  2.1553  10.8609 -13.0892  -5.5588  22.0917]\n",
      "Weights: [-4.793   0.6703 -1.1247  0.0695  0.1504]\n",
      "MSE loss: 92.3176\n",
      "Iteration: 66200\n",
      "Gradient: [  8.4781   1.7619  10.3035 -87.3994 118.4993]\n",
      "Weights: [-4.7865  0.6676 -1.1242  0.0703  0.1505]\n",
      "MSE loss: 92.6053\n",
      "Iteration: 66300\n",
      "Gradient: [   3.5294   10.7042   26.0202   39.9662 -179.851 ]\n",
      "Weights: [-4.7745  0.6579 -1.124   0.0706  0.1505]\n",
      "MSE loss: 92.4078\n",
      "Iteration: 66400\n",
      "Gradient: [ 1.036  -5.3906  8.9444 58.105  67.3699]\n",
      "Weights: [-4.7732  0.6438 -1.1256  0.0707  0.1507]\n",
      "MSE loss: 92.7588\n",
      "Iteration: 66500\n",
      "Gradient: [  -7.3365    6.6347   -9.456    -6.2661 -243.7147]\n",
      "Weights: [-4.7805  0.6559 -1.1263  0.0711  0.1507]\n",
      "MSE loss: 92.3477\n",
      "Iteration: 66600\n",
      "Gradient: [  -6.8295  -11.9497  -50.445    -0.1429 -111.9248]\n",
      "Weights: [-4.7799  0.6463 -1.1232  0.0699  0.1508]\n",
      "MSE loss: 92.6743\n",
      "Iteration: 66700\n",
      "Gradient: [ 3.919600e+00 -7.320000e-02 -7.259700e+00  8.234800e+00  2.225017e+02]\n",
      "Weights: [-4.7654  0.6481 -1.1226  0.0698  0.1507]\n",
      "MSE loss: 92.3074\n",
      "Iteration: 66800\n",
      "Gradient: [ -1.1688   7.7509   2.6296 -17.7803 148.9797]\n",
      "Weights: [-4.7759  0.6603 -1.1227  0.0699  0.1503]\n",
      "MSE loss: 92.2969\n",
      "Iteration: 66900\n",
      "Gradient: [   5.2045  -18.8191   13.3208 -130.5903  241.2274]\n",
      "Weights: [-4.7839  0.6561 -1.1233  0.0697  0.1505]\n",
      "MSE loss: 92.4301\n",
      "Iteration: 67000\n",
      "Gradient: [-10.102    4.7276 -18.6534 -99.4558 -84.9902]\n",
      "Weights: [-4.7826  0.6476 -1.1216  0.0695  0.1505]\n",
      "MSE loss: 92.9408\n",
      "Iteration: 67100\n",
      "Gradient: [ -11.9889    2.6374  -30.8535 -135.342  -170.9156]\n",
      "Weights: [-4.7743  0.6539 -1.1218  0.0692  0.1507]\n",
      "MSE loss: 92.2623\n",
      "Iteration: 67200\n",
      "Gradient: [ 1.778470e+01  8.280000e-02 -4.491500e+00  1.636063e+02  3.633172e+02]\n",
      "Weights: [-4.7718  0.6588 -1.1234  0.0691  0.1508]\n",
      "MSE loss: 92.362\n",
      "Iteration: 67300\n",
      "Gradient: [  3.052  -13.1589  47.9207  22.9617  12.7238]\n",
      "Weights: [-4.7866  0.6644 -1.1232  0.0692  0.1505]\n",
      "MSE loss: 92.2739\n",
      "Iteration: 67400\n",
      "Gradient: [ 5.500000e-02 -1.604760e+01  1.272800e+00  2.938220e+01 -1.224982e+02]\n",
      "Weights: [-4.7935  0.6759 -1.1283  0.0698  0.1504]\n",
      "MSE loss: 92.3021\n",
      "Iteration: 67500\n",
      "Gradient: [  -5.1804   27.1449    3.1111  -43.2586 -174.0765]\n",
      "Weights: [-4.8072  0.6831 -1.127   0.0689  0.1504]\n",
      "MSE loss: 92.5267\n",
      "Iteration: 67600\n",
      "Gradient: [  7.1441   5.0303   3.4178 107.9913 -34.02  ]\n",
      "Weights: [-4.7837  0.6755 -1.1281  0.0692  0.1505]\n",
      "MSE loss: 92.2645\n",
      "Iteration: 67700\n",
      "Gradient: [   3.272     0.4658   -1.0313   18.5539 -122.7071]\n",
      "Weights: [-4.7902  0.677  -1.1296  0.07    0.1506]\n",
      "MSE loss: 92.2904\n",
      "Iteration: 67800\n",
      "Gradient: [  3.771   19.234  -16.7037  -4.0669 160.7942]\n",
      "Weights: [-4.7875  0.6842 -1.1317  0.0692  0.1508]\n",
      "MSE loss: 92.3209\n",
      "Iteration: 67900\n",
      "Gradient: [ -2.4114 -13.7274 -19.1276  10.7037 227.6134]\n",
      "Weights: [-4.7986  0.6779 -1.1308  0.0692  0.1509]\n",
      "MSE loss: 92.488\n",
      "Iteration: 68000\n",
      "Gradient: [  7.2808  -4.9558  28.6597 -36.8009 548.0507]\n",
      "Weights: [-4.7935  0.6886 -1.1338  0.0685  0.1511]\n",
      "MSE loss: 92.2785\n",
      "Iteration: 68100\n",
      "Gradient: [  3.149   -1.2613 -39.936  -15.6663 -91.5604]\n",
      "Weights: [-4.801   0.6932 -1.132   0.0679  0.1511]\n",
      "MSE loss: 92.362\n",
      "Iteration: 68200\n",
      "Gradient: [ 10.0661  14.3244  -6.4506 -50.9448 269.7271]\n",
      "Weights: [-4.7979  0.6962 -1.1344  0.0681  0.1511]\n",
      "MSE loss: 92.3576\n",
      "Iteration: 68300\n",
      "Gradient: [  8.9364  17.5443  26.6233  71.8463 211.6607]\n",
      "Weights: [-4.7964  0.7072 -1.1377  0.0687  0.1509]\n",
      "MSE loss: 92.5973\n",
      "Iteration: 68400\n",
      "Gradient: [ -3.4237  -1.7986  12.6249 -14.3148 236.8264]\n",
      "Weights: [-4.7907  0.704  -1.1382  0.0688  0.1509]\n",
      "MSE loss: 92.6257\n",
      "Iteration: 68500\n",
      "Gradient: [  9.9579  -8.6334  13.6259  45.8576 144.2993]\n",
      "Weights: [-4.7776  0.689  -1.1386  0.0694  0.1512]\n",
      "MSE loss: 92.4095\n",
      "Iteration: 68600\n",
      "Gradient: [  0.4384  -0.9168 -22.1817  53.3431 -94.1419]\n",
      "Weights: [-4.7867  0.6876 -1.1359  0.0694  0.1512]\n",
      "MSE loss: 92.3459\n",
      "Iteration: 68700\n",
      "Gradient: [  -6.6236   -8.3083   21.6668 -142.1004 -266.968 ]\n",
      "Weights: [-4.7857  0.6716 -1.1324  0.0696  0.1512]\n",
      "MSE loss: 92.3124\n",
      "Iteration: 68800\n",
      "Gradient: [ 12.4832   2.1478  27.8317 -13.7182 171.3846]\n",
      "Weights: [-4.7634  0.6744 -1.1344  0.0689  0.151 ]\n",
      "MSE loss: 92.5709\n",
      "Iteration: 68900\n",
      "Gradient: [  -4.2855   -3.7195    1.1345 -121.4383 -166.1214]\n",
      "Weights: [-4.7855  0.6843 -1.1346  0.069   0.1509]\n",
      "MSE loss: 92.2766\n",
      "Iteration: 69000\n",
      "Gradient: [  0.6348  -7.6083  -8.1755 152.424  341.4291]\n",
      "Weights: [-4.7805  0.6943 -1.1363  0.069   0.1509]\n",
      "MSE loss: 92.6242\n",
      "Iteration: 69100\n",
      "Gradient: [ -6.7041  -2.4794 -29.493   15.0547  98.5656]\n",
      "Weights: [-4.788   0.6903 -1.1374  0.0686  0.1511]\n",
      "MSE loss: 92.3407\n",
      "Iteration: 69200\n",
      "Gradient: [ -5.3414 -23.0706   6.3214 -70.62   348.5071]\n",
      "Weights: [-4.7762  0.6657 -1.1306  0.0681  0.1514]\n",
      "MSE loss: 92.4306\n",
      "Iteration: 69300\n",
      "Gradient: [  1.9489   1.5563  23.7484  36.4394 469.3251]\n",
      "Weights: [-4.765   0.6654 -1.1301  0.0686  0.1514]\n",
      "MSE loss: 92.4605\n",
      "Iteration: 69400\n",
      "Gradient: [-1.7029  3.0489 20.3852 51.237  55.384 ]\n",
      "Weights: [-4.78    0.6764 -1.1314  0.069   0.1512]\n",
      "MSE loss: 92.3595\n",
      "Iteration: 69500\n",
      "Gradient: [  2.3317  -8.4536 -10.981  -41.7713  60.3686]\n",
      "Weights: [-4.7943  0.6816 -1.134   0.0691  0.1511]\n",
      "MSE loss: 92.3857\n",
      "Iteration: 69600\n",
      "Gradient: [   1.9688   -0.6611   10.5224   27.3862 -167.5445]\n",
      "Weights: [-4.7895  0.6765 -1.1347  0.0691  0.1512]\n",
      "MSE loss: 92.4896\n",
      "Iteration: 69700\n",
      "Gradient: [  4.0933   7.3343  35.4069  60.8535 167.8381]\n",
      "Weights: [-4.7835  0.6831 -1.1367  0.0694  0.1513]\n",
      "MSE loss: 92.2475\n",
      "Iteration: 69800\n",
      "Gradient: [ -3.0729   2.9045 -60.1937 135.4352  60.4273]\n",
      "Weights: [-4.7907  0.687  -1.1358  0.0685  0.1512]\n",
      "MSE loss: 92.3333\n",
      "Iteration: 69900\n",
      "Gradient: [   5.8084    4.1453    3.6871   81.6112 -230.687 ]\n",
      "Weights: [-4.8027  0.6917 -1.1339  0.0685  0.1511]\n",
      "MSE loss: 92.3647\n",
      "Iteration: 70000\n",
      "Gradient: [   3.0353  -10.2779  -29.0458 -135.1371 -406.1051]\n",
      "Weights: [-4.8105  0.7024 -1.1388  0.0682  0.1512]\n",
      "MSE loss: 92.6774\n",
      "Iteration: 70100\n",
      "Gradient: [ 6.538  -4.3125  0.6542 43.7357  2.5532]\n",
      "Weights: [-4.8059  0.7004 -1.1392  0.0688  0.1513]\n",
      "MSE loss: 92.3822\n",
      "Iteration: 70200\n",
      "Gradient: [ 14.5688   0.8392  -2.6511  90.9565 -10.13  ]\n",
      "Weights: [-4.7867  0.691  -1.1384  0.0689  0.1512]\n",
      "MSE loss: 92.2827\n",
      "Iteration: 70300\n",
      "Gradient: [  10.6882    8.2709   49.6465    8.5584 -167.916 ]\n",
      "Weights: [-4.795   0.6898 -1.137   0.0696  0.1512]\n",
      "MSE loss: 92.2796\n",
      "Iteration: 70400\n",
      "Gradient: [  3.3134 -12.7355   6.679  -13.3669 307.7169]\n",
      "Weights: [-4.7732  0.6816 -1.1367  0.0696  0.1512]\n",
      "MSE loss: 92.4327\n",
      "Iteration: 70500\n",
      "Gradient: [ -0.5101  -2.9264  19.0057 -49.2364  16.1008]\n",
      "Weights: [-4.7915  0.6828 -1.1375  0.0702  0.1512]\n",
      "MSE loss: 92.2892\n",
      "Iteration: 70600\n",
      "Gradient: [-11.5214  -6.1244 -29.2138 -25.8577 118.154 ]\n",
      "Weights: [-4.7912  0.6858 -1.1399  0.0703  0.1512]\n",
      "MSE loss: 92.3613\n",
      "Iteration: 70700\n",
      "Gradient: [  0.9202  -8.1495   2.0738  41.4905 -61.8448]\n",
      "Weights: [-4.7847  0.6852 -1.1372  0.0702  0.1509]\n",
      "MSE loss: 92.2363\n",
      "Iteration: 70800\n",
      "Gradient: [  2.8745  12.3282 -20.557  -68.8802 202.5763]\n",
      "Weights: [-4.7887  0.69   -1.1377  0.0701  0.1509]\n",
      "MSE loss: 92.2501\n",
      "Iteration: 70900\n",
      "Gradient: [  12.7703   -1.8055   31.8351  103.2804 -273.7984]\n",
      "Weights: [-4.7786  0.6852 -1.1357  0.0697  0.1509]\n",
      "MSE loss: 92.3929\n",
      "Iteration: 71000\n",
      "Gradient: [  -9.1659    4.5237   20.9407   23.8884 -103.4998]\n",
      "Weights: [-4.7824  0.6834 -1.1383  0.0701  0.151 ]\n",
      "MSE loss: 92.2622\n",
      "Iteration: 71100\n",
      "Gradient: [  3.8155 -17.3911  42.5618 -43.685   28.4904]\n",
      "Weights: [-4.7805  0.6788 -1.1398  0.0709  0.1511]\n",
      "MSE loss: 92.2731\n",
      "Iteration: 71200\n",
      "Gradient: [ -6.144   -2.2282 -20.4351 -50.0549 -70.2173]\n",
      "Weights: [-4.7882  0.6883 -1.1399  0.0708  0.1509]\n",
      "MSE loss: 92.2538\n",
      "Iteration: 71300\n",
      "Gradient: [  5.2165  28.1979  -2.4907 -21.5665 -96.5913]\n",
      "Weights: [-4.7816  0.6906 -1.1396  0.0705  0.151 ]\n",
      "MSE loss: 92.3878\n",
      "Iteration: 71400\n",
      "Gradient: [  -7.9685   14.1168   17.6802  -44.1093 -212.4932]\n",
      "Weights: [-4.7752  0.6879 -1.1422  0.0712  0.1507]\n",
      "MSE loss: 92.396\n",
      "Iteration: 71500\n",
      "Gradient: [   7.5608   25.7404   -9.5076  112.5744 -178.4895]\n",
      "Weights: [-4.7849  0.6905 -1.1408  0.0714  0.1508]\n",
      "MSE loss: 92.2539\n",
      "Iteration: 71600\n",
      "Gradient: [  3.4161 -10.8304 -50.5405  93.5904 134.6622]\n",
      "Weights: [-4.7816  0.6902 -1.1389  0.0705  0.1508]\n",
      "MSE loss: 92.3486\n",
      "Iteration: 71700\n",
      "Gradient: [  -0.5719  -12.494   -24.7487 -131.9195  228.3292]\n",
      "Weights: [-4.7834  0.6797 -1.1366  0.0704  0.1509]\n",
      "MSE loss: 92.2407\n",
      "Iteration: 71800\n",
      "Gradient: [-5.760000e-02  1.365400e+00 -6.411000e+00 -3.401740e+01  1.288094e+02]\n",
      "Weights: [-4.7955  0.6945 -1.1404  0.0706  0.1511]\n",
      "MSE loss: 92.2974\n",
      "Iteration: 71900\n",
      "Gradient: [ 11.0493  -4.8496 -60.5321  12.2623  79.1319]\n",
      "Weights: [-4.7812  0.6912 -1.1408  0.0697  0.1512]\n",
      "MSE loss: 92.3071\n",
      "Iteration: 72000\n",
      "Gradient: [-6.3433 -5.7873 49.9385 -2.6715 94.9854]\n",
      "Weights: [-4.7916  0.6825 -1.1355  0.0693  0.1513]\n",
      "MSE loss: 92.2875\n",
      "Iteration: 72100\n",
      "Gradient: [  2.8405   1.1229 -13.3343  40.7909 509.7502]\n",
      "Weights: [-4.7829  0.6821 -1.1344  0.0691  0.151 ]\n",
      "MSE loss: 92.2747\n",
      "Iteration: 72200\n",
      "Gradient: [   3.8343   -2.2133  -48.1336  100.9435 -121.5244]\n",
      "Weights: [-4.7625  0.6673 -1.1336  0.0699  0.1511]\n",
      "MSE loss: 92.4439\n",
      "Iteration: 72300\n",
      "Gradient: [  -1.0285    3.2597   32.528     3.2266 -142.0034]\n",
      "Weights: [-4.7548  0.6593 -1.1325  0.0709  0.151 ]\n",
      "MSE loss: 92.6132\n",
      "Iteration: 72400\n",
      "Gradient: [ -3.498  -10.5458  43.0573 -18.4444 119.8063]\n",
      "Weights: [-4.7674  0.6631 -1.1325  0.0713  0.1509]\n",
      "MSE loss: 92.345\n",
      "Iteration: 72500\n",
      "Gradient: [  7.5993  -5.749   25.3585 -15.3337 127.6727]\n",
      "Weights: [-4.7816  0.6672 -1.1327  0.0712  0.1509]\n",
      "MSE loss: 92.2914\n",
      "Iteration: 72600\n",
      "Gradient: [ -1.5374  -1.7444 -21.757   38.7004 251.8941]\n",
      "Weights: [-4.7783  0.6633 -1.1286  0.0703  0.1507]\n",
      "MSE loss: 92.2524\n",
      "Iteration: 72700\n",
      "Gradient: [  5.3043 -15.2993  10.3695  96.4817 209.0234]\n",
      "Weights: [-4.7651  0.663  -1.1271  0.0698  0.1505]\n",
      "MSE loss: 92.4193\n",
      "Iteration: 72800\n",
      "Gradient: [  8.5232  -1.5293 -38.2112 -33.2677 110.5551]\n",
      "Weights: [-4.7697  0.6535 -1.123   0.0693  0.1505]\n",
      "MSE loss: 92.3052\n",
      "Iteration: 72900\n",
      "Gradient: [  5.7513  -6.0302  28.8068  29.5175 -15.6665]\n",
      "Weights: [-4.7587  0.6397 -1.1219  0.0702  0.1507]\n",
      "MSE loss: 92.3548\n",
      "Iteration: 73000\n",
      "Gradient: [  4.3146  -2.5286   5.9884 -15.0253 110.6529]\n",
      "Weights: [-4.7687  0.653  -1.1214  0.0694  0.1504]\n",
      "MSE loss: 92.2754\n",
      "Iteration: 73100\n",
      "Gradient: [  10.2696   -3.528   -43.9861 -107.402    64.9203]\n",
      "Weights: [-4.7921  0.6707 -1.1219  0.0689  0.1502]\n",
      "MSE loss: 92.3033\n",
      "Iteration: 73200\n",
      "Gradient: [  9.0229  16.7217   7.15   -50.9685 -41.9528]\n",
      "Weights: [-4.7741  0.6557 -1.1193  0.0691  0.1504]\n",
      "MSE loss: 92.3724\n",
      "Iteration: 73300\n",
      "Gradient: [-10.7779   4.2036  -4.5317 -26.2786 288.5959]\n",
      "Weights: [-4.794   0.6643 -1.1225  0.0694  0.1501]\n",
      "MSE loss: 92.6082\n",
      "Iteration: 73400\n",
      "Gradient: [ 17.0703  -9.3547 -22.9808  57.0067  -0.806 ]\n",
      "Weights: [-4.7818  0.6723 -1.1229  0.0695  0.15  ]\n",
      "MSE loss: 92.475\n",
      "Iteration: 73500\n",
      "Gradient: [  -5.6732    4.5877  -18.9603   46.275  -185.5428]\n",
      "Weights: [-4.7835  0.6634 -1.1215  0.0704  0.1498]\n",
      "MSE loss: 92.2464\n",
      "Iteration: 73600\n",
      "Gradient: [ -1.8155   1.1275 -47.6276  -4.5851 -74.3183]\n",
      "Weights: [-4.7816  0.6515 -1.1224  0.0721  0.1498]\n",
      "MSE loss: 92.2921\n",
      "Iteration: 73700\n",
      "Gradient: [  -5.9988  -13.7624  -12.5807 -100.509   197.7172]\n",
      "Weights: [-4.7721  0.647  -1.1197  0.0716  0.1499]\n",
      "MSE loss: 92.2407\n",
      "Iteration: 73800\n",
      "Gradient: [  -3.5038   21.9159    9.2359   68.638  -222.8431]\n",
      "Weights: [-4.777   0.6624 -1.1228  0.0714  0.1498]\n",
      "MSE loss: 92.4096\n",
      "Iteration: 73900\n",
      "Gradient: [ -1.4743  -6.1468  17.2994  32.1782 -11.7176]\n",
      "Weights: [-4.7904  0.667  -1.1239  0.0721  0.1496]\n",
      "MSE loss: 92.2763\n",
      "Iteration: 74000\n",
      "Gradient: [  -3.2156    1.924     7.7115  -74.3369 -121.143 ]\n",
      "Weights: [-4.7751  0.6599 -1.1261  0.0726  0.1495]\n",
      "MSE loss: 92.2276\n",
      "Iteration: 74100\n",
      "Gradient: [  -2.8515  -14.7252   17.8464   13.6367 -223.0904]\n",
      "Weights: [-4.7856  0.6711 -1.1298  0.0737  0.1493]\n",
      "MSE loss: 92.192\n",
      "Iteration: 74200\n",
      "Gradient: [  6.314   20.6732  40.7422 -12.4214  42.836 ]\n",
      "Weights: [-4.7648  0.6578 -1.1301  0.075   0.1495]\n",
      "MSE loss: 92.4384\n",
      "Iteration: 74300\n",
      "Gradient: [  -2.1523   -6.4069  -11.0689   39.4463 -372.9844]\n",
      "Weights: [-4.7781  0.6553 -1.1333  0.0762  0.1494]\n",
      "MSE loss: 92.3087\n",
      "Iteration: 74400\n",
      "Gradient: [ -12.2429    5.2057   -3.8847  -32.4459 -421.7406]\n",
      "Weights: [-4.7725  0.6592 -1.1331  0.0762  0.1494]\n",
      "MSE loss: 92.1999\n",
      "Iteration: 74500\n",
      "Gradient: [   1.3531   -0.3242   16.2005 -106.4818   94.297 ]\n",
      "Weights: [-4.7837  0.6704 -1.1336  0.076   0.1493]\n",
      "MSE loss: 92.2803\n",
      "Iteration: 74600\n",
      "Gradient: [-6.587  24.9584  6.8005 79.335  57.5825]\n",
      "Weights: [-4.7943  0.6697 -1.134   0.0765  0.1494]\n",
      "MSE loss: 92.3771\n",
      "Iteration: 74700\n",
      "Gradient: [-1.375950e+01  1.178000e-01 -5.663110e+01  4.208360e+01  2.315023e+02]\n",
      "Weights: [-4.7709  0.659  -1.1343  0.0759  0.1494]\n",
      "MSE loss: 92.1979\n",
      "Iteration: 74800\n",
      "Gradient: [ 2.6692 10.1921 10.1531 34.1739 19.2069]\n",
      "Weights: [-4.7659  0.6566 -1.1333  0.0763  0.1493]\n",
      "MSE loss: 92.2015\n",
      "Iteration: 74900\n",
      "Gradient: [  -1.0964  -14.9814   14.4524  -42.2619 -300.8754]\n",
      "Weights: [-4.7733  0.663  -1.1363  0.0767  0.1497]\n",
      "MSE loss: 92.3368\n",
      "Iteration: 75000\n",
      "Gradient: [ -10.7278    2.5988  -28.1603 -160.001  -274.4774]\n",
      "Weights: [-4.7816  0.659  -1.1338  0.0768  0.1494]\n",
      "MSE loss: 92.2265\n",
      "Iteration: 75100\n",
      "Gradient: [  8.3427   4.3434  11.6027 -55.1149 151.7053]\n",
      "Weights: [-4.7685  0.6644 -1.1384  0.0774  0.1492]\n",
      "MSE loss: 92.1717\n",
      "Iteration: 75200\n",
      "Gradient: [  14.9896    0.6063   34.5984   26.4548 -198.0792]\n",
      "Weights: [-4.7771  0.6672 -1.1371  0.0774  0.1491]\n",
      "MSE loss: 92.1162\n",
      "Iteration: 75300\n",
      "Gradient: [  -4.9045   -6.4641   15.6953  -32.5485 -250.5939]\n",
      "Weights: [-4.7782  0.6602 -1.1367  0.0775  0.1492]\n",
      "MSE loss: 92.2433\n",
      "Iteration: 75400\n",
      "Gradient: [ -2.5654 -12.7377 -78.2244 -26.1613 -84.9979]\n",
      "Weights: [-4.7816  0.6573 -1.137   0.0775  0.1493]\n",
      "MSE loss: 92.5245\n",
      "Iteration: 75500\n",
      "Gradient: [ -19.7512    5.7784  -39.5804  -65.1798 -186.3638]\n",
      "Weights: [-4.782   0.6674 -1.1399  0.0776  0.1492]\n",
      "MSE loss: 92.2548\n",
      "Iteration: 75600\n",
      "Gradient: [  -6.1968   -4.2245   24.906  -103.6828  -70.4847]\n",
      "Weights: [-4.7664  0.6621 -1.1379  0.0778  0.149 ]\n",
      "MSE loss: 92.1847\n",
      "Iteration: 75700\n",
      "Gradient: [  5.8018 -18.4977  24.2205 -27.9208 186.8283]\n",
      "Weights: [-4.7808  0.6631 -1.1353  0.078   0.1492]\n",
      "MSE loss: 92.2654\n",
      "Iteration: 75800\n",
      "Gradient: [   7.065    -1.9692  -34.1132  -77.2608 -154.7625]\n",
      "Weights: [-4.77    0.6511 -1.1335  0.0782  0.149 ]\n",
      "MSE loss: 92.1545\n",
      "Iteration: 75900\n",
      "Gradient: [ 4.8816 15.2263  4.9386 82.0675 34.5214]\n",
      "Weights: [-4.7547  0.648  -1.1342  0.0778  0.1489]\n",
      "MSE loss: 92.3235\n",
      "Iteration: 76000\n",
      "Gradient: [  2.2651   5.8257  31.3434  78.0118 120.4717]\n",
      "Weights: [-4.77    0.6519 -1.1336  0.0775  0.1489]\n",
      "MSE loss: 92.3347\n",
      "Iteration: 76100\n",
      "Gradient: [  9.3479  -5.4575  -8.4859  51.2703 -40.1397]\n",
      "Weights: [-4.7773  0.6695 -1.133   0.0776  0.1488]\n",
      "MSE loss: 92.7312\n",
      "Iteration: 76200\n",
      "Gradient: [ -1.2152  16.4098  41.434  -68.5192  56.3864]\n",
      "Weights: [-4.7785  0.6665 -1.1373  0.0777  0.1488]\n",
      "MSE loss: 92.192\n",
      "Iteration: 76300\n",
      "Gradient: [  0.3571  12.3288  12.3706 -16.8222 -29.6696]\n",
      "Weights: [-4.7582  0.6594 -1.1372  0.0784  0.149 ]\n",
      "MSE loss: 92.4693\n",
      "Iteration: 76400\n",
      "Gradient: [  3.5493  21.3111  22.8418  35.8379 217.9956]\n",
      "Weights: [-4.7734  0.6606 -1.1351  0.0786  0.1487]\n",
      "MSE loss: 92.1295\n",
      "Iteration: 76500\n",
      "Gradient: [ -9.1999  -4.2767  28.0639 -75.661    2.0878]\n",
      "Weights: [-4.7983  0.6748 -1.1374  0.078   0.1486]\n",
      "MSE loss: 92.4887\n",
      "Iteration: 76600\n",
      "Gradient: [ 10.0872  21.8795 -13.3519  96.2208  10.4893]\n",
      "Weights: [-4.7885  0.6755 -1.1364  0.0776  0.1484]\n",
      "MSE loss: 92.2277\n",
      "Iteration: 76700\n",
      "Gradient: [  -6.86      0.5712  -59.8372  -45.9204 -170.8686]\n",
      "Weights: [-4.7783  0.6636 -1.1328  0.0775  0.1484]\n",
      "MSE loss: 92.2275\n",
      "Iteration: 76800\n",
      "Gradient: [   4.7741    5.4855   41.0601  -62.435  -162.1548]\n",
      "Weights: [-4.7631  0.6508 -1.1301  0.0782  0.1484]\n",
      "MSE loss: 92.2126\n",
      "Iteration: 76900\n",
      "Gradient: [   5.5576  -18.4665   23.8375  -82.3303 -189.2182]\n",
      "Weights: [-4.7638  0.6593 -1.1322  0.078   0.1482]\n",
      "MSE loss: 92.3201\n",
      "Iteration: 77000\n",
      "Gradient: [-17.0536   1.1525   8.7736 -15.2672 150.2498]\n",
      "Weights: [-4.7819  0.6588 -1.1298  0.078   0.1483]\n",
      "MSE loss: 92.146\n",
      "Iteration: 77100\n",
      "Gradient: [   3.1988   -8.3763  -57.315   -57.1309 -192.4957]\n",
      "Weights: [-4.7795  0.6467 -1.1294  0.0788  0.1485]\n",
      "MSE loss: 92.2827\n",
      "Iteration: 77200\n",
      "Gradient: [  -2.3724   -5.4226   23.6272   35.4294 -201.8143]\n",
      "Weights: [-4.7625  0.6479 -1.1312  0.0777  0.1487]\n",
      "MSE loss: 92.1938\n",
      "Iteration: 77300\n",
      "Gradient: [-0.6867 -4.8073  9.3475  2.1581 49.5972]\n",
      "Weights: [-4.767   0.6554 -1.1311  0.0777  0.1484]\n",
      "MSE loss: 92.1927\n",
      "Iteration: 77400\n",
      "Gradient: [ -0.8626  21.9295 -11.9124  38.8915 -77.2516]\n",
      "Weights: [-4.7678  0.6499 -1.1295  0.0782  0.1482]\n",
      "MSE loss: 92.1807\n",
      "Iteration: 77500\n",
      "Gradient: [  -1.6833  -12.7404  -49.5701  124.6577 -131.3489]\n",
      "Weights: [-4.7783  0.6566 -1.1305  0.079   0.1482]\n",
      "MSE loss: 92.1176\n",
      "Iteration: 77600\n",
      "Gradient: [-5.102  22.9421  6.0713 26.0306 84.7948]\n",
      "Weights: [-4.7603  0.6544 -1.1339  0.0791  0.1484]\n",
      "MSE loss: 92.2963\n",
      "Iteration: 77700\n",
      "Gradient: [  7.1292  -9.5691  25.8644 -44.6477  13.2744]\n",
      "Weights: [-4.7477  0.642  -1.1311  0.0794  0.1483]\n",
      "MSE loss: 92.4589\n",
      "Iteration: 77800\n",
      "Gradient: [  -8.02    -10.2441   -6.3731  -83.6834 -218.7304]\n",
      "Weights: [-4.7823  0.6491 -1.1312  0.08    0.1482]\n",
      "MSE loss: 92.3021\n",
      "Iteration: 77900\n",
      "Gradient: [ 11.0116  16.7205  43.5434   6.1995 -70.9149]\n",
      "Weights: [-4.771   0.6594 -1.1328  0.0802  0.1481]\n",
      "MSE loss: 92.4235\n",
      "Iteration: 78000\n",
      "Gradient: [  -7.7717  -11.734    48.609   -21.7472 -439.6344]\n",
      "Weights: [-4.7941  0.6615 -1.1358  0.0806  0.1481]\n",
      "MSE loss: 92.4924\n",
      "Iteration: 78100\n",
      "Gradient: [  7.0883   4.9204 -10.3118 -60.3921  14.8571]\n",
      "Weights: [-4.7784  0.6551 -1.1329  0.0808  0.148 ]\n",
      "MSE loss: 92.1161\n",
      "Iteration: 78200\n",
      "Gradient: [-17.7849   8.8941  -9.0215  47.8735  39.4185]\n",
      "Weights: [-4.7806  0.6495 -1.1328  0.0814  0.1478]\n",
      "MSE loss: 92.2484\n",
      "Iteration: 78300\n",
      "Gradient: [  2.5373 -12.674   -7.1174 -31.4982 157.1053]\n",
      "Weights: [-4.7671  0.6509 -1.1314  0.0814  0.1477]\n",
      "MSE loss: 92.2653\n",
      "Iteration: 78400\n",
      "Gradient: [  6.1569 -12.5779   4.3942  23.5432 213.4915]\n",
      "Weights: [-4.7545  0.6423 -1.1309  0.0807  0.1478]\n",
      "MSE loss: 92.2526\n",
      "Iteration: 78500\n",
      "Gradient: [  6.0813 -14.5892  12.3714  78.5359 188.6998]\n",
      "Weights: [-4.7933  0.6568 -1.1306  0.0808  0.1478]\n",
      "MSE loss: 92.36\n",
      "Iteration: 78600\n",
      "Gradient: [   8.163     5.5979   21.4763  -72.6552 -309.7102]\n",
      "Weights: [-4.7863  0.6654 -1.1322  0.08    0.1476]\n",
      "MSE loss: 92.1326\n",
      "Iteration: 78700\n",
      "Gradient: [ 4.1787  1.9373 -2.9822 26.4379 29.8808]\n",
      "Weights: [-4.7854  0.6604 -1.1338  0.0811  0.1475]\n",
      "MSE loss: 92.2282\n",
      "Iteration: 78800\n",
      "Gradient: [-5.0422 12.5682 18.6836 84.7482 73.079 ]\n",
      "Weights: [-4.7731  0.6603 -1.1333  0.0814  0.1476]\n",
      "MSE loss: 92.2656\n",
      "Iteration: 78900\n",
      "Gradient: [ 7.6941 13.0606 40.8823 16.5886 -2.4036]\n",
      "Weights: [-4.7764  0.6558 -1.133   0.0818  0.1477]\n",
      "MSE loss: 92.2281\n",
      "Iteration: 79000\n",
      "Gradient: [1.032120e+01 4.159740e+01 1.547560e+01 8.100000e-02 1.933337e+02]\n",
      "Weights: [-4.762   0.6442 -1.1296  0.0825  0.1474]\n",
      "MSE loss: 92.4086\n",
      "Iteration: 79100\n",
      "Gradient: [   5.8991    6.3521   19.2475  150.2425 -226.6917]\n",
      "Weights: [-4.7585  0.6437 -1.1319  0.0835  0.147 ]\n",
      "MSE loss: 92.2099\n",
      "Iteration: 79200\n",
      "Gradient: [  -2.5067  -29.1283  -29.6663   40.0234 -269.0355]\n",
      "Weights: [-4.7676  0.6454 -1.1288  0.0828  0.147 ]\n",
      "MSE loss: 92.1512\n",
      "Iteration: 79300\n",
      "Gradient: [ 11.6719   7.3325  30.2185  41.8664 194.5505]\n",
      "Weights: [-4.78    0.6492 -1.1269  0.0825  0.147 ]\n",
      "MSE loss: 92.2338\n",
      "Iteration: 79400\n",
      "Gradient: [   8.037    -3.2531  -12.4672  105.0293 -401.9458]\n",
      "Weights: [-4.7597  0.6364 -1.1261  0.0822  0.1472]\n",
      "MSE loss: 92.2127\n",
      "Iteration: 79500\n",
      "Gradient: [-11.2557  14.1246  43.4569  -1.0373  73.1624]\n",
      "Weights: [-4.7561  0.6303 -1.1263  0.0824  0.1472]\n",
      "MSE loss: 92.1485\n",
      "Iteration: 79600\n",
      "Gradient: [ -3.6245   0.433  -68.0725  30.3384  19.9791]\n",
      "Weights: [-4.7758  0.6472 -1.1275  0.0823  0.1471]\n",
      "MSE loss: 92.1259\n",
      "Iteration: 79700\n",
      "Gradient: [  1.0296  -5.007  -47.5228  23.1111  91.8837]\n",
      "Weights: [-4.7719  0.6394 -1.1265  0.0816  0.1472]\n",
      "MSE loss: 92.1503\n",
      "Iteration: 79800\n",
      "Gradient: [  5.0506   9.4442  23.4409   9.939  -72.4884]\n",
      "Weights: [-4.7805  0.6535 -1.1276  0.0813  0.1471]\n",
      "MSE loss: 92.1267\n",
      "Iteration: 79900\n",
      "Gradient: [  2.2998   6.744   31.7967 -34.1775 -27.0375]\n",
      "Weights: [-4.7766  0.6546 -1.1298  0.0815  0.1474]\n",
      "MSE loss: 92.2112\n",
      "Iteration: 80000\n",
      "Gradient: [   1.1977  -23.7385  -58.7918   36.2357 -140.2385]\n",
      "Weights: [-4.7684  0.644  -1.1293  0.0819  0.1476]\n",
      "MSE loss: 92.2077\n",
      "Iteration: 80100\n",
      "Gradient: [ -5.5311   7.7549 -22.2632  12.4076 257.5283]\n",
      "Weights: [-4.7727  0.6455 -1.128   0.0813  0.1474]\n",
      "MSE loss: 92.1023\n",
      "Iteration: 80200\n",
      "Gradient: [ -5.4476   2.2444  10.2741 -59.5459 -43.6239]\n",
      "Weights: [-4.7804  0.6508 -1.1305  0.0816  0.1476]\n",
      "MSE loss: 92.1517\n",
      "Iteration: 80300\n",
      "Gradient: [  1.2741   3.1834  11.824   88.9852 270.4076]\n",
      "Weights: [-4.778   0.6573 -1.1324  0.0816  0.1477]\n",
      "MSE loss: 92.2326\n",
      "Iteration: 80400\n",
      "Gradient: [-2.900000e-03  8.018900e+00  1.338500e+01  2.363172e+02  1.040611e+02]\n",
      "Weights: [-4.759   0.6534 -1.1322  0.0813  0.1475]\n",
      "MSE loss: 92.4432\n",
      "Iteration: 80500\n",
      "Gradient: [  2.8185 -14.1497  34.6154 118.9995 -77.9827]\n",
      "Weights: [-4.7845  0.6628 -1.133   0.0819  0.1473]\n",
      "MSE loss: 92.1653\n",
      "Iteration: 80600\n",
      "Gradient: [  -4.6726    2.6885   30.7825   52.5207 -122.8295]\n",
      "Weights: [-4.7785  0.6544 -1.1332  0.082   0.1473]\n",
      "MSE loss: 92.1395\n",
      "Iteration: 80700\n",
      "Gradient: [  10.0098    9.4992   34.1739   44.0657 -222.2686]\n",
      "Weights: [-4.7557  0.6541 -1.1334  0.082   0.1472]\n",
      "MSE loss: 92.5718\n",
      "Iteration: 80800\n",
      "Gradient: [-8.810000e-02 -8.568400e+00  3.329050e+01 -5.700800e+00  1.051936e+02]\n",
      "Weights: [-4.7841  0.6642 -1.1318  0.0813  0.1474]\n",
      "MSE loss: 92.2572\n",
      "Iteration: 80900\n",
      "Gradient: [  3.0488   0.8582  -3.0155 -22.2811  89.249 ]\n",
      "Weights: [-4.7764  0.6587 -1.1343  0.0817  0.1475]\n",
      "MSE loss: 92.073\n",
      "Iteration: 81000\n",
      "Gradient: [  3.5981  10.376  -18.294    1.9616   0.6445]\n",
      "Weights: [-4.7742  0.6598 -1.1338  0.0822  0.1473]\n",
      "MSE loss: 92.1673\n",
      "Iteration: 81100\n",
      "Gradient: [  2.3669  -8.746   12.0091 -46.1032 259.0423]\n",
      "Weights: [-4.7767  0.6651 -1.1397  0.0825  0.1474]\n",
      "MSE loss: 92.1063\n",
      "Iteration: 81200\n",
      "Gradient: [  -3.5203    2.2888   -0.7736   30.7144 -143.3194]\n",
      "Weights: [-4.7785  0.6728 -1.1412  0.0827  0.1474]\n",
      "MSE loss: 92.1086\n",
      "Iteration: 81300\n",
      "Gradient: [ -5.1832  11.2879 -25.7117 -30.6174 244.4969]\n",
      "Weights: [-4.7976  0.6755 -1.1422  0.0831  0.1474]\n",
      "MSE loss: 92.2645\n",
      "Iteration: 81400\n",
      "Gradient: [ -4.8239  11.5137 -25.5442  66.7484 -57.5439]\n",
      "Weights: [-4.7835  0.666  -1.1401  0.0834  0.1472]\n",
      "MSE loss: 92.1247\n",
      "Iteration: 81500\n",
      "Gradient: [ 8.3409 22.5457 23.3014 40.3564 -3.5957]\n",
      "Weights: [-4.7922  0.6828 -1.1416  0.0832  0.1472]\n",
      "MSE loss: 92.2171\n",
      "Iteration: 81600\n",
      "Gradient: [ -3.3251   5.5436 -14.2751   4.9247  76.6075]\n",
      "Weights: [-4.7983  0.6771 -1.1413  0.0828  0.1476]\n",
      "MSE loss: 92.2055\n",
      "Iteration: 81700\n",
      "Gradient: [  4.3454   0.278  -11.2775  -6.1168 149.3493]\n",
      "Weights: [-4.7654  0.6584 -1.1391  0.0831  0.1475]\n",
      "MSE loss: 92.1538\n",
      "Iteration: 81800\n",
      "Gradient: [  -7.5653  -14.4875  -35.2671  -83.5077 -213.7729]\n",
      "Weights: [-4.7989  0.6652 -1.1384  0.0825  0.1475]\n",
      "MSE loss: 92.7076\n",
      "Iteration: 81900\n",
      "Gradient: [  3.1354  -1.5201  -1.295   23.0264 -27.7959]\n",
      "Weights: [-4.7897  0.6723 -1.1384  0.0821  0.1474]\n",
      "MSE loss: 92.1228\n",
      "Iteration: 82000\n",
      "Gradient: [   7.04      2.931   -11.7655   32.5232 -172.7022]\n",
      "Weights: [-4.7774  0.6698 -1.1379  0.0826  0.1474]\n",
      "MSE loss: 92.3546\n",
      "Iteration: 82100\n",
      "Gradient: [ 11.1924  -0.1705  -7.7408  95.4681 153.3381]\n",
      "Weights: [-4.7784  0.6696 -1.1389  0.082   0.1476]\n",
      "MSE loss: 92.1641\n",
      "Iteration: 82200\n",
      "Gradient: [   4.2007   23.9684   41.5006  141.614  -116.076 ]\n",
      "Weights: [-4.763   0.665  -1.1403  0.0827  0.1477]\n",
      "MSE loss: 92.5447\n",
      "Iteration: 82300\n",
      "Gradient: [ -0.7547  -0.0784 -25.5606 -51.0805 -66.4492]\n",
      "Weights: [-4.7803  0.6656 -1.1402  0.0823  0.1478]\n",
      "MSE loss: 92.0514\n",
      "Iteration: 82400\n",
      "Gradient: [   2.3072    0.885     3.4911    9.1353 -253.4668]\n",
      "Weights: [-4.7788  0.6789 -1.1399  0.0811  0.1477]\n",
      "MSE loss: 92.3552\n",
      "Iteration: 82500\n",
      "Gradient: [   0.7856   12.8271   43.293   -27.843  -223.8667]\n",
      "Weights: [-4.7797  0.6637 -1.1376  0.0813  0.1478]\n",
      "MSE loss: 92.0918\n",
      "Iteration: 82600\n",
      "Gradient: [ -9.7867  16.5387  22.1262 -31.271   80.2955]\n",
      "Weights: [-4.7866  0.6637 -1.1386  0.0829  0.1477]\n",
      "MSE loss: 92.1427\n",
      "Iteration: 82700\n",
      "Gradient: [-15.2818   9.8698  31.739  -51.2044 153.7778]\n",
      "Weights: [-4.7854  0.6583 -1.1397  0.0825  0.148 ]\n",
      "MSE loss: 92.3556\n",
      "Iteration: 82800\n",
      "Gradient: [ -2.8671  11.1203  60.6251 -23.1259 136.5432]\n",
      "Weights: [-4.7806  0.6627 -1.1409  0.0823  0.1481]\n",
      "MSE loss: 92.0955\n",
      "Iteration: 82900\n",
      "Gradient: [  0.2737  -6.9016   4.2914  36.7634 -88.9929]\n",
      "Weights: [-4.7771  0.6633 -1.1408  0.0821  0.148 ]\n",
      "MSE loss: 92.0594\n",
      "Iteration: 83000\n",
      "Gradient: [  -0.4131   -8.1886  -10.7829  -41.3707 -289.5933]\n",
      "Weights: [-4.7747  0.6627 -1.1431  0.0821  0.148 ]\n",
      "MSE loss: 92.2435\n",
      "Iteration: 83100\n",
      "Gradient: [   3.9113   -6.501    33.5722  -24.8902 -304.1661]\n",
      "Weights: [-4.7838  0.6672 -1.1423  0.0825  0.1482]\n",
      "MSE loss: 92.1165\n",
      "Iteration: 83200\n",
      "Gradient: [  9.8764  15.2046 -34.2653 -30.1603 405.5614]\n",
      "Weights: [-4.7753  0.6709 -1.1421  0.0817  0.148 ]\n",
      "MSE loss: 92.122\n",
      "Iteration: 83300\n",
      "Gradient: [  -5.7495    2.7764  -54.5224   17.974  -155.3473]\n",
      "Weights: [-4.796   0.6772 -1.1442  0.0822  0.1481]\n",
      "MSE loss: 92.206\n",
      "Iteration: 83400\n",
      "Gradient: [  3.0403 -10.3281  56.3802 -73.2606  85.5732]\n",
      "Weights: [-4.7815  0.6721 -1.144   0.0825  0.1481]\n",
      "MSE loss: 92.0795\n",
      "Iteration: 83500\n",
      "Gradient: [ -8.2669 -16.1804  -1.1392 -30.3093 189.492 ]\n",
      "Weights: [-4.7864  0.6798 -1.1473  0.0825  0.148 ]\n",
      "MSE loss: 92.0651\n",
      "Iteration: 83600\n",
      "Gradient: [  0.2925  -1.0982 -33.4655   7.7199 192.2189]\n",
      "Weights: [-4.7819  0.6794 -1.1491  0.0819  0.1481]\n",
      "MSE loss: 92.3217\n",
      "Iteration: 83700\n",
      "Gradient: [-8.6475 20.1467 12.1319 31.3901 64.8476]\n",
      "Weights: [-4.7855  0.6875 -1.1492  0.082   0.1484]\n",
      "MSE loss: 92.1957\n",
      "Iteration: 83800\n",
      "Gradient: [  -6.8105  -11.2193   -1.9608   96.5589 -127.0047]\n",
      "Weights: [-4.7868  0.6871 -1.1503  0.0819  0.1483]\n",
      "MSE loss: 92.051\n",
      "Iteration: 83900\n",
      "Gradient: [ -0.466    8.8293  10.3186 -11.001   10.7188]\n",
      "Weights: [-4.7882  0.6919 -1.1517  0.0819  0.1483]\n",
      "MSE loss: 92.0616\n",
      "Iteration: 84000\n",
      "Gradient: [  -3.8227    7.5927    2.5402 -151.7179  327.7276]\n",
      "Weights: [-4.793   0.6796 -1.1494  0.0823  0.1485]\n",
      "MSE loss: 92.2196\n",
      "Iteration: 84100\n",
      "Gradient: [ -9.4509 -18.4656  -0.6077 -64.1754  20.1134]\n",
      "Weights: [-4.7953  0.6798 -1.1501  0.0821  0.1485]\n",
      "MSE loss: 92.5409\n",
      "Iteration: 84200\n",
      "Gradient: [   3.5466  -24.8133   -0.8899   18.613  -457.0045]\n",
      "Weights: [-4.7772  0.681  -1.1518  0.0826  0.1484]\n",
      "MSE loss: 92.0492\n",
      "Iteration: 84300\n",
      "Gradient: [  -4.3177    2.4432  -30.312  -113.1461 -147.7296]\n",
      "Weights: [-4.7804  0.6892 -1.1549  0.0818  0.1484]\n",
      "MSE loss: 92.2573\n",
      "Iteration: 84400\n",
      "Gradient: [-1.600000e-03 -9.789700e+00  4.315200e+00 -1.487260e+01 -1.807659e+02]\n",
      "Weights: [-4.7915  0.6891 -1.1543  0.0825  0.1485]\n",
      "MSE loss: 92.1524\n",
      "Iteration: 84500\n",
      "Gradient: [  1.1098  -7.1773   4.3392 -18.157  204.4536]\n",
      "Weights: [-4.7913  0.6912 -1.1511  0.0821  0.1486]\n",
      "MSE loss: 92.2175\n",
      "Iteration: 84600\n",
      "Gradient: [  1.7579 -24.3897  -5.3409   1.3728  30.1592]\n",
      "Weights: [-4.7987  0.703  -1.1519  0.0812  0.1484]\n",
      "MSE loss: 92.1723\n",
      "Iteration: 84700\n",
      "Gradient: [  -6.1563   -1.0106  -50.5237  -34.7852 -359.3631]\n",
      "Weights: [-4.7951  0.6937 -1.1546  0.082   0.1483]\n",
      "MSE loss: 92.3277\n",
      "Iteration: 84800\n",
      "Gradient: [   6.1415  -20.6533   -1.33    108.5633 -134.6095]\n",
      "Weights: [-4.7939  0.6894 -1.154   0.0821  0.1484]\n",
      "MSE loss: 92.3263\n",
      "Iteration: 84900\n",
      "Gradient: [  -8.808   -26.4361   18.5762 -142.5592  -63.9785]\n",
      "Weights: [-4.7942  0.6853 -1.1522  0.0829  0.1484]\n",
      "MSE loss: 92.148\n",
      "Iteration: 85000\n",
      "Gradient: [ -5.9179  10.788  -66.5842 -11.9817 118.4812]\n",
      "Weights: [-4.7898  0.688  -1.1526  0.0837  0.1479]\n",
      "MSE loss: 92.0385\n",
      "Iteration: 85100\n",
      "Gradient: [   1.9878   -0.6118  -48.0701   22.1137 -310.5227]\n",
      "Weights: [-4.782   0.6918 -1.1549  0.0844  0.1477]\n",
      "MSE loss: 92.0873\n",
      "Iteration: 85200\n",
      "Gradient: [  -5.8104   14.5895  -19.47     16.0039 -250.4287]\n",
      "Weights: [-4.7946  0.6836 -1.1517  0.0856  0.1478]\n",
      "MSE loss: 92.2297\n",
      "Iteration: 85300\n",
      "Gradient: [ -1.0327  -5.9016  -2.7939 116.1427 -76.9227]\n",
      "Weights: [-4.7808  0.6779 -1.1523  0.0849  0.1478]\n",
      "MSE loss: 92.0185\n",
      "Iteration: 85400\n",
      "Gradient: [ -1.6171 -11.3446  22.9799 -34.3055  82.5457]\n",
      "Weights: [-4.7969  0.6688 -1.1483  0.0847  0.148 ]\n",
      "MSE loss: 92.6645\n",
      "Iteration: 85500\n",
      "Gradient: [ -9.9993   1.1204 -40.6092 -98.7261 -80.8153]\n",
      "Weights: [-4.7783  0.6569 -1.1461  0.0842  0.148 ]\n",
      "MSE loss: 92.4847\n",
      "Iteration: 85600\n",
      "Gradient: [  5.1287   5.1921 -11.0926  39.4584 257.5337]\n",
      "Weights: [-4.7625  0.6611 -1.1443  0.0845  0.1477]\n",
      "MSE loss: 92.2944\n",
      "Iteration: 85700\n",
      "Gradient: [  7.4717   8.8538   0.9192   1.2232 -47.7343]\n",
      "Weights: [-4.7621  0.661  -1.145   0.0847  0.1477]\n",
      "MSE loss: 92.2317\n",
      "Iteration: 85800\n",
      "Gradient: [  3.754    1.0266  38.7495 -12.0804 344.7154]\n",
      "Weights: [-4.7612  0.6521 -1.1445  0.0853  0.1477]\n",
      "MSE loss: 92.1011\n",
      "Iteration: 85900\n",
      "Gradient: [  3.2284   6.9583  31.8994  38.9955 138.722 ]\n",
      "Weights: [-4.7485  0.6607 -1.1467  0.0851  0.1474]\n",
      "MSE loss: 92.6174\n",
      "Iteration: 86000\n",
      "Gradient: [  7.1856  15.3012 -51.3194 -49.1794  -0.6901]\n",
      "Weights: [-4.7679  0.6616 -1.1467  0.0847  0.1476]\n",
      "MSE loss: 92.0672\n",
      "Iteration: 86100\n",
      "Gradient: [ -2.9598 -50.1718  21.2161 -98.6458  40.2753]\n",
      "Weights: [-4.7785  0.663  -1.1465  0.0852  0.1477]\n",
      "MSE loss: 92.0661\n",
      "Iteration: 86200\n",
      "Gradient: [   6.432     8.9757    6.5676   80.0286 -385.691 ]\n",
      "Weights: [-4.7679  0.6583 -1.1465  0.0851  0.1476]\n",
      "MSE loss: 92.0817\n",
      "Iteration: 86300\n",
      "Gradient: [ -0.5844   6.8436 -46.3832 -48.1753 159.4734]\n",
      "Weights: [-4.7797  0.6666 -1.1467  0.0856  0.1476]\n",
      "MSE loss: 92.0415\n",
      "Iteration: 86400\n",
      "Gradient: [   2.0712   -2.074   -20.1874 -128.8725   87.3212]\n",
      "Weights: [-4.7596  0.6521 -1.1444  0.0855  0.1476]\n",
      "MSE loss: 92.1129\n",
      "Iteration: 86500\n",
      "Gradient: [   2.2642   -8.7047   17.9736   62.2111 -113.3332]\n",
      "Weights: [-4.7619  0.6492 -1.1429  0.0854  0.1475]\n",
      "MSE loss: 92.0827\n",
      "Iteration: 86600\n",
      "Gradient: [  -9.5474  -15.402    16.2707   52.3344 -150.1416]\n",
      "Weights: [-4.7975  0.6652 -1.1422  0.0851  0.1472]\n",
      "MSE loss: 92.514\n",
      "Iteration: 86700\n",
      "Gradient: [-2.942  15.8925 72.3106 42.6812  2.2927]\n",
      "Weights: [-4.7615  0.6625 -1.1427  0.085   0.1471]\n",
      "MSE loss: 92.3639\n",
      "Iteration: 86800\n",
      "Gradient: [ -3.2189  -1.2743 -10.1276  53.4668  34.7222]\n",
      "Weights: [-4.7805  0.6547 -1.1395  0.0849  0.1474]\n",
      "MSE loss: 92.1191\n",
      "Iteration: 86900\n",
      "Gradient: [  6.1296  12.0479 -14.3445  54.0769 -38.2175]\n",
      "Weights: [-4.7584  0.6441 -1.1365  0.0842  0.1473]\n",
      "MSE loss: 92.1177\n",
      "Iteration: 87000\n",
      "Gradient: [   9.8527  -15.9759    4.1932   25.0641 -204.7443]\n",
      "Weights: [-4.7558  0.6339 -1.1342  0.0842  0.1472]\n",
      "MSE loss: 92.2287\n",
      "Iteration: 87100\n",
      "Gradient: [  2.2266   6.307   11.0995 -11.7908 106.7685]\n",
      "Weights: [-4.7782  0.627  -1.1288  0.0841  0.1473]\n",
      "MSE loss: 92.8771\n",
      "Iteration: 87200\n",
      "Gradient: [  2.2089  -1.1645  16.0643 -15.0874 -43.0742]\n",
      "Weights: [-4.7634  0.636  -1.128   0.0835  0.147 ]\n",
      "MSE loss: 92.1069\n",
      "Iteration: 87300\n",
      "Gradient: [-5.09000e-02 -3.51370e+00  8.55506e+01 -9.11587e+01 -5.65540e+01]\n",
      "Weights: [-4.7554  0.6383 -1.1309  0.084   0.1472]\n",
      "MSE loss: 92.3906\n",
      "Iteration: 87400\n",
      "Gradient: [  -0.3047  -18.3405  -25.295   -45.3183 -205.0348]\n",
      "Weights: [-4.7651  0.6289 -1.1296  0.0832  0.1474]\n",
      "MSE loss: 92.3557\n",
      "Iteration: 87500\n",
      "Gradient: [  0.7526  32.3435  12.2314  32.6596 -36.5068]\n",
      "Weights: [-4.7573  0.6304 -1.1275  0.0827  0.1472]\n",
      "MSE loss: 92.1437\n",
      "Iteration: 87600\n",
      "Gradient: [  -6.4659  -11.0369   18.9615   44.8872 -125.9494]\n",
      "Weights: [-4.7521  0.6343 -1.1298  0.0833  0.1471]\n",
      "MSE loss: 92.217\n",
      "Iteration: 87700\n",
      "Gradient: [ -6.1204   9.9686  18.9383  -3.5637 184.625 ]\n",
      "Weights: [-4.7534  0.6235 -1.1284  0.0834  0.1472]\n",
      "MSE loss: 92.267\n",
      "Iteration: 87800\n",
      "Gradient: [  9.7408 -14.602  -12.6833 -14.8619 -54.6862]\n",
      "Weights: [-4.7698  0.6276 -1.1284  0.0845  0.1469]\n",
      "MSE loss: 92.3793\n",
      "Iteration: 87900\n",
      "Gradient: [  5.0266   9.8196 -61.0192 -23.215  270.9143]\n",
      "Weights: [-4.7661  0.6355 -1.1276  0.0843  0.1468]\n",
      "MSE loss: 92.139\n",
      "Iteration: 88000\n",
      "Gradient: [-20.4507  14.1582   1.1926  67.2266  38.1788]\n",
      "Weights: [-4.7811  0.6418 -1.1261  0.084   0.1466]\n",
      "MSE loss: 92.1985\n",
      "Iteration: 88100\n",
      "Gradient: [   1.4311   16.4908   -5.3521  -63.5949 -239.4744]\n",
      "Weights: [-4.7707  0.6413 -1.1255  0.0832  0.1465]\n",
      "MSE loss: 92.0953\n",
      "Iteration: 88200\n",
      "Gradient: [ -5.5961  10.0052 -16.0068  24.2012  56.514 ]\n",
      "Weights: [-4.7725  0.6367 -1.1234  0.0838  0.1464]\n",
      "MSE loss: 92.1271\n",
      "Iteration: 88300\n",
      "Gradient: [   1.2689  -10.6068  -25.0525   84.3324 -107.8181]\n",
      "Weights: [-4.7616  0.6236 -1.1211  0.0846  0.1463]\n",
      "MSE loss: 92.1311\n",
      "Iteration: 88400\n",
      "Gradient: [  6.352   -0.7885  -8.2048 -19.4202 -67.7761]\n",
      "Weights: [-4.7614  0.6381 -1.1259  0.0847  0.1462]\n",
      "MSE loss: 92.2787\n",
      "Iteration: 88500\n",
      "Gradient: [ -1.8865 -12.2825   4.0858   1.2598  52.2759]\n",
      "Weights: [-4.7498  0.6308 -1.1252  0.0844  0.146 ]\n",
      "MSE loss: 92.3663\n",
      "Iteration: 88600\n",
      "Gradient: [  -2.8912  -16.7104   20.1938   30.1147 -226.1972]\n",
      "Weights: [-4.7647  0.6301 -1.1244  0.0848  0.146 ]\n",
      "MSE loss: 92.1848\n",
      "Iteration: 88700\n",
      "Gradient: [  0.389  -16.2892  23.3391 -31.5993  82.0002]\n",
      "Weights: [-4.7653  0.6315 -1.1242  0.0857  0.1459]\n",
      "MSE loss: 92.1012\n",
      "Iteration: 88800\n",
      "Gradient: [ 11.6145  16.5721  22.686  -89.1036 -27.3445]\n",
      "Weights: [-4.7479  0.6325 -1.1269  0.0857  0.1458]\n",
      "MSE loss: 92.4479\n",
      "Iteration: 88900\n",
      "Gradient: [   4.8938    3.003    49.3595  -44.8046 -129.3836]\n",
      "Weights: [-4.7722  0.6414 -1.1275  0.0863  0.1458]\n",
      "MSE loss: 92.0906\n",
      "Iteration: 89000\n",
      "Gradient: [  3.0223   7.5682 -23.3017 -52.3554 200.034 ]\n",
      "Weights: [-4.7892  0.651  -1.1297  0.0864  0.1459]\n",
      "MSE loss: 92.2252\n",
      "Iteration: 89100\n",
      "Gradient: [   3.0347    0.4298  -14.1313 -114.7791  135.8132]\n",
      "Weights: [-4.7645  0.6401 -1.1286  0.0854  0.1461]\n",
      "MSE loss: 92.1009\n",
      "Iteration: 89200\n",
      "Gradient: [  5.8512  22.1411  37.3182 -26.1178 166.2508]\n",
      "Weights: [-4.7662  0.6425 -1.1284  0.0854  0.1463]\n",
      "MSE loss: 92.3736\n",
      "Iteration: 89300\n",
      "Gradient: [ -6.9612 -14.95    -4.9303  22.195   54.6204]\n",
      "Weights: [-4.7759  0.6439 -1.1327  0.0857  0.1463]\n",
      "MSE loss: 92.1902\n",
      "Iteration: 89400\n",
      "Gradient: [  0.64    -7.8    -13.1786  31.0457 -72.0696]\n",
      "Weights: [-4.7863  0.6543 -1.1337  0.0861  0.1464]\n",
      "MSE loss: 92.1548\n",
      "Iteration: 89500\n",
      "Gradient: [  -2.1626   -4.1845    7.9138 -142.828    22.8442]\n",
      "Weights: [-4.7792  0.6493 -1.1329  0.0852  0.1464]\n",
      "MSE loss: 92.1083\n",
      "Iteration: 89600\n",
      "Gradient: [-4.5107 10.2358 12.6379 28.3564 34.8287]\n",
      "Weights: [-4.7551  0.6431 -1.1354  0.0856  0.1466]\n",
      "MSE loss: 92.1888\n",
      "Iteration: 89700\n",
      "Gradient: [-1.057390e+01 -1.864230e+01  1.620000e-01 -3.072570e+01 -1.863645e+02]\n",
      "Weights: [-4.7761  0.6531 -1.1364  0.0861  0.1465]\n",
      "MSE loss: 92.0359\n",
      "Iteration: 89800\n",
      "Gradient: [ -13.849   -13.3619    0.2847 -107.9529  -83.963 ]\n",
      "Weights: [-4.7655  0.6387 -1.1351  0.0858  0.1465]\n",
      "MSE loss: 92.3381\n",
      "Iteration: 89900\n",
      "Gradient: [   2.8396   16.2716    9.9747 -130.12    -76.9133]\n",
      "Weights: [-4.7668  0.647  -1.1354  0.0857  0.1465]\n",
      "MSE loss: 92.0585\n",
      "Iteration: 90000\n",
      "Gradient: [  6.6666  -6.4098 -28.6965 -91.6163 216.3601]\n",
      "Weights: [-4.7725  0.6412 -1.1337  0.0863  0.1464]\n",
      "MSE loss: 92.0919\n",
      "Iteration: 90100\n",
      "Gradient: [ -2.7867   5.8646  51.5647 -18.3821  51.9585]\n",
      "Weights: [-4.7794  0.6529 -1.1343  0.086   0.1463]\n",
      "MSE loss: 92.0501\n",
      "Iteration: 90200\n",
      "Gradient: [ 9.2629 -7.9761 -6.4689 30.7773 58.7484]\n",
      "Weights: [-4.7843  0.6558 -1.1326  0.0856  0.1462]\n",
      "MSE loss: 92.1018\n",
      "Iteration: 90300\n",
      "Gradient: [  3.8556  18.7967  27.287  100.3394 -58.2439]\n",
      "Weights: [-4.7817  0.6607 -1.1346  0.0856  0.1463]\n",
      "MSE loss: 92.1439\n",
      "Iteration: 90400\n",
      "Gradient: [  -2.7973  -18.7012   24.6211  -96.5056 -210.3702]\n",
      "Weights: [-4.7782  0.659  -1.136   0.0847  0.1466]\n",
      "MSE loss: 92.0499\n",
      "Iteration: 90500\n",
      "Gradient: [  6.4086  20.1751  31.9328 138.8039 359.9629]\n",
      "Weights: [-4.7688  0.6639 -1.1378  0.0853  0.1468]\n",
      "MSE loss: 92.8409\n",
      "Iteration: 90600\n",
      "Gradient: [ -0.6568  -1.7595   9.4161 232.7584  99.9876]\n",
      "Weights: [-4.79    0.661  -1.1354  0.0852  0.1465]\n",
      "MSE loss: 92.1412\n",
      "Iteration: 90700\n",
      "Gradient: [ -14.5623   -8.7054  -20.3421  -99.7113 -297.5266]\n",
      "Weights: [-4.7703  0.6465 -1.1382  0.0866  0.1466]\n",
      "MSE loss: 92.0489\n",
      "Iteration: 90800\n",
      "Gradient: [ -5.0029   5.4153  18.9204  97.2527 300.3091]\n",
      "Weights: [-4.7757  0.6514 -1.1362  0.0868  0.1464]\n",
      "MSE loss: 92.046\n",
      "Iteration: 90900\n",
      "Gradient: [  1.6999  11.5258 -60.641  -51.2411 112.4449]\n",
      "Weights: [-4.7617  0.6469 -1.1366  0.0867  0.1464]\n",
      "MSE loss: 92.1279\n",
      "Iteration: 91000\n",
      "Gradient: [  0.9547 -16.0683  25.2161  76.6682  50.2891]\n",
      "Weights: [-4.78    0.6463 -1.1338  0.0862  0.1463]\n",
      "MSE loss: 92.1663\n",
      "Iteration: 91100\n",
      "Gradient: [   8.7957   -4.7445   -0.6415  -11.8548 -156.4242]\n",
      "Weights: [-4.7919  0.6558 -1.1334  0.0851  0.1462]\n",
      "MSE loss: 92.4777\n",
      "Iteration: 91200\n",
      "Gradient: [ -3.7658   8.8378  38.0765 -43.327  164.091 ]\n",
      "Weights: [-4.7507  0.6516 -1.1361  0.0861  0.1462]\n",
      "MSE loss: 92.8208\n",
      "Iteration: 91300\n",
      "Gradient: [   2.639   -13.2595    3.0202   42.0418 -171.4794]\n",
      "Weights: [-4.7857  0.6603 -1.1399  0.0879  0.1461]\n",
      "MSE loss: 92.06\n",
      "Iteration: 91400\n",
      "Gradient: [  4.6076  14.355   -4.9068 102.6157 -58.1921]\n",
      "Weights: [-4.7589  0.6479 -1.1402  0.0883  0.1461]\n",
      "MSE loss: 92.1106\n",
      "Iteration: 91500\n",
      "Gradient: [  8.3138   4.9767  13.2742 -54.492  -53.9204]\n",
      "Weights: [-4.7798  0.6592 -1.14    0.0884  0.1458]\n",
      "MSE loss: 92.0077\n",
      "Iteration: 91600\n",
      "Gradient: [ -4.0915   7.2047 -56.556  -13.4536 132.3637]\n",
      "Weights: [-4.7804  0.6544 -1.1409  0.0889  0.1461]\n",
      "MSE loss: 92.0343\n",
      "Iteration: 91700\n",
      "Gradient: [  8.3531 -13.1899 -42.5151 -97.773  -13.7246]\n",
      "Weights: [-4.757   0.643  -1.1417  0.0891  0.1459]\n",
      "MSE loss: 92.2043\n",
      "Iteration: 91800\n",
      "Gradient: [  -3.5273  -12.1544   -3.562  -130.6744  -43.8576]\n",
      "Weights: [-4.7625  0.6486 -1.1445  0.0902  0.1459]\n",
      "MSE loss: 92.0133\n",
      "Iteration: 91900\n",
      "Gradient: [  -0.5172    8.9461   -9.7912  -31.6516 -203.7558]\n",
      "Weights: [-4.7664  0.6485 -1.1444  0.0897  0.1461]\n",
      "MSE loss: 92.0456\n",
      "Iteration: 92000\n",
      "Gradient: [ -6.6244   0.509   -3.5644  92.9799 274.0633]\n",
      "Weights: [-4.7547  0.645  -1.1454  0.0907  0.1459]\n",
      "MSE loss: 92.0721\n",
      "Iteration: 92100\n",
      "Gradient: [-3.3749 -3.169  -2.3266 97.8461 93.4307]\n",
      "Weights: [-4.7665  0.6533 -1.147   0.0906  0.146 ]\n",
      "MSE loss: 91.9678\n",
      "Iteration: 92200\n",
      "Gradient: [ -7.9058   9.6395  12.9547  62.26   -99.9713]\n",
      "Weights: [-4.7653  0.6526 -1.1456  0.0896  0.1463]\n",
      "MSE loss: 92.0\n",
      "Iteration: 92300\n",
      "Gradient: [ -5.2876  -7.9433  -6.3796 -64.3455 255.1379]\n",
      "Weights: [-4.7822  0.6636 -1.1463  0.0898  0.1462]\n",
      "MSE loss: 92.0511\n",
      "Iteration: 92400\n",
      "Gradient: [   0.3949   -8.2859  -25.8942 -103.8901  162.5344]\n",
      "Weights: [-4.7789  0.6566 -1.1425  0.0887  0.146 ]\n",
      "MSE loss: 92.0536\n",
      "Iteration: 92500\n",
      "Gradient: [   3.8755    4.4073    1.8522    2.683  -116.1387]\n",
      "Weights: [-4.7558  0.6486 -1.1415  0.0898  0.1459]\n",
      "MSE loss: 92.3383\n",
      "Iteration: 92600\n",
      "Gradient: [-10.7396   7.4247  29.1953 -11.1406 -39.7527]\n",
      "Weights: [-4.7766  0.6496 -1.1392  0.0893  0.146 ]\n",
      "MSE loss: 92.0407\n",
      "Iteration: 92700\n",
      "Gradient: [-15.1252  -3.0258  11.8058 -51.8547 -24.4201]\n",
      "Weights: [-4.7762  0.6475 -1.1387  0.0892  0.1458]\n",
      "MSE loss: 92.071\n",
      "Iteration: 92800\n",
      "Gradient: [  5.2582  -8.5926  20.588  -21.4204 -89.7348]\n",
      "Weights: [-4.7644  0.645  -1.1405  0.0897  0.1459]\n",
      "MSE loss: 91.9914\n",
      "Iteration: 92900\n",
      "Gradient: [   7.3068    9.2081   54.3519  -74.8717 -526.0155]\n",
      "Weights: [-4.7562  0.6397 -1.1384  0.0897  0.146 ]\n",
      "MSE loss: 92.2112\n",
      "Iteration: 93000\n",
      "Gradient: [-12.3185 -14.5658 -33.7742 -80.1137 182.936 ]\n",
      "Weights: [-4.7825  0.65   -1.1406  0.0901  0.1458]\n",
      "MSE loss: 92.1154\n",
      "Iteration: 93100\n",
      "Gradient: [-15.0651   0.26     6.7724  36.1793  72.3472]\n",
      "Weights: [-4.7824  0.6542 -1.1425  0.0906  0.1455]\n",
      "MSE loss: 92.0987\n",
      "Iteration: 93200\n",
      "Gradient: [-5.3305 -0.2767 -0.1716 25.2162  0.5391]\n",
      "Weights: [-4.7576  0.6395 -1.1427  0.0908  0.1455]\n",
      "MSE loss: 92.2595\n",
      "Iteration: 93300\n",
      "Gradient: [  0.9925  18.191  -40.1639 -57.7845 -84.0583]\n",
      "Weights: [-4.7664  0.6511 -1.1415  0.0904  0.1455]\n",
      "MSE loss: 92.0066\n",
      "Iteration: 93400\n",
      "Gradient: [ -4.9515  -1.5056  -0.8444 -37.4736 259.7478]\n",
      "Weights: [-4.7914  0.6513 -1.1398  0.0905  0.1457]\n",
      "MSE loss: 92.3467\n",
      "Iteration: 93500\n",
      "Gradient: [  -5.8056    7.8874   18.4654  -35.7047 -170.4237]\n",
      "Weights: [-4.7687  0.6543 -1.1426  0.0902  0.1455]\n",
      "MSE loss: 91.9979\n",
      "Iteration: 93600\n",
      "Gradient: [  3.3766   8.0888  -8.1954 -56.9255 -36.3223]\n",
      "Weights: [-4.7675  0.6429 -1.1392  0.0908  0.1455]\n",
      "MSE loss: 91.9783\n",
      "Iteration: 93700\n",
      "Gradient: [  1.5548   1.4441  11.2782 -17.8629  81.1767]\n",
      "Weights: [-4.765   0.6419 -1.1416  0.0916  0.1457]\n",
      "MSE loss: 92.0185\n",
      "Iteration: 93800\n",
      "Gradient: [  1.1999   2.17   -12.853   96.4236 366.9529]\n",
      "Weights: [-4.7584  0.6392 -1.1426  0.0921  0.1457]\n",
      "MSE loss: 92.0581\n",
      "Iteration: 93900\n",
      "Gradient: [ 10.9979  -4.0239  -6.1963  91.867  162.216 ]\n",
      "Weights: [-4.7591  0.6403 -1.1425  0.0925  0.1454]\n",
      "MSE loss: 92.0307\n",
      "Iteration: 94000\n",
      "Gradient: [-5.92840e+00 -1.63453e+01 -4.44000e-02  8.61323e+01 -3.93294e+02]\n",
      "Weights: [-4.7645  0.6451 -1.1431  0.0923  0.1452]\n",
      "MSE loss: 91.9624\n",
      "Iteration: 94100\n",
      "Gradient: [ -14.8084  -11.7939  -24.3028 -100.0924  -36.0657]\n",
      "Weights: [-4.7841  0.6602 -1.1462  0.0918  0.1454]\n",
      "MSE loss: 92.0146\n",
      "Iteration: 94200\n",
      "Gradient: [  4.6862  -9.0725  -8.8434   7.4411 -51.9871]\n",
      "Weights: [-4.7692  0.6547 -1.1463  0.0917  0.1454]\n",
      "MSE loss: 91.9694\n",
      "Iteration: 94300\n",
      "Gradient: [ 3.46000e-02 -7.63550e+00 -5.29403e+01  9.57949e+01 -8.03422e+01]\n",
      "Weights: [-4.7862  0.6609 -1.1466  0.0918  0.1452]\n",
      "MSE loss: 92.195\n",
      "Iteration: 94400\n",
      "Gradient: [  7.3832 -15.0689 -55.0933  25.6612 116.0463]\n",
      "Weights: [-4.7573  0.6453 -1.1427  0.0909  0.1454]\n",
      "MSE loss: 92.1314\n",
      "Iteration: 94500\n",
      "Gradient: [  -1.9683   -6.8399  -52.1512    5.6038 -336.768 ]\n",
      "Weights: [-4.7751  0.6437 -1.1406  0.0916  0.1453]\n",
      "MSE loss: 92.0656\n",
      "Iteration: 94600\n",
      "Gradient: [ 10.8481 -10.5511 -24.977  -48.7496 163.8794]\n",
      "Weights: [-4.7612  0.6472 -1.1401  0.091   0.1453]\n",
      "MSE loss: 92.1204\n",
      "Iteration: 94700\n",
      "Gradient: [  -5.5947    6.2573   30.1556   77.6812 -202.2408]\n",
      "Weights: [-4.7567  0.6332 -1.1374  0.0917  0.1451]\n",
      "MSE loss: 92.0206\n",
      "Iteration: 94800\n",
      "Gradient: [  3.8529  -9.0662   8.0134 -22.314  128.2938]\n",
      "Weights: [-4.7583  0.639  -1.1379  0.0917  0.145 ]\n",
      "MSE loss: 92.0479\n",
      "Iteration: 94900\n",
      "Gradient: [  4.7332  20.3388  15.3552 122.1899 129.1522]\n",
      "Weights: [-4.765   0.6378 -1.1355  0.0912  0.1453]\n",
      "MSE loss: 92.1349\n",
      "Iteration: 95000\n",
      "Gradient: [ 15.0368  -1.4327  -3.9952  32.4802 227.7995]\n",
      "Weights: [-4.7655  0.6484 -1.1416  0.0921  0.1451]\n",
      "MSE loss: 92.0421\n",
      "Iteration: 95100\n",
      "Gradient: [ -4.0775  18.3006  46.5227 140.2423 116.9018]\n",
      "Weights: [-4.7643  0.6459 -1.1424  0.0921  0.1453]\n",
      "MSE loss: 91.9749\n",
      "Iteration: 95200\n",
      "Gradient: [ -0.764    1.7347  47.0823   4.3438 246.6726]\n",
      "Weights: [-4.769   0.6402 -1.1398  0.0926  0.1452]\n",
      "MSE loss: 91.9924\n",
      "Iteration: 95300\n",
      "Gradient: [ -6.6712  17.4241  32.7397 -62.5579 -11.9838]\n",
      "Weights: [-4.7735  0.6498 -1.1422  0.0921  0.1452]\n",
      "MSE loss: 91.9609\n",
      "Iteration: 95400\n",
      "Gradient: [  2.7292   8.7919 -22.9664 -76.9823 252.8523]\n",
      "Weights: [-4.7852  0.6586 -1.1434  0.0926  0.145 ]\n",
      "MSE loss: 92.051\n",
      "Iteration: 95500\n",
      "Gradient: [ -5.7511  -5.9374 -38.9923  25.6752 -22.9479]\n",
      "Weights: [-4.7749  0.6601 -1.1446  0.0923  0.1448]\n",
      "MSE loss: 91.9939\n",
      "Iteration: 95600\n",
      "Gradient: [  0.9812  -8.3943  30.843  -58.5079 -36.903 ]\n",
      "Weights: [-4.7988  0.6627 -1.1436  0.0922  0.1448]\n",
      "MSE loss: 92.5301\n",
      "Iteration: 95700\n",
      "Gradient: [-1.089320e+01 -4.170000e-02  2.729100e+00 -6.368500e+00 -2.891803e+02]\n",
      "Weights: [-4.7806  0.6596 -1.1427  0.0917  0.1449]\n",
      "MSE loss: 92.0032\n",
      "Iteration: 95800\n",
      "Gradient: [   6.1167  -18.4713  -53.8713 -128.1125 -287.082 ]\n",
      "Weights: [-4.7912  0.6598 -1.1441  0.0922  0.1449]\n",
      "MSE loss: 92.3363\n",
      "Iteration: 95900\n",
      "Gradient: [-5.5404  7.6414 17.3315 54.8873 -1.8208]\n",
      "Weights: [-4.7805  0.661  -1.1453  0.0931  0.145 ]\n",
      "MSE loss: 92.078\n",
      "Iteration: 96000\n",
      "Gradient: [  1.4746  20.3533  31.8572 -13.2887 198.1552]\n",
      "Weights: [-4.7524  0.6399 -1.1454  0.0944  0.1451]\n",
      "MSE loss: 92.2081\n",
      "Iteration: 96100\n",
      "Gradient: [  -3.0528  -22.9703  -17.273    -8.1504 -326.4404]\n",
      "Weights: [-4.7759  0.6458 -1.1475  0.0941  0.1452]\n",
      "MSE loss: 92.1832\n",
      "Iteration: 96200\n",
      "Gradient: [ -8.3911 -11.1012 -19.3494 -28.7029  17.5497]\n",
      "Weights: [-4.7614  0.6446 -1.1497  0.0946  0.1453]\n",
      "MSE loss: 91.9464\n",
      "Iteration: 96300\n",
      "Gradient: [  0.5085  -0.2149  45.1782  56.6435 -95.5954]\n",
      "Weights: [-4.7552  0.6433 -1.1496  0.095   0.1453]\n",
      "MSE loss: 92.0996\n",
      "Iteration: 96400\n",
      "Gradient: [  -2.464     2.6939   -1.4852  114.5431 -119.7251]\n",
      "Weights: [-4.7874  0.6499 -1.1495  0.096   0.1449]\n",
      "MSE loss: 92.3495\n",
      "Iteration: 96500\n",
      "Gradient: [ -11.8066    2.2692   39.9584 -117.1717 -236.1536]\n",
      "Weights: [-4.7777  0.64   -1.1502  0.096   0.145 ]\n",
      "MSE loss: 92.7983\n",
      "Iteration: 96600\n",
      "Gradient: [  -0.2702    3.8929  -13.6049 -172.0031 -152.5202]\n",
      "Weights: [-4.7691  0.6532 -1.149   0.095   0.1451]\n",
      "MSE loss: 92.0991\n",
      "Iteration: 96700\n",
      "Gradient: [-11.3052  17.4258  -1.5648 171.1901 128.9605]\n",
      "Weights: [-4.7837  0.6523 -1.148   0.0946  0.1452]\n",
      "MSE loss: 92.1212\n",
      "Iteration: 96800\n",
      "Gradient: [ 11.5643  11.8976  38.2989  51.0147 -99.0844]\n",
      "Weights: [-4.7503  0.6541 -1.1502  0.0941  0.1449]\n",
      "MSE loss: 92.3855\n",
      "Iteration: 96900\n",
      "Gradient: [  4.7191  -5.6686   4.1797   1.4092 -28.3625]\n",
      "Weights: [-4.7676  0.6524 -1.1499  0.0944  0.1448]\n",
      "MSE loss: 92.0363\n",
      "Iteration: 97000\n",
      "Gradient: [ -5.2046  20.6479 -20.5089  28.1239  84.8745]\n",
      "Weights: [-4.7764  0.6495 -1.1482  0.0943  0.145 ]\n",
      "MSE loss: 92.1053\n",
      "Iteration: 97100\n",
      "Gradient: [  5.329    1.9564  20.8791 107.8807 -24.7379]\n",
      "Weights: [-4.7491  0.6448 -1.1498  0.0955  0.145 ]\n",
      "MSE loss: 92.2846\n",
      "Iteration: 97200\n",
      "Gradient: [ -3.8285   5.4887 -13.3854  54.4592  45.7377]\n",
      "Weights: [-4.7834  0.6512 -1.1459  0.0948  0.145 ]\n",
      "MSE loss: 92.1146\n",
      "Iteration: 97300\n",
      "Gradient: [  4.7113   1.4518  23.622   53.9386 119.3104]\n",
      "Weights: [-4.7496  0.6477 -1.1468  0.0948  0.1446]\n",
      "MSE loss: 92.3884\n",
      "Iteration: 97400\n",
      "Gradient: [ -0.3166 -23.7761 -70.1872 -39.0582 120.7029]\n",
      "Weights: [-4.7636  0.645  -1.1486  0.0956  0.1445]\n",
      "MSE loss: 92.089\n",
      "Iteration: 97500\n",
      "Gradient: [-2.2431  8.9912 36.2555 26.6247 88.9648]\n",
      "Weights: [-4.7763  0.6581 -1.1497  0.0957  0.1446]\n",
      "MSE loss: 91.9318\n",
      "Iteration: 97600\n",
      "Gradient: [ 1.434000e-01 -6.206800e+00  3.701650e+01  1.337918e+02 -2.012845e+02]\n",
      "Weights: [-4.7906  0.6747 -1.1508  0.0953  0.1443]\n",
      "MSE loss: 92.0375\n",
      "Iteration: 97700\n",
      "Gradient: [ -4.4927 -17.5232 -12.4279 -84.958   85.2443]\n",
      "Weights: [-4.7955  0.6711 -1.1477  0.0947  0.1443]\n",
      "MSE loss: 92.1022\n",
      "Iteration: 97800\n",
      "Gradient: [ 2.678900e+00 -1.969000e-01 -1.413900e+01  5.409900e+00  2.344395e+02]\n",
      "Weights: [-4.7997  0.669  -1.1457  0.0939  0.1444]\n",
      "MSE loss: 92.2247\n",
      "Iteration: 97900\n",
      "Gradient: [ -5.222   -8.6526 -11.4637 -64.015  -88.2645]\n",
      "Weights: [-4.7821  0.6556 -1.1455  0.0938  0.1445]\n",
      "MSE loss: 92.2266\n",
      "Iteration: 98000\n",
      "Gradient: [   3.6127  -21.1725   20.3753 -125.0277 -155.1424]\n",
      "Weights: [-4.7785  0.6659 -1.1517  0.0951  0.1446]\n",
      "MSE loss: 91.9161\n",
      "Iteration: 98100\n",
      "Gradient: [   9.9159   -2.9077    0.328  -169.4541 -113.5067]\n",
      "Weights: [-4.786   0.6656 -1.1531  0.0954  0.1447]\n",
      "MSE loss: 92.0257\n",
      "Iteration: 98200\n",
      "Gradient: [  -5.0624  -12.6397   -5.0215 -111.3834   93.3514]\n",
      "Weights: [-4.7805  0.6659 -1.1517  0.095   0.1448]\n",
      "MSE loss: 91.9249\n",
      "Iteration: 98300\n",
      "Gradient: [  9.2295  11.061   34.5591  73.9271 119.4374]\n",
      "Weights: [-4.7556  0.6619 -1.1538  0.0942  0.1448]\n",
      "MSE loss: 92.2808\n",
      "Iteration: 98400\n",
      "Gradient: [  3.0286   5.3535  48.2467 -20.9498 155.6312]\n",
      "Weights: [-4.7676  0.6709 -1.1554  0.0947  0.1449]\n",
      "MSE loss: 92.1126\n",
      "Iteration: 98500\n",
      "Gradient: [ -5.6829   1.7117   3.2025   3.0677 -70.0992]\n",
      "Weights: [-4.7673  0.6587 -1.1546  0.0957  0.1448]\n",
      "MSE loss: 91.9169\n",
      "Iteration: 98600\n",
      "Gradient: [ 10.1495  -3.6121  14.1935 -79.6087 -52.1992]\n",
      "Weights: [-4.7694  0.6651 -1.1539  0.096   0.1446]\n",
      "MSE loss: 92.0398\n",
      "Iteration: 98700\n",
      "Gradient: [   4.5111   15.712   -16.7724   31.2147 -224.3126]\n",
      "Weights: [-4.7643  0.6682 -1.1561  0.0961  0.1446]\n",
      "MSE loss: 92.1848\n",
      "Iteration: 98800\n",
      "Gradient: [-2.99250e+00  3.85000e-02  9.00270e+00  4.03044e+01  1.56699e+02]\n",
      "Weights: [-4.7892  0.6825 -1.1595  0.0961  0.1449]\n",
      "MSE loss: 91.9946\n",
      "Iteration: 98900\n",
      "Gradient: [ -7.8509   8.6752  17.1381 -29.2401 301.9537]\n",
      "Weights: [-4.7912  0.6887 -1.1601  0.0966  0.1446]\n",
      "MSE loss: 92.1319\n",
      "Iteration: 99000\n",
      "Gradient: [   0.6219  -29.8396   -1.5922  -35.6139 -282.2136]\n",
      "Weights: [-4.7793  0.6767 -1.1614  0.0967  0.1448]\n",
      "MSE loss: 91.8546\n",
      "Iteration: 99100\n",
      "Gradient: [  10.8998    8.4655   -9.0495  112.8027 -247.7509]\n",
      "Weights: [-4.7582  0.6597 -1.1577  0.0968  0.1448]\n",
      "MSE loss: 92.0428\n",
      "Iteration: 99200\n",
      "Gradient: [   5.7799   14.3263   -1.7685  -23.8483 -139.9822]\n",
      "Weights: [-4.7816  0.6635 -1.1553  0.0969  0.1447]\n",
      "MSE loss: 91.9067\n",
      "Iteration: 99300\n",
      "Gradient: [1.595030e+01 2.815300e+00 1.918000e-01 7.514400e+00 1.980804e+02]\n",
      "Weights: [-4.7501  0.6462 -1.1541  0.0983  0.1444]\n",
      "MSE loss: 92.1643\n",
      "Iteration: 99400\n",
      "Gradient: [-0.2715 -7.5192 -7.966  69.2261 90.7719]\n",
      "Weights: [-4.7711  0.6404 -1.1518  0.0981  0.1445]\n",
      "MSE loss: 92.1778\n",
      "Iteration: 99500\n",
      "Gradient: [ 4.805500e+00  2.180000e-02  1.003910e+01 -1.035148e+02 -2.465110e+01]\n",
      "Weights: [-4.7531  0.6424 -1.1517  0.0977  0.1444]\n",
      "MSE loss: 91.991\n",
      "Iteration: 99600\n",
      "Gradient: [ 3.132500e+00  6.015400e+00 -1.360000e-01  7.777660e+01  2.523294e+02]\n",
      "Weights: [-4.7551  0.6441 -1.1504  0.0978  0.1442]\n",
      "MSE loss: 92.0283\n",
      "Iteration: 99700\n",
      "Gradient: [ -7.5888 -19.0286 -22.599   41.6177  78.5069]\n",
      "Weights: [-4.7739  0.6361 -1.1496  0.0986  0.1443]\n",
      "MSE loss: 92.3506\n",
      "Iteration: 99800\n",
      "Gradient: [   1.3006   11.6622  -25.623  -126.7542  260.5537]\n",
      "Weights: [-4.7434  0.6272 -1.1485  0.0982  0.1441]\n",
      "MSE loss: 92.1841\n",
      "Iteration: 99900\n",
      "Gradient: [ 11.3281 -33.763    3.3188  25.0536 -58.2103]\n",
      "Weights: [-4.7506  0.6368 -1.1513  0.0987  0.1443]\n",
      "MSE loss: 91.9957\n",
      "Iteration: 100000\n",
      "Gradient: [ -0.4409  15.6925  -0.2726  69.2294 -16.3426]\n",
      "Weights: [-4.754   0.6444 -1.1525  0.0987  0.1443]\n",
      "MSE loss: 92.1179\n",
      "Iteration: 100100\n",
      "Gradient: [ -0.2391  -1.1562  21.6591 -23.9727 173.705 ]\n",
      "Weights: [-4.7764  0.6544 -1.1562  0.0987  0.1442]\n",
      "MSE loss: 92.0236\n",
      "Iteration: 100200\n",
      "Gradient: [  -0.7529    4.2939   32.1469 -241.5616  159.7319]\n",
      "Weights: [-4.7735  0.6551 -1.1543  0.0988  0.1442]\n",
      "MSE loss: 91.8879\n",
      "Iteration: 100300\n",
      "Gradient: [  6.8363  12.5706  -5.6762 -15.3118 190.0315]\n",
      "Weights: [-4.7704  0.6578 -1.1571  0.0991  0.1441]\n",
      "MSE loss: 91.8382\n",
      "Iteration: 100400\n",
      "Gradient: [ -4.1415  -9.5871  14.4516  87.4488 -75.9128]\n",
      "Weights: [-4.7855  0.6644 -1.1578  0.099   0.1441]\n",
      "MSE loss: 91.9617\n",
      "Iteration: 100500\n",
      "Gradient: [  3.7324   8.9572 -25.9954 -85.7423  58.6308]\n",
      "Weights: [-4.7879  0.6811 -1.1611  0.0987  0.1438]\n",
      "MSE loss: 91.9265\n",
      "Iteration: 100600\n",
      "Gradient: [  7.1051   6.0805  42.1892 -64.6567 -49.4453]\n",
      "Weights: [-4.7776  0.6781 -1.1653  0.0997  0.1441]\n",
      "MSE loss: 91.8328\n",
      "Iteration: 100700\n",
      "Gradient: [ -1.7635 -23.967  -46.7448  49.0553 -15.767 ]\n",
      "Weights: [-4.773   0.6763 -1.1652  0.0987  0.1443]\n",
      "MSE loss: 91.8809\n",
      "Iteration: 100800\n",
      "Gradient: [  4.3583  10.6227  52.4493 -59.4959 203.422 ]\n",
      "Weights: [-4.7719  0.6793 -1.1667  0.0992  0.1443]\n",
      "MSE loss: 91.9\n",
      "Iteration: 100900\n",
      "Gradient: [ -2.0764  20.9857   0.7608  74.393  151.0463]\n",
      "Weights: [-4.7815  0.6778 -1.1691  0.0997  0.1448]\n",
      "MSE loss: 91.8095\n",
      "Iteration: 101000\n",
      "Gradient: [ -8.5244   4.0886 -42.223  -64.4955  55.6023]\n",
      "Weights: [-4.7767  0.6799 -1.1719  0.0995  0.1448]\n",
      "MSE loss: 91.8455\n",
      "Iteration: 101100\n",
      "Gradient: [   1.7445  -13.65     -1.4319  -20.7458 -356.2347]\n",
      "Weights: [-4.7861  0.68   -1.1704  0.0999  0.1447]\n",
      "MSE loss: 91.871\n",
      "Iteration: 101200\n",
      "Gradient: [  0.5086   4.255   -6.0234  21.9589 -62.8511]\n",
      "Weights: [-4.7836  0.6694 -1.1669  0.0994  0.145 ]\n",
      "MSE loss: 91.9964\n",
      "Iteration: 101300\n",
      "Gradient: [-12.3935   8.0327 -24.4899  11.7542  95.401 ]\n",
      "Weights: [-4.7765  0.6638 -1.1654  0.0991  0.1451]\n",
      "MSE loss: 91.9423\n",
      "Iteration: 101400\n",
      "Gradient: [ -8.8487  -3.5546  84.0678 -65.7956 -25.0376]\n",
      "Weights: [-4.7877  0.6843 -1.1689  0.0986  0.145 ]\n",
      "MSE loss: 91.8515\n",
      "Iteration: 101500\n",
      "Gradient: [  -7.9822  -18.0129   28.234  -149.1785 -189.4493]\n",
      "Weights: [-4.7988  0.6878 -1.171   0.0981  0.145 ]\n",
      "MSE loss: 92.3624\n",
      "Iteration: 101600\n",
      "Gradient: [  7.0386   0.7246   4.5469  38.9047 190.9106]\n",
      "Weights: [-4.7863  0.6973 -1.1702  0.0975  0.1451]\n",
      "MSE loss: 92.1245\n",
      "Iteration: 101700\n",
      "Gradient: [-4.0231 -4.9512 12.7027  9.4271 78.1085]\n",
      "Weights: [-4.8018  0.7022 -1.1714  0.0973  0.1451]\n",
      "MSE loss: 91.9451\n",
      "Iteration: 101800\n",
      "Gradient: [ -2.0288  11.1406  75.1159 -77.3168  15.7456]\n",
      "Weights: [-4.7837  0.6893 -1.1728  0.0982  0.1455]\n",
      "MSE loss: 91.8837\n",
      "Iteration: 101900\n",
      "Gradient: [  0.2467  13.5225   5.4342 -40.4311 108.7292]\n",
      "Weights: [-4.7809  0.6906 -1.1732  0.0973  0.1455]\n",
      "MSE loss: 91.8087\n",
      "Iteration: 102000\n",
      "Gradient: [   2.6992   11.6427   33.3982   19.4882 -130.3955]\n",
      "Weights: [-4.7721  0.6805 -1.1726  0.0982  0.1454]\n",
      "MSE loss: 91.8396\n",
      "Iteration: 102100\n",
      "Gradient: [  -6.2727   -9.6815   16.4627   28.0482 -146.579 ]\n",
      "Weights: [-4.7624  0.6745 -1.172   0.0984  0.1456]\n",
      "MSE loss: 91.9531\n",
      "Iteration: 102200\n",
      "Gradient: [  1.6087  22.2565  -1.5429 155.9856  72.2211]\n",
      "Weights: [-4.7717  0.6737 -1.1706  0.0983  0.1457]\n",
      "MSE loss: 91.8926\n",
      "Iteration: 102300\n",
      "Gradient: [  7.493    9.1783  22.8843  96.3822 223.1808]\n",
      "Weights: [-4.7697  0.6809 -1.1735  0.0984  0.1456]\n",
      "MSE loss: 91.8893\n",
      "Iteration: 102400\n",
      "Gradient: [ -4.7242   8.1596 -29.8853 -11.8634  89.5078]\n",
      "Weights: [-4.7908  0.6839 -1.1737  0.0987  0.1455]\n",
      "MSE loss: 92.0548\n",
      "Iteration: 102500\n",
      "Gradient: [ -2.4025  -6.5755 -17.0031 -42.838   35.7619]\n",
      "Weights: [-4.7772  0.6726 -1.1692  0.0992  0.1452]\n",
      "MSE loss: 91.8515\n",
      "Iteration: 102600\n",
      "Gradient: [ 3.2861  3.7968 20.7069 26.4508  8.7901]\n",
      "Weights: [-4.7741  0.6727 -1.1692  0.1002  0.145 ]\n",
      "MSE loss: 91.9656\n",
      "Iteration: 102700\n",
      "Gradient: [ -0.5751  -0.7964  -3.8427 173.8721 291.3458]\n",
      "Weights: [-4.7839  0.6821 -1.1706  0.0997  0.1449]\n",
      "MSE loss: 91.8208\n",
      "Iteration: 102800\n",
      "Gradient: [ -11.4568  -30.7627   25.9842 -101.0928   10.721 ]\n",
      "Weights: [-4.783   0.6935 -1.1718  0.0985  0.1445]\n",
      "MSE loss: 91.953\n",
      "Iteration: 102900\n",
      "Gradient: [  -9.1272  -18.8888   16.3074 -103.8767 -176.5045]\n",
      "Weights: [-4.7917  0.6961 -1.1725  0.0985  0.1447]\n",
      "MSE loss: 91.8882\n",
      "Iteration: 103000\n",
      "Gradient: [ 11.1516 -10.6876 -36.1711  70.5779 230.0675]\n",
      "Weights: [-4.7897  0.6946 -1.176   0.1002  0.1448]\n",
      "MSE loss: 91.7919\n",
      "Iteration: 103100\n",
      "Gradient: [ -4.5202   0.6187   2.6324  53.962  -41.313 ]\n",
      "Weights: [-4.8026  0.7016 -1.1807  0.1004  0.145 ]\n",
      "MSE loss: 92.1161\n",
      "Iteration: 103200\n",
      "Gradient: [   2.0872   20.4389   -0.8463 -116.6513   86.4818]\n",
      "Weights: [-4.7793  0.7011 -1.1844  0.1005  0.1451]\n",
      "MSE loss: 91.8019\n",
      "Iteration: 103300\n",
      "Gradient: [  5.8117  -2.4198 -61.0595  86.2979   3.801 ]\n",
      "Weights: [-4.7853  0.7003 -1.181   0.1004  0.1453]\n",
      "MSE loss: 91.8677\n",
      "Iteration: 103400\n",
      "Gradient: [  7.889    2.7174  -6.3763 -15.7048 100.6392]\n",
      "Weights: [-4.793   0.7074 -1.1848  0.1018  0.1449]\n",
      "MSE loss: 91.755\n",
      "Iteration: 103500\n",
      "Gradient: [ -1.306   -6.4799  15.8464 -54.5855 273.8099]\n",
      "Weights: [-4.7802  0.6885 -1.1838  0.103   0.1449]\n",
      "MSE loss: 91.8055\n",
      "Iteration: 103600\n",
      "Gradient: [ -0.6187   3.7079  12.286   15.8729 -55.6965]\n",
      "Weights: [-4.7764  0.6843 -1.1813  0.1031  0.1447]\n",
      "MSE loss: 91.7607\n",
      "Iteration: 103700\n",
      "Gradient: [ -12.6055   -7.3699  -40.5229 -104.2707  -19.9521]\n",
      "Weights: [-4.7817  0.678  -1.1812  0.1025  0.1449]\n",
      "MSE loss: 92.4124\n",
      "Iteration: 103800\n",
      "Gradient: [  3.3352  17.3315   4.3966 -16.5733 158.6752]\n",
      "Weights: [-4.7597  0.6882 -1.183   0.1025  0.1448]\n",
      "MSE loss: 92.0987\n",
      "Iteration: 103900\n",
      "Gradient: [  -2.294    17.367   -39.5721   76.0705 -382.5608]\n",
      "Weights: [-4.7732  0.6906 -1.1826  0.1031  0.1445]\n",
      "MSE loss: 91.7604\n",
      "Iteration: 104000\n",
      "Gradient: [  -7.9935   -8.1876   20.7538   66.0126 -448.6343]\n",
      "Weights: [-4.7655  0.6865 -1.1807  0.1028  0.1445]\n",
      "MSE loss: 91.9067\n",
      "Iteration: 104100\n",
      "Gradient: [  -3.8351   -8.1253  -13.4809  -14.8179 -258.9139]\n",
      "Weights: [-4.7979  0.6873 -1.181   0.1029  0.1447]\n",
      "MSE loss: 92.5003\n",
      "Iteration: 104200\n",
      "Gradient: [  1.6863  -9.4378  92.8681  71.2647 342.2064]\n",
      "Weights: [-4.79    0.7042 -1.1837  0.1024  0.1447]\n",
      "MSE loss: 91.7852\n",
      "Iteration: 104300\n",
      "Gradient: [  6.5296  -2.6722  40.2803 -29.9263 -28.4569]\n",
      "Weights: [-4.7841  0.7009 -1.1862  0.1034  0.1446]\n",
      "MSE loss: 91.7112\n",
      "Iteration: 104400\n",
      "Gradient: [ -5.5395 -10.9278  12.0128  20.7079 169.4065]\n",
      "Weights: [-4.7669  0.6928 -1.1852  0.103   0.1446]\n",
      "MSE loss: 91.8786\n",
      "Iteration: 104500\n",
      "Gradient: [ -6.0423  -5.2652   5.2025 110.9909 -72.8163]\n",
      "Weights: [-4.7856  0.6959 -1.1837  0.1031  0.1447]\n",
      "MSE loss: 91.7358\n",
      "Iteration: 104600\n",
      "Gradient: [ -0.8509   1.3037  -2.1515 -57.4051 -89.5905]\n",
      "Weights: [-4.7913  0.6913 -1.1829  0.1033  0.1446]\n",
      "MSE loss: 92.0219\n",
      "Iteration: 104700\n",
      "Gradient: [  -2.309     9.4829   16.9488   40.7788 -102.4207]\n",
      "Weights: [-4.7715  0.6845 -1.1837  0.1038  0.1447]\n",
      "MSE loss: 91.7531\n",
      "Iteration: 104800\n",
      "Gradient: [  2.6525   3.958   49.7914 -46.0556  57.8581]\n",
      "Weights: [-4.7755  0.6841 -1.1853  0.104   0.1448]\n",
      "MSE loss: 91.8327\n",
      "Iteration: 104900\n",
      "Gradient: [  -4.0458   12.4903    0.8954 -116.0971   70.8533]\n",
      "Weights: [-4.7853  0.6991 -1.1863  0.1038  0.1447]\n",
      "MSE loss: 91.7557\n",
      "Iteration: 105000\n",
      "Gradient: [ 12.694    1.6573  32.7583 -47.9055 117.9978]\n",
      "Weights: [-4.7696  0.6933 -1.1856  0.1036  0.1447]\n",
      "MSE loss: 91.8792\n",
      "Iteration: 105100\n",
      "Gradient: [   0.4373   32.1624  -14.8411  -27.1701 -365.9687]\n",
      "Weights: [-4.7769  0.6916 -1.1869  0.1037  0.1447]\n",
      "MSE loss: 91.8125\n",
      "Iteration: 105200\n",
      "Gradient: [ -1.4122  -1.0949 -23.1704  48.099   51.0025]\n",
      "Weights: [-4.7695  0.6747 -1.1816  0.1048  0.1445]\n",
      "MSE loss: 91.8094\n",
      "Iteration: 105300\n",
      "Gradient: [   7.9235   -7.0589  -25.0342 -127.4694   30.9852]\n",
      "Weights: [-4.7791  0.6888 -1.1833  0.1049  0.1443]\n",
      "MSE loss: 91.7281\n",
      "Iteration: 105400\n",
      "Gradient: [  8.1776  -0.7033   9.8596 -84.5252 -77.4158]\n",
      "Weights: [-4.7915  0.6902 -1.1834  0.1049  0.1442]\n",
      "MSE loss: 91.9586\n",
      "Iteration: 105500\n",
      "Gradient: [ 6.5562 -8.9051  6.142  29.4094 30.1964]\n",
      "Weights: [-4.7831  0.7013 -1.184   0.1049  0.1437]\n",
      "MSE loss: 91.7667\n",
      "Iteration: 105600\n",
      "Gradient: [ -1.2042  -3.2414  18.1394  81.3426 114.1102]\n",
      "Weights: [-4.8054  0.7049 -1.1816  0.105   0.1436]\n",
      "MSE loss: 91.9284\n",
      "Iteration: 105700\n",
      "Gradient: [  7.2475  18.6883  -3.1203 -62.2992 261.9061]\n",
      "Weights: [-4.7886  0.6985 -1.1812  0.1045  0.1437]\n",
      "MSE loss: 91.7356\n",
      "Iteration: 105800\n",
      "Gradient: [ -5.3459  -2.7765 -55.1608 -11.4863 142.429 ]\n",
      "Weights: [-4.7941  0.6824 -1.1803  0.1055  0.1437]\n",
      "MSE loss: 92.4447\n",
      "Iteration: 105900\n",
      "Gradient: [  -4.1071   -8.8156    9.2082 -120.7814   34.4608]\n",
      "Weights: [-4.7738  0.6795 -1.1775  0.1054  0.1435]\n",
      "MSE loss: 91.7292\n",
      "Iteration: 106000\n",
      "Gradient: [  9.3148 -11.1136 -27.4526 -93.1644  99.0867]\n",
      "Weights: [-4.7778  0.6761 -1.1777  0.1062  0.1436]\n",
      "MSE loss: 91.7435\n",
      "Iteration: 106100\n",
      "Gradient: [  5.2712  11.0189   1.2262  43.3109 -85.3025]\n",
      "Weights: [-4.7821  0.6912 -1.1824  0.1052  0.1439]\n",
      "MSE loss: 91.6969\n",
      "Iteration: 106200\n",
      "Gradient: [ -4.7973   6.358   18.1101  49.0888 -33.3768]\n",
      "Weights: [-4.7817  0.6811 -1.1812  0.1055  0.144 ]\n",
      "MSE loss: 91.8181\n",
      "Iteration: 106300\n",
      "Gradient: [ -4.0223 -27.4989 -15.0847  27.4336 258.7164]\n",
      "Weights: [-4.778   0.6805 -1.1817  0.1058  0.1442]\n",
      "MSE loss: 91.7824\n",
      "Iteration: 106400\n",
      "Gradient: [ -10.3241  -15.8458    5.061   -14.4168 -197.4301]\n",
      "Weights: [-4.7899  0.6787 -1.1782  0.1053  0.1441]\n",
      "MSE loss: 92.0177\n",
      "Iteration: 106500\n",
      "Gradient: [ 1.417   6.0052 28.4883 46.87   28.2963]\n",
      "Weights: [-4.7472  0.6724 -1.178   0.1048  0.1439]\n",
      "MSE loss: 92.3845\n",
      "Iteration: 106600\n",
      "Gradient: [  -1.4705   -0.4112   -7.9455 -142.2824  -70.5685]\n",
      "Weights: [-4.7755  0.6778 -1.1783  0.1054  0.1438]\n",
      "MSE loss: 91.7217\n",
      "Iteration: 106700\n",
      "Gradient: [   2.055   -14.8799   11.0194   29.5263 -212.4459]\n",
      "Weights: [-4.7746  0.6811 -1.18    0.105   0.1436]\n",
      "MSE loss: 91.9325\n",
      "Iteration: 106800\n",
      "Gradient: [  -2.0938    5.7242   20.1221    1.1899 -219.4312]\n",
      "Weights: [-4.7774  0.6812 -1.1776  0.1047  0.1437]\n",
      "MSE loss: 91.747\n",
      "Iteration: 106900\n",
      "Gradient: [ -0.7112  18.5042  -4.9272 -50.9114  74.0847]\n",
      "Weights: [-4.7806  0.6917 -1.1807  0.1053  0.1436]\n",
      "MSE loss: 91.7254\n",
      "Iteration: 107000\n",
      "Gradient: [-7.700000e-02 -5.353000e+00 -5.319300e+00  1.378357e+02  8.754300e+00]\n",
      "Weights: [-4.7954  0.6932 -1.1817  0.1054  0.1438]\n",
      "MSE loss: 91.8857\n",
      "Iteration: 107100\n",
      "Gradient: [  0.3042  14.1112 -23.8704  -7.2562 -61.2633]\n",
      "Weights: [-4.773   0.692  -1.183   0.1053  0.1437]\n",
      "MSE loss: 91.786\n",
      "Iteration: 107200\n",
      "Gradient: [   5.2973  -14.4261  -15.5633  -23.9716 -252.9396]\n",
      "Weights: [-4.7807  0.6895 -1.1849  0.1057  0.1437]\n",
      "MSE loss: 91.9956\n",
      "Iteration: 107300\n",
      "Gradient: [ 7.5982 -1.0199 -2.4125 79.9675 56.2193]\n",
      "Weights: [-4.7888  0.6995 -1.1852  0.106   0.1439]\n",
      "MSE loss: 91.8188\n",
      "Iteration: 107400\n",
      "Gradient: [  3.052   -9.051    7.8525 -22.3817 -42.1308]\n",
      "Weights: [-4.783   0.6975 -1.1826  0.1057  0.1435]\n",
      "MSE loss: 91.7572\n",
      "Iteration: 107500\n",
      "Gradient: [  8.8198  -1.1265  36.1998 -28.9971  92.661 ]\n",
      "Weights: [-4.7691  0.6938 -1.186   0.1061  0.1437]\n",
      "MSE loss: 91.8717\n",
      "Iteration: 107600\n",
      "Gradient: [  9.6322  19.9973 -18.1145  78.008  181.7508]\n",
      "Weights: [-4.7797  0.7053 -1.1869  0.1068  0.1433]\n",
      "MSE loss: 91.9738\n",
      "Iteration: 107700\n",
      "Gradient: [  -3.4033    1.7045  -21.657  -114.5678 -256.8067]\n",
      "Weights: [-4.7981  0.6966 -1.1842  0.1072  0.1435]\n",
      "MSE loss: 91.8541\n",
      "Iteration: 107800\n",
      "Gradient: [ -2.4378  -7.0342 -39.0108 100.4345 -98.8634]\n",
      "Weights: [-4.787   0.695  -1.1857  0.107   0.1434]\n",
      "MSE loss: 91.7097\n",
      "Iteration: 107900\n",
      "Gradient: [  4.7512  -0.7519  30.5905 -73.0643 247.4186]\n",
      "Weights: [-4.7828  0.6911 -1.1847  0.1069  0.1435]\n",
      "MSE loss: 91.6796\n",
      "Iteration: 108000\n",
      "Gradient: [  6.342    3.9341  25.6668  12.3264 -48.4051]\n",
      "Weights: [-4.7774  0.6885 -1.1854  0.1067  0.1437]\n",
      "MSE loss: 91.6806\n",
      "Iteration: 108100\n",
      "Gradient: [ 3.764900e+00 -1.713000e-01 -1.387320e+01 -1.681600e+00  2.060197e+02]\n",
      "Weights: [-4.7963  0.6985 -1.1855  0.1066  0.1434]\n",
      "MSE loss: 91.9101\n",
      "Iteration: 108200\n",
      "Gradient: [-11.8124  -7.7177  -6.1011  53.9769  86.959 ]\n",
      "Weights: [-4.7996  0.7001 -1.1861  0.1064  0.1436]\n",
      "MSE loss: 91.9473\n",
      "Iteration: 108300\n",
      "Gradient: [ -10.2104   -9.6605   13.9679 -142.6253 -152.9626]\n",
      "Weights: [-4.7816  0.6943 -1.1831  0.1063  0.1434]\n",
      "MSE loss: 91.708\n",
      "Iteration: 108400\n",
      "Gradient: [   0.653    18.7248   58.4888  -28.3839 -352.4136]\n",
      "Weights: [-4.7782  0.6891 -1.1823  0.1066  0.1434]\n",
      "MSE loss: 91.7027\n",
      "Iteration: 108500\n",
      "Gradient: [   0.8323   -2.3304   -7.2382  -19.9689 -161.7656]\n",
      "Weights: [-4.7868  0.6952 -1.1836  0.1061  0.1435]\n",
      "MSE loss: 91.7035\n",
      "Iteration: 108600\n",
      "Gradient: [  8.1606  14.4827  31.261  -12.0741 239.8796]\n",
      "Weights: [-4.7806  0.6967 -1.1828  0.1061  0.1435]\n",
      "MSE loss: 91.8967\n",
      "Iteration: 108700\n",
      "Gradient: [ -7.5567 -17.3052 -27.8197 132.3719 -12.2516]\n",
      "Weights: [-4.7925  0.6897 -1.1828  0.1064  0.1437]\n",
      "MSE loss: 91.8482\n",
      "Iteration: 108800\n",
      "Gradient: [  2.1912  31.16    16.425  -30.3347  25.2912]\n",
      "Weights: [-4.7802  0.6905 -1.1832  0.1063  0.1436]\n",
      "MSE loss: 91.6892\n",
      "Iteration: 108900\n",
      "Gradient: [   1.0652   -9.3864  -41.1631  -15.6412 -208.3278]\n",
      "Weights: [-4.7778  0.6836 -1.1822  0.1063  0.1435]\n",
      "MSE loss: 91.8241\n",
      "Iteration: 109000\n",
      "Gradient: [ 1.326000e-01  1.827710e+01  3.983600e+01 -2.618740e+01 -1.677768e+02]\n",
      "Weights: [-4.7818  0.6998 -1.1863  0.1066  0.1436]\n",
      "MSE loss: 91.7973\n",
      "Iteration: 109100\n",
      "Gradient: [   3.3842   28.2769   -3.2532  106.9041 -152.7692]\n",
      "Weights: [-4.7841  0.708  -1.1876  0.1065  0.1437]\n",
      "MSE loss: 92.195\n",
      "Iteration: 109200\n",
      "Gradient: [ -8.1823   9.5693 -29.1968 -85.7741  58.2946]\n",
      "Weights: [-4.7939  0.6987 -1.1876  0.1059  0.1437]\n",
      "MSE loss: 92.2196\n",
      "Iteration: 109300\n",
      "Gradient: [ -0.811   -1.8752  17.8838 -45.9325 110.7   ]\n",
      "Weights: [-4.7564  0.6955 -1.1898  0.1064  0.1438]\n",
      "MSE loss: 92.3258\n",
      "Iteration: 109400\n",
      "Gradient: [ 4.321600e+00 -7.730000e-02  2.545760e+01 -1.250878e+02 -3.748660e+01]\n",
      "Weights: [-4.7645  0.6881 -1.1888  0.1063  0.1441]\n",
      "MSE loss: 91.7973\n",
      "Iteration: 109500\n",
      "Gradient: [ 4.5585  6.0002 -7.4725 64.8894 98.9018]\n",
      "Weights: [-4.7729  0.6926 -1.1874  0.1063  0.144 ]\n",
      "MSE loss: 91.7239\n",
      "Iteration: 109600\n",
      "Gradient: [ -0.6874 -15.7282 -38.2265  78.2982 -14.6652]\n",
      "Weights: [-4.7849  0.6961 -1.1865  0.1052  0.1442]\n",
      "MSE loss: 91.7054\n",
      "Iteration: 109700\n",
      "Gradient: [   0.9886  -22.987    14.6711   12.6619 -300.7046]\n",
      "Weights: [-4.7937  0.6977 -1.1857  0.1055  0.1439]\n",
      "MSE loss: 91.8807\n",
      "Iteration: 109800\n",
      "Gradient: [   2.9684  -21.4885  -19.8051  -20.9797 -134.4547]\n",
      "Weights: [-4.7854  0.6907 -1.1851  0.1065  0.1438]\n",
      "MSE loss: 91.724\n",
      "Iteration: 109900\n",
      "Gradient: [ -2.6315   2.5155  -6.2559 -68.567  210.9926]\n",
      "Weights: [-4.7888  0.6954 -1.1884  0.1064  0.1439]\n",
      "MSE loss: 91.8699\n",
      "Iteration: 110000\n",
      "Gradient: [   2.4074   -2.2584   18.3597   71.3611 -111.6133]\n",
      "Weights: [-4.7737  0.691  -1.1908  0.1074  0.1438]\n",
      "MSE loss: 91.7624\n",
      "Iteration: 110100\n",
      "Gradient: [  0.916    1.8775 -17.7026  39.526   25.1168]\n",
      "Weights: [-4.7664  0.6875 -1.1859  0.1065  0.1437]\n",
      "MSE loss: 91.7906\n",
      "Iteration: 110200\n",
      "Gradient: [ 17.1104  12.4967 -28.0709  14.4867 193.7197]\n",
      "Weights: [-4.7594  0.6929 -1.187   0.1063  0.144 ]\n",
      "MSE loss: 92.5805\n",
      "Iteration: 110300\n",
      "Gradient: [  6.5307  -2.4205 -29.4303  18.9765  65.9357]\n",
      "Weights: [-4.7748  0.6895 -1.1861  0.1063  0.144 ]\n",
      "MSE loss: 91.7035\n",
      "Iteration: 110400\n",
      "Gradient: [ -1.5003   8.7639 -21.7081  53.0631 -61.2241]\n",
      "Weights: [-4.7855  0.6959 -1.1889  0.1069  0.1439]\n",
      "MSE loss: 91.7019\n",
      "Iteration: 110500\n",
      "Gradient: [  5.7343   9.7544 101.7633  51.9623 133.0208]\n",
      "Weights: [-4.788   0.7049 -1.1901  0.1073  0.1436]\n",
      "MSE loss: 91.6785\n",
      "Iteration: 110600\n",
      "Gradient: [  1.8578  -0.9573 -40.3582  71.8109 -65.6609]\n",
      "Weights: [-4.8017  0.7031 -1.1896  0.1079  0.1435]\n",
      "MSE loss: 91.9458\n",
      "Iteration: 110700\n",
      "Gradient: [  3.4617 -13.74    -6.4885   6.6823 -14.8005]\n",
      "Weights: [-4.7818  0.701  -1.1903  0.1077  0.1434]\n",
      "MSE loss: 91.6891\n",
      "Iteration: 110800\n",
      "Gradient: [ -2.8331   9.4072   5.7769 -48.083   29.3278]\n",
      "Weights: [-4.8016  0.7092 -1.1898  0.1078  0.1433]\n",
      "MSE loss: 91.7875\n",
      "Iteration: 110900\n",
      "Gradient: [   6.8494    4.3824  -14.9981  -96.9824 -142.885 ]\n",
      "Weights: [-4.776   0.703  -1.192   0.1086  0.1432]\n",
      "MSE loss: 91.7679\n",
      "Iteration: 111000\n",
      "Gradient: [  9.3629 -17.3402  21.65    29.1126 132.6513]\n",
      "Weights: [-4.7813  0.699  -1.1918  0.1089  0.1434]\n",
      "MSE loss: 91.6367\n",
      "Iteration: 111100\n",
      "Gradient: [ -8.5552   0.7258  28.4347 110.0141 -81.0867]\n",
      "Weights: [-4.771   0.6967 -1.1923  0.1085  0.1436]\n",
      "MSE loss: 91.7527\n",
      "Iteration: 111200\n",
      "Gradient: [  0.5149  -1.9802 -13.7429 105.3453  56.1197]\n",
      "Weights: [-4.7828  0.6911 -1.1913  0.1083  0.1439]\n",
      "MSE loss: 91.7444\n",
      "Iteration: 111300\n",
      "Gradient: [ -8.9582  -3.9084   4.9098 -84.7418 146.2467]\n",
      "Weights: [-4.7906  0.701  -1.1932  0.1086  0.1437]\n",
      "MSE loss: 91.7287\n",
      "Iteration: 111400\n",
      "Gradient: [   5.0996   18.5472   27.2122   30.3261 -112.3219]\n",
      "Weights: [-4.7523  0.691  -1.1916  0.1087  0.1436]\n",
      "MSE loss: 92.4917\n",
      "Iteration: 111500\n",
      "Gradient: [ -4.9466   1.5774  42.0487 -10.4078  82.9793]\n",
      "Weights: [-4.7853  0.6952 -1.1885  0.108   0.1436]\n",
      "MSE loss: 91.6844\n",
      "Iteration: 111600\n",
      "Gradient: [  4.341   -7.5743  12.7231 -40.238  -50.1625]\n",
      "Weights: [-4.78    0.689  -1.1898  0.1084  0.1436]\n",
      "MSE loss: 91.7507\n",
      "Iteration: 111700\n",
      "Gradient: [  7.1202  12.4131 -17.6234  36.0886 316.1075]\n",
      "Weights: [-4.7746  0.6925 -1.1884  0.1086  0.1436]\n",
      "MSE loss: 91.9545\n",
      "Iteration: 111800\n",
      "Gradient: [ 6.1809  3.3891 54.156  14.7243 54.084 ]\n",
      "Weights: [-4.76    0.6939 -1.1893  0.1078  0.1434]\n",
      "MSE loss: 92.176\n",
      "Iteration: 111900\n",
      "Gradient: [  -3.2984   15.5581   -5.4538    2.398  -216.401 ]\n",
      "Weights: [-4.7791  0.69   -1.1857  0.1083  0.1434]\n",
      "MSE loss: 91.7668\n",
      "Iteration: 112000\n",
      "Gradient: [   7.4342   -2.1698   13.8185   27.2439 -168.2539]\n",
      "Weights: [-4.7858  0.6973 -1.1882  0.1076  0.1434]\n",
      "MSE loss: 91.7036\n",
      "Iteration: 112100\n",
      "Gradient: [ 4.1572 15.0862 36.7767 21.0958  6.962 ]\n",
      "Weights: [-4.7768  0.6875 -1.186   0.1082  0.1434]\n",
      "MSE loss: 91.663\n",
      "Iteration: 112200\n",
      "Gradient: [   3.6302    2.9865    8.5537 -199.0046  -79.7347]\n",
      "Weights: [-4.7652  0.681  -1.1867  0.1079  0.1436]\n",
      "MSE loss: 91.733\n",
      "Iteration: 112300\n",
      "Gradient: [  7.3215  -0.2973   5.4876 -70.6321 -58.5971]\n",
      "Weights: [-4.7706  0.6831 -1.1876  0.108   0.1437]\n",
      "MSE loss: 91.6938\n",
      "Iteration: 112400\n",
      "Gradient: [  11.5995   -0.9589   29.2709   81.0485 -204.0036]\n",
      "Weights: [-4.7743  0.6955 -1.1898  0.1084  0.1436]\n",
      "MSE loss: 91.8473\n",
      "Iteration: 112500\n",
      "Gradient: [  0.6591 -12.4505  45.1331  85.0734 239.1934]\n",
      "Weights: [-4.7725  0.6861 -1.1896  0.1084  0.1437]\n",
      "MSE loss: 91.6838\n",
      "Iteration: 112600\n",
      "Gradient: [ 17.084   11.1132  22.2276   2.999  207.3429]\n",
      "Weights: [-4.7651  0.684  -1.1881  0.1094  0.1436]\n",
      "MSE loss: 92.1051\n",
      "Iteration: 112700\n",
      "Gradient: [ -2.0308   2.7159 -12.7911   8.128  214.6457]\n",
      "Weights: [-4.7636  0.6785 -1.1872  0.1094  0.1435]\n",
      "MSE loss: 91.7944\n",
      "Iteration: 112800\n",
      "Gradient: [ -14.1415   -6.8598   10.6696  -92.5802 -219.8802]\n",
      "Weights: [-4.7951  0.6886 -1.188   0.1092  0.1433]\n",
      "MSE loss: 92.1972\n",
      "Iteration: 112900\n",
      "Gradient: [  6.0552   7.0225 -17.4429  44.1737 169.7713]\n",
      "Weights: [-4.7839  0.702  -1.1887  0.1089  0.143 ]\n",
      "MSE loss: 91.7244\n",
      "Iteration: 113000\n",
      "Gradient: [ -1.8139  -2.913   21.0259 -20.7563 -51.807 ]\n",
      "Weights: [-4.776   0.6888 -1.1863  0.109   0.1429]\n",
      "MSE loss: 91.6814\n",
      "Iteration: 113100\n",
      "Gradient: [  0.9061  -9.0427  41.0178  76.086  369.6711]\n",
      "Weights: [-4.7714  0.6793 -1.1856  0.1093  0.143 ]\n",
      "MSE loss: 91.7623\n",
      "Iteration: 113200\n",
      "Gradient: [   7.0795   16.0289  -14.4345 -110.2293 -170.7989]\n",
      "Weights: [-4.7582  0.6779 -1.186   0.1095  0.1431]\n",
      "MSE loss: 91.8617\n",
      "Iteration: 113300\n",
      "Gradient: [   3.0735  -19.3247  -10.2499 -118.995   -77.8088]\n",
      "Weights: [-4.7764  0.6773 -1.1803  0.1084  0.1429]\n",
      "MSE loss: 91.6899\n",
      "Iteration: 113400\n",
      "Gradient: [ 11.662  -18.3076   3.1623 130.3928 234.2374]\n",
      "Weights: [-4.7639  0.6785 -1.1802  0.1087  0.1429]\n",
      "MSE loss: 92.0913\n",
      "Iteration: 113500\n",
      "Gradient: [  -8.14     -3.8511   -2.9468 -142.4123  -10.3816]\n",
      "Weights: [-4.789   0.687  -1.1817  0.1087  0.1428]\n",
      "MSE loss: 91.7467\n",
      "Iteration: 113600\n",
      "Gradient: [-6.227000e+00 -4.354700e+00  1.108000e-01 -2.112290e+01  2.198241e+02]\n",
      "Weights: [-4.7812  0.6885 -1.186   0.1093  0.143 ]\n",
      "MSE loss: 91.6521\n",
      "Iteration: 113700\n",
      "Gradient: [ -0.3121   6.8106  11.8552  70.9319 -37.9824]\n",
      "Weights: [-4.773   0.6824 -1.1833  0.1094  0.1427]\n",
      "MSE loss: 91.6723\n",
      "Iteration: 113800\n",
      "Gradient: [  -1.464   -11.7281  -30.0442  -41.9214 -123.6779]\n",
      "Weights: [-4.777   0.6675 -1.1773  0.1098  0.1426]\n",
      "MSE loss: 91.7537\n",
      "Iteration: 113900\n",
      "Gradient: [   1.1395   -7.0229    9.5294  -26.5146 -178.449 ]\n",
      "Weights: [-4.7641  0.664  -1.1791  0.1104  0.1427]\n",
      "MSE loss: 91.7233\n",
      "Iteration: 114000\n",
      "Gradient: [ 6.468   0.8827  0.7789 83.8725 40.6686]\n",
      "Weights: [-4.7682  0.6632 -1.1757  0.1097  0.1424]\n",
      "MSE loss: 91.7169\n",
      "Iteration: 114100\n",
      "Gradient: [  -2.4884  -17.576    13.3665   27.6139 -358.9762]\n",
      "Weights: [-4.7637  0.6593 -1.1766  0.1096  0.1426]\n",
      "MSE loss: 91.7862\n",
      "Iteration: 114200\n",
      "Gradient: [12.6056  3.1023 45.5376 50.9454 85.5998]\n",
      "Weights: [-4.742   0.6554 -1.1776  0.1099  0.1426]\n",
      "MSE loss: 92.1213\n",
      "Iteration: 114300\n",
      "Gradient: [-1.448900e+00  2.082850e+01 -1.983000e-01  8.281880e+01 -2.255141e+02]\n",
      "Weights: [-4.7698  0.668  -1.177   0.1098  0.1424]\n",
      "MSE loss: 91.6879\n",
      "Iteration: 114400\n",
      "Gradient: [  2.5026  -6.8806 -23.0022   1.5646 -35.0157]\n",
      "Weights: [-4.7669  0.6824 -1.1813  0.1092  0.1424]\n",
      "MSE loss: 91.8369\n",
      "Iteration: 114500\n",
      "Gradient: [ 0.9346 13.2957 43.7483 41.6642 37.9054]\n",
      "Weights: [-4.7777  0.674  -1.1775  0.1094  0.1426]\n",
      "MSE loss: 91.7167\n",
      "Iteration: 114600\n",
      "Gradient: [   5.3925   -1.1175   -2.7018 -103.3183  -40.3687]\n",
      "Weights: [-4.7633  0.6724 -1.1806  0.1099  0.1427]\n",
      "MSE loss: 91.8374\n",
      "Iteration: 114700\n",
      "Gradient: [ -7.5955 -10.2404 -21.8614  70.831   -2.0137]\n",
      "Weights: [-4.7955  0.6714 -1.1785  0.1103  0.1427]\n",
      "MSE loss: 92.3537\n",
      "Iteration: 114800\n",
      "Gradient: [ -5.297    8.9864  42.9217  -5.8168 -31.6991]\n",
      "Weights: [-4.7742  0.678  -1.1794  0.11    0.1424]\n",
      "MSE loss: 91.7347\n",
      "Iteration: 114900\n",
      "Gradient: [  -8.0732  -16.1033  -41.156     1.8189 -469.1451]\n",
      "Weights: [-4.7831  0.6698 -1.1801  0.1107  0.1424]\n",
      "MSE loss: 92.0363\n",
      "Iteration: 115000\n",
      "Gradient: [ -1.1555  -6.1828  51.5393  45.8971 115.8818]\n",
      "Weights: [-4.7553  0.6575 -1.1819  0.1117  0.1426]\n",
      "MSE loss: 91.7853\n",
      "Iteration: 115100\n",
      "Gradient: [  -0.9969  -18.7297   26.5407 -161.7886 -162.9014]\n",
      "Weights: [-4.777   0.6693 -1.1795  0.1112  0.1423]\n",
      "MSE loss: 91.7225\n",
      "Iteration: 115200\n",
      "Gradient: [  3.6779 -12.2715 -22.6633 -44.8668  32.7456]\n",
      "Weights: [-4.7721  0.6752 -1.1805  0.1107  0.1422]\n",
      "MSE loss: 91.6619\n",
      "Iteration: 115300\n",
      "Gradient: [  1.7711  14.1832  29.1686 -39.8879 196.8671]\n",
      "Weights: [-4.7705  0.6797 -1.1815  0.1113  0.1421]\n",
      "MSE loss: 91.8058\n",
      "Iteration: 115400\n",
      "Gradient: [  5.9404   0.7949 -13.5224  93.9908 212.5133]\n",
      "Weights: [-4.7718  0.6732 -1.1831  0.1116  0.1423]\n",
      "MSE loss: 91.6513\n",
      "Iteration: 115500\n",
      "Gradient: [ -5.6314 -18.5113  -9.1385  39.2047 -48.8593]\n",
      "Weights: [-4.7752  0.6764 -1.1846  0.1111  0.1423]\n",
      "MSE loss: 91.8113\n",
      "Iteration: 115600\n",
      "Gradient: [  -1.1929    5.3694   26.2397  -38.7194 -149.5548]\n",
      "Weights: [-4.791   0.694  -1.1864  0.1103  0.1423]\n",
      "MSE loss: 91.7616\n",
      "Iteration: 115700\n",
      "Gradient: [ -4.3289 -24.0507  19.0008  39.4397 228.7655]\n",
      "Weights: [-4.7879  0.6878 -1.1862  0.1106  0.1429]\n",
      "MSE loss: 91.8207\n",
      "Iteration: 115800\n",
      "Gradient: [  -1.1839   17.2609   14.996   -38.0713 -178.4085]\n",
      "Weights: [-4.7763  0.6899 -1.1885  0.1102  0.1429]\n",
      "MSE loss: 91.6611\n",
      "Iteration: 115900\n",
      "Gradient: [-14.481    1.9176  -3.0761 -19.9046 338.5245]\n",
      "Weights: [-4.7815  0.6888 -1.1879  0.1103  0.1429]\n",
      "MSE loss: 91.6445\n",
      "Iteration: 116000\n",
      "Gradient: [ -5.4562  -6.5238 -14.3901  82.004  278.3683]\n",
      "Weights: [-4.7955  0.6926 -1.1872  0.1108  0.1425]\n",
      "MSE loss: 91.8455\n",
      "Iteration: 116100\n",
      "Gradient: [  5.2646   3.6377   0.2971 -58.7281  68.125 ]\n",
      "Weights: [-4.7828  0.6864 -1.1844  0.111   0.1424]\n",
      "MSE loss: 91.6843\n",
      "Iteration: 116200\n",
      "Gradient: [ -1.4792 -24.5808  28.4383  53.3143  24.2914]\n",
      "Weights: [-4.7714  0.6748 -1.1819  0.1118  0.1423]\n",
      "MSE loss: 91.7841\n",
      "Iteration: 116300\n",
      "Gradient: [-1.6087 -9.4159  2.591  53.0557 66.4774]\n",
      "Weights: [-4.765   0.6759 -1.1844  0.1114  0.1423]\n",
      "MSE loss: 91.6986\n",
      "Iteration: 116400\n",
      "Gradient: [  -2.1916   10.1641    2.3918   76.6883 -246.1255]\n",
      "Weights: [-4.782   0.6817 -1.1865  0.1129  0.142 ]\n",
      "MSE loss: 91.6738\n",
      "Iteration: 116500\n",
      "Gradient: [-11.3403  12.866  -11.4696 -54.4583 -86.8539]\n",
      "Weights: [-4.767   0.6748 -1.1843  0.1129  0.142 ]\n",
      "MSE loss: 91.6997\n",
      "Iteration: 116600\n",
      "Gradient: [   0.768     0.5567  -19.2672 -107.6507   72.7055]\n",
      "Weights: [-4.7619  0.6773 -1.1858  0.1132  0.1417]\n",
      "MSE loss: 91.7824\n",
      "Iteration: 116700\n",
      "Gradient: [13.5745 18.4876 49.7672 48.5214 19.4369]\n",
      "Weights: [-4.7688  0.6703 -1.1829  0.1142  0.1418]\n",
      "MSE loss: 91.9248\n",
      "Iteration: 116800\n",
      "Gradient: [   5.9854   11.1133   46.9448    8.6075 -104.1993]\n",
      "Weights: [-4.7731  0.6715 -1.1845  0.1141  0.1418]\n",
      "MSE loss: 91.6524\n",
      "Iteration: 116900\n",
      "Gradient: [  9.5829   2.7156  54.8589  23.8469 132.8346]\n",
      "Weights: [-4.7852  0.6791 -1.1868  0.1137  0.142 ]\n",
      "MSE loss: 91.7538\n",
      "Iteration: 117000\n",
      "Gradient: [   7.0747    7.9392   38.7124  -60.8818 -137.2897]\n",
      "Weights: [-4.78    0.6752 -1.1843  0.1133  0.1417]\n",
      "MSE loss: 91.7485\n",
      "Iteration: 117100\n",
      "Gradient: [   7.5377   14.0951    9.7448 -102.0572  154.0322]\n",
      "Weights: [-4.7803  0.6749 -1.1836  0.1138  0.1415]\n",
      "MSE loss: 91.7146\n",
      "Iteration: 117200\n",
      "Gradient: [  10.0765    5.8439  -11.7671  -20.7273 -152.9707]\n",
      "Weights: [-4.7794  0.6742 -1.1814  0.113   0.1414]\n",
      "MSE loss: 91.7753\n",
      "Iteration: 117300\n",
      "Gradient: [  3.348    5.1516  19.7041 135.259  226.5269]\n",
      "Weights: [-4.7714  0.6692 -1.1812  0.1144  0.1416]\n",
      "MSE loss: 91.902\n",
      "Iteration: 117400\n",
      "Gradient: [   3.8436   24.4144   44.7782   28.8252 -119.84  ]\n",
      "Weights: [-4.7832  0.6729 -1.1794  0.1138  0.1415]\n",
      "MSE loss: 91.7921\n",
      "Iteration: 117500\n",
      "Gradient: [  -5.4734   14.4261   -1.472   -80.2878 -351.8709]\n",
      "Weights: [-4.772   0.6629 -1.1783  0.1139  0.1412]\n",
      "MSE loss: 91.7106\n",
      "Iteration: 117600\n",
      "Gradient: [  4.8281  -9.9214  12.1617 -95.9773 -35.5828]\n",
      "Weights: [-4.7575  0.6583 -1.1791  0.1145  0.1412]\n",
      "MSE loss: 91.7153\n",
      "Iteration: 117700\n",
      "Gradient: [  1.5477  15.6764  12.9849  44.6611 398.8576]\n",
      "Weights: [-4.7714  0.6636 -1.1786  0.1144  0.1414]\n",
      "MSE loss: 91.7551\n",
      "Iteration: 117800\n",
      "Gradient: [  0.9354  -8.4489 -42.8858  -8.6609 323.315 ]\n",
      "Weights: [-4.7735  0.6617 -1.1795  0.1144  0.1414]\n",
      "MSE loss: 91.7248\n",
      "Iteration: 117900\n",
      "Gradient: [ -0.5726  -2.2505   7.6389  14.1768 -25.7305]\n",
      "Weights: [-4.7707  0.6534 -1.1773  0.1148  0.1413]\n",
      "MSE loss: 91.7967\n",
      "Iteration: 118000\n",
      "Gradient: [-10.9988  -2.8301 -25.0192 -24.6239 -62.1241]\n",
      "Weights: [-4.7582  0.6601 -1.1782  0.1147  0.141 ]\n",
      "MSE loss: 91.7597\n",
      "Iteration: 118100\n",
      "Gradient: [  -2.6884  -17.0625    4.0686  -41.1432 -234.9872]\n",
      "Weights: [-4.7796  0.6602 -1.1796  0.1152  0.1411]\n",
      "MSE loss: 91.9461\n",
      "Iteration: 118200\n",
      "Gradient: [  1.6777   1.3953   5.6896 107.9152 342.5653]\n",
      "Weights: [-4.7728  0.6719 -1.1808  0.1148  0.1412]\n",
      "MSE loss: 91.7716\n",
      "Iteration: 118300\n",
      "Gradient: [ 10.4169  17.5103  28.6982  27.4433 178.7166]\n",
      "Weights: [-4.7623  0.6652 -1.1792  0.1145  0.1413]\n",
      "MSE loss: 91.9781\n",
      "Iteration: 118400\n",
      "Gradient: [  -1.1125   -6.3464   -1.4546  -37.9503 -175.0022]\n",
      "Weights: [-4.7678  0.6718 -1.1835  0.1145  0.1412]\n",
      "MSE loss: 91.6594\n",
      "Iteration: 118500\n",
      "Gradient: [  -4.3433   -9.8147  -13.3775 -118.6329  221.5757]\n",
      "Weights: [-4.776   0.6733 -1.1872  0.1156  0.1413]\n",
      "MSE loss: 91.7027\n",
      "Iteration: 118600\n",
      "Gradient: [ -6.8694   0.5663   9.414   13.4162 190.9464]\n",
      "Weights: [-4.7859  0.6755 -1.1837  0.1159  0.1411]\n",
      "MSE loss: 91.7282\n",
      "Iteration: 118700\n",
      "Gradient: [  5.0192  27.2268 -16.6005 -45.6249 234.6049]\n",
      "Weights: [-4.7886  0.6801 -1.1848  0.1162  0.1409]\n",
      "MSE loss: 91.7319\n",
      "Iteration: 118800\n",
      "Gradient: [ -3.9641  20.7513  -3.5716  -4.123  -38.017 ]\n",
      "Weights: [-4.79    0.6749 -1.1829  0.116   0.1408]\n",
      "MSE loss: 91.894\n",
      "Iteration: 118900\n",
      "Gradient: [  3.2585 -15.5848  18.6584 -32.6229  -4.4393]\n",
      "Weights: [-4.7639  0.6644 -1.1821  0.1156  0.1409]\n",
      "MSE loss: 91.6617\n",
      "Iteration: 119000\n",
      "Gradient: [ -3.1907  11.3907  10.5024 -20.6004 -98.8677]\n",
      "Weights: [-4.7551  0.6642 -1.1825  0.1166  0.1407]\n",
      "MSE loss: 91.8918\n",
      "Iteration: 119100\n",
      "Gradient: [ -21.2734   -4.5549   17.3628  -53.32   -377.6479]\n",
      "Weights: [-4.7594  0.6537 -1.1814  0.1162  0.141 ]\n",
      "MSE loss: 91.7305\n",
      "Iteration: 119200\n",
      "Gradient: [-0.5122 -5.4114 42.2144 77.276  41.496 ]\n",
      "Weights: [-4.744   0.6502 -1.1823  0.1157  0.1412]\n",
      "MSE loss: 91.9374\n",
      "Iteration: 119300\n",
      "Gradient: [  6.4739   8.4328 -32.4969 -79.8567  20.4044]\n",
      "Weights: [-4.7621  0.652  -1.1805  0.1157  0.1413]\n",
      "MSE loss: 91.7449\n",
      "Iteration: 119400\n",
      "Gradient: [  2.7749  11.2698  18.1552  16.695  210.3328]\n",
      "Weights: [-4.7605  0.6614 -1.1845  0.1166  0.1413]\n",
      "MSE loss: 91.7356\n",
      "Iteration: 119500\n",
      "Gradient: [  8.165   -0.9526 -28.9616 102.2756  46.0121]\n",
      "Weights: [-4.7777  0.6736 -1.1876  0.1158  0.1412]\n",
      "MSE loss: 91.8077\n",
      "Iteration: 119600\n",
      "Gradient: [  2.0807  -1.453    2.9456 -12.8577 -41.9308]\n",
      "Weights: [-4.7697  0.6855 -1.188   0.1151  0.1411]\n",
      "MSE loss: 91.7882\n",
      "Iteration: 119700\n",
      "Gradient: [ -9.7503   6.581   -1.7576 -19.7202 146.4015]\n",
      "Weights: [-4.7916  0.6867 -1.1895  0.1155  0.1411]\n",
      "MSE loss: 91.9774\n",
      "Iteration: 119800\n",
      "Gradient: [   8.0296    5.7099   -5.4183   -1.6365 -191.7326]\n",
      "Weights: [-4.7909  0.689  -1.1922  0.1157  0.1413]\n",
      "MSE loss: 91.888\n",
      "Iteration: 119900\n",
      "Gradient: [ -1.0954  -8.6974  17.5984   6.4774 138.0621]\n",
      "Weights: [-4.7845  0.6867 -1.1919  0.1161  0.1415]\n",
      "MSE loss: 91.6225\n",
      "Iteration: 120000\n",
      "Gradient: [  11.0897  -18.0563   -7.6965   57.8454 -119.8898]\n",
      "Weights: [-4.756   0.6827 -1.1943  0.1159  0.1414]\n",
      "MSE loss: 91.8938\n",
      "Iteration: 120100\n",
      "Gradient: [   2.7087   -6.0606    5.9578 -114.9324 -283.2244]\n",
      "Weights: [-4.7759  0.6775 -1.1923  0.1174  0.1412]\n",
      "MSE loss: 91.622\n",
      "Iteration: 120200\n",
      "Gradient: [ -1.9779 -11.3887 -20.3028  11.1632 -86.9933]\n",
      "Weights: [-4.7763  0.686  -1.1914  0.1169  0.141 ]\n",
      "MSE loss: 91.5874\n",
      "Iteration: 120300\n",
      "Gradient: [-6.9883 -7.2625 13.3124 50.1911 69.3815]\n",
      "Weights: [-4.7742  0.6862 -1.1913  0.1174  0.1407]\n",
      "MSE loss: 91.6177\n",
      "Iteration: 120400\n",
      "Gradient: [ -5.8841   5.5308  25.2589 100.2421 135.2464]\n",
      "Weights: [-4.7767  0.6839 -1.1925  0.1184  0.1405]\n",
      "MSE loss: 91.5783\n",
      "Iteration: 120500\n",
      "Gradient: [ 1.3997 21.8692 36.3061 21.9214 -4.0669]\n",
      "Weights: [-4.7833  0.696  -1.1935  0.118   0.1405]\n",
      "MSE loss: 91.6348\n",
      "Iteration: 120600\n",
      "Gradient: [-10.5655 -15.1615  -9.1271 -28.9306 -98.039 ]\n",
      "Weights: [-4.7987  0.6921 -1.1934  0.1181  0.1407]\n",
      "MSE loss: 91.8995\n",
      "Iteration: 120700\n",
      "Gradient: [   8.2297   -1.3269   34.9746 -118.3587   78.8883]\n",
      "Weights: [-4.7829  0.6893 -1.1927  0.118   0.1406]\n",
      "MSE loss: 91.5774\n",
      "Iteration: 120800\n",
      "Gradient: [ -2.9746   6.7716   2.8918  66.3802 102.871 ]\n",
      "Weights: [-4.7689  0.685  -1.1952  0.1181  0.1409]\n",
      "MSE loss: 91.5953\n",
      "Iteration: 120900\n",
      "Gradient: [  7.3986  -4.2172  -8.9002 -20.8823 298.1396]\n",
      "Weights: [-4.7737  0.6838 -1.196   0.1188  0.1408]\n",
      "MSE loss: 91.5494\n",
      "Iteration: 121000\n",
      "Gradient: [ -4.4826 -17.4719 -14.8478  70.2673 -54.4513]\n",
      "Weights: [-4.7758  0.6753 -1.1947  0.1198  0.141 ]\n",
      "MSE loss: 91.6316\n",
      "Iteration: 121100\n",
      "Gradient: [ -5.4559  -6.1765  -2.6509  61.4087 -44.1032]\n",
      "Weights: [-4.782   0.6826 -1.1952  0.1188  0.141 ]\n",
      "MSE loss: 91.6232\n",
      "Iteration: 121200\n",
      "Gradient: [  3.1294   2.0637 -15.3663 -18.3397 -30.6064]\n",
      "Weights: [-4.7696  0.689  -1.1947  0.1187  0.1409]\n",
      "MSE loss: 92.0713\n",
      "Iteration: 121300\n",
      "Gradient: [ -8.2239   6.0122  -7.0471  61.8294 281.7417]\n",
      "Weights: [-4.7705  0.684  -1.1971  0.1187  0.141 ]\n",
      "MSE loss: 91.5487\n",
      "Iteration: 121400\n",
      "Gradient: [ 11.9226  -4.8467  37.8452 -11.9466 104.8123]\n",
      "Weights: [-4.7725  0.6959 -1.2016  0.119   0.1412]\n",
      "MSE loss: 91.6485\n",
      "Iteration: 121500\n",
      "Gradient: [ -10.4903  -14.5146  -28.1225 -118.4638  -88.4135]\n",
      "Weights: [-4.7788  0.6902 -1.2022  0.1188  0.1414]\n",
      "MSE loss: 91.6375\n",
      "Iteration: 121600\n",
      "Gradient: [  -1.0148    8.7047  -10.5421 -113.8134 -176.9532]\n",
      "Weights: [-4.778   0.696  -1.2052  0.1196  0.1414]\n",
      "MSE loss: 91.5041\n",
      "Iteration: 121700\n",
      "Gradient: [  11.2336    4.029    21.4794 -124.0805  -14.3626]\n",
      "Weights: [-4.769   0.6934 -1.2047  0.1192  0.1414]\n",
      "MSE loss: 91.5518\n",
      "Iteration: 121800\n",
      "Gradient: [   0.6445   -0.9443   -5.7218  -31.2752 -225.5419]\n",
      "Weights: [-4.7798  0.7037 -1.2071  0.1187  0.1413]\n",
      "MSE loss: 91.6419\n",
      "Iteration: 121900\n",
      "Gradient: [  5.9166   0.5045 -14.1724 148.3644 239.3088]\n",
      "Weights: [-4.7677  0.7005 -1.2048  0.1184  0.1415]\n",
      "MSE loss: 91.7444\n",
      "Iteration: 122000\n",
      "Gradient: [ 12.6614 -10.2918  33.5656 102.6347 118.0543]\n",
      "Weights: [-4.7617  0.6967 -1.2026  0.1184  0.1413]\n",
      "MSE loss: 91.9673\n",
      "Iteration: 122100\n",
      "Gradient: [ -4.7607  -6.4336  -0.6001 -24.9971 -99.8933]\n",
      "Weights: [-4.7706  0.6878 -1.2008  0.1191  0.1413]\n",
      "MSE loss: 91.5588\n",
      "Iteration: 122200\n",
      "Gradient: [   8.5787    8.6228  -20.1131  119.1987 -175.434 ]\n",
      "Weights: [-4.7751  0.686  -1.1994  0.1187  0.1414]\n",
      "MSE loss: 91.5469\n",
      "Iteration: 122300\n",
      "Gradient: [  -0.6237    0.5771    1.849    38.0085 -357.612 ]\n",
      "Weights: [-4.7635  0.6918 -1.2001  0.1179  0.1414]\n",
      "MSE loss: 91.7969\n",
      "Iteration: 122400\n",
      "Gradient: [  -6.5285   -6.3193    7.1283   88.7462 -123.3583]\n",
      "Weights: [-4.7852  0.6946 -1.203   0.119   0.1414]\n",
      "MSE loss: 91.6163\n",
      "Iteration: 122500\n",
      "Gradient: [   4.6526   -7.0193   33.0566 -110.3975  368.9148]\n",
      "Weights: [-4.7746  0.6964 -1.2039  0.1188  0.1417]\n",
      "MSE loss: 91.6251\n",
      "Iteration: 122600\n",
      "Gradient: [  2.406    3.4876 -12.7555 113.1993   6.0924]\n",
      "Weights: [-4.7793  0.7025 -1.2065  0.1189  0.1416]\n",
      "MSE loss: 91.5091\n",
      "Iteration: 122700\n",
      "Gradient: [ 6.013  -4.5419 11.0116 83.5841 72.7821]\n",
      "Weights: [-4.7792  0.7    -1.208   0.1192  0.1416]\n",
      "MSE loss: 91.5147\n",
      "Iteration: 122800\n",
      "Gradient: [  -2.3001   -3.0661    6.4327 -104.7908  -69.5442]\n",
      "Weights: [-4.7852  0.6997 -1.209   0.1204  0.1416]\n",
      "MSE loss: 91.5859\n",
      "Iteration: 122900\n",
      "Gradient: [ -15.9174  -12.9822  -18.3252  117.1143 -119.9407]\n",
      "Weights: [-4.7869  0.7088 -1.2091  0.1202  0.1413]\n",
      "MSE loss: 91.5244\n",
      "Iteration: 123000\n",
      "Gradient: [ -6.9203 -19.768   -3.7085  97.6682 287.3721]\n",
      "Weights: [-4.7918  0.7176 -1.2133  0.1207  0.1414]\n",
      "MSE loss: 91.5027\n",
      "Iteration: 123100\n",
      "Gradient: [ -3.4882 -23.253  -47.7819   4.8592 214.147 ]\n",
      "Weights: [-4.8023  0.7194 -1.2155  0.1209  0.1415]\n",
      "MSE loss: 91.6906\n",
      "Iteration: 123200\n",
      "Gradient: [-10.9544  -7.9908 -19.8005 -88.3649 250.9307]\n",
      "Weights: [-4.7927  0.7156 -1.2151  0.1207  0.1415]\n",
      "MSE loss: 91.5251\n",
      "Iteration: 123300\n",
      "Gradient: [ 11.6435   9.2511 -12.2497 -36.1402 267.2809]\n",
      "Weights: [-4.7979  0.7253 -1.2153  0.1204  0.1414]\n",
      "MSE loss: 91.5165\n",
      "Iteration: 123400\n",
      "Gradient: [  4.8555   0.4243 -33.3208 -35.5898 118.4716]\n",
      "Weights: [-4.7889  0.7213 -1.2162  0.1198  0.1415]\n",
      "MSE loss: 91.5443\n",
      "Iteration: 123500\n",
      "Gradient: [  0.5587  12.2287  -1.7097  -0.4472 200.9546]\n",
      "Weights: [-4.7771  0.719  -1.2159  0.1194  0.1417]\n",
      "MSE loss: 91.5747\n",
      "Iteration: 123600\n",
      "Gradient: [ -6.7807 -13.7729  26.2059  24.2761 -11.3082]\n",
      "Weights: [-4.8056  0.7287 -1.2142  0.1193  0.1416]\n",
      "MSE loss: 91.6122\n",
      "Iteration: 123700\n",
      "Gradient: [  -8.193     9.2193  -20.6306   37.0012 -259.6507]\n",
      "Weights: [-4.7942  0.7207 -1.2174  0.1205  0.1416]\n",
      "MSE loss: 91.547\n",
      "Iteration: 123800\n",
      "Gradient: [  -0.454    -0.4912  -30.4578 -119.216  -292.1174]\n",
      "Weights: [-4.8087  0.7268 -1.2176  0.1198  0.1418]\n",
      "MSE loss: 91.8731\n",
      "Iteration: 123900\n",
      "Gradient: [  0.6315   7.9933  -0.921  -12.3467  18.4608]\n",
      "Weights: [-4.7841  0.7315 -1.221   0.1192  0.1421]\n",
      "MSE loss: 91.5707\n",
      "Iteration: 124000\n",
      "Gradient: [ -5.1328  -2.2822 -14.8082  76.3847 130.5339]\n",
      "Weights: [-4.7936  0.7331 -1.2222  0.1189  0.1422]\n",
      "MSE loss: 91.4893\n",
      "Iteration: 124100\n",
      "Gradient: [-8.401000e+00 -9.443700e+00 -3.081000e-01  9.679280e+01  3.545277e+02]\n",
      "Weights: [-4.792   0.7386 -1.2233  0.1188  0.1422]\n",
      "MSE loss: 91.5038\n",
      "Iteration: 124200\n",
      "Gradient: [ -5.2659   3.3632  -2.3735 101.2317 189.1913]\n",
      "Weights: [-4.8147  0.7494 -1.2267  0.1199  0.1422]\n",
      "MSE loss: 91.6559\n",
      "Iteration: 124300\n",
      "Gradient: [ 2.8392 15.7272 26.7175 13.1199 45.2743]\n",
      "Weights: [-4.8203  0.7635 -1.2314  0.1198  0.1421]\n",
      "MSE loss: 91.7097\n",
      "Iteration: 124400\n",
      "Gradient: [   4.7317   22.0393   36.0747   69.8356 -248.9413]\n",
      "Weights: [-4.7914  0.7558 -1.234   0.1201  0.1424]\n",
      "MSE loss: 91.6381\n",
      "Iteration: 124500\n",
      "Gradient: [  0.4205   5.4881  44.2517  69.5496 171.5776]\n",
      "Weights: [-4.7958  0.7568 -1.2332  0.1207  0.1422]\n",
      "MSE loss: 91.6579\n",
      "Iteration: 124600\n",
      "Gradient: [  -7.0427    1.6974   11.1576   -2.696  -168.8704]\n",
      "Weights: [-4.7867  0.7494 -1.2365  0.1216  0.1424]\n",
      "MSE loss: 91.5193\n",
      "Iteration: 124700\n",
      "Gradient: [   9.6942  -20.3086   27.0443  -91.5007 -174.1021]\n",
      "Weights: [-4.7883  0.7423 -1.2367  0.1218  0.1424]\n",
      "MSE loss: 91.5199\n",
      "Iteration: 124800\n",
      "Gradient: [ -5.2706  -2.8772 -55.1859 -70.3441  14.7435]\n",
      "Weights: [-4.7762  0.7331 -1.2377  0.1235  0.1424]\n",
      "MSE loss: 91.5014\n",
      "Iteration: 124900\n",
      "Gradient: [ 15.2027   5.5916  11.4452  56.096  343.8372]\n",
      "Weights: [-4.795   0.7477 -1.238   0.1234  0.1422]\n",
      "MSE loss: 91.4159\n",
      "Iteration: 125000\n",
      "Gradient: [  7.0013  -5.4953   3.5018  98.5562 -26.8099]\n",
      "Weights: [-4.8074  0.754  -1.2369  0.1236  0.142 ]\n",
      "MSE loss: 91.5008\n",
      "Iteration: 125100\n",
      "Gradient: [  3.5656   0.6095  29.4952 -42.2958  -8.4027]\n",
      "Weights: [-4.796   0.7545 -1.2379  0.1232  0.1419]\n",
      "MSE loss: 91.4347\n",
      "Iteration: 125200\n",
      "Gradient: [   0.6343  -22.1592  -30.3565  -65.2928 -125.7344]\n",
      "Weights: [-4.8206  0.7694 -1.242   0.1233  0.1418]\n",
      "MSE loss: 91.7278\n",
      "Iteration: 125300\n",
      "Gradient: [-12.2534  -6.5056 -27.1896 149.6364 145.4207]\n",
      "Weights: [-4.8107  0.768  -1.2404  0.1231  0.1418]\n",
      "MSE loss: 91.4899\n",
      "Iteration: 125400\n",
      "Gradient: [ -2.4806 -11.4058 -32.5781  -5.1595 -31.2278]\n",
      "Weights: [-4.7992  0.7529 -1.2379  0.1232  0.1419]\n",
      "MSE loss: 91.4178\n",
      "Iteration: 125500\n",
      "Gradient: [   4.4764  -20.2712  -10.3823  -55.0414 -302.7246]\n",
      "Weights: [-4.8025  0.7646 -1.2401  0.1229  0.1417]\n",
      "MSE loss: 91.4982\n",
      "Iteration: 125600\n",
      "Gradient: [   9.2553   -3.0887   14.4006  117.4414 -106.8731]\n",
      "Weights: [-4.8008  0.7667 -1.2443  0.1228  0.1421]\n",
      "MSE loss: 91.5236\n",
      "Iteration: 125700\n",
      "Gradient: [  1.7099   4.0621  39.3134 -65.5506 -19.8609]\n",
      "Weights: [-4.8001  0.7553 -1.2428  0.124   0.1425]\n",
      "MSE loss: 91.4597\n",
      "Iteration: 125800\n",
      "Gradient: [ 5.3411 13.9894 -0.7746 48.4452 39.9015]\n",
      "Weights: [-4.807   0.7663 -1.2449  0.1238  0.1422]\n",
      "MSE loss: 91.423\n",
      "Iteration: 125900\n",
      "Gradient: [  -9.2891    1.3112  -35.4399    8.1395 -322.938 ]\n",
      "Weights: [-4.7958  0.7455 -1.2403  0.1239  0.1423]\n",
      "MSE loss: 91.4684\n",
      "Iteration: 126000\n",
      "Gradient: [ -4.0222   0.9153  18.8083  46.8287 -22.1474]\n",
      "Weights: [-4.7994  0.7632 -1.2422  0.1239  0.1421]\n",
      "MSE loss: 91.6322\n",
      "Iteration: 126100\n",
      "Gradient: [ 13.1955  -3.7737  32.7575  23.6265 -45.442 ]\n",
      "Weights: [-4.8007  0.7678 -1.2451  0.1246  0.1418]\n",
      "MSE loss: 91.4632\n",
      "Iteration: 126200\n",
      "Gradient: [  2.6541  -1.8356 -11.285   92.0199 161.3677]\n",
      "Weights: [-4.7932  0.7647 -1.2474  0.1251  0.1419]\n",
      "MSE loss: 91.4665\n",
      "Iteration: 126300\n",
      "Gradient: [  -5.3114    3.8237  -36.2292  -92.467  -139.0467]\n",
      "Weights: [-4.7883  0.7472 -1.245   0.125   0.1422]\n",
      "MSE loss: 91.4997\n",
      "Iteration: 126400\n",
      "Gradient: [ -4.2131   0.3188  -4.574  -54.8377 132.5165]\n",
      "Weights: [-4.7974  0.7519 -1.2435  0.1248  0.142 ]\n",
      "MSE loss: 91.4736\n",
      "Iteration: 126500\n",
      "Gradient: [  -2.8964   -3.838     3.8963  -21.0476 -341.6585]\n",
      "Weights: [-4.8046  0.7586 -1.2426  0.1243  0.1422]\n",
      "MSE loss: 91.4495\n",
      "Iteration: 126600\n",
      "Gradient: [  0.6614  -4.2922  -7.0343 -79.0109  90.8195]\n",
      "Weights: [-4.8058  0.78   -1.2482  0.1236  0.1423]\n",
      "MSE loss: 91.7265\n",
      "Iteration: 126700\n",
      "Gradient: [ 15.523    4.2931  -4.6358  76.0897 -77.2404]\n",
      "Weights: [-4.8125  0.7874 -1.2502  0.1233  0.1423]\n",
      "MSE loss: 91.6233\n",
      "Iteration: 126800\n",
      "Gradient: [  -5.9542   18.1084   -0.9356   65.581  -292.1737]\n",
      "Weights: [-4.798   0.7747 -1.2494  0.123   0.1424]\n",
      "MSE loss: 91.5243\n",
      "Iteration: 126900\n",
      "Gradient: [  2.3662 -12.7077  -3.6807  54.8863  93.8744]\n",
      "Weights: [-4.7949  0.7682 -1.2498  0.123   0.1428]\n",
      "MSE loss: 91.4618\n",
      "Iteration: 127000\n",
      "Gradient: [  0.2971  -0.874   16.9419 -26.6885 -56.6658]\n",
      "Weights: [-4.8142  0.7751 -1.2487  0.1236  0.1428]\n",
      "MSE loss: 91.6204\n",
      "Iteration: 127100\n",
      "Gradient: [   4.6573    3.138    22.5361  -27.2116 -361.5597]\n",
      "Weights: [-4.8067  0.7724 -1.2507  0.1233  0.1426]\n",
      "MSE loss: 91.5254\n",
      "Iteration: 127200\n",
      "Gradient: [  4.7168  -0.2114 -42.3211   7.0885 -59.7761]\n",
      "Weights: [-4.8088  0.782  -1.2505  0.1233  0.1425]\n",
      "MSE loss: 91.5299\n",
      "Iteration: 127300\n",
      "Gradient: [  6.0108   1.1222   4.9675 -33.5941  65.6198]\n",
      "Weights: [-4.7852  0.7657 -1.2471  0.1231  0.1424]\n",
      "MSE loss: 91.7217\n",
      "Iteration: 127400\n",
      "Gradient: [ -4.5909  10.0782  14.5106  43.0714 106.7755]\n",
      "Weights: [-4.8129  0.7649 -1.2438  0.1228  0.1424]\n",
      "MSE loss: 91.601\n",
      "Iteration: 127500\n",
      "Gradient: [  6.0966 -10.7687  -9.8428  28.6851 -44.0349]\n",
      "Weights: [-4.7983  0.7555 -1.2406  0.1227  0.1422]\n",
      "MSE loss: 91.4563\n",
      "Iteration: 127600\n",
      "Gradient: [  1.1953   6.1205  -7.377   38.7579 -41.8232]\n",
      "Weights: [-4.8041  0.7688 -1.2429  0.1229  0.1422]\n",
      "MSE loss: 91.517\n",
      "Iteration: 127700\n",
      "Gradient: [  9.7229  -5.8104  11.9535 -33.8332  -3.9281]\n",
      "Weights: [-4.7949  0.7727 -1.2465  0.1222  0.1424]\n",
      "MSE loss: 91.6244\n",
      "Iteration: 127800\n",
      "Gradient: [ -5.4184  23.069   25.2472 -14.5391   4.8235]\n",
      "Weights: [-4.8088  0.7706 -1.2432  0.1223  0.1426]\n",
      "MSE loss: 91.5957\n",
      "Iteration: 127900\n",
      "Gradient: [  -8.5506    1.4163  -13.7293   52.0433 -187.2847]\n",
      "Weights: [-4.8072  0.7562 -1.2411  0.1228  0.1427]\n",
      "MSE loss: 91.5403\n",
      "Iteration: 128000\n",
      "Gradient: [-6.6531 30.5494 13.2937 26.8338 72.8619]\n",
      "Weights: [-4.7909  0.7641 -1.2432  0.1219  0.1426]\n",
      "MSE loss: 91.6186\n",
      "Iteration: 128100\n",
      "Gradient: [  -5.1224  -20.9484  -20.2967  -55.1887 -130.5946]\n",
      "Weights: [-4.8084  0.7642 -1.2438  0.122   0.1426]\n",
      "MSE loss: 91.5912\n",
      "Iteration: 128200\n",
      "Gradient: [-5.560000e-02 -1.583340e+01 -1.669360e+01  1.239160e+02  2.338821e+02]\n",
      "Weights: [-4.7911  0.7537 -1.2425  0.1231  0.1425]\n",
      "MSE loss: 91.4266\n",
      "Iteration: 128300\n",
      "Gradient: [  8.8378  11.5036  20.0088  46.7009 480.2937]\n",
      "Weights: [-4.7947  0.7713 -1.2462  0.1233  0.1424]\n",
      "MSE loss: 91.8578\n",
      "Iteration: 128400\n",
      "Gradient: [  6.2197  -0.9413  23.9209 -43.7599 154.803 ]\n",
      "Weights: [-4.8144  0.7719 -1.2479  0.1231  0.1425]\n",
      "MSE loss: 91.6017\n",
      "Iteration: 128500\n",
      "Gradient: [  2.7474   3.8533  48.8958  83.419  -68.6332]\n",
      "Weights: [-4.8012  0.7697 -1.2454  0.1236  0.1424]\n",
      "MSE loss: 91.7569\n",
      "Iteration: 128600\n",
      "Gradient: [2.316000e-01 4.515100e+00 2.101610e+01 8.284530e+01 3.333391e+02]\n",
      "Weights: [-4.8053  0.7596 -1.2433  0.123   0.1426]\n",
      "MSE loss: 91.4598\n",
      "Iteration: 128700\n",
      "Gradient: [ 12.9843   9.9975  46.5033  32.1948 314.4899]\n",
      "Weights: [-4.7989  0.7671 -1.243   0.123   0.1422]\n",
      "MSE loss: 91.5913\n",
      "Iteration: 128800\n",
      "Gradient: [  7.0326  -7.141  -46.004   14.5454 -56.6993]\n",
      "Weights: [-4.7971  0.764  -1.2423  0.1231  0.1422]\n",
      "MSE loss: 91.5412\n",
      "Iteration: 128900\n",
      "Gradient: [  1.8584  14.8678  18.0174 -63.4398 124.3779]\n",
      "Weights: [-4.775   0.7555 -1.2444  0.1233  0.1423]\n",
      "MSE loss: 91.8952\n",
      "Iteration: 129000\n",
      "Gradient: [ -2.9132  11.3584  -4.2642 -74.1426 -60.8989]\n",
      "Weights: [-4.8071  0.7622 -1.244   0.1228  0.1426]\n",
      "MSE loss: 91.492\n",
      "Iteration: 129100\n",
      "Gradient: [  5.7935  -4.8625  -6.2504 -24.2657 177.1457]\n",
      "Weights: [-4.81    0.7573 -1.2449  0.1238  0.1426]\n",
      "MSE loss: 91.6729\n",
      "Iteration: 129200\n",
      "Gradient: [  2.9869  -4.6575   5.2207 -21.514   82.6563]\n",
      "Weights: [-4.8074  0.7555 -1.2452  0.1235  0.1426]\n",
      "MSE loss: 91.8868\n",
      "Iteration: 129300\n",
      "Gradient: [  -5.7656   -8.0489  -44.2275   10.8649 -235.733 ]\n",
      "Weights: [-4.7907  0.7515 -1.2482  0.1248  0.1428]\n",
      "MSE loss: 91.445\n",
      "Iteration: 129400\n",
      "Gradient: [ -12.0121  -10.1598   -0.6538 -110.9478 -199.0513]\n",
      "Weights: [-4.7704  0.7546 -1.2476  0.1237  0.1426]\n",
      "MSE loss: 91.9355\n",
      "Iteration: 129500\n",
      "Gradient: [ 2.493000e-01  2.027740e+01  2.869600e+01 -9.744000e-01  3.547089e+02]\n",
      "Weights: [-4.7858  0.7538 -1.2433  0.1242  0.1426]\n",
      "MSE loss: 92.0186\n",
      "Iteration: 129600\n",
      "Gradient: [  2.1362  -0.5514  -8.5084 -87.8908   1.4809]\n",
      "Weights: [-4.8081  0.7573 -1.2409  0.1243  0.1421]\n",
      "MSE loss: 91.5102\n",
      "Iteration: 129700\n",
      "Gradient: [  3.6456  13.5491 -73.7767  32.117  -96.1363]\n",
      "Weights: [-4.7973  0.7478 -1.2404  0.1246  0.142 ]\n",
      "MSE loss: 91.419\n",
      "Iteration: 129800\n",
      "Gradient: [   0.9049  -13.6583  -25.8731  -81.7006 -192.046 ]\n",
      "Weights: [-4.7869  0.7462 -1.2391  0.1242  0.1422]\n",
      "MSE loss: 91.6389\n",
      "Iteration: 129900\n",
      "Gradient: [ -0.866   -9.2298  -1.816    6.767  104.823 ]\n",
      "Weights: [-4.8081  0.7627 -1.2411  0.124   0.1419]\n",
      "MSE loss: 91.4724\n",
      "Iteration: 130000\n",
      "Gradient: [  -1.9021   -3.9785  -23.3353   25.012  -121.9502]\n",
      "Weights: [-4.8056  0.7547 -1.2399  0.1236  0.1423]\n",
      "MSE loss: 91.5208\n",
      "Iteration: 130100\n",
      "Gradient: [   0.6179   -4.4053  -23.2848  -83.0629 -133.3338]\n",
      "Weights: [-4.7924  0.7426 -1.2387  0.1221  0.1426]\n",
      "MSE loss: 91.7048\n",
      "Iteration: 130200\n",
      "Gradient: [ -0.4418   9.0522   9.2567  85.4369 156.0555]\n",
      "Weights: [-4.7879  0.7454 -1.2371  0.1225  0.1425]\n",
      "MSE loss: 91.5121\n",
      "Iteration: 130300\n",
      "Gradient: [  1.7707   6.493   17.6711 -39.1719 370.4408]\n",
      "Weights: [-4.8009  0.7512 -1.2378  0.1224  0.1424]\n",
      "MSE loss: 91.4414\n",
      "Iteration: 130400\n",
      "Gradient: [   0.7483    3.171   -57.275  -108.4498  -66.1023]\n",
      "Weights: [-4.7893  0.7427 -1.236   0.122   0.1426]\n",
      "MSE loss: 91.4421\n",
      "Iteration: 130500\n",
      "Gradient: [ -13.1034    1.4452  -20.4069 -139.2314 -336.1633]\n",
      "Weights: [-4.8072  0.7496 -1.2352  0.1216  0.1425]\n",
      "MSE loss: 91.5691\n",
      "Iteration: 130600\n",
      "Gradient: [ -2.2442 -34.5275 -15.0069  37.4764 -48.1138]\n",
      "Weights: [-4.8128  0.7427 -1.2348  0.1228  0.1424]\n",
      "MSE loss: 91.9936\n",
      "Iteration: 130700\n",
      "Gradient: [  2.0913  -5.5095 -49.0125 -22.1191 -21.1806]\n",
      "Weights: [-4.7884  0.7349 -1.2337  0.1226  0.1422]\n",
      "MSE loss: 91.4886\n",
      "Iteration: 130800\n",
      "Gradient: [ -3.814   -0.8143 -26.0278  -8.1745 232.5218]\n",
      "Weights: [-4.7974  0.7448 -1.2326  0.1221  0.1421]\n",
      "MSE loss: 91.4263\n",
      "Iteration: 130900\n",
      "Gradient: [  2.6719  31.8608  41.1808 -72.7108 360.0545]\n",
      "Weights: [-4.8036  0.7533 -1.2342  0.1232  0.1418]\n",
      "MSE loss: 91.576\n",
      "Iteration: 131000\n",
      "Gradient: [ -8.8517  23.5371 -16.632  -87.504  -55.0983]\n",
      "Weights: [-4.7834  0.746  -1.2365  0.1236  0.1416]\n",
      "MSE loss: 91.5361\n",
      "Iteration: 131100\n",
      "Gradient: [  6.7773   4.1065  46.3022 -15.8068  50.7213]\n",
      "Weights: [-4.7938  0.7474 -1.2353  0.1232  0.1419]\n",
      "MSE loss: 91.4337\n",
      "Iteration: 131200\n",
      "Gradient: [   3.621   -24.5981  -43.4776 -100.0355  207.0192]\n",
      "Weights: [-4.798   0.7482 -1.2402  0.125   0.1417]\n",
      "MSE loss: 91.4313\n",
      "Iteration: 131300\n",
      "Gradient: [ -13.42     -6.0206  -10.7785   60.6025 -250.8569]\n",
      "Weights: [-4.8216  0.7551 -1.2389  0.1246  0.1417]\n",
      "MSE loss: 92.045\n",
      "Iteration: 131400\n",
      "Gradient: [  1.5544 -13.6268 -18.3343 -38.7653  50.7346]\n",
      "Weights: [-4.8053  0.7568 -1.2403  0.1243  0.1418]\n",
      "MSE loss: 91.4261\n",
      "Iteration: 131500\n",
      "Gradient: [   1.1489   -6.8698  -26.8941  -20.2879 -247.2391]\n",
      "Weights: [-4.8121  0.7637 -1.2393  0.1241  0.1415]\n",
      "MSE loss: 91.4813\n",
      "Iteration: 131600\n",
      "Gradient: [   5.4664   -9.9062  -60.6906  -40.2658 -162.9581]\n",
      "Weights: [-4.8121  0.7601 -1.2382  0.1239  0.1413]\n",
      "MSE loss: 91.6849\n",
      "Iteration: 131700\n",
      "Gradient: [   9.0215   -8.3399  -26.8166  -19.1832 -336.1723]\n",
      "Weights: [-4.7848  0.7442 -1.2366  0.124   0.1416]\n",
      "MSE loss: 91.478\n",
      "Iteration: 131800\n",
      "Gradient: [ 1.2227  5.4382  8.7671 30.861  19.3371]\n",
      "Weights: [-4.7808  0.7357 -1.2376  0.1254  0.1419]\n",
      "MSE loss: 91.4754\n",
      "Iteration: 131900\n",
      "Gradient: [3.42000e-02 2.77500e+00 7.28670e+00 9.52518e+01 6.09505e+01]\n",
      "Weights: [-4.7829  0.7354 -1.236   0.126   0.1416]\n",
      "MSE loss: 91.6109\n",
      "Iteration: 132000\n",
      "Gradient: [   6.5134   -0.7797    4.6359 -148.618     9.0377]\n",
      "Weights: [-4.7973  0.7432 -1.2371  0.1254  0.1416]\n",
      "MSE loss: 91.4105\n",
      "Iteration: 132100\n",
      "Gradient: [  1.5651   4.626   10.4341 -99.5085 293.8907]\n",
      "Weights: [-4.7957  0.7437 -1.24    0.1252  0.1419]\n",
      "MSE loss: 91.4242\n",
      "Iteration: 132200\n",
      "Gradient: [   4.0683  -12.9766   21.838    27.068  -161.9069]\n",
      "Weights: [-4.7772  0.7424 -1.241   0.1259  0.1418]\n",
      "MSE loss: 91.6997\n",
      "Iteration: 132300\n",
      "Gradient: [  -3.3143   -7.8767    8.5613 -123.9911 -247.1758]\n",
      "Weights: [-4.7872  0.7407 -1.2401  0.1255  0.1418]\n",
      "MSE loss: 91.3952\n",
      "Iteration: 132400\n",
      "Gradient: [   0.7442    9.2516  -15.3112 -118.1721  -83.2092]\n",
      "Weights: [-4.7931  0.7375 -1.2409  0.1258  0.1419]\n",
      "MSE loss: 91.6149\n",
      "Iteration: 132500\n",
      "Gradient: [ -11.5633   -4.9547   12.8502  -63.315  -263.9214]\n",
      "Weights: [-4.7956  0.7424 -1.2416  0.126   0.1417]\n",
      "MSE loss: 91.6231\n",
      "Iteration: 132600\n",
      "Gradient: [ 22.6605 -12.8442  17.228  141.017  284.4856]\n",
      "Weights: [-4.7854  0.7474 -1.2424  0.1265  0.1417]\n",
      "MSE loss: 91.6063\n",
      "Iteration: 132700\n",
      "Gradient: [  8.7063  15.6167  49.0423 -16.6589 342.674 ]\n",
      "Weights: [-4.7821  0.7494 -1.2432  0.1269  0.1416]\n",
      "MSE loss: 91.7402\n",
      "Iteration: 132800\n",
      "Gradient: [ -4.3652 -22.9946  20.7398 -38.8515 150.6522]\n",
      "Weights: [-4.7846  0.7454 -1.245   0.127   0.1415]\n",
      "MSE loss: 91.4462\n",
      "Iteration: 132900\n",
      "Gradient: [  6.452    4.118   -9.4385  53.8303 478.5424]\n",
      "Weights: [-4.7924  0.7454 -1.2455  0.1282  0.1414]\n",
      "MSE loss: 91.3622\n",
      "Iteration: 133000\n",
      "Gradient: [ -1.4929 -12.7053  39.4694  21.5596 261.9242]\n",
      "Weights: [-4.7829  0.7446 -1.2436  0.1281  0.1414]\n",
      "MSE loss: 91.6499\n",
      "Iteration: 133100\n",
      "Gradient: [ -1.8327   3.0364 -32.8686 -24.3034 264.2755]\n",
      "Weights: [-4.7927  0.7495 -1.2447  0.1284  0.1412]\n",
      "MSE loss: 91.3784\n",
      "Iteration: 133200\n",
      "Gradient: [  1.1988  13.8467  32.7831 -42.3081 176.8879]\n",
      "Weights: [-4.7856  0.7498 -1.2461  0.1287  0.141 ]\n",
      "MSE loss: 91.3821\n",
      "Iteration: 133300\n",
      "Gradient: [ -4.5659  -4.0242 -44.2149 -80.8066 -40.8269]\n",
      "Weights: [-4.7858  0.7405 -1.243   0.129   0.141 ]\n",
      "MSE loss: 91.3295\n",
      "Iteration: 133400\n",
      "Gradient: [ -8.4018  10.4207  -7.9414   4.8257 224.4445]\n",
      "Weights: [-4.8047  0.7559 -1.2457  0.1295  0.1405]\n",
      "MSE loss: 91.4345\n",
      "Iteration: 133500\n",
      "Gradient: [ -2.7    -20.6106 -33.6815  58.5527 -41.1968]\n",
      "Weights: [-4.7829  0.7481 -1.2445  0.1294  0.1406]\n",
      "MSE loss: 91.4504\n",
      "Iteration: 133600\n",
      "Gradient: [  3.9355   3.6553  24.6577 -38.3821 341.8439]\n",
      "Weights: [-4.7907  0.7492 -1.2456  0.1301  0.1407]\n",
      "MSE loss: 91.3645\n",
      "Iteration: 133700\n",
      "Gradient: [ -3.3397 -15.5287 -40.4033 -21.8769 -81.4164]\n",
      "Weights: [-4.8068  0.7527 -1.2443  0.1304  0.1403]\n",
      "MSE loss: 91.3903\n",
      "Iteration: 133800\n",
      "Gradient: [  15.0429  -38.4387  -12.677   -87.9473 -117.7784]\n",
      "Weights: [-4.7972  0.7572 -1.2436  0.1296  0.1403]\n",
      "MSE loss: 91.4302\n",
      "Iteration: 133900\n",
      "Gradient: [  -0.8238   -0.9677   18.7675    4.2335 -103.687 ]\n",
      "Weights: [-4.7951  0.7537 -1.2427  0.1283  0.1405]\n",
      "MSE loss: 91.4089\n",
      "Iteration: 134000\n",
      "Gradient: [   7.9543    3.478    11.3707 -148.0512 -197.2871]\n",
      "Weights: [-4.7901  0.7522 -1.242   0.1287  0.1406]\n",
      "MSE loss: 91.5188\n",
      "Iteration: 134100\n",
      "Gradient: [  -5.0318   13.3656    1.4889 -109.2466  -71.2955]\n",
      "Weights: [-4.8027  0.7523 -1.2431  0.1295  0.1405]\n",
      "MSE loss: 91.3424\n",
      "Iteration: 134200\n",
      "Gradient: [  -3.2304   17.6314   22.1812  -30.0845 -393.0056]\n",
      "Weights: [-4.7984  0.7495 -1.2402  0.129   0.1406]\n",
      "MSE loss: 91.4618\n",
      "Iteration: 134300\n",
      "Gradient: [  5.2546 -19.5095  38.34    90.5242  65.0323]\n",
      "Weights: [-4.7805  0.7429 -1.2405  0.129   0.1406]\n",
      "MSE loss: 91.6224\n",
      "Iteration: 134400\n",
      "Gradient: [  1.9247  -0.3068  11.687   31.775  -37.6117]\n",
      "Weights: [-4.8006  0.753  -1.2418  0.129   0.1406]\n",
      "MSE loss: 91.419\n",
      "Iteration: 134500\n",
      "Gradient: [   2.4341   13.6436   -9.1574  -16.8885 -355.8388]\n",
      "Weights: [-4.7987  0.7574 -1.2448  0.1292  0.1406]\n",
      "MSE loss: 91.388\n",
      "Iteration: 134600\n",
      "Gradient: [  -5.7863   -9.1291   28.0314  -52.6565 -160.2488]\n",
      "Weights: [-4.8064  0.7526 -1.2452  0.1296  0.1405]\n",
      "MSE loss: 91.5332\n",
      "Iteration: 134700\n",
      "Gradient: [   4.7761   -9.6991   23.5569   17.7728 -412.1119]\n",
      "Weights: [-4.776   0.7436 -1.248   0.1293  0.1407]\n",
      "MSE loss: 91.7447\n",
      "Iteration: 134800\n",
      "Gradient: [-7.800000e-03 -3.095200e+00 -1.141310e+01  1.435427e+02 -4.735570e+01]\n",
      "Weights: [-4.7901  0.7342 -1.2456  0.131   0.141 ]\n",
      "MSE loss: 91.4418\n",
      "Iteration: 134900\n",
      "Gradient: [  -2.7557   22.1881   -9.2341 -118.7746 -169.8256]\n",
      "Weights: [-4.7866  0.7324 -1.2465  0.1312  0.1409]\n",
      "MSE loss: 91.4764\n",
      "Iteration: 135000\n",
      "Gradient: [ -4.5976  -2.5164 -10.641   80.5473  70.6721]\n",
      "Weights: [-4.7768  0.7333 -1.246   0.1312  0.1408]\n",
      "MSE loss: 91.3733\n",
      "Iteration: 135100\n",
      "Gradient: [  5.1058  14.4943  27.7833 -99.8473  12.2346]\n",
      "Weights: [-4.7686  0.7284 -1.2433  0.131   0.1408]\n",
      "MSE loss: 91.5614\n",
      "Iteration: 135200\n",
      "Gradient: [-1.878   8.6562 -5.4917 40.3077 59.3703]\n",
      "Weights: [-4.7766  0.7264 -1.2439  0.1316  0.1409]\n",
      "MSE loss: 91.4837\n",
      "Iteration: 135300\n",
      "Gradient: [  -0.1517  -20.1331  -27.7032 -105.446   144.2604]\n",
      "Weights: [-4.7954  0.737  -1.2454  0.1317  0.1405]\n",
      "MSE loss: 91.4688\n",
      "Iteration: 135400\n",
      "Gradient: [  10.0229  -11.529   -16.328     7.4675 -163.9818]\n",
      "Weights: [-4.7858  0.7378 -1.2442  0.1316  0.1401]\n",
      "MSE loss: 91.3422\n",
      "Iteration: 135500\n",
      "Gradient: [ -1.4566 -11.2023 -30.1017  55.5974  55.7858]\n",
      "Weights: [-4.7947  0.7406 -1.2416  0.1317  0.14  ]\n",
      "MSE loss: 91.2983\n",
      "Iteration: 135600\n",
      "Gradient: [   0.2849    5.9431    2.4337  -51.7233 -212.5324]\n",
      "Weights: [-4.7802  0.7324 -1.2448  0.1327  0.14  ]\n",
      "MSE loss: 91.3271\n",
      "Iteration: 135700\n",
      "Gradient: [-4.01000e-02 -5.97410e+00 -5.45330e+00  6.34651e+01 -8.84865e+01]\n",
      "Weights: [-4.7876  0.7387 -1.2455  0.1331  0.1401]\n",
      "MSE loss: 91.3304\n",
      "Iteration: 135800\n",
      "Gradient: [-11.9696   0.1702   4.5471 -60.3744 -55.5197]\n",
      "Weights: [-4.7961  0.748  -1.2479  0.1326  0.1403]\n",
      "MSE loss: 91.3146\n",
      "Iteration: 135900\n",
      "Gradient: [   0.4632   10.1633   18.2933   29.5334 -101.2648]\n",
      "Weights: [-4.7768  0.7389 -1.2504  0.1319  0.1405]\n",
      "MSE loss: 91.4753\n",
      "Iteration: 136000\n",
      "Gradient: [ -2.83     3.8468  21.7728 -55.8299  -4.5101]\n",
      "Weights: [-4.791   0.7422 -1.2477  0.1317  0.1409]\n",
      "MSE loss: 91.4475\n",
      "Iteration: 136100\n",
      "Gradient: [  7.77     4.6975 -26.7677  44.8659  81.8473]\n",
      "Weights: [-4.7707  0.7386 -1.2496  0.1323  0.1407]\n",
      "MSE loss: 91.5821\n",
      "Iteration: 136200\n",
      "Gradient: [   1.0613   11.0648   36.529   -20.4507 -136.1492]\n",
      "Weights: [-4.7803  0.7485 -1.2518  0.1324  0.1405]\n",
      "MSE loss: 91.4111\n",
      "Iteration: 136300\n",
      "Gradient: [  -6.5544  -36.9779  -28.4753  -71.0174 -178.3682]\n",
      "Weights: [-4.8099  0.7531 -1.252   0.1324  0.1403]\n",
      "MSE loss: 91.9254\n",
      "Iteration: 136400\n",
      "Gradient: [ -3.6853 -20.9484 -14.8817 -50.6291 238.6119]\n",
      "Weights: [-4.7924  0.7533 -1.2517  0.1322  0.1403]\n",
      "MSE loss: 91.2854\n",
      "Iteration: 136500\n",
      "Gradient: [   4.9966   11.8034   19.7356    2.9424 -121.2354]\n",
      "Weights: [-4.8087  0.7704 -1.2547  0.132   0.1403]\n",
      "MSE loss: 91.3176\n",
      "Iteration: 136600\n",
      "Gradient: [ -11.9124  -18.2976  -17.379     4.578  -104.6045]\n",
      "Weights: [-4.7986  0.7514 -1.2547  0.1329  0.1405]\n",
      "MSE loss: 91.5259\n",
      "Iteration: 136700\n",
      "Gradient: [  2.0275  17.2726 -11.5754  31.1668   2.2133]\n",
      "Weights: [-4.7907  0.7593 -1.2546  0.1325  0.1405]\n",
      "MSE loss: 91.3337\n",
      "Iteration: 136800\n",
      "Gradient: [   1.605    -9.9933  -18.8809   72.1117 -251.8952]\n",
      "Weights: [-4.7809  0.7548 -1.2558  0.1323  0.1407]\n",
      "MSE loss: 91.402\n",
      "Iteration: 136900\n",
      "Gradient: [-3.8908  2.3765 -7.0717 23.4177 97.2637]\n",
      "Weights: [-4.7926  0.7632 -1.2551  0.1326  0.1406]\n",
      "MSE loss: 91.5385\n",
      "Iteration: 137000\n",
      "Gradient: [  13.7581    7.3207   -0.1486 -115.3185   55.7041]\n",
      "Weights: [-4.7936  0.7539 -1.2558  0.1325  0.1406]\n",
      "MSE loss: 91.4034\n",
      "Iteration: 137100\n",
      "Gradient: [   5.6636   -9.8433    7.1622   30.1152 -146.9638]\n",
      "Weights: [-4.7894  0.7534 -1.2547  0.1328  0.1407]\n",
      "MSE loss: 91.2997\n",
      "Iteration: 137200\n",
      "Gradient: [ 3.591000e-01 -1.530000e-02 -3.266400e+01 -3.783640e+01 -3.920029e+02]\n",
      "Weights: [-4.7918  0.7567 -1.2548  0.1322  0.1406]\n",
      "MSE loss: 91.2822\n",
      "Iteration: 137300\n",
      "Gradient: [ -2.8199  12.5621  45.8103  28.3364 271.2951]\n",
      "Weights: [-4.7952  0.7625 -1.2558  0.1324  0.1408]\n",
      "MSE loss: 91.4491\n",
      "Iteration: 137400\n",
      "Gradient: [  2.9321  17.5993  25.7378  46.9937 -12.8889]\n",
      "Weights: [-4.8033  0.7624 -1.2549  0.1323  0.1405]\n",
      "MSE loss: 91.2967\n",
      "Iteration: 137500\n",
      "Gradient: [-11.806   15.7483   7.9329 -75.7096  -8.7993]\n",
      "Weights: [-4.7939  0.7526 -1.2512  0.132   0.1405]\n",
      "MSE loss: 91.2686\n",
      "Iteration: 137600\n",
      "Gradient: [  10.8998    4.1697  -29.8597  -72.9462 -187.6169]\n",
      "Weights: [-4.7953  0.7621 -1.256   0.1323  0.1405]\n",
      "MSE loss: 91.2896\n",
      "Iteration: 137700\n",
      "Gradient: [  4.7228   6.1366  32.6975  52.5491 -45.4541]\n",
      "Weights: [-4.7926  0.7597 -1.2527  0.133   0.1402]\n",
      "MSE loss: 91.427\n",
      "Iteration: 137800\n",
      "Gradient: [  1.2981   1.288   23.9538 -43.5448 368.5248]\n",
      "Weights: [-4.791   0.7477 -1.2496  0.1333  0.1402]\n",
      "MSE loss: 91.2991\n",
      "Iteration: 137900\n",
      "Gradient: [  3.8175   9.9297  45.2644  -2.6906 -88.423 ]\n",
      "Weights: [-4.7926  0.7533 -1.2498  0.1333  0.1399]\n",
      "MSE loss: 91.3129\n",
      "Iteration: 138000\n",
      "Gradient: [  2.6537  -6.9501 -15.655   91.4486 -94.4569]\n",
      "Weights: [-4.7957  0.7462 -1.2515  0.134   0.14  ]\n",
      "MSE loss: 91.375\n",
      "Iteration: 138100\n",
      "Gradient: [  1.586    3.1563  -4.6418 -31.1044  41.2212]\n",
      "Weights: [-4.7846  0.7497 -1.2537  0.1343  0.1401]\n",
      "MSE loss: 91.343\n",
      "Iteration: 138200\n",
      "Gradient: [ -3.7014   4.5153 -12.164   13.9377 319.6981]\n",
      "Weights: [-4.779   0.7456 -1.2516  0.1338  0.1399]\n",
      "MSE loss: 91.359\n",
      "Iteration: 138300\n",
      "Gradient: [ -3.6934 -19.6617 -14.4073   9.5154  99.6283]\n",
      "Weights: [-4.7867  0.7362 -1.2531  0.1349  0.1401]\n",
      "MSE loss: 91.6093\n",
      "Iteration: 138400\n",
      "Gradient: [  0.9578   1.5869  14.2289  34.222  310.2367]\n",
      "Weights: [-4.7959  0.7462 -1.2525  0.135   0.1401]\n",
      "MSE loss: 91.3124\n",
      "Iteration: 138500\n",
      "Gradient: [ -9.5163  -3.107   29.4488   7.6858 -82.4731]\n",
      "Weights: [-4.795   0.7423 -1.2498  0.1341  0.1398]\n",
      "MSE loss: 91.4334\n",
      "Iteration: 138600\n",
      "Gradient: [  6.6211  11.9822 -48.1623 -39.1632 -58.4127]\n",
      "Weights: [-4.7915  0.7432 -1.2475  0.1338  0.1398]\n",
      "MSE loss: 91.259\n",
      "Iteration: 138700\n",
      "Gradient: [  2.3129   1.7401 -38.2919   9.7722 136.9915]\n",
      "Weights: [-4.7952  0.7433 -1.2479  0.135   0.1396]\n",
      "MSE loss: 91.3133\n",
      "Iteration: 138800\n",
      "Gradient: [  6.8118  -0.9576   0.7313  23.8694 156.0281]\n",
      "Weights: [-4.7972  0.7451 -1.2459  0.1346  0.1395]\n",
      "MSE loss: 91.3184\n",
      "Iteration: 138900\n",
      "Gradient: [ -4.6427   4.4584  -9.8112  76.6424 480.3806]\n",
      "Weights: [-4.7851  0.746  -1.2443  0.1338  0.1393]\n",
      "MSE loss: 91.6039\n",
      "Iteration: 139000\n",
      "Gradient: [ 1.6658 -9.7296 -5.93   17.2155 62.0041]\n",
      "Weights: [-4.8076  0.7426 -1.2406  0.1325  0.1395]\n",
      "MSE loss: 91.5006\n",
      "Iteration: 139100\n",
      "Gradient: [ -11.0297   -2.3048   33.7026  -53.063  -186.0488]\n",
      "Weights: [-4.8066  0.7576 -1.2453  0.1329  0.1393]\n",
      "MSE loss: 91.3567\n",
      "Iteration: 139200\n",
      "Gradient: [   4.4451   -2.0063   10.5255  -39.0718 -112.5818]\n",
      "Weights: [-4.7866  0.7429 -1.2453  0.1327  0.1396]\n",
      "MSE loss: 91.3228\n",
      "Iteration: 139300\n",
      "Gradient: [  -9.8039    3.4498  -31.088   -19.7994 -291.3122]\n",
      "Weights: [-4.8198  0.7511 -1.2484  0.1333  0.1398]\n",
      "MSE loss: 92.2641\n",
      "Iteration: 139400\n",
      "Gradient: [ -4.7965   6.234   20.4609 -26.7639 203.1699]\n",
      "Weights: [-4.7927  0.7434 -1.2474  0.1333  0.1398]\n",
      "MSE loss: 91.3203\n",
      "Iteration: 139500\n",
      "Gradient: [-11.2898  10.8546   8.1746 -68.1639 106.0403]\n",
      "Weights: [-4.7896  0.7435 -1.2494  0.1337  0.1399]\n",
      "MSE loss: 91.2618\n",
      "Iteration: 139600\n",
      "Gradient: [ -5.7478  26.8748  24.4021 -61.6513 230.4878]\n",
      "Weights: [-4.7801  0.7428 -1.2501  0.1338  0.1399]\n",
      "MSE loss: 91.3243\n",
      "Iteration: 139700\n",
      "Gradient: [  6.6432  13.0447 -20.4241 -93.6038 303.6402]\n",
      "Weights: [-4.8029  0.7538 -1.2544  0.1348  0.1401]\n",
      "MSE loss: 91.3305\n",
      "Iteration: 139800\n",
      "Gradient: [ -7.438   14.4516 -35.9092 -50.1644 -99.79  ]\n",
      "Weights: [-4.7937  0.7492 -1.2537  0.1342  0.1399]\n",
      "MSE loss: 91.4428\n",
      "Iteration: 139900\n",
      "Gradient: [  9.5026  19.7735  55.0879 102.7753 318.1795]\n",
      "Weights: [-4.7972  0.7566 -1.2491  0.1344  0.1395]\n",
      "MSE loss: 91.4551\n",
      "Iteration: 140000\n",
      "Gradient: [ -3.2569  21.6918 -22.9406  62.833   89.3455]\n",
      "Weights: [-4.7882  0.7508 -1.2499  0.1345  0.1397]\n",
      "MSE loss: 91.5244\n",
      "Iteration: 140100\n",
      "Gradient: [   8.6092  -30.9665  -55.2141    3.478  -211.3131]\n",
      "Weights: [-4.7853  0.7377 -1.2472  0.1342  0.1396]\n",
      "MSE loss: 91.2727\n",
      "Iteration: 140200\n",
      "Gradient: [   6.7351  -11.5859  103.3017   -4.31   -278.0993]\n",
      "Weights: [-4.777   0.7374 -1.2491  0.1346  0.1396]\n",
      "MSE loss: 91.331\n",
      "Iteration: 140300\n",
      "Gradient: [ -7.1852   1.3163  -7.3932 -28.8416 -48.4838]\n",
      "Weights: [-4.7888  0.741  -1.2483  0.1346  0.1397]\n",
      "MSE loss: 91.2417\n",
      "Iteration: 140400\n",
      "Gradient: [  2.4746  -9.9305 -14.1516  94.4833 127.0693]\n",
      "Weights: [-4.784   0.7356 -1.247   0.1347  0.1397]\n",
      "MSE loss: 91.2641\n",
      "Iteration: 140500\n",
      "Gradient: [   1.3072  -14.5875    4.905   -47.8326 -293.2774]\n",
      "Weights: [-4.7935  0.7496 -1.2506  0.1346  0.1394]\n",
      "MSE loss: 91.2591\n",
      "Iteration: 140600\n",
      "Gradient: [ 1.148000e-01 -5.874300e+00  3.796200e+01 -3.173250e+01 -1.334385e+02]\n",
      "Weights: [-4.7762  0.741  -1.2516  0.1356  0.1396]\n",
      "MSE loss: 91.371\n",
      "Iteration: 140700\n",
      "Gradient: [   6.3077    6.8961   37.3494   29.6046 -144.4488]\n",
      "Weights: [-4.8073  0.759  -1.2518  0.1342  0.1397]\n",
      "MSE loss: 91.3147\n",
      "Iteration: 140800\n",
      "Gradient: [  3.0307  -5.3415  -6.6064  59.0177 -83.0588]\n",
      "Weights: [-4.8022  0.7507 -1.2503  0.1338  0.1399]\n",
      "MSE loss: 91.3177\n",
      "Iteration: 140900\n",
      "Gradient: [  7.6735   4.8365  20.6682 -85.8655 121.2332]\n",
      "Weights: [-4.768   0.7355 -1.2489  0.1341  0.1401]\n",
      "MSE loss: 91.7523\n",
      "Iteration: 141000\n",
      "Gradient: [ -7.8878   6.4734 -18.4853  -8.9825 276.0875]\n",
      "Weights: [-4.7826  0.7404 -1.2497  0.1332  0.1403]\n",
      "MSE loss: 91.2847\n",
      "Iteration: 141100\n",
      "Gradient: [  4.9445  -5.4535  17.6749 -58.7728 -78.6085]\n",
      "Weights: [-4.7845  0.7336 -1.2483  0.1332  0.1403]\n",
      "MSE loss: 91.409\n",
      "Iteration: 141200\n",
      "Gradient: [ -7.322    8.458   22.1193  27.8146 297.3176]\n",
      "Weights: [-4.7837  0.7306 -1.2485  0.1349  0.1401]\n",
      "MSE loss: 91.3471\n",
      "Iteration: 141300\n",
      "Gradient: [ -7.5294   2.0656  14.0399 144.5585 191.855 ]\n",
      "Weights: [-4.7836  0.7351 -1.2504  0.1348  0.14  ]\n",
      "MSE loss: 91.2937\n",
      "Iteration: 141400\n",
      "Gradient: [-13.8129  21.6179  18.9505 -51.3438 170.4703]\n",
      "Weights: [-4.7869  0.7442 -1.2532  0.1348  0.1401]\n",
      "MSE loss: 91.2587\n",
      "Iteration: 141500\n",
      "Gradient: [ 15.6343  -2.9336  11.5544  28.2627 493.9483]\n",
      "Weights: [-4.7694  0.7556 -1.2591  0.1354  0.14  ]\n",
      "MSE loss: 92.0265\n",
      "Iteration: 141600\n",
      "Gradient: [-4.9298 -1.1803 -9.1439 59.2982 40.7615]\n",
      "Weights: [-4.7906  0.7475 -1.2576  0.1361  0.14  ]\n",
      "MSE loss: 91.2846\n",
      "Iteration: 141700\n",
      "Gradient: [  -4.365    -5.2058    8.9127 -107.7889  -31.5072]\n",
      "Weights: [-4.7899  0.7522 -1.2604  0.1361  0.1401]\n",
      "MSE loss: 91.2682\n",
      "Iteration: 141800\n",
      "Gradient: [   0.3467   -2.9241  -34.8793  137.9787 -292.8332]\n",
      "Weights: [-4.7929  0.7495 -1.2622  0.1368  0.1403]\n",
      "MSE loss: 91.4145\n",
      "Iteration: 141900\n",
      "Gradient: [ -12.3422   16.3214    9.5501   34.3748 -144.9011]\n",
      "Weights: [-4.8144  0.7623 -1.2627  0.1369  0.1401]\n",
      "MSE loss: 91.6474\n",
      "Iteration: 142000\n",
      "Gradient: [  18.1881   -2.7634  -16.46    -59.0535 -127.8675]\n",
      "Weights: [-4.7798  0.7646 -1.2663  0.136   0.1405]\n",
      "MSE loss: 91.6417\n",
      "Iteration: 142100\n",
      "Gradient: [   2.653    11.981   -15.8236 -138.6658 -270.9735]\n",
      "Weights: [-4.7757  0.7648 -1.2662  0.1355  0.14  ]\n",
      "MSE loss: 91.6803\n",
      "Iteration: 142200\n",
      "Gradient: [ 13.557   32.9102  -4.7685  62.7925 266.1212]\n",
      "Weights: [-4.7845  0.7603 -1.2632  0.1367  0.1402]\n",
      "MSE loss: 91.5016\n",
      "Iteration: 142300\n",
      "Gradient: [  1.2142   1.7008 -15.7082  22.6842 331.5017]\n",
      "Weights: [-4.7877  0.7545 -1.2653  0.1371  0.1403]\n",
      "MSE loss: 91.2763\n",
      "Iteration: 142400\n",
      "Gradient: [  -2.7109   -0.773   -16.5508 -127.3046   17.1335]\n",
      "Weights: [-4.8051  0.7596 -1.2657  0.1377  0.14  ]\n",
      "MSE loss: 91.6071\n",
      "Iteration: 142500\n",
      "Gradient: [ -1.4411 -19.0175 -16.2898 -79.2829  80.0457]\n",
      "Weights: [-4.7894  0.7535 -1.2628  0.1383  0.1396]\n",
      "MSE loss: 91.203\n",
      "Iteration: 142600\n",
      "Gradient: [ -10.7843   -7.6557   -2.746   -55.64   -150.5883]\n",
      "Weights: [-4.7906  0.7558 -1.2644  0.1387  0.1393]\n",
      "MSE loss: 91.3136\n",
      "Iteration: 142700\n",
      "Gradient: [  12.5326  -15.1026   15.1664  -15.2934 -442.0093]\n",
      "Weights: [-4.7745  0.7492 -1.2643  0.1391  0.1394]\n",
      "MSE loss: 91.3329\n",
      "Iteration: 142800\n",
      "Gradient: [  7.1187   8.0492 -12.3769 -48.949   34.5807]\n",
      "Weights: [-4.81    0.7645 -1.2632  0.1389  0.1393]\n",
      "MSE loss: 91.3242\n",
      "Iteration: 142900\n",
      "Gradient: [ 15.9573   2.0952  15.3443  98.9175 228.8588]\n",
      "Weights: [-4.7846  0.7631 -1.2641  0.1386  0.1393]\n",
      "MSE loss: 91.4047\n",
      "Iteration: 143000\n",
      "Gradient: [ -2.1351  -3.0975  11.9217 -52.0115 289.7002]\n",
      "Weights: [-4.804   0.7702 -1.2685  0.1381  0.1398]\n",
      "MSE loss: 91.2559\n",
      "Iteration: 143100\n",
      "Gradient: [  -3.7075  -14.2395  -33.722  -143.1805  -23.4695]\n",
      "Weights: [-4.8017  0.7671 -1.2675  0.1385  0.1395]\n",
      "MSE loss: 91.3696\n",
      "Iteration: 143200\n",
      "Gradient: [ -5.1394 -19.6894 -33.2099 -59.9367 -15.5403]\n",
      "Weights: [-4.7907  0.76   -1.2655  0.139   0.1394]\n",
      "MSE loss: 91.1741\n",
      "Iteration: 143300\n",
      "Gradient: [ 3.390000e-02  1.824950e+01 -2.219810e+01  9.176210e+01 -2.366428e+02]\n",
      "Weights: [-4.7909  0.7565 -1.2641  0.1399  0.1392]\n",
      "MSE loss: 91.185\n",
      "Iteration: 143400\n",
      "Gradient: [ -2.7667  -4.7628  49.5909 -51.3862 464.4469]\n",
      "Weights: [-4.783   0.7495 -1.2651  0.1402  0.1393]\n",
      "MSE loss: 91.1982\n",
      "Iteration: 143500\n",
      "Gradient: [   8.7021    6.0373  -43.0777   62.7552 -371.0382]\n",
      "Weights: [-4.7737  0.7451 -1.2667  0.1403  0.1394]\n",
      "MSE loss: 91.3266\n",
      "Iteration: 143600\n",
      "Gradient: [ -12.2611   14.496    26.2862 -186.9282 -440.2138]\n",
      "Weights: [-4.7885  0.751  -1.2663  0.1399  0.1395]\n",
      "MSE loss: 91.2615\n",
      "Iteration: 143700\n",
      "Gradient: [   4.8057  -16.6491    3.3221  -36.0147 -162.31  ]\n",
      "Weights: [-4.7956  0.7585 -1.2663  0.1397  0.1394]\n",
      "MSE loss: 91.1994\n",
      "Iteration: 143800\n",
      "Gradient: [ 17.36     6.1075  17.7394  56.9294 -70.6552]\n",
      "Weights: [-4.7805  0.7571 -1.2655  0.1397  0.1394]\n",
      "MSE loss: 91.3876\n",
      "Iteration: 143900\n",
      "Gradient: [  15.0843    2.6201   -1.2688   79.7368 -199.2058]\n",
      "Weights: [-4.7802  0.7554 -1.2632  0.1391  0.1393]\n",
      "MSE loss: 91.3563\n",
      "Iteration: 144000\n",
      "Gradient: [ 1.479000e-01 -1.612570e+01  2.402100e+01  3.594470e+01  1.871474e+02]\n",
      "Weights: [-4.7916  0.7574 -1.263   0.1394  0.1393]\n",
      "MSE loss: 91.2565\n",
      "Iteration: 144100\n",
      "Gradient: [ -3.3121  -0.2175  11.4231 -56.0175 -82.0225]\n",
      "Weights: [-4.8053  0.764  -1.2636  0.1393  0.139 ]\n",
      "MSE loss: 91.2392\n",
      "Iteration: 144200\n",
      "Gradient: [  2.6032  -4.2397 -49.5821 -56.0381 -28.5482]\n",
      "Weights: [-4.7914  0.7604 -1.2631  0.1395  0.139 ]\n",
      "MSE loss: 91.2264\n",
      "Iteration: 144300\n",
      "Gradient: [  -5.9457  -26.3202  -30.6749  -26.1379 -255.6352]\n",
      "Weights: [-4.7998  0.764  -1.2635  0.1388  0.139 ]\n",
      "MSE loss: 91.2223\n",
      "Iteration: 144400\n",
      "Gradient: [  7.878   -8.7386  33.7153 -54.285   16.0485]\n",
      "Weights: [-4.7707  0.7647 -1.264   0.1394  0.1388]\n",
      "MSE loss: 92.4269\n",
      "Iteration: 144500\n",
      "Gradient: [ -0.7938   7.615   21.2199 151.2687 -56.3587]\n",
      "Weights: [-4.8058  0.7732 -1.2619  0.1384  0.1388]\n",
      "MSE loss: 91.2701\n",
      "Iteration: 144600\n",
      "Gradient: [  -0.3949    7.8401  -71.9179   97.9043 -197.2704]\n",
      "Weights: [-4.79    0.7587 -1.262   0.1397  0.1389]\n",
      "MSE loss: 91.2513\n",
      "Iteration: 144700\n",
      "Gradient: [  -9.4099  -19.1947  -52.4004  -25.3754 -176.4498]\n",
      "Weights: [-4.7852  0.7535 -1.2632  0.1395  0.1391]\n",
      "MSE loss: 91.1946\n",
      "Iteration: 144800\n",
      "Gradient: [ -4.1168   0.9163  18.559  -53.8729 -98.8921]\n",
      "Weights: [-4.7895  0.7513 -1.2614  0.1394  0.139 ]\n",
      "MSE loss: 91.1886\n",
      "Iteration: 144900\n",
      "Gradient: [   1.1537   21.6813   30.8934  -27.4892 -193.9654]\n",
      "Weights: [-4.7756  0.7494 -1.262   0.1398  0.1389]\n",
      "MSE loss: 91.3337\n",
      "Iteration: 145000\n",
      "Gradient: [  -2.9043   -4.1382  -44.9253  -88.6152 -242.7117]\n",
      "Weights: [-4.784   0.7507 -1.2601  0.1397  0.1388]\n",
      "MSE loss: 91.2127\n",
      "Iteration: 145100\n",
      "Gradient: [   0.6047   -8.4863    2.3023 -140.5411  168.3118]\n",
      "Weights: [-4.8173  0.7702 -1.2611  0.1389  0.1387]\n",
      "MSE loss: 91.3991\n",
      "Iteration: 145200\n",
      "Gradient: [ -1.6281  -4.3919  21.2565 171.4895 -62.2532]\n",
      "Weights: [-4.792   0.7659 -1.2627  0.1386  0.1391]\n",
      "MSE loss: 91.3818\n",
      "Iteration: 145300\n",
      "Gradient: [ -5.3742  -5.6771 -22.5424  20.5789 123.3953]\n",
      "Weights: [-4.8001  0.7614 -1.2633  0.1381  0.1395]\n",
      "MSE loss: 91.2209\n",
      "Iteration: 145400\n",
      "Gradient: [-10.5869 -10.6816 -17.0009 -72.2906 117.7142]\n",
      "Weights: [-4.8024  0.7627 -1.2634  0.1382  0.1396]\n",
      "MSE loss: 91.2244\n",
      "Iteration: 145500\n",
      "Gradient: [  3.4516  -4.1022   3.3073  51.5852 143.5686]\n",
      "Weights: [-4.79    0.7615 -1.2642  0.138   0.1395]\n",
      "MSE loss: 91.2008\n",
      "Iteration: 145600\n",
      "Gradient: [   0.2493    2.0003  -19.7747  -54.0297 -233.0848]\n",
      "Weights: [-4.8054  0.7669 -1.2632  0.1368  0.1396]\n",
      "MSE loss: 91.3753\n",
      "Iteration: 145700\n",
      "Gradient: [  4.6232  14.3138  -4.955  -64.8616 -29.1918]\n",
      "Weights: [-4.8043  0.7698 -1.2637  0.1365  0.14  ]\n",
      "MSE loss: 91.2522\n",
      "Iteration: 145800\n",
      "Gradient: [-9.6778 -7.396  18.7288 38.7019 89.0113]\n",
      "Weights: [-4.8078  0.7729 -1.2662  0.137   0.1399]\n",
      "MSE loss: 91.2579\n",
      "Iteration: 145900\n",
      "Gradient: [ 10.7458  -1.9048 -18.6963  51.7246  64.5003]\n",
      "Weights: [-4.8011  0.78   -1.2695  0.138   0.1396]\n",
      "MSE loss: 91.253\n",
      "Iteration: 146000\n",
      "Gradient: [ -12.4749    3.8458  -30.4259  -13.7283 -194.9432]\n",
      "Weights: [-4.8045  0.7819 -1.2701  0.1373  0.1397]\n",
      "MSE loss: 91.2291\n",
      "Iteration: 146100\n",
      "Gradient: [  3.9038  -1.4686  27.4115 -56.2772 -55.2459]\n",
      "Weights: [-4.7939  0.7777 -1.2692  0.1377  0.1398]\n",
      "MSE loss: 91.4086\n",
      "Iteration: 146200\n",
      "Gradient: [  7.254   -9.3704 -13.7128 -15.7885 107.6442]\n",
      "Weights: [-4.8122  0.7752 -1.2689  0.1376  0.1399]\n",
      "MSE loss: 91.356\n",
      "Iteration: 146300\n",
      "Gradient: [-1.391700e+00  1.053000e-01  2.011120e+01 -1.127660e+02  3.046918e+02]\n",
      "Weights: [-4.8002  0.7703 -1.2684  0.1378  0.1399]\n",
      "MSE loss: 91.1951\n",
      "Iteration: 146400\n",
      "Gradient: [   3.3725   -0.2919   25.0006   93.2123 -245.3023]\n",
      "Weights: [-4.8139  0.7775 -1.2683  0.1376  0.1397]\n",
      "MSE loss: 91.3416\n",
      "Iteration: 146500\n",
      "Gradient: [ -1.7528  -3.3246  19.9702 -39.5704 482.1592]\n",
      "Weights: [-4.7936  0.7656 -1.271   0.1384  0.1399]\n",
      "MSE loss: 91.3748\n",
      "Iteration: 146600\n",
      "Gradient: [  0.8274 -16.3997  29.8701  78.7335  47.595 ]\n",
      "Weights: [-4.8001  0.7728 -1.2735  0.1394  0.14  ]\n",
      "MSE loss: 91.1999\n",
      "Iteration: 146700\n",
      "Gradient: [  9.1979   5.1872 -21.6389  71.6783 341.7276]\n",
      "Weights: [-4.7871  0.7721 -1.2725  0.1395  0.1397]\n",
      "MSE loss: 91.3263\n",
      "Iteration: 146800\n",
      "Gradient: [  0.7892 -13.6259  16.6925  46.4035 148.9388]\n",
      "Weights: [-4.8067  0.7757 -1.2713  0.139   0.1398]\n",
      "MSE loss: 91.2111\n",
      "Iteration: 146900\n",
      "Gradient: [ -1.5226  15.1002 -21.3733 -69.6948  94.3525]\n",
      "Weights: [-4.8022  0.771  -1.2722  0.1398  0.1399]\n",
      "MSE loss: 91.2343\n",
      "Iteration: 147000\n",
      "Gradient: [ 10.2344 -13.3203 -14.8361  93.5582 162.1715]\n",
      "Weights: [-4.7784  0.7608 -1.2712  0.1394  0.1399]\n",
      "MSE loss: 91.3328\n",
      "Iteration: 147100\n",
      "Gradient: [   1.4809    9.9481  -42.221  -147.3725   -9.3209]\n",
      "Weights: [-4.7865  0.7698 -1.2741  0.1395  0.1398]\n",
      "MSE loss: 91.2383\n",
      "Iteration: 147200\n",
      "Gradient: [ -17.9835  -26.4556   -5.336   -36.8628 -210.17  ]\n",
      "Weights: [-4.7945  0.7694 -1.2721  0.1399  0.1394]\n",
      "MSE loss: 91.211\n",
      "Iteration: 147300\n",
      "Gradient: [ -9.1512  -2.584  -19.8253  47.3189  76.6185]\n",
      "Weights: [-4.7973  0.7661 -1.273   0.1409  0.1394]\n",
      "MSE loss: 91.2714\n",
      "Iteration: 147400\n",
      "Gradient: [   7.8526  -16.893   -10.1916  -34.7845 -366.639 ]\n",
      "Weights: [-4.7961  0.7717 -1.2756  0.1417  0.1393]\n",
      "MSE loss: 91.1356\n",
      "Iteration: 147500\n",
      "Gradient: [  -1.4822  -12.0542  -15.5241 -108.023    71.9548]\n",
      "Weights: [-4.7981  0.7721 -1.2749  0.142   0.1391]\n",
      "MSE loss: 91.1272\n",
      "Iteration: 147600\n",
      "Gradient: [  3.8566 -16.9265   4.3201  46.4395 -26.4269]\n",
      "Weights: [-4.8098  0.7725 -1.2734  0.1415  0.139 ]\n",
      "MSE loss: 91.4927\n",
      "Iteration: 147700\n",
      "Gradient: [  8.3806 -18.6512 -13.8777  87.1154 225.1789]\n",
      "Weights: [-4.7969  0.7778 -1.2752  0.1414  0.139 ]\n",
      "MSE loss: 91.1675\n",
      "Iteration: 147800\n",
      "Gradient: [  5.9479   9.1291  30.3657  37.2094 146.6983]\n",
      "Weights: [-4.7988  0.774  -1.2728  0.1417  0.1388]\n",
      "MSE loss: 91.1376\n",
      "Iteration: 147900\n",
      "Gradient: [  10.4258  -19.7615  -33.8863  -21.048  -535.3566]\n",
      "Weights: [-4.808   0.7783 -1.2704  0.1413  0.1386]\n",
      "MSE loss: 91.173\n",
      "Iteration: 148000\n",
      "Gradient: [   2.914    -0.7459    0.2336   46.8327 -100.7309]\n",
      "Weights: [-4.8169  0.7825 -1.2714  0.1418  0.1386]\n",
      "MSE loss: 91.2592\n",
      "Iteration: 148100\n",
      "Gradient: [  3.9023   2.3915 -20.5191  -8.2248 206.7659]\n",
      "Weights: [-4.8018  0.7792 -1.2716  0.1412  0.1388]\n",
      "MSE loss: 91.2027\n",
      "Iteration: 148200\n",
      "Gradient: [ -5.7197 -13.1827  30.5034  64.1338 -99.9486]\n",
      "Weights: [-4.8074  0.7721 -1.2692  0.1416  0.1387]\n",
      "MSE loss: 91.1723\n",
      "Iteration: 148300\n",
      "Gradient: [ -0.9862   7.704   12.4215 -81.6912 133.4584]\n",
      "Weights: [-4.7784  0.767  -1.274   0.1419  0.1389]\n",
      "MSE loss: 91.3898\n",
      "Iteration: 148400\n",
      "Gradient: [  3.4437 -10.7455 -47.8986   2.0132 508.034 ]\n",
      "Weights: [-4.7922  0.7797 -1.2766  0.1423  0.139 ]\n",
      "MSE loss: 91.4176\n",
      "Iteration: 148500\n",
      "Gradient: [ -1.024   15.9079  44.3195 -70.3456 328.0091]\n",
      "Weights: [-4.8078  0.7867 -1.2782  0.142   0.1389]\n",
      "MSE loss: 91.1726\n",
      "Iteration: 148600\n",
      "Gradient: [ -3.2832 -26.7832  -0.5307 -93.3213  20.4344]\n",
      "Weights: [-4.8036  0.7781 -1.2752  0.1422  0.1388]\n",
      "MSE loss: 91.1267\n",
      "Iteration: 148700\n",
      "Gradient: [   8.1173   -1.0924   15.6599   35.9199 -224.2651]\n",
      "Weights: [-4.7986  0.7785 -1.2745  0.1424  0.1388]\n",
      "MSE loss: 91.2395\n",
      "Iteration: 148800\n",
      "Gradient: [-11.1489  -9.226  -24.9919  33.8839 -66.1093]\n",
      "Weights: [-4.8077  0.7833 -1.2756  0.1422  0.1388]\n",
      "MSE loss: 91.1455\n",
      "Iteration: 148900\n",
      "Gradient: [ 10.3293  -2.5985  11.8807  35.2511 177.6926]\n",
      "Weights: [-4.7989  0.7899 -1.2808  0.1429  0.1388]\n",
      "MSE loss: 91.286\n",
      "Iteration: 149000\n",
      "Gradient: [ -3.5968 -29.615    7.0405 -22.6324   4.7032]\n",
      "Weights: [-4.8318  0.7988 -1.2819  0.1426  0.1388]\n",
      "MSE loss: 91.7575\n",
      "Iteration: 149100\n",
      "Gradient: [   7.1901  -15.4384   26.9719   64.6963 -207.5244]\n",
      "Weights: [-4.8236  0.8054 -1.2831  0.1429  0.1388]\n",
      "MSE loss: 91.2727\n",
      "Iteration: 149200\n",
      "Gradient: [   2.6754   16.0848    0.7444 -109.343    63.7837]\n",
      "Weights: [-4.8166  0.799  -1.2819  0.1423  0.1388]\n",
      "MSE loss: 91.2332\n",
      "Iteration: 149300\n",
      "Gradient: [ -4.1     21.6558 -24.4036 117.3637  -5.1074]\n",
      "Weights: [-4.8149  0.7948 -1.2834  0.144   0.1389]\n",
      "MSE loss: 91.2241\n",
      "Iteration: 149400\n",
      "Gradient: [  1.6845  11.2898  29.4481   0.9616 214.2319]\n",
      "Weights: [-4.8019  0.7885 -1.2846  0.1434  0.1389]\n",
      "MSE loss: 91.1849\n",
      "Iteration: 149500\n",
      "Gradient: [   4.7294   26.402    29.9312  171.1515 -133.9483]\n",
      "Weights: [-4.802   0.8003 -1.2838  0.1428  0.1388]\n",
      "MSE loss: 91.3997\n",
      "Iteration: 149600\n",
      "Gradient: [  0.1138  13.9759  36.2914 -61.3038 -37.2607]\n",
      "Weights: [-4.8176  0.7996 -1.2832  0.1432  0.1388]\n",
      "MSE loss: 91.185\n",
      "Iteration: 149700\n",
      "Gradient: [ -3.6217  -8.8702 -29.4571  43.933  252.7119]\n",
      "Weights: [-4.8071  0.8001 -1.2851  0.1433  0.1388]\n",
      "MSE loss: 91.2213\n",
      "Iteration: 149800\n",
      "Gradient: [-10.1683 -17.1138 -43.9478 -39.6916  -7.1715]\n",
      "Weights: [-4.7947  0.7797 -1.2848  0.1443  0.1387]\n",
      "MSE loss: 91.4327\n",
      "Iteration: 149900\n",
      "Gradient: [-9.030000e-02  1.613460e+01 -5.089100e+00 -1.164107e+02  3.742314e+02]\n",
      "Weights: [-4.8015  0.7984 -1.2884  0.1444  0.1389]\n",
      "MSE loss: 91.1965\n",
      "Iteration: 150000\n",
      "Gradient: [ -1.2384   4.9086 -29.0594 104.767   43.0145]\n",
      "Weights: [-4.8148  0.7904 -1.2869  0.1462  0.1388]\n",
      "MSE loss: 91.2065\n",
      "Iteration: 150100\n",
      "Gradient: [  -6.6594  -21.5325   39.0991    9.7866 -103.8295]\n",
      "Weights: [-4.794   0.777  -1.2863  0.1463  0.1387]\n",
      "MSE loss: 91.0972\n",
      "Iteration: 150200\n",
      "Gradient: [ -4.3688   5.4124   1.6649 -82.0963  27.0225]\n",
      "Weights: [-4.8094  0.7835 -1.2879  0.1467  0.1384]\n",
      "MSE loss: 91.6129\n",
      "Iteration: 150300\n",
      "Gradient: [ -18.1948   -6.8312  -32.0717 -107.3171 -197.6374]\n",
      "Weights: [-4.8175  0.7912 -1.2928  0.1478  0.1385]\n",
      "MSE loss: 91.5234\n",
      "Iteration: 150400\n",
      "Gradient: [  8.1475  13.1     52.7182  30.069  -57.5325]\n",
      "Weights: [-4.8034  0.7975 -1.2932  0.1479  0.1385]\n",
      "MSE loss: 91.1443\n",
      "Iteration: 150500\n",
      "Gradient: [  6.3071   3.6022 -14.4444  95.6954  44.8866]\n",
      "Weights: [-4.806   0.798  -1.2941  0.1484  0.1383]\n",
      "MSE loss: 91.0514\n",
      "Iteration: 150600\n",
      "Gradient: [  0.6157 -15.0531 -11.5511 100.022  124.1005]\n",
      "Weights: [-4.8179  0.8018 -1.2927  0.1482  0.1384]\n",
      "MSE loss: 91.2269\n",
      "Iteration: 150700\n",
      "Gradient: [  -1.3078    3.5505    2.1763 -103.4244  301.0708]\n",
      "Weights: [-4.8226  0.8125 -1.295   0.1473  0.1382]\n",
      "MSE loss: 91.1995\n",
      "Iteration: 150800\n",
      "Gradient: [ -8.5489 -14.3677  38.6077 -74.7246 -82.8233]\n",
      "Weights: [-4.8068  0.8095 -1.2957  0.1476  0.1383]\n",
      "MSE loss: 91.2395\n",
      "Iteration: 150900\n",
      "Gradient: [   6.2389   -0.31    -31.1104  158.0685 -163.4743]\n",
      "Weights: [-4.811   0.8162 -1.3004  0.1483  0.1385]\n",
      "MSE loss: 91.202\n",
      "Iteration: 151000\n",
      "Gradient: [ -3.9059 -14.1374  -7.9876 -70.7129  44.3077]\n",
      "Weights: [-4.8003  0.8104 -1.3     0.1478  0.1383]\n",
      "MSE loss: 91.2576\n",
      "Iteration: 151100\n",
      "Gradient: [  6.6569  15.1192  19.6658  18.491  246.5286]\n",
      "Weights: [-4.8138  0.8084 -1.3009  0.1489  0.1386]\n",
      "MSE loss: 91.0576\n",
      "Iteration: 151200\n",
      "Gradient: [-1.6927 -1.7679 62.8326  5.778  15.5129]\n",
      "Weights: [-4.8026  0.8027 -1.2983  0.1478  0.1388]\n",
      "MSE loss: 91.107\n",
      "Iteration: 151300\n",
      "Gradient: [  -1.098   -13.3168   11.1071  -64.4796 -139.8584]\n",
      "Weights: [-4.7977  0.8039 -1.3016  0.1477  0.1389]\n",
      "MSE loss: 91.1625\n",
      "Iteration: 151400\n",
      "Gradient: [ -8.5708 -11.8768  41.3293  41.7432 105.1183]\n",
      "Weights: [-4.8199  0.8074 -1.2987  0.1475  0.1388]\n",
      "MSE loss: 91.2432\n",
      "Iteration: 151500\n",
      "Gradient: [   4.0782   16.1467   41.2477   13.3039 -154.5419]\n",
      "Weights: [-4.7976  0.8139 -1.3004  0.1468  0.1388]\n",
      "MSE loss: 91.4324\n",
      "Iteration: 151600\n",
      "Gradient: [ -1.9602 -12.1281  29.9147 -40.0515 493.2136]\n",
      "Weights: [-4.8123  0.8124 -1.3001  0.1472  0.1389]\n",
      "MSE loss: 91.0872\n",
      "Iteration: 151700\n",
      "Gradient: [ 5.3794  5.6702 28.261  36.3831 84.0181]\n",
      "Weights: [-4.7805  0.8023 -1.301   0.1474  0.1389]\n",
      "MSE loss: 91.707\n",
      "Iteration: 151800\n",
      "Gradient: [ -3.0926  13.6995 -51.157   51.7111 -34.0994]\n",
      "Weights: [-4.8038  0.7977 -1.2964  0.1484  0.1388]\n",
      "MSE loss: 91.1424\n",
      "Iteration: 151900\n",
      "Gradient: [ 12.6229  -6.4653  33.4646 -16.1761 -31.2595]\n",
      "Weights: [-4.7867  0.7953 -1.2974  0.1484  0.1386]\n",
      "MSE loss: 91.3336\n",
      "Iteration: 152000\n",
      "Gradient: [  -3.4346    3.6243    7.4086   27.1907 -262.8271]\n",
      "Weights: [-4.7898  0.7961 -1.2978  0.1489  0.1386]\n",
      "MSE loss: 91.3489\n",
      "Iteration: 152100\n",
      "Gradient: [  4.0098   3.5943 -17.9913  38.0814 315.4401]\n",
      "Weights: [-4.8119  0.8054 -1.3002  0.1493  0.1385]\n",
      "MSE loss: 91.0417\n",
      "Iteration: 152200\n",
      "Gradient: [  4.5786   5.0595 -15.7205  28.9565  98.1023]\n",
      "Weights: [-4.8121  0.8033 -1.299   0.1498  0.1384]\n",
      "MSE loss: 91.0845\n",
      "Iteration: 152300\n",
      "Gradient: [  8.2005  -5.707   -9.5067  43.2204 235.464 ]\n",
      "Weights: [-4.8074  0.8013 -1.2982  0.1492  0.1384]\n",
      "MSE loss: 91.038\n",
      "Iteration: 152400\n",
      "Gradient: [ -0.851    1.874   37.5548 -95.7112 -43.4308]\n",
      "Weights: [-4.8128  0.7999 -1.2992  0.1493  0.1387]\n",
      "MSE loss: 91.1157\n",
      "Iteration: 152500\n",
      "Gradient: [  3.0535  -6.9377 -34.5296 -51.7494 -83.7166]\n",
      "Weights: [-4.7867  0.7907 -1.2981  0.1486  0.1386]\n",
      "MSE loss: 91.1865\n",
      "Iteration: 152600\n",
      "Gradient: [  0.4409   8.1818 -15.894   42.4284 404.9592]\n",
      "Weights: [-4.7986  0.795  -1.2954  0.1487  0.1385]\n",
      "MSE loss: 91.1318\n",
      "Iteration: 152700\n",
      "Gradient: [  -3.624     1.0811  -16.9524  -29.246  -122.1091]\n",
      "Weights: [-4.8119  0.7993 -1.297   0.1486  0.1384]\n",
      "MSE loss: 91.1921\n",
      "Iteration: 152800\n",
      "Gradient: [ 6.9565 19.5473 45.1887 81.3201 93.3383]\n",
      "Weights: [-4.7821  0.7972 -1.2953  0.1484  0.1383]\n",
      "MSE loss: 91.9815\n",
      "Iteration: 152900\n",
      "Gradient: [  0.2242  -2.4748 -23.6558  -9.8013 -46.6179]\n",
      "Weights: [-4.7941  0.7836 -1.2927  0.1491  0.1384]\n",
      "MSE loss: 91.0847\n",
      "Iteration: 153000\n",
      "Gradient: [ 0.393  -3.2341  4.7022 42.1519 66.407 ]\n",
      "Weights: [-4.8046  0.7895 -1.2924  0.1483  0.1384]\n",
      "MSE loss: 91.0521\n",
      "Iteration: 153100\n",
      "Gradient: [ -1.8163 -13.7888 -36.296   19.3303 104.1038]\n",
      "Weights: [-4.8118  0.7911 -1.2903  0.1483  0.1381]\n",
      "MSE loss: 91.1062\n",
      "Iteration: 153200\n",
      "Gradient: [  0.918    2.5852 -12.9829 -47.2849 146.8892]\n",
      "Weights: [-4.7986  0.7926 -1.2918  0.1485  0.1383]\n",
      "MSE loss: 91.1911\n",
      "Iteration: 153300\n",
      "Gradient: [ -0.1863  -0.9522 -15.2494  20.7904  38.1582]\n",
      "Weights: [-4.8054  0.7849 -1.2913  0.1491  0.1381]\n",
      "MSE loss: 91.1366\n",
      "Iteration: 153400\n",
      "Gradient: [   9.0126    4.7002  -37.8231   -3.7126 -118.0808]\n",
      "Weights: [-4.8076  0.7915 -1.2911  0.1493  0.138 ]\n",
      "MSE loss: 91.0834\n",
      "Iteration: 153500\n",
      "Gradient: [  -3.4052   -1.0516   50.8609   37.6015 -102.353 ]\n",
      "Weights: [-4.817   0.8003 -1.2917  0.1484  0.1382]\n",
      "MSE loss: 91.1578\n",
      "Iteration: 153600\n",
      "Gradient: [   9.4511    1.4549  -26.7989 -106.6367   62.2326]\n",
      "Weights: [-4.7964  0.7999 -1.2934  0.1479  0.138 ]\n",
      "MSE loss: 91.2565\n",
      "Iteration: 153700\n",
      "Gradient: [   2.9504    6.1585   -2.6889   49.3525 -137.6273]\n",
      "Weights: [-4.8102  0.806  -1.2935  0.1483  0.1381]\n",
      "MSE loss: 91.222\n",
      "Iteration: 153800\n",
      "Gradient: [  -4.347     3.0063  -16.5498  -39.6533 -130.0367]\n",
      "Weights: [-4.8148  0.7949 -1.2941  0.1484  0.1381]\n",
      "MSE loss: 91.5324\n",
      "Iteration: 153900\n",
      "Gradient: [  2.6724   6.1056 -14.7456  41.1954 -19.3889]\n",
      "Weights: [-4.8071  0.8004 -1.2957  0.1489  0.1381]\n",
      "MSE loss: 91.0334\n",
      "Iteration: 154000\n",
      "Gradient: [  10.4757   13.8629   40.4361    0.3471 -136.4052]\n",
      "Weights: [-4.8005  0.8025 -1.2947  0.1487  0.1378]\n",
      "MSE loss: 91.1864\n",
      "Iteration: 154100\n",
      "Gradient: [   9.5379    6.8596    4.0515   59.3086 -254.7108]\n",
      "Weights: [-4.7988  0.801  -1.2929  0.1482  0.1379]\n",
      "MSE loss: 91.2639\n",
      "Iteration: 154200\n",
      "Gradient: [   3.0179  -18.8355   25.4606   -1.7719 -238.6137]\n",
      "Weights: [-4.8048  0.7926 -1.2928  0.1489  0.1381]\n",
      "MSE loss: 91.0234\n",
      "Iteration: 154300\n",
      "Gradient: [ -2.6987   6.0677   8.4128 175.7436 251.2176]\n",
      "Weights: [-4.7978  0.7924 -1.2923  0.1491  0.1382]\n",
      "MSE loss: 91.24\n",
      "Iteration: 154400\n",
      "Gradient: [  -3.0256  -11.5685  -21.339    -6.8327 -244.7227]\n",
      "Weights: [-4.8063  0.7851 -1.2919  0.1488  0.1381]\n",
      "MSE loss: 91.3253\n",
      "Iteration: 154500\n",
      "Gradient: [  0.2023   2.5481   9.2162 -21.2509 -14.5008]\n",
      "Weights: [-4.8084  0.7912 -1.2897  0.1487  0.1381]\n",
      "MSE loss: 91.0876\n",
      "Iteration: 154600\n",
      "Gradient: [ -1.6581   3.6635   1.4986 169.7332 259.1504]\n",
      "Weights: [-4.8005  0.7904 -1.2887  0.1482  0.1379]\n",
      "MSE loss: 91.0807\n",
      "Iteration: 154700\n",
      "Gradient: [   3.8238   26.68    -16.6476  -33.3001 -112.693 ]\n",
      "Weights: [-4.7986  0.7936 -1.2901  0.148   0.1378]\n",
      "MSE loss: 91.1421\n",
      "Iteration: 154800\n",
      "Gradient: [  6.9423   2.8593  49.8779  33.5308 172.4957]\n",
      "Weights: [-4.802   0.7924 -1.2921  0.1502  0.1377]\n",
      "MSE loss: 91.1157\n",
      "Iteration: 154900\n",
      "Gradient: [  -2.6532    2.0138   26.619  -101.0997 -141.8063]\n",
      "Weights: [-4.788   0.7829 -1.2905  0.1499  0.1378]\n",
      "MSE loss: 91.2104\n",
      "Iteration: 155000\n",
      "Gradient: [  4.0987   4.2658 -24.159  -15.8727 -15.4381]\n",
      "Weights: [-4.7879  0.7675 -1.2851  0.1493  0.1379]\n",
      "MSE loss: 91.0454\n",
      "Iteration: 155100\n",
      "Gradient: [   8.3699   13.5635   22.1739 -116.9524  171.1786]\n",
      "Weights: [-4.7731  0.768  -1.2851  0.1491  0.1377]\n",
      "MSE loss: 91.3962\n",
      "Iteration: 155200\n",
      "Gradient: [ -1.1478   9.2722 -14.7016 193.9598 262.4338]\n",
      "Weights: [-4.7972  0.7728 -1.2802  0.1482  0.1379]\n",
      "MSE loss: 91.3011\n",
      "Iteration: 155300\n",
      "Gradient: [ 11.3147 -25.188  -23.5412 -14.3904 277.8596]\n",
      "Weights: [-4.7869  0.7596 -1.2772  0.1483  0.1376]\n",
      "MSE loss: 91.0577\n",
      "Iteration: 155400\n",
      "Gradient: [  1.7937  -6.7998 -46.228    8.1055 307.2806]\n",
      "Weights: [-4.7826  0.7621 -1.277   0.1477  0.1376]\n",
      "MSE loss: 91.1469\n",
      "Iteration: 155500\n",
      "Gradient: [ -7.3417   3.9818 -12.8324 -57.1417 -40.9886]\n",
      "Weights: [-4.7923  0.7609 -1.2756  0.148   0.1375]\n",
      "MSE loss: 91.0382\n",
      "Iteration: 155600\n",
      "Gradient: [ -3.3202 -12.283   -8.3338 -79.3114  84.1565]\n",
      "Weights: [-4.7917  0.7553 -1.275   0.1482  0.1375]\n",
      "MSE loss: 91.1022\n",
      "Iteration: 155700\n",
      "Gradient: [  -0.5447  -10.3022   12.365  -183.1942   59.5408]\n",
      "Weights: [-4.8108  0.7687 -1.2736  0.1475  0.1374]\n",
      "MSE loss: 91.1908\n",
      "Iteration: 155800\n",
      "Gradient: [-4.9491 16.5436 60.4112 45.9316 92.1457]\n",
      "Weights: [-4.7915  0.7699 -1.277   0.1482  0.1374]\n",
      "MSE loss: 91.2015\n",
      "Iteration: 155900\n",
      "Gradient: [  0.2641  11.3936  -3.1299 -37.3699 196.038 ]\n",
      "Weights: [-4.7954  0.759  -1.2748  0.149   0.1372]\n",
      "MSE loss: 91.0492\n",
      "Iteration: 156000\n",
      "Gradient: [  1.3665   9.3094 -45.6998  40.0409 -94.5876]\n",
      "Weights: [-4.7841  0.7493 -1.2739  0.1486  0.1374]\n",
      "MSE loss: 91.0801\n",
      "Iteration: 156100\n",
      "Gradient: [-11.5793   5.2858 -54.4979 -78.692   13.7617]\n",
      "Weights: [-4.783   0.7455 -1.271   0.149   0.1372]\n",
      "MSE loss: 91.0694\n",
      "Iteration: 156200\n",
      "Gradient: [  6.8861  15.4559  40.4708  67.7888 290.5685]\n",
      "Weights: [-4.7963  0.7538 -1.2726  0.1496  0.137 ]\n",
      "MSE loss: 91.0906\n",
      "Iteration: 156300\n",
      "Gradient: [  4.0922  20.3631   8.1878 116.4799   5.5775]\n",
      "Weights: [-4.7669  0.7494 -1.2737  0.15    0.1367]\n",
      "MSE loss: 91.4875\n",
      "Iteration: 156400\n",
      "Gradient: [  2.8449  29.3171 -15.8434 -79.6353 -76.2747]\n",
      "Weights: [-4.7889  0.7562 -1.2739  0.1501  0.1366]\n",
      "MSE loss: 91.0234\n",
      "Iteration: 156500\n",
      "Gradient: [  14.6877   -2.8661   14.526   -83.6809 -227.3032]\n",
      "Weights: [-4.7877  0.7586 -1.2735  0.1502  0.1365]\n",
      "MSE loss: 91.1019\n",
      "Iteration: 156600\n",
      "Gradient: [  8.2263   9.1441 -34.7267  63.3468  81.8217]\n",
      "Weights: [-4.789   0.7537 -1.2753  0.1509  0.1366]\n",
      "MSE loss: 91.0123\n",
      "Iteration: 156700\n",
      "Gradient: [ -0.9572 -19.4155  -7.7777  78.388  137.224 ]\n",
      "Weights: [-4.7969  0.7619 -1.2776  0.1506  0.1367]\n",
      "MSE loss: 91.0223\n",
      "Iteration: 156800\n",
      "Gradient: [ -11.877     8.4774  -19.9649   54.8127 -162.6922]\n",
      "Weights: [-4.7922  0.7514 -1.2756  0.151   0.1367]\n",
      "MSE loss: 91.0995\n",
      "Iteration: 156900\n",
      "Gradient: [  6.0984  -5.5782  21.629  -34.0362 133.5075]\n",
      "Weights: [-4.7859  0.7527 -1.277   0.1512  0.137 ]\n",
      "MSE loss: 91.0816\n",
      "Iteration: 157000\n",
      "Gradient: [ -8.9462  -8.0311  33.817   62.7926 299.4404]\n",
      "Weights: [-4.7975  0.7569 -1.2749  0.1511  0.1368]\n",
      "MSE loss: 91.127\n",
      "Iteration: 157100\n",
      "Gradient: [ 14.7111   6.8791  31.0559  49.744  -11.5484]\n",
      "Weights: [-4.7839  0.7592 -1.2767  0.1514  0.1367]\n",
      "MSE loss: 91.4383\n",
      "Iteration: 157200\n",
      "Gradient: [  14.2519  -12.8055   14.2355  -91.3654 -425.1419]\n",
      "Weights: [-4.782   0.7562 -1.2785  0.1511  0.1367]\n",
      "MSE loss: 91.0649\n",
      "Iteration: 157300\n",
      "Gradient: [   1.0985    4.1645  -10.5457 -103.2359  180.6336]\n",
      "Weights: [-4.7855  0.7564 -1.2771  0.1512  0.1368]\n",
      "MSE loss: 91.1085\n",
      "Iteration: 157400\n",
      "Gradient: [  3.0539  11.7768  17.6923 -62.3243  -9.2746]\n",
      "Weights: [-4.7865  0.7606 -1.2782  0.1507  0.1368]\n",
      "MSE loss: 91.0444\n",
      "Iteration: 157500\n",
      "Gradient: [  3.8804 -17.6302  37.4513  -8.6632 -81.9248]\n",
      "Weights: [-4.8016  0.7676 -1.2769  0.1503  0.1368]\n",
      "MSE loss: 91.0582\n",
      "Iteration: 157600\n",
      "Gradient: [  6.8582  23.0335 -37.6279 -41.5614 -72.5902]\n",
      "Weights: [-4.8015  0.7646 -1.2804  0.1511  0.137 ]\n",
      "MSE loss: 91.0727\n",
      "Iteration: 157700\n",
      "Gradient: [   0.6548    6.084    40.1957    9.7714 -212.1048]\n",
      "Weights: [-4.7918  0.7731 -1.2823  0.1516  0.1367]\n",
      "MSE loss: 91.207\n",
      "Iteration: 157800\n",
      "Gradient: [ -0.414    0.5739 -28.8545  54.4218  25.9837]\n",
      "Weights: [-4.7893  0.7615 -1.2801  0.1505  0.137 ]\n",
      "MSE loss: 91.0305\n",
      "Iteration: 157900\n",
      "Gradient: [ -8.4271   6.821   12.1959  13.5933 103.0135]\n",
      "Weights: [-4.7974  0.7726 -1.2827  0.15    0.1371]\n",
      "MSE loss: 91.0236\n",
      "Iteration: 158000\n",
      "Gradient: [   5.883    10.7272   -2.7551   40.414  -246.3152]\n",
      "Weights: [-4.8066  0.7786 -1.2819  0.1494  0.1372]\n",
      "MSE loss: 91.0494\n",
      "Iteration: 158100\n",
      "Gradient: [  6.4763   6.9149  -5.2011 102.4635 474.1664]\n",
      "Weights: [-4.8081  0.7795 -1.2825  0.1491  0.1375]\n",
      "MSE loss: 91.0917\n",
      "Iteration: 158200\n",
      "Gradient: [  9.581   18.702  -23.8985 -18.2469 262.0315]\n",
      "Weights: [-4.7832  0.775  -1.2843  0.1491  0.1376]\n",
      "MSE loss: 91.3315\n",
      "Iteration: 158300\n",
      "Gradient: [   3.2689   -8.1021  -41.2693   -0.5387 -206.1028]\n",
      "Weights: [-4.802   0.7741 -1.2819  0.1481  0.1379]\n",
      "MSE loss: 91.0551\n",
      "Iteration: 158400\n",
      "Gradient: [-5.187300e+00 -1.421000e-01  2.074790e+01 -5.743100e+00 -2.315463e+02]\n",
      "Weights: [-4.7838  0.7633 -1.2831  0.1479  0.1377]\n",
      "MSE loss: 91.5445\n",
      "Iteration: 158500\n",
      "Gradient: [ -7.5707  18.7985 -19.4869  72.0469 394.6798]\n",
      "Weights: [-4.7828  0.7633 -1.282   0.1487  0.138 ]\n",
      "MSE loss: 91.1281\n",
      "Iteration: 158600\n",
      "Gradient: [ -3.0877  -3.3313 -30.7934  -2.3635  67.2865]\n",
      "Weights: [-4.7727  0.7532 -1.2817  0.1487  0.138 ]\n",
      "MSE loss: 91.2061\n",
      "Iteration: 158700\n",
      "Gradient: [ -1.3847 -16.7653  11.6243 -16.9984 286.7746]\n",
      "Weights: [-4.7896  0.7584 -1.2822  0.1485  0.138 ]\n",
      "MSE loss: 91.3005\n",
      "Iteration: 158800\n",
      "Gradient: [  6.593   -8.2559 -11.3267  28.9501 121.7638]\n",
      "Weights: [-4.7938  0.7694 -1.2832  0.1483  0.1379]\n",
      "MSE loss: 91.0574\n",
      "Iteration: 158900\n",
      "Gradient: [   2.6593    8.5502  -21.5815  -64.4197 -171.4997]\n",
      "Weights: [-4.807   0.7818 -1.2822  0.1473  0.1378]\n",
      "MSE loss: 91.0692\n",
      "Iteration: 159000\n",
      "Gradient: [-10.6922   5.2877 -12.0751  37.6568 172.8359]\n",
      "Weights: [-4.8043  0.7823 -1.2825  0.1469  0.138 ]\n",
      "MSE loss: 91.0573\n",
      "Iteration: 159100\n",
      "Gradient: [ -5.8677 -12.142   11.4329 -13.0839 287.3001]\n",
      "Weights: [-4.798   0.7629 -1.2774  0.147   0.1381]\n",
      "MSE loss: 91.1237\n",
      "Iteration: 159200\n",
      "Gradient: [  2.7297   2.0994  14.0715 -51.3666  28.1422]\n",
      "Weights: [-4.7973  0.7661 -1.2786  0.1477  0.1379]\n",
      "MSE loss: 91.0721\n",
      "Iteration: 159300\n",
      "Gradient: [-10.6218 -18.2717 -36.3181 -81.653  -89.6852]\n",
      "Weights: [-4.7959  0.7639 -1.2776  0.1471  0.1378]\n",
      "MSE loss: 91.1715\n",
      "Iteration: 159400\n",
      "Gradient: [  3.6733   9.8574 -41.7043 -65.1215  64.2841]\n",
      "Weights: [-4.771   0.7644 -1.2808  0.1473  0.1379]\n",
      "MSE loss: 91.475\n",
      "Iteration: 159500\n",
      "Gradient: [  -9.8292   -3.3243  -16.9328 -117.7023  261.0031]\n",
      "Weights: [-4.7888  0.7585 -1.2787  0.1473  0.138 ]\n",
      "MSE loss: 91.172\n",
      "Iteration: 159600\n",
      "Gradient: [   1.9904   -9.342   -46.8841   45.5414 -268.217 ]\n",
      "Weights: [-4.7944  0.7644 -1.2796  0.1477  0.1381]\n",
      "MSE loss: 91.0728\n",
      "Iteration: 159700\n",
      "Gradient: [ 8.962  10.6869 11.9569 47.3231 29.2205]\n",
      "Weights: [-4.7793  0.7616 -1.276   0.1474  0.1381]\n",
      "MSE loss: 91.9366\n",
      "Iteration: 159800\n",
      "Gradient: [  4.9069  22.4475  22.9788 106.498   61.2376]\n",
      "Weights: [-4.8009  0.7785 -1.2779  0.147   0.1376]\n",
      "MSE loss: 91.1508\n",
      "Iteration: 159900\n",
      "Gradient: [  -2.8249   -8.9542   -1.7289  -17.4455 -197.2683]\n",
      "Weights: [-4.7946  0.7677 -1.2775  0.1471  0.1374]\n",
      "MSE loss: 91.2882\n",
      "Iteration: 160000\n",
      "Gradient: [ -1.6113  11.8735  -5.3885  79.748  177.1146]\n",
      "Weights: [-4.8021  0.7751 -1.2784  0.1477  0.1376]\n",
      "MSE loss: 91.1283\n",
      "Iteration: 160100\n",
      "Gradient: [   1.2472   12.0504   24.2284  -67.5415 -103.1333]\n",
      "Weights: [-4.7955  0.7743 -1.2809  0.148   0.1377]\n",
      "MSE loss: 91.0818\n",
      "Iteration: 160200\n",
      "Gradient: [   8.903    -4.214   -17.2361   48.6327 -101.4883]\n",
      "Weights: [-4.7955  0.7897 -1.2864  0.1482  0.1375]\n",
      "MSE loss: 91.248\n",
      "Iteration: 160300\n",
      "Gradient: [   1.7971   -6.1569  -40.4662 -122.7123   75.9497]\n",
      "Weights: [-4.7965  0.7834 -1.2828  0.1482  0.1375]\n",
      "MSE loss: 91.2638\n",
      "Iteration: 160400\n",
      "Gradient: [  -5.1012    3.244   -22.8347   -0.3576 -147.0415]\n",
      "Weights: [-4.8112  0.7862 -1.2837  0.1482  0.1376]\n",
      "MSE loss: 91.0805\n",
      "Iteration: 160500\n",
      "Gradient: [ -7.8014  14.2317  -1.6464  59.9548 -54.2769]\n",
      "Weights: [-4.8146  0.7865 -1.2829  0.1486  0.1375]\n",
      "MSE loss: 91.1411\n",
      "Iteration: 160600\n",
      "Gradient: [   8.3686    5.3942  -51.5607 -105.494    18.1326]\n",
      "Weights: [-4.8046  0.7754 -1.2846  0.15    0.1374]\n",
      "MSE loss: 91.065\n",
      "Iteration: 160700\n",
      "Gradient: [   6.0348   -2.0534    2.4329  113.8017 -479.1247]\n",
      "Weights: [-4.7901  0.7768 -1.283   0.149   0.1374]\n",
      "MSE loss: 91.1744\n",
      "Iteration: 160800\n",
      "Gradient: [ -16.441   -32.7832  -22.7319   36.3317 -146.7817]\n",
      "Weights: [-4.7909  0.767  -1.2843  0.1498  0.1373]\n",
      "MSE loss: 91.282\n",
      "Iteration: 160900\n",
      "Gradient: [  -5.7598    6.0784    5.4625    9.9189 -101.5274]\n",
      "Weights: [-4.7914  0.7743 -1.2844  0.1499  0.1373]\n",
      "MSE loss: 91.0493\n",
      "Iteration: 161000\n",
      "Gradient: [  -4.9799   -7.1051   -2.6213  -54.3031 -155.965 ]\n",
      "Weights: [-4.7993  0.7708 -1.2838  0.1506  0.1373]\n",
      "MSE loss: 91.0186\n",
      "Iteration: 161100\n",
      "Gradient: [ -9.6241 -17.9971  44.7746  43.663  120.8863]\n",
      "Weights: [-4.7945  0.7601 -1.2811  0.1504  0.1374]\n",
      "MSE loss: 91.0957\n",
      "Iteration: 161200\n",
      "Gradient: [ -0.5159 -20.0695  40.752   -3.8556  40.8344]\n",
      "Weights: [-4.7852  0.7637 -1.2839  0.1504  0.1371]\n",
      "MSE loss: 91.3152\n",
      "Iteration: 161300\n",
      "Gradient: [ -3.358   10.6583 -14.363    2.5631  59.0806]\n",
      "Weights: [-4.788   0.7666 -1.2847  0.1505  0.1373]\n",
      "MSE loss: 91.035\n",
      "Iteration: 161400\n",
      "Gradient: [   7.3121   -8.9644   -7.1843  -38.224  -223.2665]\n",
      "Weights: [-4.797   0.7648 -1.2835  0.1509  0.1375]\n",
      "MSE loss: 91.0785\n",
      "Iteration: 161500\n",
      "Gradient: [  3.0229  -4.5352  41.6323 -39.5554 212.6007]\n",
      "Weights: [-4.7934  0.7856 -1.2869  0.1496  0.1374]\n",
      "MSE loss: 91.2419\n",
      "Iteration: 161600\n",
      "Gradient: [  -2.689    16.3716   21.2507  -21.5883 -118.6721]\n",
      "Weights: [-4.8008  0.7795 -1.2881  0.1496  0.1379]\n",
      "MSE loss: 91.0279\n",
      "Iteration: 161700\n",
      "Gradient: [ -3.5316 -13.2804 -57.424  -41.8519 249.7845]\n",
      "Weights: [-4.8019  0.7714 -1.2853  0.1496  0.1381]\n",
      "MSE loss: 91.1601\n",
      "Iteration: 161800\n",
      "Gradient: [  -0.6327   29.8072  -19.279    31.3521 -296.872 ]\n",
      "Weights: [-4.7993  0.7731 -1.2856  0.1499  0.1377]\n",
      "MSE loss: 91.0371\n",
      "Iteration: 161900\n",
      "Gradient: [ -0.2049   8.1012  34.9439   5.6691 -50.4669]\n",
      "Weights: [-4.7918  0.7636 -1.2814  0.1495  0.1377]\n",
      "MSE loss: 91.0513\n",
      "Iteration: 162000\n",
      "Gradient: [ -7.016  -12.2145 -10.8594  48.486  -48.6503]\n",
      "Weights: [-4.7996  0.7721 -1.2838  0.1489  0.1376]\n",
      "MSE loss: 91.1328\n",
      "Iteration: 162100\n",
      "Gradient: [  4.7503  13.4749   3.3807 -46.6232 181.8431]\n",
      "Weights: [-4.8098  0.7843 -1.2821  0.1487  0.1374]\n",
      "MSE loss: 91.1043\n",
      "Iteration: 162200\n",
      "Gradient: [-3.800000e-02  3.663500e+00  8.490900e+00  7.595410e+01 -3.081028e+02]\n",
      "Weights: [-4.7978  0.7782 -1.2827  0.1487  0.1375]\n",
      "MSE loss: 91.0563\n",
      "Iteration: 162300\n",
      "Gradient: [   8.7336   -2.8617   -8.6419  -63.4238 -278.4766]\n",
      "Weights: [-4.7886  0.7703 -1.2861  0.1485  0.138 ]\n",
      "MSE loss: 91.1043\n",
      "Iteration: 162400\n",
      "Gradient: [ -3.6281  -8.2728 -66.7056 154.394  -67.5408]\n",
      "Weights: [-4.8063  0.7806 -1.2855  0.1477  0.1381]\n",
      "MSE loss: 91.1146\n",
      "Iteration: 162500\n",
      "Gradient: [ 11.6274   3.4926   3.9879  -9.0359 189.5293]\n",
      "Weights: [-4.8031  0.7772 -1.2832  0.1483  0.1379]\n",
      "MSE loss: 91.0589\n",
      "Iteration: 162600\n",
      "Gradient: [ -4.6493 -10.7288  13.5157   3.4117 131.4602]\n",
      "Weights: [-4.8039  0.7849 -1.2859  0.1486  0.1378]\n",
      "MSE loss: 91.0566\n",
      "Iteration: 162700\n",
      "Gradient: [  -4.5807    2.4484  -48.8441  -45.8605 -153.4892]\n",
      "Weights: [-4.8171  0.7939 -1.2869  0.1485  0.1375]\n",
      "MSE loss: 91.1405\n",
      "Iteration: 162800\n",
      "Gradient: [   6.8392   -2.4385   23.9227  -54.948  -395.4955]\n",
      "Weights: [-4.7988  0.7837 -1.2841  0.1484  0.1375]\n",
      "MSE loss: 91.0872\n",
      "Iteration: 162900\n",
      "Gradient: [  7.4009   4.1941  19.1212  29.0945 165.7123]\n",
      "Weights: [-4.7869  0.7713 -1.283   0.1493  0.1376]\n",
      "MSE loss: 91.1514\n",
      "Iteration: 163000\n",
      "Gradient: [  -9.3957   -4.5836   40.1866  -88.5135 -122.61  ]\n",
      "Weights: [-4.7981  0.7746 -1.2835  0.1495  0.1375]\n",
      "MSE loss: 91.0373\n",
      "Iteration: 163100\n",
      "Gradient: [  0.0892  -2.0024   3.8631 -78.2436 -83.4005]\n",
      "Weights: [-4.7903  0.7749 -1.2864  0.1498  0.1375]\n",
      "MSE loss: 91.0382\n",
      "Iteration: 163200\n",
      "Gradient: [ -10.7012  -15.583   -48.9346   64.3711 -440.9206]\n",
      "Weights: [-4.8083  0.781  -1.2897  0.1507  0.1374]\n",
      "MSE loss: 91.2076\n",
      "Iteration: 163300\n",
      "Gradient: [-1.612100e+00 -1.671000e-01  7.528300e+00  7.606700e+01 -2.197235e+02]\n",
      "Weights: [-4.8014  0.7812 -1.2896  0.1505  0.1376]\n",
      "MSE loss: 91.0037\n",
      "Iteration: 163400\n",
      "Gradient: [-9.700000e-02  3.098300e+00 -1.088000e+00  1.958495e+02 -3.128860e+01]\n",
      "Weights: [-4.7934  0.7845 -1.2887  0.1496  0.1376]\n",
      "MSE loss: 91.1214\n",
      "Iteration: 163500\n",
      "Gradient: [ -3.5819   6.3155  -7.5679   9.5656 197.7079]\n",
      "Weights: [-4.8059  0.7901 -1.2923  0.1505  0.1374]\n",
      "MSE loss: 91.1104\n",
      "Iteration: 163600\n",
      "Gradient: [-14.0134 -11.8039  -8.4684 -85.3699  -9.8337]\n",
      "Weights: [-4.8008  0.7854 -1.2903  0.1508  0.1373]\n",
      "MSE loss: 90.9935\n",
      "Iteration: 163700\n",
      "Gradient: [   6.2728    6.5774   11.9839  107.8806 -258.7796]\n",
      "Weights: [-4.8101  0.7881 -1.2895  0.1504  0.1373]\n",
      "MSE loss: 91.0747\n",
      "Iteration: 163800\n",
      "Gradient: [  4.5891   5.404   -5.753  156.4403 199.5301]\n",
      "Weights: [-4.8163  0.8041 -1.2939  0.1519  0.137 ]\n",
      "MSE loss: 91.135\n",
      "Iteration: 163900\n",
      "Gradient: [ -4.3007  10.1112   7.7445 -65.5553 -94.2548]\n",
      "Weights: [-4.8092  0.7972 -1.2951  0.1522  0.1369]\n",
      "MSE loss: 91.0711\n",
      "Iteration: 164000\n",
      "Gradient: [  6.0763 -11.1376  -9.18    50.6915 -36.1583]\n",
      "Weights: [-4.8152  0.7859 -1.2947  0.1538  0.1371]\n",
      "MSE loss: 91.209\n",
      "Iteration: 164100\n",
      "Gradient: [  -3.9349    2.9946   -1.5281 -110.7613   81.0109]\n",
      "Weights: [-4.7952  0.782  -1.2958  0.1535  0.1373]\n",
      "MSE loss: 90.9754\n",
      "Iteration: 164200\n",
      "Gradient: [-1.8134 20.2365 25.5578 36.1681 57.0482]\n",
      "Weights: [-4.7972  0.7871 -1.2963  0.1534  0.1373]\n",
      "MSE loss: 91.0512\n",
      "Iteration: 164300\n",
      "Gradient: [  -6.941    27.5525  -73.4491  -47.9758 -365.8759]\n",
      "Weights: [-4.7754  0.7719 -1.2965  0.1543  0.1372]\n",
      "MSE loss: 91.1792\n",
      "Iteration: 164400\n",
      "Gradient: [ -4.7465  -0.5931  65.6556  65.501  -41.7388]\n",
      "Weights: [-4.774   0.7582 -1.2948  0.1556  0.137 ]\n",
      "MSE loss: 91.1208\n",
      "Iteration: 164500\n",
      "Gradient: [-11.5911   4.0254 -18.6134 -28.5922 553.4507]\n",
      "Weights: [-4.7884  0.7669 -1.2958  0.156   0.1368]\n",
      "MSE loss: 91.0404\n",
      "Iteration: 164600\n",
      "Gradient: [ 1.683000e-01 -1.377320e+01 -8.767200e+00 -5.202720e+01  3.243101e+02]\n",
      "Weights: [-4.7938  0.7784 -1.2971  0.1561  0.1366]\n",
      "MSE loss: 90.9223\n",
      "Iteration: 164700\n",
      "Gradient: [ -2.3601   8.2684 -42.8959 -33.004   66.2049]\n",
      "Weights: [-4.7836  0.7732 -1.297   0.1559  0.1366]\n",
      "MSE loss: 91.0028\n",
      "Iteration: 164800\n",
      "Gradient: [   1.2624   -3.6043   15.7615 -144.3486   17.5477]\n",
      "Weights: [-4.8124  0.7742 -1.2927  0.1558  0.1366]\n",
      "MSE loss: 91.3679\n",
      "Iteration: 164900\n",
      "Gradient: [ -3.0426   5.1759 -37.0327  81.1966 -92.1105]\n",
      "Weights: [-4.809   0.7745 -1.2919  0.1557  0.1363]\n",
      "MSE loss: 91.225\n",
      "Iteration: 165000\n",
      "Gradient: [ -1.179    1.1427 -18.9781 -79.555  -75.7451]\n",
      "Weights: [-4.7912  0.7693 -1.2916  0.1563  0.1363]\n",
      "MSE loss: 90.9368\n",
      "Iteration: 165100\n",
      "Gradient: [  -2.3031  -19.661    -6.649   -31.2082 -311.9247]\n",
      "Weights: [-4.7847  0.7686 -1.2919  0.1562  0.1361]\n",
      "MSE loss: 90.9666\n",
      "Iteration: 165200\n",
      "Gradient: [ -6.1878   7.7865   1.3562 -16.6853  50.8622]\n",
      "Weights: [-4.7888  0.7619 -1.2927  0.157   0.1364]\n",
      "MSE loss: 90.9918\n",
      "Iteration: 165300\n",
      "Gradient: [ -2.3508  10.7629  55.9263  -1.9935 191.2273]\n",
      "Weights: [-4.7859  0.7665 -1.2887  0.1562  0.1359]\n",
      "MSE loss: 90.9891\n",
      "Iteration: 165400\n",
      "Gradient: [   5.8649   10.55     23.2469   11.5111 -369.0647]\n",
      "Weights: [-4.8013  0.7725 -1.2867  0.1557  0.1358]\n",
      "MSE loss: 90.9516\n",
      "Iteration: 165500\n",
      "Gradient: [ -5.1567  -6.1751  12.2299  20.5212 456.3665]\n",
      "Weights: [-4.8031  0.7821 -1.2891  0.1559  0.1359]\n",
      "MSE loss: 91.1678\n",
      "Iteration: 165600\n",
      "Gradient: [ -2.8575  19.4292 -35.4855  18.3599 258.7992]\n",
      "Weights: [-4.7905  0.7786 -1.2899  0.1562  0.1356]\n",
      "MSE loss: 91.1268\n",
      "Iteration: 165700\n",
      "Gradient: [ -2.1582 -19.9714 -31.7768 -31.2535 -99.6197]\n",
      "Weights: [-4.8273  0.7807 -1.2844  0.1554  0.1354]\n",
      "MSE loss: 91.5349\n",
      "Iteration: 165800\n",
      "Gradient: [-11.7613  -3.1143   5.3473 -87.4679  26.5048]\n",
      "Weights: [-4.8114  0.7864 -1.287   0.1548  0.1356]\n",
      "MSE loss: 91.0516\n",
      "Iteration: 165900\n",
      "Gradient: [ -8.1417 -21.4478 -20.5817 -64.4149 -14.8494]\n",
      "Weights: [-4.7952  0.7778 -1.287   0.1548  0.1358]\n",
      "MSE loss: 91.0476\n",
      "Iteration: 166000\n",
      "Gradient: [  -0.4998   12.2013  -10.6372 -114.8464  -35.0005]\n",
      "Weights: [-4.8038  0.7727 -1.2871  0.1552  0.1357]\n",
      "MSE loss: 91.0997\n",
      "Iteration: 166100\n",
      "Gradient: [  0.6085   2.6131  39.1397 -38.2404 214.6672]\n",
      "Weights: [-4.7964  0.775  -1.2867  0.1546  0.1359]\n",
      "MSE loss: 90.9723\n",
      "Iteration: 166200\n",
      "Gradient: [   4.6507   -5.7699   11.2598 -139.0329  198.6628]\n",
      "Weights: [-4.7918  0.7688 -1.2884  0.1558  0.1359]\n",
      "MSE loss: 90.9308\n",
      "Iteration: 166300\n",
      "Gradient: [ 10.2304  -6.6366 -13.9776  -1.6473 -61.8946]\n",
      "Weights: [-4.7973  0.7603 -1.2882  0.1566  0.1358]\n",
      "MSE loss: 91.298\n",
      "Iteration: 166400\n",
      "Gradient: [ -8.3649 -10.3212  19.2735 -20.0241 329.3751]\n",
      "Weights: [-4.7982  0.7586 -1.285   0.1574  0.1355]\n",
      "MSE loss: 91.0112\n",
      "Iteration: 166500\n",
      "Gradient: [  -2.2177    9.903   -24.2778   -5.8361 -178.0561]\n",
      "Weights: [-4.7853  0.7499 -1.285   0.1573  0.1356]\n",
      "MSE loss: 91.0498\n",
      "Iteration: 166600\n",
      "Gradient: [ -1.6538  -2.6308   8.4458  -8.0076 128.5975]\n",
      "Weights: [-4.7913  0.7573 -1.285   0.1579  0.1357]\n",
      "MSE loss: 91.1569\n",
      "Iteration: 166700\n",
      "Gradient: [  3.9404   9.4292  11.6994 -57.2941 171.6878]\n",
      "Weights: [-4.7875  0.7586 -1.2855  0.1568  0.1356]\n",
      "MSE loss: 90.9268\n",
      "Iteration: 166800\n",
      "Gradient: [   2.7949    1.2659   16.7766   37.3827 -164.1013]\n",
      "Weights: [-4.7967  0.7576 -1.2864  0.1571  0.1357]\n",
      "MSE loss: 91.089\n",
      "Iteration: 166900\n",
      "Gradient: [  -3.2239   13.6872  -23.3413  -66.6741 -126.0681]\n",
      "Weights: [-4.796   0.7732 -1.2898  0.1571  0.1357]\n",
      "MSE loss: 90.9722\n",
      "Iteration: 167000\n",
      "Gradient: [ -4.9397  -0.374  -11.4422 119.1531 -29.6838]\n",
      "Weights: [-4.8036  0.7737 -1.2887  0.1574  0.1354]\n",
      "MSE loss: 90.9452\n",
      "Iteration: 167100\n",
      "Gradient: [  2.269    1.4632   0.5549 -10.5946 184.3444]\n",
      "Weights: [-4.7933  0.7716 -1.2896  0.158   0.1354]\n",
      "MSE loss: 90.9887\n",
      "Iteration: 167200\n",
      "Gradient: [ -5.4    -23.2551 -30.438  -70.0041 -40.8338]\n",
      "Weights: [-4.7969  0.7593 -1.291   0.1586  0.1356]\n",
      "MSE loss: 91.2481\n",
      "Iteration: 167300\n",
      "Gradient: [ -4.9422  -0.468  -11.9964   6.0417 175.2276]\n",
      "Weights: [-4.7925  0.7659 -1.2898  0.1583  0.1355]\n",
      "MSE loss: 90.9073\n",
      "Iteration: 167400\n",
      "Gradient: [-1.3431  1.0551 -0.2901 89.1074 34.2572]\n",
      "Weights: [-4.7828  0.7715 -1.2915  0.1578  0.1355]\n",
      "MSE loss: 91.1838\n",
      "Iteration: 167500\n",
      "Gradient: [ -1.091   15.9703   5.3628 -32.8026  63.0897]\n",
      "Weights: [-4.8092  0.7821 -1.2942  0.1575  0.1355]\n",
      "MSE loss: 91.0734\n",
      "Iteration: 167600\n",
      "Gradient: [ -13.1701   -4.1829   -8.6261 -102.1464 -194.1287]\n",
      "Weights: [-4.8074  0.7897 -1.2961  0.1574  0.1354]\n",
      "MSE loss: 91.0173\n",
      "Iteration: 167700\n",
      "Gradient: [  2.8451  -1.4736  25.5461 -96.1878  -5.6684]\n",
      "Weights: [-4.8086  0.7859 -1.2934  0.1563  0.1354]\n",
      "MSE loss: 91.3475\n",
      "Iteration: 167800\n",
      "Gradient: [ -5.3659  -8.356   39.6753  -4.012  -29.5793]\n",
      "Weights: [-4.7991  0.7842 -1.2937  0.1562  0.1359]\n",
      "MSE loss: 90.9494\n",
      "Iteration: 167900\n",
      "Gradient: [  1.1168   6.8417 -30.6253 -12.8292 -83.2034]\n",
      "Weights: [-4.819   0.7884 -1.2912  0.1558  0.1359]\n",
      "MSE loss: 91.1258\n",
      "Iteration: 168000\n",
      "Gradient: [ -5.3975  11.7525 -22.0958  76.7612  58.4959]\n",
      "Weights: [-4.813   0.7913 -1.2919  0.1553  0.1358]\n",
      "MSE loss: 91.0198\n",
      "Iteration: 168100\n",
      "Gradient: [   2.0865   19.5588  -11.69    145.5648 -174.6414]\n",
      "Weights: [-4.7964  0.7777 -1.2894  0.1546  0.1361]\n",
      "MSE loss: 90.9593\n",
      "Iteration: 168200\n",
      "Gradient: [  -2.9184   -0.5242   -1.9193 -113.0023  -30.2064]\n",
      "Weights: [-4.806   0.7888 -1.2884  0.1538  0.136 ]\n",
      "MSE loss: 91.0755\n",
      "Iteration: 168300\n",
      "Gradient: [ -3.8638  -2.8464 -14.1972 -21.0837 176.5481]\n",
      "Weights: [-4.8261  0.7959 -1.2902  0.1542  0.1361]\n",
      "MSE loss: 91.2287\n",
      "Iteration: 168400\n",
      "Gradient: [ 6.844700e+00  1.100310e+01 -1.534920e+01 -8.200000e-03 -1.312329e+02]\n",
      "Weights: [-4.8064  0.7846 -1.2902  0.1548  0.136 ]\n",
      "MSE loss: 90.9784\n",
      "Iteration: 168500\n",
      "Gradient: [  6.0145  -4.6316  69.6781 143.5922 -67.8489]\n",
      "Weights: [-4.8141  0.7861 -1.2881  0.1546  0.1361]\n",
      "MSE loss: 91.1063\n",
      "Iteration: 168600\n",
      "Gradient: [ -3.0105  14.2328 -44.1676 118.2441 110.8155]\n",
      "Weights: [-4.8223  0.7961 -1.2889  0.1545  0.1359]\n",
      "MSE loss: 91.2627\n",
      "Iteration: 168700\n",
      "Gradient: [  8.4907  -0.9222  36.7062  54.4601 -35.0082]\n",
      "Weights: [-4.82    0.7951 -1.2887  0.1535  0.1363]\n",
      "MSE loss: 91.2707\n",
      "Iteration: 168800\n",
      "Gradient: [  -4.8823   -5.737    14.0976 -119.8477  -70.8294]\n",
      "Weights: [-4.8251  0.7832 -1.2876  0.1537  0.1363]\n",
      "MSE loss: 91.5351\n",
      "Iteration: 168900\n",
      "Gradient: [   3.6204    8.3928   27.4213 -124.8915  137.0857]\n",
      "Weights: [-4.7977  0.7824 -1.2866  0.153   0.1362]\n",
      "MSE loss: 91.0817\n",
      "Iteration: 169000\n",
      "Gradient: [  -4.52     26.878   -31.8629  -70.6191 -247.8153]\n",
      "Weights: [-4.7968  0.7769 -1.2887  0.1542  0.1362]\n",
      "MSE loss: 90.958\n",
      "Iteration: 169100\n",
      "Gradient: [  2.4558   2.004  -37.6077   8.5449 435.0451]\n",
      "Weights: [-4.8023  0.7808 -1.292   0.1555  0.1362]\n",
      "MSE loss: 90.9496\n",
      "Iteration: 169200\n",
      "Gradient: [  -0.1797  -22.4071   -2.3618  -77.3744 -134.0565]\n",
      "Weights: [-4.8112  0.7874 -1.2931  0.1547  0.1361]\n",
      "MSE loss: 91.1472\n",
      "Iteration: 169300\n",
      "Gradient: [ -3.3425 -34.295   10.2965  38.0297 250.4662]\n",
      "Weights: [-4.7914  0.7762 -1.2938  0.1551  0.1365]\n",
      "MSE loss: 90.9551\n",
      "Iteration: 169400\n",
      "Gradient: [  5.7323   8.2037  57.0995   5.3445 128.1965]\n",
      "Weights: [-4.7997  0.7763 -1.2917  0.1554  0.1364]\n",
      "MSE loss: 90.9514\n",
      "Iteration: 169500\n",
      "Gradient: [-11.3744   4.2054 -40.3383  -7.8517 537.0215]\n",
      "Weights: [-4.8082  0.7856 -1.2929  0.155   0.1365]\n",
      "MSE loss: 91.0071\n",
      "Iteration: 169600\n",
      "Gradient: [  -2.0008    1.3201  -22.2444   49.7687 -204.9403]\n",
      "Weights: [-4.7957  0.7749 -1.2911  0.1561  0.1362]\n",
      "MSE loss: 91.0592\n",
      "Iteration: 169700\n",
      "Gradient: [  2.0939   6.9639  35.5966 -93.5084  60.1926]\n",
      "Weights: [-4.7939  0.768  -1.2891  0.1559  0.1361]\n",
      "MSE loss: 90.9272\n",
      "Iteration: 169800\n",
      "Gradient: [  -4.961     2.1685  -40.3096  -40.9369 -201.1246]\n",
      "Weights: [-4.8017  0.7739 -1.2908  0.1554  0.1361]\n",
      "MSE loss: 91.0976\n",
      "Iteration: 169900\n",
      "Gradient: [   1.7066    7.685    31.7411  -96.2607 -214.2467]\n",
      "Weights: [-4.7933  0.7699 -1.2906  0.156   0.1362]\n",
      "MSE loss: 90.9231\n",
      "Iteration: 170000\n",
      "Gradient: [  -2.8909  -11.6752  -61.5171   35.6858 -322.1536]\n",
      "Weights: [-4.7926  0.7729 -1.293   0.1564  0.1359]\n",
      "MSE loss: 91.0464\n",
      "Iteration: 170100\n",
      "Gradient: [  2.7123  -9.1649   2.629  -33.3273  58.1549]\n",
      "Weights: [-4.8021  0.7854 -1.2946  0.1566  0.1359]\n",
      "MSE loss: 90.9528\n",
      "Iteration: 170200\n",
      "Gradient: [  -7.1458    6.0205   -8.5842 -100.5504 -246.4983]\n",
      "Weights: [-4.8062  0.7902 -1.2972  0.1557  0.136 ]\n",
      "MSE loss: 91.0978\n",
      "Iteration: 170300\n",
      "Gradient: [  15.4631  -10.9238  -51.7098  -78.4186 -301.6131]\n",
      "Weights: [-4.7939  0.7826 -1.2985  0.1557  0.1364]\n",
      "MSE loss: 91.1112\n",
      "Iteration: 170400\n",
      "Gradient: [-14.9055 -10.9303 -31.3211  50.8653 147.9504]\n",
      "Weights: [-4.818   0.7938 -1.2982  0.1558  0.1365]\n",
      "MSE loss: 91.0927\n",
      "Iteration: 170500\n",
      "Gradient: [ -9.3788  -9.632    5.4162   0.62   433.1446]\n",
      "Weights: [-4.8237  0.8056 -1.2965  0.1555  0.1362]\n",
      "MSE loss: 91.2597\n",
      "Iteration: 170600\n",
      "Gradient: [ -5.0512 -10.3449   2.131  -54.648  -51.4955]\n",
      "Weights: [-4.8055  0.8    -1.3001  0.1558  0.1361]\n",
      "MSE loss: 91.0016\n",
      "Iteration: 170700\n",
      "Gradient: [  5.5298 -11.2889   3.7061  44.6312 -10.7126]\n",
      "Weights: [-4.8236  0.8078 -1.2998  0.1563  0.136 ]\n",
      "MSE loss: 91.1185\n",
      "Iteration: 170800\n",
      "Gradient: [  5.5295  13.2304 -10.0153  39.8504 193.4219]\n",
      "Weights: [-4.8109  0.801  -1.3009  0.1569  0.1362]\n",
      "MSE loss: 91.1066\n",
      "Iteration: 170900\n",
      "Gradient: [   5.7799   -0.4466   10.1724   41.2622 -162.7795]\n",
      "Weights: [-4.8014  0.7914 -1.3024  0.1572  0.1363]\n",
      "MSE loss: 90.8991\n",
      "Iteration: 171000\n",
      "Gradient: [  1.185   14.371   14.8113  42.1987 324.9388]\n",
      "Weights: [-4.7967  0.7961 -1.3017  0.157   0.1363]\n",
      "MSE loss: 91.1638\n",
      "Iteration: 171100\n",
      "Gradient: [  2.5888  -2.7121   9.3714  24.1955 243.9367]\n",
      "Weights: [-4.7972  0.7911 -1.3024  0.1579  0.1362]\n",
      "MSE loss: 90.9903\n",
      "Iteration: 171200\n",
      "Gradient: [   6.6123   23.6819  -43.1939  -43.4758 -119.0041]\n",
      "Weights: [-4.7866  0.7804 -1.3015  0.1577  0.1364]\n",
      "MSE loss: 90.9591\n",
      "Iteration: 171300\n",
      "Gradient: [ -1.1877   8.2535  28.642  111.5615 -57.8322]\n",
      "Weights: [-4.7997  0.7796 -1.2994  0.1578  0.1364]\n",
      "MSE loss: 90.9388\n",
      "Iteration: 171400\n",
      "Gradient: [-1.973000e-01 -9.811600e+00  6.234340e+01 -2.654030e+01  3.251989e+02]\n",
      "Weights: [-4.7982  0.789  -1.3024  0.158   0.1362]\n",
      "MSE loss: 90.905\n",
      "Iteration: 171500\n",
      "Gradient: [ -2.3462  -5.7288 -51.3171 111.2546 239.608 ]\n",
      "Weights: [-4.7912  0.7908 -1.3047  0.158   0.1361]\n",
      "MSE loss: 90.9964\n",
      "Iteration: 171600\n",
      "Gradient: [12.4308  0.9548 94.9813 26.1673 15.3824]\n",
      "Weights: [-4.7959  0.7919 -1.3031  0.1587  0.1359]\n",
      "MSE loss: 90.9683\n",
      "Iteration: 171700\n",
      "Gradient: [  0.8677  -5.3226  37.5855 -42.4971 229.1931]\n",
      "Weights: [-4.8117  0.7973 -1.3052  0.1597  0.1358]\n",
      "MSE loss: 90.9194\n",
      "Iteration: 171800\n",
      "Gradient: [ -3.0676 -22.2229   8.9262 -87.2591  58.671 ]\n",
      "Weights: [-4.8123  0.7969 -1.3047  0.1594  0.1359]\n",
      "MSE loss: 90.9279\n",
      "Iteration: 171900\n",
      "Gradient: [  -1.156     8.0981  -19.3297  -81.8809 -125.4539]\n",
      "Weights: [-4.7958  0.7957 -1.3059  0.159   0.1358]\n",
      "MSE loss: 90.9498\n",
      "Iteration: 172000\n",
      "Gradient: [   7.8034   16.2253   32.6131   12.5829 -105.1842]\n",
      "Weights: [-4.7898  0.7916 -1.3062  0.16    0.1358]\n",
      "MSE loss: 91.0604\n",
      "Iteration: 172100\n",
      "Gradient: [  5.5812 -16.2911  69.7373 -31.1547 144.3608]\n",
      "Weights: [-4.7936  0.7908 -1.3045  0.1593  0.1359]\n",
      "MSE loss: 90.9805\n",
      "Iteration: 172200\n",
      "Gradient: [  0.4428 -17.1689  15.1957  -4.7839 441.8631]\n",
      "Weights: [-4.8266  0.8028 -1.3039  0.1585  0.1361]\n",
      "MSE loss: 91.2029\n",
      "Iteration: 172300\n",
      "Gradient: [ -0.5155  21.3629  19.9549 -86.5606 -24.1364]\n",
      "Weights: [-4.8067  0.8043 -1.3044  0.1582  0.1358]\n",
      "MSE loss: 91.0086\n",
      "Iteration: 172400\n",
      "Gradient: [  -3.8826   -3.9402  -50.9125  -68.1043 -149.0799]\n",
      "Weights: [-4.798   0.7938 -1.3053  0.1597  0.1356]\n",
      "MSE loss: 90.8995\n",
      "Iteration: 172500\n",
      "Gradient: [ -8.5664  -3.8721  11.916  -43.3003 -61.0893]\n",
      "Weights: [-4.8083  0.7938 -1.3043  0.1597  0.1358]\n",
      "MSE loss: 90.8905\n",
      "Iteration: 172600\n",
      "Gradient: [  -6.3421   -1.3306  -18.7087 -148.735   -46.478 ]\n",
      "Weights: [-4.8135  0.7993 -1.3082  0.1599  0.1359]\n",
      "MSE loss: 90.9962\n",
      "Iteration: 172700\n",
      "Gradient: [ -9.197  -10.487  -31.0369 -84.3858 -40.7827]\n",
      "Weights: [-4.8071  0.8021 -1.3089  0.159   0.1359]\n",
      "MSE loss: 91.0351\n",
      "Iteration: 172800\n",
      "Gradient: [  3.6606  -4.5474  14.1959  -8.7748 -56.0199]\n",
      "Weights: [-4.8076  0.7986 -1.304   0.1601  0.1354]\n",
      "MSE loss: 90.9602\n",
      "Iteration: 172900\n",
      "Gradient: [  -2.8381  -13.1354  -35.7239  -56.5681 -413.1783]\n",
      "Weights: [-4.7786  0.782  -1.3048  0.1592  0.1356]\n",
      "MSE loss: 91.3993\n",
      "Iteration: 173000\n",
      "Gradient: [  9.6405  -1.0655  18.2351  -8.3697 110.9914]\n",
      "Weights: [-4.7935  0.7844 -1.3048  0.1606  0.1357]\n",
      "MSE loss: 90.8653\n",
      "Iteration: 173100\n",
      "Gradient: [  5.7703  19.2535  37.4268  -4.8187 310.7006]\n",
      "Weights: [-4.7958  0.7921 -1.3064  0.1606  0.1355]\n",
      "MSE loss: 90.8876\n",
      "Iteration: 173200\n",
      "Gradient: [  -4.6819  -28.9263  -35.2691  -74.91   -293.1687]\n",
      "Weights: [-4.8077  0.7957 -1.3074  0.1603  0.1356]\n",
      "MSE loss: 90.9421\n",
      "Iteration: 173300\n",
      "Gradient: [  1.8344  -5.9547 -20.0766 -58.0528 303.2235]\n",
      "Weights: [-4.8178  0.8035 -1.3066  0.1597  0.1357]\n",
      "MSE loss: 90.9602\n",
      "Iteration: 173400\n",
      "Gradient: [  0.68   -13.9251   7.9029  -2.9278 -86.7514]\n",
      "Weights: [-4.8014  0.79   -1.3046  0.1607  0.1357]\n",
      "MSE loss: 90.9752\n",
      "Iteration: 173500\n",
      "Gradient: [-3.4129 10.1297  3.2102 90.56   74.0093]\n",
      "Weights: [-4.7938  0.7929 -1.3039  0.16    0.1356]\n",
      "MSE loss: 91.1147\n",
      "Iteration: 173600\n",
      "Gradient: [ -4.4226 -15.6294 -12.1679  88.2145 370.1889]\n",
      "Weights: [-4.8047  0.7872 -1.3039  0.1596  0.1357]\n",
      "MSE loss: 91.0332\n",
      "Iteration: 173700\n",
      "Gradient: [ 9.070000e-02 -1.503740e+01 -4.639200e+00 -1.039071e+02  1.122280e+02]\n",
      "Weights: [-4.7996  0.7771 -1.2999  0.1606  0.1354]\n",
      "MSE loss: 90.9092\n",
      "Iteration: 173800\n",
      "Gradient: [   8.1881   16.3118   26.0268 -116.5936  -13.6403]\n",
      "Weights: [-4.7748  0.7708 -1.3016  0.1615  0.1355]\n",
      "MSE loss: 91.177\n",
      "Iteration: 173900\n",
      "Gradient: [ -1.806    1.0512 -27.2806   3.008  240.8601]\n",
      "Weights: [-4.797   0.7797 -1.3003  0.1611  0.1353]\n",
      "MSE loss: 90.8679\n",
      "Iteration: 174000\n",
      "Gradient: [  -2.0901   -3.4556  -26.2637 -100.7486 -131.3565]\n",
      "Weights: [-4.8007  0.7876 -1.3024  0.1596  0.1354]\n",
      "MSE loss: 90.9673\n",
      "Iteration: 174100\n",
      "Gradient: [  3.1626 -29.7715 -20.662  -93.9127 -62.6505]\n",
      "Weights: [-4.7918  0.7742 -1.3     0.1602  0.1355]\n",
      "MSE loss: 90.8828\n",
      "Iteration: 174200\n",
      "Gradient: [  0.4557  -6.2739 -20.3879   1.4956 -63.7633]\n",
      "Weights: [-4.8003  0.7771 -1.3006  0.1609  0.1353]\n",
      "MSE loss: 90.9648\n",
      "Iteration: 174300\n",
      "Gradient: [ 7.2735  2.6869 -2.2714 60.5721 69.6608]\n",
      "Weights: [-4.811   0.7742 -1.2972  0.1606  0.1354]\n",
      "MSE loss: 91.1966\n",
      "Iteration: 174400\n",
      "Gradient: [ -5.6839 -15.8649  -7.9176 -36.9851 -14.9912]\n",
      "Weights: [-4.798   0.7787 -1.3007  0.1603  0.1355]\n",
      "MSE loss: 90.8864\n",
      "Iteration: 174500\n",
      "Gradient: [  5.0668   1.0322  24.7971  38.1373 437.6424]\n",
      "Weights: [-4.8039  0.7839 -1.3016  0.1603  0.1355]\n",
      "MSE loss: 90.9036\n",
      "Iteration: 174600\n",
      "Gradient: [  3.105   13.5235   9.0249 -58.5919 111.7219]\n",
      "Weights: [-4.7845  0.7824 -1.3024  0.1605  0.1354]\n",
      "MSE loss: 91.07\n",
      "Iteration: 174700\n",
      "Gradient: [ -3.9876  23.4075 -18.585  -46.7188  94.8852]\n",
      "Weights: [-4.8077  0.7788 -1.2991  0.1612  0.1354]\n",
      "MSE loss: 91.0108\n",
      "Iteration: 174800\n",
      "Gradient: [   4.5289   -3.1575   28.7897  -60.2811 -153.9083]\n",
      "Weights: [-4.7903  0.7816 -1.3025  0.1615  0.1354]\n",
      "MSE loss: 91.0385\n",
      "Iteration: 174900\n",
      "Gradient: [  3.8355  14.0156 -25.109   65.1414  98.0374]\n",
      "Weights: [-4.7735  0.7797 -1.3069  0.1618  0.1355]\n",
      "MSE loss: 91.2958\n",
      "Iteration: 175000\n",
      "Gradient: [  1.2162  10.6543  31.4984 -25.0575   6.0959]\n",
      "Weights: [-4.7929  0.7723 -1.3026  0.1614  0.1358]\n",
      "MSE loss: 90.9189\n",
      "Iteration: 175100\n",
      "Gradient: [ -17.8428    7.4682  -38.8138  -37.5056 -276.6889]\n",
      "Weights: [-4.7992  0.7708 -1.302   0.1609  0.1357]\n",
      "MSE loss: 91.2869\n",
      "Iteration: 175200\n",
      "Gradient: [-1.55000e-02 -3.16477e+01 -4.27031e+01 -6.77331e+01  3.33861e+01]\n",
      "Weights: [-4.7982  0.781  -1.3035  0.161   0.1356]\n",
      "MSE loss: 90.8631\n",
      "Iteration: 175300\n",
      "Gradient: [ 20.285    1.4935 -31.7234 100.9967 121.1872]\n",
      "Weights: [-4.7797  0.7725 -1.3     0.1609  0.1355]\n",
      "MSE loss: 91.12\n",
      "Iteration: 175400\n",
      "Gradient: [  5.6904 -13.5894  49.7642 -26.7157 380.1317]\n",
      "Weights: [-4.7721  0.7641 -1.2969  0.1605  0.1356]\n",
      "MSE loss: 91.3305\n",
      "Iteration: 175500\n",
      "Gradient: [11.999   2.1204 -3.5985 45.1496 45.8679]\n",
      "Weights: [-4.7953  0.7616 -1.2955  0.1608  0.1356]\n",
      "MSE loss: 91.0327\n",
      "Iteration: 175600\n",
      "Gradient: [ -0.7089 -23.9308  28.58     9.8069 -85.7915]\n",
      "Weights: [-4.7843  0.7712 -1.297   0.1605  0.1354]\n",
      "MSE loss: 91.0369\n",
      "Iteration: 175700\n",
      "Gradient: [ -2.4644  26.3009   5.7234 163.0626 -29.1068]\n",
      "Weights: [-4.8082  0.7823 -1.2947  0.1593  0.1354]\n",
      "MSE loss: 91.0589\n",
      "Iteration: 175800\n",
      "Gradient: [  -6.8895  -22.647    -1.5935   33.5372 -225.1369]\n",
      "Weights: [-4.7998  0.7799 -1.2969  0.159   0.1352]\n",
      "MSE loss: 91.0572\n",
      "Iteration: 175900\n",
      "Gradient: [   2.7712   -2.0483    9.6735 -105.2526 -141.8553]\n",
      "Weights: [-4.802   0.7766 -1.2959  0.1598  0.1352]\n",
      "MSE loss: 90.9273\n",
      "Iteration: 176000\n",
      "Gradient: [ 7.6877 -1.9772 24.5549 77.6403 81.6501]\n",
      "Weights: [-4.7856  0.7671 -1.2951  0.1603  0.1352]\n",
      "MSE loss: 90.8952\n",
      "Iteration: 176100\n",
      "Gradient: [ -0.8915  10.22     0.2428 -15.6093 194.104 ]\n",
      "Weights: [-4.8005  0.7677 -1.2935  0.1602  0.1355]\n",
      "MSE loss: 91.0329\n",
      "Iteration: 176200\n",
      "Gradient: [  -6.684    -9.9617   16.6331 -122.7588  147.46  ]\n",
      "Weights: [-4.8066  0.7731 -1.294   0.1604  0.1353]\n",
      "MSE loss: 91.0802\n",
      "Iteration: 176300\n",
      "Gradient: [   2.4424    9.7956  -24.7274 -108.9208  -73.0523]\n",
      "Weights: [-4.7956  0.7696 -1.2959  0.1605  0.1351]\n",
      "MSE loss: 90.9345\n",
      "Iteration: 176400\n",
      "Gradient: [  -6.0336   -9.3607   16.7011  100.2172 -323.5141]\n",
      "Weights: [-4.8     0.7695 -1.2958  0.1604  0.1352]\n",
      "MSE loss: 91.0372\n",
      "Iteration: 176500\n",
      "Gradient: [ -10.1603   29.5138  -19.4024   69.6951 -164.5407]\n",
      "Weights: [-4.8229  0.7806 -1.2929  0.1597  0.135 ]\n",
      "MSE loss: 91.3787\n",
      "Iteration: 176600\n",
      "Gradient: [ -8.1466   0.2079 -18.3161  66.9122 156.1694]\n",
      "Weights: [-4.8031  0.7745 -1.2952  0.1606  0.135 ]\n",
      "MSE loss: 90.9102\n",
      "Iteration: 176700\n",
      "Gradient: [  3.8099  -4.1816 -25.9216  15.8514 518.6858]\n",
      "Weights: [-4.7905  0.7659 -1.2929  0.1603  0.1351]\n",
      "MSE loss: 90.8901\n",
      "Iteration: 176800\n",
      "Gradient: [  5.052  -17.1959 -34.6503  56.851  208.7491]\n",
      "Weights: [-4.7889  0.7656 -1.2883  0.1593  0.1347]\n",
      "MSE loss: 90.9392\n",
      "Iteration: 176900\n",
      "Gradient: [4.222800e+00 5.660000e-02 6.342190e+01 6.481360e+01 3.177011e+02]\n",
      "Weights: [-4.7844  0.7621 -1.2897  0.1603  0.1348]\n",
      "MSE loss: 90.9822\n",
      "Iteration: 177000\n",
      "Gradient: [ -3.2716 -21.7358 -12.8927  55.9223 -95.2811]\n",
      "Weights: [-4.7803  0.761  -1.2913  0.1597  0.1351]\n",
      "MSE loss: 90.9516\n",
      "Iteration: 177100\n",
      "Gradient: [  9.0534   1.5703  62.6924 -65.2292 -70.3032]\n",
      "Weights: [-4.7703  0.7549 -1.289   0.16    0.1349]\n",
      "MSE loss: 91.1838\n",
      "Iteration: 177200\n",
      "Gradient: [  10.1313   24.8529   34.5603 -106.8903  421.3194]\n",
      "Weights: [-4.788   0.7636 -1.2888  0.1597  0.1349]\n",
      "MSE loss: 90.9931\n",
      "Iteration: 177300\n",
      "Gradient: [   2.0704  -15.7339  -24.7877   46.4744 -266.8959]\n",
      "Weights: [-4.7805  0.761  -1.2891  0.1589  0.135 ]\n",
      "MSE loss: 90.9913\n",
      "Iteration: 177400\n",
      "Gradient: [  0.3226 -20.4577 -54.0638 -39.3388  83.2503]\n",
      "Weights: [-4.7937  0.7609 -1.2853  0.1584  0.1349]\n",
      "MSE loss: 90.9147\n",
      "Iteration: 177500\n",
      "Gradient: [ 3.15000e-02  3.07850e+00  1.25320e+01 -4.91131e+01 -1.05888e+01]\n",
      "Weights: [-4.7969  0.7662 -1.2844  0.1579  0.1348]\n",
      "MSE loss: 90.9507\n",
      "Iteration: 177600\n",
      "Gradient: [ -5.7188 -20.0119 -32.1227 -50.6712  53.7644]\n",
      "Weights: [-4.7976  0.7628 -1.285   0.1583  0.1348]\n",
      "MSE loss: 90.9546\n",
      "Iteration: 177700\n",
      "Gradient: [ -2.8041  13.2274 -11.4216  -5.1943 105.1364]\n",
      "Weights: [-4.8086  0.7696 -1.2838  0.1587  0.1345]\n",
      "MSE loss: 91.0239\n",
      "Iteration: 177800\n",
      "Gradient: [  -2.5297    9.5883  -20.6906   39.7676 -141.6333]\n",
      "Weights: [-4.7901  0.7569 -1.2821  0.1582  0.1346]\n",
      "MSE loss: 90.9575\n",
      "Iteration: 177900\n",
      "Gradient: [ -4.5651 -15.9923  16.7172 -54.807   18.3429]\n",
      "Weights: [-4.8009  0.7606 -1.2845  0.1586  0.1349]\n",
      "MSE loss: 90.9872\n",
      "Iteration: 178000\n",
      "Gradient: [ -0.6287  -1.088   16.8818  -8.5601 370.4699]\n",
      "Weights: [-4.7972  0.7579 -1.2879  0.1598  0.1351]\n",
      "MSE loss: 90.9887\n",
      "Iteration: 178100\n",
      "Gradient: [   6.075    13.7712   23.1939   35.2694 -241.4321]\n",
      "Weights: [-4.794   0.7589 -1.2878  0.1594  0.1349]\n",
      "MSE loss: 90.958\n",
      "Iteration: 178200\n",
      "Gradient: [ -2.3111  11.0352 -63.7561  49.8559 153.3698]\n",
      "Weights: [-4.7853  0.7561 -1.2866  0.1599  0.1348]\n",
      "MSE loss: 90.9051\n",
      "Iteration: 178300\n",
      "Gradient: [ 2.2491  1.8509 -3.2377 87.351  31.4741]\n",
      "Weights: [-4.7998  0.7637 -1.2867  0.1605  0.1346]\n",
      "MSE loss: 90.9553\n",
      "Iteration: 178400\n",
      "Gradient: [ -1.2999   9.54    11.5009  21.5489 200.3529]\n",
      "Weights: [-4.7995  0.77   -1.2901  0.161   0.1344]\n",
      "MSE loss: 90.9051\n",
      "Iteration: 178500\n",
      "Gradient: [  8.5408   7.4243  -1.2612  95.1555 -32.7841]\n",
      "Weights: [-4.7936  0.7648 -1.2881  0.1611  0.1343]\n",
      "MSE loss: 90.9268\n",
      "Iteration: 178600\n",
      "Gradient: [  2.7198   2.198  -45.2909 -51.2195 -83.2827]\n",
      "Weights: [-4.8094  0.7757 -1.2883  0.1608  0.134 ]\n",
      "MSE loss: 90.9966\n",
      "Iteration: 178700\n",
      "Gradient: [ -7.71   -13.6464  46.7762  71.3565 114.7096]\n",
      "Weights: [-4.8148  0.7707 -1.2855  0.1606  0.134 ]\n",
      "MSE loss: 91.1493\n",
      "Iteration: 178800\n",
      "Gradient: [   1.889     8.0865  -26.4534  -28.703  -375.6102]\n",
      "Weights: [-4.7901  0.7652 -1.2876  0.1602  0.1341]\n",
      "MSE loss: 91.0488\n",
      "Iteration: 178900\n",
      "Gradient: [  8.5416  13.9047 -20.0842  16.3047 499.0046]\n",
      "Weights: [-4.7873  0.7696 -1.2904  0.1604  0.1345]\n",
      "MSE loss: 91.0541\n",
      "Iteration: 179000\n",
      "Gradient: [  -7.156    -7.9034    6.0613   11.0932 -108.5042]\n",
      "Weights: [-4.8143  0.7837 -1.2938  0.1606  0.1344]\n",
      "MSE loss: 91.041\n",
      "Iteration: 179100\n",
      "Gradient: [-12.6707 -11.7674 -14.1715 119.241  -91.27  ]\n",
      "Weights: [-4.8073  0.7815 -1.2932  0.1604  0.1345]\n",
      "MSE loss: 90.938\n",
      "Iteration: 179200\n",
      "Gradient: [  0.8224  11.082   11.789  -36.0095 -35.423 ]\n",
      "Weights: [-4.8111  0.7734 -1.2934  0.1614  0.1345]\n",
      "MSE loss: 91.1194\n",
      "Iteration: 179300\n",
      "Gradient: [  -0.2841  -11.8639  -28.4877   15.1386 -259.6332]\n",
      "Weights: [-4.8027  0.776  -1.2961  0.1617  0.1346]\n",
      "MSE loss: 90.8925\n",
      "Iteration: 179400\n",
      "Gradient: [   2.1216    9.7087   30.9588   56.4442 -367.2758]\n",
      "Weights: [-4.7986  0.7826 -1.2989  0.1616  0.1345]\n",
      "MSE loss: 90.9262\n",
      "Iteration: 179500\n",
      "Gradient: [ 6.38   10.9306 28.8836  5.4913 67.4779]\n",
      "Weights: [-4.7979  0.7771 -1.3021  0.1631  0.1348]\n",
      "MSE loss: 90.8345\n",
      "Iteration: 179600\n",
      "Gradient: [ -3.0371   4.3399 -10.8011 105.7914  59.1882]\n",
      "Weights: [-4.7971  0.778  -1.3004  0.162   0.1348]\n",
      "MSE loss: 90.8816\n",
      "Iteration: 179700\n",
      "Gradient: [   7.1237  -13.5649   11.2409 -125.466  -261.6914]\n",
      "Weights: [-4.8109  0.7808 -1.3014  0.1621  0.1348]\n",
      "MSE loss: 91.296\n",
      "Iteration: 179800\n",
      "Gradient: [  4.3755   5.289   -0.6755  63.6807 -64.9922]\n",
      "Weights: [-4.7857  0.7635 -1.2957  0.1624  0.135 ]\n",
      "MSE loss: 91.0084\n",
      "Iteration: 179900\n",
      "Gradient: [   0.8344    6.9972    5.2141   -7.377  -332.1243]\n",
      "Weights: [-4.7742  0.7616 -1.297   0.162   0.1348]\n",
      "MSE loss: 91.0138\n",
      "Iteration: 180000\n",
      "Gradient: [ -7.3079   4.885   14.1956 -66.4385 -58.4472]\n",
      "Weights: [-4.8041  0.7709 -1.2983  0.1622  0.1348]\n",
      "MSE loss: 91.1668\n",
      "Iteration: 180100\n",
      "Gradient: [ -1.9416   8.427  -11.8702 -88.5138 -54.506 ]\n",
      "Weights: [-4.7949  0.7733 -1.2993  0.1632  0.1346]\n",
      "MSE loss: 90.8349\n",
      "Iteration: 180200\n",
      "Gradient: [  -5.3224  -17.2628   13.2818   -6.5014 -213.1847]\n",
      "Weights: [-4.8025  0.7659 -1.2971  0.1636  0.1347]\n",
      "MSE loss: 91.0206\n",
      "Iteration: 180300\n",
      "Gradient: [  0.3218   6.0732 -25.7961 -72.922  107.3215]\n",
      "Weights: [-4.7896  0.7654 -1.2962  0.1633  0.1346]\n",
      "MSE loss: 90.9307\n",
      "Iteration: 180400\n",
      "Gradient: [ -0.8026  -8.8725 -31.2879   9.0895 102.009 ]\n",
      "Weights: [-4.7978  0.7677 -1.2949  0.1631  0.1343]\n",
      "MSE loss: 90.8669\n",
      "Iteration: 180500\n",
      "Gradient: [ -3.0201  -9.786  -51.2352  90.3944  21.4182]\n",
      "Weights: [-4.808   0.7673 -1.2941  0.1626  0.1344]\n",
      "MSE loss: 91.1914\n",
      "Iteration: 180600\n",
      "Gradient: [  1.8272  -6.9149 -11.6683 -22.5148 174.5669]\n",
      "Weights: [-4.7901  0.76   -1.2925  0.1619  0.1345]\n",
      "MSE loss: 90.9142\n",
      "Iteration: 180700\n",
      "Gradient: [  -9.7047    2.4652   24.7782 -137.5031 -320.8448]\n",
      "Weights: [-4.7815  0.7516 -1.2961  0.1627  0.1348]\n",
      "MSE loss: 91.1498\n",
      "Iteration: 180800\n",
      "Gradient: [  -1.9221   -7.2059  -62.5731   63.2719 -250.8428]\n",
      "Weights: [-4.7909  0.7581 -1.2955  0.1633  0.1348]\n",
      "MSE loss: 90.9414\n",
      "Iteration: 180900\n",
      "Gradient: [  -1.0336   -3.5455   12.8621  -17.1611 -172.1311]\n",
      "Weights: [-4.7756  0.7534 -1.2957  0.163   0.1348]\n",
      "MSE loss: 90.9225\n",
      "Iteration: 181000\n",
      "Gradient: [ -7.022   -0.4801  17.599  -28.0719  25.9605]\n",
      "Weights: [-4.7956  0.7543 -1.2909  0.1629  0.1346]\n",
      "MSE loss: 91.014\n",
      "Iteration: 181100\n",
      "Gradient: [  2.8442   1.4653 -25.7015  99.4835 -73.9313]\n",
      "Weights: [-4.782   0.7509 -1.292   0.1644  0.1343]\n",
      "MSE loss: 91.035\n",
      "Iteration: 181200\n",
      "Gradient: [ -5.7831  -2.3766 -23.7893 -26.611  111.1122]\n",
      "Weights: [-4.7886  0.7562 -1.2943  0.163   0.1344]\n",
      "MSE loss: 91.0702\n",
      "Iteration: 181300\n",
      "Gradient: [  6.2522 -16.2799   2.8219 -31.3603 188.0161]\n",
      "Weights: [-4.7741  0.7481 -1.2938  0.1641  0.1344]\n",
      "MSE loss: 90.913\n",
      "Iteration: 181400\n",
      "Gradient: [ -2.2233  -4.1408 -26.6184  44.4739 197.4957]\n",
      "Weights: [-4.7731  0.7534 -1.2934  0.1638  0.1343]\n",
      "MSE loss: 91.0382\n",
      "Iteration: 181500\n",
      "Gradient: [  2.2867   3.2976 -13.7569 -88.8617 204.9041]\n",
      "Weights: [-4.7824  0.7533 -1.2947  0.1651  0.1339]\n",
      "MSE loss: 90.8697\n",
      "Iteration: 181600\n",
      "Gradient: [ -0.5052  -6.5081  13.0449  65.4072 -32.3407]\n",
      "Weights: [-4.7862  0.7507 -1.2917  0.1653  0.1337]\n",
      "MSE loss: 90.8478\n",
      "Iteration: 181700\n",
      "Gradient: [ -9.1297 -13.1136 -30.6221  90.146  344.2556]\n",
      "Weights: [-4.7915  0.7481 -1.2932  0.1658  0.1337]\n",
      "MSE loss: 91.1588\n",
      "Iteration: 181800\n",
      "Gradient: [ -1.4789  11.7435  13.762  -58.6084 -18.7776]\n",
      "Weights: [-4.7888  0.7555 -1.2958  0.1659  0.1339]\n",
      "MSE loss: 90.8584\n",
      "Iteration: 181900\n",
      "Gradient: [ -2.3966  17.9208  18.2256  -9.55   184.4498]\n",
      "Weights: [-4.7818  0.7647 -1.2968  0.1649  0.1337]\n",
      "MSE loss: 90.9135\n",
      "Iteration: 182000\n",
      "Gradient: [  -3.2129    7.6432    5.4857  -48.5921 -219.2787]\n",
      "Weights: [-4.782   0.7655 -1.2972  0.1652  0.1338]\n",
      "MSE loss: 90.9854\n",
      "Iteration: 182100\n",
      "Gradient: [ -3.968   17.1905 -35.5925  -4.1848  35.1462]\n",
      "Weights: [-4.7997  0.7764 -1.3009  0.1658  0.1337]\n",
      "MSE loss: 90.8119\n",
      "Iteration: 182200\n",
      "Gradient: [18.0076  9.8887 34.1342 41.2852 21.3757]\n",
      "Weights: [-4.8083  0.7938 -1.3056  0.1662  0.1341]\n",
      "MSE loss: 91.446\n",
      "Iteration: 182300\n",
      "Gradient: [  -3.825     0.6467  -52.3269  -26.597  -154.2484]\n",
      "Weights: [-4.8138  0.7941 -1.3086  0.1655  0.1341]\n",
      "MSE loss: 90.9457\n",
      "Iteration: 182400\n",
      "Gradient: [   1.5129   14.2467  -24.0487   65.2826 -104.5226]\n",
      "Weights: [-4.8213  0.8076 -1.3122  0.1663  0.1341]\n",
      "MSE loss: 90.9881\n",
      "Iteration: 182500\n",
      "Gradient: [  -1.3304  -14.47      9.8623 -101.296  -377.5684]\n",
      "Weights: [-4.8164  0.801  -1.3137  0.1664  0.1341]\n",
      "MSE loss: 90.9572\n",
      "Iteration: 182600\n",
      "Gradient: [   3.8422    5.4206   -8.0081   36.8284 -130.2072]\n",
      "Weights: [-4.8167  0.8069 -1.3136  0.1667  0.1342]\n",
      "MSE loss: 91.0148\n",
      "Iteration: 182700\n",
      "Gradient: [16.3639  5.1356 20.6699  1.5596 90.1325]\n",
      "Weights: [-4.7856  0.7915 -1.312   0.1665  0.1341]\n",
      "MSE loss: 91.1431\n",
      "Iteration: 182800\n",
      "Gradient: [  8.5773  22.5649   3.0467  19.73   400.9214]\n",
      "Weights: [-4.7989  0.7911 -1.3101  0.1664  0.1343]\n",
      "MSE loss: 90.9535\n",
      "Iteration: 182900\n",
      "Gradient: [   5.8235  -19.7698  -19.8913   -0.7157 -207.6647]\n",
      "Weights: [-4.7774  0.7767 -1.3072  0.1656  0.1342]\n",
      "MSE loss: 91.039\n",
      "Iteration: 183000\n",
      "Gradient: [  1.8739  -9.7462 -51.2828  62.0944 -58.7551]\n",
      "Weights: [-4.7876  0.7752 -1.3069  0.1657  0.1344]\n",
      "MSE loss: 90.8052\n",
      "Iteration: 183100\n",
      "Gradient: [  -7.6924    8.1306  -64.934    59.1792 -126.0953]\n",
      "Weights: [-4.8117  0.7748 -1.305   0.1659  0.1343]\n",
      "MSE loss: 91.3708\n",
      "Iteration: 183200\n",
      "Gradient: [4.107900e+00 6.300000e-03 3.148720e+01 6.180710e+01 1.104299e+02]\n",
      "Weights: [-4.7979  0.7897 -1.3095  0.1664  0.1341]\n",
      "MSE loss: 90.8603\n",
      "Iteration: 183300\n",
      "Gradient: [   3.5056    1.0935  -18.5935  -14.7446 -229.3064]\n",
      "Weights: [-4.8064  0.7941 -1.3126  0.1659  0.1342]\n",
      "MSE loss: 90.963\n",
      "Iteration: 183400\n",
      "Gradient: [   7.7079  -18.6189   27.3953   46.086  -190.2569]\n",
      "Weights: [-4.8079  0.8024 -1.3129  0.1651  0.1344]\n",
      "MSE loss: 90.8318\n",
      "Iteration: 183500\n",
      "Gradient: [ -2.7713   3.5294  12.747  -95.1133 282.407 ]\n",
      "Weights: [-4.8095  0.8003 -1.3113  0.1646  0.1344]\n",
      "MSE loss: 90.8452\n",
      "Iteration: 183600\n",
      "Gradient: [ -3.2283   0.5365 -16.5866  20.7251 -58.7623]\n",
      "Weights: [-4.7921  0.7834 -1.3091  0.1646  0.1347]\n",
      "MSE loss: 90.8064\n",
      "Iteration: 183700\n",
      "Gradient: [  -2.0569    5.6574   30.3835  106.3378 -117.1875]\n",
      "Weights: [-4.7911  0.7831 -1.3087  0.1652  0.1347]\n",
      "MSE loss: 90.8933\n",
      "Iteration: 183800\n",
      "Gradient: [ -0.8498  -4.2541  19.0933 -11.5866 -16.6507]\n",
      "Weights: [-4.7936  0.7778 -1.3062  0.165   0.1347]\n",
      "MSE loss: 90.8555\n",
      "Iteration: 183900\n",
      "Gradient: [-13.3614  -4.4971   0.7946 -51.118  -77.4159]\n",
      "Weights: [-4.7935  0.7721 -1.3074  0.1648  0.1348]\n",
      "MSE loss: 91.0318\n",
      "Iteration: 184000\n",
      "Gradient: [ -4.6043  30.2017  28.9547  48.3476 153.4568]\n",
      "Weights: [-4.805   0.778  -1.3056  0.1646  0.1347]\n",
      "MSE loss: 91.0434\n",
      "Iteration: 184100\n",
      "Gradient: [  6.1646  -4.7265 -18.6382 -39.5621 228.63  ]\n",
      "Weights: [-4.7906  0.775  -1.3055  0.1647  0.1346]\n",
      "MSE loss: 90.8174\n",
      "Iteration: 184200\n",
      "Gradient: [  -0.4634    6.6515  -71.3626  -66.7446 -257.0769]\n",
      "Weights: [-4.7956  0.779  -1.3032  0.1643  0.1344]\n",
      "MSE loss: 90.8068\n",
      "Iteration: 184300\n",
      "Gradient: [ -9.5538   4.8328 -54.0624 -73.1008 241.5775]\n",
      "Weights: [-4.8014  0.7745 -1.3001  0.1639  0.1344]\n",
      "MSE loss: 90.8648\n",
      "Iteration: 184400\n",
      "Gradient: [ -10.9524  -38.8368    2.0051  -97.8866 -161.1615]\n",
      "Weights: [-4.8016  0.7695 -1.3003  0.164   0.1344]\n",
      "MSE loss: 91.1533\n",
      "Iteration: 184500\n",
      "Gradient: [17.0263 13.9511 28.4422 53.8512 95.1452]\n",
      "Weights: [-4.774   0.7748 -1.3028  0.1635  0.1345]\n",
      "MSE loss: 91.2525\n",
      "Iteration: 184600\n",
      "Gradient: [-7.68000e-02 -5.72930e+00 -5.76820e+00 -1.40264e+02 -2.94992e+01]\n",
      "Weights: [-4.792   0.7665 -1.2985  0.1628  0.1348]\n",
      "MSE loss: 90.8654\n",
      "Iteration: 184700\n",
      "Gradient: [ -4.919  -15.6025 -27.6014  63.1917  70.4819]\n",
      "Weights: [-4.8056  0.7725 -1.2966  0.162   0.1347]\n",
      "MSE loss: 91.0119\n",
      "Iteration: 184800\n",
      "Gradient: [ 5.19500e-01 -1.15000e-02  5.18030e+00 -8.97306e+01 -7.89715e+01]\n",
      "Weights: [-4.7957  0.7757 -1.2981  0.1622  0.1346]\n",
      "MSE loss: 90.8424\n",
      "Iteration: 184900\n",
      "Gradient: [ -1.8325 -24.3411  16.6027 101.1101  95.4085]\n",
      "Weights: [-4.8125  0.7784 -1.2989  0.1625  0.1346]\n",
      "MSE loss: 91.1357\n",
      "Iteration: 185000\n",
      "Gradient: [-6.64000e-02  1.00000e-03  2.57971e+01  2.03149e+01 -3.86898e+01]\n",
      "Weights: [-4.8091  0.7944 -1.3026  0.1617  0.1347]\n",
      "MSE loss: 90.902\n",
      "Iteration: 185100\n",
      "Gradient: [ 12.153   20.0853  -3.7226  67.9599 309.3455]\n",
      "Weights: [-4.8166  0.8069 -1.304   0.1613  0.1348]\n",
      "MSE loss: 91.1497\n",
      "Iteration: 185200\n",
      "Gradient: [  10.4679    8.6536   27.8875   53.8578 -221.877 ]\n",
      "Weights: [-4.8222  0.8197 -1.3077  0.1611  0.1348]\n",
      "MSE loss: 91.2429\n",
      "Iteration: 185300\n",
      "Gradient: [   4.3003  -14.6237  -13.8196  -13.3384 -294.6529]\n",
      "Weights: [-4.8123  0.8052 -1.3112  0.1616  0.135 ]\n",
      "MSE loss: 91.2803\n",
      "Iteration: 185400\n",
      "Gradient: [-2.7208 -1.9425 48.0809 -4.7934 62.6094]\n",
      "Weights: [-4.7991  0.8002 -1.3114  0.1627  0.1351]\n",
      "MSE loss: 90.8881\n",
      "Iteration: 185500\n",
      "Gradient: [   1.6765   -3.0967    5.7071  132.4958 -115.7733]\n",
      "Weights: [-4.7915  0.799  -1.3116  0.1618  0.1352]\n",
      "MSE loss: 91.019\n",
      "Iteration: 185600\n",
      "Gradient: [ -9.3433  15.2453   2.3134 -47.3196 301.378 ]\n",
      "Weights: [-4.8098  0.7953 -1.3104  0.1624  0.1354]\n",
      "MSE loss: 90.9162\n",
      "Iteration: 185700\n",
      "Gradient: [-4.92000e-02  3.55960e+00 -1.06172e+01  1.23058e+02 -4.55964e+01]\n",
      "Weights: [-4.8138  0.7945 -1.3111  0.164   0.1354]\n",
      "MSE loss: 91.0761\n",
      "Iteration: 185800\n",
      "Gradient: [   7.338     8.4396   24.4986    7.0883 -287.1512]\n",
      "Weights: [-4.792   0.788  -1.3082  0.1636  0.1349]\n",
      "MSE loss: 90.8889\n",
      "Iteration: 185900\n",
      "Gradient: [-2.7582  6.5264 12.5391  9.5116 69.8528]\n",
      "Weights: [-4.8005  0.7982 -1.3113  0.1639  0.1352]\n",
      "MSE loss: 91.1682\n",
      "Iteration: 186000\n",
      "Gradient: [ -8.3728  -4.5702 -34.9456  51.4462  92.3925]\n",
      "Weights: [-4.8068  0.7907 -1.3084  0.1641  0.135 ]\n",
      "MSE loss: 90.9097\n",
      "Iteration: 186100\n",
      "Gradient: [  2.2802  -2.3434  64.0458  15.9038 -34.7506]\n",
      "Weights: [-4.8011  0.7944 -1.3105  0.1636  0.1347]\n",
      "MSE loss: 90.959\n",
      "Iteration: 186200\n",
      "Gradient: [  -5.6783    0.7194   -6.3048  -59.3799 -341.774 ]\n",
      "Weights: [-4.8199  0.7977 -1.31    0.1644  0.1348]\n",
      "MSE loss: 91.0373\n",
      "Iteration: 186300\n",
      "Gradient: [   1.7483   12.9048   25.9364 -155.1502  104.2274]\n",
      "Weights: [-4.7904  0.7887 -1.3107  0.1642  0.1349]\n",
      "MSE loss: 90.8682\n",
      "Iteration: 186400\n",
      "Gradient: [   2.6363  -13.39     47.123   -36.2444 -224.7312]\n",
      "Weights: [-4.7925  0.7798 -1.3099  0.1646  0.135 ]\n",
      "MSE loss: 90.8461\n",
      "Iteration: 186500\n",
      "Gradient: [  -3.586   -14.9485  -12.0524  -14.5794 -193.8364]\n",
      "Weights: [-4.8088  0.7786 -1.3091  0.1645  0.1351]\n",
      "MSE loss: 91.4233\n",
      "Iteration: 186600\n",
      "Gradient: [  -9.0319    3.1709   40.2358    7.7579 -280.3241]\n",
      "Weights: [-4.7872  0.7797 -1.3098  0.165   0.1351]\n",
      "MSE loss: 90.9296\n",
      "Iteration: 186700\n",
      "Gradient: [ -4.5755   5.8512  16.7231 -47.612  213.3405]\n",
      "Weights: [-4.808   0.7866 -1.3107  0.1651  0.135 ]\n",
      "MSE loss: 90.9596\n",
      "Iteration: 186800\n",
      "Gradient: [-10.9638 -17.9796   0.4008 -40.4735   9.3567]\n",
      "Weights: [-4.8003  0.7917 -1.3148  0.1657  0.1349]\n",
      "MSE loss: 90.7893\n",
      "Iteration: 186900\n",
      "Gradient: [  -2.5014   14.2186    7.7906  -51.2594 -124.9544]\n",
      "Weights: [-4.7914  0.7849 -1.3147  0.1657  0.1349]\n",
      "MSE loss: 90.8518\n",
      "Iteration: 187000\n",
      "Gradient: [ 7.65    3.8709 37.131   4.7207 -6.9416]\n",
      "Weights: [-4.7779  0.7885 -1.3137  0.165   0.135 ]\n",
      "MSE loss: 91.293\n",
      "Iteration: 187100\n",
      "Gradient: [-11.4681   2.3072   6.7885  -8.0499  47.8581]\n",
      "Weights: [-4.8169  0.8037 -1.3153  0.1648  0.1349]\n",
      "MSE loss: 90.9371\n",
      "Iteration: 187200\n",
      "Gradient: [  -0.289    -7.9351  -25.3726  111.1227 -119.9491]\n",
      "Weights: [-4.8105  0.8079 -1.3156  0.1657  0.1346]\n",
      "MSE loss: 90.9266\n",
      "Iteration: 187300\n",
      "Gradient: [-7.3347 -1.6904  3.56   25.0944 47.2171]\n",
      "Weights: [-4.8108  0.8062 -1.3195  0.1663  0.1347]\n",
      "MSE loss: 90.8005\n",
      "Iteration: 187400\n",
      "Gradient: [ 8.5361 -4.3911  5.8878 -8.3882 -0.6445]\n",
      "Weights: [-4.8155  0.8131 -1.3228  0.1664  0.1347]\n",
      "MSE loss: 90.8778\n",
      "Iteration: 187500\n",
      "Gradient: [11.5365  6.8996 36.5052 -6.7928 -9.709 ]\n",
      "Weights: [-4.7857  0.7935 -1.3203  0.1672  0.1348]\n",
      "MSE loss: 90.8985\n",
      "Iteration: 187600\n",
      "Gradient: [ -5.8864  10.2047 -46.7077  96.1558 142.1024]\n",
      "Weights: [-4.8051  0.8052 -1.3213  0.1669  0.1348]\n",
      "MSE loss: 90.7573\n",
      "Iteration: 187700\n",
      "Gradient: [  0.204   18.8159   3.099   57.2085 121.7051]\n",
      "Weights: [-4.7906  0.7986 -1.3199  0.167   0.1347]\n",
      "MSE loss: 90.9227\n",
      "Iteration: 187800\n",
      "Gradient: [ -5.5512  -5.0219 -22.2051  10.3885 418.2406]\n",
      "Weights: [-4.8168  0.7972 -1.3184  0.1665  0.1349]\n",
      "MSE loss: 91.2936\n",
      "Iteration: 187900\n",
      "Gradient: [-4.1016 10.6346 -9.3187 -0.1134 53.7938]\n",
      "Weights: [-4.8005  0.7972 -1.3187  0.1663  0.135 ]\n",
      "MSE loss: 90.7814\n",
      "Iteration: 188000\n",
      "Gradient: [  2.5905  -6.1391  47.7672  97.2811 110.8553]\n",
      "Weights: [-4.8015  0.8019 -1.3193  0.1658  0.1349]\n",
      "MSE loss: 90.7772\n",
      "Iteration: 188100\n",
      "Gradient: [ -4.0388  -0.7911 -32.7494  61.6045 189.0586]\n",
      "Weights: [-4.8021  0.8014 -1.3194  0.1665  0.1348]\n",
      "MSE loss: 90.7634\n",
      "Iteration: 188200\n",
      "Gradient: [ -9.9254   7.7654  50.6974  47.7913 157.9823]\n",
      "Weights: [-4.8006  0.8051 -1.3198  0.1659  0.1348]\n",
      "MSE loss: 90.8005\n",
      "Iteration: 188300\n",
      "Gradient: [  8.7197 -18.7073   5.5779 -54.5445 149.08  ]\n",
      "Weights: [-4.8031  0.8092 -1.3198  0.165   0.1348]\n",
      "MSE loss: 90.8582\n",
      "Iteration: 188400\n",
      "Gradient: [   0.902     2.1641  -15.8074  -64.4337 -157.3641]\n",
      "Weights: [-4.8056  0.8095 -1.3231  0.1661  0.135 ]\n",
      "MSE loss: 90.7688\n",
      "Iteration: 188500\n",
      "Gradient: [-0.5908 10.7502  9.1225 49.9549  2.7894]\n",
      "Weights: [-4.8022  0.8085 -1.3196  0.1655  0.135 ]\n",
      "MSE loss: 90.8794\n",
      "Iteration: 188600\n",
      "Gradient: [  4.8541  -8.0088 -19.8569 -58.2379 221.6616]\n",
      "Weights: [-4.8012  0.7967 -1.3181  0.1658  0.1349]\n",
      "MSE loss: 90.8202\n",
      "Iteration: 188700\n",
      "Gradient: [  2.6348  -4.6059  23.8748 -41.585  -47.4518]\n",
      "Weights: [-4.8121  0.8071 -1.3209  0.1664  0.1349]\n",
      "MSE loss: 90.8137\n",
      "Iteration: 188800\n",
      "Gradient: [  -7.0576   14.7569   28.0024  -40.4282 -101.7878]\n",
      "Weights: [-4.7865  0.7963 -1.323   0.1665  0.135 ]\n",
      "MSE loss: 90.9213\n",
      "Iteration: 188900\n",
      "Gradient: [ -13.6371  -25.557    33.281  -165.1461  138.3774]\n",
      "Weights: [-4.7972  0.7995 -1.3227  0.1667  0.1351]\n",
      "MSE loss: 90.7813\n",
      "Iteration: 189000\n",
      "Gradient: [-16.1947  -4.9882 -24.5911  46.8279 -32.4785]\n",
      "Weights: [-4.802   0.8136 -1.3275  0.1667  0.135 ]\n",
      "MSE loss: 90.8211\n",
      "Iteration: 189100\n",
      "Gradient: [  9.3592  -5.9042   8.2356 -25.9021 -64.1737]\n",
      "Weights: [-4.7936  0.7941 -1.3266  0.1679  0.1352]\n",
      "MSE loss: 90.9835\n",
      "Iteration: 189200\n",
      "Gradient: [-11.6211 -21.4406 -22.5387 -53.8771 -40.1837]\n",
      "Weights: [-4.7875  0.7857 -1.3258  0.1683  0.1352]\n",
      "MSE loss: 91.1921\n",
      "Iteration: 189300\n",
      "Gradient: [ -7.5911   6.101   12.7433  82.9627 342.7937]\n",
      "Weights: [-4.7877  0.797  -1.3264  0.1684  0.1353]\n",
      "MSE loss: 90.9866\n",
      "Iteration: 189400\n",
      "Gradient: [  9.9576  -2.6175 -31.7058  45.0444 370.161 ]\n",
      "Weights: [-4.7718  0.7938 -1.3301  0.1681  0.1355]\n",
      "MSE loss: 91.148\n",
      "Iteration: 189500\n",
      "Gradient: [ -15.0244  -18.4153   26.0465   76.0529 -237.134 ]\n",
      "Weights: [-4.8074  0.8082 -1.3306  0.1677  0.1353]\n",
      "MSE loss: 91.0704\n",
      "Iteration: 189600\n",
      "Gradient: [ -4.8342  23.5067   9.5108  18.9181 203.6791]\n",
      "Weights: [-4.795   0.7992 -1.3298  0.1683  0.1355]\n",
      "MSE loss: 90.8532\n",
      "Iteration: 189700\n",
      "Gradient: [   2.457    -6.6387    1.8098  -65.3017 -166.4552]\n",
      "Weights: [-4.7979  0.8082 -1.3305  0.1685  0.1352]\n",
      "MSE loss: 90.7595\n",
      "Iteration: 189800\n",
      "Gradient: [ -3.9265  11.28     3.1668 -18.2649  31.5517]\n",
      "Weights: [-4.8029  0.8164 -1.3314  0.1678  0.1352]\n",
      "MSE loss: 90.766\n",
      "Iteration: 189900\n",
      "Gradient: [ -6.3735   8.105   19.5084 -90.7397 -62.5583]\n",
      "Weights: [-4.802   0.8139 -1.3307  0.1676  0.1354]\n",
      "MSE loss: 90.7877\n",
      "Iteration: 190000\n",
      "Gradient: [ 10.8451  11.019  -45.2067  72.0504 187.371 ]\n",
      "Weights: [-4.7943  0.8087 -1.3303  0.1676  0.1353]\n",
      "MSE loss: 90.8008\n",
      "Iteration: 190100\n",
      "Gradient: [  6.9977  20.1968  10.3237 -52.0483 199.6956]\n",
      "Weights: [-4.8085  0.8187 -1.3297  0.1672  0.1353]\n",
      "MSE loss: 90.7986\n",
      "Iteration: 190200\n",
      "Gradient: [ -0.7697   0.1196 -17.4271 -56.0505  85.173 ]\n",
      "Weights: [-4.8039  0.8219 -1.3303  0.1665  0.1353]\n",
      "MSE loss: 90.8249\n",
      "Iteration: 190300\n",
      "Gradient: [  9.3956  -9.905  -29.5601   1.057   20.9563]\n",
      "Weights: [-4.8082  0.8197 -1.3291  0.1668  0.135 ]\n",
      "MSE loss: 90.8081\n",
      "Iteration: 190400\n",
      "Gradient: [ -5.6263  17.7329  26.0703  75.2817 149.2883]\n",
      "Weights: [-4.8102  0.8233 -1.3281  0.167   0.1348]\n",
      "MSE loss: 90.8065\n",
      "Iteration: 190500\n",
      "Gradient: [  -1.0365    2.4764  -39.0194  -58.9054 -133.5917]\n",
      "Weights: [-4.8029  0.8126 -1.3252  0.1673  0.1349]\n",
      "MSE loss: 90.8243\n",
      "Iteration: 190600\n",
      "Gradient: [ -1.5737 -14.6623  -9.1932 -91.8465 -58.2988]\n",
      "Weights: [-4.7978  0.7991 -1.3236  0.1675  0.1349]\n",
      "MSE loss: 90.7741\n",
      "Iteration: 190700\n",
      "Gradient: [  -4.5506   -6.8273  -74.2957  -25.4365 -240.8069]\n",
      "Weights: [-4.8123  0.8049 -1.3239  0.167   0.1351]\n",
      "MSE loss: 90.9534\n",
      "Iteration: 190800\n",
      "Gradient: [ -6.5601  -8.5936 -21.8833  72.8751 -43.2117]\n",
      "Weights: [-4.8141  0.8052 -1.3259  0.1666  0.1353]\n",
      "MSE loss: 91.3285\n",
      "Iteration: 190900\n",
      "Gradient: [   3.9941   -8.6952  -52.2261 -107.7846  -33.286 ]\n",
      "Weights: [-4.7938  0.8108 -1.3265  0.1659  0.1352]\n",
      "MSE loss: 90.8918\n",
      "Iteration: 191000\n",
      "Gradient: [  -2.7784    6.651    -0.7841   35.4321 -107.8188]\n",
      "Weights: [-4.802   0.816  -1.3266  0.1662  0.1355]\n",
      "MSE loss: 91.0583\n",
      "Iteration: 191100\n",
      "Gradient: [-2.9875  2.2174 21.8288  3.5964 17.3283]\n",
      "Weights: [-4.8191  0.8182 -1.3265  0.1661  0.1354]\n",
      "MSE loss: 90.8779\n",
      "Iteration: 191200\n",
      "Gradient: [  11.0814    0.5305  -32.3991    1.4391 -186.2723]\n",
      "Weights: [-4.8052  0.818  -1.3274  0.1665  0.1349]\n",
      "MSE loss: 90.8362\n",
      "Iteration: 191300\n",
      "Gradient: [-4.8777  6.3861  9.7979 89.7323 77.0888]\n",
      "Weights: [-4.8139  0.8123 -1.3263  0.1672  0.135 ]\n",
      "MSE loss: 90.8653\n",
      "Iteration: 191400\n",
      "Gradient: [ -4.6455  -8.7559  -6.4454  55.35   306.0929]\n",
      "Weights: [-4.8071  0.8115 -1.327   0.1677  0.1349]\n",
      "MSE loss: 90.7502\n",
      "Iteration: 191500\n",
      "Gradient: [   3.3676   -7.6277   -7.7505 -131.1549 -215.9788]\n",
      "Weights: [-4.8378  0.828  -1.3285  0.1666  0.1351]\n",
      "MSE loss: 91.383\n",
      "Iteration: 191600\n",
      "Gradient: [  0.7536 -10.5314  -0.1055  42.4248 -20.8422]\n",
      "Weights: [-4.8084  0.8156 -1.3275  0.1669  0.1349]\n",
      "MSE loss: 90.8937\n",
      "Iteration: 191700\n",
      "Gradient: [ -14.0689  -10.5107  -37.7941  -53.4437 -163.2773]\n",
      "Weights: [-4.7904  0.8073 -1.3261  0.1666  0.1349]\n",
      "MSE loss: 90.948\n",
      "Iteration: 191800\n",
      "Gradient: [  6.9518  -4.3966  -3.2475 -18.6259 125.3182]\n",
      "Weights: [-4.8184  0.8191 -1.3245  0.1666  0.135 ]\n",
      "MSE loss: 90.8516\n",
      "Iteration: 191900\n",
      "Gradient: [  -3.4832  -14.5112    4.0623  -32.1987 -329.4375]\n",
      "Weights: [-4.8189  0.8232 -1.3275  0.1663  0.135 ]\n",
      "MSE loss: 90.8756\n",
      "Iteration: 192000\n",
      "Gradient: [  -1.3186   18.9702  -25.8984  115.8833 -126.3429]\n",
      "Weights: [-4.8273  0.8208 -1.3283  0.1664  0.1354]\n",
      "MSE loss: 91.1205\n",
      "Iteration: 192100\n",
      "Gradient: [   3.1987    6.484   -16.4396   -0.8384 -346.5125]\n",
      "Weights: [-4.8139  0.8121 -1.3283  0.1666  0.1354]\n",
      "MSE loss: 91.0537\n",
      "Iteration: 192200\n",
      "Gradient: [   4.3468   11.4415   -3.1034   59.1978 -284.6294]\n",
      "Weights: [-4.792   0.8126 -1.3287  0.1664  0.1352]\n",
      "MSE loss: 90.9378\n",
      "Iteration: 192300\n",
      "Gradient: [  9.6194   2.4268  39.8748 -25.5718  60.0003]\n",
      "Weights: [-4.8139  0.8293 -1.3288  0.1662  0.1351]\n",
      "MSE loss: 90.902\n",
      "Iteration: 192400\n",
      "Gradient: [  -3.964    20.3593   -7.4619   40.3316 -233.5046]\n",
      "Weights: [-4.826   0.8369 -1.3324  0.166   0.1354]\n",
      "MSE loss: 90.8709\n",
      "Iteration: 192500\n",
      "Gradient: [   7.557     5.3508  -18.2519    8.6633 -140.5544]\n",
      "Weights: [-4.8121  0.8378 -1.3327  0.165   0.1355]\n",
      "MSE loss: 90.9467\n",
      "Iteration: 192600\n",
      "Gradient: [  -0.8891   12.7783   46.7878   -4.3538 -144.0964]\n",
      "Weights: [-4.816   0.8354 -1.3316  0.1653  0.1355]\n",
      "MSE loss: 90.8643\n",
      "Iteration: 192700\n",
      "Gradient: [ 7.930000e-02 -6.306200e+00  2.838130e+01 -1.145345e+02  3.287630e+01]\n",
      "Weights: [-4.8218  0.838  -1.331   0.165   0.1355]\n",
      "MSE loss: 90.8754\n",
      "Iteration: 192800\n",
      "Gradient: [-16.4085 -19.9099 -20.1997  48.1257 -62.6322]\n",
      "Weights: [-4.8179  0.8339 -1.3327  0.165   0.1357]\n",
      "MSE loss: 90.8503\n",
      "Iteration: 192900\n",
      "Gradient: [ -9.9628   8.6067   0.5953  64.5242 120.1918]\n",
      "Weights: [-4.8152  0.829  -1.3292  0.1648  0.1354]\n",
      "MSE loss: 90.8495\n",
      "Iteration: 193000\n",
      "Gradient: [ -2.2906 -10.4759 -11.2749  -4.8343 -83.3247]\n",
      "Weights: [-4.7962  0.8107 -1.3306  0.1657  0.1357]\n",
      "MSE loss: 90.9618\n",
      "Iteration: 193100\n",
      "Gradient: [ 4.589  10.1448 45.3078 61.6934 98.2537]\n",
      "Weights: [-4.7978  0.8143 -1.3305  0.1656  0.1361]\n",
      "MSE loss: 90.9007\n",
      "Iteration: 193200\n",
      "Gradient: [ -7.5977 -23.9547  17.7611 -38.4101 285.3553]\n",
      "Weights: [-4.7885  0.7976 -1.3289  0.166   0.1361]\n",
      "MSE loss: 90.9471\n",
      "Iteration: 193300\n",
      "Gradient: [  -9.6657   29.2442    2.1002 -104.4203 -277.4809]\n",
      "Weights: [-4.806   0.8114 -1.3298  0.1656  0.1358]\n",
      "MSE loss: 90.9988\n",
      "Iteration: 193400\n",
      "Gradient: [ -0.6084 -10.2278 -25.4027 -48.0995  59.7317]\n",
      "Weights: [-4.8114  0.8152 -1.3245  0.1641  0.1358]\n",
      "MSE loss: 90.8224\n",
      "Iteration: 193500\n",
      "Gradient: [  -4.1802    0.8735  -25.4056  -43.3283 -250.0385]\n",
      "Weights: [-4.8131  0.8119 -1.3227  0.1634  0.1358]\n",
      "MSE loss: 90.9763\n",
      "Iteration: 193600\n",
      "Gradient: [  2.6512 -14.8552  15.1416   9.3516  -5.9672]\n",
      "Weights: [-4.8223  0.813  -1.3223  0.1635  0.136 ]\n",
      "MSE loss: 91.1132\n",
      "Iteration: 193700\n",
      "Gradient: [ 6.696500e+00  3.660000e-02 -3.435300e+00 -3.030760e+01 -2.541514e+02]\n",
      "Weights: [-4.801   0.8181 -1.3226  0.1622  0.1358]\n",
      "MSE loss: 90.9382\n",
      "Iteration: 193800\n",
      "Gradient: [  2.7535  -1.7409 -20.9313  -1.3856 -20.9776]\n",
      "Weights: [-4.8076  0.8138 -1.3241  0.1626  0.136 ]\n",
      "MSE loss: 91.011\n",
      "Iteration: 193900\n",
      "Gradient: [  -5.2519   -9.0326    5.084  -104.2122  106.5045]\n",
      "Weights: [-4.8104  0.8174 -1.3244  0.1622  0.1364]\n",
      "MSE loss: 90.8462\n",
      "Iteration: 194000\n",
      "Gradient: [ -4.131    4.7743  15.9637 -32.8883  38.5395]\n",
      "Weights: [-4.7987  0.8114 -1.3255  0.163   0.1362]\n",
      "MSE loss: 90.8613\n",
      "Iteration: 194100\n",
      "Gradient: [  9.9761  -1.2552  16.8948 112.5603  77.831 ]\n",
      "Weights: [-4.797   0.8096 -1.324   0.1632  0.1362]\n",
      "MSE loss: 90.8896\n",
      "Iteration: 194200\n",
      "Gradient: [   7.0435    2.9778  -57.001    60.9283 -255.609 ]\n",
      "Weights: [-4.7944  0.8093 -1.3235  0.1627  0.1361]\n",
      "MSE loss: 90.9106\n",
      "Iteration: 194300\n",
      "Gradient: [ -10.6589  -25.7474  -18.8896  -22.5936 -128.7924]\n",
      "Weights: [-4.8216  0.8128 -1.3219  0.1626  0.1361]\n",
      "MSE loss: 91.2476\n",
      "Iteration: 194400\n",
      "Gradient: [ -3.536   -5.5495 -20.1624 -91.3046 -62.6735]\n",
      "Weights: [-4.8175  0.8163 -1.3206  0.1619  0.1362]\n",
      "MSE loss: 90.909\n",
      "Iteration: 194500\n",
      "Gradient: [  6.5197   2.4465 -27.1555 146.409  159.2707]\n",
      "Weights: [-4.7939  0.8052 -1.32    0.1624  0.1361]\n",
      "MSE loss: 90.907\n",
      "Iteration: 194600\n",
      "Gradient: [  -8.327   -11.5348    5.6085  -73.4856 -203.9398]\n",
      "Weights: [-4.8027  0.8071 -1.3211  0.1626  0.136 ]\n",
      "MSE loss: 90.8549\n",
      "Iteration: 194700\n",
      "Gradient: [ 2.1803 11.6762 47.5863 55.0838 68.0059]\n",
      "Weights: [-4.8052  0.8159 -1.3228  0.1626  0.1361]\n",
      "MSE loss: 90.8578\n",
      "Iteration: 194800\n",
      "Gradient: [ -3.2953   2.596  -54.1129 -80.6607 -24.2852]\n",
      "Weights: [-4.8075  0.8146 -1.3231  0.1624  0.1361]\n",
      "MSE loss: 90.841\n",
      "Iteration: 194900\n",
      "Gradient: [ -12.9488   -8.9222  -23.4379 -107.9823   -9.5409]\n",
      "Weights: [-4.8133  0.8106 -1.3212  0.1635  0.1359]\n",
      "MSE loss: 90.8804\n",
      "Iteration: 195000\n",
      "Gradient: [-10.9259 -14.438   15.4397  31.7974 -79.91  ]\n",
      "Weights: [-4.803   0.8068 -1.3228  0.1637  0.1359]\n",
      "MSE loss: 90.8555\n",
      "Iteration: 195100\n",
      "Gradient: [  -6.5705   17.6     -35.8745  -12.5748 -582.2094]\n",
      "Weights: [-4.8166  0.8116 -1.3197  0.1633  0.1356]\n",
      "MSE loss: 90.9201\n",
      "Iteration: 195200\n",
      "Gradient: [ 8.4761 -3.7444  7.5624 74.9843 -2.4345]\n",
      "Weights: [-4.8047  0.8072 -1.3185  0.1637  0.1357]\n",
      "MSE loss: 90.945\n",
      "Iteration: 195300\n",
      "Gradient: [   2.4037    7.5771  -20.3386  154.6006 -105.1893]\n",
      "Weights: [-4.7938  0.8064 -1.3176  0.1628  0.1357]\n",
      "MSE loss: 91.0548\n",
      "Iteration: 195400\n",
      "Gradient: [  -1.5249   -5.6177   -5.5441 -124.2554  -98.1046]\n",
      "Weights: [-4.8077  0.8028 -1.3159  0.1628  0.1358]\n",
      "MSE loss: 90.8436\n",
      "Iteration: 195500\n",
      "Gradient: [ -0.7119   7.3622 -11.1759 -17.1864 162.0411]\n",
      "Weights: [-4.7971  0.8007 -1.3174  0.1633  0.1358]\n",
      "MSE loss: 90.8981\n",
      "Iteration: 195600\n",
      "Gradient: [   8.7549  -15.0786   23.844   -59.096  -401.9992]\n",
      "Weights: [-4.7944  0.7928 -1.3177  0.1642  0.1358]\n",
      "MSE loss: 90.8539\n",
      "Iteration: 195700\n",
      "Gradient: [  -3.9326   -5.8573   -9.8654  -68.6934 -297.5541]\n",
      "Weights: [-4.8224  0.8137 -1.3178  0.1627  0.1357]\n",
      "MSE loss: 90.9879\n",
      "Iteration: 195800\n",
      "Gradient: [  -1.5545  -21.0504  -60.8347  -21.8548 -173.5913]\n",
      "Weights: [-4.8183  0.8139 -1.3145  0.1623  0.1356]\n",
      "MSE loss: 91.0209\n",
      "Iteration: 195900\n",
      "Gradient: [  3.3553  11.1306  62.2668  20.508  500.8397]\n",
      "Weights: [-4.8072  0.8159 -1.3133  0.1613  0.1357]\n",
      "MSE loss: 91.4515\n",
      "Iteration: 196000\n",
      "Gradient: [ -0.0711  -4.0915  11.2091 -55.8746 -45.5094]\n",
      "Weights: [-4.803   0.8065 -1.3159  0.1622  0.1357]\n",
      "MSE loss: 90.8534\n",
      "Iteration: 196100\n",
      "Gradient: [  -1.181     2.4967   -0.7569 -138.3186  281.1087]\n",
      "Weights: [-4.8164  0.813  -1.316   0.1621  0.1357]\n",
      "MSE loss: 90.8965\n",
      "Iteration: 196200\n",
      "Gradient: [  -6.2785   -2.6892  -39.3453 -185.8219  -79.3389]\n",
      "Weights: [-4.8215  0.8142 -1.3178  0.1617  0.1358]\n",
      "MSE loss: 91.0724\n",
      "Iteration: 196300\n",
      "Gradient: [-6.020800e+00  2.194000e-01  1.394860e+01 -1.591370e+02  3.802981e+02]\n",
      "Weights: [-4.8357  0.8251 -1.3205  0.1619  0.1361]\n",
      "MSE loss: 91.2646\n",
      "Iteration: 196400\n",
      "Gradient: [   4.9926    6.4229  -13.9574  -28.7704 -179.7311]\n",
      "Weights: [-4.8244  0.8316 -1.3225  0.1617  0.136 ]\n",
      "MSE loss: 91.0116\n",
      "Iteration: 196500\n",
      "Gradient: [  6.8232   9.3535  11.8274  -2.4363 118.929 ]\n",
      "Weights: [-4.8081  0.8184 -1.3226  0.1621  0.136 ]\n",
      "MSE loss: 90.8487\n",
      "Iteration: 196600\n",
      "Gradient: [ -2.9899 -12.6661   4.8731  19.8123  -2.4184]\n",
      "Weights: [-4.8193  0.8298 -1.3247  0.1624  0.1363]\n",
      "MSE loss: 91.1745\n",
      "Iteration: 196700\n",
      "Gradient: [   5.3826    4.7195   18.1919  -59.3282 -132.0797]\n",
      "Weights: [-4.8246  0.8307 -1.3292  0.1627  0.1362]\n",
      "MSE loss: 91.0833\n",
      "Iteration: 196800\n",
      "Gradient: [ 5.2969 10.0774  9.9279 43.795  22.8529]\n",
      "Weights: [-4.8337  0.8516 -1.3336  0.1635  0.1361]\n",
      "MSE loss: 91.132\n",
      "Iteration: 196900\n",
      "Gradient: [-6.960000e-02  7.714800e+00 -2.687610e+01 -2.536240e+01  1.782105e+02]\n",
      "Weights: [-4.8244  0.8342 -1.3334  0.1643  0.1358]\n",
      "MSE loss: 91.2214\n",
      "Iteration: 197000\n",
      "Gradient: [  2.1705   4.772  -32.0739  92.5555 148.8207]\n",
      "Weights: [-4.8183  0.8318 -1.3334  0.1642  0.1359]\n",
      "MSE loss: 91.0772\n",
      "Iteration: 197100\n",
      "Gradient: [ -12.1243  -19.6902    0.5611   26.8282 -224.3035]\n",
      "Weights: [-4.8295  0.8358 -1.3316  0.1641  0.1359]\n",
      "MSE loss: 91.0217\n",
      "Iteration: 197200\n",
      "Gradient: [  -0.9528   -2.3146  -22.8246  -53.0253 -185.7605]\n",
      "Weights: [-4.8112  0.8271 -1.331   0.1634  0.1361]\n",
      "MSE loss: 90.9151\n",
      "Iteration: 197300\n",
      "Gradient: [  5.3585  19.0003 -10.5528 121.3339 130.2515]\n",
      "Weights: [-4.8058  0.8307 -1.3339  0.1634  0.1363]\n",
      "MSE loss: 90.8873\n",
      "Iteration: 197400\n",
      "Gradient: [  -8.9939    4.4355    1.2737  -64.1573 -187.1336]\n",
      "Weights: [-4.8265  0.8285 -1.3342  0.1637  0.1367]\n",
      "MSE loss: 91.2836\n",
      "Iteration: 197500\n",
      "Gradient: [  -1.0532   11.6759    3.663   -14.6674 -315.6124]\n",
      "Weights: [-4.8093  0.835  -1.3382  0.1637  0.1368]\n",
      "MSE loss: 90.8535\n",
      "Iteration: 197600\n",
      "Gradient: [-7.2249 -1.1786 11.6574 -8.9352  7.9398]\n",
      "Weights: [-4.8072  0.8262 -1.3333  0.1633  0.1367]\n",
      "MSE loss: 90.8468\n",
      "Iteration: 197700\n",
      "Gradient: [-7.3432 -1.6112 -8.0765 24.1593 -1.902 ]\n",
      "Weights: [-4.8181  0.8291 -1.33    0.163   0.1365]\n",
      "MSE loss: 90.872\n",
      "Iteration: 197800\n",
      "Gradient: [ 12.607    2.5872  19.3759 112.2572  47.9693]\n",
      "Weights: [-4.8011  0.8259 -1.3304  0.163   0.1364]\n",
      "MSE loss: 90.9654\n",
      "Iteration: 197900\n",
      "Gradient: [-15.3082 -11.5335   7.1461 -18.4074  88.4945]\n",
      "Weights: [-4.8059  0.8219 -1.3289  0.163   0.1365]\n",
      "MSE loss: 90.8733\n",
      "Iteration: 198000\n",
      "Gradient: [ -11.046    -6.4016    8.1497  -14.3586 -357.7727]\n",
      "Weights: [-4.8005  0.8075 -1.328   0.1631  0.1366]\n",
      "MSE loss: 91.0306\n",
      "Iteration: 198100\n",
      "Gradient: [-6.582800e+00  2.746000e-01  2.120000e+00  8.891150e+01 -3.006136e+02]\n",
      "Weights: [-4.7919  0.8038 -1.3234  0.1621  0.1367]\n",
      "MSE loss: 90.9236\n",
      "Iteration: 198200\n",
      "Gradient: [  -2.2771    0.4027    0.2809   11.474  -104.7634]\n",
      "Weights: [-4.8163  0.8166 -1.3227  0.1618  0.1365]\n",
      "MSE loss: 90.927\n",
      "Iteration: 198300\n",
      "Gradient: [ -3.7387  -2.4878 -21.2201 -72.3524  98.0575]\n",
      "Weights: [-4.8023  0.8149 -1.3243  0.1621  0.1364]\n",
      "MSE loss: 90.8556\n",
      "Iteration: 198400\n",
      "Gradient: [  2.1483  -2.6431 -23.4997 -37.1664  90.597 ]\n",
      "Weights: [-4.7966  0.8084 -1.3253  0.1631  0.1366]\n",
      "MSE loss: 90.9796\n",
      "Iteration: 198500\n",
      "Gradient: [  -4.295    -2.011   -37.9902  -10.4204 -223.9025]\n",
      "Weights: [-4.8101  0.8045 -1.3236  0.1629  0.1363]\n",
      "MSE loss: 91.3504\n",
      "Iteration: 198600\n",
      "Gradient: [  -4.3675  -13.0179    0.8562   55.6274 -119.0914]\n",
      "Weights: [-4.8154  0.8114 -1.325   0.1639  0.136 ]\n",
      "MSE loss: 91.1067\n",
      "Iteration: 198700\n",
      "Gradient: [-2.1419 -4.6948 -6.7686 75.7113 86.5835]\n",
      "Weights: [-4.8075  0.817  -1.3222  0.1639  0.1356]\n",
      "MSE loss: 90.908\n",
      "Iteration: 198800\n",
      "Gradient: [ -5.4996   1.3488 -23.4223 -30.3458 -61.3022]\n",
      "Weights: [-4.8049  0.8085 -1.3226  0.1636  0.1358]\n",
      "MSE loss: 90.8977\n",
      "Iteration: 198900\n",
      "Gradient: [  1.5936   7.3728  21.5653 177.1722 119.0254]\n",
      "Weights: [-4.8071  0.821  -1.3242  0.164   0.1359]\n",
      "MSE loss: 91.258\n",
      "Iteration: 199000\n",
      "Gradient: [  -7.064   -11.1411  -34.9863 -201.1107  -52.179 ]\n",
      "Weights: [-4.8187  0.8264 -1.3262  0.1637  0.1357]\n",
      "MSE loss: 90.8597\n",
      "Iteration: 199100\n",
      "Gradient: [ -1.8862  -7.7254  -1.2429   8.457  144.7524]\n",
      "Weights: [-4.8047  0.826  -1.3288  0.1635  0.1359]\n",
      "MSE loss: 90.8897\n",
      "Iteration: 199200\n",
      "Gradient: [ -4.2734  -5.1855  34.2569 -62.1797 -82.3225]\n",
      "Weights: [-4.8185  0.8351 -1.33    0.1631  0.1359]\n",
      "MSE loss: 90.8737\n",
      "Iteration: 199300\n",
      "Gradient: [   7.0418    6.9324  -23.3451   36.5933 -364.1984]\n",
      "Weights: [-4.8177  0.8367 -1.3307  0.1632  0.1361]\n",
      "MSE loss: 90.8841\n",
      "Iteration: 199400\n",
      "Gradient: [ 12.7242 -14.2111 -15.4367 -44.1227 280.6805]\n",
      "Weights: [-4.8066  0.8241 -1.3276  0.1629  0.1361]\n",
      "MSE loss: 90.855\n",
      "Iteration: 199500\n",
      "Gradient: [ 9.5572 17.2926 18.7662  8.5077 80.612 ]\n",
      "Weights: [-4.7808  0.8121 -1.325   0.1629  0.1362]\n",
      "MSE loss: 91.5849\n",
      "Iteration: 199600\n",
      "Gradient: [  -7.0946   10.9179  -30.1327 -127.3379 -406.7566]\n",
      "Weights: [-4.809   0.8112 -1.3241  0.1627  0.1362]\n",
      "MSE loss: 90.991\n",
      "Iteration: 199700\n",
      "Gradient: [   9.883   -13.6092   16.6646    5.8992 -278.0342]\n",
      "Weights: [-4.8139  0.8114 -1.3221  0.1625  0.1363]\n",
      "MSE loss: 90.9387\n",
      "Iteration: 199800\n",
      "Gradient: [  -4.5131    5.9182   -6.4782  -17.7332 -102.0482]\n",
      "Weights: [-4.7947  0.8068 -1.3224  0.1619  0.1365]\n",
      "MSE loss: 90.8891\n",
      "Iteration: 199900\n",
      "Gradient: [ -3.333  -14.4531 -12.8708  47.3543 308.6081]\n",
      "Weights: [-4.8186  0.8081 -1.3207  0.1626  0.1364]\n",
      "MSE loss: 91.1296\n"
     ]
    }
   ],
   "source": [
    "weights_5, losses_5, iter_final_5, fit_time_5 = model_fit(X_train, y_train,\n",
    "                                                          learning_rate=[1e-3, 1e-4, 1e-5, 1e-6, 1e-7],\n",
    "                                                          tolerance=(0.2**2 * N_points),\n",
    "                                                          beta=0.4,\n",
    "                                                          batch_ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45310014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Gradient: [ 3232.2956  4402.7926  7785.6578 14794.7759 31771.3655]\n",
      "Weights: [-0.001  -0.0001 -0.     -0.     -0.    ]\n",
      "MSE loss: 39312.0109\n",
      "Iteration: 100\n",
      "Gradient: [    1.5996    96.822    208.9188    50.2622 -1066.8014]\n",
      "Weights: [-4.7695 -0.7066  0.0923  0.0955  0.0412]\n",
      "MSE loss: 444.9471\n",
      "Iteration: 200\n",
      "Gradient: [   4.0082   38.1315   66.1652  -51.1935 -493.5062]\n",
      "Weights: [-4.4873 -1.0499  0.0625  0.1105  0.0531]\n",
      "MSE loss: 275.868\n",
      "Iteration: 300\n",
      "Gradient: [  16.8264   30.9993   18.0861   -8.2613 -926.9618]\n",
      "Weights: [-4.3165 -1.2138  0.0401  0.1191  0.059 ]\n",
      "MSE loss: 229.0924\n",
      "Iteration: 400\n",
      "Gradient: [ 17.5154  47.3809  73.6792 205.2618 111.4398]\n",
      "Weights: [-4.2316 -1.282   0.0189  0.1207  0.0635]\n",
      "MSE loss: 212.2709\n",
      "Iteration: 500\n",
      "Gradient: [ -13.5381  -46.208    -1.6095  -45.3356 -286.0314]\n",
      "Weights: [-4.1832e+00 -1.3190e+00 -1.0000e-04  1.2300e-01  6.6700e-02]\n",
      "MSE loss: 204.8232\n",
      "Iteration: 600\n",
      "Gradient: [  15.516   -18.9261   10.9645 -104.1766 -590.7582]\n",
      "Weights: [-4.163  -1.3065 -0.0226  0.1218  0.069 ]\n",
      "MSE loss: 199.8126\n",
      "Iteration: 700\n",
      "Gradient: [ 13.9244   9.6845  37.2646 129.0044 285.7894]\n",
      "Weights: [-4.1638 -1.2838 -0.0382  0.1203  0.0709]\n",
      "MSE loss: 196.3065\n",
      "Iteration: 800\n",
      "Gradient: [ -2.028   41.4246  25.9175 -26.5503 443.8582]\n",
      "Weights: [-4.1862 -1.2539 -0.0538  0.122   0.072 ]\n",
      "MSE loss: 194.7073\n",
      "Iteration: 900\n",
      "Gradient: [  20.4103   -9.1416   59.6646  -35.3478 -537.3499]\n",
      "Weights: [-4.184  -1.229  -0.065   0.1198  0.0727]\n",
      "MSE loss: 191.9064\n",
      "Iteration: 1000\n",
      "Gradient: [  5.7373 -25.779  103.6881  20.8502  86.5131]\n",
      "Weights: [-4.1809 -1.2244 -0.0794  0.118   0.0742]\n",
      "MSE loss: 188.6287\n",
      "Iteration: 1100\n",
      "Gradient: [ -14.1423  -39.8939  -64.2748 -492.4686 -372.9096]\n",
      "Weights: [-4.1651 -1.2135 -0.0966  0.1172  0.076 ]\n",
      "MSE loss: 185.8374\n",
      "Iteration: 1200\n",
      "Gradient: [ 0.2496 -8.0729 41.7787 46.7194  1.1338]\n",
      "Weights: [-4.194  -1.1807 -0.1105  0.117   0.0773]\n",
      "MSE loss: 182.5997\n",
      "Iteration: 1300\n",
      "Gradient: [ 28.1036 -19.3078 -33.3574  61.4876 137.7635]\n",
      "Weights: [-4.1931 -1.1586 -0.1221  0.1165  0.0781]\n",
      "MSE loss: 180.6248\n",
      "Iteration: 1400\n",
      "Gradient: [-11.6763  20.981   30.1114  56.0303 324.9672]\n",
      "Weights: [-4.2044 -1.1261 -0.138   0.1156  0.0791]\n",
      "MSE loss: 177.8291\n",
      "Iteration: 1500\n",
      "Gradient: [  6.7685  23.7851  59.8075 160.1523 445.5454]\n",
      "Weights: [-4.2268 -1.0914 -0.1513  0.1139  0.0803]\n",
      "MSE loss: 175.1171\n",
      "Iteration: 1600\n",
      "Gradient: [  -5.1269  -41.6525 -111.5517  -64.8951 -266.8418]\n",
      "Weights: [-4.2376 -1.0556 -0.1692  0.112   0.0815]\n",
      "MSE loss: 171.88\n",
      "Iteration: 1700\n",
      "Gradient: [ 20.4794   5.028   42.3503 -66.8106 182.2902]\n",
      "Weights: [-4.2436 -1.0255 -0.1866  0.1116  0.0827]\n",
      "MSE loss: 169.088\n",
      "Iteration: 1800\n",
      "Gradient: [  -6.0928  -37.2906 -149.9058 -266.0119 -209.8969]\n",
      "Weights: [-4.2675 -0.9915 -0.2045  0.1116  0.0838]\n",
      "MSE loss: 166.3729\n",
      "Iteration: 1900\n",
      "Gradient: [  10.4114  -18.2943  -69.3115  -31.1389 -228.7507]\n",
      "Weights: [-4.2715 -0.9684 -0.2181  0.11    0.0847]\n",
      "MSE loss: 164.4915\n",
      "Iteration: 2000\n",
      "Gradient: [   0.9274  -15.2884   23.2537  -15.7418 -265.9441]\n",
      "Weights: [-4.2658 -0.9448 -0.2362  0.1093  0.0864]\n",
      "MSE loss: 161.2871\n",
      "Iteration: 2100\n",
      "Gradient: [  -8.0553   -7.739    -7.2136 -152.1173 -620.255 ]\n",
      "Weights: [-4.2713 -0.911  -0.2517  0.1078  0.0875]\n",
      "MSE loss: 158.7934\n",
      "Iteration: 2200\n",
      "Gradient: [-14.0236 -13.5229  87.1302  28.4092 213.9371]\n",
      "Weights: [-4.2841 -0.8876 -0.2635  0.1076  0.0884]\n",
      "MSE loss: 156.9179\n",
      "Iteration: 2300\n",
      "Gradient: [-3.968000e-01 -5.698600e+00 -2.796990e+01 -6.127820e+01 -9.327563e+02]\n",
      "Weights: [-4.2904 -0.8666 -0.2761  0.1057  0.0898]\n",
      "MSE loss: 154.6668\n",
      "Iteration: 2400\n",
      "Gradient: [  25.475     4.9904  -27.1552 -351.9843 -296.0681]\n",
      "Weights: [-4.2965 -0.8401 -0.2848  0.1051  0.0904]\n",
      "MSE loss: 153.7833\n",
      "Iteration: 2500\n",
      "Gradient: [  10.2078    5.3175    0.2691  -45.3339 -173.2473]\n",
      "Weights: [-4.3084 -0.8185 -0.3001  0.1028  0.0916]\n",
      "MSE loss: 151.197\n",
      "Iteration: 2600\n",
      "Gradient: [  -8.5832    5.0899  -62.8225 -208.0422 -400.2059]\n",
      "Weights: [-4.3348 -0.7732 -0.3157  0.101   0.0928]\n",
      "MSE loss: 148.4918\n",
      "Iteration: 2700\n",
      "Gradient: [ 10.2772  15.0141  39.8966 232.188  112.3145]\n",
      "Weights: [-4.316  -0.7515 -0.3276  0.0998  0.094 ]\n",
      "MSE loss: 147.4432\n",
      "Iteration: 2800\n",
      "Gradient: [ 14.8032   7.3438 115.5204 -50.7024 428.3114]\n",
      "Weights: [-4.3458 -0.7295 -0.3366  0.0986  0.0952]\n",
      "MSE loss: 145.3366\n",
      "Iteration: 2900\n",
      "Gradient: [  29.3545   -2.7558   15.0558  101.7793 -132.5754]\n",
      "Weights: [-4.3373 -0.7198 -0.3466  0.0971  0.0964]\n",
      "MSE loss: 143.6961\n",
      "Iteration: 3000\n",
      "Gradient: [-10.4703  11.0981   3.9535 -31.8799  99.7832]\n",
      "Weights: [-4.3418 -0.6927 -0.359   0.0946  0.0977]\n",
      "MSE loss: 141.7549\n",
      "Iteration: 3100\n",
      "Gradient: [  -6.2189   -6.4501  -32.598    -7.3191 -261.0158]\n",
      "Weights: [-4.3739 -0.6765 -0.3703  0.0946  0.0987]\n",
      "MSE loss: 140.1337\n",
      "Iteration: 3200\n",
      "Gradient: [ -8.4214  11.3604  50.3586  60.8824 273.7954]\n",
      "Weights: [-4.36   -0.6621 -0.3803  0.0943  0.0993]\n",
      "MSE loss: 138.7092\n",
      "Iteration: 3300\n",
      "Gradient: [-12.4546  16.4374   1.7252 -34.4599 -87.9792]\n",
      "Weights: [-4.3564 -0.6496 -0.3862  0.0922  0.1001]\n",
      "MSE loss: 137.7033\n",
      "Iteration: 3400\n",
      "Gradient: [  -1.146    12.6183   50.0649  -46.3422 -188.9987]\n",
      "Weights: [-4.3702 -0.6318 -0.4017  0.0929  0.101 ]\n",
      "MSE loss: 136.1632\n",
      "Iteration: 3500\n",
      "Gradient: [-20.1741  33.256   94.7118 115.702  553.1678]\n",
      "Weights: [-4.3848 -0.607  -0.4159  0.0942  0.1019]\n",
      "MSE loss: 134.6276\n",
      "Iteration: 3600\n",
      "Gradient: [-23.8467 -45.5992  22.5813 286.7395  22.7938]\n",
      "Weights: [-4.398  -0.5777 -0.4305  0.0923  0.1027]\n",
      "MSE loss: 133.456\n",
      "Iteration: 3700\n",
      "Gradient: [  -5.6315  -34.2785  -62.4208 -207.1312  -20.4311]\n",
      "Weights: [-4.4004 -0.5426 -0.4443  0.0916  0.1037]\n",
      "MSE loss: 130.9488\n",
      "Iteration: 3800\n",
      "Gradient: [ 15.8744  41.1007  23.5037 261.0262 587.0079]\n",
      "Weights: [-4.4103 -0.5215 -0.4584  0.0913  0.1049]\n",
      "MSE loss: 129.3696\n",
      "Iteration: 3900\n",
      "Gradient: [  -5.3958   -8.9334  -68.6603  -50.022  -381.7835]\n",
      "Weights: [-4.4073 -0.4982 -0.4694  0.0896  0.1056]\n",
      "MSE loss: 128.0505\n",
      "Iteration: 4000\n",
      "Gradient: [-1.298620e+01 -2.981110e+01 -6.895000e-01 -1.268300e+01 -7.678539e+02]\n",
      "Weights: [-4.4181 -0.4876 -0.4776  0.0889  0.1065]\n",
      "MSE loss: 127.0878\n",
      "Iteration: 4100\n",
      "Gradient: [  -7.6919  -15.5846   47.0786 -293.1702   67.1144]\n",
      "Weights: [-4.4126 -0.4609 -0.4866  0.088   0.1073]\n",
      "MSE loss: 126.6716\n",
      "Iteration: 4200\n",
      "Gradient: [  -1.849     9.5839  -37.0694 -141.845  -204.5014]\n",
      "Weights: [-4.4373 -0.446  -0.4943  0.0873  0.1079]\n",
      "MSE loss: 124.9887\n",
      "Iteration: 4300\n",
      "Gradient: [ 13.0467  10.9086  35.7641 -30.2978 119.8742]\n",
      "Weights: [-4.4201 -0.4404 -0.5048  0.0871  0.1087]\n",
      "MSE loss: 124.0844\n",
      "Iteration: 4400\n",
      "Gradient: [ -5.1396   0.4485  36.6328 -38.8935  96.257 ]\n",
      "Weights: [-4.4184 -0.4408 -0.5125  0.0871  0.1096]\n",
      "MSE loss: 123.4338\n",
      "Iteration: 4500\n",
      "Gradient: [1.090710e+01 7.100000e-03 4.182820e+01 8.348980e+01 6.646283e+02]\n",
      "Weights: [-4.4423 -0.4124 -0.5209  0.0866  0.1101]\n",
      "MSE loss: 122.3144\n",
      "Iteration: 4600\n",
      "Gradient: [   6.5716   10.4863   24.5193  188.4532 -246.7528]\n",
      "Weights: [-4.4506 -0.3976 -0.5256  0.0859  0.1105]\n",
      "MSE loss: 121.7306\n",
      "Iteration: 4700\n",
      "Gradient: [ 20.0745   0.7694 -35.1958  91.8363  74.5352]\n",
      "Weights: [-4.4517 -0.3717 -0.5415  0.0856  0.1112]\n",
      "MSE loss: 120.2848\n",
      "Iteration: 4800\n",
      "Gradient: [ -14.5713   -5.3344  -27.5276  -29.0832 -107.4521]\n",
      "Weights: [-4.4862 -0.3512 -0.5499  0.0855  0.1122]\n",
      "MSE loss: 120.0152\n",
      "Iteration: 4900\n",
      "Gradient: [   3.2925    5.03     24.3275  155.6924 -105.3625]\n",
      "Weights: [-4.4618 -0.32   -0.5615  0.0837  0.1129]\n",
      "MSE loss: 118.8662\n",
      "Iteration: 5000\n",
      "Gradient: [  30.2975  -21.2634  -86.5136   77.5404 -490.5526]\n",
      "Weights: [-4.4445 -0.3108 -0.5699  0.0829  0.1136]\n",
      "MSE loss: 119.1605\n",
      "Iteration: 5100\n",
      "Gradient: [  3.7711   2.369   21.1359   2.2576 169.3186]\n",
      "Weights: [-4.4782 -0.3029 -0.5743  0.0829  0.114 ]\n",
      "MSE loss: 116.8798\n",
      "Iteration: 5200\n",
      "Gradient: [ 16.1993   0.7276  -7.6133  64.0968 323.7906]\n",
      "Weights: [-4.4897 -0.2794 -0.5833  0.0807  0.1145]\n",
      "MSE loss: 116.2231\n",
      "Iteration: 5300\n",
      "Gradient: [ 10.6548   8.8663  64.9536 161.6393 341.8472]\n",
      "Weights: [-4.4728 -0.2724 -0.5886  0.0807  0.1152]\n",
      "MSE loss: 115.8436\n",
      "Iteration: 5400\n",
      "Gradient: [ -14.9464  -17.2717  -28.2663  -31.2261 -421.7808]\n",
      "Weights: [-4.5019 -0.2744 -0.5927  0.0812  0.1156]\n",
      "MSE loss: 115.8718\n",
      "Iteration: 5500\n",
      "Gradient: [-22.5749  -5.8495  17.74    64.5545 197.5904]\n",
      "Weights: [-4.5058 -0.2572 -0.5958  0.0803  0.1161]\n",
      "MSE loss: 114.7778\n",
      "Iteration: 5600\n",
      "Gradient: [ 2.922000e-01  8.280000e-02  3.222080e+01  2.434790e+01 -1.046407e+02]\n",
      "Weights: [-4.5086 -0.2545 -0.6047  0.0812  0.1164]\n",
      "MSE loss: 114.7454\n",
      "Iteration: 5700\n",
      "Gradient: [  17.89     -6.7876  -12.2402    3.6437 -145.5987]\n",
      "Weights: [-4.458  -0.2537 -0.6095  0.0809  0.117 ]\n",
      "MSE loss: 115.1049\n",
      "Iteration: 5800\n",
      "Gradient: [  -7.8019   13.9748   44.2851  148.7798 -371.7555]\n",
      "Weights: [-4.5099 -0.2323 -0.6161  0.0788  0.1178]\n",
      "MSE loss: 113.3359\n",
      "Iteration: 5900\n",
      "Gradient: [ -17.016   -17.5619    9.1762  -20.2851 -537.684 ]\n",
      "Weights: [-4.5199 -0.2169 -0.6229  0.0785  0.118 ]\n",
      "MSE loss: 113.2088\n",
      "Iteration: 6000\n",
      "Gradient: [ -12.5419   -7.5299  -92.2898 -126.2967  -83.9854]\n",
      "Weights: [-4.5137 -0.2017 -0.6293  0.0773  0.1189]\n",
      "MSE loss: 111.6837\n",
      "Iteration: 6100\n",
      "Gradient: [-10.1715  -9.3284  57.4489  99.3476 113.3851]\n",
      "Weights: [-4.5274 -0.1732 -0.6369  0.0767  0.1192]\n",
      "MSE loss: 111.109\n",
      "Iteration: 6200\n",
      "Gradient: [  -5.4233    3.9715   41.848  -137.9574 -153.8785]\n",
      "Weights: [-4.5229 -0.1647 -0.6482  0.0758  0.1199]\n",
      "MSE loss: 110.4117\n",
      "Iteration: 6300\n",
      "Gradient: [  -0.8873  -24.6866 -122.2745 -125.0243 -666.7982]\n",
      "Weights: [-4.4994 -0.1658 -0.6527  0.0765  0.1205]\n",
      "MSE loss: 110.2806\n",
      "Iteration: 6400\n",
      "Gradient: [-14.0669  15.5677 -11.4846  60.1316  -6.2979]\n",
      "Weights: [-4.5576 -0.1321 -0.6581  0.0748  0.1208]\n",
      "MSE loss: 109.8307\n",
      "Iteration: 6500\n",
      "Gradient: [   3.2281  -11.9861 -100.4045 -237.9409 -352.6678]\n",
      "Weights: [-4.5406 -0.1251 -0.6664  0.0742  0.1211]\n",
      "MSE loss: 109.388\n",
      "Iteration: 6600\n",
      "Gradient: [ -9.7921  10.8271 -43.8871  86.0721 302.9535]\n",
      "Weights: [-4.5392 -0.1079 -0.6753  0.0747  0.1219]\n",
      "MSE loss: 108.0778\n",
      "Iteration: 6700\n",
      "Gradient: [ 17.2405  16.6789  21.6997 129.2021 407.956 ]\n",
      "Weights: [-4.533  -0.0995 -0.6765  0.0728  0.1224]\n",
      "MSE loss: 108.1802\n",
      "Iteration: 6800\n",
      "Gradient: [  -8.1006  -49.2686  -76.5311 -265.256     9.0459]\n",
      "Weights: [-4.5376 -0.1012 -0.6843  0.0732  0.1229]\n",
      "MSE loss: 107.668\n",
      "Iteration: 6900\n",
      "Gradient: [  2.2429  35.1051  63.4711 -11.3595 207.0235]\n",
      "Weights: [-4.5293 -0.0914 -0.6905  0.0736  0.1237]\n",
      "MSE loss: 107.3106\n",
      "Iteration: 7000\n",
      "Gradient: [-13.0493  10.7488  -1.9776 176.5174 410.0126]\n",
      "Weights: [-4.5467 -0.0996 -0.6955  0.0746  0.1242]\n",
      "MSE loss: 107.1341\n",
      "Iteration: 7100\n",
      "Gradient: [-21.9985  25.2208 -24.9594 -35.4997 -20.3965]\n",
      "Weights: [-4.5536 -0.0702 -0.7039  0.0732  0.1244]\n",
      "MSE loss: 106.2687\n",
      "Iteration: 7200\n",
      "Gradient: [  23.0644  -13.2593    2.1131  254.5301 -266.9382]\n",
      "Weights: [-4.5472 -0.0532 -0.7076  0.0731  0.1246]\n",
      "MSE loss: 106.0978\n",
      "Iteration: 7300\n",
      "Gradient: [ 1.765470e+01  1.195000e-01  1.540800e+01  1.642573e+02 -3.383413e+02]\n",
      "Weights: [-4.543  -0.0361 -0.7194  0.0712  0.1258]\n",
      "MSE loss: 105.2551\n",
      "Iteration: 7400\n",
      "Gradient: [  3.6766 -16.9483 -11.5102  -3.2635  39.9627]\n",
      "Weights: [-4.5624 -0.0221 -0.7286  0.07    0.1266]\n",
      "MSE loss: 104.4657\n",
      "Iteration: 7500\n",
      "Gradient: [ -2.4998  23.9909  52.336   49.4728 588.9461]\n",
      "Weights: [-4.5581 -0.0091 -0.7308  0.0698  0.1268]\n",
      "MSE loss: 104.3228\n",
      "Iteration: 7600\n",
      "Gradient: [  -2.8138   -3.775   -31.7372  -54.3341 -230.1768]\n",
      "Weights: [-4.5887e+00  4.1000e-03 -7.4010e-01  6.9600e-02  1.2750e-01]\n",
      "MSE loss: 103.9501\n",
      "Iteration: 7700\n",
      "Gradient: [  8.4389  26.594  103.5974 251.1604  89.357 ]\n",
      "Weights: [-4.5909  0.0285 -0.748   0.0701  0.1277]\n",
      "MSE loss: 103.0504\n",
      "Iteration: 7800\n",
      "Gradient: [  -3.5271  -16.8596  -45.3643  -30.3514 -285.7575]\n",
      "Weights: [-4.5763  0.031  -0.7526  0.0694  0.1282]\n",
      "MSE loss: 102.83\n",
      "Iteration: 7900\n",
      "Gradient: [  -2.522   -13.0706  -43.8155 -140.1227 -190.5332]\n",
      "Weights: [-4.5742  0.0382 -0.7616  0.0695  0.1288]\n",
      "MSE loss: 102.2959\n",
      "Iteration: 8000\n",
      "Gradient: [   7.9373   -8.4474   27.6113   36.7085 -330.4849]\n",
      "Weights: [-4.5947  0.0534 -0.7612  0.0683  0.1291]\n",
      "MSE loss: 102.1773\n",
      "Iteration: 8100\n",
      "Gradient: [  15.4012  -43.2044  -12.3905  -66.7137 -572.1574]\n",
      "Weights: [-4.5827  0.0598 -0.7667  0.0677  0.1293]\n",
      "MSE loss: 102.0157\n",
      "Iteration: 8200\n",
      "Gradient: [  -4.4042   16.734   104.0804 -110.6529 -426.0779]\n",
      "Weights: [-4.5841  0.0611 -0.7714  0.0677  0.1299]\n",
      "MSE loss: 101.5883\n",
      "Iteration: 8300\n",
      "Gradient: [ -11.1444   21.9765  -21.0105 -183.1985   -8.4879]\n",
      "Weights: [-4.6096  0.0752 -0.7769  0.0676  0.1303]\n",
      "MSE loss: 101.348\n",
      "Iteration: 8400\n",
      "Gradient: [ -23.612   -31.1948  -40.9737   39.49   -338.4801]\n",
      "Weights: [-4.6141  0.0835 -0.7801  0.067   0.1305]\n",
      "MSE loss: 101.2206\n",
      "Iteration: 8500\n",
      "Gradient: [  -2.1726    3.9204  -19.5475   -4.1139 -287.476 ]\n",
      "Weights: [-4.6078  0.0995 -0.7877  0.0667  0.1308]\n",
      "MSE loss: 100.6409\n",
      "Iteration: 8600\n",
      "Gradient: [  8.6171   7.4335  68.3375 -72.5202 458.9664]\n",
      "Weights: [-4.6052  0.1087 -0.797   0.0667  0.1313]\n",
      "MSE loss: 100.3964\n",
      "Iteration: 8700\n",
      "Gradient: [-11.9298  13.144   77.8665  91.0911  91.6663]\n",
      "Weights: [-4.6244  0.1122 -0.8006  0.0678  0.1319]\n",
      "MSE loss: 100.3216\n",
      "Iteration: 8800\n",
      "Gradient: [   3.7999   12.9628  -16.9024  197.3503 -484.8617]\n",
      "Weights: [-4.615   0.1157 -0.8078  0.0685  0.1319]\n",
      "MSE loss: 100.2685\n",
      "Iteration: 8900\n",
      "Gradient: [   4.3156   30.2586   -5.2593   80.1391 -243.2274]\n",
      "Weights: [-4.6285  0.1461 -0.8092  0.0668  0.132 ]\n",
      "MSE loss: 99.6898\n",
      "Iteration: 9000\n",
      "Gradient: [  1.4308  28.3668  93.5805 133.3678 174.9292]\n",
      "Weights: [-4.6241  0.1411 -0.8133  0.0677  0.1322]\n",
      "MSE loss: 99.4801\n",
      "Iteration: 9100\n",
      "Gradient: [   1.8575  -17.8571  -73.073  -145.4727  -71.37  ]\n",
      "Weights: [-4.61    0.1426 -0.8164  0.0674  0.1325]\n",
      "MSE loss: 99.4255\n",
      "Iteration: 9200\n",
      "Gradient: [  -2.7834  -12.8104   31.6377  -95.629  -487.5516]\n",
      "Weights: [-4.6257  0.1501 -0.8245  0.0661  0.1334]\n",
      "MSE loss: 99.5604\n",
      "Iteration: 9300\n",
      "Gradient: [ -6.0181   1.9318  81.2645  92.7783 209.3456]\n",
      "Weights: [-4.6392  0.155  -0.8269  0.0668  0.1336]\n",
      "MSE loss: 99.5996\n",
      "Iteration: 9400\n",
      "Gradient: [ -8.6602 -11.939   31.1465 -78.6862 272.8564]\n",
      "Weights: [-4.6132  0.173  -0.8359  0.0672  0.1341]\n",
      "MSE loss: 98.9327\n",
      "Iteration: 9500\n",
      "Gradient: [  -7.313   -19.9675   22.4083  -65.4693 -335.0871]\n",
      "Weights: [-4.6432  0.1737 -0.8376  0.0671  0.1344]\n",
      "MSE loss: 98.6864\n",
      "Iteration: 9600\n",
      "Gradient: [ -20.9759  -44.6683  -56.2256 -159.7917  -10.1759]\n",
      "Weights: [-4.6639  0.1934 -0.847   0.0674  0.1344]\n",
      "MSE loss: 99.896\n",
      "Iteration: 9700\n",
      "Gradient: [ -12.6369  -20.3429  -15.2837  -22.0531 -104.6989]\n",
      "Weights: [-4.6253  0.1999 -0.8513  0.0661  0.135 ]\n",
      "MSE loss: 97.8542\n",
      "Iteration: 9800\n",
      "Gradient: [  5.7328 -18.6548 -42.6878 -22.2221 203.1191]\n",
      "Weights: [-4.6287  0.2074 -0.8551  0.0667  0.1355]\n",
      "MSE loss: 98.006\n",
      "Iteration: 9900\n",
      "Gradient: [-19.68     1.7518  26.0174 174.2204 -29.2225]\n",
      "Weights: [-4.6495  0.2139 -0.8608  0.067   0.1357]\n",
      "MSE loss: 97.5015\n",
      "Iteration: 10000\n",
      "Gradient: [  3.45    24.5691 -20.6319 -77.0126  67.5933]\n",
      "Weights: [-4.6426  0.2225 -0.8625  0.0655  0.1361]\n",
      "MSE loss: 97.23\n",
      "Iteration: 10100\n",
      "Gradient: [   4.7941  -18.3032  -91.651  -226.4938 -392.5384]\n",
      "Weights: [-4.649   0.2492 -0.8739  0.0652  0.1365]\n",
      "MSE loss: 96.8663\n",
      "Iteration: 10200\n",
      "Gradient: [ -12.9652   -8.3424   16.759    15.2582 -270.5571]\n",
      "Weights: [-4.6855  0.2636 -0.877   0.0645  0.1367]\n",
      "MSE loss: 97.5175\n",
      "Iteration: 10300\n",
      "Gradient: [ 2.4344 -3.6904 45.5651 58.2942 31.0082]\n",
      "Weights: [-4.6726  0.2717 -0.8863  0.0655  0.1372]\n",
      "MSE loss: 96.4839\n",
      "Iteration: 10400\n",
      "Gradient: [ -1.4678   5.6852   7.6157 -85.7542 119.2465]\n",
      "Weights: [-4.6721  0.2835 -0.8893  0.0644  0.1375]\n",
      "MSE loss: 96.2563\n",
      "Iteration: 10500\n",
      "Gradient: [ 10.8968  44.9467 167.0564 207.9755 339.8616]\n",
      "Weights: [-4.657   0.2849 -0.8898  0.0642  0.1381]\n",
      "MSE loss: 97.5464\n",
      "Iteration: 10600\n",
      "Gradient: [16.2008 -6.3707 12.3865 69.4743 55.9232]\n",
      "Weights: [-4.6398  0.2759 -0.8933  0.0638  0.1386]\n",
      "MSE loss: 96.9786\n",
      "Iteration: 10700\n",
      "Gradient: [  16.2752   -4.9241    3.2353   81.5402 -299.5111]\n",
      "Weights: [-4.6545  0.2989 -0.894   0.0624  0.1387]\n",
      "MSE loss: 98.2972\n",
      "Iteration: 10800\n",
      "Gradient: [  10.4967    5.5111   13.8691 -173.8365 -623.1432]\n",
      "Weights: [-4.6935  0.3156 -0.9012  0.0635  0.1386]\n",
      "MSE loss: 96.1903\n",
      "Iteration: 10900\n",
      "Gradient: [   4.4185    5.8514  -54.8272  -82.1257 -375.1   ]\n",
      "Weights: [-4.6737  0.3163 -0.9069  0.0634  0.1388]\n",
      "MSE loss: 95.7997\n",
      "Iteration: 11000\n",
      "Gradient: [ -10.1035   -1.9076  -99.4772 -204.0819  -42.3316]\n",
      "Weights: [-4.6618  0.2949 -0.9058  0.0639  0.1393]\n",
      "MSE loss: 95.6558\n",
      "Iteration: 11100\n",
      "Gradient: [ 11.1856  -4.7389  52.3196 164.1181 653.0652]\n",
      "Weights: [-4.6711  0.3008 -0.9062  0.0646  0.1393]\n",
      "MSE loss: 95.9829\n",
      "Iteration: 11200\n",
      "Gradient: [ -6.6701  63.5783  99.2209  58.8234 612.6784]\n",
      "Weights: [-4.6672  0.3145 -0.9094  0.0646  0.1393]\n",
      "MSE loss: 96.7314\n",
      "Iteration: 11300\n",
      "Gradient: [ -16.5236   25.3247   -2.3506 -183.5034   74.2259]\n",
      "Weights: [-4.6805  0.3126 -0.9119  0.0631  0.1395]\n",
      "MSE loss: 95.6695\n",
      "Iteration: 11400\n",
      "Gradient: [   6.6687   14.6855  -19.6178  -63.2937 -418.294 ]\n",
      "Weights: [-4.6645  0.3096 -0.9121  0.0633  0.1396]\n",
      "MSE loss: 95.4521\n",
      "Iteration: 11500\n",
      "Gradient: [ 30.3503   5.9376  44.3787 180.1193 163.0701]\n",
      "Weights: [-4.6531  0.3175 -0.9154  0.0627  0.14  ]\n",
      "MSE loss: 96.1394\n",
      "Iteration: 11600\n",
      "Gradient: [   9.8182   -2.7138    7.7578 -120.3633  -19.6394]\n",
      "Weights: [-4.6667  0.3308 -0.9191  0.0616  0.1404]\n",
      "MSE loss: 95.4806\n",
      "Iteration: 11700\n",
      "Gradient: [  -8.035    -2.1242   -2.8214 -191.7925 -153.577 ]\n",
      "Weights: [-4.6625  0.3198 -0.9198  0.0622  0.1405]\n",
      "MSE loss: 95.2325\n",
      "Iteration: 11800\n",
      "Gradient: [  5.2888   1.259  -32.589  -38.9461 267.3244]\n",
      "Weights: [-4.6621  0.3226 -0.9243  0.0628  0.141 ]\n",
      "MSE loss: 95.2564\n",
      "Iteration: 11900\n",
      "Gradient: [  15.7767    8.2294  -37.9004 -137.1621   39.47  ]\n",
      "Weights: [-4.6655  0.3362 -0.9289  0.0624  0.1407]\n",
      "MSE loss: 95.0683\n",
      "Iteration: 12000\n",
      "Gradient: [  -6.7668  -18.5008  -40.7976 -213.9748 -239.772 ]\n",
      "Weights: [-4.6768  0.3486 -0.9318  0.061   0.1414]\n",
      "MSE loss: 94.8066\n",
      "Iteration: 12100\n",
      "Gradient: [  -5.7157    2.7963   10.9305 -241.1001  143.6547]\n",
      "Weights: [-4.6661  0.3359 -0.9315  0.0619  0.1415]\n",
      "MSE loss: 94.9072\n",
      "Iteration: 12200\n",
      "Gradient: [ -15.097   -26.4186  -96.4606  -49.6297 -349.3396]\n",
      "Weights: [-4.6957  0.3485 -0.9351  0.0622  0.1413]\n",
      "MSE loss: 95.4328\n",
      "Iteration: 12300\n",
      "Gradient: [  3.2999   7.9827  18.1098  44.1356 611.2376]\n",
      "Weights: [-4.6841  0.3567 -0.9402  0.0622  0.1416]\n",
      "MSE loss: 94.6728\n",
      "Iteration: 12400\n",
      "Gradient: [  6.28     3.5664  45.5153 -23.7761 315.1588]\n",
      "Weights: [-4.6888  0.3749 -0.9439  0.0616  0.1421]\n",
      "MSE loss: 94.8105\n",
      "Iteration: 12500\n",
      "Gradient: [   9.0929   -3.4922  -67.8564 -129.3684   28.7666]\n",
      "Weights: [-4.6811  0.3734 -0.9483  0.0615  0.1424]\n",
      "MSE loss: 94.49\n",
      "Iteration: 12600\n",
      "Gradient: [ 10.4595 -13.609   26.0585 -14.7932 446.9542]\n",
      "Weights: [-4.6886  0.3802 -0.9501  0.0615  0.1425]\n",
      "MSE loss: 94.4456\n",
      "Iteration: 12700\n",
      "Gradient: [  2.4873  28.9543 129.2399  94.3355  11.0637]\n",
      "Weights: [-4.7027  0.3829 -0.9516  0.0611  0.1427]\n",
      "MSE loss: 94.4029\n",
      "Iteration: 12800\n",
      "Gradient: [  -4.4702    5.4399   44.3404  178.9284 -336.3132]\n",
      "Weights: [-4.6791  0.3914 -0.9539  0.0609  0.1429]\n",
      "MSE loss: 95.5604\n",
      "Iteration: 12900\n",
      "Gradient: [  5.9626 -29.4384 -93.2842 -52.4199 111.9481]\n",
      "Weights: [-4.6902  0.3845 -0.9553  0.0605  0.1432]\n",
      "MSE loss: 94.1818\n",
      "Iteration: 13000\n",
      "Gradient: [ -1.5209   2.1261  40.1113 -36.493  303.2948]\n",
      "Weights: [-4.6905  0.3878 -0.9591  0.0612  0.1432]\n",
      "MSE loss: 94.1489\n",
      "Iteration: 13100\n",
      "Gradient: [ -3.3446   6.4934  43.9436  56.4198 -87.0066]\n",
      "Weights: [-4.7133  0.3962 -0.9581  0.0603  0.1433]\n",
      "MSE loss: 94.436\n",
      "Iteration: 13200\n",
      "Gradient: [-1.890000e-02  2.786010e+01  3.737700e+00  3.538280e+01  1.262906e+02]\n",
      "Weights: [-4.711   0.4003 -0.9602  0.0603  0.1434]\n",
      "MSE loss: 94.2209\n",
      "Iteration: 13300\n",
      "Gradient: [  -3.3608  -11.777    -5.7231  -53.1611 -539.644 ]\n",
      "Weights: [-4.6966  0.3984 -0.9626  0.0604  0.1434]\n",
      "MSE loss: 94.0891\n",
      "Iteration: 13400\n",
      "Gradient: [ -12.3125  -31.0281  -52.3628  -57.4253 -397.4017]\n",
      "Weights: [-4.7125  0.411  -0.9663  0.0602  0.1436]\n",
      "MSE loss: 94.1465\n",
      "Iteration: 13500\n",
      "Gradient: [  0.9268 -24.1402  94.0769 -68.5537  61.3525]\n",
      "Weights: [-4.7139  0.4229 -0.9689  0.0597  0.1437]\n",
      "MSE loss: 93.9079\n",
      "Iteration: 13600\n",
      "Gradient: [ -6.0329  40.8934 -37.9941  46.3687 539.7652]\n",
      "Weights: [-4.7129  0.4237 -0.9718  0.0598  0.1437]\n",
      "MSE loss: 94.1421\n",
      "Iteration: 13700\n",
      "Gradient: [ -8.0929 -18.7913   4.3838  41.9971 317.022 ]\n",
      "Weights: [-4.727   0.4421 -0.9765  0.0598  0.1441]\n",
      "MSE loss: 93.7917\n",
      "Iteration: 13800\n",
      "Gradient: [  -9.2176    9.1192  -16.0334  -68.2027 -110.371 ]\n",
      "Weights: [-4.7164  0.442  -0.9794  0.0599  0.1445]\n",
      "MSE loss: 93.725\n",
      "Iteration: 13900\n",
      "Gradient: [-11.0456  17.5197  34.2577 123.5549 331.9394]\n",
      "Weights: [-4.7108  0.431  -0.982   0.0603  0.1446]\n",
      "MSE loss: 93.8515\n",
      "Iteration: 14000\n",
      "Gradient: [  -8.5346   -8.0206  -65.6583 -239.5129 -145.7327]\n",
      "Weights: [-4.7134  0.428  -0.9823  0.0606  0.1447]\n",
      "MSE loss: 94.0958\n",
      "Iteration: 14100\n",
      "Gradient: [ -10.2477  -22.4368  -53.4192   -4.989  -148.7028]\n",
      "Weights: [-4.7362  0.4448 -0.9829  0.0587  0.145 ]\n",
      "MSE loss: 94.4708\n",
      "Iteration: 14200\n",
      "Gradient: [  6.0199  20.8068  22.5199 193.2131 445.6371]\n",
      "Weights: [-4.7139  0.453  -0.9851  0.0572  0.1452]\n",
      "MSE loss: 93.5823\n",
      "Iteration: 14300\n",
      "Gradient: [  -1.5359  -13.7198  -55.6273 -216.8307 -811.0361]\n",
      "Weights: [-4.7209  0.454  -0.9852  0.0568  0.1456]\n",
      "MSE loss: 93.4778\n",
      "Iteration: 14400\n",
      "Gradient: [  16.5158  -17.723   -50.1401 -109.6044   21.2664]\n",
      "Weights: [-4.7168  0.4637 -0.9882  0.0559  0.1457]\n",
      "MSE loss: 93.5079\n",
      "Iteration: 14500\n",
      "Gradient: [ 19.3229 -10.7368 -35.6616   8.5821 117.6424]\n",
      "Weights: [-4.7256  0.4837 -0.9933  0.0548  0.146 ]\n",
      "MSE loss: 93.5573\n",
      "Iteration: 14600\n",
      "Gradient: [  6.7556  18.266   48.6568 -45.4044 182.5625]\n",
      "Weights: [-4.7263  0.4809 -0.9952  0.0564  0.146 ]\n",
      "MSE loss: 93.4339\n",
      "Iteration: 14700\n",
      "Gradient: [  6.8349  14.0666  99.679  215.1006 233.0977]\n",
      "Weights: [-4.718   0.4787 -0.9963  0.0573  0.1461]\n",
      "MSE loss: 93.85\n",
      "Iteration: 14800\n",
      "Gradient: [ 13.8363  11.5408  16.5011  32.0176 133.2683]\n",
      "Weights: [-4.6988  0.4701 -1.0001  0.0584  0.1461]\n",
      "MSE loss: 93.978\n",
      "Iteration: 14900\n",
      "Gradient: [ 15.0895   7.0655  10.9443  -9.8692 -85.6392]\n",
      "Weights: [-4.7042  0.4629 -0.9989  0.0583  0.1463]\n",
      "MSE loss: 93.4442\n",
      "Iteration: 15000\n",
      "Gradient: [  -3.1602  -23.1776  -77.4901 -110.3113 -250.764 ]\n",
      "Weights: [-4.711   0.4755 -0.9989  0.0577  0.1461]\n",
      "MSE loss: 93.6575\n",
      "Iteration: 15100\n",
      "Gradient: [  -4.2465   -2.8643  -36.5292  -17.3112 -146.8245]\n",
      "Weights: [-4.7198  0.4736 -1.0038  0.058   0.1467]\n",
      "MSE loss: 93.207\n",
      "Iteration: 15200\n",
      "Gradient: [ -2.6337 -18.2193   6.9904 -37.8916 278.084 ]\n",
      "Weights: [-4.7289  0.4818 -1.0031  0.0586  0.1464]\n",
      "MSE loss: 93.2961\n",
      "Iteration: 15300\n",
      "Gradient: [   1.7377   27.6293   35.2176  117.7814 -128.77  ]\n",
      "Weights: [-4.7295  0.4793 -1.0012  0.0582  0.1463]\n",
      "MSE loss: 93.2407\n",
      "Iteration: 15400\n",
      "Gradient: [-12.6099   6.7471  56.583  186.9192 170.9657]\n",
      "Weights: [-4.7108  0.482  -1.0036  0.0582  0.1461]\n",
      "MSE loss: 93.5632\n",
      "Iteration: 15500\n",
      "Gradient: [   7.3541   -6.7421   39.826   -51.0385 -276.1704]\n",
      "Weights: [-4.7356  0.4874 -1.0028  0.0577  0.1465]\n",
      "MSE loss: 93.2531\n",
      "Iteration: 15600\n",
      "Gradient: [ 1.631860e+01  1.651000e-01  2.083500e+00 -4.894170e+01  4.939657e+02]\n",
      "Weights: [-4.7165  0.4918 -1.0026  0.0559  0.1466]\n",
      "MSE loss: 93.8146\n",
      "Iteration: 15700\n",
      "Gradient: [  9.7691   6.9309 -16.8344 243.4163 256.3958]\n",
      "Weights: [-4.727   0.4956 -1.0069  0.0563  0.1471]\n",
      "MSE loss: 93.3755\n",
      "Iteration: 15800\n",
      "Gradient: [  17.7464    2.0506  -13.6021   91.3511 -188.4323]\n",
      "Weights: [-4.7014  0.4847 -1.0047  0.0558  0.1472]\n",
      "MSE loss: 94.3731\n",
      "Iteration: 15900\n",
      "Gradient: [ -7.5766  -1.468  -63.38   -35.7587 151.2038]\n",
      "Weights: [-4.7409  0.4917 -1.0102  0.0574  0.1472]\n",
      "MSE loss: 93.2907\n",
      "Iteration: 16000\n",
      "Gradient: [  1.1296  28.1396  43.4115 285.3397 513.275 ]\n",
      "Weights: [-4.7267  0.4967 -1.0084  0.056   0.1474]\n",
      "MSE loss: 93.388\n",
      "Iteration: 16100\n",
      "Gradient: [ 13.7278 -15.5167 -79.9484 -75.6673 -61.0235]\n",
      "Weights: [-4.7148  0.4959 -1.0157  0.0559  0.1479]\n",
      "MSE loss: 93.1691\n",
      "Iteration: 16200\n",
      "Gradient: [15.2107 15.5906 34.3055 13.6001  0.9668]\n",
      "Weights: [-4.7029  0.4968 -1.0145  0.0562  0.1478]\n",
      "MSE loss: 94.2052\n",
      "Iteration: 16300\n",
      "Gradient: [  -6.7591  -25.8722   44.2491 -176.1929  228.7342]\n",
      "Weights: [-4.722   0.5024 -1.0157  0.0564  0.1478]\n",
      "MSE loss: 93.2235\n",
      "Iteration: 16400\n",
      "Gradient: [  10.2096  -17.6621    1.5764   -9.743  -264.8094]\n",
      "Weights: [-4.7353  0.4961 -1.0157  0.0572  0.1477]\n",
      "MSE loss: 93.1759\n",
      "Iteration: 16500\n",
      "Gradient: [ -20.527   -20.1123  -47.2121 -229.8287   35.8563]\n",
      "Weights: [-4.7406  0.5094 -1.0176  0.0556  0.1479]\n",
      "MSE loss: 93.0651\n",
      "Iteration: 16600\n",
      "Gradient: [ -28.5737  -23.28    -53.2465 -164.0442  -49.6977]\n",
      "Weights: [-4.7605  0.5064 -1.0157  0.0554  0.1479]\n",
      "MSE loss: 94.435\n",
      "Iteration: 16700\n",
      "Gradient: [ -21.2027   -3.3849  -44.7837  111.03   -360.7912]\n",
      "Weights: [-4.7469  0.5017 -1.015   0.0558  0.1481]\n",
      "MSE loss: 93.3464\n",
      "Iteration: 16800\n",
      "Gradient: [   5.0924    3.0889  -33.6958  -12.9858 -421.7142]\n",
      "Weights: [-4.7303  0.5012 -1.0171  0.0549  0.1478]\n",
      "MSE loss: 93.7844\n",
      "Iteration: 16900\n",
      "Gradient: [  -2.6699   -7.2472  -84.3643  -85.4287 -187.406 ]\n",
      "Weights: [-4.736   0.5093 -1.0192  0.0552  0.1482]\n",
      "MSE loss: 93.0254\n",
      "Iteration: 17000\n",
      "Gradient: [ -5.8386   1.0539 -52.0015 -20.9608 415.8147]\n",
      "Weights: [-4.7646  0.5257 -1.0221  0.0546  0.1485]\n",
      "MSE loss: 93.5342\n",
      "Iteration: 17100\n",
      "Gradient: [ -20.9362  -42.5166  -43.243  -137.4737 -477.0125]\n",
      "Weights: [-4.7398  0.5138 -1.0234  0.0539  0.1488]\n",
      "MSE loss: 93.4311\n",
      "Iteration: 17200\n",
      "Gradient: [  24.2186   -1.55     -9.1935 -116.1665   56.0343]\n",
      "Weights: [-4.7226  0.5164 -1.0243  0.0549  0.1486]\n",
      "MSE loss: 93.0688\n",
      "Iteration: 17300\n",
      "Gradient: [ 23.307   30.577   28.055  156.6988 135.3582]\n",
      "Weights: [-4.7378  0.5177 -1.0263  0.0554  0.1487]\n",
      "MSE loss: 93.0116\n",
      "Iteration: 17400\n",
      "Gradient: [ -3.8807  -7.5797  27.1283  28.6281 -79.7934]\n",
      "Weights: [-4.7344  0.5245 -1.0233  0.0553  0.1486]\n",
      "MSE loss: 93.5387\n",
      "Iteration: 17500\n",
      "Gradient: [ 18.8794  35.2557  29.3135  -9.6129 771.2266]\n",
      "Weights: [-4.74    0.5311 -1.027   0.0551  0.1488]\n",
      "MSE loss: 93.1603\n",
      "Iteration: 17600\n",
      "Gradient: [  -8.076   -22.6019  -33.3942 -192.977  -316.9089]\n",
      "Weights: [-4.7516  0.5359 -1.0314  0.0545  0.1488]\n",
      "MSE loss: 93.3068\n",
      "Iteration: 17700\n",
      "Gradient: [-14.2914 -18.8072  60.0627 164.5727 158.0275]\n",
      "Weights: [-4.7502  0.5367 -1.028   0.0538  0.1489]\n",
      "MSE loss: 92.8687\n",
      "Iteration: 17800\n",
      "Gradient: [  5.0001  -8.2456  39.6566  68.1048 250.4533]\n",
      "Weights: [-4.7523  0.5394 -1.0277  0.0539  0.149 ]\n",
      "MSE loss: 93.0013\n",
      "Iteration: 17900\n",
      "Gradient: [  8.4601  -2.9713 112.2158  28.5836 403.071 ]\n",
      "Weights: [-4.7591  0.5436 -1.0304  0.0539  0.1492]\n",
      "MSE loss: 92.9247\n",
      "Iteration: 18000\n",
      "Gradient: [  14.4589   -4.928   -49.0204  -21.0341 -252.6349]\n",
      "Weights: [-4.7316  0.5321 -1.0299  0.0544  0.1492]\n",
      "MSE loss: 93.1756\n",
      "Iteration: 18100\n",
      "Gradient: [  -9.5407  -41.7182  -76.5818 -242.7623 -787.8738]\n",
      "Weights: [-4.7313  0.5271 -1.033   0.0553  0.149 ]\n",
      "MSE loss: 92.9335\n",
      "Iteration: 18200\n",
      "Gradient: [  -9.725   -24.7609    3.9942   -4.706  -323.673 ]\n",
      "Weights: [-4.7383  0.5261 -1.035   0.0563  0.1491]\n",
      "MSE loss: 92.9772\n",
      "Iteration: 18300\n",
      "Gradient: [ -43.8662   -2.6696    0.5813  -38.7702 -121.328 ]\n",
      "Weights: [-4.7676  0.5317 -1.0334  0.0575  0.1487]\n",
      "MSE loss: 93.8062\n",
      "Iteration: 18400\n",
      "Gradient: [ -0.5519 -19.4251  29.702   52.7775 354.11  ]\n",
      "Weights: [-4.747   0.5357 -1.0374  0.058   0.1487]\n",
      "MSE loss: 92.8047\n",
      "Iteration: 18500\n",
      "Gradient: [  3.416   -3.55    -4.132  109.1809 629.1452]\n",
      "Weights: [-4.751   0.5431 -1.0362  0.0578  0.1487]\n",
      "MSE loss: 93.085\n",
      "Iteration: 18600\n",
      "Gradient: [ -21.0637  -41.1148 -125.1051 -192.0141 -397.818 ]\n",
      "Weights: [-4.7539  0.5359 -1.0367  0.0568  0.1486]\n",
      "MSE loss: 93.6104\n",
      "Iteration: 18700\n",
      "Gradient: [  3.0744 -12.1029 -25.6153  28.6093  38.3457]\n",
      "Weights: [-4.7376  0.5367 -1.038   0.0573  0.1488]\n",
      "MSE loss: 92.7785\n",
      "Iteration: 18800\n",
      "Gradient: [   0.4537    7.6569  -81.1144 -204.1899  160.755 ]\n",
      "Weights: [-4.7312  0.5411 -1.0407  0.058   0.1488]\n",
      "MSE loss: 93.0244\n",
      "Iteration: 18900\n",
      "Gradient: [ 20.8751  -8.2575  70.7417 103.3927 198.0043]\n",
      "Weights: [-4.7258  0.5362 -1.0402  0.0581  0.1488]\n",
      "MSE loss: 93.0702\n",
      "Iteration: 19000\n",
      "Gradient: [ -20.7629   -0.6577  -26.0022 -153.3556  583.0477]\n",
      "Weights: [-4.754   0.534  -1.0411  0.0588  0.1488]\n",
      "MSE loss: 93.3526\n",
      "Iteration: 19100\n",
      "Gradient: [ -3.1823  -5.2448  37.6721  40.6362 138.9199]\n",
      "Weights: [-4.7552  0.5416 -1.0395  0.0586  0.1485]\n",
      "MSE loss: 92.9006\n",
      "Iteration: 19200\n",
      "Gradient: [ 13.2434  28.6062  43.2051  33.6143 421.245 ]\n",
      "Weights: [-4.7378  0.5419 -1.039   0.0588  0.1483]\n",
      "MSE loss: 92.9385\n",
      "Iteration: 19300\n",
      "Gradient: [  -3.0185   12.9998   23.3417   48.5121 -233.9934]\n",
      "Weights: [-4.7587  0.559  -1.0418  0.0578  0.1486]\n",
      "MSE loss: 92.8735\n",
      "Iteration: 19400\n",
      "Gradient: [ -6.6177  -5.5377 -73.1185  -6.6578   0.6777]\n",
      "Weights: [-4.765   0.5621 -1.0485  0.0582  0.1487]\n",
      "MSE loss: 93.2018\n",
      "Iteration: 19500\n",
      "Gradient: [  4.4956  20.417   51.5     94.3171 175.4413]\n",
      "Weights: [-4.7479  0.568  -1.0509  0.0583  0.149 ]\n",
      "MSE loss: 92.849\n",
      "Iteration: 19600\n",
      "Gradient: [-7.0212 11.2876 17.385  10.8357 92.5152]\n",
      "Weights: [-4.7654  0.5722 -1.0495  0.0574  0.149 ]\n",
      "MSE loss: 92.7278\n",
      "Iteration: 19700\n",
      "Gradient: [   4.2568  -23.105    -1.8315   -6.7764 -266.7492]\n",
      "Weights: [-4.7473  0.5669 -1.0521  0.0573  0.1493]\n",
      "MSE loss: 92.699\n",
      "Iteration: 19800\n",
      "Gradient: [  10.2164   18.6779  -53.5336  114.9059 -548.1298]\n",
      "Weights: [-4.7463  0.579  -1.0537  0.0567  0.1494]\n",
      "MSE loss: 93.1545\n",
      "Iteration: 19900\n",
      "Gradient: [ -17.7766   10.1391  -12.6254   47.0958 -289.0885]\n",
      "Weights: [-4.7872  0.5857 -1.0564  0.0566  0.1497]\n",
      "MSE loss: 93.3842\n",
      "Iteration: 20000\n",
      "Gradient: [ 21.9776 -13.3627   1.8013  51.425  103.1197]\n",
      "Weights: [-4.7653  0.5843 -1.0557  0.0567  0.1496]\n",
      "MSE loss: 92.6898\n",
      "Iteration: 20100\n",
      "Gradient: [  17.9169    4.7477   36.4519  251.592  1056.0907]\n",
      "Weights: [-4.7512  0.5841 -1.0535  0.0561  0.1497]\n",
      "MSE loss: 93.6138\n",
      "Iteration: 20200\n",
      "Gradient: [-19.8487  -8.9413   6.4923 -86.0761 101.5412]\n",
      "Weights: [-4.7558  0.5639 -1.0536  0.0573  0.1497]\n",
      "MSE loss: 92.7223\n",
      "Iteration: 20300\n",
      "Gradient: [  0.6867 -15.9476  17.2933 -15.1542 309.9142]\n",
      "Weights: [-4.7355  0.546  -1.0554  0.0582  0.1502]\n",
      "MSE loss: 92.8414\n",
      "Iteration: 20400\n",
      "Gradient: [ -16.5455  -26.2099  -22.725  -217.8752 -177.5199]\n",
      "Weights: [-4.7524  0.5554 -1.0527  0.0575  0.15  ]\n",
      "MSE loss: 92.7901\n",
      "Iteration: 20500\n",
      "Gradient: [ -2.2393   6.8478  30.0105 174.6057 376.0972]\n",
      "Weights: [-4.7378  0.5724 -1.0571  0.057   0.15  ]\n",
      "MSE loss: 93.1493\n",
      "Iteration: 20600\n",
      "Gradient: [ -0.9508  43.063   -3.4747  76.6926 580.4981]\n",
      "Weights: [-4.7624  0.5866 -1.057   0.0571  0.1496]\n",
      "MSE loss: 92.7675\n",
      "Iteration: 20700\n",
      "Gradient: [   1.8301   12.3726  -38.1804 -347.2262   -2.5764]\n",
      "Weights: [-4.7446  0.5744 -1.0566  0.0578  0.1494]\n",
      "MSE loss: 92.7992\n",
      "Iteration: 20800\n",
      "Gradient: [-2.702000e-01 -2.399800e+00 -9.424400e+00 -1.318008e+02 -3.782685e+02]\n",
      "Weights: [-4.7653  0.5585 -1.0536  0.0593  0.149 ]\n",
      "MSE loss: 93.7147\n",
      "Iteration: 20900\n",
      "Gradient: [-21.6843 -30.0326  -8.4258  35.1762 -83.1146]\n",
      "Weights: [-4.7484  0.5522 -1.0528  0.0597  0.149 ]\n",
      "MSE loss: 92.9019\n",
      "Iteration: 21000\n",
      "Gradient: [  -0.7791  -13.8     -80.862  -128.9143 -230.7556]\n",
      "Weights: [-4.7649  0.5611 -1.0523  0.0596  0.1492]\n",
      "MSE loss: 92.8808\n",
      "Iteration: 21100\n",
      "Gradient: [  14.5069   -8.82     58.6338  139.0294 -468.1135]\n",
      "Weights: [-4.739   0.5718 -1.0568  0.0591  0.1496]\n",
      "MSE loss: 93.7575\n",
      "Iteration: 21200\n",
      "Gradient: [ 1.335000e-01 -1.545040e+01 -6.386220e+01 -1.491510e+02 -1.613694e+02]\n",
      "Weights: [-4.7314  0.5624 -1.0579  0.0587  0.1498]\n",
      "MSE loss: 92.9993\n",
      "Iteration: 21300\n",
      "Gradient: [  -7.4071   -2.8057   17.4198 -226.1169   19.9142]\n",
      "Weights: [-4.7458  0.5543 -1.0487  0.0574  0.1497]\n",
      "MSE loss: 92.7048\n",
      "Iteration: 21400\n",
      "Gradient: [  17.7108  -15.1221  -32.0578 -111.6964 -261.445 ]\n",
      "Weights: [-4.7408  0.5647 -1.0551  0.0588  0.1495]\n",
      "MSE loss: 92.8657\n",
      "Iteration: 21500\n",
      "Gradient: [ -17.993   -10.9387  -21.7649 -168.0934 -295.5496]\n",
      "Weights: [-4.7719  0.5703 -1.0575  0.0584  0.1493]\n",
      "MSE loss: 94.0019\n",
      "Iteration: 21600\n",
      "Gradient: [ -24.6425  -32.1978  -96.2941 -158.4145 -275.9083]\n",
      "Weights: [-4.7584  0.5771 -1.0573  0.0574  0.1496]\n",
      "MSE loss: 92.6376\n",
      "Iteration: 21700\n",
      "Gradient: [  4.3743  15.9381 -13.9918  12.1533 187.0686]\n",
      "Weights: [-4.7532  0.5679 -1.0584  0.0585  0.1499]\n",
      "MSE loss: 92.6156\n",
      "Iteration: 21800\n",
      "Gradient: [ -12.3994    9.6493  -55.6637   93.5524 -359.9277]\n",
      "Weights: [-4.7512  0.5716 -1.0587  0.0574  0.1499]\n",
      "MSE loss: 92.6488\n",
      "Iteration: 21900\n",
      "Gradient: [ -20.4676   -6.9319  -74.5915  -61.5162 -763.8058]\n",
      "Weights: [-4.7593  0.5614 -1.0552  0.058   0.1498]\n",
      "MSE loss: 92.9528\n",
      "Iteration: 22000\n",
      "Gradient: [  3.0406  -1.2606  84.9861 108.7743 242.561 ]\n",
      "Weights: [-4.7618  0.5652 -1.0533  0.0582  0.1497]\n",
      "MSE loss: 92.7472\n",
      "Iteration: 22100\n",
      "Gradient: [  6.6906  -6.1101 -13.2159  99.8235 254.4206]\n",
      "Weights: [-4.7347  0.5615 -1.0559  0.0588  0.1497]\n",
      "MSE loss: 92.9753\n",
      "Iteration: 22200\n",
      "Gradient: [   2.1641   11.4339  -41.7795  121.0061 -248.969 ]\n",
      "Weights: [-4.7464  0.5625 -1.0582  0.0595  0.1495]\n",
      "MSE loss: 92.6263\n",
      "Iteration: 22300\n",
      "Gradient: [  6.2779  38.1571  52.6138 258.9532 508.956 ]\n",
      "Weights: [-4.7276  0.5533 -1.0554  0.0591  0.1499]\n",
      "MSE loss: 93.1431\n",
      "Iteration: 22400\n",
      "Gradient: [   3.3565  -19.721    59.6756   34.0131 -235.0088]\n",
      "Weights: [-4.7616  0.5679 -1.0569  0.0591  0.1495]\n",
      "MSE loss: 92.7221\n",
      "Iteration: 22500\n",
      "Gradient: [-11.4739 -27.0664 -42.3304 -42.6781 218.9412]\n",
      "Weights: [-4.7586  0.5498 -1.0529  0.0596  0.1493]\n",
      "MSE loss: 93.6119\n",
      "Iteration: 22600\n",
      "Gradient: [-16.2112   7.5446 -28.6536 -74.3529 370.3139]\n",
      "Weights: [-4.7563  0.5498 -1.049   0.0603  0.149 ]\n",
      "MSE loss: 92.8458\n",
      "Iteration: 22700\n",
      "Gradient: [  -1.1752  -14.2491  -14.2121   17.0961 -494.353 ]\n",
      "Weights: [-4.7517  0.5522 -1.0519  0.0608  0.1486]\n",
      "MSE loss: 92.8542\n",
      "Iteration: 22800\n",
      "Gradient: [-25.3673  12.182   56.6084 172.4955 -11.4683]\n",
      "Weights: [-4.7617  0.5677 -1.0543  0.0602  0.1489]\n",
      "MSE loss: 92.6725\n",
      "Iteration: 22900\n",
      "Gradient: [  4.3575  30.069   60.713    1.934  515.3684]\n",
      "Weights: [-4.7652  0.5709 -1.0574  0.0608  0.1489]\n",
      "MSE loss: 92.7145\n",
      "Iteration: 23000\n",
      "Gradient: [ 12.9636   9.5034  22.0275 127.0083 168.3872]\n",
      "Weights: [-4.7401  0.5702 -1.0586  0.061   0.1489]\n",
      "MSE loss: 92.9802\n",
      "Iteration: 23100\n",
      "Gradient: [  11.0626  -56.4751  -66.721  -265.0457 -733.7959]\n",
      "Weights: [-4.7578  0.5593 -1.0568  0.061   0.1487]\n",
      "MSE loss: 93.2921\n",
      "Iteration: 23200\n",
      "Gradient: [   6.5278  -78.5473 -139.293  -204.8623 -718.5442]\n",
      "Weights: [-4.7452  0.5698 -1.0614  0.0596  0.1491]\n",
      "MSE loss: 92.9981\n",
      "Iteration: 23300\n",
      "Gradient: [  12.7312  -24.1682  -28.1238  -17.6329 -589.4426]\n",
      "Weights: [-4.7395  0.5727 -1.0601  0.0603  0.1493]\n",
      "MSE loss: 93.1509\n",
      "Iteration: 23400\n",
      "Gradient: [-1.4796 22.8892 34.2692 12.3819 75.5333]\n",
      "Weights: [-4.7507  0.5843 -1.0618  0.06    0.1492]\n",
      "MSE loss: 92.9654\n",
      "Iteration: 23500\n",
      "Gradient: [-1.322910e+01  4.785500e+00 -4.210000e-02  9.075720e+01  4.474463e+02]\n",
      "Weights: [-4.763   0.5898 -1.0667  0.0601  0.1494]\n",
      "MSE loss: 92.5428\n",
      "Iteration: 23600\n",
      "Gradient: [ -6.9256   1.2684 -11.3511 -51.8995 238.0841]\n",
      "Weights: [-4.7443  0.5757 -1.0679  0.0616  0.1496]\n",
      "MSE loss: 92.6294\n",
      "Iteration: 23700\n",
      "Gradient: [ -12.7807    5.5413   28.6588  -11.9117 -106.5016]\n",
      "Weights: [-4.7589  0.5843 -1.0674  0.0617  0.1492]\n",
      "MSE loss: 92.5187\n",
      "Iteration: 23800\n",
      "Gradient: [-11.4457   9.222   46.1449  -1.988  107.0359]\n",
      "Weights: [-4.7506  0.5744 -1.0703  0.0631  0.1494]\n",
      "MSE loss: 92.5347\n",
      "Iteration: 23900\n",
      "Gradient: [   4.9105    3.6122  -20.7341 -157.4614 -387.6314]\n",
      "Weights: [-4.7349  0.5662 -1.0703  0.0631  0.1496]\n",
      "MSE loss: 92.6412\n",
      "Iteration: 24000\n",
      "Gradient: [-11.4613 -26.2205 -64.7593 -94.0315 -14.7202]\n",
      "Weights: [-4.761   0.5887 -1.077   0.0625  0.1498]\n",
      "MSE loss: 92.6574\n",
      "Iteration: 24100\n",
      "Gradient: [-10.1883  23.7858 -77.6082   6.1664  -7.9928]\n",
      "Weights: [-4.7645  0.594  -1.0795  0.0622  0.1501]\n",
      "MSE loss: 92.6284\n",
      "Iteration: 24200\n",
      "Gradient: [ -8.3182  26.7807  31.7005 185.0379 165.9435]\n",
      "Weights: [-4.7837  0.614  -1.0879  0.0623  0.1501]\n",
      "MSE loss: 93.6405\n",
      "Iteration: 24300\n",
      "Gradient: [-15.5257 -27.4783 -25.8026 120.0004 193.6567]\n",
      "Weights: [-4.7809  0.6085 -1.0831  0.063   0.1498]\n",
      "MSE loss: 92.7706\n",
      "Iteration: 24400\n",
      "Gradient: [  5.9598   5.4099  73.1499 -32.4735 -73.9206]\n",
      "Weights: [-4.7609  0.6159 -1.0898  0.0635  0.1501]\n",
      "MSE loss: 92.4251\n",
      "Iteration: 24500\n",
      "Gradient: [ -28.1192   -4.5087    1.982   113.8594 -207.2274]\n",
      "Weights: [-4.7747  0.6137 -1.0882  0.0637  0.15  ]\n",
      "MSE loss: 92.4897\n",
      "Iteration: 24600\n",
      "Gradient: [   3.4408  -32.051  -103.236   -62.5476  -47.0291]\n",
      "Weights: [-4.7506  0.6027 -1.0875  0.0644  0.15  ]\n",
      "MSE loss: 92.5257\n",
      "Iteration: 24700\n",
      "Gradient: [  1.1561  18.8625  47.9661  76.2093 467.9186]\n",
      "Weights: [-4.762   0.5906 -1.0866  0.0658  0.1498]\n",
      "MSE loss: 92.887\n",
      "Iteration: 24800\n",
      "Gradient: [  0.315   14.564   19.0644 -75.6119 287.0758]\n",
      "Weights: [-4.7523  0.5939 -1.0861  0.0667  0.1494]\n",
      "MSE loss: 92.4282\n",
      "Iteration: 24900\n",
      "Gradient: [-30.7071 -27.8802 -33.5584 -70.164   99.8704]\n",
      "Weights: [-4.77    0.596  -1.0879  0.0677  0.149 ]\n",
      "MSE loss: 92.961\n",
      "Iteration: 25000\n",
      "Gradient: [   7.9737   16.8145 -127.4814  -12.4897 -368.6498]\n",
      "Weights: [-4.7455  0.6029 -1.0896  0.0678  0.149 ]\n",
      "MSE loss: 92.7072\n",
      "Iteration: 25100\n",
      "Gradient: [ 15.5691  30.4094  76.8835  54.9576 199.5236]\n",
      "Weights: [-4.7449  0.603  -1.0909  0.0685  0.149 ]\n",
      "MSE loss: 92.7489\n",
      "Iteration: 25200\n",
      "Gradient: [   4.6398   10.0203    6.9836 -143.2233  102.1497]\n",
      "Weights: [-4.7572  0.6184 -1.0932  0.069   0.149 ]\n",
      "MSE loss: 93.6496\n",
      "Iteration: 25300\n",
      "Gradient: [  -2.0657   -0.6625   28.772    27.4181 -338.0603]\n",
      "Weights: [-4.7761  0.6126 -1.0959  0.069   0.1492]\n",
      "MSE loss: 92.5677\n",
      "Iteration: 25400\n",
      "Gradient: [ 17.4721  17.8126  90.6803 153.034  177.4148]\n",
      "Weights: [-4.7729  0.6165 -1.0986  0.0698  0.1494]\n",
      "MSE loss: 92.5735\n",
      "Iteration: 25500\n",
      "Gradient: [  3.2394 -15.4802  -6.1681 -21.482   32.657 ]\n",
      "Weights: [-4.7492  0.6099 -1.1005  0.0693  0.1495]\n",
      "MSE loss: 92.4227\n",
      "Iteration: 25600\n",
      "Gradient: [ 15.4392  36.3724  78.1653 179.2818 524.2034]\n",
      "Weights: [-4.758   0.6264 -1.1041  0.0692  0.1497]\n",
      "MSE loss: 92.6798\n",
      "Iteration: 25700\n",
      "Gradient: [   6.3679  -29.1199  -17.8594  -41.0513 -499.6735]\n",
      "Weights: [-4.7659  0.627  -1.1062  0.0686  0.1497]\n",
      "MSE loss: 92.3418\n",
      "Iteration: 25800\n",
      "Gradient: [   4.3692   -7.8687   13.2435   58.149  -166.2709]\n",
      "Weights: [-4.7559  0.6295 -1.1073  0.0683  0.1497]\n",
      "MSE loss: 92.4239\n",
      "Iteration: 25900\n",
      "Gradient: [  0.4396  22.2968  38.5491  16.3993 -16.7479]\n",
      "Weights: [-4.7564  0.6379 -1.1084  0.0681  0.15  ]\n",
      "MSE loss: 92.8255\n",
      "Iteration: 26000\n",
      "Gradient: [  1.7995  13.3772  -7.1048 -53.0093 -71.4028]\n",
      "Weights: [-4.7695  0.6346 -1.1075  0.0671  0.15  ]\n",
      "MSE loss: 92.4314\n",
      "Iteration: 26100\n",
      "Gradient: [ -21.9383  -16.695  -106.6687 -183.9306 -358.183 ]\n",
      "Weights: [-4.795   0.6564 -1.111   0.0666  0.1503]\n",
      "MSE loss: 92.483\n",
      "Iteration: 26200\n",
      "Gradient: [  -4.0484  -27.6959  -70.4158 -150.1228 -269.5572]\n",
      "Weights: [-4.79    0.6515 -1.1107  0.0674  0.15  ]\n",
      "MSE loss: 92.4418\n",
      "Iteration: 26300\n",
      "Gradient: [ -0.3325  18.5622  37.567  -88.8384 180.7979]\n",
      "Weights: [-4.7782  0.646  -1.1146  0.0684  0.1502]\n",
      "MSE loss: 92.3875\n",
      "Iteration: 26400\n",
      "Gradient: [  7.3009  18.7169   1.7003 -18.5357  57.6247]\n",
      "Weights: [-4.7539  0.6471 -1.1129  0.0671  0.1506]\n",
      "MSE loss: 93.3839\n",
      "Iteration: 26500\n",
      "Gradient: [-23.4775 -24.4746   7.3141 -43.1206 345.6525]\n",
      "Weights: [-4.7939  0.6414 -1.1106  0.0673  0.1502]\n",
      "MSE loss: 93.4833\n",
      "Iteration: 26600\n",
      "Gradient: [ -10.8257   29.5865   82.5013   14.7226 -366.4847]\n",
      "Weights: [-4.7759  0.643  -1.1109  0.0676  0.1502]\n",
      "MSE loss: 92.31\n",
      "Iteration: 26700\n",
      "Gradient: [  7.6899  38.5267  19.4792  43.1375 427.462 ]\n",
      "Weights: [-4.772   0.6434 -1.1123  0.0675  0.1506]\n",
      "MSE loss: 92.353\n",
      "Iteration: 26800\n",
      "Gradient: [  -0.4977  -16.0478   37.3632 -155.163  -172.4358]\n",
      "Weights: [-4.7675  0.6409 -1.1126  0.0676  0.1504]\n",
      "MSE loss: 92.2967\n",
      "Iteration: 26900\n",
      "Gradient: [ 12.7496   6.0907 -48.49   -38.6408  58.9016]\n",
      "Weights: [-4.7683  0.6413 -1.1122  0.0678  0.1503]\n",
      "MSE loss: 92.2891\n",
      "Iteration: 27000\n",
      "Gradient: [   1.11      2.2326   21.0739 -156.0319 -189.1921]\n",
      "Weights: [-4.7791  0.651  -1.1125  0.068   0.1499]\n",
      "MSE loss: 92.2882\n",
      "Iteration: 27100\n",
      "Gradient: [   3.2135    1.8593   18.0538 -209.611   121.2635]\n",
      "Weights: [-4.7624  0.6455 -1.1106  0.0679  0.15  ]\n",
      "MSE loss: 92.6535\n",
      "Iteration: 27200\n",
      "Gradient: [ -1.6999  42.9739  13.6373 125.5     -0.3246]\n",
      "Weights: [-4.7569  0.6403 -1.1124  0.0684  0.1502]\n",
      "MSE loss: 92.5535\n",
      "Iteration: 27300\n",
      "Gradient: [  13.0816  -28.8802   -9.7935 -204.9647 -890.7247]\n",
      "Weights: [-4.7612  0.6318 -1.1101  0.0683  0.1502]\n",
      "MSE loss: 92.3217\n",
      "Iteration: 27400\n",
      "Gradient: [  19.5393    5.0171  -82.5837  -18.417  -389.4281]\n",
      "Weights: [-4.7382  0.6142 -1.1031  0.0693  0.15  ]\n",
      "MSE loss: 93.4576\n",
      "Iteration: 27500\n",
      "Gradient: [ -10.9999   -7.1021 -127.9539 -167.4749    7.7464]\n",
      "Weights: [-4.7345  0.6015 -1.0996  0.069   0.1495]\n",
      "MSE loss: 92.6773\n",
      "Iteration: 27600\n",
      "Gradient: [ 22.3995  30.2059  -2.1774 342.0294  47.667 ]\n",
      "Weights: [-4.7473  0.6125 -1.1038  0.0692  0.1501]\n",
      "MSE loss: 92.7248\n",
      "Iteration: 27700\n",
      "Gradient: [ -5.5244  33.1812  28.6142 -68.2143 231.472 ]\n",
      "Weights: [-4.7523  0.6145 -1.099   0.0669  0.1498]\n",
      "MSE loss: 92.4386\n",
      "Iteration: 27800\n",
      "Gradient: [ 22.3545   3.291   81.7303  31.7373 116.186 ]\n",
      "Weights: [-4.7415  0.6058 -1.0939  0.068   0.1497]\n",
      "MSE loss: 93.384\n",
      "Iteration: 27900\n",
      "Gradient: [ -2.9391  18.9028  -7.6611   0.8403 112.4657]\n",
      "Weights: [-4.7741  0.6207 -1.0943  0.0672  0.1492]\n",
      "MSE loss: 92.3699\n",
      "Iteration: 28000\n",
      "Gradient: [   3.0038   36.8079   57.3202  157.0992 -156.02  ]\n",
      "Weights: [-4.7724  0.6259 -1.0938  0.0671  0.1492]\n",
      "MSE loss: 92.5219\n",
      "Iteration: 28100\n",
      "Gradient: [  -3.2922   32.6131   17.0565  -68.8182 -112.581 ]\n",
      "Weights: [-4.7453  0.6102 -1.0937  0.0688  0.149 ]\n",
      "MSE loss: 93.0759\n",
      "Iteration: 28200\n",
      "Gradient: [  32.8262   39.9854  106.0917  -13.1082 -279.8582]\n",
      "Weights: [-4.7323  0.5958 -1.0972  0.0701  0.1491]\n",
      "MSE loss: 92.6935\n",
      "Iteration: 28300\n",
      "Gradient: [  11.9318   15.496   -31.9315  -54.8255 -128.2033]\n",
      "Weights: [-4.7381  0.591  -1.0966  0.0708  0.1494]\n",
      "MSE loss: 92.6211\n",
      "Iteration: 28400\n",
      "Gradient: [-11.2127   9.3103   5.1426 202.8185 794.7712]\n",
      "Weights: [-4.7482  0.5955 -1.0928  0.0706  0.1489]\n",
      "MSE loss: 92.5008\n",
      "Iteration: 28500\n",
      "Gradient: [ -4.237   28.8956  21.5091 -51.2821 368.4879]\n",
      "Weights: [-4.7532  0.5993 -1.0918  0.0712  0.1484]\n",
      "MSE loss: 92.4455\n",
      "Iteration: 28600\n",
      "Gradient: [ -2.0858 -28.4219 -38.8767 -58.4001  70.2232]\n",
      "Weights: [-4.7797  0.6132 -1.0942  0.0707  0.148 ]\n",
      "MSE loss: 92.8532\n",
      "Iteration: 28700\n",
      "Gradient: [-12.1034  11.11   -17.9466  31.5419 483.3435]\n",
      "Weights: [-4.7802  0.6098 -1.0969  0.071   0.1484]\n",
      "MSE loss: 93.2103\n",
      "Iteration: 28800\n",
      "Gradient: [ -4.0262  -9.1312 -29.3061   1.7209  91.6151]\n",
      "Weights: [-4.7586  0.6113 -1.0916  0.07    0.1483]\n",
      "MSE loss: 92.586\n",
      "Iteration: 28900\n",
      "Gradient: [-13.2879  -0.9705 -16.2182 -80.7173 307.076 ]\n",
      "Weights: [-4.7787  0.6168 -1.0927  0.0695  0.1486]\n",
      "MSE loss: 92.451\n",
      "Iteration: 29000\n",
      "Gradient: [  11.556    30.9387  -14.5402 -151.2219   31.2486]\n",
      "Weights: [-4.7401  0.6057 -1.0942  0.0687  0.149 ]\n",
      "MSE loss: 92.7841\n",
      "Iteration: 29100\n",
      "Gradient: [  13.7525   19.3532  144.3343  294.9112 1066.2656]\n",
      "Weights: [-4.7529  0.6017 -1.0931  0.0703  0.1489]\n",
      "MSE loss: 92.6187\n",
      "Iteration: 29200\n",
      "Gradient: [ -2.153  -14.5601 -27.7442 -77.5426 224.8589]\n",
      "Weights: [-4.7386  0.5924 -1.0906  0.0698  0.149 ]\n",
      "MSE loss: 92.81\n",
      "Iteration: 29300\n",
      "Gradient: [-10.549  -22.2394 -41.6104 -45.5406 -93.506 ]\n",
      "Weights: [-4.7639  0.6113 -1.0938  0.0702  0.1486]\n",
      "MSE loss: 92.3825\n",
      "Iteration: 29400\n",
      "Gradient: [ -1.9707 -10.0662 -11.4081  -0.223  156.3012]\n",
      "Weights: [-4.7458  0.5947 -1.0963  0.0717  0.1486]\n",
      "MSE loss: 92.4066\n",
      "Iteration: 29500\n",
      "Gradient: [  1.7459  19.0247  22.2146 159.4877 -71.1576]\n",
      "Weights: [-4.7451  0.6078 -1.0972  0.0715  0.1487]\n",
      "MSE loss: 93.0328\n",
      "Iteration: 29600\n",
      "Gradient: [  6.294   10.4524 -21.8977 -66.6052 193.1248]\n",
      "Weights: [-4.7609  0.6156 -1.0995  0.0714  0.1488]\n",
      "MSE loss: 92.516\n",
      "Iteration: 29700\n",
      "Gradient: [-13.2789  31.7857  -3.7606 -54.6527 656.7325]\n",
      "Weights: [-4.7645  0.6127 -1.1015  0.0709  0.1488]\n",
      "MSE loss: 92.5269\n",
      "Iteration: 29800\n",
      "Gradient: [ 14.0271  11.1607 -22.7601 -74.0496 -37.962 ]\n",
      "Weights: [-4.7519  0.6301 -1.1057  0.0711  0.1488]\n",
      "MSE loss: 92.9053\n",
      "Iteration: 29900\n",
      "Gradient: [ -20.2449  -45.6148  -43.9359 -121.047  -251.8317]\n",
      "Weights: [-4.7738  0.6251 -1.1043  0.0711  0.1487]\n",
      "MSE loss: 92.4216\n",
      "Iteration: 30000\n",
      "Gradient: [  9.7612  16.2076  38.2461  20.4118 623.3288]\n",
      "Weights: [-4.7702  0.6338 -1.1036  0.0716  0.1488]\n",
      "MSE loss: 93.1374\n",
      "Iteration: 30100\n",
      "Gradient: [ -15.352   -13.5827  -20.2779  -54.6041 -730.1252]\n",
      "Weights: [-4.7727  0.625  -1.1025  0.0724  0.1481]\n",
      "MSE loss: 92.3014\n",
      "Iteration: 30200\n",
      "Gradient: [ -7.8635  23.9596  55.0497 -34.3386 118.8094]\n",
      "Weights: [-4.7653  0.619  -1.0998  0.0704  0.1488]\n",
      "MSE loss: 92.2918\n",
      "Iteration: 30300\n",
      "Gradient: [  -6.3062    7.0608  -15.6142 -313.7732 -190.1734]\n",
      "Weights: [-4.756   0.6028 -1.098   0.0709  0.1488]\n",
      "MSE loss: 92.4526\n",
      "Iteration: 30400\n",
      "Gradient: [ -8.8863  23.5078  56.4619  47.654  167.5755]\n",
      "Weights: [-4.7689  0.6029 -1.0951  0.0705  0.1487]\n",
      "MSE loss: 92.7656\n",
      "Iteration: 30500\n",
      "Gradient: [ -2.8712   3.8531 -56.4218  -8.7268 153.4678]\n",
      "Weights: [-4.7719  0.6161 -1.0991  0.072   0.1486]\n",
      "MSE loss: 92.4194\n",
      "Iteration: 30600\n",
      "Gradient: [  4.0055  15.641   21.5139 106.1148 422.0965]\n",
      "Weights: [-4.7587  0.6202 -1.1028  0.0727  0.1483]\n",
      "MSE loss: 92.3584\n",
      "Iteration: 30700\n",
      "Gradient: [  9.5529  -5.2095  -8.554  -21.6949 379.2956]\n",
      "Weights: [-4.7641  0.6271 -1.1039  0.0736  0.148 ]\n",
      "MSE loss: 92.5427\n",
      "Iteration: 30800\n",
      "Gradient: [  -6.3396   -8.4105   -1.8873   53.0132 -289.1829]\n",
      "Weights: [-4.7684  0.6102 -1.1052  0.0747  0.1479]\n",
      "MSE loss: 92.8792\n",
      "Iteration: 30900\n",
      "Gradient: [-15.5393  11.6357  37.7249 249.4391 423.5048]\n",
      "Weights: [-4.7761  0.631  -1.1054  0.0733  0.148 ]\n",
      "MSE loss: 92.2799\n",
      "Iteration: 31000\n",
      "Gradient: [-12.014   19.2545  45.6218  42.9445 212.757 ]\n",
      "Weights: [-4.7827  0.6321 -1.1049  0.0736  0.148 ]\n",
      "MSE loss: 92.3471\n",
      "Iteration: 31100\n",
      "Gradient: [ 10.4166  23.072   45.8089 206.7239 645.3798]\n",
      "Weights: [-4.7734  0.6334 -1.109   0.0745  0.1479]\n",
      "MSE loss: 92.2397\n",
      "Iteration: 31200\n",
      "Gradient: [  -8.643   -16.1745   63.8097   41.2991 -142.0814]\n",
      "Weights: [-4.7686  0.6403 -1.1099  0.0748  0.148 ]\n",
      "MSE loss: 92.917\n",
      "Iteration: 31300\n",
      "Gradient: [ 6.539  12.1864 19.3039 27.0941 76.4348]\n",
      "Weights: [-4.7635  0.631  -1.1132  0.076   0.1481]\n",
      "MSE loss: 92.2722\n",
      "Iteration: 31400\n",
      "Gradient: [-11.0373 -31.4305 -31.9978  32.2039  36.9561]\n",
      "Weights: [-4.7611  0.633  -1.1166  0.076   0.1484]\n",
      "MSE loss: 92.3215\n",
      "Iteration: 31500\n",
      "Gradient: [  14.46      3.5786   16.8329  163.4009 -223.0826]\n",
      "Weights: [-4.7682  0.6341 -1.1133  0.0761  0.148 ]\n",
      "MSE loss: 92.2586\n",
      "Iteration: 31600\n",
      "Gradient: [ -7.911  -12.6413  12.4482 -54.9278 167.5979]\n",
      "Weights: [-4.7779  0.6332 -1.1146  0.0759  0.1481]\n",
      "MSE loss: 92.3478\n",
      "Iteration: 31700\n",
      "Gradient: [ 14.4117   7.6712  31.0273 459.2759  97.1034]\n",
      "Weights: [-4.7595  0.6402 -1.1135  0.0751  0.1482]\n",
      "MSE loss: 93.0721\n",
      "Iteration: 31800\n",
      "Gradient: [   0.7319  -26.1216 -112.8137  -60.5399 -589.1129]\n",
      "Weights: [-4.7728  0.6299 -1.1184  0.0773  0.148 ]\n",
      "MSE loss: 92.4998\n",
      "Iteration: 31900\n",
      "Gradient: [  -7.3748  -57.6034   -7.6661 -100.1258  -80.955 ]\n",
      "Weights: [-4.7761  0.6281 -1.1199  0.0787  0.148 ]\n",
      "MSE loss: 92.5674\n",
      "Iteration: 32000\n",
      "Gradient: [  8.376   14.8072  40.8252  38.4074 415.7032]\n",
      "Weights: [-4.7454  0.6242 -1.1185  0.0786  0.1479]\n",
      "MSE loss: 92.579\n",
      "Iteration: 32100\n",
      "Gradient: [ 11.7096 -15.7755  65.8061  91.956   24.243 ]\n",
      "Weights: [-4.7393  0.6173 -1.121   0.0794  0.148 ]\n",
      "MSE loss: 92.4525\n",
      "Iteration: 32200\n",
      "Gradient: [-1.631930e+01 -9.070000e-02 -4.628700e+01 -4.535690e+01  2.591529e+02]\n",
      "Weights: [-4.7881  0.6425 -1.125   0.0785  0.1476]\n",
      "MSE loss: 93.8708\n",
      "Iteration: 32300\n",
      "Gradient: [ -13.9016   -7.498   -59.6017   46.6603 -429.4231]\n",
      "Weights: [-4.7919  0.6507 -1.1258  0.0784  0.1478]\n",
      "MSE loss: 92.7586\n",
      "Iteration: 32400\n",
      "Gradient: [   3.4647   -8.1605   30.4108  -46.1376 -197.3259]\n",
      "Weights: [-4.7829  0.6683 -1.1303  0.0797  0.1473]\n",
      "MSE loss: 92.234\n",
      "Iteration: 32500\n",
      "Gradient: [ 11.7914  13.7777  66.738  -68.3831 186.8178]\n",
      "Weights: [-4.7814  0.6671 -1.1275  0.0795  0.1475]\n",
      "MSE loss: 92.8094\n",
      "Iteration: 32600\n",
      "Gradient: [-8.000000e-04  4.167760e+01  4.741990e+01  3.084981e+02  4.834291e+02]\n",
      "Weights: [-4.8031  0.6684 -1.1258  0.0796  0.1473]\n",
      "MSE loss: 92.4716\n",
      "Iteration: 32700\n",
      "Gradient: [ -11.4038  -10.3626   40.1884 -132.2364   50.7797]\n",
      "Weights: [-4.7863  0.6585 -1.1305  0.081   0.1475]\n",
      "MSE loss: 92.1471\n",
      "Iteration: 32800\n",
      "Gradient: [ -0.0931 -12.8666 -53.2621 -32.7289 -52.4334]\n",
      "Weights: [-4.7556  0.6462 -1.1304  0.0813  0.1474]\n",
      "MSE loss: 92.3568\n",
      "Iteration: 32900\n",
      "Gradient: [ -13.5201  -58.5773  -79.6292 -126.3237 -558.5566]\n",
      "Weights: [-4.7952  0.6584 -1.1333  0.0819  0.1473]\n",
      "MSE loss: 92.6741\n",
      "Iteration: 33000\n",
      "Gradient: [  9.0693  19.3435 133.9643   3.2221 359.3486]\n",
      "Weights: [-4.7727  0.6658 -1.1354  0.0811  0.1477]\n",
      "MSE loss: 92.3247\n",
      "Iteration: 33100\n",
      "Gradient: [-31.5532   1.5994 -51.8342 -38.9896  99.7999]\n",
      "Weights: [-4.7785  0.654  -1.1372  0.0817  0.148 ]\n",
      "MSE loss: 92.2446\n",
      "Iteration: 33200\n",
      "Gradient: [  18.2176   20.6102  127.5023  236.8374 1053.8609]\n",
      "Weights: [-4.7885  0.6653 -1.1352  0.0821  0.1479]\n",
      "MSE loss: 92.5242\n",
      "Iteration: 33300\n",
      "Gradient: [  -1.0822  -17.6383  -47.7678 -159.3917 -543.8889]\n",
      "Weights: [-4.7784  0.6526 -1.1371  0.0826  0.1477]\n",
      "MSE loss: 92.2536\n",
      "Iteration: 33400\n",
      "Gradient: [ -21.4312  -13.7702  -34.2268   22.2714 -500.145 ]\n",
      "Weights: [-4.791   0.6603 -1.1383  0.0818  0.1478]\n",
      "MSE loss: 92.6456\n",
      "Iteration: 33500\n",
      "Gradient: [  -5.7896  -12.8543  -42.1291 -128.3265  307.2875]\n",
      "Weights: [-4.7679  0.6646 -1.138   0.0818  0.1478]\n",
      "MSE loss: 92.4463\n",
      "Iteration: 33600\n",
      "Gradient: [  5.4475  36.6579  61.3732 191.9709 533.3796]\n",
      "Weights: [-4.7801  0.6683 -1.1377  0.0825  0.1476]\n",
      "MSE loss: 92.2775\n",
      "Iteration: 33700\n",
      "Gradient: [   5.3119  -35.2267   58.8727 -121.1075  227.9085]\n",
      "Weights: [-4.7673  0.6421 -1.1379  0.083   0.148 ]\n",
      "MSE loss: 92.3034\n",
      "Iteration: 33800\n",
      "Gradient: [  -2.5241   -6.2059  -20.5688 -122.478  -200.6425]\n",
      "Weights: [-4.7624  0.6461 -1.1399  0.0837  0.1478]\n",
      "MSE loss: 92.1266\n",
      "Iteration: 33900\n",
      "Gradient: [  3.8887  22.0799 -28.3453 -89.1433 319.166 ]\n",
      "Weights: [-4.7665  0.6546 -1.1403  0.0838  0.1478]\n",
      "MSE loss: 92.1484\n",
      "Iteration: 34000\n",
      "Gradient: [  -2.1329  -16.6019   55.6158 -137.6541 -430.9012]\n",
      "Weights: [-4.7537  0.6371 -1.138   0.0838  0.1475]\n",
      "MSE loss: 92.444\n",
      "Iteration: 34100\n",
      "Gradient: [  -4.3223   12.6175  -69.5769   58.5636 -514.3262]\n",
      "Weights: [-4.7609  0.643  -1.1334  0.0831  0.1473]\n",
      "MSE loss: 92.1057\n",
      "Iteration: 34200\n",
      "Gradient: [ 21.5675  24.814   37.5897 218.1779 470.9233]\n",
      "Weights: [-4.7554  0.6497 -1.1389  0.0843  0.1475]\n",
      "MSE loss: 92.3465\n",
      "Iteration: 34300\n",
      "Gradient: [ -17.703   -13.6746    0.907  -134.5252 -253.5126]\n",
      "Weights: [-4.774   0.6419 -1.1374  0.0847  0.1471]\n",
      "MSE loss: 92.7694\n",
      "Iteration: 34400\n",
      "Gradient: [  -4.3566  -30.4391   12.1962   68.3994 -141.5549]\n",
      "Weights: [-4.7827  0.6486 -1.1373  0.0847  0.1469]\n",
      "MSE loss: 92.8357\n",
      "Iteration: 34500\n",
      "Gradient: [  1.0374 -13.2283 -50.9073  26.0292 -55.018 ]\n",
      "Weights: [-4.7772  0.6552 -1.1342  0.0836  0.1472]\n",
      "MSE loss: 92.145\n",
      "Iteration: 34600\n",
      "Gradient: [ 13.8438  30.0533  35.3678 -95.349   31.4311]\n",
      "Weights: [-4.7695  0.6523 -1.1338  0.0836  0.1468]\n",
      "MSE loss: 92.0873\n",
      "Iteration: 34700\n",
      "Gradient: [  -9.6601  -18.9215   -8.9635  -41.5331 -154.127 ]\n",
      "Weights: [-4.7877  0.6615 -1.1328  0.0828  0.147 ]\n",
      "MSE loss: 92.1281\n",
      "Iteration: 34800\n",
      "Gradient: [   4.4105  -64.7444  -28.1096 -181.5161 -677.8968]\n",
      "Weights: [-4.7717  0.6548 -1.132   0.0817  0.1472]\n",
      "MSE loss: 92.1063\n",
      "Iteration: 34900\n",
      "Gradient: [-20.3146 -13.7724 -64.6774 -33.0945  99.1521]\n",
      "Weights: [-4.7615  0.6455 -1.1332  0.0828  0.1472]\n",
      "MSE loss: 92.1567\n",
      "Iteration: 35000\n",
      "Gradient: [  -3.042   -57.1515  -70.7259 -128.376  -450.7411]\n",
      "Weights: [-4.7666  0.6356 -1.1319  0.0837  0.1471]\n",
      "MSE loss: 92.3403\n",
      "Iteration: 35100\n",
      "Gradient: [ -6.5543  -4.391  -30.5855  56.136   -9.9079]\n",
      "Weights: [-4.7928  0.6497 -1.13    0.083   0.1469]\n",
      "MSE loss: 92.6407\n",
      "Iteration: 35200\n",
      "Gradient: [   5.001    -8.3399   44.8677 -139.9532  614.1509]\n",
      "Weights: [-4.7758  0.6528 -1.133   0.0839  0.1466]\n",
      "MSE loss: 92.1049\n",
      "Iteration: 35300\n",
      "Gradient: [ -15.6586   -7.8921    5.0527 -160.4729 -106.9576]\n",
      "Weights: [-4.7831  0.6451 -1.1327  0.0856  0.1463]\n",
      "MSE loss: 92.4872\n",
      "Iteration: 35400\n",
      "Gradient: [   1.7631  -31.3562   25.6257   43.888  -420.7844]\n",
      "Weights: [-4.7602  0.6473 -1.1342  0.0871  0.1458]\n",
      "MSE loss: 92.237\n",
      "Iteration: 35500\n",
      "Gradient: [ -12.4504   -1.2711    8.1751 -116.9471  -13.7118]\n",
      "Weights: [-4.7704  0.6591 -1.1368  0.0871  0.1461]\n",
      "MSE loss: 92.476\n",
      "Iteration: 35600\n",
      "Gradient: [   2.3433  -37.1659  -29.085  -114.4292   37.1967]\n",
      "Weights: [-4.7563  0.6556 -1.1419  0.0881  0.1461]\n",
      "MSE loss: 92.4215\n",
      "Iteration: 35700\n",
      "Gradient: [  5.6585  -2.9645 -29.0403 -32.3096 -28.7547]\n",
      "Weights: [-4.7654  0.6514 -1.1395  0.0886  0.1459]\n",
      "MSE loss: 92.0665\n",
      "Iteration: 35800\n",
      "Gradient: [  -0.2284  -11.3257  -31.6908  -87.71   -173.65  ]\n",
      "Weights: [-4.7744  0.6624 -1.142   0.0888  0.1458]\n",
      "MSE loss: 92.0534\n",
      "Iteration: 35900\n",
      "Gradient: [-30.6527 -23.4092   0.9936 -97.6422 156.8472]\n",
      "Weights: [-4.8041  0.6592 -1.1407  0.0894  0.1455]\n",
      "MSE loss: 93.3078\n",
      "Iteration: 36000\n",
      "Gradient: [  -3.19     -8.8608  -55.9488 -146.194  -121.2612]\n",
      "Weights: [-4.773   0.6427 -1.1372  0.0885  0.1459]\n",
      "MSE loss: 92.1446\n",
      "Iteration: 36100\n",
      "Gradient: [ -1.0371  -7.9412  -9.8354 -92.0465 122.9484]\n",
      "Weights: [-4.7544  0.6382 -1.1371  0.0893  0.1458]\n",
      "MSE loss: 92.1555\n",
      "Iteration: 36200\n",
      "Gradient: [  9.7452  59.4241  87.3449 188.6345 315.3316]\n",
      "Weights: [-4.7514  0.6325 -1.14    0.0902  0.1459]\n",
      "MSE loss: 92.1154\n",
      "Iteration: 36300\n",
      "Gradient: [   5.271    27.4583   16.2133   90.5712 -206.3991]\n",
      "Weights: [-4.7526  0.6414 -1.142   0.0907  0.1458]\n",
      "MSE loss: 92.1699\n",
      "Iteration: 36400\n",
      "Gradient: [ -15.9031   -6.3272    6.7733 -294.8831   53.7821]\n",
      "Weights: [-4.7718  0.6354 -1.1425  0.0913  0.1459]\n",
      "MSE loss: 92.4874\n",
      "Iteration: 36500\n",
      "Gradient: [  4.4365   4.7199  20.9104  20.7584 154.4072]\n",
      "Weights: [-4.7673  0.6449 -1.1457  0.0921  0.1458]\n",
      "MSE loss: 91.9958\n",
      "Iteration: 36600\n",
      "Gradient: [ -2.0641  -3.7624  -3.2826 -81.4239 222.8445]\n",
      "Weights: [-4.7727  0.6463 -1.1484  0.0916  0.1459]\n",
      "MSE loss: 92.4586\n",
      "Iteration: 36700\n",
      "Gradient: [  -4.9775  -16.5566  -10.9037 -186.1328 -184.9859]\n",
      "Weights: [-4.744   0.6419 -1.1489  0.093   0.146 ]\n",
      "MSE loss: 92.5639\n",
      "Iteration: 36800\n",
      "Gradient: [  3.2358  24.8395  11.6667 129.172  563.0129]\n",
      "Weights: [-4.7654  0.6401 -1.1471  0.0927  0.1457]\n",
      "MSE loss: 92.1841\n",
      "Iteration: 36900\n",
      "Gradient: [ -1.9672   5.8506  12.2492  14.209  214.0005]\n",
      "Weights: [-4.7657  0.653  -1.1472  0.0929  0.1453]\n",
      "MSE loss: 91.9748\n",
      "Iteration: 37000\n",
      "Gradient: [   8.7214   11.1508  -30.3202 -232.8128 -175.6492]\n",
      "Weights: [-4.7974  0.6693 -1.1488  0.0925  0.1451]\n",
      "MSE loss: 92.325\n",
      "Iteration: 37100\n",
      "Gradient: [  11.8768    3.6252  -37.6393   73.2478 -299.6005]\n",
      "Weights: [-4.7816  0.6617 -1.1506  0.0941  0.1452]\n",
      "MSE loss: 91.9447\n",
      "Iteration: 37200\n",
      "Gradient: [   1.0201    1.073    57.5713 -129.8356   46.5234]\n",
      "Weights: [-4.7718  0.6594 -1.1531  0.0942  0.1452]\n",
      "MSE loss: 91.9199\n",
      "Iteration: 37300\n",
      "Gradient: [  7.4069   2.9833 -16.837  417.2262 146.4338]\n",
      "Weights: [-4.7707  0.6533 -1.1525  0.0943  0.1457]\n",
      "MSE loss: 92.01\n",
      "Iteration: 37400\n",
      "Gradient: [  8.7746 -37.1187 -28.875  -42.2855 205.0671]\n",
      "Weights: [-4.7669  0.6527 -1.1547  0.0933  0.146 ]\n",
      "MSE loss: 92.0149\n",
      "Iteration: 37500\n",
      "Gradient: [  14.6035   42.139    15.5878  184.2903 -159.1653]\n",
      "Weights: [-4.7754  0.6715 -1.1545  0.0929  0.146 ]\n",
      "MSE loss: 92.505\n",
      "Iteration: 37600\n",
      "Gradient: [  7.8074   8.1592  50.6606 128.3546 450.1593]\n",
      "Weights: [-4.7641  0.6635 -1.1575  0.0943  0.1458]\n",
      "MSE loss: 92.0456\n",
      "Iteration: 37700\n",
      "Gradient: [-11.0894  16.5395  85.7117 116.0508 373.4845]\n",
      "Weights: [-4.7813  0.6714 -1.1581  0.0942  0.1457]\n",
      "MSE loss: 91.9105\n",
      "Iteration: 37800\n",
      "Gradient: [-32.4586  44.8867   3.5178 -71.4624 106.6061]\n",
      "Weights: [-4.7959  0.6707 -1.1552  0.0944  0.1455]\n",
      "MSE loss: 92.2122\n",
      "Iteration: 37900\n",
      "Gradient: [ -8.5061  -3.7025  -3.4694  -6.1456 -97.4626]\n",
      "Weights: [-4.7797  0.6644 -1.1558  0.094   0.1452]\n",
      "MSE loss: 92.2816\n",
      "Iteration: 38000\n",
      "Gradient: [-20.9268  29.2989  42.934  100.3061 229.1873]\n",
      "Weights: [-4.7926  0.6634 -1.1532  0.0941  0.1452]\n",
      "MSE loss: 92.5644\n",
      "Iteration: 38100\n",
      "Gradient: [  10.7399   -7.8066   -5.8219  -95.0485 -117.3362]\n",
      "Weights: [-4.7618  0.6622 -1.1562  0.0934  0.1455]\n",
      "MSE loss: 92.0385\n",
      "Iteration: 38200\n",
      "Gradient: [ 10.0267  19.791  -39.8037 132.2341 302.2887]\n",
      "Weights: [-4.7401  0.649  -1.1535  0.0944  0.1458]\n",
      "MSE loss: 92.9793\n",
      "Iteration: 38300\n",
      "Gradient: [ 10.4068  12.0284 -28.6035  64.232   36.9471]\n",
      "Weights: [-4.7661  0.6606 -1.1553  0.0933  0.1459]\n",
      "MSE loss: 91.9369\n",
      "Iteration: 38400\n",
      "Gradient: [ -21.6133  -22.3859 -149.4531 -110.3371 -104.0573]\n",
      "Weights: [-4.7596  0.6356 -1.1548  0.0939  0.1459]\n",
      "MSE loss: 93.4911\n",
      "Iteration: 38500\n",
      "Gradient: [  15.1266  -36.4742   -3.5918 -156.4479 -352.3431]\n",
      "Weights: [-4.7577  0.6547 -1.1549  0.0948  0.1453]\n",
      "MSE loss: 91.9902\n",
      "Iteration: 38600\n",
      "Gradient: [  8.4864   8.4102 -12.8905 -67.4013  30.056 ]\n",
      "Weights: [-4.7609  0.6538 -1.1522  0.0952  0.1453]\n",
      "MSE loss: 92.2154\n",
      "Iteration: 38700\n",
      "Gradient: [  -9.4319    2.9841   34.0234    7.3731 -440.5066]\n",
      "Weights: [-4.77    0.66   -1.1512  0.0934  0.1453]\n",
      "MSE loss: 91.9162\n",
      "Iteration: 38800\n",
      "Gradient: [ 1.84000e-02 -2.71642e+01  7.09130e+00  4.84589e+01 -8.05321e+01]\n",
      "Weights: [-4.7664  0.6663 -1.1546  0.0936  0.1454]\n",
      "MSE loss: 92.0447\n",
      "Iteration: 38900\n",
      "Gradient: [ -16.1611    9.8557   -9.6175   -7.1341 -508.368 ]\n",
      "Weights: [-4.7714  0.6613 -1.1555  0.0946  0.1457]\n",
      "MSE loss: 91.9945\n",
      "Iteration: 39000\n",
      "Gradient: [  13.5062    1.9331  -76.7763 -157.8853 -201.2506]\n",
      "Weights: [-4.7546  0.6567 -1.1559  0.0939  0.1458]\n",
      "MSE loss: 92.1584\n",
      "Iteration: 39100\n",
      "Gradient: [  4.5684  13.3967 -22.8602 127.7709  13.2947]\n",
      "Weights: [-4.765   0.6467 -1.1512  0.0952  0.1454]\n",
      "MSE loss: 91.9741\n",
      "Iteration: 39200\n",
      "Gradient: [   4.4176   -5.5364   49.1075  -79.4678 -267.379 ]\n",
      "Weights: [-4.7616  0.6424 -1.1489  0.0955  0.1451]\n",
      "MSE loss: 91.9763\n",
      "Iteration: 39300\n",
      "Gradient: [  0.5867 -22.5258   4.1782 -96.8845 269.0333]\n",
      "Weights: [-4.7712  0.6415 -1.1479  0.0949  0.1448]\n",
      "MSE loss: 92.5138\n",
      "Iteration: 39400\n",
      "Gradient: [ 13.335   -2.3612  63.0474  87.531  -45.7727]\n",
      "Weights: [-4.7724  0.6458 -1.1467  0.0945  0.1448]\n",
      "MSE loss: 92.0722\n",
      "Iteration: 39500\n",
      "Gradient: [  11.0127    6.4947   25.4605    5.8128 -357.038 ]\n",
      "Weights: [-4.7419  0.6483 -1.1522  0.0959  0.1449]\n",
      "MSE loss: 92.7797\n",
      "Iteration: 39600\n",
      "Gradient: [  9.6302 -46.9318 -34.3527   8.1084 120.5436]\n",
      "Weights: [-4.7687  0.6501 -1.1512  0.0964  0.1446]\n",
      "MSE loss: 91.9054\n",
      "Iteration: 39700\n",
      "Gradient: [-20.9976   1.7979  -0.236   38.0052 -37.7372]\n",
      "Weights: [-4.7673  0.6454 -1.15    0.0973  0.1444]\n",
      "MSE loss: 91.9009\n",
      "Iteration: 39800\n",
      "Gradient: [   3.3525  -17.1534   37.4927 -175.9849  182.0598]\n",
      "Weights: [-4.7476  0.6358 -1.1476  0.0982  0.1442]\n",
      "MSE loss: 92.2619\n",
      "Iteration: 39900\n",
      "Gradient: [  21.5576    3.1806  -56.3293 -162.3979  -73.944 ]\n",
      "Weights: [-4.736   0.6291 -1.146   0.0981  0.1441]\n",
      "MSE loss: 92.6082\n",
      "Iteration: 40000\n",
      "Gradient: [  1.6293   3.3971  64.0275 172.2194  32.3761]\n",
      "Weights: [-4.7726  0.6395 -1.1486  0.0987  0.144 ]\n",
      "MSE loss: 92.0527\n",
      "Iteration: 40100\n",
      "Gradient: [ -5.6866   6.5603  36.1182  95.6177 392.258 ]\n",
      "Weights: [-4.7621  0.6428 -1.1491  0.0983  0.1443]\n",
      "MSE loss: 92.0843\n",
      "Iteration: 40200\n",
      "Gradient: [  12.9865    4.5926   -9.7454  -67.8706 -131.73  ]\n",
      "Weights: [-4.7571  0.6374 -1.1507  0.0988  0.1439]\n",
      "MSE loss: 92.0454\n",
      "Iteration: 40300\n",
      "Gradient: [-10.8793 -14.1304 -51.3989 -12.7548 110.1738]\n",
      "Weights: [-4.7782  0.648  -1.1518  0.098   0.1441]\n",
      "MSE loss: 92.3387\n",
      "Iteration: 40400\n",
      "Gradient: [  30.6595   -8.8258  -58.3875  -42.0214 -312.8141]\n",
      "Weights: [-4.7467  0.6482 -1.1526  0.0993  0.144 ]\n",
      "MSE loss: 92.9242\n",
      "Iteration: 40500\n",
      "Gradient: [-12.804    1.3026 -11.7026  17.0929 397.4197]\n",
      "Weights: [-4.7598  0.6469 -1.1536  0.0976  0.1444]\n",
      "MSE loss: 91.9264\n",
      "Iteration: 40600\n",
      "Gradient: [  7.0739  15.4636  65.7796 106.9106 140.67  ]\n",
      "Weights: [-4.7677  0.6545 -1.1575  0.098   0.1448]\n",
      "MSE loss: 91.8715\n",
      "Iteration: 40700\n",
      "Gradient: [  -1.9766  -30.1193  -54.1439  -77.55   -327.24  ]\n",
      "Weights: [-4.7686  0.6609 -1.1635  0.0988  0.1447]\n",
      "MSE loss: 92.0005\n",
      "Iteration: 40800\n",
      "Gradient: [-11.5407   6.1675  39.4005  89.5259 795.9859]\n",
      "Weights: [-4.7785  0.6553 -1.1582  0.0988  0.1446]\n",
      "MSE loss: 92.0391\n",
      "Iteration: 40900\n",
      "Gradient: [ -6.734    8.8726  -5.4414  25.586  215.4237]\n",
      "Weights: [-4.7982  0.6845 -1.163   0.098   0.1442]\n",
      "MSE loss: 92.2021\n",
      "Iteration: 41000\n",
      "Gradient: [   4.0538   -2.9261   -7.5959  135.6506 -237.7841]\n",
      "Weights: [-4.7711  0.6863 -1.1659  0.0986  0.1444]\n",
      "MSE loss: 92.4461\n",
      "Iteration: 41100\n",
      "Gradient: [   4.243     4.1136    0.5583 -101.3124   25.0123]\n",
      "Weights: [-4.77    0.6687 -1.1668  0.0997  0.1446]\n",
      "MSE loss: 91.8293\n",
      "Iteration: 41200\n",
      "Gradient: [  2.0762 -22.8471   5.7452 -47.9427 288.6897]\n",
      "Weights: [-4.7825  0.6753 -1.1683  0.1001  0.1446]\n",
      "MSE loss: 91.8347\n",
      "Iteration: 41300\n",
      "Gradient: [  3.1555   7.4708  11.8905  71.2601 339.4626]\n",
      "Weights: [-4.7939  0.6941 -1.1733  0.0997  0.1446]\n",
      "MSE loss: 91.8619\n",
      "Iteration: 41400\n",
      "Gradient: [-14.4829 -24.6418 -19.2439 -79.1871 -15.769 ]\n",
      "Weights: [-4.792   0.6912 -1.1785  0.1012  0.1446]\n",
      "MSE loss: 92.1534\n",
      "Iteration: 41500\n",
      "Gradient: [  17.1319   10.3565  -38.0266   42.9177 -280.0532]\n",
      "Weights: [-4.7747  0.684  -1.1759  0.1018  0.1446]\n",
      "MSE loss: 91.7867\n",
      "Iteration: 41600\n",
      "Gradient: [-6.7763 28.6262 31.4463 24.628  -9.2911]\n",
      "Weights: [-4.7873  0.6853 -1.175   0.1015  0.1447]\n",
      "MSE loss: 91.8458\n",
      "Iteration: 41700\n",
      "Gradient: [ -5.7103  11.0666  31.344   83.4724 282.6375]\n",
      "Weights: [-4.8091  0.6916 -1.1764  0.1022  0.1445]\n",
      "MSE loss: 92.5623\n",
      "Iteration: 41800\n",
      "Gradient: [ 10.8737  15.4224  59.7296 160.7657 -15.7354]\n",
      "Weights: [-4.7773  0.6908 -1.1797  0.1026  0.1446]\n",
      "MSE loss: 91.7981\n",
      "Iteration: 41900\n",
      "Gradient: [ 15.162  -13.463   45.1734 130.758  266.1261]\n",
      "Weights: [-4.7816  0.6939 -1.1804  0.1025  0.1445]\n",
      "MSE loss: 91.7345\n",
      "Iteration: 42000\n",
      "Gradient: [-15.5671  -2.0433  46.4779 -25.9881 234.275 ]\n",
      "Weights: [-4.791   0.6835 -1.1756  0.1022  0.1444]\n",
      "MSE loss: 92.0054\n",
      "Iteration: 42100\n",
      "Gradient: [ -19.0226  -26.4943  -92.4957   -1.7369 -789.7404]\n",
      "Weights: [-4.7922  0.6869 -1.1812  0.1029  0.1444]\n",
      "MSE loss: 92.5693\n",
      "Iteration: 42200\n",
      "Gradient: [ -9.3076 -40.3817 -39.3205 122.6431  18.5789]\n",
      "Weights: [-4.787   0.6829 -1.1786  0.1028  0.1446]\n",
      "MSE loss: 91.9836\n",
      "Iteration: 42300\n",
      "Gradient: [-23.3826 -13.9254  -8.2929 -88.531  138.9839]\n",
      "Weights: [-4.7708  0.6807 -1.1746  0.1017  0.1447]\n",
      "MSE loss: 91.9143\n",
      "Iteration: 42400\n",
      "Gradient: [ 10.1545 -15.1503 -31.5345 -48.8932 294.1952]\n",
      "Weights: [-4.7481  0.6657 -1.1761  0.1019  0.1449]\n",
      "MSE loss: 92.096\n",
      "Iteration: 42500\n",
      "Gradient: [  11.2439   46.587   -23.0571   30.6584 -215.1624]\n",
      "Weights: [-4.756   0.6702 -1.1736  0.1026  0.1447]\n",
      "MSE loss: 92.3838\n",
      "Iteration: 42600\n",
      "Gradient: [  3.5153 -21.1167  44.9909  34.9472 132.2349]\n",
      "Weights: [-4.7844  0.6781 -1.1733  0.1019  0.1444]\n",
      "MSE loss: 91.9023\n",
      "Iteration: 42700\n",
      "Gradient: [  -8.0713  -15.1097  -24.6767   72.2815 -335.5029]\n",
      "Weights: [-4.7857  0.6785 -1.1768  0.1037  0.1444]\n",
      "MSE loss: 91.912\n",
      "Iteration: 42800\n",
      "Gradient: [ -5.0384  12.2501 -52.157  -12.2314  32.9664]\n",
      "Weights: [-4.7771  0.6828 -1.1781  0.1025  0.1442]\n",
      "MSE loss: 91.9684\n",
      "Iteration: 42900\n",
      "Gradient: [  -6.0792   22.1219  -61.7317  -28.118  -237.4524]\n",
      "Weights: [-4.7931  0.6916 -1.177   0.102   0.1442]\n",
      "MSE loss: 91.9484\n",
      "Iteration: 43000\n",
      "Gradient: [2.635000e-01 9.645000e+00 6.809140e+01 5.140240e+01 4.787214e+02]\n",
      "Weights: [-4.7793  0.6829 -1.172   0.1019  0.1443]\n",
      "MSE loss: 91.8663\n",
      "Iteration: 43100\n",
      "Gradient: [  9.4341   5.6866  67.9533  93.7168 492.6781]\n",
      "Weights: [-4.7757  0.6965 -1.1751  0.1022  0.1443]\n",
      "MSE loss: 93.1663\n",
      "Iteration: 43200\n",
      "Gradient: [   0.851   -17.5005  -18.8777  -29.9733 -435.2745]\n",
      "Weights: [-4.769   0.6723 -1.1762  0.1033  0.1444]\n",
      "MSE loss: 91.783\n",
      "Iteration: 43300\n",
      "Gradient: [-22.5263  -4.3032 -48.5074  23.7822   2.09  ]\n",
      "Weights: [-4.772   0.6773 -1.176   0.1034  0.1441]\n",
      "MSE loss: 91.7459\n",
      "Iteration: 43400\n",
      "Gradient: [   9.9491  -46.1967   -8.3917   16.3006 -303.7718]\n",
      "Weights: [-4.7708  0.6835 -1.1764  0.1037  0.144 ]\n",
      "MSE loss: 91.9025\n",
      "Iteration: 43500\n",
      "Gradient: [ -7.8625  14.7837  40.8592  43.4439 752.9725]\n",
      "Weights: [-4.7695  0.672  -1.1755  0.1047  0.1437]\n",
      "MSE loss: 91.7692\n",
      "Iteration: 43600\n",
      "Gradient: [  6.2769   4.7176 -16.323   18.0673 -27.6744]\n",
      "Weights: [-4.7638  0.6693 -1.1745  0.1053  0.1439]\n",
      "MSE loss: 92.0756\n",
      "Iteration: 43700\n",
      "Gradient: [  9.309  -12.6873   1.8391 165.9095 160.0394]\n",
      "Weights: [-4.7729  0.6829 -1.1743  0.105   0.1436]\n",
      "MSE loss: 92.4691\n",
      "Iteration: 43800\n",
      "Gradient: [  23.8526   23.4192  -64.3538  -96.0991 -121.7133]\n",
      "Weights: [-4.7448  0.6738 -1.1782  0.105   0.1439]\n",
      "MSE loss: 92.8105\n",
      "Iteration: 43900\n",
      "Gradient: [ 22.914   31.8539  31.1121  50.8252 566.9188]\n",
      "Weights: [-4.7605  0.6874 -1.1783  0.1056  0.1436]\n",
      "MSE loss: 93.2573\n",
      "Iteration: 44000\n",
      "Gradient: [ 13.4488  56.0587  29.5225  45.0821 734.2264]\n",
      "Weights: [-4.7601  0.6847 -1.1764  0.104   0.1439]\n",
      "MSE loss: 92.7509\n",
      "Iteration: 44100\n",
      "Gradient: [  -5.175     9.4901  -34.4567 -102.11    -82.8258]\n",
      "Weights: [-4.7783  0.6788 -1.1755  0.1046  0.1439]\n",
      "MSE loss: 91.8064\n",
      "Iteration: 44200\n",
      "Gradient: [  -5.7827   12.2184  -16.6424  -46.5971 -103.7205]\n",
      "Weights: [-4.7724  0.6779 -1.1761  0.1044  0.1438]\n",
      "MSE loss: 91.7328\n",
      "Iteration: 44300\n",
      "Gradient: [  -5.9882    3.5404 -107.3873  -65.6442 -424.888 ]\n",
      "Weights: [-4.7705  0.6844 -1.1792  0.1034  0.144 ]\n",
      "MSE loss: 91.8459\n",
      "Iteration: 44400\n",
      "Gradient: [  6.0057  31.3422 -65.1326  92.4074 -72.5408]\n",
      "Weights: [-4.7694  0.6821 -1.1758  0.1047  0.1436]\n",
      "MSE loss: 91.9644\n",
      "Iteration: 44500\n",
      "Gradient: [ 20.851   52.3498  53.2679 147.4846 272.128 ]\n",
      "Weights: [-4.7569  0.6799 -1.1779  0.1062  0.1436]\n",
      "MSE loss: 92.9231\n",
      "Iteration: 44600\n",
      "Gradient: [ -13.3266   -9.476    26.2149   75.6795 -151.4395]\n",
      "Weights: [-4.8016  0.6849 -1.1782  0.1062  0.1433]\n",
      "MSE loss: 92.4747\n",
      "Iteration: 44700\n",
      "Gradient: [   2.6384   15.1727  -11.0908  -55.7968 -719.9249]\n",
      "Weights: [-4.7843  0.6889 -1.1782  0.1057  0.1431]\n",
      "MSE loss: 91.7807\n",
      "Iteration: 44800\n",
      "Gradient: [-18.6156  -1.8919  75.0269 -40.8428 -41.6208]\n",
      "Weights: [-4.7903  0.6937 -1.1834  0.1067  0.1435]\n",
      "MSE loss: 91.7385\n",
      "Iteration: 44900\n",
      "Gradient: [  -1.4913  -32.6662  -50.8867  -29.3672 -233.3937]\n",
      "Weights: [-4.7752  0.6944 -1.1837  0.1066  0.1434]\n",
      "MSE loss: 91.894\n",
      "Iteration: 45000\n",
      "Gradient: [ 15.499   31.2108  44.7963   4.0018 174.6032]\n",
      "Weights: [-4.7778  0.6961 -1.1829  0.1064  0.1435]\n",
      "MSE loss: 92.0324\n",
      "Iteration: 45100\n",
      "Gradient: [ -2.3178 -17.518   98.0165  93.3533 623.9495]\n",
      "Weights: [-4.7738  0.6889 -1.1832  0.1078  0.1434]\n",
      "MSE loss: 92.0488\n",
      "Iteration: 45200\n",
      "Gradient: [ -11.0448   -7.3504  -28.0072 -138.6081 -612.8097]\n",
      "Weights: [-4.7842  0.6897 -1.1877  0.1081  0.1434]\n",
      "MSE loss: 91.8397\n",
      "Iteration: 45300\n",
      "Gradient: [ -4.9387   2.1844 -44.0117 -15.9976  16.255 ]\n",
      "Weights: [-4.791   0.6934 -1.1869  0.1092  0.1429]\n",
      "MSE loss: 91.7919\n",
      "Iteration: 45400\n",
      "Gradient: [ 20.2281  -3.4146 -16.5621 -63.3505 -17.9184]\n",
      "Weights: [-4.779   0.6893 -1.1852  0.1087  0.143 ]\n",
      "MSE loss: 91.6599\n",
      "Iteration: 45500\n",
      "Gradient: [ 15.6762 -18.5096  -2.1883 -20.9048 393.6799]\n",
      "Weights: [-4.756   0.679  -1.1871  0.1094  0.1433]\n",
      "MSE loss: 92.0193\n",
      "Iteration: 45600\n",
      "Gradient: [ -13.6925    4.55    -56.4208 -107.6565  249.1717]\n",
      "Weights: [-4.7747  0.6765 -1.1865  0.108   0.1434]\n",
      "MSE loss: 92.5369\n",
      "Iteration: 45700\n",
      "Gradient: [  -3.6311   -1.1824  -14.7653   84.164  -265.9329]\n",
      "Weights: [-4.7657  0.6739 -1.1831  0.1088  0.1435]\n",
      "MSE loss: 91.8866\n",
      "Iteration: 45800\n",
      "Gradient: [  3.7733  27.9723  96.5155 149.9828 736.1584]\n",
      "Weights: [-4.7645  0.6836 -1.1871  0.109   0.1432]\n",
      "MSE loss: 91.7616\n",
      "Iteration: 45900\n",
      "Gradient: [   6.0351    4.7812  -31.0684  -41.9139 -186.7419]\n",
      "Weights: [-4.7704  0.6895 -1.1923  0.1102  0.1435]\n",
      "MSE loss: 91.7528\n",
      "Iteration: 46000\n",
      "Gradient: [ -13.3109   -1.3795   42.4596 -130.1508 -155.8864]\n",
      "Weights: [-4.7913  0.6915 -1.1926  0.11    0.1434]\n",
      "MSE loss: 92.0576\n",
      "Iteration: 46100\n",
      "Gradient: [  1.6269   0.9759  58.0921 -64.7834  77.6549]\n",
      "Weights: [-4.7752  0.6976 -1.1922  0.1102  0.1433]\n",
      "MSE loss: 91.9519\n",
      "Iteration: 46200\n",
      "Gradient: [ -9.3825  -0.2592 -12.6014 -42.1884   9.9953]\n",
      "Weights: [-4.7954  0.7077 -1.1933  0.1097  0.1429]\n",
      "MSE loss: 91.7472\n",
      "Iteration: 46300\n",
      "Gradient: [  2.6726 -13.8047  90.1488  69.5517  42.9401]\n",
      "Weights: [-4.779   0.7055 -1.1942  0.1116  0.1429]\n",
      "MSE loss: 92.2756\n",
      "Iteration: 46400\n",
      "Gradient: [ -14.1439   17.9301  -73.4091   55.8357 -227.0232]\n",
      "Weights: [-4.7757  0.6879 -1.1976  0.1126  0.1431]\n",
      "MSE loss: 91.7459\n",
      "Iteration: 46500\n",
      "Gradient: [ 11.6886  -2.3317  15.7525  28.8308 230.2212]\n",
      "Weights: [-4.7704  0.6971 -1.1977  0.1124  0.1432]\n",
      "MSE loss: 91.9889\n",
      "Iteration: 46600\n",
      "Gradient: [  9.4011   9.09     2.9517   3.1831 220.6698]\n",
      "Weights: [-4.7765  0.6996 -1.1978  0.1125  0.143 ]\n",
      "MSE loss: 91.7309\n",
      "Iteration: 46700\n",
      "Gradient: [  17.1005   -6.1478    5.5027   25.6833 -211.4687]\n",
      "Weights: [-4.7724  0.6919 -1.194   0.1125  0.1427]\n",
      "MSE loss: 91.6769\n",
      "Iteration: 46800\n",
      "Gradient: [  1.5146  -5.9992  -1.9151  78.5266 245.2728]\n",
      "Weights: [-4.7965  0.6992 -1.1906  0.1122  0.1423]\n",
      "MSE loss: 91.7323\n",
      "Iteration: 46900\n",
      "Gradient: [  9.416  -35.3557 -61.9836 -92.1603  98.8543]\n",
      "Weights: [-4.7971  0.6987 -1.1893  0.1114  0.1422]\n",
      "MSE loss: 91.8403\n",
      "Iteration: 47000\n",
      "Gradient: [3.979000e-01 5.399100e+01 9.086620e+01 1.817078e+02 7.152582e+02]\n",
      "Weights: [-4.8027  0.7026 -1.188   0.1117  0.1423]\n",
      "MSE loss: 91.8981\n",
      "Iteration: 47100\n",
      "Gradient: [ 13.0552  32.1982 -30.0594 203.3811 209.6036]\n",
      "Weights: [-4.7847  0.7078 -1.194   0.1119  0.1424]\n",
      "MSE loss: 91.7986\n",
      "Iteration: 47200\n",
      "Gradient: [  -8.5719   -9.9456   -5.0108 -320.634  -128.029 ]\n",
      "Weights: [-4.7999  0.7001 -1.1933  0.1125  0.1424]\n",
      "MSE loss: 91.9196\n",
      "Iteration: 47300\n",
      "Gradient: [  2.9745  21.7201   0.3192  33.7943 -81.3293]\n",
      "Weights: [-4.7842  0.7036 -1.1939  0.1116  0.1424]\n",
      "MSE loss: 91.6406\n",
      "Iteration: 47400\n",
      "Gradient: [ -27.9805  -34.7152  -53.4855  -95.5449 -635.0587]\n",
      "Weights: [-4.7998  0.7064 -1.1965  0.1118  0.1424]\n",
      "MSE loss: 92.2316\n",
      "Iteration: 47500\n",
      "Gradient: [   2.9362   25.516    61.833   -44.3146 -176.8655]\n",
      "Weights: [-4.78    0.7017 -1.198   0.1129  0.1427]\n",
      "MSE loss: 91.6072\n",
      "Iteration: 47600\n",
      "Gradient: [  12.3142   26.0151   25.2654 -211.4223  171.5002]\n",
      "Weights: [-4.7745  0.7124 -1.2044  0.1139  0.1426]\n",
      "MSE loss: 91.8017\n",
      "Iteration: 47700\n",
      "Gradient: [ -7.6042  15.6542   5.0207  91.2989 246.1679]\n",
      "Weights: [-4.7971  0.7235 -1.2065  0.1144  0.1427]\n",
      "MSE loss: 91.7238\n",
      "Iteration: 47800\n",
      "Gradient: [ -3.2854  32.9486 101.8993 108.2463 137.0512]\n",
      "Weights: [-4.7903  0.7146 -1.2091  0.1148  0.143 ]\n",
      "MSE loss: 91.5749\n",
      "Iteration: 47900\n",
      "Gradient: [-11.9304  -4.7555  47.6705 -71.6273  98.5387]\n",
      "Weights: [-4.8132  0.7288 -1.2109  0.1143  0.1428]\n",
      "MSE loss: 92.1178\n",
      "Iteration: 48000\n",
      "Gradient: [ 21.8924 -31.0729 -11.6449 -96.1061   3.3057]\n",
      "Weights: [-4.7737  0.7192 -1.21    0.1141  0.143 ]\n",
      "MSE loss: 91.8931\n",
      "Iteration: 48100\n",
      "Gradient: [   6.8824    6.676   -34.5995  -43.4786 -367.8588]\n",
      "Weights: [-4.7864  0.7239 -1.2114  0.1138  0.143 ]\n",
      "MSE loss: 91.5661\n",
      "Iteration: 48200\n",
      "Gradient: [-16.3856  51.619  102.697  140.6596 430.7535]\n",
      "Weights: [-4.7916  0.7146 -1.206   0.1133  0.143 ]\n",
      "MSE loss: 91.6047\n",
      "Iteration: 48300\n",
      "Gradient: [ 1.7723 -9.7081 57.6109 -1.168  60.1444]\n",
      "Weights: [-4.7801  0.7099 -1.2055  0.1143  0.1428]\n",
      "MSE loss: 91.6161\n",
      "Iteration: 48400\n",
      "Gradient: [ -16.2319  -28.0526  -33.5428 -243.2886 -140.6063]\n",
      "Weights: [-4.7805  0.7079 -1.2077  0.1144  0.1428]\n",
      "MSE loss: 91.6844\n",
      "Iteration: 48500\n",
      "Gradient: [ -11.0289   16.0429   36.97    147.2821 -156.8988]\n",
      "Weights: [-4.7845  0.7121 -1.2072  0.1145  0.1429]\n",
      "MSE loss: 91.5804\n",
      "Iteration: 48600\n",
      "Gradient: [  3.5206 -28.5359 -15.0108  66.3201 -41.2471]\n",
      "Weights: [-4.7651  0.7109 -1.2096  0.1154  0.1429]\n",
      "MSE loss: 92.2324\n",
      "Iteration: 48700\n",
      "Gradient: [   3.7726  -38.2675  -41.113   -58.7356 -751.879 ]\n",
      "Weights: [-4.788   0.7186 -1.2145  0.1161  0.1428]\n",
      "MSE loss: 91.577\n",
      "Iteration: 48800\n",
      "Gradient: [ -10.2826    8.3304   81.0647 -121.8592   48.8116]\n",
      "Weights: [-4.8012  0.7222 -1.2106  0.1151  0.143 ]\n",
      "MSE loss: 91.7004\n",
      "Iteration: 48900\n",
      "Gradient: [ -11.3923   13.1373  -15.8538 -171.658   358.4199]\n",
      "Weights: [-4.7887  0.7164 -1.214   0.1163  0.1431]\n",
      "MSE loss: 91.5896\n",
      "Iteration: 49000\n",
      "Gradient: [  -4.4879    6.9881   10.5768  222.0583 -472.536 ]\n",
      "Weights: [-4.7753  0.7198 -1.2141  0.1149  0.1433]\n",
      "MSE loss: 91.7354\n",
      "Iteration: 49100\n",
      "Gradient: [ -0.4637 -16.6054 -27.1345 -11.7371  85.4114]\n",
      "Weights: [-4.7918  0.7342 -1.2175  0.115   0.1433]\n",
      "MSE loss: 91.697\n",
      "Iteration: 49200\n",
      "Gradient: [   0.3947    6.8525  -19.9905 -135.9411  107.3981]\n",
      "Weights: [-4.7805  0.7182 -1.2172  0.1165  0.1432]\n",
      "MSE loss: 91.553\n",
      "Iteration: 49300\n",
      "Gradient: [ -10.3742  -24.8899   -8.5741 -215.0142 -547.8343]\n",
      "Weights: [-4.7776  0.7138 -1.2171  0.1167  0.1432]\n",
      "MSE loss: 91.5662\n",
      "Iteration: 49400\n",
      "Gradient: [  3.309   30.7738  34.7446 111.5501 -35.2542]\n",
      "Weights: [-4.7643  0.7077 -1.2114  0.1162  0.1427]\n",
      "MSE loss: 91.7884\n",
      "Iteration: 49500\n",
      "Gradient: [   8.7493  -15.0211  -55.3073 -120.2029 -131.2737]\n",
      "Weights: [-4.7673  0.7179 -1.2152  0.117   0.1426]\n",
      "MSE loss: 91.9787\n",
      "Iteration: 49600\n",
      "Gradient: [-10.3239   8.3677 -12.1213 133.0494  24.3018]\n",
      "Weights: [-4.7869  0.7092 -1.2139  0.1178  0.1426]\n",
      "MSE loss: 91.6985\n",
      "Iteration: 49700\n",
      "Gradient: [-20.5517 -28.9834 -19.9412 100.001  133.0079]\n",
      "Weights: [-4.7809  0.706  -1.2176  0.1186  0.1426]\n",
      "MSE loss: 91.9919\n",
      "Iteration: 49800\n",
      "Gradient: [  3.1974 -15.0568 -20.3714  78.5499  10.0694]\n",
      "Weights: [-4.7559  0.7016 -1.2172  0.1187  0.143 ]\n",
      "MSE loss: 92.0521\n",
      "Iteration: 49900\n",
      "Gradient: [  7.4354 -10.4271  94.6957  60.6179  93.9597]\n",
      "Weights: [-4.7794  0.7102 -1.2164  0.1186  0.1429]\n",
      "MSE loss: 91.7243\n",
      "Iteration: 50000\n",
      "Gradient: [ -2.2622 -26.7452  22.5155 -71.7313 252.5247]\n",
      "Weights: [-4.7938  0.7109 -1.2171  0.1197  0.1427]\n",
      "MSE loss: 91.8601\n",
      "Iteration: 50100\n",
      "Gradient: [  -4.9703  -12.204   -38.0363 -130.8633 -154.2375]\n",
      "Weights: [-4.7728  0.7174 -1.2225  0.1188  0.1426]\n",
      "MSE loss: 91.6635\n",
      "Iteration: 50200\n",
      "Gradient: [ -9.2097  32.8032  71.1116 135.7684 -63.8232]\n",
      "Weights: [-4.7925  0.7228 -1.2169  0.1186  0.1423]\n",
      "MSE loss: 91.5005\n",
      "Iteration: 50300\n",
      "Gradient: [  11.2271   11.6221   55.1241  -13.5374 -173.2896]\n",
      "Weights: [-4.7739  0.7186 -1.2146  0.1186  0.1424]\n",
      "MSE loss: 92.4568\n",
      "Iteration: 50400\n",
      "Gradient: [   9.3076    7.9462   32.1817   47.418  -140.3646]\n",
      "Weights: [-4.7984  0.7102 -1.2143  0.1202  0.1419]\n",
      "MSE loss: 91.8843\n",
      "Iteration: 50500\n",
      "Gradient: [   6.5754   38.6905  -14.1999  -20.6488 -140.209 ]\n",
      "Weights: [-4.7691  0.7132 -1.2132  0.1194  0.1418]\n",
      "MSE loss: 91.9598\n",
      "Iteration: 50600\n",
      "Gradient: [   2.4514   26.933    26.7509  -55.3632 -253.0638]\n",
      "Weights: [-4.7852  0.7128 -1.2158  0.1198  0.1417]\n",
      "MSE loss: 91.6712\n",
      "Iteration: 50700\n",
      "Gradient: [  3.1604  36.6502  61.0767 296.7255 888.2185]\n",
      "Weights: [-4.7832  0.723  -1.2168  0.1197  0.1418]\n",
      "MSE loss: 91.5852\n",
      "Iteration: 50800\n",
      "Gradient: [  -6.7367   19.745   -42.8546  -46.3034 -171.9775]\n",
      "Weights: [-4.7836  0.7258 -1.2181  0.1186  0.1422]\n",
      "MSE loss: 91.5472\n",
      "Iteration: 50900\n",
      "Gradient: [  3.5137  -5.3963  10.3166 202.4303 310.5543]\n",
      "Weights: [-4.791   0.7147 -1.2149  0.1192  0.1421]\n",
      "MSE loss: 91.5519\n",
      "Iteration: 51000\n",
      "Gradient: [ -17.2836   29.4321   92.6579  134.37   -167.8107]\n",
      "Weights: [-4.7978  0.7211 -1.2143  0.1184  0.1422]\n",
      "MSE loss: 91.58\n",
      "Iteration: 51100\n",
      "Gradient: [   5.5087  -13.7681  -67.3538 -107.0773 -850.0449]\n",
      "Weights: [-4.7827  0.7211 -1.2139  0.1168  0.1425]\n",
      "MSE loss: 91.5529\n",
      "Iteration: 51200\n",
      "Gradient: [  -5.9004   -3.0053  -71.7939 -235.3691 -264.7239]\n",
      "Weights: [-4.7848  0.7143 -1.2135  0.1177  0.1422]\n",
      "MSE loss: 91.5585\n",
      "Iteration: 51300\n",
      "Gradient: [-4.3069 49.044  39.6942 37.4283 60.8828]\n",
      "Weights: [-4.7873  0.7158 -1.2094  0.118   0.1422]\n",
      "MSE loss: 92.0013\n",
      "Iteration: 51400\n",
      "Gradient: [ 12.3055  -3.4865  -3.5701 202.6453 736.4465]\n",
      "Weights: [-4.7741  0.702  -1.2072  0.1181  0.1421]\n",
      "MSE loss: 91.6677\n",
      "Iteration: 51500\n",
      "Gradient: [ 1.6863 51.3086 63.0727 80.8861 -8.8389]\n",
      "Weights: [-4.7747  0.7071 -1.2061  0.1173  0.1421]\n",
      "MSE loss: 91.8754\n",
      "Iteration: 51600\n",
      "Gradient: [  -6.6237    6.7237    8.7197   96.4814 -128.4223]\n",
      "Weights: [-4.7824  0.7063 -1.2059  0.1172  0.1417]\n",
      "MSE loss: 91.5722\n",
      "Iteration: 51700\n",
      "Gradient: [  6.501   19.3524  22.2645   3.761  459.7892]\n",
      "Weights: [-4.7914  0.7243 -1.2091  0.1183  0.1414]\n",
      "MSE loss: 91.7003\n",
      "Iteration: 51800\n",
      "Gradient: [   2.3654  -18.0088   20.3043  146.2591 -243.5107]\n",
      "Weights: [-4.8068  0.7287 -1.2073  0.1179  0.1412]\n",
      "MSE loss: 91.6899\n",
      "Iteration: 51900\n",
      "Gradient: [ -4.8938  45.6493  81.6832  75.0551 597.5644]\n",
      "Weights: [-4.7946  0.7146 -1.207   0.1187  0.1416]\n",
      "MSE loss: 91.7104\n",
      "Iteration: 52000\n",
      "Gradient: [  -8.4269  -21.1329 -117.7825 -159.2158  145.9791]\n",
      "Weights: [-4.775   0.6964 -1.2103  0.1205  0.1416]\n",
      "MSE loss: 91.5259\n",
      "Iteration: 52100\n",
      "Gradient: [ 15.2476  19.9219  53.5549  38.384  350.758 ]\n",
      "Weights: [-4.7745  0.7196 -1.2142  0.1202  0.1415]\n",
      "MSE loss: 92.1535\n",
      "Iteration: 52200\n",
      "Gradient: [  4.482   20.7489  20.6018 207.8309 480.6304]\n",
      "Weights: [-4.7802  0.7015 -1.2128  0.1205  0.1415]\n",
      "MSE loss: 91.754\n",
      "Iteration: 52300\n",
      "Gradient: [   5.4421  -12.7578  -41.9064 -141.7002 -513.8497]\n",
      "Weights: [-4.7734  0.7071 -1.2129  0.1205  0.1415]\n",
      "MSE loss: 91.5176\n",
      "Iteration: 52400\n",
      "Gradient: [ -4.9311  -4.447   14.2778  84.6393 392.9143]\n",
      "Weights: [-4.7927  0.7209 -1.2136  0.1201  0.1412]\n",
      "MSE loss: 91.51\n",
      "Iteration: 52500\n",
      "Gradient: [-11.1423  -1.3881  71.7592 128.7157 403.6228]\n",
      "Weights: [-4.7845  0.7112 -1.2127  0.1208  0.1414]\n",
      "MSE loss: 91.5026\n",
      "Iteration: 52600\n",
      "Gradient: [  11.9608   12.1525   18.7878 -149.6212 -223.1603]\n",
      "Weights: [-4.778   0.7168 -1.2159  0.1212  0.1411]\n",
      "MSE loss: 91.5346\n",
      "Iteration: 52700\n",
      "Gradient: [  -5.1832  -11.7514   33.227  -158.3944   75.6494]\n",
      "Weights: [-4.7933  0.7165 -1.2145  0.1212  0.1411]\n",
      "MSE loss: 91.5469\n",
      "Iteration: 52800\n",
      "Gradient: [-13.4949  -4.1871  23.4572  86.062  -37.268 ]\n",
      "Weights: [-4.8074  0.7202 -1.2157  0.1214  0.1411]\n",
      "MSE loss: 92.1184\n",
      "Iteration: 52900\n",
      "Gradient: [  16.4771  -17.4929   34.6266 -206.3024   52.0495]\n",
      "Weights: [-4.7861  0.7289 -1.2226  0.1214  0.1411]\n",
      "MSE loss: 91.7011\n",
      "Iteration: 53000\n",
      "Gradient: [ -13.4022  -12.9565  -74.79   -180.0148 -779.5107]\n",
      "Weights: [-4.8008  0.7224 -1.2222  0.1214  0.1415]\n",
      "MSE loss: 92.46\n",
      "Iteration: 53100\n",
      "Gradient: [-10.2586 -16.8909  -6.2978   7.5151  97.6704]\n",
      "Weights: [-4.7904  0.7258 -1.2207  0.1211  0.1416]\n",
      "MSE loss: 91.4447\n",
      "Iteration: 53200\n",
      "Gradient: [ 10.5954   6.5548 -81.3645 -54.8413 392.953 ]\n",
      "Weights: [-4.776   0.722  -1.2234  0.122   0.1416]\n",
      "MSE loss: 91.5102\n",
      "Iteration: 53300\n",
      "Gradient: [  14.6324  -13.6682   53.5218   41.9342 -204.146 ]\n",
      "Weights: [-4.7774  0.7187 -1.2244  0.123   0.1415]\n",
      "MSE loss: 91.4474\n",
      "Iteration: 53400\n",
      "Gradient: [  10.2427  -28.4253   -4.949    69.4967 -246.4081]\n",
      "Weights: [-4.7743  0.7162 -1.2223  0.1227  0.1415]\n",
      "MSE loss: 91.4882\n",
      "Iteration: 53500\n",
      "Gradient: [   1.687   -34.6953  -50.7617  -64.1632 -112.8008]\n",
      "Weights: [-4.7987  0.7242 -1.2246  0.1235  0.1413]\n",
      "MSE loss: 91.6999\n",
      "Iteration: 53600\n",
      "Gradient: [  -5.0276   -3.0401   22.434    66.3845 -396.3976]\n",
      "Weights: [-4.7985  0.7375 -1.2249  0.1229  0.1412]\n",
      "MSE loss: 91.4599\n",
      "Iteration: 53700\n",
      "Gradient: [  -0.8533  -10.5374 -123.4664 -115.3182 -238.0568]\n",
      "Weights: [-4.7828  0.7279 -1.2302  0.1242  0.1413]\n",
      "MSE loss: 91.48\n",
      "Iteration: 53800\n",
      "Gradient: [ -12.7619  -36.6366   38.1568  161.6324 -196.7363]\n",
      "Weights: [-4.8084  0.7288 -1.2297  0.1248  0.1411]\n",
      "MSE loss: 92.6199\n",
      "Iteration: 53900\n",
      "Gradient: [  -8.6208  -36.0632   -6.0664 -194.3306 -548.8023]\n",
      "Weights: [-4.7981  0.7359 -1.2328  0.1248  0.1411]\n",
      "MSE loss: 91.8826\n",
      "Iteration: 54000\n",
      "Gradient: [-12.0312  27.1756  50.6428 117.3911  25.7798]\n",
      "Weights: [-4.7798  0.7367 -1.234   0.1258  0.1412]\n",
      "MSE loss: 91.6538\n",
      "Iteration: 54100\n",
      "Gradient: [  -5.915    -0.4969    5.2251 -109.7346  -37.4964]\n",
      "Weights: [-4.7817  0.7268 -1.232   0.1272  0.141 ]\n",
      "MSE loss: 91.5644\n",
      "Iteration: 54200\n",
      "Gradient: [   5.1604  -32.3242  -50.3603 -251.1119 -410.0672]\n",
      "Weights: [-4.7862  0.7329 -1.2342  0.1271  0.1407]\n",
      "MSE loss: 91.3873\n",
      "Iteration: 54300\n",
      "Gradient: [  7.0683  -6.2595 -38.8064 156.7086 -92.0656]\n",
      "Weights: [-4.7837  0.7334 -1.2381  0.1283  0.1407]\n",
      "MSE loss: 91.3654\n",
      "Iteration: 54400\n",
      "Gradient: [  6.5425 -20.1607  -8.577   67.9531  54.5494]\n",
      "Weights: [-4.7703  0.7318 -1.2365  0.1291  0.1403]\n",
      "MSE loss: 91.6945\n",
      "Iteration: 54500\n",
      "Gradient: [  9.77    19.966  -41.4288  10.0564 169.2103]\n",
      "Weights: [-4.7805  0.7261 -1.2366  0.13    0.1402]\n",
      "MSE loss: 91.3767\n",
      "Iteration: 54600\n",
      "Gradient: [  -5.8573    0.8628   -4.8407  149.7694 -121.8193]\n",
      "Weights: [-4.7838  0.724  -1.2377  0.1305  0.1404]\n",
      "MSE loss: 91.3693\n",
      "Iteration: 54700\n",
      "Gradient: [ 5.74000e-02 -3.51740e+01 -3.23265e+01  8.91150e+00  6.89257e+01]\n",
      "Weights: [-4.7838  0.7227 -1.2361  0.1309  0.14  ]\n",
      "MSE loss: 91.4149\n",
      "Iteration: 54800\n",
      "Gradient: [  -1.3455    5.6535   -8.1596 -119.8731 -110.2657]\n",
      "Weights: [-4.7887  0.7438 -1.2383  0.1309  0.1398]\n",
      "MSE loss: 91.6743\n",
      "Iteration: 54900\n",
      "Gradient: [  1.6498  10.8676  40.6328 -41.7207 501.2729]\n",
      "Weights: [-4.8141  0.7459 -1.2398  0.1311  0.1395]\n",
      "MSE loss: 92.0478\n",
      "Iteration: 55000\n",
      "Gradient: [   6.3216  -15.1736  -11.3098  -56.5635 -166.5446]\n",
      "Weights: [-4.796   0.7416 -1.2428  0.1325  0.1398]\n",
      "MSE loss: 91.2911\n",
      "Iteration: 55100\n",
      "Gradient: [   8.5275  -11.9684  -44.0144  -25.3171 -195.4556]\n",
      "Weights: [-4.779   0.733  -1.2441  0.1328  0.1399]\n",
      "MSE loss: 91.3176\n",
      "Iteration: 55200\n",
      "Gradient: [ -1.232  -30.6709  23.8358  33.0012  68.3308]\n",
      "Weights: [-4.7869  0.7382 -1.2467  0.1338  0.1397]\n",
      "MSE loss: 91.264\n",
      "Iteration: 55300\n",
      "Gradient: [  -8.4798  -19.3923  -24.4015   31.1623 -123.7847]\n",
      "Weights: [-4.7993  0.7376 -1.2449  0.1328  0.1398]\n",
      "MSE loss: 91.7593\n",
      "Iteration: 55400\n",
      "Gradient: [   0.4044   -7.9325  -34.7677   53.7374 -181.2738]\n",
      "Weights: [-4.7827  0.7371 -1.245   0.133   0.1398]\n",
      "MSE loss: 91.2862\n",
      "Iteration: 55500\n",
      "Gradient: [ 29.2229   2.3918  56.3215 121.8096  96.1483]\n",
      "Weights: [-4.7828  0.7481 -1.2424  0.1325  0.1397]\n",
      "MSE loss: 92.3089\n",
      "Iteration: 55600\n",
      "Gradient: [  -6.9812    9.6023   26.6682 -127.9512  132.8433]\n",
      "Weights: [-4.8086  0.7537 -1.247   0.1322  0.1399]\n",
      "MSE loss: 91.4643\n",
      "Iteration: 55700\n",
      "Gradient: [ -1.2551  48.321   43.9615 107.5968 501.3928]\n",
      "Weights: [-4.7916  0.74   -1.244   0.1317  0.1401]\n",
      "MSE loss: 91.3126\n",
      "Iteration: 55800\n",
      "Gradient: [ -10.1256   -3.2224   40.1533  -69.5378 -132.3634]\n",
      "Weights: [-4.8047  0.7364 -1.2414  0.1318  0.1402]\n",
      "MSE loss: 91.6353\n",
      "Iteration: 55900\n",
      "Gradient: [  9.4674 -39.4204   9.0859  30.8237 130.8451]\n",
      "Weights: [-4.7744  0.731  -1.243   0.1314  0.1402]\n",
      "MSE loss: 91.3876\n",
      "Iteration: 56000\n",
      "Gradient: [   6.2542  -17.0543   17.0102   94.8351 -314.8275]\n",
      "Weights: [-4.7818  0.7328 -1.2439  0.1326  0.1401]\n",
      "MSE loss: 91.2901\n",
      "Iteration: 56100\n",
      "Gradient: [  15.3125   20.3194  -12.8992   57.3123 -228.2921]\n",
      "Weights: [-4.7924  0.7515 -1.2431  0.1328  0.1397]\n",
      "MSE loss: 92.1033\n",
      "Iteration: 56200\n",
      "Gradient: [ -4.8681   1.6889  -5.2898 -40.1642 -63.3452]\n",
      "Weights: [-4.7917  0.7365 -1.2418  0.1317  0.14  ]\n",
      "MSE loss: 91.3097\n",
      "Iteration: 56300\n",
      "Gradient: [  21.0383  -20.6744   69.6682 -125.1958  246.2057]\n",
      "Weights: [-4.7642  0.7332 -1.2401  0.1313  0.1403]\n",
      "MSE loss: 92.6066\n",
      "Iteration: 56400\n",
      "Gradient: [ -19.6999   -0.5214   29.8423 -146.3727   -6.0897]\n",
      "Weights: [-4.771   0.7274 -1.2395  0.1316  0.1401]\n",
      "MSE loss: 91.5625\n",
      "Iteration: 56500\n",
      "Gradient: [-13.739  -12.9006 -51.0275 -41.4895 -14.0418]\n",
      "Weights: [-4.7811  0.731  -1.2426  0.1327  0.1398]\n",
      "MSE loss: 91.3079\n",
      "Iteration: 56600\n",
      "Gradient: [ -6.3217  26.6407  51.3029  26.8806 299.409 ]\n",
      "Weights: [-4.7985  0.7388 -1.2396  0.133   0.1395]\n",
      "MSE loss: 91.3446\n",
      "Iteration: 56700\n",
      "Gradient: [ -19.3964  -24.0233  -73.6324  -31.8189 -295.8897]\n",
      "Weights: [-4.7859  0.7271 -1.242   0.1319  0.1402]\n",
      "MSE loss: 91.5263\n",
      "Iteration: 56800\n",
      "Gradient: [  1.4322 -12.6204  -5.1254  53.7105  99.3977]\n",
      "Weights: [-4.7792  0.7244 -1.2412  0.1324  0.1403]\n",
      "MSE loss: 91.3491\n",
      "Iteration: 56900\n",
      "Gradient: [ 13.4833  -5.8682 -28.2784  74.1706 345.2193]\n",
      "Weights: [-4.7779  0.7313 -1.242   0.1326  0.1401]\n",
      "MSE loss: 91.451\n",
      "Iteration: 57000\n",
      "Gradient: [-4.583000e-01 -3.637200e+01  5.043770e+01 -3.480090e+01 -6.337935e+02]\n",
      "Weights: [-4.7767  0.7272 -1.2401  0.132   0.1401]\n",
      "MSE loss: 91.4037\n",
      "Iteration: 57100\n",
      "Gradient: [-17.5872  20.093   47.1388   8.8363 166.3821]\n",
      "Weights: [-4.7932  0.7353 -1.2403  0.1319  0.1399]\n",
      "MSE loss: 91.3037\n",
      "Iteration: 57200\n",
      "Gradient: [  -8.6098   -2.6198  -39.1814  -10.9251 -192.5949]\n",
      "Weights: [-4.8063  0.7353 -1.2411  0.1328  0.1399]\n",
      "MSE loss: 91.6447\n",
      "Iteration: 57300\n",
      "Gradient: [ 10.8346  19.5712  20.223  -71.6818  33.0139]\n",
      "Weights: [-4.7681  0.7204 -1.2377  0.1339  0.1394]\n",
      "MSE loss: 91.5961\n",
      "Iteration: 57400\n",
      "Gradient: [ 22.5815  12.1311  26.9138 -63.2135 334.083 ]\n",
      "Weights: [-4.7693  0.7068 -1.2393  0.1349  0.1397]\n",
      "MSE loss: 91.4244\n",
      "Iteration: 57500\n",
      "Gradient: [ -6.7765  30.7182 -30.4813   8.4861 218.4466]\n",
      "Weights: [-4.7758  0.7066 -1.2363  0.1348  0.1392]\n",
      "MSE loss: 91.4725\n",
      "Iteration: 57600\n",
      "Gradient: [ -3.5208 -26.7341  -2.4864  80.5822 -11.1397]\n",
      "Weights: [-4.7783  0.7152 -1.2336  0.1346  0.1388]\n",
      "MSE loss: 91.2933\n",
      "Iteration: 57700\n",
      "Gradient: [   3.9224   19.0514    5.6855  -14.2776 -141.5643]\n",
      "Weights: [-4.772   0.7135 -1.2337  0.1349  0.1387]\n",
      "MSE loss: 91.3416\n",
      "Iteration: 57800\n",
      "Gradient: [   0.3077    4.726   -20.8187 -105.418    27.8545]\n",
      "Weights: [-4.7717  0.7225 -1.2359  0.1346  0.1389]\n",
      "MSE loss: 91.6673\n",
      "Iteration: 57900\n",
      "Gradient: [  6.4551 -10.6005  31.0875  19.1505   2.8142]\n",
      "Weights: [-4.7728  0.7122 -1.2355  0.1358  0.139 ]\n",
      "MSE loss: 91.5263\n",
      "Iteration: 58000\n",
      "Gradient: [  2.2249  18.8752  13.7008  10.312  -10.6887]\n",
      "Weights: [-4.7837  0.7159 -1.2353  0.1345  0.1388]\n",
      "MSE loss: 91.4552\n",
      "Iteration: 58100\n",
      "Gradient: [  5.5241 -11.2912 -11.9434   1.1739  70.0616]\n",
      "Weights: [-4.792   0.7396 -1.239   0.1339  0.1387]\n",
      "MSE loss: 91.3261\n",
      "Iteration: 58200\n",
      "Gradient: [   4.5531    5.2554    4.5244  -55.834  -127.505 ]\n",
      "Weights: [-4.7957  0.7293 -1.2401  0.1351  0.139 ]\n",
      "MSE loss: 91.363\n",
      "Iteration: 58300\n",
      "Gradient: [  10.9273  -14.6727  -31.4181 -268.9357 -518.7415]\n",
      "Weights: [-4.7761  0.7325 -1.2433  0.1354  0.1389]\n",
      "MSE loss: 91.3743\n",
      "Iteration: 58400\n",
      "Gradient: [ 15.4847  26.6656  26.5125  79.7112 760.542 ]\n",
      "Weights: [-4.7969  0.7405 -1.2436  0.1356  0.139 ]\n",
      "MSE loss: 91.3152\n",
      "Iteration: 58500\n",
      "Gradient: [ 21.9933  28.6359 110.3054 214.8435  78.3818]\n",
      "Weights: [-4.7832  0.7419 -1.2488  0.1365  0.139 ]\n",
      "MSE loss: 91.3223\n",
      "Iteration: 58600\n",
      "Gradient: [-22.5197  -8.7903  14.2264   7.0123 -43.5601]\n",
      "Weights: [-4.8059  0.7423 -1.248   0.1364  0.1391]\n",
      "MSE loss: 91.4745\n",
      "Iteration: 58700\n",
      "Gradient: [ 29.6191  45.4023  95.7535 330.5259 538.722 ]\n",
      "Weights: [-4.7708  0.7355 -1.2487  0.1368  0.1393]\n",
      "MSE loss: 91.9457\n",
      "Iteration: 58800\n",
      "Gradient: [-16.3768  21.3631 -69.8178 -30.6174  23.6478]\n",
      "Weights: [-4.7782  0.7253 -1.248   0.1371  0.1393]\n",
      "MSE loss: 91.2767\n",
      "Iteration: 58900\n",
      "Gradient: [ -1.0699 -21.1651  30.1636 -20.3126 -44.7925]\n",
      "Weights: [-4.7784  0.7209 -1.2483  0.1379  0.1389]\n",
      "MSE loss: 91.5944\n",
      "Iteration: 59000\n",
      "Gradient: [ -3.3021  24.7353  26.2283  47.3237 162.2732]\n",
      "Weights: [-4.7865  0.7249 -1.2483  0.1388  0.1389]\n",
      "MSE loss: 91.3214\n",
      "Iteration: 59100\n",
      "Gradient: [  -0.9572   -6.3661  -93.6699   56.9427 -400.584 ]\n",
      "Weights: [-4.7856  0.7373 -1.2489  0.1382  0.1389]\n",
      "MSE loss: 91.4196\n",
      "Iteration: 59200\n",
      "Gradient: [-11.0044   7.8505   1.7891 122.1848 210.1085]\n",
      "Weights: [-4.8029  0.7373 -1.2517  0.1383  0.1388]\n",
      "MSE loss: 91.8557\n",
      "Iteration: 59300\n",
      "Gradient: [ -8.3972   0.2864  67.727  185.6529 261.8125]\n",
      "Weights: [-4.774   0.7277 -1.2536  0.1387  0.1389]\n",
      "MSE loss: 91.4538\n",
      "Iteration: 59400\n",
      "Gradient: [ -2.9184 -10.0815 -12.5512  91.1263 292.4948]\n",
      "Weights: [-4.7822  0.7262 -1.2502  0.1399  0.1389]\n",
      "MSE loss: 91.4007\n",
      "Iteration: 59500\n",
      "Gradient: [  -2.733    26.9846  -27.3157  -72.4496 -424.2077]\n",
      "Weights: [-4.8037  0.7407 -1.2534  0.1392  0.1384]\n",
      "MSE loss: 92.1088\n",
      "Iteration: 59600\n",
      "Gradient: [  6.5422  28.294   47.5383 252.4915 472.9794]\n",
      "Weights: [-4.7966  0.754  -1.2528  0.1398  0.1382]\n",
      "MSE loss: 91.5993\n",
      "Iteration: 59700\n",
      "Gradient: [  -7.5734  -27.6371    9.6281 -239.3859  -20.392 ]\n",
      "Weights: [-4.7928  0.7432 -1.2547  0.1405  0.1382]\n",
      "MSE loss: 91.2127\n",
      "Iteration: 59800\n",
      "Gradient: [ -4.1744  -7.6799 -20.1552  18.3752 -36.3761]\n",
      "Weights: [-4.7936  0.7396 -1.2535  0.1411  0.138 ]\n",
      "MSE loss: 91.2209\n",
      "Iteration: 59900\n",
      "Gradient: [  3.9751  -9.6722  81.4434 -71.1007  74.7282]\n",
      "Weights: [-4.7623  0.7077 -1.2487  0.1419  0.138 ]\n",
      "MSE loss: 91.416\n",
      "Iteration: 60000\n",
      "Gradient: [ -1.3902  13.4809  17.2798  34.0556 136.2397]\n",
      "Weights: [-4.7849  0.7213 -1.2494  0.1426  0.1376]\n",
      "MSE loss: 91.288\n",
      "Iteration: 60100\n",
      "Gradient: [ -2.8245  -6.2227  13.5755 -13.2787 280.5765]\n",
      "Weights: [-4.769   0.7199 -1.2504  0.1431  0.1377]\n",
      "MSE loss: 91.2919\n",
      "Iteration: 60200\n",
      "Gradient: [   7.8517  -25.483    44.6514  -26.0518 -153.996 ]\n",
      "Weights: [-4.7835  0.7286 -1.2518  0.1431  0.1378]\n",
      "MSE loss: 91.2527\n",
      "Iteration: 60300\n",
      "Gradient: [  3.5804  25.3095  10.5696 173.602   29.1708]\n",
      "Weights: [-4.7808  0.7382 -1.2566  0.1424  0.1377]\n",
      "MSE loss: 91.1952\n",
      "Iteration: 60400\n",
      "Gradient: [  10.4413   -2.2105  -42.1792    9.2134 -152.3665]\n",
      "Weights: [-4.7779  0.7355 -1.2549  0.1428  0.1376]\n",
      "MSE loss: 91.1962\n",
      "Iteration: 60500\n",
      "Gradient: [-13.1527 -20.7591 -32.0431 -22.883  446.742 ]\n",
      "Weights: [-4.81    0.7407 -1.2592  0.1445  0.1374]\n",
      "MSE loss: 92.2454\n",
      "Iteration: 60600\n",
      "Gradient: [ -7.3053 -11.7037 -40.0184 -47.691   69.9029]\n",
      "Weights: [-4.7945  0.7464 -1.2613  0.1453  0.1372]\n",
      "MSE loss: 91.149\n",
      "Iteration: 60700\n",
      "Gradient: [-6.370000e-02 -7.641900e+00 -4.568820e+01 -2.024796e+02 -1.804860e+01]\n",
      "Weights: [-4.8066  0.7602 -1.2652  0.146   0.137 ]\n",
      "MSE loss: 91.1867\n",
      "Iteration: 60800\n",
      "Gradient: [ 21.6938  34.7577  82.3507 174.4205 229.627 ]\n",
      "Weights: [-4.7658  0.737  -1.2677  0.1484  0.1373]\n",
      "MSE loss: 91.4964\n",
      "Iteration: 60900\n",
      "Gradient: [  4.4443   3.788   53.2726  38.9616 450.2166]\n",
      "Weights: [-4.7672  0.7373 -1.2647  0.1486  0.1369]\n",
      "MSE loss: 91.6933\n",
      "Iteration: 61000\n",
      "Gradient: [   2.7407   17.6113  -34.4884   67.0687 -355.7045]\n",
      "Weights: [-4.7816  0.7327 -1.2611  0.1499  0.1365]\n",
      "MSE loss: 91.3686\n",
      "Iteration: 61100\n",
      "Gradient: [   1.603   -12.9331  -13.0658 -112.7375 -383.5421]\n",
      "Weights: [-4.7685  0.7251 -1.2626  0.1505  0.1363]\n",
      "MSE loss: 91.1572\n",
      "Iteration: 61200\n",
      "Gradient: [ 22.885   18.8579  74.8657  86.2804 133.2224]\n",
      "Weights: [-4.7941  0.7453 -1.2654  0.1505  0.1362]\n",
      "MSE loss: 91.1091\n",
      "Iteration: 61300\n",
      "Gradient: [   0.6428  -13.7607  -29.0052 -139.1633   86.8268]\n",
      "Weights: [-4.7918  0.7555 -1.2686  0.1499  0.1361]\n",
      "MSE loss: 91.0813\n",
      "Iteration: 61400\n",
      "Gradient: [  9.6525  -4.8849  64.1536  66.535  136.9323]\n",
      "Weights: [-4.7773  0.7428 -1.2648  0.1496  0.1363]\n",
      "MSE loss: 91.3024\n",
      "Iteration: 61500\n",
      "Gradient: [  2.04   -10.1935  47.5339 -32.7932  87.1056]\n",
      "Weights: [-4.7876  0.7461 -1.2642  0.1497  0.1362]\n",
      "MSE loss: 91.1747\n",
      "Iteration: 61600\n",
      "Gradient: [-3.5258 39.2453 31.335  77.1263 24.2463]\n",
      "Weights: [-4.7886  0.7422 -1.2617  0.1507  0.1357]\n",
      "MSE loss: 91.1596\n",
      "Iteration: 61700\n",
      "Gradient: [ 9.0297 -8.0482 15.175  44.7431 11.3208]\n",
      "Weights: [-4.7664  0.7348 -1.2623  0.1517  0.1353]\n",
      "MSE loss: 91.5922\n",
      "Iteration: 61800\n",
      "Gradient: [  3.2897  18.5266  86.2504 165.8398 536.5533]\n",
      "Weights: [-4.7939  0.7382 -1.2601  0.1523  0.1351]\n",
      "MSE loss: 91.1275\n",
      "Iteration: 61900\n",
      "Gradient: [   6.1469   -2.6779  -34.5601  -11.3327 -193.9205]\n",
      "Weights: [-4.7963  0.7349 -1.2632  0.1526  0.1352]\n",
      "MSE loss: 91.3934\n",
      "Iteration: 62000\n",
      "Gradient: [  14.5656  -11.9538  -97.8372 -118.3652 -419.5263]\n",
      "Weights: [-4.7811  0.7366 -1.2665  0.1536  0.1349]\n",
      "MSE loss: 91.1421\n",
      "Iteration: 62100\n",
      "Gradient: [  6.4322  26.4434 105.6423 109.4263 465.5396]\n",
      "Weights: [-4.791   0.738  -1.2649  0.1543  0.1349]\n",
      "MSE loss: 91.0389\n",
      "Iteration: 62200\n",
      "Gradient: [  0.8885  40.6628  10.3998   3.9241 -99.1048]\n",
      "Weights: [-4.7973  0.7461 -1.2622  0.1537  0.1345]\n",
      "MSE loss: 91.1791\n",
      "Iteration: 62300\n",
      "Gradient: [  5.2371  18.8477  29.9821 -31.8817 242.1504]\n",
      "Weights: [-4.812   0.7495 -1.2641  0.1534  0.1346]\n",
      "MSE loss: 91.4215\n",
      "Iteration: 62400\n",
      "Gradient: [   1.3162   -8.1173   55.9489   53.6591 -389.7891]\n",
      "Weights: [-4.7926  0.7484 -1.2658  0.1535  0.1349]\n",
      "MSE loss: 91.1534\n",
      "Iteration: 62500\n",
      "Gradient: [ -16.8346  -17.833  -140.9336  107.8801 -231.817 ]\n",
      "Weights: [-4.8138  0.7519 -1.2674  0.1541  0.1348]\n",
      "MSE loss: 91.4393\n",
      "Iteration: 62600\n",
      "Gradient: [  17.3668   -6.9458    0.1803 -101.2712   14.5068]\n",
      "Weights: [-4.786   0.7465 -1.2668  0.1545  0.1346]\n",
      "MSE loss: 91.171\n",
      "Iteration: 62700\n",
      "Gradient: [  5.8795   4.9222  26.1149 109.4623 -73.7439]\n",
      "Weights: [-4.7818  0.7508 -1.2744  0.1558  0.135 ]\n",
      "MSE loss: 91.191\n",
      "Iteration: 62800\n",
      "Gradient: [ -11.6415    4.4664   57.2077 -102.6279  180.0987]\n",
      "Weights: [-4.7882  0.7397 -1.2723  0.1562  0.135 ]\n",
      "MSE loss: 90.9891\n",
      "Iteration: 62900\n",
      "Gradient: [  4.5382  42.892    5.9558  -6.0286 719.5163]\n",
      "Weights: [-4.7862  0.7344 -1.2677  0.1557  0.1351]\n",
      "MSE loss: 91.1752\n",
      "Iteration: 63000\n",
      "Gradient: [   7.2264  -29.7126  -71.6164 -103.5234   81.9869]\n",
      "Weights: [-4.7611  0.7252 -1.2689  0.1563  0.135 ]\n",
      "MSE loss: 91.4062\n",
      "Iteration: 63100\n",
      "Gradient: [  4.9151  81.0269  80.8117 -48.8899 315.2052]\n",
      "Weights: [-4.7894  0.7364 -1.2648  0.1556  0.1345]\n",
      "MSE loss: 91.0503\n",
      "Iteration: 63200\n",
      "Gradient: [ 11.3043  21.8632  92.5879 140.6195 523.4914]\n",
      "Weights: [-4.7888  0.7272 -1.2622  0.1556  0.1347]\n",
      "MSE loss: 91.0923\n",
      "Iteration: 63300\n",
      "Gradient: [ -3.0999  55.5607  63.4197 194.8458 581.9801]\n",
      "Weights: [-4.7823  0.7214 -1.2582  0.1552  0.1345]\n",
      "MSE loss: 91.1439\n",
      "Iteration: 63400\n",
      "Gradient: [13.0747 -4.4922  2.9894 23.9651 39.8954]\n",
      "Weights: [-4.7784  0.7245 -1.2617  0.1554  0.1344]\n",
      "MSE loss: 91.0324\n",
      "Iteration: 63500\n",
      "Gradient: [  9.4874   4.9427  71.8938 278.4354 286.6157]\n",
      "Weights: [-4.7725  0.7189 -1.2568  0.1549  0.1341]\n",
      "MSE loss: 91.132\n",
      "Iteration: 63600\n",
      "Gradient: [ -0.7237  -5.0684 -55.5946   8.7556 -48.4965]\n",
      "Weights: [-4.8039  0.7358 -1.2602  0.1542  0.1343]\n",
      "MSE loss: 91.4448\n",
      "Iteration: 63700\n",
      "Gradient: [  -1.6173   -3.978    -7.2417  -78.9667 -183.4634]\n",
      "Weights: [-4.7936  0.7221 -1.2634  0.1559  0.1346]\n",
      "MSE loss: 91.5435\n",
      "Iteration: 63800\n",
      "Gradient: [-9.6766 12.5903 -7.5169 61.6    12.675 ]\n",
      "Weights: [-4.7745  0.7188 -1.2606  0.1553  0.1345]\n",
      "MSE loss: 91.042\n",
      "Iteration: 63900\n",
      "Gradient: [ -17.4709   -8.0758  -53.6041 -137.8751 -910.0868]\n",
      "Weights: [-4.7922  0.7082 -1.2572  0.1557  0.1343]\n",
      "MSE loss: 92.1661\n",
      "Iteration: 64000\n",
      "Gradient: [  -2.2258  -28.722   -37.3522  -57.1186 -329.4969]\n",
      "Weights: [-4.7677  0.7174 -1.2584  0.1564  0.1339]\n",
      "MSE loss: 91.2206\n",
      "Iteration: 64100\n",
      "Gradient: [  -6.0513    6.8824  -56.1725 -101.797   158.3483]\n",
      "Weights: [-4.7886  0.7123 -1.2602  0.1561  0.1341]\n",
      "MSE loss: 92.4029\n",
      "Iteration: 64200\n",
      "Gradient: [ -6.2023  27.1656  21.1689  74.8066 213.8911]\n",
      "Weights: [-4.7765  0.7195 -1.2623  0.1574  0.1341]\n",
      "MSE loss: 91.0202\n",
      "Iteration: 64300\n",
      "Gradient: [  -3.78      2.9865 -111.4092 -133.0143 -822.0596]\n",
      "Weights: [-4.7763  0.7118 -1.2638  0.1576  0.134 ]\n",
      "MSE loss: 91.663\n",
      "Iteration: 64400\n",
      "Gradient: [  9.7224  20.7019  38.0999 256.3347  78.9011]\n",
      "Weights: [-4.765   0.7189 -1.2642  0.1579  0.1343]\n",
      "MSE loss: 91.4565\n",
      "Iteration: 64500\n",
      "Gradient: [-12.7306  11.5715 -35.9948  85.2595 356.6184]\n",
      "Weights: [-4.7793  0.7175 -1.264   0.1572  0.1345]\n",
      "MSE loss: 91.0647\n",
      "Iteration: 64600\n",
      "Gradient: [ -20.4711  -13.2991   -5.8268  153.3641 -396.6787]\n",
      "Weights: [-4.7921  0.7248 -1.2641  0.1561  0.1343]\n",
      "MSE loss: 91.5315\n",
      "Iteration: 64700\n",
      "Gradient: [ -12.9077   -5.5806  -36.2386 -354.6043 -320.0281]\n",
      "Weights: [-4.7933  0.7372 -1.2675  0.156   0.1345]\n",
      "MSE loss: 91.1\n",
      "Iteration: 64800\n",
      "Gradient: [  0.97    -2.754    1.3683 -77.4642 257.4792]\n",
      "Weights: [-4.7733  0.7316 -1.2678  0.1557  0.1347]\n",
      "MSE loss: 91.0658\n",
      "Iteration: 64900\n",
      "Gradient: [ -8.2591   8.3284  41.5449 -34.0946 286.3135]\n",
      "Weights: [-4.7757  0.7353 -1.2711  0.1562  0.1352]\n",
      "MSE loss: 91.2285\n",
      "Iteration: 65000\n",
      "Gradient: [  5.929   -9.8061  22.274   34.8167 479.4324]\n",
      "Weights: [-4.7803  0.7332 -1.2722  0.1572  0.1349]\n",
      "MSE loss: 90.9795\n",
      "Iteration: 65100\n",
      "Gradient: [ -31.0739   -4.325   -60.787    47.7714 -316.582 ]\n",
      "Weights: [-4.7765  0.7178 -1.2703  0.1579  0.1347]\n",
      "MSE loss: 91.3226\n",
      "Iteration: 65200\n",
      "Gradient: [  23.661   -23.9764  -29.7603 -100.7804 -158.5724]\n",
      "Weights: [-4.7623  0.7162 -1.2689  0.1579  0.1347]\n",
      "MSE loss: 91.1037\n",
      "Iteration: 65300\n",
      "Gradient: [  1.1866  40.171    1.5274 104.1504 -73.4704]\n",
      "Weights: [-4.7715  0.7234 -1.2703  0.1579  0.1348]\n",
      "MSE loss: 91.0447\n",
      "Iteration: 65400\n",
      "Gradient: [-5.9132  5.8326 29.869   9.4762 -2.5196]\n",
      "Weights: [-4.7917  0.7241 -1.269   0.1569  0.1347]\n",
      "MSE loss: 91.8233\n",
      "Iteration: 65500\n",
      "Gradient: [  1.094    9.7976  36.024   94.0817 -68.9675]\n",
      "Weights: [-4.7974  0.7455 -1.2757  0.1579  0.1344]\n",
      "MSE loss: 91.2967\n",
      "Iteration: 65600\n",
      "Gradient: [-12.573    9.5393 -10.9621  22.7202  23.4419]\n",
      "Weights: [-4.7962  0.7515 -1.2785  0.1584  0.1346]\n",
      "MSE loss: 90.9715\n",
      "Iteration: 65700\n",
      "Gradient: [ -35.2508   -9.2772   -7.5775 -106.9667  -50.7003]\n",
      "Weights: [-4.7941  0.7513 -1.2775  0.1577  0.1345]\n",
      "MSE loss: 91.0144\n",
      "Iteration: 65800\n",
      "Gradient: [  4.8304   1.9582  19.8527 -71.0293  10.2295]\n",
      "Weights: [-4.7991  0.7494 -1.2766  0.1578  0.1346]\n",
      "MSE loss: 91.0816\n",
      "Iteration: 65900\n",
      "Gradient: [  5.4238 -10.1198 -33.2141 -24.433   22.1879]\n",
      "Weights: [-4.7766  0.7462 -1.2774  0.1575  0.1347]\n",
      "MSE loss: 91.0698\n",
      "Iteration: 66000\n",
      "Gradient: [  6.0925  14.5109  25.1862  21.9763 228.6599]\n",
      "Weights: [-4.7834  0.7367 -1.2753  0.1587  0.1347]\n",
      "MSE loss: 90.9616\n",
      "Iteration: 66100\n",
      "Gradient: [   6.9081   -1.9254   35.3814  -95.4524 -237.6007]\n",
      "Weights: [-4.7951  0.7379 -1.276   0.1592  0.1345]\n",
      "MSE loss: 91.1848\n",
      "Iteration: 66200\n",
      "Gradient: [ 11.4464  -6.6975 -17.6749  18.6664 232.6591]\n",
      "Weights: [-4.7913  0.7347 -1.2741  0.1598  0.1343]\n",
      "MSE loss: 91.0736\n",
      "Iteration: 66300\n",
      "Gradient: [-1.992000e-01 -1.013610e+01 -5.287660e+01 -2.083979e+02 -7.215900e+01]\n",
      "Weights: [-4.7892  0.7392 -1.2782  0.1598  0.1343]\n",
      "MSE loss: 91.1081\n",
      "Iteration: 66400\n",
      "Gradient: [  7.5181   1.4009   6.7333 216.3217 264.7069]\n",
      "Weights: [-4.7854  0.7475 -1.2822  0.1602  0.1342]\n",
      "MSE loss: 90.9983\n",
      "Iteration: 66500\n",
      "Gradient: [  2.2151  43.9392 -21.7591 -38.147   65.9894]\n",
      "Weights: [-4.7897  0.751  -1.2836  0.1598  0.1344]\n",
      "MSE loss: 91.0023\n",
      "Iteration: 66600\n",
      "Gradient: [  6.0966 -21.5131 -62.8135  33.5203 511.3374]\n",
      "Weights: [-4.8023  0.7591 -1.2824  0.1593  0.1341]\n",
      "MSE loss: 91.2992\n",
      "Iteration: 66700\n",
      "Gradient: [ 26.0936   5.8955   2.2316   3.3246 494.9378]\n",
      "Weights: [-4.8009  0.7489 -1.2807  0.1604  0.1343]\n",
      "MSE loss: 91.113\n",
      "Iteration: 66800\n",
      "Gradient: [  2.3246   7.3085  13.2113  25.8824 313.3331]\n",
      "Weights: [-4.8058  0.7462 -1.2802  0.1605  0.1343]\n",
      "MSE loss: 91.4887\n",
      "Iteration: 66900\n",
      "Gradient: [ -25.8249  -11.3092   59.559    24.0206 -720.6382]\n",
      "Weights: [-4.803   0.7427 -1.2771  0.1594  0.1341]\n",
      "MSE loss: 91.7766\n",
      "Iteration: 67000\n",
      "Gradient: [  2.1126   5.1861  36.1722 131.5697  68.8346]\n",
      "Weights: [-4.7707  0.7432 -1.2776  0.1599  0.1343]\n",
      "MSE loss: 91.633\n",
      "Iteration: 67100\n",
      "Gradient: [ 13.1916  28.4544  12.2178  66.5158 389.101 ]\n",
      "Weights: [-4.7812  0.7455 -1.2779  0.1601  0.1343]\n",
      "MSE loss: 91.2148\n",
      "Iteration: 67200\n",
      "Gradient: [  15.9488  -30.37    -13.1208    3.5687 -218.3177]\n",
      "Weights: [-4.7794  0.7458 -1.2798  0.1596  0.1343]\n",
      "MSE loss: 90.9616\n",
      "Iteration: 67300\n",
      "Gradient: [  -9.8012  -16.6634   63.6855 -130.4341 -369.8446]\n",
      "Weights: [-4.7897  0.7517 -1.2798  0.1596  0.1342]\n",
      "MSE loss: 90.9274\n",
      "Iteration: 67400\n",
      "Gradient: [   4.2267    3.032    -8.7856  133.4772 -306.2451]\n",
      "Weights: [-4.7695  0.7328 -1.2815  0.1613  0.1342]\n",
      "MSE loss: 91.0658\n",
      "Iteration: 67500\n",
      "Gradient: [ 13.1797  35.8309  98.6736  78.701  363.5457]\n",
      "Weights: [-4.7847  0.7454 -1.2802  0.1611  0.1341]\n",
      "MSE loss: 90.9367\n",
      "Iteration: 67600\n",
      "Gradient: [ -24.5714  -10.2897  -57.1515  -95.6362 -706.5798]\n",
      "Weights: [-4.7873  0.7369 -1.2842  0.1624  0.134 ]\n",
      "MSE loss: 91.5376\n",
      "Iteration: 67700\n",
      "Gradient: [ -19.4905  -25.8516  -38.1462 -160.7255  -10.7017]\n",
      "Weights: [-4.7902  0.732  -1.2827  0.1628  0.134 ]\n",
      "MSE loss: 91.7179\n",
      "Iteration: 67800\n",
      "Gradient: [  -6.0125  -11.4338  -19.6924   61.0943 -198.3392]\n",
      "Weights: [-4.7705  0.7428 -1.2815  0.1627  0.1335]\n",
      "MSE loss: 91.1602\n",
      "Iteration: 67900\n",
      "Gradient: [ -7.4865   1.6774  98.9321 138.2934 254.4515]\n",
      "Weights: [-4.8102  0.7422 -1.2789  0.1623  0.1338]\n",
      "MSE loss: 91.6884\n",
      "Iteration: 68000\n",
      "Gradient: [-22.5199  -8.5027 -42.5977 -38.5753 349.7883]\n",
      "Weights: [-4.791   0.7444 -1.2827  0.1629  0.1337]\n",
      "MSE loss: 90.9464\n",
      "Iteration: 68100\n",
      "Gradient: [   3.8018  -18.6274    6.0817   -9.2053 -128.3347]\n",
      "Weights: [-4.7842  0.7448 -1.2832  0.1635  0.1335]\n",
      "MSE loss: 90.8778\n",
      "Iteration: 68200\n",
      "Gradient: [  3.474    1.928   32.6914  92.4974 -27.3257]\n",
      "Weights: [-4.7858  0.746  -1.2822  0.1628  0.1335]\n",
      "MSE loss: 90.8891\n",
      "Iteration: 68300\n",
      "Gradient: [  1.6896  40.3238  40.2729  57.6484 271.7073]\n",
      "Weights: [-4.8001  0.7717 -1.2919  0.1628  0.1336]\n",
      "MSE loss: 90.9501\n",
      "Iteration: 68400\n",
      "Gradient: [  30.4585   29.9202   44.2486  130.5263 -326.4164]\n",
      "Weights: [-4.7929  0.7794 -1.2911  0.1636  0.1334]\n",
      "MSE loss: 91.8532\n",
      "Iteration: 68500\n",
      "Gradient: [ -3.0407  19.35     8.604  -54.3898 247.6235]\n",
      "Weights: [-4.8015  0.7727 -1.2956  0.1643  0.1336]\n",
      "MSE loss: 90.9026\n",
      "Iteration: 68600\n",
      "Gradient: [  4.5841  14.2507 -10.8211  46.951   -9.5944]\n",
      "Weights: [-4.7952  0.7729 -1.2969  0.1651  0.1337]\n",
      "MSE loss: 90.8991\n",
      "Iteration: 68700\n",
      "Gradient: [  3.4936  43.9925  73.6478 201.427  485.2182]\n",
      "Weights: [-4.8086  0.79   -1.3007  0.1658  0.1335]\n",
      "MSE loss: 91.1533\n",
      "Iteration: 68800\n",
      "Gradient: [-22.6499  33.8364  21.1485 -38.1224 103.4778]\n",
      "Weights: [-4.7974  0.7834 -1.3045  0.1652  0.1337]\n",
      "MSE loss: 90.9873\n",
      "Iteration: 68900\n",
      "Gradient: [  -6.8099  -15.0191  -60.8091 -177.3302 -410.8096]\n",
      "Weights: [-4.7932  0.7819 -1.3053  0.1653  0.1339]\n",
      "MSE loss: 90.9008\n",
      "Iteration: 69000\n",
      "Gradient: [  5.1068   0.5654 -18.8113  80.2131 -89.2277]\n",
      "Weights: [-4.7879  0.7926 -1.3108  0.166   0.134 ]\n",
      "MSE loss: 91.0908\n",
      "Iteration: 69100\n",
      "Gradient: [  -3.0213   10.8865  -75.6181  -74.6103 -199.8507]\n",
      "Weights: [-4.7965  0.7885 -1.3093  0.1659  0.1341]\n",
      "MSE loss: 90.8067\n",
      "Iteration: 69200\n",
      "Gradient: [  8.2426   5.5805  -6.6326  56.6782 104.5186]\n",
      "Weights: [-4.783   0.7765 -1.3024  0.1658  0.1338]\n",
      "MSE loss: 91.087\n",
      "Iteration: 69300\n",
      "Gradient: [ -8.8938  20.5375 120.6309  30.0103 210.1595]\n",
      "Weights: [-4.7868  0.7777 -1.3034  0.1671  0.1337]\n",
      "MSE loss: 91.3048\n",
      "Iteration: 69400\n",
      "Gradient: [ -16.9805   19.2271   37.7743 -119.6393  305.8534]\n",
      "Weights: [-4.7744  0.7699 -1.3037  0.1673  0.1336]\n",
      "MSE loss: 91.1868\n",
      "Iteration: 69500\n",
      "Gradient: [  -4.4221  -20.1505   -2.474  -232.7741 -728.0546]\n",
      "Weights: [-4.7767  0.7599 -1.3038  0.1692  0.1334]\n",
      "MSE loss: 90.8678\n",
      "Iteration: 69600\n",
      "Gradient: [ 28.4537  39.8502 -10.8133 254.2741 610.919 ]\n",
      "Weights: [-4.7837  0.7869 -1.3103  0.1702  0.1332]\n",
      "MSE loss: 91.9242\n",
      "Iteration: 69700\n",
      "Gradient: [ -8.1896 -21.6695  12.9434  56.8322 166.6319]\n",
      "Weights: [-4.8016  0.7724 -1.3145  0.1705  0.1336]\n",
      "MSE loss: 91.6439\n",
      "Iteration: 69800\n",
      "Gradient: [-21.1349  13.7953   1.0694  24.5843 -22.5916]\n",
      "Weights: [-4.8128  0.7754 -1.3132  0.1709  0.1332]\n",
      "MSE loss: 92.118\n",
      "Iteration: 69900\n",
      "Gradient: [ -19.4296    8.4974  -13.4791   83.8306 -248.0538]\n",
      "Weights: [-4.8135  0.7821 -1.3091  0.1704  0.1332]\n",
      "MSE loss: 90.9874\n",
      "Iteration: 70000\n",
      "Gradient: [  -8.2991   50.4769   44.3447   54.2238 -175.2809]\n",
      "Weights: [-4.7864  0.7774 -1.3076  0.1696  0.1332]\n",
      "MSE loss: 91.027\n",
      "Iteration: 70100\n",
      "Gradient: [-20.3961  16.4387  29.453  -76.3949 287.1932]\n",
      "Weights: [-4.8082  0.7847 -1.3117  0.1705  0.1332]\n",
      "MSE loss: 90.8332\n",
      "Iteration: 70200\n",
      "Gradient: [ 24.6761  36.114   62.8786 170.361   82.1138]\n",
      "Weights: [-4.809   0.8018 -1.3176  0.1704  0.1333]\n",
      "MSE loss: 90.8117\n",
      "Iteration: 70300\n",
      "Gradient: [  3.2275 -14.1658 -30.5622 -37.4126 -53.8506]\n",
      "Weights: [-4.8088  0.8066 -1.3211  0.1705  0.1333]\n",
      "MSE loss: 90.7843\n",
      "Iteration: 70400\n",
      "Gradient: [  -7.4819   10.3543  -55.8257 -163.5704 -452.9986]\n",
      "Weights: [-4.7916  0.7783 -1.3163  0.1715  0.1334]\n",
      "MSE loss: 90.7292\n",
      "Iteration: 70500\n",
      "Gradient: [  0.5058  23.551   43.7719 123.0659  48.1275]\n",
      "Weights: [-4.7915  0.7887 -1.3169  0.1704  0.1334]\n",
      "MSE loss: 90.7557\n",
      "Iteration: 70600\n",
      "Gradient: [ -0.8121  43.4452  -0.4813  26.8183 314.7905]\n",
      "Weights: [-4.7981  0.7876 -1.3181  0.1708  0.1337]\n",
      "MSE loss: 90.7265\n",
      "Iteration: 70700\n",
      "Gradient: [ -8.5309 -11.3745   9.1597 -93.4963 248.7097]\n",
      "Weights: [-4.8139  0.7999 -1.3206  0.1699  0.1339]\n",
      "MSE loss: 90.8916\n",
      "Iteration: 70800\n",
      "Gradient: [   4.0267  -28.1437   55.8652 -133.8159 -506.4837]\n",
      "Weights: [-4.8007  0.7998 -1.3194  0.1698  0.1339]\n",
      "MSE loss: 90.8883\n",
      "Iteration: 70900\n",
      "Gradient: [ -7.253   14.0338 -27.3555  -8.2883 271.2388]\n",
      "Weights: [-4.8001  0.7905 -1.3176  0.1684  0.1341]\n",
      "MSE loss: 90.8178\n",
      "Iteration: 71000\n",
      "Gradient: [-19.5632  22.6249  43.6457  85.973   17.334 ]\n",
      "Weights: [-4.8075  0.7874 -1.3161  0.1681  0.1342]\n",
      "MSE loss: 91.1602\n",
      "Iteration: 71100\n",
      "Gradient: [   9.3228  -18.8201  -67.9155 -173.6309   76.6503]\n",
      "Weights: [-4.7899  0.7876 -1.3154  0.1672  0.1343]\n",
      "MSE loss: 90.8079\n",
      "Iteration: 71200\n",
      "Gradient: [  -5.4809   -8.1406  -55.4023 -161.2573 -305.6894]\n",
      "Weights: [-4.8109  0.7859 -1.3131  0.167   0.1344]\n",
      "MSE loss: 91.2971\n",
      "Iteration: 71300\n",
      "Gradient: [  5.0606  31.8545 -39.1716 -42.1895 -66.5421]\n",
      "Weights: [-4.7903  0.787  -1.3168  0.1685  0.1342]\n",
      "MSE loss: 90.7622\n",
      "Iteration: 71400\n",
      "Gradient: [1.740000e-01 3.316610e+01 4.354570e+01 1.135787e+02 3.515329e+02]\n",
      "Weights: [-4.815   0.7958 -1.3174  0.1686  0.1342]\n",
      "MSE loss: 90.9758\n",
      "Iteration: 71500\n",
      "Gradient: [-15.0986   7.7553  64.0129  63.4999  24.4834]\n",
      "Weights: [-4.8092  0.7936 -1.3169  0.1691  0.1341]\n",
      "MSE loss: 90.829\n",
      "Iteration: 71600\n",
      "Gradient: [ -14.1318    2.9221   -4.4769   33.0334 -123.5404]\n",
      "Weights: [-4.8119  0.8012 -1.32    0.1689  0.1339]\n",
      "MSE loss: 90.8879\n",
      "Iteration: 71700\n",
      "Gradient: [ -10.2133  -19.6101  -54.4459 -167.9627 -113.7488]\n",
      "Weights: [-4.7952  0.7866 -1.3191  0.1694  0.1341]\n",
      "MSE loss: 90.8326\n",
      "Iteration: 71800\n",
      "Gradient: [ -8.7854 -17.9034  25.7042  29.6446 197.8623]\n",
      "Weights: [-4.8011  0.787  -1.3195  0.1702  0.1339]\n",
      "MSE loss: 90.8921\n",
      "Iteration: 71900\n",
      "Gradient: [-24.5388   6.0564 -19.5175   8.0413 -17.1464]\n",
      "Weights: [-4.826   0.7925 -1.3213  0.1714  0.1337]\n",
      "MSE loss: 92.1828\n",
      "Iteration: 72000\n",
      "Gradient: [ -8.066   -2.8376  23.8784  -1.1264 -45.8394]\n",
      "Weights: [-4.8098  0.8009 -1.3244  0.1723  0.1335]\n",
      "MSE loss: 90.7521\n",
      "Iteration: 72100\n",
      "Gradient: [  3.9406  40.5917  31.9491 102.0484 424.6713]\n",
      "Weights: [-4.7998  0.8069 -1.3273  0.1726  0.1335]\n",
      "MSE loss: 90.8514\n",
      "Iteration: 72200\n",
      "Gradient: [-2.123  12.965  26.994  -5.8481 45.4634]\n",
      "Weights: [-4.7999  0.7932 -1.3231  0.1731  0.1333]\n",
      "MSE loss: 90.695\n",
      "Iteration: 72300\n",
      "Gradient: [  6.5633 -14.2612   9.6744 -69.1731 201.0542]\n",
      "Weights: [-4.7835  0.7893 -1.3216  0.1738  0.1328]\n",
      "MSE loss: 90.9345\n",
      "Iteration: 72400\n",
      "Gradient: [   4.9469   11.7767   23.297    99.1417 -115.058 ]\n",
      "Weights: [-4.8053  0.7971 -1.3236  0.1737  0.1327]\n",
      "MSE loss: 90.8345\n",
      "Iteration: 72500\n",
      "Gradient: [-9.697  -2.2507 59.8124 45.5227 85.3419]\n",
      "Weights: [-4.8089  0.7868 -1.323   0.1742  0.1327]\n",
      "MSE loss: 91.7076\n",
      "Iteration: 72600\n",
      "Gradient: [  14.0589    0.5362    6.0257   31.2993 -135.5021]\n",
      "Weights: [-4.7924  0.7915 -1.3247  0.1745  0.133 ]\n",
      "MSE loss: 90.7036\n",
      "Iteration: 72700\n",
      "Gradient: [  7.8966  34.5568  47.4804 165.215   96.5632]\n",
      "Weights: [-4.7899  0.7928 -1.3218  0.1742  0.1328]\n",
      "MSE loss: 91.0302\n",
      "Iteration: 72800\n",
      "Gradient: [   0.4367    6.1519  -22.2732   12.2656 -235.4096]\n",
      "Weights: [-4.7949  0.7957 -1.3221  0.1733  0.1331]\n",
      "MSE loss: 90.9066\n",
      "Iteration: 72900\n",
      "Gradient: [  1.5147 -21.9168 -22.5712  74.2002 309.3036]\n",
      "Weights: [-4.8054  0.8003 -1.3264  0.173   0.1332]\n",
      "MSE loss: 90.812\n",
      "Iteration: 73000\n",
      "Gradient: [  3.3662 -10.4698  16.92     2.6542 -10.0114]\n",
      "Weights: [-4.7976  0.7881 -1.3252  0.1748  0.1331]\n",
      "MSE loss: 90.6977\n",
      "Iteration: 73100\n",
      "Gradient: [  -6.897   -10.0157   -4.1364  -50.7397 -512.3679]\n",
      "Weights: [-4.8032  0.804  -1.3311  0.1744  0.133 ]\n",
      "MSE loss: 90.8472\n",
      "Iteration: 73200\n",
      "Gradient: [ 1.420490e+01 -1.528480e+01 -1.520000e-01 -1.279184e+02 -3.055559e+02]\n",
      "Weights: [-4.788   0.7974 -1.3299  0.1735  0.1334]\n",
      "MSE loss: 90.7688\n",
      "Iteration: 73300\n",
      "Gradient: [  -5.5338    8.7755   19.3189  -23.0138 -446.3608]\n",
      "Weights: [-4.793   0.7985 -1.3315  0.1738  0.1335]\n",
      "MSE loss: 90.7707\n",
      "Iteration: 73400\n",
      "Gradient: [ -2.6402  16.9512 -50.4187 -63.5907  60.7837]\n",
      "Weights: [-4.7937  0.7904 -1.3293  0.175   0.1333]\n",
      "MSE loss: 90.7134\n",
      "Iteration: 73500\n",
      "Gradient: [ -12.6262    5.7184  -47.4973 -107.1144 -540.6272]\n",
      "Weights: [-4.8039  0.7973 -1.3298  0.1755  0.1333]\n",
      "MSE loss: 90.7288\n",
      "Iteration: 73600\n",
      "Gradient: [18.0021 -7.4592 43.4098 81.2728 -0.2561]\n",
      "Weights: [-4.7906  0.8022 -1.3268  0.1758  0.1327]\n",
      "MSE loss: 91.5593\n",
      "Iteration: 73700\n",
      "Gradient: [ 14.5474 -12.89    76.8236 -11.8962 165.9822]\n",
      "Weights: [-4.8022  0.8033 -1.3276  0.1759  0.1324]\n",
      "MSE loss: 90.6737\n",
      "Iteration: 73800\n",
      "Gradient: [  -4.7291   -4.4892    6.7637  -66.986  -165.458 ]\n",
      "Weights: [-4.7997  0.8003 -1.3298  0.177   0.1323]\n",
      "MSE loss: 90.6323\n",
      "Iteration: 73900\n",
      "Gradient: [ -11.4899  -18.6126    6.8726 -124.4602  -26.8319]\n",
      "Weights: [-4.7923  0.7924 -1.3328  0.1787  0.1325]\n",
      "MSE loss: 90.6191\n",
      "Iteration: 74000\n",
      "Gradient: [  12.8644   18.2972  -66.432    39.867  -287.3284]\n",
      "Weights: [-4.8027  0.8053 -1.3342  0.1791  0.1322]\n",
      "MSE loss: 90.6975\n",
      "Iteration: 74100\n",
      "Gradient: [ 20.2    -27.8138 -34.7417  70.9618  93.3141]\n",
      "Weights: [-4.793   0.809  -1.334   0.179   0.1318]\n",
      "MSE loss: 90.9671\n",
      "Iteration: 74200\n",
      "Gradient: [-4.70000e-03  5.69634e+01  6.27538e+01  2.71470e+01 -7.10993e+01]\n",
      "Weights: [-4.8069  0.8225 -1.3395  0.1783  0.1321]\n",
      "MSE loss: 90.6893\n",
      "Iteration: 74300\n",
      "Gradient: [ 19.1629  44.013  138.1526 304.3675 731.7881]\n",
      "Weights: [-4.7986  0.8071 -1.3355  0.1797  0.1321]\n",
      "MSE loss: 90.8557\n",
      "Iteration: 74400\n",
      "Gradient: [ 22.6723  41.2045 -20.8573 368.6321 411.4366]\n",
      "Weights: [-4.7966  0.8166 -1.3418  0.1798  0.132 ]\n",
      "MSE loss: 90.7055\n",
      "Iteration: 74500\n",
      "Gradient: [  -7.9289   12.3561  -17.668   102.8657 -399.5051]\n",
      "Weights: [-4.7966  0.7992 -1.3368  0.18    0.1321]\n",
      "MSE loss: 90.6069\n",
      "Iteration: 74600\n",
      "Gradient: [-15.4621 -21.5116  21.9715 -82.1879   1.3465]\n",
      "Weights: [-4.814   0.813  -1.3405  0.1808  0.1319]\n",
      "MSE loss: 90.7146\n",
      "Iteration: 74700\n",
      "Gradient: [-13.3985  17.3549 -55.5621  44.1799  37.3295]\n",
      "Weights: [-4.7973  0.8053 -1.3417  0.181   0.1321]\n",
      "MSE loss: 90.5828\n",
      "Iteration: 74800\n",
      "Gradient: [  -5.5841  -31.4629   15.1982 -140.3827 -631.7035]\n",
      "Weights: [-4.8178  0.8025 -1.3361  0.1797  0.1318]\n",
      "MSE loss: 91.8917\n",
      "Iteration: 74900\n",
      "Gradient: [ -3.1572   3.992  -11.1416 -66.2768 427.8074]\n",
      "Weights: [-4.7989  0.7916 -1.3337  0.1818  0.1317]\n",
      "MSE loss: 90.6224\n",
      "Iteration: 75000\n",
      "Gradient: [ 12.7097  12.3819  46.4232 240.0791 462.0351]\n",
      "Weights: [-4.7796  0.7867 -1.3356  0.1825  0.1317]\n",
      "MSE loss: 90.818\n",
      "Iteration: 75100\n",
      "Gradient: [  6.3083  11.7231 -80.7236 137.9777 120.9856]\n",
      "Weights: [-4.7829  0.8001 -1.3361  0.1819  0.1316]\n",
      "MSE loss: 91.4844\n",
      "Iteration: 75200\n",
      "Gradient: [   1.9737    8.5405 -124.0152 -127.5342 -326.5859]\n",
      "Weights: [-4.781   0.7937 -1.3376  0.1822  0.1314]\n",
      "MSE loss: 90.7364\n",
      "Iteration: 75300\n",
      "Gradient: [-11.4772  25.4735  38.0663 171.3493 169.3296]\n",
      "Weights: [-4.7987  0.8001 -1.3377  0.1832  0.1315]\n",
      "MSE loss: 90.8118\n",
      "Iteration: 75400\n",
      "Gradient: [  -4.3403  -10.7798   -4.2572   22.3026 -541.2352]\n",
      "Weights: [-4.7793  0.7823 -1.3411  0.1837  0.1316]\n",
      "MSE loss: 90.893\n",
      "Iteration: 75500\n",
      "Gradient: [ 20.7962   1.1895   7.1413  33.916  -44.4439]\n",
      "Weights: [-4.7738  0.7745 -1.3347  0.1839  0.1315]\n",
      "MSE loss: 90.7638\n",
      "Iteration: 75600\n",
      "Gradient: [  -7.2273  -16.6849  -58.4565 -162.1493 -134.5626]\n",
      "Weights: [-4.8037  0.7992 -1.3374  0.1819  0.1313]\n",
      "MSE loss: 90.8366\n",
      "Iteration: 75700\n",
      "Gradient: [  -24.4394   -83.0759  -153.0792  -124.2784 -1015.0978]\n",
      "Weights: [-4.788   0.7927 -1.3375  0.1824  0.1315]\n",
      "MSE loss: 90.5868\n",
      "Iteration: 75800\n",
      "Gradient: [  -1.4628   -5.7142  -13.3163   17.9847 -309.9069]\n",
      "Weights: [-4.8041  0.8012 -1.3393  0.1822  0.1315]\n",
      "MSE loss: 90.7078\n",
      "Iteration: 75900\n",
      "Gradient: [ -8.121   24.8821  12.1917  95.7251 -65.8262]\n",
      "Weights: [-4.8016  0.8031 -1.3413  0.1827  0.1318]\n",
      "MSE loss: 90.5717\n",
      "Iteration: 76000\n",
      "Gradient: [  -3.0931  -38.6698  -55.6437 -182.3005 -629.8793]\n",
      "Weights: [-4.7912  0.7976 -1.3395  0.1833  0.1314]\n",
      "MSE loss: 90.5729\n",
      "Iteration: 76100\n",
      "Gradient: [-13.8057  39.9781 111.2952 344.752  602.9816]\n",
      "Weights: [-4.8117  0.8018 -1.3424  0.1835  0.1315]\n",
      "MSE loss: 91.0513\n",
      "Iteration: 76200\n",
      "Gradient: [  15.3282    2.7639   -3.0607   67.5827 -459.3711]\n",
      "Weights: [-4.7673  0.7804 -1.3439  0.1853  0.1318]\n",
      "MSE loss: 90.9147\n",
      "Iteration: 76300\n",
      "Gradient: [  -2.5199  -25.6283  -62.4078  -60.1329 -113.8996]\n",
      "Weights: [-4.7848  0.7879 -1.342   0.1847  0.1317]\n",
      "MSE loss: 90.6363\n",
      "Iteration: 76400\n",
      "Gradient: [  6.0041  13.7525 -83.3052 126.8067 -22.2242]\n",
      "Weights: [-4.7931  0.8035 -1.343   0.1832  0.1316]\n",
      "MSE loss: 90.5525\n",
      "Iteration: 76500\n",
      "Gradient: [-13.5928  12.38    13.6932 171.1065  91.5112]\n",
      "Weights: [-4.8207  0.8111 -1.3431  0.1835  0.1313]\n",
      "MSE loss: 91.0302\n",
      "Iteration: 76600\n",
      "Gradient: [ -3.5048 -24.2036  59.726   94.6665  17.1408]\n",
      "Weights: [-4.82    0.8184 -1.3469  0.1826  0.1318]\n",
      "MSE loss: 90.9887\n",
      "Iteration: 76700\n",
      "Gradient: [  12.7201   13.5417   13.6991 -112.8965   45.8125]\n",
      "Weights: [-4.7879  0.8277 -1.3497  0.1821  0.1317]\n",
      "MSE loss: 91.2751\n",
      "Iteration: 76800\n",
      "Gradient: [-12.0651  -2.4914 -19.9229 -74.825  287.9053]\n",
      "Weights: [-4.8166  0.8271 -1.347   0.1829  0.1316]\n",
      "MSE loss: 90.6357\n",
      "Iteration: 76900\n",
      "Gradient: [  -5.6322   21.4646  -95.7167  -15.9726 -160.8542]\n",
      "Weights: [-4.7954  0.8278 -1.3496  0.1832  0.1316]\n",
      "MSE loss: 91.1513\n",
      "Iteration: 77000\n",
      "Gradient: [ -15.635   -30.4776  -79.4317   39.0151 -722.4781]\n",
      "Weights: [-4.8175  0.8278 -1.3493  0.1823  0.1318]\n",
      "MSE loss: 90.6607\n",
      "Iteration: 77100\n",
      "Gradient: [ 13.4151  50.3648 -23.2203 -95.9104 623.5982]\n",
      "Weights: [-4.8021  0.8262 -1.3486  0.1829  0.1316]\n",
      "MSE loss: 90.7245\n",
      "Iteration: 77200\n",
      "Gradient: [   1.4842   -7.9086  -78.6286 -159.7271  453.2051]\n",
      "Weights: [-4.8038  0.822  -1.3484  0.1832  0.1317]\n",
      "MSE loss: 90.6194\n",
      "Iteration: 77300\n",
      "Gradient: [ 5.259   5.1948 14.508  40.3708  1.8718]\n",
      "Weights: [-4.8108  0.8219 -1.3479  0.1847  0.1315]\n",
      "MSE loss: 90.8633\n",
      "Iteration: 77400\n",
      "Gradient: [  0.425   -9.672   69.54    36.1046 346.1502]\n",
      "Weights: [-4.8044  0.8193 -1.3524  0.186   0.1313]\n",
      "MSE loss: 90.4984\n",
      "Iteration: 77500\n",
      "Gradient: [ -1.265   44.89    -4.8721 -24.9505  78.7271]\n",
      "Weights: [-4.7817  0.8136 -1.3544  0.1879  0.131 ]\n",
      "MSE loss: 91.0332\n",
      "Iteration: 77600\n",
      "Gradient: [ 18.46   -10.023  -61.6186 -56.306  141.5238]\n",
      "Weights: [-4.7994  0.8157 -1.3491  0.1876  0.1304]\n",
      "MSE loss: 90.5619\n",
      "Iteration: 77700\n",
      "Gradient: [  11.1394    2.2616   35.7129 -149.1295  386.8893]\n",
      "Weights: [-4.7942  0.8112 -1.3525  0.1887  0.1306]\n",
      "MSE loss: 90.5446\n",
      "Iteration: 77800\n",
      "Gradient: [ -5.4156  -5.7189  35.936  288.4234 261.4792]\n",
      "Weights: [-4.8162  0.8162 -1.3514  0.1888  0.1304]\n",
      "MSE loss: 90.6095\n",
      "Iteration: 77900\n",
      "Gradient: [ 14.0623 -42.3164 -25.5953 -80.5985 437.6142]\n",
      "Weights: [-4.7765  0.7982 -1.3525  0.1902  0.1301]\n",
      "MSE loss: 90.6875\n",
      "Iteration: 78000\n",
      "Gradient: [  -0.5022   -9.0835   -7.8098  -37.5332 -100.2219]\n",
      "Weights: [-4.8089  0.797  -1.347   0.1904  0.1301]\n",
      "MSE loss: 90.7343\n",
      "Iteration: 78100\n",
      "Gradient: [ -2.6322  12.3082  52.404  227.6919 255.485 ]\n",
      "Weights: [-4.7897  0.7899 -1.3464  0.1899  0.1302]\n",
      "MSE loss: 90.5006\n",
      "Iteration: 78200\n",
      "Gradient: [ 17.1676  11.8133  50.2951  89.3221 792.9042]\n",
      "Weights: [-4.7903  0.8005 -1.347   0.1895  0.1301]\n",
      "MSE loss: 90.5716\n",
      "Iteration: 78300\n",
      "Gradient: [ -16.5962   -3.8149  -20.4416 -111.4921 -130.0472]\n",
      "Weights: [-4.7967  0.8074 -1.3491  0.1907  0.1297]\n",
      "MSE loss: 90.5779\n",
      "Iteration: 78400\n",
      "Gradient: [ 23.7797  56.0346  59.9773 150.7362 723.1065]\n",
      "Weights: [-4.7955  0.8138 -1.3475  0.1906  0.1298]\n",
      "MSE loss: 92.114\n",
      "Iteration: 78500\n",
      "Gradient: [ 2.109000e-01 -3.306610e+01 -8.075500e+00 -2.003633e+02 -2.854221e+02]\n",
      "Weights: [-4.8086  0.8011 -1.3502  0.1912  0.1294]\n",
      "MSE loss: 91.2907\n",
      "Iteration: 78600\n",
      "Gradient: [  -2.5476  -28.8423  -21.7521 -300.4371 -468.3992]\n",
      "Weights: [-4.7918  0.8025 -1.3477  0.1911  0.1294]\n",
      "MSE loss: 90.5452\n",
      "Iteration: 78700\n",
      "Gradient: [ -10.3252  -19.5316  -83.9172  -97.3535 -743.5885]\n",
      "Weights: [-4.8042  0.808  -1.3515  0.1914  0.1294]\n",
      "MSE loss: 90.5612\n",
      "Iteration: 78800\n",
      "Gradient: [  2.3423  18.727   36.1227 -99.8386 -40.5337]\n",
      "Weights: [-4.7976  0.8117 -1.3528  0.1924  0.1294]\n",
      "MSE loss: 90.5658\n",
      "Iteration: 78900\n",
      "Gradient: [10.0373  1.0411  8.2735 62.1244 46.8485]\n",
      "Weights: [-4.801   0.803  -1.3557  0.193   0.1296]\n",
      "MSE loss: 90.6022\n",
      "Iteration: 79000\n",
      "Gradient: [  11.592    28.0617   42.0224 -134.7427   55.7183]\n",
      "Weights: [-4.7878  0.8075 -1.3536  0.193   0.1297]\n",
      "MSE loss: 91.245\n",
      "Iteration: 79100\n",
      "Gradient: [  3.4943  -9.0719 -10.8535 169.2996 257.2548]\n",
      "Weights: [-4.7971  0.8098 -1.356   0.1925  0.1297]\n",
      "MSE loss: 90.4191\n",
      "Iteration: 79200\n",
      "Gradient: [  13.1049    1.4491  -12.4311  -72.4214 -767.0658]\n",
      "Weights: [-4.7841  0.7974 -1.3541  0.1929  0.1297]\n",
      "MSE loss: 90.4843\n",
      "Iteration: 79300\n",
      "Gradient: [ -8.5721 -22.2759 -55.4528  71.4173 182.3871]\n",
      "Weights: [-4.7838  0.7863 -1.3526  0.1937  0.1297]\n",
      "MSE loss: 90.5153\n",
      "Iteration: 79400\n",
      "Gradient: [  -3.0873    3.8209  -13.7649  137.5347 -111.1213]\n",
      "Weights: [-4.8016  0.8007 -1.3511  0.193   0.1294]\n",
      "MSE loss: 90.464\n",
      "Iteration: 79500\n",
      "Gradient: [  2.984   20.9141 -51.5505 101.3701 104.732 ]\n",
      "Weights: [-4.7854  0.801  -1.3503  0.1924  0.1294]\n",
      "MSE loss: 90.7136\n",
      "Iteration: 79600\n",
      "Gradient: [  -8.8812   -6.386   -18.2675   -1.4131 -469.5982]\n",
      "Weights: [-4.7947  0.803  -1.3516  0.1913  0.1296]\n",
      "MSE loss: 90.4961\n",
      "Iteration: 79700\n",
      "Gradient: [   5.8059   17.3372  -30.5637   -9.8802 -331.571 ]\n",
      "Weights: [-4.8011  0.8208 -1.355   0.1917  0.1296]\n",
      "MSE loss: 90.6861\n",
      "Iteration: 79800\n",
      "Gradient: [   6.1496    7.3917   21.2126  -43.8918 -223.3969]\n",
      "Weights: [-4.8045  0.8156 -1.3576  0.194   0.1292]\n",
      "MSE loss: 90.4082\n",
      "Iteration: 79900\n",
      "Gradient: [ -7.2068  16.1777 -74.551   13.7891 -91.369 ]\n",
      "Weights: [-4.8014  0.8294 -1.3648  0.1946  0.1291]\n",
      "MSE loss: 90.4894\n",
      "Iteration: 80000\n",
      "Gradient: [ 7.613600e+00 -2.156000e-01 -9.786500e+00 -1.208791e+02 -5.732788e+02]\n",
      "Weights: [-4.7807  0.8235 -1.3675  0.1963  0.129 ]\n",
      "MSE loss: 91.012\n",
      "Iteration: 80100\n",
      "Gradient: [ -4.8218  28.2029  49.2812  64.5189 305.1731]\n",
      "Weights: [-4.8043  0.8278 -1.3726  0.1972  0.1294]\n",
      "MSE loss: 90.3414\n",
      "Iteration: 80200\n",
      "Gradient: [ -6.6767  12.2075  54.8557  23.686  637.8367]\n",
      "Weights: [-4.8291  0.8452 -1.3729  0.1959  0.1294]\n",
      "MSE loss: 90.6274\n",
      "Iteration: 80300\n",
      "Gradient: [ -5.8899  20.0005  11.5371 144.7415 101.3851]\n",
      "Weights: [-4.835   0.8586 -1.3724  0.196   0.1291]\n",
      "MSE loss: 90.7515\n",
      "Iteration: 80400\n",
      "Gradient: [  10.0969   -8.9636   37.1082 -109.1163 -112.1694]\n",
      "Weights: [-4.8099  0.8466 -1.3746  0.1969  0.1292]\n",
      "MSE loss: 90.6068\n",
      "Iteration: 80500\n",
      "Gradient: [ 14.4898  -2.5549  83.7621 -36.9257 272.8829]\n",
      "Weights: [-4.8074  0.8289 -1.3741  0.1979  0.1293]\n",
      "MSE loss: 90.3574\n",
      "Iteration: 80600\n",
      "Gradient: [-18.8445   5.2691 -38.5394  61.1375 516.4789]\n",
      "Weights: [-4.8275  0.8267 -1.3719  0.1987  0.129 ]\n",
      "MSE loss: 91.168\n",
      "Iteration: 80700\n",
      "Gradient: [ -17.5137   12.1746  -58.5514 -137.8408 -252.2082]\n",
      "Weights: [-4.8119  0.8355 -1.373   0.1974  0.1291]\n",
      "MSE loss: 90.3614\n",
      "Iteration: 80800\n",
      "Gradient: [   0.8942  -33.9857  -90.6875  -38.2907 -314.6393]\n",
      "Weights: [-4.7879  0.8268 -1.3724  0.1968  0.129 ]\n",
      "MSE loss: 90.6275\n",
      "Iteration: 80900\n",
      "Gradient: [ 13.0618  18.995   21.6554 130.8216 171.7104]\n",
      "Weights: [-4.7874  0.8279 -1.3714  0.198   0.1291]\n",
      "MSE loss: 91.0858\n",
      "Iteration: 81000\n",
      "Gradient: [  -2.5052    9.3308   -7.2622   -5.0403 -178.8612]\n",
      "Weights: [-4.8068  0.8306 -1.3703  0.198   0.1286]\n",
      "MSE loss: 90.3555\n",
      "Iteration: 81100\n",
      "Gradient: [ 17.9292  11.079   65.7191  11.6788 597.2302]\n",
      "Weights: [-4.7898  0.8186 -1.3709  0.1992  0.1288]\n",
      "MSE loss: 90.4154\n",
      "Iteration: 81200\n",
      "Gradient: [ 16.4959   0.8329  12.3081 148.4594 222.8449]\n",
      "Weights: [-4.7875  0.8255 -1.3751  0.2004  0.1286]\n",
      "MSE loss: 90.6393\n",
      "Iteration: 81300\n",
      "Gradient: [   4.1634  -27.5887   42.2517   10.6037 -328.4444]\n",
      "Weights: [-4.7836  0.8154 -1.3748  0.2008  0.1286]\n",
      "MSE loss: 90.4281\n",
      "Iteration: 81400\n",
      "Gradient: [   5.9586   -7.7414   -8.0016  199.2207 -352.6453]\n",
      "Weights: [-4.7989  0.8202 -1.3727  0.2     0.1289]\n",
      "MSE loss: 90.3658\n",
      "Iteration: 81500\n",
      "Gradient: [  5.9119  -2.1972  63.1134 191.6129 263.5391]\n",
      "Weights: [-4.8023  0.8288 -1.3758  0.2001  0.1289]\n",
      "MSE loss: 90.3478\n",
      "Iteration: 81600\n",
      "Gradient: [ -22.8834  -39.3908  -46.6917 -181.4244  -67.6279]\n",
      "Weights: [-4.8079  0.8264 -1.3785  0.2004  0.1287]\n",
      "MSE loss: 90.925\n",
      "Iteration: 81700\n",
      "Gradient: [ -12.1523   10.9875   28.669   -13.4748 -356.9276]\n",
      "Weights: [-4.8101  0.8324 -1.3876  0.2019  0.1288]\n",
      "MSE loss: 91.7818\n",
      "Iteration: 81800\n",
      "Gradient: [ 15.1519 -43.7349 -25.222   93.9757  35.1332]\n",
      "Weights: [-4.7706  0.8167 -1.3882  0.2035  0.1293]\n",
      "MSE loss: 90.706\n",
      "Iteration: 81900\n",
      "Gradient: [ 25.0124  36.257   17.288   54.6111 -69.7946]\n",
      "Weights: [-4.7776  0.8309 -1.3875  0.2038  0.1291]\n",
      "MSE loss: 91.6739\n",
      "Iteration: 82000\n",
      "Gradient: [-24.8324 -34.229  -91.0431  -1.6857 296.2273]\n",
      "Weights: [-4.8165  0.8405 -1.3882  0.2031  0.1286]\n",
      "MSE loss: 90.7582\n",
      "Iteration: 82100\n",
      "Gradient: [ 15.8014 -17.3464  17.5613  86.6704 460.5379]\n",
      "Weights: [-4.7902  0.8328 -1.3863  0.2036  0.1286]\n",
      "MSE loss: 90.414\n",
      "Iteration: 82200\n",
      "Gradient: [  -7.5233    7.3748   22.7255  104.2123 -114.3126]\n",
      "Weights: [-4.8157  0.8424 -1.3884  0.2032  0.1287]\n",
      "MSE loss: 90.4186\n",
      "Iteration: 82300\n",
      "Gradient: [ 0.4113  0.8036 -4.4893 24.7186 48.437 ]\n",
      "Weights: [-4.8188  0.854  -1.3914  0.2038  0.1284]\n",
      "MSE loss: 90.3079\n",
      "Iteration: 82400\n",
      "Gradient: [   0.3354  -24.6737  -60.3239  -69.6812 -296.5626]\n",
      "Weights: [-4.8259  0.8566 -1.3906  0.2037  0.1284]\n",
      "MSE loss: 90.3727\n",
      "Iteration: 82500\n",
      "Gradient: [ -2.2958  -9.7697 -51.3134  44.2641  73.5784]\n",
      "Weights: [-4.816   0.8652 -1.3964  0.2038  0.1286]\n",
      "MSE loss: 90.2727\n",
      "Iteration: 82600\n",
      "Gradient: [ 3.700000e-02 -2.531100e+00 -1.603900e+00 -5.842860e+01  4.114947e+02]\n",
      "Weights: [-4.83    0.8781 -1.3987  0.2017  0.1289]\n",
      "MSE loss: 90.4902\n",
      "Iteration: 82700\n",
      "Gradient: [   2.129   -17.2033   -3.4656  -19.1809 -165.8011]\n",
      "Weights: [-4.8094  0.8749 -1.3992  0.2013  0.1292]\n",
      "MSE loss: 90.5127\n",
      "Iteration: 82800\n",
      "Gradient: [  -0.7255    6.2959    5.3612  -42.0768 -391.1747]\n",
      "Weights: [-4.8211  0.8591 -1.3986  0.2019  0.1294]\n",
      "MSE loss: 91.086\n",
      "Iteration: 82900\n",
      "Gradient: [   8.1303  -39.4174  -32.5895 -214.8157   69.1865]\n",
      "Weights: [-4.8099  0.858  -1.3993  0.2031  0.1291]\n",
      "MSE loss: 90.4724\n",
      "Iteration: 83000\n",
      "Gradient: [  3.8568  15.6957  15.8125  54.8355 132.6923]\n",
      "Weights: [-4.8073  0.8643 -1.4015  0.2038  0.1292]\n",
      "MSE loss: 90.283\n",
      "Iteration: 83100\n",
      "Gradient: [   7.9019  -22.7547    4.0176   60.4675 -208.0968]\n",
      "Weights: [-4.8209  0.8796 -1.4076  0.2047  0.1293]\n",
      "MSE loss: 90.3028\n",
      "Iteration: 83200\n",
      "Gradient: [-11.6209   0.5542  51.9731  18.1296  34.85  ]\n",
      "Weights: [-4.8255  0.8737 -1.4075  0.2045  0.1292]\n",
      "MSE loss: 90.5612\n",
      "Iteration: 83300\n",
      "Gradient: [ 12.3749 -17.5319 -16.3873  72.7186 -45.3549]\n",
      "Weights: [-4.8148  0.8708 -1.4053  0.2055  0.1289]\n",
      "MSE loss: 90.2342\n",
      "Iteration: 83400\n",
      "Gradient: [   6.3612  -43.051    15.1196  -94.6834 -395.6776]\n",
      "Weights: [-4.8093  0.8682 -1.4062  0.2064  0.1288]\n",
      "MSE loss: 90.2736\n",
      "Iteration: 83500\n",
      "Gradient: [   0.8589    9.3122   37.0315  -16.8207 -276.7959]\n",
      "Weights: [-4.8219  0.8827 -1.4097  0.207   0.1288]\n",
      "MSE loss: 90.4516\n",
      "Iteration: 83600\n",
      "Gradient: [ -10.6991    6.0769   28.9359 -134.0006  159.1954]\n",
      "Weights: [-4.8134  0.8655 -1.407   0.2085  0.1284]\n",
      "MSE loss: 90.2062\n",
      "Iteration: 83700\n",
      "Gradient: [ 11.3849  22.3157  68.4357 172.4797 140.209 ]\n",
      "Weights: [-4.8193  0.8735 -1.4117  0.2086  0.1282]\n",
      "MSE loss: 90.3859\n",
      "Iteration: 83800\n",
      "Gradient: [-10.6284 -21.1104 -66.6988  99.0721 182.4355]\n",
      "Weights: [-4.8144  0.8645 -1.4072  0.2094  0.1279]\n",
      "MSE loss: 90.1769\n",
      "Iteration: 83900\n",
      "Gradient: [  16.2738  -13.156   -97.1675  -14.7356 -310.0042]\n",
      "Weights: [-4.7679  0.8443 -1.406   0.21    0.128 ]\n",
      "MSE loss: 91.3995\n",
      "Iteration: 84000\n",
      "Gradient: [  -4.8354  -11.5082  -20.2795 -275.8191 -180.9697]\n",
      "Weights: [-4.8003  0.8421 -1.4033  0.2089  0.1278]\n",
      "MSE loss: 91.2726\n",
      "Iteration: 84100\n",
      "Gradient: [  1.8152 -21.0794 -51.639    4.8329  41.0808]\n",
      "Weights: [-4.8045  0.8504 -1.4082  0.2107  0.1279]\n",
      "MSE loss: 90.3333\n",
      "Iteration: 84200\n",
      "Gradient: [ 30.7373  -6.8639 -18.12    64.9889 538.5084]\n",
      "Weights: [-4.7903  0.8574 -1.4093  0.21    0.1279]\n",
      "MSE loss: 90.4216\n",
      "Iteration: 84300\n",
      "Gradient: [-10.4495  -6.5018 -22.5626 -78.6054  32.7407]\n",
      "Weights: [-4.8213  0.8744 -1.4133  0.2094  0.1281]\n",
      "MSE loss: 90.3673\n",
      "Iteration: 84400\n",
      "Gradient: [  -2.5989  -48.7994   -9.9546  -92.7797 -259.3015]\n",
      "Weights: [-4.8102  0.8796 -1.4123  0.2088  0.1283]\n",
      "MSE loss: 90.4372\n",
      "Iteration: 84500\n",
      "Gradient: [ 12.3983  19.7157   5.3377 -50.6114  83.6554]\n",
      "Weights: [-4.8104  0.8888 -1.4131  0.2075  0.1283]\n",
      "MSE loss: 90.6649\n",
      "Iteration: 84600\n",
      "Gradient: [ 27.2868  -2.6228  11.0563  43.4842 -70.4841]\n",
      "Weights: [-4.8078  0.8804 -1.412   0.208   0.1284]\n",
      "MSE loss: 90.4731\n",
      "Iteration: 84700\n",
      "Gradient: [ -15.6375  -24.8476   20.6971 -125.3427 -115.4548]\n",
      "Weights: [-4.8102  0.874  -1.4098  0.2072  0.1286]\n",
      "MSE loss: 90.2475\n",
      "Iteration: 84800\n",
      "Gradient: [ -11.7525  -36.85    -45.0164 -141.5549 -440.4038]\n",
      "Weights: [-4.8149  0.8708 -1.4104  0.2083  0.1287]\n",
      "MSE loss: 90.2162\n",
      "Iteration: 84900\n",
      "Gradient: [  -8.5508  -26.858   -25.2153 -113.8785  -93.1996]\n",
      "Weights: [-4.8215  0.8624 -1.4084  0.208   0.1286]\n",
      "MSE loss: 90.6534\n",
      "Iteration: 85000\n",
      "Gradient: [  -3.0369    0.4055   -5.8177  345.4895 -208.0294]\n",
      "Weights: [-4.8074  0.8552 -1.4059  0.2082  0.1285]\n",
      "MSE loss: 90.2506\n",
      "Iteration: 85100\n",
      "Gradient: [ -2.5278 -24.3599 -54.3377  16.3412  22.7823]\n",
      "Weights: [-4.8087  0.859  -1.4052  0.2078  0.1285]\n",
      "MSE loss: 90.1905\n",
      "Iteration: 85200\n",
      "Gradient: [ -3.8802  10.0587  33.3574 107.4458 184.9302]\n",
      "Weights: [-4.8051  0.8615 -1.405   0.2069  0.1286]\n",
      "MSE loss: 90.2246\n",
      "Iteration: 85300\n",
      "Gradient: [ -7.3451   9.6836 -39.7792 -16.0274 158.8858]\n",
      "Weights: [-4.7971  0.8574 -1.406   0.2079  0.1285]\n",
      "MSE loss: 90.3112\n",
      "Iteration: 85400\n",
      "Gradient: [  16.484    -8.9863   12.5291   39.3899 -495.9721]\n",
      "Weights: [-4.8047  0.8632 -1.4064  0.2085  0.1284]\n",
      "MSE loss: 90.4462\n",
      "Iteration: 85500\n",
      "Gradient: [   6.8282   -4.2656  -19.3123   -4.3069 -355.904 ]\n",
      "Weights: [-4.8228  0.8648 -1.4036  0.2075  0.1282]\n",
      "MSE loss: 90.2958\n",
      "Iteration: 85600\n",
      "Gradient: [ -27.8556  -34.372   -16.3703   29.4963 -338.3407]\n",
      "Weights: [-4.8271  0.8669 -1.4054  0.2059  0.1285]\n",
      "MSE loss: 91.1861\n",
      "Iteration: 85700\n",
      "Gradient: [  2.6138  22.3465  73.2971 206.7133  52.5081]\n",
      "Weights: [-4.7991  0.8638 -1.4002  0.2052  0.1287]\n",
      "MSE loss: 90.8757\n",
      "Iteration: 85800\n",
      "Gradient: [ 12.452   24.1996  34.5476 -31.629  -46.4693]\n",
      "Weights: [-4.8187  0.8634 -1.3977  0.2054  0.1287]\n",
      "MSE loss: 90.4742\n",
      "Iteration: 85900\n",
      "Gradient: [  13.609    27.6741   37.1021   -8.8361 -304.2337]\n",
      "Weights: [-4.8067  0.8669 -1.4021  0.2064  0.1284]\n",
      "MSE loss: 90.4483\n",
      "Iteration: 86000\n",
      "Gradient: [  7.91    24.1115 -61.1595 -74.2381 -71.2161]\n",
      "Weights: [-4.7774  0.846  -1.4023  0.2082  0.1282]\n",
      "MSE loss: 90.91\n",
      "Iteration: 86100\n",
      "Gradient: [ 20.5986 -12.0103 -17.149   -6.8249 -52.9586]\n",
      "Weights: [-4.7854  0.8543 -1.4043  0.2088  0.1279]\n",
      "MSE loss: 90.7364\n",
      "Iteration: 86200\n",
      "Gradient: [  6.0669  -6.537    2.3105  24.9092 401.4597]\n",
      "Weights: [-4.8175  0.8585 -1.4038  0.2087  0.128 ]\n",
      "MSE loss: 90.2777\n",
      "Iteration: 86300\n",
      "Gradient: [ -6.3325  45.0186  42.0619  11.273  420.6082]\n",
      "Weights: [-4.8033  0.8577 -1.402   0.2089  0.1278]\n",
      "MSE loss: 90.3408\n",
      "Iteration: 86400\n",
      "Gradient: [   4.1298   17.9464  -28.0048 -100.5064  123.5341]\n",
      "Weights: [-4.8054  0.8347 -1.3977  0.2106  0.1275]\n",
      "MSE loss: 90.3418\n",
      "Iteration: 86500\n",
      "Gradient: [   9.9424    6.705    15.0086 -168.1539 -235.7763]\n",
      "Weights: [-4.7948  0.8412 -1.3977  0.2097  0.1275]\n",
      "MSE loss: 90.2354\n",
      "Iteration: 86600\n",
      "Gradient: [  1.386    3.5933  57.0784 160.0696  11.7684]\n",
      "Weights: [-4.7833  0.8268 -1.3951  0.2106  0.1276]\n",
      "MSE loss: 90.5548\n",
      "Iteration: 86700\n",
      "Gradient: [ -1.3489   9.4224 -20.9744 -44.7444   0.3834]\n",
      "Weights: [-4.8123  0.8325 -1.3998  0.2105  0.1275]\n",
      "MSE loss: 91.7733\n",
      "Iteration: 86800\n",
      "Gradient: [  -9.4053  -27.8075  -61.2638  -57.7924 -159.8076]\n",
      "Weights: [-4.8045  0.8338 -1.3969  0.2109  0.1274]\n",
      "MSE loss: 90.2665\n",
      "Iteration: 86900\n",
      "Gradient: [-11.5545  -7.5578  -7.951   14.8088 264.0003]\n",
      "Weights: [-4.8002  0.8312 -1.394   0.2105  0.1271]\n",
      "MSE loss: 90.2247\n",
      "Iteration: 87000\n",
      "Gradient: [ 16.6051   6.1963  51.6461  68.2681 141.9892]\n",
      "Weights: [-4.7999  0.8365 -1.3916  0.2098  0.127 ]\n",
      "MSE loss: 90.2168\n",
      "Iteration: 87100\n",
      "Gradient: [  -5.8358  -24.98    -57.3028 -109.5437 -214.8981]\n",
      "Weights: [-4.784   0.8233 -1.3881  0.2091  0.127 ]\n",
      "MSE loss: 90.3405\n",
      "Iteration: 87200\n",
      "Gradient: [  9.7193   6.6145  -4.7703 191.7956 296.5659]\n",
      "Weights: [-4.7917  0.8255 -1.3882  0.2092  0.1273]\n",
      "MSE loss: 90.422\n",
      "Iteration: 87300\n",
      "Gradient: [  10.78     -6.1741    0.7664   24.7057 -495.8385]\n",
      "Weights: [-4.8023  0.8349 -1.3894  0.2088  0.1273]\n",
      "MSE loss: 90.279\n",
      "Iteration: 87400\n",
      "Gradient: [ -8.4716   7.8862  33.9863 -10.3466  48.0552]\n",
      "Weights: [-4.7994  0.8272 -1.3907  0.2091  0.1272]\n",
      "MSE loss: 90.3492\n",
      "Iteration: 87500\n",
      "Gradient: [ -22.154   -13.5597  -59.1423 -168.8863   32.7316]\n",
      "Weights: [-4.7982  0.8302 -1.3921  0.2099  0.1269]\n",
      "MSE loss: 90.2742\n",
      "Iteration: 87600\n",
      "Gradient: [   4.7327   16.7042  -18.4886   16.1721 -211.6002]\n",
      "Weights: [-4.7867  0.8328 -1.3959  0.2109  0.1274]\n",
      "MSE loss: 90.6264\n",
      "Iteration: 87700\n",
      "Gradient: [  5.2441  15.8484  35.3957 -90.0752 615.0664]\n",
      "Weights: [-4.8153  0.8437 -1.3923  0.2097  0.1273]\n",
      "MSE loss: 90.4629\n",
      "Iteration: 87800\n",
      "Gradient: [ 13.0285  17.758  -17.5145 -36.2905 -89.1241]\n",
      "Weights: [-4.8152  0.8494 -1.3922  0.2093  0.1266]\n",
      "MSE loss: 90.2388\n",
      "Iteration: 87900\n",
      "Gradient: [ -7.6724  -1.0793  44.6628 100.3546 177.7317]\n",
      "Weights: [-4.8226  0.84   -1.3906  0.2111  0.1264]\n",
      "MSE loss: 90.4364\n",
      "Iteration: 88000\n",
      "Gradient: [  1.4088 -46.835  -31.2237 -13.9756 109.5622]\n",
      "Weights: [-4.7952  0.8394 -1.3904  0.2106  0.1264]\n",
      "MSE loss: 90.5378\n",
      "Iteration: 88100\n",
      "Gradient: [ -8.1389  -4.0417  75.9043  77.5708 214.8015]\n",
      "Weights: [-4.8114  0.8508 -1.395   0.212   0.1263]\n",
      "MSE loss: 90.4503\n",
      "Iteration: 88200\n",
      "Gradient: [  -7.7119  -15.6866  -52.8689 -167.1159  290.1208]\n",
      "Weights: [-4.8339  0.8597 -1.3985  0.2114  0.1264]\n",
      "MSE loss: 90.6762\n",
      "Iteration: 88300\n",
      "Gradient: [   1.522    -9.8466  -58.3306  -27.9077 -355.2012]\n",
      "Weights: [-4.8095  0.8452 -1.3949  0.2112  0.1264]\n",
      "MSE loss: 90.1721\n",
      "Iteration: 88400\n",
      "Gradient: [ -6.1795   6.6199  24.1294 -11.1541 392.2256]\n",
      "Weights: [-4.7981  0.827  -1.3956  0.2133  0.1266]\n",
      "MSE loss: 90.1792\n",
      "Iteration: 88500\n",
      "Gradient: [  -4.7089  -26.5572  -49.4568 -174.8359  -34.7434]\n",
      "Weights: [-4.7904  0.8299 -1.3987  0.2135  0.1268]\n",
      "MSE loss: 90.2176\n",
      "Iteration: 88600\n",
      "Gradient: [ 14.7071  -3.1161  -3.1602  53.7115 141.7216]\n",
      "Weights: [-4.8061  0.8475 -1.3995  0.2129  0.1265]\n",
      "MSE loss: 90.1846\n",
      "Iteration: 88700\n",
      "Gradient: [ 15.6117  31.2221  37.7529  18.6599 362.0105]\n",
      "Weights: [-4.7851  0.8312 -1.3972  0.2132  0.1264]\n",
      "MSE loss: 90.3117\n",
      "Iteration: 88800\n",
      "Gradient: [  -4.5096    3.1556  -22.5446 -111.5508  142.7952]\n",
      "Weights: [-4.7932  0.8329 -1.3968  0.2136  0.1263]\n",
      "MSE loss: 90.1717\n",
      "Iteration: 88900\n",
      "Gradient: [  10.165   -24.473     4.1922 -121.1984   72.3214]\n",
      "Weights: [-4.7905  0.8273 -1.4     0.2152  0.1265]\n",
      "MSE loss: 90.1949\n",
      "Iteration: 89000\n",
      "Gradient: [   2.3228   26.0506  -73.5182  106.8798 -445.8982]\n",
      "Weights: [-4.7907  0.8319 -1.3987  0.2156  0.1263]\n",
      "MSE loss: 90.725\n",
      "Iteration: 89100\n",
      "Gradient: [ 26.1339  12.9037 -31.5011 104.3833 111.1234]\n",
      "Weights: [-4.7675  0.818  -1.3954  0.2144  0.1262]\n",
      "MSE loss: 90.7628\n",
      "Iteration: 89200\n",
      "Gradient: [  1.6698 -28.0941  79.2101 107.2031 419.2249]\n",
      "Weights: [-4.7851  0.8038 -1.3888  0.2154  0.126 ]\n",
      "MSE loss: 90.3252\n",
      "Iteration: 89300\n",
      "Gradient: [  -8.5224  -11.5812  -83.9234 -214.3505 -827.6197]\n",
      "Weights: [-4.7696  0.8066 -1.3908  0.2146  0.1258]\n",
      "MSE loss: 90.4736\n",
      "Iteration: 89400\n",
      "Gradient: [  12.734     1.5384   21.6909   44.8677 -122.8822]\n",
      "Weights: [-4.7911  0.8203 -1.3956  0.2169  0.1257]\n",
      "MSE loss: 90.2382\n",
      "Iteration: 89500\n",
      "Gradient: [  7.9613  26.7449  -4.3933  35.7933 365.4392]\n",
      "Weights: [-4.7885  0.8338 -1.3987  0.2164  0.1255]\n",
      "MSE loss: 90.3613\n",
      "Iteration: 89600\n",
      "Gradient: [-13.701   18.2083 -74.2955  -9.9567 -31.2701]\n",
      "Weights: [-4.8065  0.8318 -1.4005  0.2175  0.1252]\n",
      "MSE loss: 90.416\n",
      "Iteration: 89700\n",
      "Gradient: [ 18.4069 -29.3246 -16.1682 143.3851 -66.2421]\n",
      "Weights: [-4.795   0.842  -1.4032  0.2183  0.1255]\n",
      "MSE loss: 90.8253\n",
      "Iteration: 89800\n",
      "Gradient: [  4.0523  20.6644  46.5808  27.106  443.7823]\n",
      "Weights: [-4.8133  0.8459 -1.4033  0.2178  0.1252]\n",
      "MSE loss: 90.12\n",
      "Iteration: 89900\n",
      "Gradient: [  1.3083  46.4081   6.9297  -5.154  311.4607]\n",
      "Weights: [-4.804   0.8528 -1.408   0.2181  0.1254]\n",
      "MSE loss: 90.1609\n",
      "Iteration: 90000\n",
      "Gradient: [-9.416  -4.9745 94.9382 32.7978 77.9437]\n",
      "Weights: [-4.7939  0.8379 -1.4043  0.2178  0.1257]\n",
      "MSE loss: 90.2104\n",
      "Iteration: 90100\n",
      "Gradient: [ -8.8047  31.6138  14.9199 235.1526 411.4852]\n",
      "Weights: [-4.8246  0.8552 -1.4062  0.2174  0.1256]\n",
      "MSE loss: 90.2465\n",
      "Iteration: 90200\n",
      "Gradient: [  8.3339 -33.2761   8.4121 -10.0719 120.7305]\n",
      "Weights: [-4.7967  0.8436 -1.4051  0.2179  0.1252]\n",
      "MSE loss: 90.143\n",
      "Iteration: 90300\n",
      "Gradient: [  20.7341  -19.8521  -12.26    -93.4165 -296.6495]\n",
      "Weights: [-4.7811  0.8397 -1.4036  0.2187  0.125 ]\n",
      "MSE loss: 90.8983\n",
      "Iteration: 90400\n",
      "Gradient: [ 18.9161  -1.9991  -4.2152  93.8782 144.5447]\n",
      "Weights: [-4.7959  0.8342 -1.4031  0.2186  0.1254]\n",
      "MSE loss: 90.1395\n",
      "Iteration: 90500\n",
      "Gradient: [ 2.449000e-01 -2.117690e+01 -6.478400e+01 -1.117918e+02 -2.458655e+02]\n",
      "Weights: [-4.8019  0.8403 -1.4042  0.2183  0.1252]\n",
      "MSE loss: 90.0903\n",
      "Iteration: 90600\n",
      "Gradient: [-14.9905  20.2898  71.0678 171.8113 221.4917]\n",
      "Weights: [-4.8313  0.8525 -1.4059  0.2185  0.1253]\n",
      "MSE loss: 90.557\n",
      "Iteration: 90700\n",
      "Gradient: [ -3.2329 -37.3318 -93.0742 -99.0159  81.5643]\n",
      "Weights: [-4.8067  0.841  -1.4087  0.2184  0.1254]\n",
      "MSE loss: 90.6856\n",
      "Iteration: 90800\n",
      "Gradient: [-16.1364 -15.2217  60.9989 208.8497 133.9858]\n",
      "Weights: [-4.8206  0.8418 -1.4069  0.2188  0.1255]\n",
      "MSE loss: 90.647\n",
      "Iteration: 90900\n",
      "Gradient: [  18.9012   -0.3862   78.1665    7.0892 -247.9518]\n",
      "Weights: [-4.801   0.8454 -1.4086  0.2185  0.1254]\n",
      "MSE loss: 90.0995\n",
      "Iteration: 91000\n",
      "Gradient: [   8.5445   -1.2586  -46.4661 -118.9504 -226.9273]\n",
      "Weights: [-4.7941  0.8482 -1.411   0.2188  0.1255]\n",
      "MSE loss: 90.1505\n",
      "Iteration: 91100\n",
      "Gradient: [  -8.209    -6.4268   -8.1948 -136.5516 -506.9904]\n",
      "Weights: [-4.8007  0.8449 -1.4058  0.2178  0.1253]\n",
      "MSE loss: 90.1019\n",
      "Iteration: 91200\n",
      "Gradient: [  4.832   25.7642  68.2462 144.366  370.6199]\n",
      "Weights: [-4.7889  0.8373 -1.4035  0.2184  0.1255]\n",
      "MSE loss: 90.5837\n",
      "Iteration: 91300\n",
      "Gradient: [ -12.3715   11.7084  -39.6014  -73.6993 -129.1043]\n",
      "Weights: [-4.8216  0.8354 -1.3974  0.217   0.1255]\n",
      "MSE loss: 90.4884\n",
      "Iteration: 91400\n",
      "Gradient: [-11.793   17.4286  82.4811 -19.3739 -34.9035]\n",
      "Weights: [-4.8363  0.8571 -1.3992  0.2159  0.125 ]\n",
      "MSE loss: 90.6485\n",
      "Iteration: 91500\n",
      "Gradient: [-7.270000e-02 -9.081200e+00 -1.020880e+01 -1.346150e+01 -3.938253e+02]\n",
      "Weights: [-4.7943  0.8375 -1.4048  0.2177  0.1255]\n",
      "MSE loss: 90.1148\n",
      "Iteration: 91600\n",
      "Gradient: [ 10.8421   6.3097  28.9059 114.2261 628.1862]\n",
      "Weights: [-4.7911  0.8334 -1.4028  0.2177  0.1254]\n",
      "MSE loss: 90.1315\n",
      "Iteration: 91700\n",
      "Gradient: [ 10.6995  10.6702  45.3916  65.8797 288.1678]\n",
      "Weights: [-4.7902  0.8423 -1.3982  0.2169  0.1251]\n",
      "MSE loss: 91.0186\n",
      "Iteration: 91800\n",
      "Gradient: [ -28.7409  -26.6298  -56.9197 -294.5974 -141.1549]\n",
      "Weights: [-4.826   0.8503 -1.4014  0.2163  0.1252]\n",
      "MSE loss: 90.6181\n",
      "Iteration: 91900\n",
      "Gradient: [ -1.3641  31.5171  59.9476  16.2366 164.4496]\n",
      "Weights: [-4.7975  0.8331 -1.4002  0.2173  0.1253]\n",
      "MSE loss: 90.1007\n",
      "Iteration: 92000\n",
      "Gradient: [ 22.7393  -6.5668  71.3252 -98.6744  -7.5049]\n",
      "Weights: [-4.7832  0.8314 -1.3992  0.2164  0.1255]\n",
      "MSE loss: 90.3771\n",
      "Iteration: 92100\n",
      "Gradient: [   6.2286  -10.8045  -48.0912   -8.0611 -278.0164]\n",
      "Weights: [-4.8004  0.8299 -1.3994  0.2173  0.1256]\n",
      "MSE loss: 90.1169\n",
      "Iteration: 92200\n",
      "Gradient: [3.229000e-01 1.104370e+01 5.599050e+01 1.357626e+02 6.390375e+02]\n",
      "Weights: [-4.8014  0.8311 -1.4001  0.2181  0.1253]\n",
      "MSE loss: 90.0989\n",
      "Iteration: 92300\n",
      "Gradient: [ -12.9433   24.7805   88.3723  165.3444 -165.8549]\n",
      "Weights: [-4.8086  0.8361 -1.4022  0.2186  0.1253]\n",
      "MSE loss: 90.1421\n",
      "Iteration: 92400\n",
      "Gradient: [   9.5862  -10.9475  -31.1045 -169.3546 -176.6366]\n",
      "Weights: [-4.8137  0.8347 -1.4034  0.2193  0.1252]\n",
      "MSE loss: 90.3598\n",
      "Iteration: 92500\n",
      "Gradient: [-23.8147  -2.4925 -24.5912 -51.3126 295.6793]\n",
      "Weights: [-4.8173  0.8342 -1.4058  0.2189  0.1254]\n",
      "MSE loss: 91.2205\n",
      "Iteration: 92600\n",
      "Gradient: [ -11.669    14.6623   22.3795   19.5575 -225.2946]\n",
      "Weights: [-4.8128  0.8279 -1.4043  0.2179  0.1257]\n",
      "MSE loss: 91.4919\n",
      "Iteration: 92700\n",
      "Gradient: [  6.3808  17.0251  38.7005 -11.4313 241.3028]\n",
      "Weights: [-4.7869  0.8384 -1.4045  0.2171  0.1259]\n",
      "MSE loss: 90.4262\n",
      "Iteration: 92800\n",
      "Gradient: [  -9.1582  -44.9675  -73.0714 -323.5705 -454.3657]\n",
      "Weights: [-4.8283  0.8484 -1.4071  0.2168  0.1257]\n",
      "MSE loss: 91.4695\n",
      "Iteration: 92900\n",
      "Gradient: [-11.8407 -33.6735 -47.5935  69.2144 128.9025]\n",
      "Weights: [-4.8227  0.8541 -1.409   0.2173  0.1258]\n",
      "MSE loss: 90.3786\n",
      "Iteration: 93000\n",
      "Gradient: [  2.4717 -32.8383  -6.066  -35.7984 -94.8461]\n",
      "Weights: [-4.8067  0.8488 -1.4087  0.2174  0.1259]\n",
      "MSE loss: 90.0668\n",
      "Iteration: 93100\n",
      "Gradient: [ -19.8557  -15.9774  -48.6631 -108.3821 -339.9909]\n",
      "Weights: [-4.8275  0.8624 -1.4103  0.2169  0.1257]\n",
      "MSE loss: 90.4784\n",
      "Iteration: 93200\n",
      "Gradient: [ -10.7082   18.0306   25.2636   43.1762 -167.9903]\n",
      "Weights: [-4.818   0.8569 -1.4122  0.2176  0.1261]\n",
      "MSE loss: 90.1674\n",
      "Iteration: 93300\n",
      "Gradient: [ -24.2865  -42.5346   43.3823  104.8007 -383.7231]\n",
      "Weights: [-4.8166  0.8586 -1.4128  0.2169  0.1259]\n",
      "MSE loss: 90.4668\n",
      "Iteration: 93400\n",
      "Gradient: [ 6.00000e-03  3.73390e+00 -9.07240e+00  3.83856e+01  2.90751e+01]\n",
      "Weights: [-4.8107  0.8728 -1.4168  0.2166  0.1262]\n",
      "MSE loss: 90.1453\n",
      "Iteration: 93500\n",
      "Gradient: [  -5.5752  -21.8602  -31.8786 -105.3582  172.5006]\n",
      "Weights: [-4.8236  0.8635 -1.4158  0.2165  0.1262]\n",
      "MSE loss: 90.9941\n",
      "Iteration: 93600\n",
      "Gradient: [   8.6992    1.8569 -123.6707    4.7709 -582.5399]\n",
      "Weights: [-4.8105  0.8674 -1.412   0.2159  0.126 ]\n",
      "MSE loss: 90.1552\n",
      "Iteration: 93700\n",
      "Gradient: [ 18.4487  28.9899  37.4765 198.5638 235.3219]\n",
      "Weights: [-4.807   0.8684 -1.4144  0.2168  0.1263]\n",
      "MSE loss: 90.4771\n",
      "Iteration: 93800\n",
      "Gradient: [ 14.1649  -1.726  -20.0815 116.6414 488.2173]\n",
      "Weights: [-4.805   0.8609 -1.4153  0.2182  0.1261]\n",
      "MSE loss: 90.1733\n",
      "Iteration: 93900\n",
      "Gradient: [   6.373   -11.1599    5.9736  -53.3821 -330.0812]\n",
      "Weights: [-4.802   0.8656 -1.4174  0.2185  0.1261]\n",
      "MSE loss: 90.4074\n",
      "Iteration: 94000\n",
      "Gradient: [  9.1902  40.9801 -13.7464  90.1797 510.8312]\n",
      "Weights: [-4.8138  0.8782 -1.4186  0.2188  0.126 ]\n",
      "MSE loss: 90.847\n",
      "Iteration: 94100\n",
      "Gradient: [ -5.3633  -3.5992  -5.0949 156.9265 117.2073]\n",
      "Weights: [-4.8096  0.8604 -1.4198  0.2198  0.1259]\n",
      "MSE loss: 90.0503\n",
      "Iteration: 94200\n",
      "Gradient: [-16.5553 -34.4662 -17.0363 158.623  100.4211]\n",
      "Weights: [-4.8299  0.8705 -1.4175  0.2191  0.1257]\n",
      "MSE loss: 90.2664\n",
      "Iteration: 94300\n",
      "Gradient: [ -14.4446  -16.2889  -30.052   -73.4383 -506.0125]\n",
      "Weights: [-4.8235  0.8663 -1.4214  0.2195  0.1259]\n",
      "MSE loss: 90.4996\n",
      "Iteration: 94400\n",
      "Gradient: [  -0.2956   14.7233   44.     -167.6786  -76.8835]\n",
      "Weights: [-4.8163  0.884  -1.4244  0.2195  0.126 ]\n",
      "MSE loss: 90.3541\n",
      "Iteration: 94500\n",
      "Gradient: [  1.866   14.5239   0.66   -40.0768   9.1135]\n",
      "Weights: [-4.8268  0.8924 -1.4297  0.2183  0.1262]\n",
      "MSE loss: 90.2525\n",
      "Iteration: 94600\n",
      "Gradient: [ 13.8795 -40.6874  15.948   90.1741 351.1388]\n",
      "Weights: [-4.8187  0.9008 -1.4316  0.2196  0.1263]\n",
      "MSE loss: 90.7685\n",
      "Iteration: 94700\n",
      "Gradient: [  -4.8732    5.8459  -76.6018 -102.179  -461.5409]\n",
      "Weights: [-4.7975  0.8875 -1.4326  0.2195  0.1267]\n",
      "MSE loss: 90.9447\n",
      "Iteration: 94800\n",
      "Gradient: [  8.3671  -1.1164 -34.4998 -61.2988 668.7205]\n",
      "Weights: [-4.806   0.8779 -1.4287  0.2202  0.1263]\n",
      "MSE loss: 90.1779\n",
      "Iteration: 94900\n",
      "Gradient: [   3.1706   -7.9863   84.8659 -141.4976 -226.2076]\n",
      "Weights: [-4.8163  0.8784 -1.4294  0.22    0.1264]\n",
      "MSE loss: 90.0378\n",
      "Iteration: 95000\n",
      "Gradient: [  -4.3103  -10.9046  -70.1108  -76.9871 -243.5303]\n",
      "Weights: [-4.8202  0.8841 -1.434   0.22    0.1265]\n",
      "MSE loss: 90.2953\n",
      "Iteration: 95100\n",
      "Gradient: [   2.5204  -15.5267   -8.5244  -17.9127 -398.0519]\n",
      "Weights: [-4.8113  0.874  -1.4263  0.2206  0.1261]\n",
      "MSE loss: 90.0536\n",
      "Iteration: 95200\n",
      "Gradient: [  15.5626  -62.2248  -27.9671 -388.2273 -339.0411]\n",
      "Weights: [-4.8075  0.8785 -1.4259  0.221   0.1256]\n",
      "MSE loss: 90.2378\n",
      "Iteration: 95300\n",
      "Gradient: [ -2.9687  24.9979  -3.1372  98.4568 192.8231]\n",
      "Weights: [-4.8083  0.8717 -1.4311  0.2216  0.1259]\n",
      "MSE loss: 90.2586\n",
      "Iteration: 95400\n",
      "Gradient: [  5.6567   8.9097  33.8682 182.5355 176.7806]\n",
      "Weights: [-4.8039  0.8862 -1.4356  0.2225  0.1261]\n",
      "MSE loss: 90.3686\n",
      "Iteration: 95500\n",
      "Gradient: [  -6.2727   -7.5067  -20.2417  -93.5328 -119.6888]\n",
      "Weights: [-4.8274  0.8801 -1.4385  0.223   0.1264]\n",
      "MSE loss: 90.6463\n",
      "Iteration: 95600\n",
      "Gradient: [ -0.       7.1436  13.3982  89.7072 193.0865]\n",
      "Weights: [-4.8075  0.8858 -1.4373  0.2241  0.1259]\n",
      "MSE loss: 90.3644\n",
      "Iteration: 95700\n",
      "Gradient: [  -4.3375  -18.6931  -54.2063  -93.494  -430.7875]\n",
      "Weights: [-4.8031  0.8727 -1.4349  0.2243  0.1256]\n",
      "MSE loss: 89.9968\n",
      "Iteration: 95800\n",
      "Gradient: [-1.346000e+00 -3.296000e-01  5.128430e+01  1.208216e+02  6.014627e+02]\n",
      "Weights: [-4.8048  0.8704 -1.4376  0.2256  0.1256]\n",
      "MSE loss: 90.0441\n",
      "Iteration: 95900\n",
      "Gradient: [  2.1971   8.47    75.2369 128.5488 609.0244]\n",
      "Weights: [-4.8075  0.8803 -1.4384  0.2252  0.1256]\n",
      "MSE loss: 89.9943\n",
      "Iteration: 96000\n",
      "Gradient: [   4.1094   -0.5928    0.4228 -285.0259 -378.776 ]\n",
      "Weights: [-4.7953  0.8746 -1.4382  0.2243  0.1258]\n",
      "MSE loss: 90.1245\n",
      "Iteration: 96100\n",
      "Gradient: [-12.982   14.1934 -40.5718  77.9931  -7.8373]\n",
      "Weights: [-4.8146  0.8817 -1.4379  0.225   0.1255]\n",
      "MSE loss: 89.9597\n",
      "Iteration: 96200\n",
      "Gradient: [   5.3644   -8.5854  -82.6959 -190.757  -341.6303]\n",
      "Weights: [-4.8076  0.8955 -1.4425  0.2253  0.1255]\n",
      "MSE loss: 90.2835\n",
      "Iteration: 96300\n",
      "Gradient: [  8.9209  -2.5795  17.5229  52.0391 225.1824]\n",
      "Weights: [-4.8132  0.894  -1.4429  0.2257  0.1257]\n",
      "MSE loss: 90.208\n",
      "Iteration: 96400\n",
      "Gradient: [-12.1332   7.4509 -24.1773  91.5684 -36.5843]\n",
      "Weights: [-4.8282  0.8871 -1.4474  0.2268  0.1257]\n",
      "MSE loss: 90.8871\n",
      "Iteration: 96500\n",
      "Gradient: [ -8.8539 -17.6885 -30.5969  43.8668  25.6789]\n",
      "Weights: [-4.8244  0.8878 -1.4459  0.2272  0.1253]\n",
      "MSE loss: 90.4587\n",
      "Iteration: 96600\n",
      "Gradient: [   1.6994  -52.9836  -59.3457 -221.1486 -867.6262]\n",
      "Weights: [-4.8082  0.8964 -1.4546  0.2279  0.1255]\n",
      "MSE loss: 90.2653\n",
      "Iteration: 96700\n",
      "Gradient: [  -9.6268   -3.6625   58.4737  190.2475 -118.8357]\n",
      "Weights: [-4.8255  0.8986 -1.4541  0.2289  0.1255]\n",
      "MSE loss: 90.149\n",
      "Iteration: 96800\n",
      "Gradient: [  7.7317 -24.2919  18.8981 -58.0376 150.397 ]\n",
      "Weights: [-4.8122  0.8938 -1.4505  0.2298  0.1249]\n",
      "MSE loss: 89.9005\n",
      "Iteration: 96900\n",
      "Gradient: [  6.8821  12.9423  -4.7729 -62.4539 226.0272]\n",
      "Weights: [-4.8271  0.9012 -1.4507  0.2313  0.1244]\n",
      "MSE loss: 89.9602\n",
      "Iteration: 97000\n",
      "Gradient: [ 10.6493 -15.9131 -33.9724  33.1907 520.9937]\n",
      "Weights: [-4.8052  0.894  -1.4519  0.2321  0.1244]\n",
      "MSE loss: 90.2297\n",
      "Iteration: 97100\n",
      "Gradient: [ 17.6416 -10.3153 -32.2562  27.0944 332.1261]\n",
      "Weights: [-4.8036  0.8923 -1.4519  0.232   0.1243]\n",
      "MSE loss: 90.0586\n",
      "Iteration: 97200\n",
      "Gradient: [ 14.1017  31.9953 -38.7321  37.8034 159.2315]\n",
      "Weights: [-4.8046  0.8929 -1.4563  0.2338  0.1243]\n",
      "MSE loss: 90.0344\n",
      "Iteration: 97300\n",
      "Gradient: [ -0.6702 -11.2439 -74.8774 -82.4987 -83.8952]\n",
      "Weights: [-4.7984  0.8854 -1.4538  0.2327  0.1246]\n",
      "MSE loss: 90.0409\n",
      "Iteration: 97400\n",
      "Gradient: [-11.7892 -21.5589  -0.7883 -56.3647 231.1524]\n",
      "Weights: [-4.8173  0.893  -1.4547  0.232   0.1244]\n",
      "MSE loss: 90.2113\n",
      "Iteration: 97500\n",
      "Gradient: [ 19.0961  15.7683   9.1687  26.3735 -16.227 ]\n",
      "Weights: [-4.811   0.9045 -1.4561  0.2321  0.1244]\n",
      "MSE loss: 90.0806\n",
      "Iteration: 97600\n",
      "Gradient: [-16.9591  23.7066  97.0059 172.2437  62.069 ]\n",
      "Weights: [-4.8476  0.9094 -1.4569  0.2327  0.1244]\n",
      "MSE loss: 90.4837\n",
      "Iteration: 97700\n",
      "Gradient: [ 14.4331  46.5986  19.8763  -0.3151 181.0168]\n",
      "Weights: [-4.8125  0.9014 -1.4579  0.2316  0.1245]\n",
      "MSE loss: 90.009\n",
      "Iteration: 97800\n",
      "Gradient: [ -15.5359   19.6222   -9.1741  -21.294  -128.3072]\n",
      "Weights: [-4.8348  0.9161 -1.4597  0.2322  0.1245]\n",
      "MSE loss: 89.926\n",
      "Iteration: 97900\n",
      "Gradient: [  4.3746  10.7874 -98.5687 141.6808 -49.2477]\n",
      "Weights: [-4.8302  0.9123 -1.4599  0.2321  0.1244]\n",
      "MSE loss: 89.9839\n",
      "Iteration: 98000\n",
      "Gradient: [ -1.5098  -5.4524 -91.3145 -19.8367 -94.1549]\n",
      "Weights: [-4.8301  0.9142 -1.4593  0.2308  0.1245]\n",
      "MSE loss: 90.2597\n",
      "Iteration: 98100\n",
      "Gradient: [ -3.9737 -25.2916 -33.563   29.3482 142.1585]\n",
      "Weights: [-4.8211  0.9216 -1.4603  0.2311  0.1248]\n",
      "MSE loss: 90.2768\n",
      "Iteration: 98200\n",
      "Gradient: [  -5.2201   15.0234  -20.9137  180.9721 -376.7255]\n",
      "Weights: [-4.828   0.9214 -1.4632  0.2306  0.1249]\n",
      "MSE loss: 89.9807\n",
      "Iteration: 98300\n",
      "Gradient: [  19.6642   24.5244  -15.2884  120.2742 -192.5291]\n",
      "Weights: [-4.8202  0.9151 -1.4642  0.231   0.1253]\n",
      "MSE loss: 89.9063\n",
      "Iteration: 98400\n",
      "Gradient: [ -1.208   -0.6278  59.6082 -72.4023  62.8687]\n",
      "Weights: [-4.8224  0.9128 -1.4659  0.2309  0.1257]\n",
      "MSE loss: 89.9562\n",
      "Iteration: 98500\n",
      "Gradient: [  6.581  -18.3715 -23.8986 -16.3767 215.7744]\n",
      "Weights: [-4.8258  0.9201 -1.4685  0.2309  0.1258]\n",
      "MSE loss: 89.9333\n",
      "Iteration: 98600\n",
      "Gradient: [  6.4201   2.7968  48.0671 371.4775 138.9181]\n",
      "Weights: [-4.8475  0.9395 -1.4696  0.2305  0.1256]\n",
      "MSE loss: 90.0717\n",
      "Iteration: 98700\n",
      "Gradient: [ -6.8962  -3.6924 -12.9607 -86.8218 216.8313]\n",
      "Weights: [-4.833   0.9264 -1.469   0.231   0.1255]\n",
      "MSE loss: 89.9708\n",
      "Iteration: 98800\n",
      "Gradient: [  -1.3595   10.4227   63.6918  -72.5505 -257.6867]\n",
      "Weights: [-4.819   0.9286 -1.47    0.2314  0.1257]\n",
      "MSE loss: 90.3221\n",
      "Iteration: 98900\n",
      "Gradient: [ 14.1081  -8.254    3.5162  34.8502 335.9414]\n",
      "Weights: [-4.8326  0.9336 -1.4684  0.2304  0.1257]\n",
      "MSE loss: 90.0079\n",
      "Iteration: 99000\n",
      "Gradient: [-1.23000e-02 -1.86400e-01 -3.76352e+01  8.91540e+01  3.69057e+02]\n",
      "Weights: [-4.8124  0.9298 -1.4694  0.2298  0.1258]\n",
      "MSE loss: 90.3705\n",
      "Iteration: 99100\n",
      "Gradient: [ -5.0222  11.2077  31.181   68.8419 200.3716]\n",
      "Weights: [-4.8513  0.948  -1.4709  0.2293  0.1257]\n",
      "MSE loss: 90.129\n",
      "Iteration: 99200\n",
      "Gradient: [ -3.9275 -25.6896 -39.2979  -5.751  -39.2465]\n",
      "Weights: [-4.8274  0.9326 -1.4714  0.2301  0.1258]\n",
      "MSE loss: 89.9455\n",
      "Iteration: 99300\n",
      "Gradient: [ -21.8479  -13.5379  -48.8695  -29.0621 -134.6163]\n",
      "Weights: [-4.8375  0.9389 -1.4763  0.2302  0.126 ]\n",
      "MSE loss: 90.3129\n",
      "Iteration: 99400\n",
      "Gradient: [ -6.6291  -3.2691 -59.4328 105.9177 189.321 ]\n",
      "Weights: [-4.8093  0.9233 -1.4762  0.2327  0.126 ]\n",
      "MSE loss: 90.1611\n",
      "Iteration: 99500\n",
      "Gradient: [-10.6054  -3.2127  32.3166  59.2414 146.1229]\n",
      "Weights: [-4.8442  0.9282 -1.4741  0.2328  0.1259]\n",
      "MSE loss: 90.3335\n",
      "Iteration: 99600\n",
      "Gradient: [  1.6142   9.107   13.1073  40.7    509.3643]\n",
      "Weights: [-4.8471  0.9421 -1.4783  0.2331  0.1258]\n",
      "MSE loss: 90.1403\n",
      "Iteration: 99700\n",
      "Gradient: [  6.6304   6.8469  68.0895 135.5764 546.6595]\n",
      "Weights: [-4.8417  0.9305 -1.474   0.2329  0.1256]\n",
      "MSE loss: 90.124\n",
      "Iteration: 99800\n",
      "Gradient: [   8.4363   -4.0292   28.151  -104.3231   63.7225]\n",
      "Weights: [-4.8063  0.9138 -1.4714  0.2331  0.1255]\n",
      "MSE loss: 90.0516\n",
      "Iteration: 99900\n",
      "Gradient: [ -23.5291   26.8559  -28.1334  -16.1502 -247.8783]\n",
      "Weights: [-4.8292  0.9258 -1.4731  0.2332  0.1252]\n",
      "MSE loss: 89.9464\n",
      "Iteration: 100000\n",
      "Gradient: [ -4.055   -4.2529  -1.3598 -82.5465 298.9397]\n",
      "Weights: [-4.8111  0.9282 -1.4735  0.233   0.1254]\n",
      "MSE loss: 90.3886\n",
      "Iteration: 100100\n",
      "Gradient: [   5.0222  -21.256     7.5732  -37.0367 -101.4054]\n",
      "Weights: [-4.8158  0.925  -1.4745  0.234   0.1252]\n",
      "MSE loss: 89.9772\n",
      "Iteration: 100200\n",
      "Gradient: [   3.8162   -5.6372    3.3288  173.3202 -417.9715]\n",
      "Weights: [-4.8382  0.9226 -1.4698  0.234   0.1247]\n",
      "MSE loss: 90.1587\n",
      "Iteration: 100300\n",
      "Gradient: [   3.9526  -28.9657  -69.1883 -145.4099  252.9685]\n",
      "Weights: [-4.8363  0.9233 -1.4706  0.233   0.1251]\n",
      "MSE loss: 90.1984\n",
      "Iteration: 100400\n",
      "Gradient: [   2.0865   18.5599  -64.6684  -15.9128 -200.0987]\n",
      "Weights: [-4.8415  0.9329 -1.4713  0.2333  0.1249]\n",
      "MSE loss: 89.9962\n",
      "Iteration: 100500\n",
      "Gradient: [  -9.1153   -4.5538  -46.8572  -36.239  -272.5844]\n",
      "Weights: [-4.8426  0.9356 -1.473   0.2325  0.1251]\n",
      "MSE loss: 90.1216\n",
      "Iteration: 100600\n",
      "Gradient: [  0.4701   6.3462   4.68   157.1115 360.4167]\n",
      "Weights: [-4.8356  0.9399 -1.4767  0.2343  0.1251]\n",
      "MSE loss: 89.9561\n",
      "Iteration: 100700\n",
      "Gradient: [  -2.9143   -1.1892  -90.9765  -93.1358 -406.7944]\n",
      "Weights: [-4.8235  0.9199 -1.4727  0.2337  0.1253]\n",
      "MSE loss: 89.8919\n",
      "Iteration: 100800\n",
      "Gradient: [  4.897  -15.4047   8.9556 136.7258 136.1687]\n",
      "Weights: [-4.8093  0.9061 -1.4697  0.2338  0.1254]\n",
      "MSE loss: 89.9543\n",
      "Iteration: 100900\n",
      "Gradient: [   4.8638   18.3444    4.2539 -100.7494  261.6701]\n",
      "Weights: [-4.8297  0.9198 -1.47    0.2335  0.125 ]\n",
      "MSE loss: 89.961\n",
      "Iteration: 101000\n",
      "Gradient: [-21.9387 -63.0269 -97.7347 155.2473  35.1254]\n",
      "Weights: [-4.8362  0.9057 -1.4685  0.2335  0.1254]\n",
      "MSE loss: 91.1318\n",
      "Iteration: 101100\n",
      "Gradient: [  5.1667  12.042  -38.0836  15.7299 470.5741]\n",
      "Weights: [-4.8113  0.9108 -1.4685  0.2351  0.1251]\n",
      "MSE loss: 90.4737\n",
      "Iteration: 101200\n",
      "Gradient: [ 32.1859   5.996  -36.1341  96.6682  -2.1726]\n",
      "Weights: [-4.7988  0.9149 -1.4674  0.2355  0.1243]\n",
      "MSE loss: 91.2034\n",
      "Iteration: 101300\n",
      "Gradient: [ 10.1281 -17.98    -8.1155  64.2403  52.8681]\n",
      "Weights: [-4.8217  0.9097 -1.4678  0.235   0.1245]\n",
      "MSE loss: 89.956\n",
      "Iteration: 101400\n",
      "Gradient: [  -7.9472    4.0998  -65.4596 -116.4719 -263.5877]\n",
      "Weights: [-4.8411  0.9148 -1.467   0.235   0.1244]\n",
      "MSE loss: 90.385\n",
      "Iteration: 101500\n",
      "Gradient: [  12.9982   -3.8294   65.8918  -12.6446 -263.7316]\n",
      "Weights: [-4.8056  0.9143 -1.4651  0.2334  0.1243]\n",
      "MSE loss: 90.2024\n",
      "Iteration: 101600\n",
      "Gradient: [  8.9486   4.8384  31.5817  74.7059 485.1519]\n",
      "Weights: [-4.8277  0.9207 -1.466   0.233   0.1245]\n",
      "MSE loss: 89.9257\n",
      "Iteration: 101700\n",
      "Gradient: [ -17.9432   26.7052    5.7652   38.6326 -523.4112]\n",
      "Weights: [-4.8085  0.9067 -1.4629  0.2346  0.1243]\n",
      "MSE loss: 90.0521\n",
      "Iteration: 101800\n",
      "Gradient: [ -22.8279   -5.7245   31.8784 -138.6203 -225.9742]\n",
      "Weights: [-4.827   0.9007 -1.4598  0.2334  0.1242]\n",
      "MSE loss: 90.4398\n",
      "Iteration: 101900\n",
      "Gradient: [  5.7404 -11.2838  67.8125  26.7467 -69.9476]\n",
      "Weights: [-4.8112  0.8924 -1.4526  0.233   0.1244]\n",
      "MSE loss: 90.1736\n",
      "Iteration: 102000\n",
      "Gradient: [ -0.7614 -34.4085 -58.1315 -68.1596 -31.0634]\n",
      "Weights: [-4.8388  0.9096 -1.4568  0.2331  0.124 ]\n",
      "MSE loss: 90.1424\n",
      "Iteration: 102100\n",
      "Gradient: [  -2.7005  -13.6406  -51.3862  -88.6669 -125.4768]\n",
      "Weights: [-4.8182  0.8972 -1.4557  0.2334  0.1241]\n",
      "MSE loss: 89.8441\n",
      "Iteration: 102200\n",
      "Gradient: [ -3.0755  -3.5277 -10.002   39.7054 239.6541]\n",
      "Weights: [-4.8203  0.9003 -1.4544  0.233   0.1241]\n",
      "MSE loss: 89.8611\n",
      "Iteration: 102300\n",
      "Gradient: [  10.6358  -40.6662 -133.426   -59.9477 -208.0073]\n",
      "Weights: [-4.8184  0.8991 -1.4558  0.2342  0.1237]\n",
      "MSE loss: 89.8305\n",
      "Iteration: 102400\n",
      "Gradient: [   7.5261  -18.0767  -79.3976 -384.2521 -583.9745]\n",
      "Weights: [-4.8074  0.8925 -1.4563  0.2353  0.1237]\n",
      "MSE loss: 89.8889\n",
      "Iteration: 102500\n",
      "Gradient: [ 1.120560e+01  1.323000e-01 -5.364400e+01  6.749460e+01 -4.270469e+02]\n",
      "Weights: [-4.8074  0.8938 -1.4559  0.2356  0.1236]\n",
      "MSE loss: 90.0158\n",
      "Iteration: 102600\n",
      "Gradient: [-27.5769  -7.0586 -28.9783   9.4983 -57.9047]\n",
      "Weights: [-4.8306  0.8998 -1.4556  0.2353  0.1238]\n",
      "MSE loss: 89.9936\n",
      "Iteration: 102700\n",
      "Gradient: [ 6.3845  4.2168 26.0406 66.1557 92.6959]\n",
      "Weights: [-4.8189  0.8881 -1.4529  0.2348  0.1235]\n",
      "MSE loss: 90.0521\n",
      "Iteration: 102800\n",
      "Gradient: [ -10.7924   -0.3865  -14.5616  -22.7726 -163.1723]\n",
      "Weights: [-4.8163  0.8822 -1.4488  0.2345  0.1235]\n",
      "MSE loss: 89.8741\n",
      "Iteration: 102900\n",
      "Gradient: [-23.6933  -0.7743  14.4812 254.7296  19.33  ]\n",
      "Weights: [-4.8086  0.8726 -1.4457  0.2347  0.1234]\n",
      "MSE loss: 89.8474\n",
      "Iteration: 103000\n",
      "Gradient: [-19.7009  28.4953 -57.8489  64.2143 -12.2894]\n",
      "Weights: [-4.8151  0.8703 -1.4463  0.2348  0.1235]\n",
      "MSE loss: 90.1726\n",
      "Iteration: 103100\n",
      "Gradient: [  2.0472 -14.2099   7.0676 -64.2847  55.1154]\n",
      "Weights: [-4.7905  0.8715 -1.4472  0.2343  0.1236]\n",
      "MSE loss: 90.0844\n",
      "Iteration: 103200\n",
      "Gradient: [ 5.58290e+00  8.30000e-03  4.85017e+01 -7.02204e+01 -9.84527e+01]\n",
      "Weights: [-4.8081  0.883  -1.4474  0.2343  0.1234]\n",
      "MSE loss: 89.8953\n",
      "Iteration: 103300\n",
      "Gradient: [  17.606    13.8703  -51.7391  -77.9752 -305.4647]\n",
      "Weights: [-4.8145  0.8833 -1.4445  0.2348  0.1227]\n",
      "MSE loss: 89.8505\n",
      "Iteration: 103400\n",
      "Gradient: [  -8.1095  -23.4184  -22.6937  -86.5421 -299.164 ]\n",
      "Weights: [-4.8292  0.8929 -1.4526  0.2358  0.1232]\n",
      "MSE loss: 89.9951\n",
      "Iteration: 103500\n",
      "Gradient: [  10.8827  -17.7309  -55.2407  -89.1137 -363.7546]\n",
      "Weights: [-4.8019  0.8919 -1.4537  0.2346  0.1234]\n",
      "MSE loss: 90.0354\n",
      "Iteration: 103600\n",
      "Gradient: [ -17.4584    9.667   -34.8172  -68.9509 -256.9833]\n",
      "Weights: [-4.821   0.8992 -1.4564  0.2363  0.1234]\n",
      "MSE loss: 89.8678\n",
      "Iteration: 103700\n",
      "Gradient: [  -8.6855   -6.3278   13.1054 -147.447  -418.6873]\n",
      "Weights: [-4.8272  0.8943 -1.4586  0.2369  0.1233]\n",
      "MSE loss: 90.2046\n",
      "Iteration: 103800\n",
      "Gradient: [  8.7532  15.223  -15.4213   7.7148 -90.3243]\n",
      "Weights: [-4.8138  0.888  -1.4605  0.2383  0.1233]\n",
      "MSE loss: 89.8605\n",
      "Iteration: 103900\n",
      "Gradient: [ 19.3048  26.3107  68.1429 -13.9294 -31.4424]\n",
      "Weights: [-4.7986  0.8889 -1.4576  0.2373  0.1237]\n",
      "MSE loss: 90.6599\n",
      "Iteration: 104000\n",
      "Gradient: [ 20.5577  26.4201   1.6468 182.7916 236.9201]\n",
      "Weights: [-4.8084  0.8894 -1.4617  0.2379  0.1236]\n",
      "MSE loss: 89.8219\n",
      "Iteration: 104100\n",
      "Gradient: [ -0.9767 -11.6311  67.7286 327.5899 -87.0141]\n",
      "Weights: [-4.7975  0.8939 -1.4632  0.238   0.1238]\n",
      "MSE loss: 90.4983\n",
      "Iteration: 104200\n",
      "Gradient: [-16.5876  16.8295  36.3183 137.256   51.8317]\n",
      "Weights: [-4.8139  0.8897 -1.4605  0.2382  0.1234]\n",
      "MSE loss: 89.8158\n",
      "Iteration: 104300\n",
      "Gradient: [ -14.1768  -25.7444  -22.8846 -171.6985 -296.5373]\n",
      "Weights: [-4.8099  0.8927 -1.4561  0.237   0.1232]\n",
      "MSE loss: 89.8854\n",
      "Iteration: 104400\n",
      "Gradient: [  9.618    5.5139  -1.313   89.3819 256.8281]\n",
      "Weights: [-4.8092  0.8996 -1.4598  0.2374  0.1232]\n",
      "MSE loss: 89.9818\n",
      "Iteration: 104500\n",
      "Gradient: [  0.8365  25.5983  -5.1811 208.0617 -33.3321]\n",
      "Weights: [-4.8175  0.8803 -1.4575  0.238   0.1232]\n",
      "MSE loss: 90.375\n",
      "Iteration: 104600\n",
      "Gradient: [ 6.33410e+00 -7.29030e+00  2.21000e-02 -5.96123e+01  7.48721e+01]\n",
      "Weights: [-4.7807  0.8763 -1.4556  0.2379  0.1231]\n",
      "MSE loss: 90.4786\n",
      "Iteration: 104700\n",
      "Gradient: [ -37.552    -3.2725    9.5093 -136.0858 -194.6423]\n",
      "Weights: [-4.8164  0.8713 -1.4533  0.2379  0.1231]\n",
      "MSE loss: 90.5815\n",
      "Iteration: 104800\n",
      "Gradient: [ 14.7028  31.0743 115.8469 106.1609 513.9406]\n",
      "Weights: [-4.7789  0.8739 -1.4503  0.2376  0.123 ]\n",
      "MSE loss: 91.4667\n",
      "Iteration: 104900\n",
      "Gradient: [ -9.581    5.9906 -28.556  160.6611  46.2159]\n",
      "Weights: [-4.82    0.8983 -1.4568  0.2374  0.1231]\n",
      "MSE loss: 89.9657\n",
      "Iteration: 105000\n",
      "Gradient: [ -12.1719    4.1278  -47.4676  -47.0161 -190.2762]\n",
      "Weights: [-4.832   0.8997 -1.4552  0.2357  0.1232]\n",
      "MSE loss: 90.0614\n",
      "Iteration: 105100\n",
      "Gradient: [-10.8866  24.0685 -49.7873 113.5406  34.1491]\n",
      "Weights: [-4.8272  0.8801 -1.4495  0.2358  0.1231]\n",
      "MSE loss: 90.4674\n",
      "Iteration: 105200\n",
      "Gradient: [  -6.9701   -5.9105  -98.3438 -242.2971 -135.0697]\n",
      "Weights: [-4.8013  0.8827 -1.4499  0.235   0.1233]\n",
      "MSE loss: 89.9397\n",
      "Iteration: 105300\n",
      "Gradient: [  3.9267   2.4927  12.7779 178.2682 342.3765]\n",
      "Weights: [-4.8058  0.8821 -1.4449  0.2342  0.123 ]\n",
      "MSE loss: 89.947\n",
      "Iteration: 105400\n",
      "Gradient: [-2.202200e+00  7.130000e-02 -6.356490e+01  1.570841e+02 -6.399381e+02]\n",
      "Weights: [-4.8112  0.8755 -1.4427  0.2345  0.123 ]\n",
      "MSE loss: 89.836\n",
      "Iteration: 105500\n",
      "Gradient: [ -7.1389 -18.2625 -44.8481 -21.7815 157.6448]\n",
      "Weights: [-4.819   0.8766 -1.4421  0.2335  0.1233]\n",
      "MSE loss: 89.907\n",
      "Iteration: 105600\n",
      "Gradient: [  -6.8843    7.7083  -38.633   -10.404  -556.1646]\n",
      "Weights: [-4.8308  0.8712 -1.442   0.2344  0.123 ]\n",
      "MSE loss: 90.9878\n",
      "Iteration: 105700\n",
      "Gradient: [ -8.4729   2.2799  21.0777   2.294  139.9321]\n",
      "Weights: [-4.8453  0.8848 -1.443   0.2343  0.123 ]\n",
      "MSE loss: 90.8302\n",
      "Iteration: 105800\n",
      "Gradient: [ -3.5103   4.0515  47.9271 -62.7953 -14.5645]\n",
      "Weights: [-4.8186  0.8681 -1.4421  0.235   0.123 ]\n",
      "MSE loss: 90.1404\n",
      "Iteration: 105900\n",
      "Gradient: [  8.8839 -15.6052   2.6458  71.7522 229.6619]\n",
      "Weights: [-4.8219  0.8866 -1.4426  0.2358  0.1225]\n",
      "MSE loss: 90.1701\n",
      "Iteration: 106000\n",
      "Gradient: [ 18.6145  25.6997  42.223   60.3395 503.3616]\n",
      "Weights: [-4.7968  0.8747 -1.4412  0.2356  0.1224]\n",
      "MSE loss: 90.3395\n",
      "Iteration: 106100\n",
      "Gradient: [  -4.7624   14.788    41.2974  -71.0775 -359.9097]\n",
      "Weights: [-4.8295  0.892  -1.4438  0.2362  0.1223]\n",
      "MSE loss: 90.1215\n",
      "Iteration: 106200\n",
      "Gradient: [  -6.348    27.7985   17.4406   87.6352 -122.904 ]\n",
      "Weights: [-4.8162  0.8921 -1.445   0.2357  0.1223]\n",
      "MSE loss: 90.106\n",
      "Iteration: 106300\n",
      "Gradient: [  -2.0342    2.454    23.3458   86.4545 -388.6661]\n",
      "Weights: [-4.8096  0.8863 -1.4469  0.2363  0.1228]\n",
      "MSE loss: 90.3614\n",
      "Iteration: 106400\n",
      "Gradient: [ 11.9812  48.2591  86.8326 200.7652 822.7983]\n",
      "Weights: [-4.8151  0.8832 -1.4469  0.2354  0.123 ]\n",
      "MSE loss: 89.8453\n",
      "Iteration: 106500\n",
      "Gradient: [   5.4144  -34.3405  -12.7002 -218.9738 -702.2816]\n",
      "Weights: [-4.8264  0.8905 -1.4484  0.2359  0.1227]\n",
      "MSE loss: 89.8902\n",
      "Iteration: 106600\n",
      "Gradient: [ 17.5443  37.9639   1.535   14.4883 150.6837]\n",
      "Weights: [-4.8214  0.8934 -1.4473  0.2361  0.1228]\n",
      "MSE loss: 90.3151\n",
      "Iteration: 106700\n",
      "Gradient: [-15.5045  13.5441  41.0559 177.4115 -15.2359]\n",
      "Weights: [-4.8341  0.8837 -1.4519  0.2372  0.1225]\n",
      "MSE loss: 91.3772\n",
      "Iteration: 106800\n",
      "Gradient: [  -9.2332  -16.3716   18.3695 -108.8931 -366.7709]\n",
      "Weights: [-4.8069  0.8834 -1.4526  0.2373  0.1227]\n",
      "MSE loss: 89.8327\n",
      "Iteration: 106900\n",
      "Gradient: [-18.2742 -18.9476  58.5329  18.9219 299.692 ]\n",
      "Weights: [-4.8333  0.8817 -1.4504  0.2373  0.1229]\n",
      "MSE loss: 90.4912\n",
      "Iteration: 107000\n",
      "Gradient: [  13.3276   -5.6591   56.7025 -227.7742   36.708 ]\n",
      "Weights: [-4.7919  0.8901 -1.4562  0.2376  0.1228]\n",
      "MSE loss: 90.566\n",
      "Iteration: 107100\n",
      "Gradient: [  5.852  -13.2422  39.0115 134.4792 -79.1034]\n",
      "Weights: [-4.8092  0.8847 -1.4599  0.2391  0.1228]\n",
      "MSE loss: 89.9886\n",
      "Iteration: 107200\n",
      "Gradient: [  -6.2053   -1.1274  -98.1515 -194.1435 -113.6852]\n",
      "Weights: [-4.8069  0.8777 -1.4571  0.2393  0.1228]\n",
      "MSE loss: 89.8981\n",
      "Iteration: 107300\n",
      "Gradient: [  0.8221   6.9939  68.0886 -37.7415 391.0149]\n",
      "Weights: [-4.7971  0.8758 -1.4561  0.2399  0.1226]\n",
      "MSE loss: 89.8399\n",
      "Iteration: 107400\n",
      "Gradient: [ -23.443    11.1593   48.023     2.5376 -267.997 ]\n",
      "Weights: [-4.8259  0.8985 -1.4602  0.2399  0.1225]\n",
      "MSE loss: 89.8067\n",
      "Iteration: 107500\n",
      "Gradient: [  -8.6211  -35.2902   -5.1822 -193.2467 -382.1555]\n",
      "Weights: [-4.8163  0.8796 -1.4583  0.2407  0.1225]\n",
      "MSE loss: 90.0041\n",
      "Iteration: 107600\n",
      "Gradient: [  -3.684   -20.5306  -70.9619   50.1235 -116.4262]\n",
      "Weights: [-4.7986  0.8679 -1.4552  0.2402  0.1227]\n",
      "MSE loss: 89.8531\n",
      "Iteration: 107700\n",
      "Gradient: [  5.3408  10.9076  65.7584 -43.7888 480.6101]\n",
      "Weights: [-4.7918  0.8758 -1.4574  0.2407  0.1227]\n",
      "MSE loss: 90.2197\n",
      "Iteration: 107800\n",
      "Gradient: [14.2193 -9.3499 -1.7373 63.3545 50.4359]\n",
      "Weights: [-4.7984  0.8721 -1.4614  0.2414  0.1227]\n",
      "MSE loss: 90.029\n",
      "Iteration: 107900\n",
      "Gradient: [ 19.5828 -14.3078 -31.6369 114.1972 -36.1695]\n",
      "Weights: [-4.7911  0.8839 -1.4611  0.2414  0.1224]\n",
      "MSE loss: 90.2138\n",
      "Iteration: 108000\n",
      "Gradient: [  2.8942  12.7038  -6.4987   4.459  167.3018]\n",
      "Weights: [-4.7996  0.8755 -1.4573  0.2415  0.1224]\n",
      "MSE loss: 89.8443\n",
      "Iteration: 108100\n",
      "Gradient: [  12.9822    2.6604   49.9446  141.5714 -338.8175]\n",
      "Weights: [-4.8003  0.8634 -1.4591  0.2417  0.1224]\n",
      "MSE loss: 90.7248\n",
      "Iteration: 108200\n",
      "Gradient: [ -2.588    3.0309 -47.5168 -12.5505 -99.2636]\n",
      "Weights: [-4.7906  0.8627 -1.4577  0.2434  0.122 ]\n",
      "MSE loss: 89.8575\n",
      "Iteration: 108300\n",
      "Gradient: [-18.5314  -9.9815 -59.2066  23.7187 -94.0631]\n",
      "Weights: [-4.8039  0.8704 -1.4574  0.2427  0.1219]\n",
      "MSE loss: 89.8508\n",
      "Iteration: 108400\n",
      "Gradient: [ 3.839000e-01 -1.510720e+01 -6.175550e+01 -2.082243e+02 -6.719188e+02]\n",
      "Weights: [-4.7874  0.8602 -1.4582  0.2437  0.1219]\n",
      "MSE loss: 89.938\n",
      "Iteration: 108500\n",
      "Gradient: [  -1.3948  -33.8982  -54.0131 -224.0608 -448.457 ]\n",
      "Weights: [-4.8208  0.8802 -1.4606  0.2429  0.1221]\n",
      "MSE loss: 90.1311\n",
      "Iteration: 108600\n",
      "Gradient: [ 23.8181  28.6549 -26.0617 -73.9881 388.5093]\n",
      "Weights: [-4.8057  0.8962 -1.4582  0.2404  0.1222]\n",
      "MSE loss: 90.3875\n",
      "Iteration: 108700\n",
      "Gradient: [   8.6209    6.5781   70.1715 -144.363   248.4553]\n",
      "Weights: [-4.8188  0.9033 -1.4638  0.2419  0.1219]\n",
      "MSE loss: 89.7719\n",
      "Iteration: 108800\n",
      "Gradient: [  11.39     -1.1924  -92.4484  -75.6877 -198.3962]\n",
      "Weights: [-4.8176  0.907  -1.4683  0.2421  0.1222]\n",
      "MSE loss: 89.744\n",
      "Iteration: 108900\n",
      "Gradient: [ -21.1057   -6.5723   22.7556   -7.5713 -180.4158]\n",
      "Weights: [-4.8336  0.9046 -1.4699  0.2431  0.122 ]\n",
      "MSE loss: 90.4087\n",
      "Iteration: 109000\n",
      "Gradient: [ -12.5659  -20.4064  -85.7688 -256.4817 -123.3463]\n",
      "Weights: [-4.8248  0.9106 -1.4772  0.2437  0.1222]\n",
      "MSE loss: 90.3048\n",
      "Iteration: 109100\n",
      "Gradient: [  20.3057    3.6604   91.0807  -20.7219 -279.7756]\n",
      "Weights: [-4.7874  0.9113 -1.4816  0.2451  0.1224]\n",
      "MSE loss: 90.7979\n",
      "Iteration: 109200\n",
      "Gradient: [   5.7705    2.3144  -22.3379  -24.0948 -445.2343]\n",
      "Weights: [-4.8099  0.9142 -1.4865  0.2472  0.1224]\n",
      "MSE loss: 89.7372\n",
      "Iteration: 109300\n",
      "Gradient: [12.8628 10.6573  2.8558 33.8987 19.7842]\n",
      "Weights: [-4.8342  0.9355 -1.4899  0.2473  0.1223]\n",
      "MSE loss: 89.7628\n",
      "Iteration: 109400\n",
      "Gradient: [-14.2403 -20.3934 -58.3255  20.0998   7.3552]\n",
      "Weights: [-4.8299  0.9271 -1.4917  0.2484  0.122 ]\n",
      "MSE loss: 89.8801\n",
      "Iteration: 109500\n",
      "Gradient: [  14.9893   -6.1269  -31.6651   82.4373 -109.1989]\n",
      "Weights: [-4.8268  0.932  -1.4951  0.2491  0.122 ]\n",
      "MSE loss: 89.7185\n",
      "Iteration: 109600\n",
      "Gradient: [   2.1434  -13.2273  -93.0892 -300.8587 -472.6527]\n",
      "Weights: [-4.8161  0.9349 -1.4996  0.2495  0.1222]\n",
      "MSE loss: 89.6939\n",
      "Iteration: 109700\n",
      "Gradient: [ -4.3033  38.873   40.6874 -82.3113 698.8862]\n",
      "Weights: [-4.8223  0.9379 -1.4972  0.2493  0.1224]\n",
      "MSE loss: 89.9983\n",
      "Iteration: 109800\n",
      "Gradient: [ -4.6799  41.8373  83.879   34.0599 -28.0502]\n",
      "Weights: [-4.8277  0.946  -1.4994  0.2493  0.1221]\n",
      "MSE loss: 89.7221\n",
      "Iteration: 109900\n",
      "Gradient: [-1.19680e+00  1.28352e+01  7.67000e-02  4.19508e+01  8.56655e+01]\n",
      "Weights: [-4.8327  0.9507 -1.5036  0.2495  0.1222]\n",
      "MSE loss: 89.6756\n",
      "Iteration: 110000\n",
      "Gradient: [-30.7685  -8.9964  69.2143  -0.5163 -76.2629]\n",
      "Weights: [-4.8681  0.9581 -1.504   0.2488  0.1224]\n",
      "MSE loss: 91.3723\n",
      "Iteration: 110100\n",
      "Gradient: [  21.8036   -9.4507  -62.5142 -201.9095  266.53  ]\n",
      "Weights: [-4.8214  0.9514 -1.5052  0.2499  0.1222]\n",
      "MSE loss: 89.7772\n",
      "Iteration: 110200\n",
      "Gradient: [-1.446400e+00 -5.500000e-02 -1.957470e+01 -1.160643e+02  3.043246e+02]\n",
      "Weights: [-4.8286  0.9434 -1.5028  0.2507  0.1223]\n",
      "MSE loss: 89.6785\n",
      "Iteration: 110300\n",
      "Gradient: [-17.2515  29.3902  47.988   63.3338 159.571 ]\n",
      "Weights: [-4.8372  0.9447 -1.502   0.2518  0.1222]\n",
      "MSE loss: 90.0966\n",
      "Iteration: 110400\n",
      "Gradient: [   0.3507    6.128     4.4429 -111.9412  105.4213]\n",
      "Weights: [-4.8142  0.9405 -1.5059  0.2517  0.1217]\n",
      "MSE loss: 89.8271\n",
      "Iteration: 110500\n",
      "Gradient: [  6.6027  11.3024 -19.8681  12.4688 187.1764]\n",
      "Weights: [-4.8166  0.9398 -1.5043  0.2523  0.1217]\n",
      "MSE loss: 89.6776\n",
      "Iteration: 110600\n",
      "Gradient: [  13.5755    1.4     -14.7413 -235.1933  271.904 ]\n",
      "Weights: [-4.8078  0.9429 -1.5015  0.2521  0.1218]\n",
      "MSE loss: 91.4018\n",
      "Iteration: 110700\n",
      "Gradient: [ 10.8186  30.7605 105.0136 108.4343 -97.7759]\n",
      "Weights: [-4.8285  0.9389 -1.4992  0.2512  0.1216]\n",
      "MSE loss: 89.6117\n",
      "Iteration: 110800\n",
      "Gradient: [  2.4332   3.617   32.3065 250.8957 464.5517]\n",
      "Weights: [-4.8205  0.9356 -1.4979  0.2519  0.1215]\n",
      "MSE loss: 89.8221\n",
      "Iteration: 110900\n",
      "Gradient: [  10.9599    9.5572   -2.0149 -114.2614 -188.6378]\n",
      "Weights: [-4.8231  0.9349 -1.5041  0.2525  0.1219]\n",
      "MSE loss: 89.6343\n",
      "Iteration: 111000\n",
      "Gradient: [-3.9690e+00  1.5460e-01 -1.9234e+00  5.6990e+00 -6.2356e+02]\n",
      "Weights: [-4.822   0.9303 -1.503   0.2526  0.1217]\n",
      "MSE loss: 89.8222\n",
      "Iteration: 111100\n",
      "Gradient: [  13.8667   39.2916   39.6054   76.3181 -239.6113]\n",
      "Weights: [-4.8043  0.9437 -1.5058  0.2527  0.1218]\n",
      "MSE loss: 90.7988\n",
      "Iteration: 111200\n",
      "Gradient: [   1.0534   -7.8698   23.4214  136.0996 -364.0717]\n",
      "Weights: [-4.8162  0.9325 -1.502   0.2528  0.1219]\n",
      "MSE loss: 89.7984\n",
      "Iteration: 111300\n",
      "Gradient: [ 14.4834  36.1834  47.9245 157.6665 122.6293]\n",
      "Weights: [-4.8074  0.9358 -1.5049  0.2528  0.1221]\n",
      "MSE loss: 90.407\n",
      "Iteration: 111400\n",
      "Gradient: [ -12.6834  -23.7089 -110.5851 -241.0584 -250.6433]\n",
      "Weights: [-4.8294  0.9286 -1.4977  0.2509  0.122 ]\n",
      "MSE loss: 89.7648\n",
      "Iteration: 111500\n",
      "Gradient: [  8.7431  24.7733  13.7435 201.7011 -92.1215]\n",
      "Weights: [-4.8162  0.9357 -1.4985  0.2509  0.1216]\n",
      "MSE loss: 89.7111\n",
      "Iteration: 111600\n",
      "Gradient: [  -4.5387  -13.7944  -59.385   -90.2039 -447.5361]\n",
      "Weights: [-4.8124  0.9066 -1.4996  0.2535  0.1217]\n",
      "MSE loss: 90.4958\n",
      "Iteration: 111700\n",
      "Gradient: [  6.9742  43.4379  -4.4767 177.5833 944.3311]\n",
      "Weights: [-4.8223  0.9328 -1.4986  0.2525  0.1217]\n",
      "MSE loss: 89.9785\n",
      "Iteration: 111800\n",
      "Gradient: [  -0.828   -27.5793  -49.3388 -108.4854 -405.0326]\n",
      "Weights: [-4.8143  0.9315 -1.4986  0.2523  0.1214]\n",
      "MSE loss: 89.697\n",
      "Iteration: 111900\n",
      "Gradient: [  1.1267  37.4557  90.5623 187.4271 -24.668 ]\n",
      "Weights: [-4.8132  0.9429 -1.5013  0.2535  0.1212]\n",
      "MSE loss: 90.7088\n",
      "Iteration: 112000\n",
      "Gradient: [ -1.6669  33.6995  28.758   71.34   141.4477]\n",
      "Weights: [-4.8214  0.9393 -1.5047  0.2533  0.1215]\n",
      "MSE loss: 89.5979\n",
      "Iteration: 112100\n",
      "Gradient: [-5.180000e-02 -7.097000e+00 -3.229500e+01  1.474640e+01 -2.906968e+02]\n",
      "Weights: [-4.819   0.9477 -1.5032  0.2526  0.1215]\n",
      "MSE loss: 90.2816\n",
      "Iteration: 112200\n",
      "Gradient: [ -5.6414  -3.6408  10.3488  63.3086 371.2712]\n",
      "Weights: [-4.852   0.9483 -1.503   0.252   0.1215]\n",
      "MSE loss: 90.2222\n",
      "Iteration: 112300\n",
      "Gradient: [-17.6852 -20.1016 -34.0243 167.3789 230.5773]\n",
      "Weights: [-4.8474  0.9352 -1.5014  0.2523  0.1215]\n",
      "MSE loss: 91.0625\n",
      "Iteration: 112400\n",
      "Gradient: [   7.3514   18.9046  -45.3778  -50.497  -269.7526]\n",
      "Weights: [-4.8221  0.9553 -1.5087  0.2536  0.1215]\n",
      "MSE loss: 90.1804\n",
      "Iteration: 112500\n",
      "Gradient: [ -19.5003    7.473   -27.0928 -160.8741  -24.6787]\n",
      "Weights: [-4.8386  0.9485 -1.5076  0.253   0.1215]\n",
      "MSE loss: 89.8476\n",
      "Iteration: 112600\n",
      "Gradient: [ -9.2579  10.6201 -15.8138 -48.7063 -40.7839]\n",
      "Weights: [-4.8439  0.9509 -1.5084  0.2528  0.1218]\n",
      "MSE loss: 89.9022\n",
      "Iteration: 112700\n",
      "Gradient: [-17.5506 -14.4356  -3.8236 -49.6539  68.9117]\n",
      "Weights: [-4.8477  0.946  -1.5061  0.2528  0.1218]\n",
      "MSE loss: 90.1625\n",
      "Iteration: 112800\n",
      "Gradient: [  11.4374   14.0058  -99.7069 -197.5996 -443.0072]\n",
      "Weights: [-4.8073  0.9367 -1.5065  0.2523  0.1221]\n",
      "MSE loss: 89.8486\n",
      "Iteration: 112900\n",
      "Gradient: [  1.0598 -22.7984  20.0896   6.174  371.3002]\n",
      "Weights: [-4.8313  0.9447 -1.5041  0.2517  0.1219]\n",
      "MSE loss: 89.6208\n",
      "Iteration: 113000\n",
      "Gradient: [  -6.3077  -38.1415  -62.2922 -161.7614 -202.6529]\n",
      "Weights: [-4.8244  0.9415 -1.5043  0.2512  0.122 ]\n",
      "MSE loss: 89.6651\n",
      "Iteration: 113100\n",
      "Gradient: [ -24.0737  -47.3552  -25.0084  -18.3381 -298.7714]\n",
      "Weights: [-4.8429  0.9413 -1.5029  0.2501  0.1222]\n",
      "MSE loss: 90.6229\n",
      "Iteration: 113200\n",
      "Gradient: [ -20.8027  -21.8954 -104.7577 -177.0313 -624.8101]\n",
      "Weights: [-4.819   0.9474 -1.5078  0.2514  0.1221]\n",
      "MSE loss: 89.7085\n",
      "Iteration: 113300\n",
      "Gradient: [ 14.4924  37.1392 138.0169  -4.7493 141.5837]\n",
      "Weights: [-4.8226  0.9428 -1.5044  0.252   0.1221]\n",
      "MSE loss: 89.8291\n",
      "Iteration: 113400\n",
      "Gradient: [  -8.4709   18.977     4.6489 -156.644   396.8426]\n",
      "Weights: [-4.8403  0.9512 -1.5059  0.2504  0.1222]\n",
      "MSE loss: 89.8937\n",
      "Iteration: 113500\n",
      "Gradient: [ -21.5069  -61.9507  -79.8931 -131.5706 -729.131 ]\n",
      "Weights: [-4.8258  0.9325 -1.5033  0.2511  0.1222]\n",
      "MSE loss: 89.8767\n",
      "Iteration: 113600\n",
      "Gradient: [  -2.685    -2.3387 -109.7714   -6.2618 -469.9019]\n",
      "Weights: [-4.8257  0.9518 -1.5075  0.2514  0.122 ]\n",
      "MSE loss: 89.6672\n",
      "Iteration: 113700\n",
      "Gradient: [-24.6012 -11.635   24.9118 -45.9872 354.0944]\n",
      "Weights: [-4.8511  0.9527 -1.5086  0.2529  0.1218]\n",
      "MSE loss: 90.1157\n",
      "Iteration: 113800\n",
      "Gradient: [ -0.2294   8.4864 -27.6893  13.8149 228.4509]\n",
      "Weights: [-4.8482  0.9644 -1.5113  0.2535  0.1216]\n",
      "MSE loss: 89.7067\n",
      "Iteration: 113900\n",
      "Gradient: [   6.6373  -21.2452   49.0635   21.5222 -286.6774]\n",
      "Weights: [-4.8287  0.9547 -1.5098  0.2532  0.1216]\n",
      "MSE loss: 89.6318\n",
      "Iteration: 114000\n",
      "Gradient: [  13.6213   -3.2349  -46.4075 -141.0311   56.3746]\n",
      "Weights: [-4.8179  0.9596 -1.5133  0.2535  0.1215]\n",
      "MSE loss: 89.9827\n",
      "Iteration: 114100\n",
      "Gradient: [  -8.887   -37.6636 -121.3457 -194.3351 -621.7907]\n",
      "Weights: [-4.8205  0.9484 -1.5167  0.2562  0.1212]\n",
      "MSE loss: 89.7766\n",
      "Iteration: 114200\n",
      "Gradient: [ -13.6491  -32.155     9.8502 -136.8183 -569.5081]\n",
      "Weights: [-4.8401  0.9486 -1.5119  0.255   0.1214]\n",
      "MSE loss: 89.9412\n",
      "Iteration: 114300\n",
      "Gradient: [  -8.7574   -4.9279   40.917   -61.9854 -455.6659]\n",
      "Weights: [-4.8249  0.956  -1.5136  0.2552  0.1213]\n",
      "MSE loss: 89.6596\n",
      "Iteration: 114400\n",
      "Gradient: [ -10.1244   13.5383  -11.7337  127.7807 -121.3743]\n",
      "Weights: [-4.8232  0.9479 -1.5153  0.2557  0.1215]\n",
      "MSE loss: 89.6312\n",
      "Iteration: 114500\n",
      "Gradient: [ 12.1936  -6.6804 -21.8461  16.9421 166.3198]\n",
      "Weights: [-4.8376  0.951  -1.5113  0.2552  0.1214]\n",
      "MSE loss: 89.6299\n",
      "Iteration: 114600\n",
      "Gradient: [  11.2095   -1.608     5.8799  -79.2658 -185.2259]\n",
      "Weights: [-4.8176  0.9461 -1.5092  0.2559  0.121 ]\n",
      "MSE loss: 89.8766\n",
      "Iteration: 114700\n",
      "Gradient: [  -7.9405   15.9389  -50.648  -120.7517 -218.2078]\n",
      "Weights: [-4.8371  0.9578 -1.5131  0.2569  0.1208]\n",
      "MSE loss: 89.6058\n",
      "Iteration: 114800\n",
      "Gradient: [  13.4325   -0.2813   22.5215    2.9674 -123.9906]\n",
      "Weights: [-4.8197  0.9414 -1.5072  0.2553  0.1208]\n",
      "MSE loss: 89.623\n",
      "Iteration: 114900\n",
      "Gradient: [ -12.3382   -5.1348   -3.9584   17.9286 -357.1153]\n",
      "Weights: [-4.8517  0.9617 -1.5095  0.2556  0.1208]\n",
      "MSE loss: 89.7545\n",
      "Iteration: 115000\n",
      "Gradient: [ 13.3014  30.8303  -8.3708 165.2084   9.1057]\n",
      "Weights: [-4.8371  0.9556 -1.5104  0.2559  0.1209]\n",
      "MSE loss: 89.5974\n",
      "Iteration: 115100\n",
      "Gradient: [  3.9879  -0.7801  35.6721   8.5315 -81.4273]\n",
      "Weights: [-4.8182  0.9371 -1.5067  0.2567  0.1209]\n",
      "MSE loss: 89.7627\n",
      "Iteration: 115200\n",
      "Gradient: [-14.3086   3.291   12.3537  55.8605 -33.9952]\n",
      "Weights: [-4.8283  0.9443 -1.5094  0.2567  0.1206]\n",
      "MSE loss: 89.5845\n",
      "Iteration: 115300\n",
      "Gradient: [  2.6898  16.8973  18.4306  42.2328 352.3242]\n",
      "Weights: [-4.8119  0.934  -1.5086  0.2582  0.1206]\n",
      "MSE loss: 89.8444\n",
      "Iteration: 115400\n",
      "Gradient: [ -12.2942  -10.8599   -4.9706  -40.8135 -751.6039]\n",
      "Weights: [-4.8132  0.9117 -1.5065  0.2586  0.1207]\n",
      "MSE loss: 90.0063\n",
      "Iteration: 115500\n",
      "Gradient: [ -3.4595  16.8263  41.7476 -32.7891  -9.7182]\n",
      "Weights: [-4.8113  0.9181 -1.5059  0.259   0.1207]\n",
      "MSE loss: 89.6657\n",
      "Iteration: 115600\n",
      "Gradient: [   7.1794   -3.8336   -2.6488 -140.9603   35.5125]\n",
      "Weights: [-4.8093  0.9298 -1.5066  0.2584  0.1204]\n",
      "MSE loss: 89.7968\n",
      "Iteration: 115700\n",
      "Gradient: [  4.0947   6.0521  50.5547 -23.8245  24.6959]\n",
      "Weights: [-4.8023  0.9301 -1.512   0.2592  0.1204]\n",
      "MSE loss: 89.7397\n",
      "Iteration: 115800\n",
      "Gradient: [  -7.1701  -34.2348  -30.1084  -93.7525 -547.2621]\n",
      "Weights: [-4.8159  0.9312 -1.5144  0.2601  0.1206]\n",
      "MSE loss: 89.5667\n",
      "Iteration: 115900\n",
      "Gradient: [  -3.6626  -23.2624  -19.4439   63.1146 -151.9262]\n",
      "Weights: [-4.8134  0.9246 -1.5125  0.2603  0.1206]\n",
      "MSE loss: 89.6079\n",
      "Iteration: 116000\n",
      "Gradient: [-11.4673  26.1191  54.1985  67.5991 617.5127]\n",
      "Weights: [-4.8514  0.9399 -1.5131  0.2604  0.1201]\n",
      "MSE loss: 90.7699\n",
      "Iteration: 116100\n",
      "Gradient: [ 3.7054  2.2308 57.4129  9.4551 32.5953]\n",
      "Weights: [-4.8367  0.9504 -1.5137  0.2602  0.1197]\n",
      "MSE loss: 89.5735\n",
      "Iteration: 116200\n",
      "Gradient: [  -0.7058   14.1691  -64.7109  -75.1873 -364.8793]\n",
      "Weights: [-4.8459  0.9605 -1.5147  0.2601  0.1197]\n",
      "MSE loss: 89.6023\n",
      "Iteration: 116300\n",
      "Gradient: [ -26.9855  -34.9095  -47.7329 -199.5672 -718.2103]\n",
      "Weights: [-4.8468  0.9455 -1.5145  0.2607  0.1197]\n",
      "MSE loss: 90.5973\n",
      "Iteration: 116400\n",
      "Gradient: [ -5.8739  26.6963  -2.4544  45.5697 286.5957]\n",
      "Weights: [-4.8437  0.9408 -1.5121  0.2613  0.1196]\n",
      "MSE loss: 89.9666\n",
      "Iteration: 116500\n",
      "Gradient: [  10.057    23.9011   29.0544  -16.1516 -153.3463]\n",
      "Weights: [-4.8136  0.9324 -1.5114  0.2622  0.1194]\n",
      "MSE loss: 89.5409\n",
      "Iteration: 116600\n",
      "Gradient: [  -4.6304  -39.987   -82.5288    7.491  -677.8626]\n",
      "Weights: [-4.826   0.9338 -1.5099  0.262   0.1194]\n",
      "MSE loss: 89.4828\n",
      "Iteration: 116700\n",
      "Gradient: [  9.0188  15.9153  68.5696 214.365    2.6162]\n",
      "Weights: [-4.8299  0.9257 -1.5085  0.2623  0.1193]\n",
      "MSE loss: 89.8261\n",
      "Iteration: 116800\n",
      "Gradient: [-18.5638   9.8365 -53.6473  -4.6895  37.3416]\n",
      "Weights: [-4.839   0.934  -1.5095  0.2621  0.1192]\n",
      "MSE loss: 89.9001\n",
      "Iteration: 116900\n",
      "Gradient: [ -12.427     8.4331  -40.0833  135.043  -315.0773]\n",
      "Weights: [-4.8187  0.9345 -1.5149  0.2634  0.1192]\n",
      "MSE loss: 89.48\n",
      "Iteration: 117000\n",
      "Gradient: [ 29.1874  -9.818   -4.0785 157.7949 -18.553 ]\n",
      "Weights: [-4.8067  0.9363 -1.5129  0.2643  0.1191]\n",
      "MSE loss: 90.7494\n",
      "Iteration: 117100\n",
      "Gradient: [   9.5082   15.7899  -30.1759 -126.2973  216.8611]\n",
      "Weights: [-4.8095  0.9353 -1.5112  0.2638  0.1189]\n",
      "MSE loss: 90.0578\n",
      "Iteration: 117200\n",
      "Gradient: [  -4.8954   -4.6742   10.2783   16.4147 -382.5772]\n",
      "Weights: [-4.8307  0.9261 -1.511   0.2644  0.1187]\n",
      "MSE loss: 89.9829\n",
      "Iteration: 117300\n",
      "Gradient: [  7.1426 -12.0292  34.5198 198.3805 139.2547]\n",
      "Weights: [-4.7992  0.9171 -1.5102  0.265   0.1189]\n",
      "MSE loss: 89.7134\n",
      "Iteration: 117400\n",
      "Gradient: [ -7.9748 -19.083  -14.6035  -5.0582 200.6938]\n",
      "Weights: [-4.8181  0.9187 -1.5117  0.2661  0.1187]\n",
      "MSE loss: 89.5025\n",
      "Iteration: 117500\n",
      "Gradient: [  23.4985   -8.3334   30.7583   76.0165 -138.2785]\n",
      "Weights: [-4.8087  0.9462 -1.5145  0.2646  0.1185]\n",
      "MSE loss: 90.7162\n",
      "Iteration: 117600\n",
      "Gradient: [  6.5663 -26.4456  32.8157 -21.2169  76.4481]\n",
      "Weights: [-4.8223  0.9421 -1.5169  0.2649  0.1187]\n",
      "MSE loss: 89.4463\n",
      "Iteration: 117700\n",
      "Gradient: [   5.243    21.7337   53.4839  104.9692 -133.8618]\n",
      "Weights: [-4.8221  0.9338 -1.5159  0.2653  0.1188]\n",
      "MSE loss: 89.4316\n",
      "Iteration: 117800\n",
      "Gradient: [-4.797200e+00 -3.036000e-01  6.241210e+01  8.646770e+01  3.607414e+02]\n",
      "Weights: [-4.8493  0.9365 -1.5158  0.2655  0.1187]\n",
      "MSE loss: 90.6503\n",
      "Iteration: 117900\n",
      "Gradient: [  3.2688  19.8432  48.4205 112.2654 180.1545]\n",
      "Weights: [-4.8388  0.9501 -1.515   0.2651  0.1187]\n",
      "MSE loss: 89.7145\n",
      "Iteration: 118000\n",
      "Gradient: [-2.381000e-01 -2.804090e+01 -3.470890e+01  3.578370e+01 -3.500784e+02]\n",
      "Weights: [-4.8183  0.932  -1.5154  0.2652  0.1187]\n",
      "MSE loss: 89.4426\n",
      "Iteration: 118100\n",
      "Gradient: [ -5.8514  -2.0872 -15.5573 -14.6698 -14.6962]\n",
      "Weights: [-4.8147  0.9314 -1.5148  0.2647  0.119 ]\n",
      "MSE loss: 89.4714\n",
      "Iteration: 118200\n",
      "Gradient: [ -0.8065  -7.8999  -0.5908 -42.915  -40.1055]\n",
      "Weights: [-4.8197  0.9301 -1.5137  0.2643  0.1189]\n",
      "MSE loss: 89.5026\n",
      "Iteration: 118300\n",
      "Gradient: [-16.3109  18.3819 -56.4188 215.1999  43.4082]\n",
      "Weights: [-4.8258  0.9308 -1.5156  0.2652  0.1192]\n",
      "MSE loss: 89.5225\n",
      "Iteration: 118400\n",
      "Gradient: [ -17.9777  -24.4374  -51.6398  -98.2702 -158.0343]\n",
      "Weights: [-4.8282  0.9224 -1.5112  0.2642  0.119 ]\n",
      "MSE loss: 90.0111\n",
      "Iteration: 118500\n",
      "Gradient: [  7.9231  53.959  114.2707 188.61   345.6493]\n",
      "Weights: [-4.8101  0.9309 -1.5134  0.2653  0.119 ]\n",
      "MSE loss: 90.0398\n",
      "Iteration: 118600\n",
      "Gradient: [   2.1109  -52.9613  -88.0801 -158.1222 -507.9885]\n",
      "Weights: [-4.8083  0.9277 -1.5161  0.2653  0.1188]\n",
      "MSE loss: 89.5408\n",
      "Iteration: 118700\n",
      "Gradient: [ -1.9776 -20.3332 -50.6809 -20.2855  62.1302]\n",
      "Weights: [-4.8265  0.9402 -1.5147  0.264   0.1188]\n",
      "MSE loss: 89.4821\n",
      "Iteration: 118800\n",
      "Gradient: [   4.5886    8.7022  -82.4786 -149.9744 -475.3553]\n",
      "Weights: [-4.8242  0.9533 -1.5198  0.2636  0.119 ]\n",
      "MSE loss: 89.5439\n",
      "Iteration: 118900\n",
      "Gradient: [   1.6856  -11.1647  -47.8963 -126.9665 -173.6691]\n",
      "Weights: [-4.8206  0.9463 -1.5206  0.2651  0.1192]\n",
      "MSE loss: 89.5704\n",
      "Iteration: 119000\n",
      "Gradient: [ -5.4145  29.122   84.0085 114.7018 431.2394]\n",
      "Weights: [-4.8281  0.9543 -1.5235  0.2653  0.119 ]\n",
      "MSE loss: 89.4476\n",
      "Iteration: 119100\n",
      "Gradient: [  -5.9596  -41.4021  -41.8271    3.6244 -558.0488]\n",
      "Weights: [-4.8169  0.9308 -1.525   0.2673  0.1192]\n",
      "MSE loss: 89.6821\n",
      "Iteration: 119200\n",
      "Gradient: [  0.325   -1.4124 -40.4053  64.7187 -10.5821]\n",
      "Weights: [-4.8328  0.935  -1.5219  0.2677  0.1187]\n",
      "MSE loss: 89.8336\n",
      "Iteration: 119300\n",
      "Gradient: [  2.3679 -15.9886  15.1281  77.1834 260.9313]\n",
      "Weights: [-4.8292  0.9358 -1.5238  0.2684  0.1186]\n",
      "MSE loss: 89.779\n",
      "Iteration: 119400\n",
      "Gradient: [  -2.5167  -31.4667   -5.7456   88.1664 -314.6355]\n",
      "Weights: [-4.82    0.9233 -1.5176  0.2685  0.1183]\n",
      "MSE loss: 89.5295\n",
      "Iteration: 119500\n",
      "Gradient: [   9.6538   -3.0029 -107.6955  -91.8516  -97.4614]\n",
      "Weights: [-4.8033  0.9302 -1.5234  0.2688  0.1185]\n",
      "MSE loss: 89.5355\n",
      "Iteration: 119600\n",
      "Gradient: [ -7.8356 -10.4608 -23.1813 111.9453 230.1189]\n",
      "Weights: [-4.8152  0.9347 -1.524   0.2704  0.1183]\n",
      "MSE loss: 89.6983\n",
      "Iteration: 119700\n",
      "Gradient: [ 23.3714  11.5217  40.2299 219.9974 605.3972]\n",
      "Weights: [-4.819   0.9365 -1.5225  0.2696  0.1181]\n",
      "MSE loss: 89.4019\n",
      "Iteration: 119800\n",
      "Gradient: [ 12.4581   3.1314  31.1289 204.3765 277.4603]\n",
      "Weights: [-4.8163  0.9398 -1.5197  0.2683  0.1184]\n",
      "MSE loss: 89.895\n",
      "Iteration: 119900\n",
      "Gradient: [  2.6067  18.8115  41.2866 157.0368 323.6388]\n",
      "Weights: [-4.8323  0.935  -1.5173  0.2677  0.1182]\n",
      "MSE loss: 89.5378\n",
      "Iteration: 120000\n",
      "Gradient: [ -4.4747 -21.24   -86.0098  45.8128 -59.7772]\n",
      "Weights: [-4.8121  0.9286 -1.5168  0.268   0.1181]\n",
      "MSE loss: 89.4231\n",
      "Iteration: 120100\n",
      "Gradient: [ -16.5065  -20.9696  -45.3723  -94.679  -274.2488]\n",
      "Weights: [-4.8149  0.9379 -1.5179  0.2673  0.118 ]\n",
      "MSE loss: 89.4996\n",
      "Iteration: 120200\n",
      "Gradient: [  7.7039  -1.3479 -56.5519 -17.9494 161.5851]\n",
      "Weights: [-4.8178  0.9329 -1.5169  0.2669  0.1183]\n",
      "MSE loss: 89.4061\n",
      "Iteration: 120300\n",
      "Gradient: [  20.4355    1.6168   22.5704  -75.4781 -262.8078]\n",
      "Weights: [-4.8134  0.9469 -1.5194  0.2669  0.1181]\n",
      "MSE loss: 89.8672\n",
      "Iteration: 120400\n",
      "Gradient: [ -1.6352  -2.8889 -25.9501 -46.3717 -41.5479]\n",
      "Weights: [-4.807   0.9273 -1.5191  0.2685  0.1184]\n",
      "MSE loss: 89.5374\n",
      "Iteration: 120500\n",
      "Gradient: [ -19.7267  -40.6355  -45.4063 -100.905  -486.7856]\n",
      "Weights: [-4.8272  0.9216 -1.5177  0.2674  0.1183]\n",
      "MSE loss: 91.118\n",
      "Iteration: 120600\n",
      "Gradient: [  5.3482 -13.036  -33.1619   8.0098 102.1178]\n",
      "Weights: [-4.8139  0.928  -1.5157  0.2681  0.1182]\n",
      "MSE loss: 89.5382\n",
      "Iteration: 120700\n",
      "Gradient: [-2.751200e+00  8.250000e-02 -1.078220e+01  6.200000e-02  3.272236e+02]\n",
      "Weights: [-4.8107  0.9245 -1.5169  0.2677  0.1181]\n",
      "MSE loss: 89.5125\n",
      "Iteration: 120800\n",
      "Gradient: [   1.5575  -11.5396  -92.329  -127.4093 -345.804 ]\n",
      "Weights: [-4.8244  0.933  -1.521   0.2682  0.1181]\n",
      "MSE loss: 89.8642\n",
      "Iteration: 120900\n",
      "Gradient: [-12.3682  -8.2276   6.6243  67.1029 -71.3024]\n",
      "Weights: [-4.842   0.9306 -1.5168  0.268   0.1183]\n",
      "MSE loss: 90.1685\n",
      "Iteration: 121000\n",
      "Gradient: [ -19.4687   -1.0158  -85.3228 -120.1093  -32.7803]\n",
      "Weights: [-4.827   0.9308 -1.5167  0.2673  0.1183]\n",
      "MSE loss: 89.5643\n",
      "Iteration: 121100\n",
      "Gradient: [  -6.0563   15.3816  -32.4573  -90.6544 -354.5544]\n",
      "Weights: [-4.8102  0.9272 -1.5153  0.2665  0.1185]\n",
      "MSE loss: 89.4647\n",
      "Iteration: 121200\n",
      "Gradient: [ -1.8872  22.4229  -0.3499  35.5149 -59.8943]\n",
      "Weights: [-4.7995  0.9172 -1.5107  0.2676  0.1182]\n",
      "MSE loss: 90.1378\n",
      "Iteration: 121300\n",
      "Gradient: [  0.8969  -2.3603 -29.357   31.4696 -13.5748]\n",
      "Weights: [-4.8023  0.9186 -1.5132  0.2678  0.1181]\n",
      "MSE loss: 89.5664\n",
      "Iteration: 121400\n",
      "Gradient: [ 14.1648  36.2048 -20.3614  20.9616 163.4303]\n",
      "Weights: [-4.8101  0.9208 -1.5116  0.2675  0.1178]\n",
      "MSE loss: 89.4534\n",
      "Iteration: 121500\n",
      "Gradient: [   7.5644   -3.0418  -30.7722 -200.1619 -134.5961]\n",
      "Weights: [-4.7982  0.9227 -1.5174  0.2695  0.1177]\n",
      "MSE loss: 89.669\n",
      "Iteration: 121600\n",
      "Gradient: [  -1.7622    7.4765   40.7178 -125.401   243.6884]\n",
      "Weights: [-4.8241  0.9322 -1.5206  0.2712  0.1178]\n",
      "MSE loss: 89.5714\n",
      "Iteration: 121700\n",
      "Gradient: [  -8.0618  -22.9615  -32.8146   84.599  -184.4336]\n",
      "Weights: [-4.8015  0.9188 -1.5203  0.2717  0.1178]\n",
      "MSE loss: 89.6444\n",
      "Iteration: 121800\n",
      "Gradient: [  5.1686  18.5605  45.9885  86.326  384.1238]\n",
      "Weights: [-4.8136  0.9279 -1.5219  0.2711  0.1177]\n",
      "MSE loss: 89.3693\n",
      "Iteration: 121900\n",
      "Gradient: [ -25.113     7.6899   26.4491 -130.2884  331.2061]\n",
      "Weights: [-4.8445  0.9372 -1.5246  0.2695  0.1182]\n",
      "MSE loss: 90.9373\n",
      "Iteration: 122000\n",
      "Gradient: [  2.3659   5.9048  82.7325  69.0982 311.4113]\n",
      "Weights: [-4.8202  0.9497 -1.5294  0.2689  0.1185]\n",
      "MSE loss: 89.399\n",
      "Iteration: 122100\n",
      "Gradient: [   7.5002   -3.4716  -99.479  -199.6523 -468.5778]\n",
      "Weights: [-4.8224  0.9539 -1.5298  0.2692  0.1184]\n",
      "MSE loss: 89.4251\n",
      "Iteration: 122200\n",
      "Gradient: [  -4.8444   -0.7238   -0.2928  111.1453 -218.2643]\n",
      "Weights: [-4.819   0.9455 -1.5277  0.2689  0.1184]\n",
      "MSE loss: 89.419\n",
      "Iteration: 122300\n",
      "Gradient: [   1.288    -4.809     4.0594  -82.238  -711.2111]\n",
      "Weights: [-4.8185  0.9437 -1.5265  0.2684  0.1185]\n",
      "MSE loss: 89.4078\n",
      "Iteration: 122400\n",
      "Gradient: [  -3.5283   -7.7763  -42.6114 -142.3804 -401.0449]\n",
      "Weights: [-4.837   0.9469 -1.5267  0.2686  0.1184]\n",
      "MSE loss: 89.7605\n",
      "Iteration: 122500\n",
      "Gradient: [  6.1447 -57.9017  -9.1588  26.4988 208.8558]\n",
      "Weights: [-4.827   0.9472 -1.5285  0.2685  0.1187]\n",
      "MSE loss: 89.4268\n",
      "Iteration: 122600\n",
      "Gradient: [   4.4884  -14.3986  -23.7129 -139.3968 -365.7967]\n",
      "Weights: [-4.8017  0.9413 -1.5299  0.2689  0.1187]\n",
      "MSE loss: 89.6752\n",
      "Iteration: 122700\n",
      "Gradient: [  -1.8999  -27.2493   -6.417   164.6228 -226.7056]\n",
      "Weights: [-4.8264  0.9365 -1.5267  0.2698  0.1186]\n",
      "MSE loss: 89.5512\n",
      "Iteration: 122800\n",
      "Gradient: [   0.2101   -1.554    -3.3774   23.4183 -143.6592]\n",
      "Weights: [-4.8256  0.951  -1.532   0.2693  0.1186]\n",
      "MSE loss: 89.46\n",
      "Iteration: 122900\n",
      "Gradient: [ -2.3195  32.0231  18.1484  35.2275 326.0214]\n",
      "Weights: [-4.829   0.9464 -1.5285  0.2687  0.1186]\n",
      "MSE loss: 89.5185\n",
      "Iteration: 123000\n",
      "Gradient: [-16.5486   1.5342 -72.1162 125.5969  -9.9669]\n",
      "Weights: [-4.8307  0.9406 -1.5287  0.2693  0.1185]\n",
      "MSE loss: 89.9877\n",
      "Iteration: 123100\n",
      "Gradient: [   3.0629  -21.5735 -116.6149 -119.2853  -90.6836]\n",
      "Weights: [-4.8216  0.9598 -1.5327  0.2697  0.1184]\n",
      "MSE loss: 89.5607\n",
      "Iteration: 123200\n",
      "Gradient: [-10.2468  32.7974  80.959  156.9013 702.972 ]\n",
      "Weights: [-4.838   0.9612 -1.5322  0.2706  0.1185]\n",
      "MSE loss: 89.6774\n",
      "Iteration: 123300\n",
      "Gradient: [   7.0967  -36.203   -65.8287  -60.4155 -154.7984]\n",
      "Weights: [-4.8106  0.9544 -1.5291  0.2693  0.118 ]\n",
      "MSE loss: 89.8456\n",
      "Iteration: 123400\n",
      "Gradient: [ -7.6517  -5.1345 -39.454  -13.1692 -58.9591]\n",
      "Weights: [-4.8359  0.9627 -1.5325  0.2696  0.1182]\n",
      "MSE loss: 89.4139\n",
      "Iteration: 123500\n",
      "Gradient: [ 20.2049  26.922   24.5638 277.022  476.0141]\n",
      "Weights: [-4.8145  0.967  -1.5315  0.2707  0.118 ]\n",
      "MSE loss: 91.4509\n",
      "Iteration: 123600\n",
      "Gradient: [ 12.0636   6.7082 107.0065  59.9015 256.636 ]\n",
      "Weights: [-4.8312  0.9494 -1.5306  0.2709  0.118 ]\n",
      "MSE loss: 89.4657\n",
      "Iteration: 123700\n",
      "Gradient: [ -36.9459  -31.9927  -47.9093 -180.6897   38.255 ]\n",
      "Weights: [-4.8492  0.946  -1.5299  0.271   0.1179]\n",
      "MSE loss: 91.0875\n",
      "Iteration: 123800\n",
      "Gradient: [ 13.6438 -18.8185  33.9782 -35.4999 -94.7591]\n",
      "Weights: [-4.8272  0.9545 -1.5301  0.2711  0.1177]\n",
      "MSE loss: 89.3711\n",
      "Iteration: 123900\n",
      "Gradient: [ 13.4978  14.1212 124.9958 127.3742  44.0901]\n",
      "Weights: [-4.8136  0.9563 -1.5331  0.2715  0.118 ]\n",
      "MSE loss: 89.9042\n",
      "Iteration: 124000\n",
      "Gradient: [ 10.2063   4.8317  88.1238  53.3179 214.7234]\n",
      "Weights: [-4.8234  0.9503 -1.5295  0.2706  0.1182]\n",
      "MSE loss: 89.4553\n",
      "Iteration: 124100\n",
      "Gradient: [  10.2156   11.8731    4.9067   44.5538 -182.5054]\n",
      "Weights: [-4.8225  0.9506 -1.5342  0.271   0.1184]\n",
      "MSE loss: 89.3634\n",
      "Iteration: 124200\n",
      "Gradient: [  8.1587 -15.9925 -38.176   93.5423  80.929 ]\n",
      "Weights: [-4.8255  0.9534 -1.5343  0.271   0.1183]\n",
      "MSE loss: 89.3688\n",
      "Iteration: 124300\n",
      "Gradient: [  -2.7629   -9.35     71.3826   57.7238 -202.0399]\n",
      "Weights: [-4.8273  0.9494 -1.5279  0.2701  0.1184]\n",
      "MSE loss: 89.554\n",
      "Iteration: 124400\n",
      "Gradient: [   9.3474  -19.6841  -64.0284 -183.7726 -174.8974]\n",
      "Weights: [-4.8107  0.9428 -1.5321  0.2699  0.1186]\n",
      "MSE loss: 89.467\n",
      "Iteration: 124500\n",
      "Gradient: [  13.871     5.8253  -15.118   -47.8322 -128.9706]\n",
      "Weights: [-4.8253  0.9411 -1.5299  0.2693  0.1189]\n",
      "MSE loss: 89.5722\n",
      "Iteration: 124600\n",
      "Gradient: [  -6.627    -2.7125    9.8596   13.9615 -226.4374]\n",
      "Weights: [-4.8363  0.9538 -1.5284  0.2682  0.1185]\n",
      "MSE loss: 89.6368\n",
      "Iteration: 124700\n",
      "Gradient: [ -13.0071   11.1994  -27.6096 -105.8786  262.9934]\n",
      "Weights: [-4.8283  0.9509 -1.529   0.2687  0.1188]\n",
      "MSE loss: 89.4077\n",
      "Iteration: 124800\n",
      "Gradient: [-18.7894 -18.8647   3.5019 -62.265  423.4232]\n",
      "Weights: [-4.8373  0.9561 -1.5288  0.2682  0.1187]\n",
      "MSE loss: 89.4511\n",
      "Iteration: 124900\n",
      "Gradient: [  13.6724  -18.9352    6.1901 -111.5011  -87.7381]\n",
      "Weights: [-4.8256  0.9659 -1.5269  0.2671  0.1184]\n",
      "MSE loss: 89.8895\n",
      "Iteration: 125000\n",
      "Gradient: [  -9.1198    0.2568  -16.3049 -158.3046  105.4458]\n",
      "Weights: [-4.8407  0.9693 -1.5339  0.2688  0.1185]\n",
      "MSE loss: 89.4253\n",
      "Iteration: 125100\n",
      "Gradient: [  2.6928   3.3125  70.3881 -81.5643 336.6143]\n",
      "Weights: [-4.8297  0.9644 -1.5358  0.2698  0.1184]\n",
      "MSE loss: 89.4048\n",
      "Iteration: 125200\n",
      "Gradient: [ -10.5478  -27.836   -24.759  -145.2315  -99.5517]\n",
      "Weights: [-4.8478  0.9686 -1.5353  0.2706  0.118 ]\n",
      "MSE loss: 89.6335\n",
      "Iteration: 125300\n",
      "Gradient: [ -30.506   -52.931   -71.0279  137.9031 -181.2788]\n",
      "Weights: [-4.8529  0.9682 -1.5359  0.2702  0.1182]\n",
      "MSE loss: 90.128\n",
      "Iteration: 125400\n",
      "Gradient: [-1.216200e+01 -3.534730e+01  4.813000e-01 -1.534786e+02 -7.398036e+02]\n",
      "Weights: [-4.838   0.9502 -1.5311  0.2709  0.1181]\n",
      "MSE loss: 89.6526\n",
      "Iteration: 125500\n",
      "Gradient: [  7.7057   3.1073  38.7466 299.423   83.2203]\n",
      "Weights: [-4.8212  0.9577 -1.5338  0.2702  0.1183]\n",
      "MSE loss: 89.4233\n",
      "Iteration: 125600\n",
      "Gradient: [ 11.0062  46.5287  39.9981  58.5539 492.9245]\n",
      "Weights: [-4.819   0.9433 -1.5281  0.2705  0.1181]\n",
      "MSE loss: 89.3695\n",
      "Iteration: 125700\n",
      "Gradient: [  -7.6641    5.933    44.8422   66.9741 -249.2225]\n",
      "Weights: [-4.8238  0.9366 -1.5284  0.2719  0.1179]\n",
      "MSE loss: 89.5001\n",
      "Iteration: 125800\n",
      "Gradient: [ -5.3253   4.6356  66.3373  27.4687 -64.2508]\n",
      "Weights: [-4.8447  0.9614 -1.5301  0.2711  0.1177]\n",
      "MSE loss: 89.4735\n",
      "Iteration: 125900\n",
      "Gradient: [ -30.0977   17.8101   40.3207  251.0152 -150.6591]\n",
      "Weights: [-4.8489  0.9659 -1.5351  0.2718  0.1177]\n",
      "MSE loss: 89.6675\n",
      "Iteration: 126000\n",
      "Gradient: [  -3.0782  -38.9223  -93.1157 -123.6655 -473.7022]\n",
      "Weights: [-4.8309  0.9615 -1.5386  0.2714  0.118 ]\n",
      "MSE loss: 89.7997\n",
      "Iteration: 126100\n",
      "Gradient: [  6.1467  29.6601  -6.1423 157.4683  79.2414]\n",
      "Weights: [-4.8589  0.9913 -1.5382  0.2701  0.1179]\n",
      "MSE loss: 89.6324\n",
      "Iteration: 126200\n",
      "Gradient: [  3.9131 -24.7079 -26.813  -29.437    6.299 ]\n",
      "Weights: [-4.8418  0.9719 -1.5401  0.2719  0.1179]\n",
      "MSE loss: 89.4803\n",
      "Iteration: 126300\n",
      "Gradient: [ -10.648   -40.7808  -67.38   -128.1393 -477.4633]\n",
      "Weights: [-4.8275  0.9718 -1.5431  0.2723  0.1182]\n",
      "MSE loss: 89.3925\n",
      "Iteration: 126400\n",
      "Gradient: [  -2.5314   20.7756   10.2232 -268.9336  -68.7691]\n",
      "Weights: [-4.8377  0.9656 -1.5458  0.274   0.1182]\n",
      "MSE loss: 89.559\n",
      "Iteration: 126500\n",
      "Gradient: [-15.0468  14.7458   4.0333 253.3279 108.2126]\n",
      "Weights: [-4.8421  0.9619 -1.5428  0.2748  0.1179]\n",
      "MSE loss: 89.5909\n",
      "Iteration: 126600\n",
      "Gradient: [  -6.8034  -22.8354   28.3202  -67.4846 -472.9149]\n",
      "Weights: [-4.8313  0.9523 -1.5447  0.2753  0.1177]\n",
      "MSE loss: 90.4761\n",
      "Iteration: 126700\n",
      "Gradient: [ -4.8296  25.0133  58.8174  83.741  362.846 ]\n",
      "Weights: [-4.8478  0.9692 -1.5426  0.2744  0.1176]\n",
      "MSE loss: 89.6637\n",
      "Iteration: 126800\n",
      "Gradient: [  -2.4623  -45.4302   11.4103  -94.8201 -206.6498]\n",
      "Weights: [-4.835   0.9733 -1.5446  0.2758  0.1175]\n",
      "MSE loss: 89.5551\n",
      "Iteration: 126900\n",
      "Gradient: [ 12.3581  22.7166   3.0671 135.1303 186.4812]\n",
      "Weights: [-4.8593  0.9921 -1.5505  0.2763  0.1171]\n",
      "MSE loss: 89.5979\n",
      "Iteration: 127000\n",
      "Gradient: [  5.8156   0.0983  50.1455  45.9368 -57.6926]\n",
      "Weights: [-4.8324  0.9781 -1.5454  0.2767  0.1169]\n",
      "MSE loss: 89.6306\n",
      "Iteration: 127100\n",
      "Gradient: [-2.18000e-02 -1.11030e+01 -2.89576e+01 -5.90317e+01  3.06535e+01]\n",
      "Weights: [-4.835   0.9762 -1.5487  0.277   0.1169]\n",
      "MSE loss: 89.373\n",
      "Iteration: 127200\n",
      "Gradient: [  -3.2898   -4.7505   38.4043 -126.8678  353.3812]\n",
      "Weights: [-4.8305  0.9653 -1.5465  0.2774  0.1168]\n",
      "MSE loss: 89.4425\n",
      "Iteration: 127300\n",
      "Gradient: [   7.957     5.7138   -9.8954 -115.3357  300.0319]\n",
      "Weights: [-4.832   0.9698 -1.5457  0.2773  0.1167]\n",
      "MSE loss: 89.3257\n",
      "Iteration: 127400\n",
      "Gradient: [   1.155   -25.6242  -18.1051   -1.2827 -404.1629]\n",
      "Weights: [-4.8283  0.9706 -1.5493  0.2775  0.1168]\n",
      "MSE loss: 89.5062\n",
      "Iteration: 127500\n",
      "Gradient: [ 11.8424 -27.5791  -5.0329 163.4292 360.3732]\n",
      "Weights: [-4.8197  0.9758 -1.5506  0.2783  0.117 ]\n",
      "MSE loss: 89.8817\n",
      "Iteration: 127600\n",
      "Gradient: [   1.9679  -32.047   -47.2594   -8.8788 -187.7426]\n",
      "Weights: [-4.8409  0.9754 -1.5527  0.2788  0.1169]\n",
      "MSE loss: 89.4224\n",
      "Iteration: 127700\n",
      "Gradient: [ 20.6254  -1.1452  16.2036 -14.3026 -39.3598]\n",
      "Weights: [-4.8187  0.9797 -1.5525  0.279   0.117 ]\n",
      "MSE loss: 90.3333\n",
      "Iteration: 127800\n",
      "Gradient: [   1.1923  -29.2756   49.1074 -137.3128  316.5941]\n",
      "Weights: [-4.8313  0.9652 -1.5526  0.2804  0.117 ]\n",
      "MSE loss: 89.2778\n",
      "Iteration: 127900\n",
      "Gradient: [   4.3512   -6.9052    4.7524 -184.4952  -78.3674]\n",
      "Weights: [-4.8088  0.9486 -1.5491  0.2811  0.1169]\n",
      "MSE loss: 89.5878\n",
      "Iteration: 128000\n",
      "Gradient: [  -2.3621  -22.4206   13.7648   79.1254 -137.9842]\n",
      "Weights: [-4.8209  0.9629 -1.5514  0.2803  0.1166]\n",
      "MSE loss: 89.2812\n",
      "Iteration: 128100\n",
      "Gradient: [ 22.3705  37.9842  77.1891 113.7183 360.3241]\n",
      "Weights: [-4.8328  0.9794 -1.5503  0.2804  0.1164]\n",
      "MSE loss: 89.9759\n",
      "Iteration: 128200\n",
      "Gradient: [  2.5977 -14.607  -21.4105 137.3952 498.8205]\n",
      "Weights: [-4.8658  0.9974 -1.5559  0.279   0.1166]\n",
      "MSE loss: 89.7759\n",
      "Iteration: 128300\n",
      "Gradient: [-13.4752  17.1674 103.3983 -66.7015 -82.3527]\n",
      "Weights: [-4.8463  0.9849 -1.5561  0.2796  0.1167]\n",
      "MSE loss: 89.4161\n",
      "Iteration: 128400\n",
      "Gradient: [ -10.133   -10.1692   -2.4869  -79.7023 -180.3368]\n",
      "Weights: [-4.8476  0.9802 -1.5554  0.2795  0.1169]\n",
      "MSE loss: 89.5568\n",
      "Iteration: 128500\n",
      "Gradient: [  2.5553 -14.8646 -16.7873  47.0206 -77.0279]\n",
      "Weights: [-4.838   0.9863 -1.5578  0.2788  0.1174]\n",
      "MSE loss: 89.3298\n",
      "Iteration: 128600\n",
      "Gradient: [ 10.5974  26.2196  53.3332 106.9025 307.1867]\n",
      "Weights: [-4.8114  0.969  -1.5478  0.2778  0.1171]\n",
      "MSE loss: 90.297\n",
      "Iteration: 128700\n",
      "Gradient: [ 9.2942 37.6742 17.3018 42.1981 95.9457]\n",
      "Weights: [-4.8337  0.9768 -1.5511  0.2783  0.1173]\n",
      "MSE loss: 89.5515\n",
      "Iteration: 128800\n",
      "Gradient: [-21.0238   9.0684  52.9283  88.6689 396.9711]\n",
      "Weights: [-4.8276  0.9617 -1.549   0.2789  0.1169]\n",
      "MSE loss: 89.316\n",
      "Iteration: 128900\n",
      "Gradient: [  -5.6871  -22.3364  -69.7268 -164.6001  -30.1526]\n",
      "Weights: [-4.8364  0.9685 -1.5487  0.2785  0.117 ]\n",
      "MSE loss: 89.2895\n",
      "Iteration: 129000\n",
      "Gradient: [  30.0117   -8.7119   22.2045 -194.6365 -680.0392]\n",
      "Weights: [-4.8067  0.9583 -1.5532  0.2787  0.1175]\n",
      "MSE loss: 89.4804\n",
      "Iteration: 129100\n",
      "Gradient: [  6.0717 -42.9447  21.1909 -53.3659  78.4066]\n",
      "Weights: [-4.8308  0.9737 -1.5544  0.2791  0.1171]\n",
      "MSE loss: 89.2682\n",
      "Iteration: 129200\n",
      "Gradient: [  -5.5125  -19.5819  -66.7894  -49.813  -210.0214]\n",
      "Weights: [-4.8281  0.9762 -1.557   0.28    0.1169]\n",
      "MSE loss: 89.2984\n",
      "Iteration: 129300\n",
      "Gradient: [ -1.997   -8.0572  58.4776 -58.5117 267.8911]\n",
      "Weights: [-4.8055  0.9677 -1.5559  0.2811  0.1167]\n",
      "MSE loss: 89.8922\n",
      "Iteration: 129400\n",
      "Gradient: [   4.7402   12.4102   18.2108 -255.6997  -47.436 ]\n",
      "Weights: [-4.8333  0.9706 -1.5575  0.2822  0.1167]\n",
      "MSE loss: 89.2534\n",
      "Iteration: 129500\n",
      "Gradient: [  -5.8459  -16.3133  -41.8644 -384.7293 -540.7916]\n",
      "Weights: [-4.8412  0.9672 -1.5585  0.282   0.1166]\n",
      "MSE loss: 90.3438\n",
      "Iteration: 129600\n",
      "Gradient: [ -5.1887   8.8325  40.4063 259.9097 189.6351]\n",
      "Weights: [-4.8481  0.9806 -1.5589  0.2829  0.1164]\n",
      "MSE loss: 89.3844\n",
      "Iteration: 129700\n",
      "Gradient: [  9.1103  -7.2972  13.2153  83.8099 426.8473]\n",
      "Weights: [-4.8104  0.9693 -1.556   0.2828  0.1164]\n",
      "MSE loss: 90.1355\n",
      "Iteration: 129800\n",
      "Gradient: [  9.8074  -4.3508 -32.6721  20.5392 499.0708]\n",
      "Weights: [-4.8404  0.9744 -1.5586  0.2826  0.1165]\n",
      "MSE loss: 89.3828\n",
      "Iteration: 129900\n",
      "Gradient: [-1.705000e-01 -3.562070e+01  5.162970e+01 -9.234470e+01 -2.587847e+02]\n",
      "Weights: [-4.8189  0.9618 -1.559   0.2833  0.1165]\n",
      "MSE loss: 89.3162\n",
      "Iteration: 130000\n",
      "Gradient: [   4.4911  -10.4407   19.6209 -125.5618 -174.7442]\n",
      "Weights: [-4.8497  0.9754 -1.5601  0.2836  0.1163]\n",
      "MSE loss: 89.7676\n",
      "Iteration: 130100\n",
      "Gradient: [ -3.2669 -12.46   -10.5275 -47.6069 142.0506]\n",
      "Weights: [-4.8339  0.9845 -1.5656  0.2834  0.1165]\n",
      "MSE loss: 89.2098\n",
      "Iteration: 130200\n",
      "Gradient: [  -1.1407    2.2305  -63.0245   25.727  -150.1039]\n",
      "Weights: [-4.8434  0.9827 -1.5616  0.2843  0.1161]\n",
      "MSE loss: 89.2488\n",
      "Iteration: 130300\n",
      "Gradient: [11.0922 10.0721 51.5744 65.2611 92.8054]\n",
      "Weights: [-4.837   0.9888 -1.5642  0.284   0.116 ]\n",
      "MSE loss: 89.2256\n",
      "Iteration: 130400\n",
      "Gradient: [-1.108390e+01  6.702000e+00  2.615000e-01 -9.389210e+01  4.300538e+02]\n",
      "Weights: [-4.8475  0.9784 -1.5639  0.286   0.1159]\n",
      "MSE loss: 89.5008\n",
      "Iteration: 130500\n",
      "Gradient: [  4.3122  12.0943  16.9026  37.69   516.0709]\n",
      "Weights: [-4.8265  0.9767 -1.5598  0.285   0.1158]\n",
      "MSE loss: 89.4799\n",
      "Iteration: 130600\n",
      "Gradient: [  -9.0957  -38.2929   19.6579 -114.5589  -10.5107]\n",
      "Weights: [-4.8323  0.971  -1.5625  0.2855  0.1159]\n",
      "MSE loss: 89.2892\n",
      "Iteration: 130700\n",
      "Gradient: [ -0.4836   4.3337 -20.9656 -77.8114 395.0199]\n",
      "Weights: [-4.8315  0.9652 -1.559   0.2862  0.1158]\n",
      "MSE loss: 89.2263\n",
      "Iteration: 130800\n",
      "Gradient: [ -3.8112  19.2404  57.691   53.1097 646.1435]\n",
      "Weights: [-4.8018  0.9599 -1.5607  0.287   0.1157]\n",
      "MSE loss: 90.0166\n",
      "Iteration: 130900\n",
      "Gradient: [ 12.5442  17.3779  25.2149 -12.106  146.2935]\n",
      "Weights: [-4.8061  0.96   -1.5592  0.2863  0.1157]\n",
      "MSE loss: 89.6075\n",
      "Iteration: 131000\n",
      "Gradient: [ -2.5203   1.6312  -1.6264 197.8289 -64.5723]\n",
      "Weights: [-4.8152  0.9655 -1.5594  0.2868  0.1157]\n",
      "MSE loss: 89.9564\n",
      "Iteration: 131100\n",
      "Gradient: [-15.3345  17.5654 -22.1369  23.3139 195.0407]\n",
      "Weights: [-4.8344  0.9688 -1.5602  0.2861  0.1156]\n",
      "MSE loss: 89.2461\n",
      "Iteration: 131200\n",
      "Gradient: [-16.1336   4.4117 -17.4539 -88.2579 -77.2431]\n",
      "Weights: [-4.8445  0.9836 -1.5608  0.2856  0.1154]\n",
      "MSE loss: 89.2324\n",
      "Iteration: 131300\n",
      "Gradient: [  5.0577   0.3943 -63.2237 -72.4431 108.9821]\n",
      "Weights: [-4.8058  0.9693 -1.5596  0.2861  0.1156]\n",
      "MSE loss: 90.463\n",
      "Iteration: 131400\n",
      "Gradient: [ -2.72   -35.8764 -22.7025  24.7452 -52.9305]\n",
      "Weights: [-4.8463  0.9719 -1.561   0.2869  0.1154]\n",
      "MSE loss: 89.5657\n",
      "Iteration: 131500\n",
      "Gradient: [  1.2722  -7.6017  28.7396 120.1685 468.7535]\n",
      "Weights: [-4.8231  0.9712 -1.5618  0.2879  0.1152]\n",
      "MSE loss: 89.3452\n",
      "Iteration: 131600\n",
      "Gradient: [   0.4725   14.0558  -10.4866 -129.4295 -216.392 ]\n",
      "Weights: [-4.8383  0.9783 -1.5635  0.2881  0.1149]\n",
      "MSE loss: 89.191\n",
      "Iteration: 131700\n",
      "Gradient: [   8.7097   14.7036   69.7986  291.7309 -172.283 ]\n",
      "Weights: [-4.8274  0.9814 -1.5648  0.2888  0.1149]\n",
      "MSE loss: 89.4446\n",
      "Iteration: 131800\n",
      "Gradient: [  5.5652  23.6279 -12.1232 169.0099 383.7658]\n",
      "Weights: [-4.8197  0.9694 -1.563   0.2896  0.1149]\n",
      "MSE loss: 89.5116\n",
      "Iteration: 131900\n",
      "Gradient: [  3.776   15.1478 -23.2571 -64.0125  77.4585]\n",
      "Weights: [-4.8343  0.9878 -1.5657  0.2886  0.1147]\n",
      "MSE loss: 89.2813\n",
      "Iteration: 132000\n",
      "Gradient: [ 11.4937  21.784   38.7767  -6.0387 222.7767]\n",
      "Weights: [-4.8455  1.0017 -1.5687  0.2893  0.1148]\n",
      "MSE loss: 90.0403\n",
      "Iteration: 132100\n",
      "Gradient: [  -8.455   -33.7078  -33.033  -329.8193 -600.706 ]\n",
      "Weights: [-4.8296  0.9755 -1.5656  0.2892  0.1149]\n",
      "MSE loss: 89.1261\n",
      "Iteration: 132200\n",
      "Gradient: [  3.6762  34.4931  30.5724 121.9428 466.9137]\n",
      "Weights: [-4.816   0.9634 -1.5637  0.2903  0.1149]\n",
      "MSE loss: 89.381\n",
      "Iteration: 132300\n",
      "Gradient: [  -0.5457  -12.0184  -26.9816  103.1167 -283.0313]\n",
      "Weights: [-4.8311  0.9653 -1.5653  0.2909  0.1145]\n",
      "MSE loss: 89.3182\n",
      "Iteration: 132400\n",
      "Gradient: [ -9.5148  -3.8288  34.4344 -23.2802 289.2888]\n",
      "Weights: [-4.8412  0.9635 -1.5625  0.2915  0.1141]\n",
      "MSE loss: 89.5783\n",
      "Iteration: 132500\n",
      "Gradient: [15.3994 19.3339 35.759  67.6411 12.6946]\n",
      "Weights: [-4.8262  0.9726 -1.5685  0.2923  0.1143]\n",
      "MSE loss: 89.1162\n",
      "Iteration: 132600\n",
      "Gradient: [ -6.216    9.5638 -17.9567 165.9439 103.6974]\n",
      "Weights: [-4.817   0.9628 -1.5654  0.2915  0.1145]\n",
      "MSE loss: 89.1675\n",
      "Iteration: 132700\n",
      "Gradient: [  1.5118  42.6866  39.9207 264.8475 -58.2895]\n",
      "Weights: [-4.8135  0.9653 -1.5658  0.2919  0.1143]\n",
      "MSE loss: 89.2756\n",
      "Iteration: 132800\n",
      "Gradient: [   8.0961   -9.0522   13.5473  143.83   -263.4751]\n",
      "Weights: [-4.8079  0.9714 -1.567   0.2922  0.1142]\n",
      "MSE loss: 89.9625\n",
      "Iteration: 132900\n",
      "Gradient: [ -4.5328 -43.7357 -45.3088  56.7968  31.2243]\n",
      "Weights: [-4.8269  0.9558 -1.5668  0.2933  0.1141]\n",
      "MSE loss: 89.5017\n",
      "Iteration: 133000\n",
      "Gradient: [  -0.4506  -24.4545  -54.2762 -200.7832 -262.9132]\n",
      "Weights: [-4.831   0.9595 -1.561   0.293   0.1138]\n",
      "MSE loss: 89.128\n",
      "Iteration: 133100\n",
      "Gradient: [   8.4799   20.1262  -51.4383 -157.3965 -413.1949]\n",
      "Weights: [-4.8148  0.9664 -1.5633  0.2928  0.1138]\n",
      "MSE loss: 89.632\n",
      "Iteration: 133200\n",
      "Gradient: [-18.5706  20.1981  18.1972 122.1228  -1.5699]\n",
      "Weights: [-4.852   0.9628 -1.5651  0.293   0.1139]\n",
      "MSE loss: 90.8246\n",
      "Iteration: 133300\n",
      "Gradient: [ -17.2197   -7.666   -98.9548  -21.2477 -127.3237]\n",
      "Weights: [-4.8269  0.9665 -1.5632  0.2917  0.1139]\n",
      "MSE loss: 89.1244\n",
      "Iteration: 133400\n",
      "Gradient: [ 15.273   11.9192 -88.2541  16.9428 -43.0383]\n",
      "Weights: [-4.8117  0.9668 -1.5635  0.2924  0.1138]\n",
      "MSE loss: 89.6636\n",
      "Iteration: 133500\n",
      "Gradient: [ -12.4884    6.3896   31.5481  -27.3718 -104.6352]\n",
      "Weights: [-4.8485  0.9677 -1.5627  0.2924  0.1138]\n",
      "MSE loss: 89.6366\n",
      "Iteration: 133600\n",
      "Gradient: [  -5.4231  -55.5874 -161.4635 -377.792  -815.7517]\n",
      "Weights: [-4.8361  0.9654 -1.562   0.2914  0.114 ]\n",
      "MSE loss: 89.3071\n",
      "Iteration: 133700\n",
      "Gradient: [  4.9866 -33.261  -27.83    17.0408  67.9941]\n",
      "Weights: [-4.8285  0.9606 -1.562   0.292   0.114 ]\n",
      "MSE loss: 89.1339\n",
      "Iteration: 133800\n",
      "Gradient: [ 14.0168 -22.9165   9.8701  15.8621 -92.5568]\n",
      "Weights: [-4.8265  0.9532 -1.5599  0.2916  0.1141]\n",
      "MSE loss: 89.2671\n",
      "Iteration: 133900\n",
      "Gradient: [   5.2374   15.7014   22.0245 -175.4074 -421.7373]\n",
      "Weights: [-4.8489  0.9772 -1.561   0.2917  0.1136]\n",
      "MSE loss: 89.2792\n",
      "Iteration: 134000\n",
      "Gradient: [ -8.3468  44.6516  17.624  132.1762 173.7616]\n",
      "Weights: [-4.8587  0.9938 -1.5625  0.2907  0.1137]\n",
      "MSE loss: 89.5155\n",
      "Iteration: 134100\n",
      "Gradient: [ 7.165000e+00 -2.090130e+01 -1.096000e-01 -2.157739e+02 -5.874351e+02]\n",
      "Weights: [-4.8358  0.9791 -1.5664  0.291   0.1142]\n",
      "MSE loss: 89.1187\n",
      "Iteration: 134200\n",
      "Gradient: [ 16.6421 -14.8025 -19.8974  65.5098  39.4081]\n",
      "Weights: [-4.8387  0.9716 -1.5675  0.2912  0.1145]\n",
      "MSE loss: 89.3841\n",
      "Iteration: 134300\n",
      "Gradient: [ -19.5466    6.44      2.0845  -84.7905 -131.9683]\n",
      "Weights: [-4.8235  0.972  -1.5662  0.2916  0.1143]\n",
      "MSE loss: 89.1827\n",
      "Iteration: 134400\n",
      "Gradient: [ -1.9309  16.7643  47.8023 -87.998  -18.6034]\n",
      "Weights: [-4.848   0.9889 -1.5702  0.2918  0.1142]\n",
      "MSE loss: 89.1962\n",
      "Iteration: 134500\n",
      "Gradient: [  3.6435  28.4306  11.4252 -20.836  125.5251]\n",
      "Weights: [-4.8296  0.9709 -1.5711  0.2936  0.1141]\n",
      "MSE loss: 89.1031\n",
      "Iteration: 134600\n",
      "Gradient: [ -10.0544   -8.103   -55.1101 -224.5446 -304.9787]\n",
      "Weights: [-4.8204  0.9592 -1.5712  0.2944  0.1142]\n",
      "MSE loss: 89.2326\n",
      "Iteration: 134700\n",
      "Gradient: [ -4.2765  19.309   23.4191  27.6095 -85.1819]\n",
      "Weights: [-4.8269  0.9676 -1.5702  0.294   0.1141]\n",
      "MSE loss: 89.0761\n",
      "Iteration: 134800\n",
      "Gradient: [ -25.687   -20.7473   -3.6223    6.8397 -378.3096]\n",
      "Weights: [-4.8318  0.9585 -1.5686  0.2942  0.1142]\n",
      "MSE loss: 89.4534\n",
      "Iteration: 134900\n",
      "Gradient: [ -11.7626    1.2098   49.1769 -149.4451 -446.1644]\n",
      "Weights: [-4.8261  0.9742 -1.571   0.2939  0.1138]\n",
      "MSE loss: 89.074\n",
      "Iteration: 135000\n",
      "Gradient: [ -11.4597  -34.1769 -105.7361 -190.1189 -335.4912]\n",
      "Weights: [-4.8496  0.9805 -1.571   0.2927  0.1138]\n",
      "MSE loss: 90.1294\n",
      "Iteration: 135100\n",
      "Gradient: [  10.3968  -10.4958  -46.3506 -111.1782 -237.0209]\n",
      "Weights: [-4.8352  0.9901 -1.5712  0.2922  0.1138]\n",
      "MSE loss: 89.1984\n",
      "Iteration: 135200\n",
      "Gradient: [  18.483   -16.3191  -35.3286 -115.5498  114.9594]\n",
      "Weights: [-4.8283  0.9853 -1.5722  0.2926  0.114 ]\n",
      "MSE loss: 89.1866\n",
      "Iteration: 135300\n",
      "Gradient: [ -18.3639  -14.455    28.5964  -55.1675 -102.9181]\n",
      "Weights: [-4.859   0.9947 -1.5781  0.2931  0.1142]\n",
      "MSE loss: 90.022\n",
      "Iteration: 135400\n",
      "Gradient: [  5.3072 -30.9702 -24.2825 -70.5843  52.4013]\n",
      "Weights: [-4.8249  0.9983 -1.5818  0.2938  0.1144]\n",
      "MSE loss: 89.4724\n",
      "Iteration: 135500\n",
      "Gradient: [  5.9709  14.5281 -11.7984 -29.8634  -4.4807]\n",
      "Weights: [-4.8391  1.0075 -1.5804  0.2927  0.1145]\n",
      "MSE loss: 89.4944\n",
      "Iteration: 135600\n",
      "Gradient: [-12.202   -4.3844 -32.4391 -12.2008 -96.0711]\n",
      "Weights: [-4.8566  1.0005 -1.5837  0.2931  0.1147]\n",
      "MSE loss: 89.8909\n",
      "Iteration: 135700\n",
      "Gradient: [   4.2431  -28.1698  -42.9906 -275.7906  -12.4046]\n",
      "Weights: [-4.8398  0.9925 -1.5831  0.2939  0.1147]\n",
      "MSE loss: 89.1935\n",
      "Iteration: 135800\n",
      "Gradient: [ -15.7904   -0.8159  -39.8987 -141.4154   76.0689]\n",
      "Weights: [-4.8304  0.9718 -1.5769  0.2943  0.1147]\n",
      "MSE loss: 89.2136\n",
      "Iteration: 135900\n",
      "Gradient: [ -3.8738 -26.5528  58.0138 216.7511 416.0387]\n",
      "Weights: [-4.8156  0.9654 -1.576   0.2953  0.1145]\n",
      "MSE loss: 89.167\n",
      "Iteration: 136000\n",
      "Gradient: [   3.7397  -19.603  -105.779   186.7416  216.0622]\n",
      "Weights: [-4.8187  0.9833 -1.5754  0.2934  0.1144]\n",
      "MSE loss: 89.5403\n",
      "Iteration: 136100\n",
      "Gradient: [  0.5168  41.7066  32.6798 -22.8465 450.8574]\n",
      "Weights: [-4.8257  0.9792 -1.5766  0.2945  0.1144]\n",
      "MSE loss: 89.1603\n",
      "Iteration: 136200\n",
      "Gradient: [ 31.2725  33.7676  51.4941  45.544  372.3066]\n",
      "Weights: [-4.8119  0.9841 -1.577   0.2945  0.114 ]\n",
      "MSE loss: 89.7396\n",
      "Iteration: 136300\n",
      "Gradient: [   5.3219  -12.9534  -12.6161 -115.0093 -156.103 ]\n",
      "Weights: [-4.8267  0.9765 -1.575   0.2948  0.114 ]\n",
      "MSE loss: 89.0491\n",
      "Iteration: 136400\n",
      "Gradient: [  3.1736 -21.5131  63.2611  81.5191 289.659 ]\n",
      "Weights: [-4.8288  0.9831 -1.5728  0.294   0.114 ]\n",
      "MSE loss: 89.3175\n",
      "Iteration: 136500\n",
      "Gradient: [  -4.3745    2.8703  -58.0447 -192.9187 -198.0212]\n",
      "Weights: [-4.8303  0.9822 -1.5732  0.2933  0.114 ]\n",
      "MSE loss: 89.0843\n",
      "Iteration: 136600\n",
      "Gradient: [ 16.9387  -2.2894  42.1562  79.7696 110.8606]\n",
      "Weights: [-4.8343  0.9881 -1.5735  0.2943  0.114 ]\n",
      "MSE loss: 89.6202\n",
      "Iteration: 136700\n",
      "Gradient: [ -4.9849  28.2897  14.6337  21.2613 330.5957]\n",
      "Weights: [-4.8243  0.9908 -1.5761  0.2941  0.1139]\n",
      "MSE loss: 89.6074\n",
      "Iteration: 136800\n",
      "Gradient: [  -9.04    -36.0175 -114.2681  -91.1466 -382.3091]\n",
      "Weights: [-4.8431  0.9892 -1.5764  0.2945  0.1139]\n",
      "MSE loss: 89.1247\n",
      "Iteration: 136900\n",
      "Gradient: [   3.4224  -13.0959  -37.2672   55.0421 -387.2968]\n",
      "Weights: [-4.8361  0.9851 -1.5782  0.2947  0.114 ]\n",
      "MSE loss: 89.1372\n",
      "Iteration: 137000\n",
      "Gradient: [  -6.5066  -17.3902   -3.0097 -245.1062 -523.8988]\n",
      "Weights: [-4.8407  0.9795 -1.5809  0.2951  0.1144]\n",
      "MSE loss: 89.6594\n",
      "Iteration: 137100\n",
      "Gradient: [-1.400100e+01 -1.169000e-01  3.598840e+01  1.394313e+02  4.831246e+02]\n",
      "Weights: [-4.8364  0.9679 -1.5798  0.2956  0.1144]\n",
      "MSE loss: 90.3612\n",
      "Iteration: 137200\n",
      "Gradient: [ -11.3183   14.6061   13.8872 -118.0708 -158.7432]\n",
      "Weights: [-4.8232  0.9696 -1.5792  0.2959  0.1141]\n",
      "MSE loss: 89.3148\n",
      "Iteration: 137300\n",
      "Gradient: [  9.1893  32.9765  11.4017 -78.0924 418.5884]\n",
      "Weights: [-4.8236  0.9748 -1.5776  0.296   0.1141]\n",
      "MSE loss: 89.0829\n",
      "Iteration: 137400\n",
      "Gradient: [ 16.7449  25.9712  52.6627 178.4263 198.8732]\n",
      "Weights: [-4.8197  0.9867 -1.5805  0.296   0.1141]\n",
      "MSE loss: 89.5944\n",
      "Iteration: 137500\n",
      "Gradient: [  5.533   12.3293  85.2262 141.8017 152.2531]\n",
      "Weights: [-4.8536  0.9881 -1.5818  0.2968  0.114 ]\n",
      "MSE loss: 89.5442\n",
      "Iteration: 137600\n",
      "Gradient: [ -5.5203  13.8512   7.9922 -28.8397  -0.798 ]\n",
      "Weights: [-4.8212  0.9842 -1.5827  0.297   0.114 ]\n",
      "MSE loss: 89.2145\n",
      "Iteration: 137700\n",
      "Gradient: [  18.3533   12.257     8.6973  -91.0486 -522.7691]\n",
      "Weights: [-4.8221  0.9864 -1.5868  0.296   0.1144]\n",
      "MSE loss: 89.1484\n",
      "Iteration: 137800\n",
      "Gradient: [  -2.1703   -4.2095  -97.4307 -107.0795  466.6995]\n",
      "Weights: [-4.8411  0.9865 -1.5869  0.2967  0.1145]\n",
      "MSE loss: 89.391\n",
      "Iteration: 137900\n",
      "Gradient: [ -7.059  -17.8625 -35.2906  40.5403 110.1411]\n",
      "Weights: [-4.842   0.9849 -1.5868  0.2964  0.1145]\n",
      "MSE loss: 89.8041\n",
      "Iteration: 138000\n",
      "Gradient: [  0.4475   4.4291  -8.1047  18.5073 -74.8048]\n",
      "Weights: [-4.8301  0.9891 -1.5842  0.2967  0.1139]\n",
      "MSE loss: 89.0333\n",
      "Iteration: 138100\n",
      "Gradient: [  10.915   -37.2137  -35.0964 -203.4803 -198.2286]\n",
      "Weights: [-4.8326  1.0041 -1.5908  0.2973  0.1141]\n",
      "MSE loss: 89.1472\n",
      "Iteration: 138200\n",
      "Gradient: [ 11.4449  -4.0089  54.9377   8.5418 483.3667]\n",
      "Weights: [-4.8395  1.0069 -1.5879  0.2976  0.1137]\n",
      "MSE loss: 89.3208\n",
      "Iteration: 138300\n",
      "Gradient: [ -1.0832   0.7635 -76.8215 135.2829 243.8239]\n",
      "Weights: [-4.8325  0.987  -1.5848  0.2981  0.1137]\n",
      "MSE loss: 89.0164\n",
      "Iteration: 138400\n",
      "Gradient: [ -9.505   -1.4683  75.5342 258.3662   5.2799]\n",
      "Weights: [-4.841   0.9869 -1.5883  0.2991  0.1137]\n",
      "MSE loss: 89.343\n",
      "Iteration: 138500\n",
      "Gradient: [ -2.9335 -21.034   11.8804  88.0279 254.3875]\n",
      "Weights: [-4.8119  0.9798 -1.5883  0.2978  0.1142]\n",
      "MSE loss: 89.1862\n",
      "Iteration: 138600\n",
      "Gradient: [   4.6147  -24.0101 -102.2438  -62.9349  -87.9862]\n",
      "Weights: [-4.8251  0.985  -1.5902  0.2986  0.1144]\n",
      "MSE loss: 89.1225\n",
      "Iteration: 138700\n",
      "Gradient: [ 17.13    10.894  -69.4604  65.9042 -69.1975]\n",
      "Weights: [-4.8107  0.9961 -1.5886  0.2979  0.1142]\n",
      "MSE loss: 90.8391\n",
      "Iteration: 138800\n",
      "Gradient: [ -26.8603   -3.0046  -14.5572   10.3533 -298.008 ]\n",
      "Weights: [-4.848   0.987  -1.5888  0.2982  0.1142]\n",
      "MSE loss: 89.8655\n",
      "Iteration: 138900\n",
      "Gradient: [ 11.3936  44.8225  27.2941 225.0436 510.3072]\n",
      "Weights: [-4.8083  0.9848 -1.5878  0.299   0.1138]\n",
      "MSE loss: 89.7712\n",
      "Iteration: 139000\n",
      "Gradient: [  0.1252 -10.243    0.705  -64.0918  21.37  ]\n",
      "Weights: [-4.8229  0.9832 -1.5848  0.2981  0.1136]\n",
      "MSE loss: 89.0402\n",
      "Iteration: 139100\n",
      "Gradient: [  8.8845  14.1224  74.8495  51.2616 428.2643]\n",
      "Weights: [-4.8474  0.9822 -1.5842  0.2984  0.1138]\n",
      "MSE loss: 89.6272\n",
      "Iteration: 139200\n",
      "Gradient: [  2.1394  43.741   35.543  204.2956 540.5538]\n",
      "Weights: [-4.8329  0.9794 -1.583   0.2983  0.1135]\n",
      "MSE loss: 89.1727\n",
      "Iteration: 139300\n",
      "Gradient: [ -7.1469  12.9602  28.6481 -26.8418 122.0648]\n",
      "Weights: [-4.8203  0.9883 -1.5875  0.2989  0.1138]\n",
      "MSE loss: 89.3235\n",
      "Iteration: 139400\n",
      "Gradient: [  3.4237  12.1562 -14.7298 -20.6071 254.7474]\n",
      "Weights: [-4.8306  0.9957 -1.5886  0.2994  0.1132]\n",
      "MSE loss: 89.037\n",
      "Iteration: 139500\n",
      "Gradient: [ -17.7883  -16.5966   54.1319  -17.954  -188.6022]\n",
      "Weights: [-4.8498  1.0072 -1.5918  0.3009  0.1134]\n",
      "MSE loss: 89.52\n",
      "Iteration: 139600\n",
      "Gradient: [  6.5537  24.1109   1.6962 101.5243 373.297 ]\n",
      "Weights: [-4.8272  0.9985 -1.5938  0.3019  0.1131]\n",
      "MSE loss: 89.2393\n",
      "Iteration: 139700\n",
      "Gradient: [   9.1526  -19.5454  -25.5581 -212.876   -35.5325]\n",
      "Weights: [-4.8273  0.9896 -1.5934  0.3023  0.1128]\n",
      "MSE loss: 89.0879\n",
      "Iteration: 139800\n",
      "Gradient: [  -2.0311   -5.7406  -90.9537  -63.6689 -361.1908]\n",
      "Weights: [-4.8234  0.9761 -1.5919  0.3043  0.1128]\n",
      "MSE loss: 89.0055\n",
      "Iteration: 139900\n",
      "Gradient: [ -24.3059   12.1551  -41.3553 -134.0059 -554.6272]\n",
      "Weights: [-4.8279  0.9739 -1.5931  0.3046  0.1127]\n",
      "MSE loss: 89.4287\n",
      "Iteration: 140000\n",
      "Gradient: [  14.1216   -4.655   -44.0276  -49.9694 -152.8654]\n",
      "Weights: [-4.8172  0.9723 -1.5925  0.3049  0.1127]\n",
      "MSE loss: 89.0182\n",
      "Iteration: 140100\n",
      "Gradient: [ -8.7907   6.9729 -79.0865  47.8161  48.3435]\n",
      "Weights: [-4.8192  0.9708 -1.5937  0.3061  0.1126]\n",
      "MSE loss: 89.0438\n",
      "Iteration: 140200\n",
      "Gradient: [ 0.8918 20.8695 12.2018 14.3238 -1.142 ]\n",
      "Weights: [-4.8122  0.9807 -1.5944  0.3065  0.1122]\n",
      "MSE loss: 89.2975\n",
      "Iteration: 140300\n",
      "Gradient: [  15.3467   -1.1665   13.3908  -27.5546 -351.3204]\n",
      "Weights: [-4.8088  0.9787 -1.5945  0.3062  0.112 ]\n",
      "MSE loss: 89.1387\n",
      "Iteration: 140400\n",
      "Gradient: [ -11.0653  -30.1684  -50.2337   54.8077 -388.0131]\n",
      "Weights: [-4.854   0.9867 -1.5918  0.3067  0.1119]\n",
      "MSE loss: 89.5102\n",
      "Iteration: 140500\n",
      "Gradient: [  -8.4819  -23.4209  -39.9677  -64.579  -620.8522]\n",
      "Weights: [-4.8251  0.9906 -1.5971  0.3069  0.1118]\n",
      "MSE loss: 88.9295\n",
      "Iteration: 140600\n",
      "Gradient: [  2.7646  45.3641  23.5721 145.9326 306.2613]\n",
      "Weights: [-4.8176  0.9906 -1.5981  0.3071  0.112 ]\n",
      "MSE loss: 89.1723\n",
      "Iteration: 140700\n",
      "Gradient: [ 11.8212  19.853   27.314   -0.6557 236.2735]\n",
      "Weights: [-4.8353  0.9903 -1.5954  0.3062  0.1119]\n",
      "MSE loss: 88.957\n",
      "Iteration: 140800\n",
      "Gradient: [-16.6336   4.7415  21.3826 115.1932 240.9715]\n",
      "Weights: [-4.8582  1.0052 -1.5993  0.3056  0.1121]\n",
      "MSE loss: 89.5668\n",
      "Iteration: 140900\n",
      "Gradient: [  11.1954  -15.581    18.3541   77.7597 -192.5512]\n",
      "Weights: [-4.8318  0.9984 -1.6001  0.3065  0.1122]\n",
      "MSE loss: 88.9349\n",
      "Iteration: 141000\n",
      "Gradient: [ 11.3954  35.9687 -34.8641  53.9458 543.0693]\n",
      "Weights: [-4.8277  0.9853 -1.5966  0.3066  0.1124]\n",
      "MSE loss: 88.9968\n",
      "Iteration: 141100\n",
      "Gradient: [  -1.6066  -40.2197  -54.351  -213.0482 -657.0544]\n",
      "Weights: [-4.8319  0.9834 -1.5978  0.3072  0.1119]\n",
      "MSE loss: 89.2978\n",
      "Iteration: 141200\n",
      "Gradient: [  8.1112  -2.2982 -37.8084 -37.4896  44.5462]\n",
      "Weights: [-4.8447  1.0024 -1.5983  0.3072  0.1117]\n",
      "MSE loss: 88.963\n",
      "Iteration: 141300\n",
      "Gradient: [25.4835 23.581  27.8209 56.6582 53.7282]\n",
      "Weights: [-4.8292  1.0125 -1.6018  0.3071  0.1118]\n",
      "MSE loss: 89.8009\n",
      "Iteration: 141400\n",
      "Gradient: [ -9.51    -2.0741 -33.3197  54.6321  39.7936]\n",
      "Weights: [-4.8413  1.0055 -1.6038  0.3074  0.1122]\n",
      "MSE loss: 88.935\n",
      "Iteration: 141500\n",
      "Gradient: [ -5.6912  26.1671  30.9864 -12.7458 180.1483]\n",
      "Weights: [-4.8306  1.0052 -1.6046  0.3079  0.1122]\n",
      "MSE loss: 89.2036\n",
      "Iteration: 141600\n",
      "Gradient: [ -5.1535   2.3486  27.777  -87.4319 191.6743]\n",
      "Weights: [-4.8071  0.9772 -1.601   0.3094  0.1122]\n",
      "MSE loss: 89.4232\n",
      "Iteration: 141700\n",
      "Gradient: [ 15.1013   6.7121  40.7474 -56.9533 256.9915]\n",
      "Weights: [-4.8045  0.9765 -1.6017  0.3101  0.1118]\n",
      "MSE loss: 89.2024\n",
      "Iteration: 141800\n",
      "Gradient: [ 16.4003  -1.0638 -37.0067  14.0918 -46.0719]\n",
      "Weights: [-4.8159  0.9768 -1.597   0.3093  0.1116]\n",
      "MSE loss: 88.9736\n",
      "Iteration: 141900\n",
      "Gradient: [  -8.6459    1.351  -115.237  -121.6486 -328.2942]\n",
      "Weights: [-4.8333  0.9902 -1.6011  0.309   0.1116]\n",
      "MSE loss: 88.957\n",
      "Iteration: 142000\n",
      "Gradient: [   4.4146    3.7928   -6.172  -201.0245 -274.2902]\n",
      "Weights: [-4.8152  0.9988 -1.6046  0.3095  0.1115]\n",
      "MSE loss: 89.3808\n",
      "Iteration: 142100\n",
      "Gradient: [  4.4832  14.4346  39.6855 253.3713 690.841 ]\n",
      "Weights: [-4.8438  0.9935 -1.6008  0.3108  0.1111]\n",
      "MSE loss: 89.0012\n",
      "Iteration: 142200\n",
      "Gradient: [   2.6927  -33.815   -35.1165   33.6816 -494.7625]\n",
      "Weights: [-4.8292  0.9864 -1.6011  0.3102  0.1111]\n",
      "MSE loss: 89.1029\n",
      "Iteration: 142300\n",
      "Gradient: [ -0.6281 -16.2723 -50.3863 -54.1739  80.72  ]\n",
      "Weights: [-4.8283  0.9795 -1.6009  0.3114  0.1113]\n",
      "MSE loss: 88.9621\n",
      "Iteration: 142400\n",
      "Gradient: [   2.5188  -22.7258  -11.5691 -187.4156 -493.402 ]\n",
      "Weights: [-4.832   0.978  -1.6012  0.3108  0.1111]\n",
      "MSE loss: 89.8801\n",
      "Iteration: 142500\n",
      "Gradient: [  -2.708   -79.0388  -61.61    -69.5052 -194.6393]\n",
      "Weights: [-4.8292  0.9827 -1.5999  0.3102  0.1112]\n",
      "MSE loss: 89.0591\n",
      "Iteration: 142600\n",
      "Gradient: [ -3.6656  24.4643  78.1646 123.4055 393.6667]\n",
      "Weights: [-4.8171  0.9845 -1.6012  0.3102  0.1114]\n",
      "MSE loss: 88.9404\n",
      "Iteration: 142700\n",
      "Gradient: [  -3.501   -19.4912   -5.0199  -91.8072 -226.2052]\n",
      "Weights: [-4.818   0.9864 -1.601   0.3102  0.1112]\n",
      "MSE loss: 88.9398\n",
      "Iteration: 142800\n",
      "Gradient: [   1.5313   -9.9169  -21.3347  -90.8076 -293.108 ]\n",
      "Weights: [-4.8094  0.9843 -1.6019  0.3113  0.111 ]\n",
      "MSE loss: 89.1409\n",
      "Iteration: 142900\n",
      "Gradient: [ 3.8293 -2.9214 -6.114  70.2517 -5.9854]\n",
      "Weights: [-4.8312  0.9801 -1.599   0.3112  0.1113]\n",
      "MSE loss: 88.9798\n",
      "Iteration: 143000\n",
      "Gradient: [  -9.3452  -30.4127 -108.737  -122.4821 -357.031 ]\n",
      "Weights: [-4.8373  0.9745 -1.5954  0.311   0.1111]\n",
      "MSE loss: 89.1923\n",
      "Iteration: 143100\n",
      "Gradient: [  5.6266  18.7774   7.4788  84.1626 -11.5949]\n",
      "Weights: [-4.7921  0.966  -1.5974  0.3132  0.1106]\n",
      "MSE loss: 89.8876\n",
      "Iteration: 143200\n",
      "Gradient: [  -8.5227   32.4462  -23.8729   31.715  -482.4908]\n",
      "Weights: [-4.8168  0.9635 -1.6001  0.3137  0.1106]\n",
      "MSE loss: 89.2507\n",
      "Iteration: 143300\n",
      "Gradient: [  11.894     5.2582   44.5911  142.8437 -237.4152]\n",
      "Weights: [-4.8211  0.9676 -1.5997  0.3144  0.1105]\n",
      "MSE loss: 88.9572\n",
      "Iteration: 143400\n",
      "Gradient: [   3.5328  -12.2397  -26.2484  -28.1706 -149.7338]\n",
      "Weights: [-4.8221  0.9678 -1.5987  0.3134  0.1104]\n",
      "MSE loss: 89.1813\n",
      "Iteration: 143500\n",
      "Gradient: [  2.2841  21.3173  39.5785  27.9026 687.4759]\n",
      "Weights: [-4.83    0.9802 -1.5991  0.3131  0.1104]\n",
      "MSE loss: 88.87\n",
      "Iteration: 143600\n",
      "Gradient: [  -3.6602    4.085    64.043   -75.3406 -247.4282]\n",
      "Weights: [-4.8171  0.9737 -1.5957  0.3129  0.1103]\n",
      "MSE loss: 88.9247\n",
      "Iteration: 143700\n",
      "Gradient: [  4.4784  -7.7135  20.7346 -18.835  157.4387]\n",
      "Weights: [-4.8195  0.9655 -1.5972  0.3133  0.1106]\n",
      "MSE loss: 88.9541\n",
      "Iteration: 143800\n",
      "Gradient: [-1.06200e-01  1.27506e+01 -4.08593e+01  6.25289e+01 -1.88838e+02]\n",
      "Weights: [-4.8213  0.9652 -1.5933  0.3126  0.1104]\n",
      "MSE loss: 88.9073\n",
      "Iteration: 143900\n",
      "Gradient: [ 13.9186   3.2915  50.0732 118.1864 394.8728]\n",
      "Weights: [-4.8177  0.978  -1.5963  0.3118  0.1104]\n",
      "MSE loss: 88.91\n",
      "Iteration: 144000\n",
      "Gradient: [  0.3316   3.0315 -20.9863 100.4924  44.8845]\n",
      "Weights: [-4.836   0.9793 -1.5939  0.3113  0.1103]\n",
      "MSE loss: 89.0171\n",
      "Iteration: 144100\n",
      "Gradient: [ -1.5625  18.3298   7.9851 -39.8452 -32.0915]\n",
      "Weights: [-4.8358  0.9811 -1.5886  0.3108  0.11  ]\n",
      "MSE loss: 88.957\n",
      "Iteration: 144200\n",
      "Gradient: [ -5.9113  -4.6747   4.571  -47.6372 184.0975]\n",
      "Weights: [-4.8313  0.9797 -1.5907  0.3099  0.1106]\n",
      "MSE loss: 88.9015\n",
      "Iteration: 144300\n",
      "Gradient: [-13.8323 -34.4539  25.0286 -48.854  -32.5656]\n",
      "Weights: [-4.826   0.9866 -1.5974  0.3113  0.1104]\n",
      "MSE loss: 88.8848\n",
      "Iteration: 144400\n",
      "Gradient: [  3.6358  15.876  -34.4307 181.744  128.0741]\n",
      "Weights: [-4.8206  0.9797 -1.6001  0.3117  0.1109]\n",
      "MSE loss: 88.8766\n",
      "Iteration: 144500\n",
      "Gradient: [  4.286   13.7525  50.1496  46.0454 721.8146]\n",
      "Weights: [-4.8175  0.9791 -1.6029  0.3118  0.1112]\n",
      "MSE loss: 88.9106\n",
      "Iteration: 144600\n",
      "Gradient: [  -2.7461  -29.7477   16.5909 -111.567   478.3788]\n",
      "Weights: [-4.8294  0.9872 -1.6009  0.3117  0.1109]\n",
      "MSE loss: 88.8604\n",
      "Iteration: 144700\n",
      "Gradient: [ -17.2427   25.0639  -41.8608  241.615  -261.491 ]\n",
      "Weights: [-4.8526  0.9923 -1.6039  0.3119  0.1109]\n",
      "MSE loss: 89.8905\n",
      "Iteration: 144800\n",
      "Gradient: [ -2.3231  11.3507 -40.6462 -42.6498 -21.5111]\n",
      "Weights: [-4.8421  0.9963 -1.6056  0.3119  0.111 ]\n",
      "MSE loss: 89.0459\n",
      "Iteration: 144900\n",
      "Gradient: [-21.     -13.1234 -76.241   44.5093 -62.8503]\n",
      "Weights: [-4.8329  1.0019 -1.6066  0.3106  0.1112]\n",
      "MSE loss: 88.866\n",
      "Iteration: 145000\n",
      "Gradient: [ 10.1846  31.7524  -3.8299 -45.5526  85.1491]\n",
      "Weights: [-4.8278  1.0204 -1.6075  0.3094  0.1114]\n",
      "MSE loss: 90.3312\n",
      "Iteration: 145100\n",
      "Gradient: [ -10.3572   -2.158   -44.1422   72.8645 -166.017 ]\n",
      "Weights: [-4.8331  0.9966 -1.608   0.3105  0.1112]\n",
      "MSE loss: 89.4664\n",
      "Iteration: 145200\n",
      "Gradient: [ 13.6652   6.7846 -54.1775  66.6536 215.2308]\n",
      "Weights: [-4.8208  0.9958 -1.6059  0.3113  0.1112]\n",
      "MSE loss: 89.0345\n",
      "Iteration: 145300\n",
      "Gradient: [-15.1393  20.3114  18.3372  96.5637 -46.9684]\n",
      "Weights: [-4.8258  0.9896 -1.6056  0.3118  0.1111]\n",
      "MSE loss: 88.8681\n",
      "Iteration: 145400\n",
      "Gradient: [ 15.5713  -4.9078   6.0709 155.914  268.3836]\n",
      "Weights: [-4.8146  0.9858 -1.6098  0.3133  0.1112]\n",
      "MSE loss: 88.9274\n",
      "Iteration: 145500\n",
      "Gradient: [  -6.6392  -23.3867  -26.4676  -27.2491 -104.5977]\n",
      "Weights: [-4.8395  0.9924 -1.6105  0.3141  0.1108]\n",
      "MSE loss: 89.3357\n",
      "Iteration: 145600\n",
      "Gradient: [-18.2634 -20.4701 -13.2137 -22.3098  -7.8061]\n",
      "Weights: [-4.814   0.9865 -1.6096  0.3148  0.1109]\n",
      "MSE loss: 89.076\n",
      "Iteration: 145700\n",
      "Gradient: [   0.9892  -16.0099  -58.0447 -280.5853 -752.8438]\n",
      "Weights: [-4.8228  0.9854 -1.6064  0.3158  0.1103]\n",
      "MSE loss: 88.9048\n",
      "Iteration: 145800\n",
      "Gradient: [  0.8109 -13.5577 -36.9722 -37.5991 443.3566]\n",
      "Weights: [-4.8238  0.9891 -1.6086  0.3163  0.11  ]\n",
      "MSE loss: 88.8123\n",
      "Iteration: 145900\n",
      "Gradient: [  11.7895  -27.0747  -50.2293 -339.7703 -281.6083]\n",
      "Weights: [-4.825   0.9917 -1.6084  0.3171  0.1094]\n",
      "MSE loss: 88.829\n",
      "Iteration: 146000\n",
      "Gradient: [   0.3802   27.1027   19.7522  171.2459 -255.5315]\n",
      "Weights: [-4.837   0.9825 -1.6049  0.3178  0.1095]\n",
      "MSE loss: 88.9544\n",
      "Iteration: 146100\n",
      "Gradient: [ -1.9487 -25.2233  34.2207  94.0075 147.6228]\n",
      "Weights: [-4.8404  0.9887 -1.6044  0.3173  0.1095]\n",
      "MSE loss: 88.923\n",
      "Iteration: 146200\n",
      "Gradient: [ -8.2225 -12.3588 -29.9271  21.3862 463.1201]\n",
      "Weights: [-4.8381  0.9849 -1.6074  0.3175  0.1094]\n",
      "MSE loss: 89.3642\n",
      "Iteration: 146300\n",
      "Gradient: [   2.6675  -46.9513  -58.0608 -148.1728  300.3144]\n",
      "Weights: [-4.8173  0.9759 -1.6051  0.3172  0.1092]\n",
      "MSE loss: 89.2266\n",
      "Iteration: 146400\n",
      "Gradient: [ -5.6396  34.9628   5.9226 111.8471 -46.2966]\n",
      "Weights: [-4.8393  0.9869 -1.6052  0.3171  0.1094]\n",
      "MSE loss: 89.0189\n",
      "Iteration: 146500\n",
      "Gradient: [   2.0846   -1.7786   52.0052  164.1743 -100.0468]\n",
      "Weights: [-4.8444  0.9926 -1.6076  0.3173  0.1095]\n",
      "MSE loss: 89.0154\n",
      "Iteration: 146600\n",
      "Gradient: [ -5.1799  -0.5124 -31.1764 102.2515 364.651 ]\n",
      "Weights: [-4.8363  0.9867 -1.6084  0.3173  0.1096]\n",
      "MSE loss: 89.0821\n",
      "Iteration: 146700\n",
      "Gradient: [  -6.1821    1.5874  -26.7294 -145.5323 -243.2695]\n",
      "Weights: [-4.8407  0.9936 -1.6059  0.3158  0.1098]\n",
      "MSE loss: 88.8856\n",
      "Iteration: 146800\n",
      "Gradient: [-7.150000e-01 -2.117800e+00  3.646210e+01  8.971720e+01 -7.229849e+02]\n",
      "Weights: [-4.8199  0.985  -1.6081  0.3163  0.1101]\n",
      "MSE loss: 88.8203\n",
      "Iteration: 146900\n",
      "Gradient: [   8.0358   24.7961   16.1541   11.5773 -494.3601]\n",
      "Weights: [-4.8235  0.9869 -1.6026  0.3153  0.1099]\n",
      "MSE loss: 89.0403\n",
      "Iteration: 147000\n",
      "Gradient: [  -2.0889   -4.4562  -61.6987  -47.7282 -192.7867]\n",
      "Weights: [-4.8114  0.974  -1.6044  0.3154  0.1102]\n",
      "MSE loss: 88.8992\n",
      "Iteration: 147100\n",
      "Gradient: [ -2.8995  17.7879  -2.2822 103.7088 -54.2177]\n",
      "Weights: [-4.8471  0.9849 -1.6038  0.3153  0.11  ]\n",
      "MSE loss: 89.5707\n",
      "Iteration: 147200\n",
      "Gradient: [  8.6388  13.933   46.7756 -31.7551 378.2514]\n",
      "Weights: [-4.8215  0.9989 -1.6052  0.316   0.1097]\n",
      "MSE loss: 89.924\n",
      "Iteration: 147300\n",
      "Gradient: [ -13.1574  -22.5823  -52.5931 -147.9781 -173.6916]\n",
      "Weights: [-4.8254  0.9791 -1.6071  0.3166  0.1098]\n",
      "MSE loss: 89.1173\n",
      "Iteration: 147400\n",
      "Gradient: [ 8.523000e+00  4.873000e-01 -5.219520e+01 -5.332660e+01 -5.984067e+02]\n",
      "Weights: [-4.8195  0.985  -1.6097  0.3167  0.1102]\n",
      "MSE loss: 88.8292\n",
      "Iteration: 147500\n",
      "Gradient: [ -12.6255  -14.2034   18.3564   73.0698 -277.6788]\n",
      "Weights: [-4.8468  0.9976 -1.6086  0.3163  0.1096]\n",
      "MSE loss: 89.1694\n",
      "Iteration: 147600\n",
      "Gradient: [   0.4252  -16.5745  -68.4501 -165.8724 -298.8045]\n",
      "Weights: [-4.8159  0.9787 -1.6057  0.3163  0.1098]\n",
      "MSE loss: 88.8691\n",
      "Iteration: 147700\n",
      "Gradient: [ -3.5404  38.974    6.2147  99.3851 194.8097]\n",
      "Weights: [-4.8191  0.9805 -1.6018  0.3161  0.1098]\n",
      "MSE loss: 89.1048\n",
      "Iteration: 147800\n",
      "Gradient: [ -6.1024 -11.1267  56.0676 186.9541 -70.3586]\n",
      "Weights: [-4.8331  0.9825 -1.6032  0.3155  0.1099]\n",
      "MSE loss: 88.9287\n",
      "Iteration: 147900\n",
      "Gradient: [ -2.3199  -3.7774  40.2952 212.2135  -1.635 ]\n",
      "Weights: [-4.8322  0.9804 -1.5999  0.3146  0.1098]\n",
      "MSE loss: 88.9554\n",
      "Iteration: 148000\n",
      "Gradient: [  1.6586  27.0124  67.6507 188.115  213.0847]\n",
      "Weights: [-4.8233  0.9799 -1.6     0.3154  0.1097]\n",
      "MSE loss: 88.8758\n",
      "Iteration: 148100\n",
      "Gradient: [  8.1521  33.4842  71.7241 371.8103 466.9143]\n",
      "Weights: [-4.8398  0.9937 -1.6027  0.3149  0.11  ]\n",
      "MSE loss: 88.969\n",
      "Iteration: 148200\n",
      "Gradient: [  -2.2938  -14.3074   -3.036  -107.51    432.2673]\n",
      "Weights: [-4.8219  0.9719 -1.6024  0.3155  0.1103]\n",
      "MSE loss: 88.904\n",
      "Iteration: 148300\n",
      "Gradient: [ 17.823   31.3481  55.7695  -2.2882 -29.0392]\n",
      "Weights: [-4.7884  0.9721 -1.6053  0.3165  0.1101]\n",
      "MSE loss: 90.1601\n",
      "Iteration: 148400\n",
      "Gradient: [ -25.6775    1.873   -62.3584   12.204  -325.8222]\n",
      "Weights: [-4.8241  0.9651 -1.6057  0.3171  0.1099]\n",
      "MSE loss: 89.9148\n",
      "Iteration: 148500\n",
      "Gradient: [ -15.1729   13.2453   15.283   -80.5362 -340.6549]\n",
      "Weights: [-4.8251  0.9822 -1.6068  0.3162  0.1099]\n",
      "MSE loss: 88.9258\n",
      "Iteration: 148600\n",
      "Gradient: [ -11.5987  -16.4287   -6.2316   38.0746 -258.1732]\n",
      "Weights: [-4.8267  0.9873 -1.605   0.3164  0.1094]\n",
      "MSE loss: 88.8423\n",
      "Iteration: 148700\n",
      "Gradient: [-17.8534  -5.3631 -67.5054 -39.0549 -62.8593]\n",
      "Weights: [-4.8395  0.9843 -1.6003  0.3157  0.1094]\n",
      "MSE loss: 89.017\n",
      "Iteration: 148800\n",
      "Gradient: [ -12.3574  -29.6288   -8.5937 -104.727  -576.2128]\n",
      "Weights: [-4.8311  0.9741 -1.5994  0.3157  0.1094]\n",
      "MSE loss: 89.3281\n",
      "Iteration: 148900\n",
      "Gradient: [  12.3012  -25.244   -60.6474 -228.9429 -318.1379]\n",
      "Weights: [-4.8205  0.9804 -1.5994  0.316   0.1095]\n",
      "MSE loss: 88.9985\n",
      "Iteration: 149000\n",
      "Gradient: [-13.7637   5.761    0.8117  28.8954 315.4292]\n",
      "Weights: [-4.8331  0.9795 -1.602   0.316   0.1096]\n",
      "MSE loss: 88.9728\n",
      "Iteration: 149100\n",
      "Gradient: [   2.0633   16.1063   36.2138   60.629  -133.7387]\n",
      "Weights: [-4.8471  1.0038 -1.6098  0.3165  0.11  ]\n",
      "MSE loss: 88.9685\n",
      "Iteration: 149200\n",
      "Gradient: [  1.3262  -6.6852 -55.7232 -39.3502 -81.9002]\n",
      "Weights: [-4.8267  0.9901 -1.6152  0.3185  0.1098]\n",
      "MSE loss: 88.9304\n",
      "Iteration: 149300\n",
      "Gradient: [  -8.2101  -24.4289  -32.3516   47.2409 -209.6437]\n",
      "Weights: [-4.835   0.9967 -1.6152  0.3184  0.1098]\n",
      "MSE loss: 88.8605\n",
      "Iteration: 149400\n",
      "Gradient: [  1.4885  23.9732  51.7097 166.3757  49.8304]\n",
      "Weights: [-4.8261  0.9935 -1.6124  0.3175  0.1099]\n",
      "MSE loss: 88.783\n",
      "Iteration: 149500\n",
      "Gradient: [  1.7313 -12.2651  33.1115 -21.2     97.2788]\n",
      "Weights: [-4.8096  0.9874 -1.6109  0.3177  0.1097]\n",
      "MSE loss: 89.0781\n",
      "Iteration: 149600\n",
      "Gradient: [  16.2174   20.0063  -13.8042  -13.9264 -131.6539]\n",
      "Weights: [-4.8233  0.9934 -1.6135  0.3185  0.1095]\n",
      "MSE loss: 88.7977\n",
      "Iteration: 149700\n",
      "Gradient: [ -42.3232    6.4151  -29.6534  -48.7368 -157.3815]\n",
      "Weights: [-4.8332  0.9841 -1.6116  0.3183  0.1097]\n",
      "MSE loss: 89.3366\n",
      "Iteration: 149800\n",
      "Gradient: [  7.3461  13.5132  16.758  -73.2258  -6.4963]\n",
      "Weights: [-4.8328  0.9849 -1.6091  0.3187  0.1095]\n",
      "MSE loss: 88.8644\n",
      "Iteration: 149900\n",
      "Gradient: [  6.7709   4.1303 -27.6256  28.9663 231.8388]\n",
      "Weights: [-4.8228  0.9949 -1.6114  0.3179  0.1096]\n",
      "MSE loss: 88.9199\n",
      "Iteration: 150000\n",
      "Gradient: [ -8.6606  20.7553  17.3215 -19.5096 168.6445]\n",
      "Weights: [-4.8489  1.0029 -1.6122  0.318   0.1097]\n",
      "MSE loss: 88.962\n",
      "Iteration: 150100\n",
      "Gradient: [-23.5697  23.6313  -7.8962 -37.7576 516.564 ]\n",
      "Weights: [-4.8367  0.9819 -1.6114  0.3193  0.1095]\n",
      "MSE loss: 89.3701\n",
      "Iteration: 150200\n",
      "Gradient: [   9.6946   14.265   -51.2184  -31.6277 -174.7634]\n",
      "Weights: [-4.8258  0.9872 -1.6112  0.3187  0.1095]\n",
      "MSE loss: 88.7943\n",
      "Iteration: 150300\n",
      "Gradient: [-10.0002  -7.3343 -45.1827 -95.9328 -14.5812]\n",
      "Weights: [-4.8404  0.9907 -1.6122  0.3193  0.1094]\n",
      "MSE loss: 89.0703\n",
      "Iteration: 150400\n",
      "Gradient: [  -3.876     7.279    11.0356  -52.3819 -168.6735]\n",
      "Weights: [-4.8485  1.0061 -1.6148  0.319   0.1093]\n",
      "MSE loss: 88.9465\n",
      "Iteration: 150500\n",
      "Gradient: [ -11.9835  -17.7364  -45.9883  -79.2942 -383.6864]\n",
      "Weights: [-4.832   0.9942 -1.6137  0.3183  0.1094]\n",
      "MSE loss: 89.1081\n",
      "Iteration: 150600\n",
      "Gradient: [ 15.5658   0.7742  43.803   55.4815 124.7678]\n",
      "Weights: [-4.812   0.9804 -1.61    0.3183  0.1097]\n",
      "MSE loss: 88.8704\n",
      "Iteration: 150700\n",
      "Gradient: [ 15.6246  -1.358   61.3474 196.0798 194.9619]\n",
      "Weights: [-4.803   0.9804 -1.6135  0.3199  0.1096]\n",
      "MSE loss: 89.1837\n",
      "Iteration: 150800\n",
      "Gradient: [   6.7214   -2.4118   31.0713  -67.1179 -278.8661]\n",
      "Weights: [-4.8262  0.9944 -1.6105  0.3187  0.1095]\n",
      "MSE loss: 89.089\n",
      "Iteration: 150900\n",
      "Gradient: [  15.2709   50.0331   79.9842  176.9088 1227.0904]\n",
      "Weights: [-4.8064  0.9793 -1.6078  0.3194  0.1096]\n",
      "MSE loss: 89.9485\n",
      "Iteration: 151000\n",
      "Gradient: [  2.3912  29.4566  90.5229 355.7534 286.958 ]\n",
      "Weights: [-4.8086  0.962  -1.6065  0.3204  0.1096]\n",
      "MSE loss: 89.0769\n",
      "Iteration: 151100\n",
      "Gradient: [  13.359   -15.1331  -28.2537 -127.0123 -493.0084]\n",
      "Weights: [-4.7905  0.9682 -1.6112  0.3201  0.1096]\n",
      "MSE loss: 89.3822\n",
      "Iteration: 151200\n",
      "Gradient: [ 11.3311 -16.0691  88.3257  13.1109 -57.0945]\n",
      "Weights: [-4.8168  0.9861 -1.6129  0.3202  0.1094]\n",
      "MSE loss: 88.997\n",
      "Iteration: 151300\n",
      "Gradient: [  5.3454   7.783   19.2485  54.7733 110.8485]\n",
      "Weights: [-4.8114  0.9945 -1.6147  0.3204  0.1092]\n",
      "MSE loss: 89.5043\n",
      "Iteration: 151400\n",
      "Gradient: [   4.3039    8.7475  -33.8774  208.8196 -665.8148]\n",
      "Weights: [-4.8321  0.998  -1.6163  0.3206  0.1089]\n",
      "MSE loss: 88.8658\n",
      "Iteration: 151500\n",
      "Gradient: [  7.64     8.8113  27.1982 -39.9365  19.0004]\n",
      "Weights: [-4.8256  1.0013 -1.6173  0.3216  0.1088]\n",
      "MSE loss: 88.9253\n",
      "Iteration: 151600\n",
      "Gradient: [ -7.571   26.8798  22.3383  11.2165 127.0994]\n",
      "Weights: [-4.8317  1.0047 -1.6218  0.3228  0.1089]\n",
      "MSE loss: 88.756\n",
      "Iteration: 151700\n",
      "Gradient: [  -9.8546   13.8334  -27.8753 -114.8644 -281.9837]\n",
      "Weights: [-4.8272  1.0015 -1.6222  0.3223  0.109 ]\n",
      "MSE loss: 88.7338\n",
      "Iteration: 151800\n",
      "Gradient: [  -8.7111   -6.2637   21.776   -64.7558 -213.8335]\n",
      "Weights: [-4.8433  1.0049 -1.6248  0.3223  0.1091]\n",
      "MSE loss: 89.3438\n",
      "Iteration: 151900\n",
      "Gradient: [  11.3749  -18.9359  -59.7616  -47.2768 -596.5575]\n",
      "Weights: [-4.831   1.0052 -1.6259  0.3225  0.1094]\n",
      "MSE loss: 88.7262\n",
      "Iteration: 152000\n",
      "Gradient: [-18.6963 -23.8605  29.8664 -36.9607 -48.1618]\n",
      "Weights: [-4.8286  1.0055 -1.6313  0.3226  0.1097]\n",
      "MSE loss: 88.9565\n",
      "Iteration: 152100\n",
      "Gradient: [-4.1465  1.9529 25.7368 -8.1366 69.9077]\n",
      "Weights: [-4.8246  0.9964 -1.6302  0.3238  0.1098]\n",
      "MSE loss: 88.8779\n",
      "Iteration: 152200\n",
      "Gradient: [  -2.6066    2.3027   70.2683   57.8865 1159.0124]\n",
      "Weights: [-4.8319  1.0044 -1.6278  0.3236  0.1095]\n",
      "MSE loss: 88.783\n",
      "Iteration: 152300\n",
      "Gradient: [-13.9691 -49.0069 -24.1221 112.1248  57.2896]\n",
      "Weights: [-4.8526  1.0074 -1.6283  0.3241  0.1093]\n",
      "MSE loss: 89.3285\n",
      "Iteration: 152400\n",
      "Gradient: [  -8.3941   23.8225   12.1763 -105.2028 -217.0109]\n",
      "Weights: [-4.8276  0.9988 -1.6268  0.3243  0.109 ]\n",
      "MSE loss: 88.7599\n",
      "Iteration: 152500\n",
      "Gradient: [ -8.6339 -15.9586 -39.7751   8.4292 -88.7958]\n",
      "Weights: [-4.8228  0.9935 -1.6261  0.3241  0.1088]\n",
      "MSE loss: 89.1131\n",
      "Iteration: 152600\n",
      "Gradient: [  18.1355  -22.5254  -87.0179  126.6825 -173.6481]\n",
      "Weights: [-4.8123  0.9966 -1.6225  0.3238  0.1088]\n",
      "MSE loss: 89.1517\n",
      "Iteration: 152700\n",
      "Gradient: [  8.5365   3.6438 -46.8072 -32.9697 538.2093]\n",
      "Weights: [-4.8081  0.9913 -1.6212  0.3225  0.1093]\n",
      "MSE loss: 89.1148\n",
      "Iteration: 152800\n",
      "Gradient: [-14.4147  23.3257 -27.8158 114.9653 333.7872]\n",
      "Weights: [-4.8188  0.9933 -1.6186  0.3218  0.1093]\n",
      "MSE loss: 88.9453\n",
      "Iteration: 152900\n",
      "Gradient: [-12.9667  -3.6386   3.1251 112.3649 105.5215]\n",
      "Weights: [-4.8283  0.9893 -1.6164  0.3213  0.1092]\n",
      "MSE loss: 88.8121\n",
      "Iteration: 153000\n",
      "Gradient: [ 37.0637  12.4942  27.9285  17.1779 412.2095]\n",
      "Weights: [-4.819   1.005  -1.6163  0.3203  0.109 ]\n",
      "MSE loss: 89.3894\n",
      "Iteration: 153100\n",
      "Gradient: [  17.9992  -12.9391   13.0292 -133.3404  -21.133 ]\n",
      "Weights: [-4.8237  0.9973 -1.6147  0.3206  0.1089]\n",
      "MSE loss: 88.8703\n",
      "Iteration: 153200\n",
      "Gradient: [ 10.8046  18.4417  95.1385 199.0834 781.9451]\n",
      "Weights: [-4.8251  1.007  -1.6187  0.3206  0.1092]\n",
      "MSE loss: 89.0977\n",
      "Iteration: 153300\n",
      "Gradient: [  -6.803    16.7751   36.4014 -176.2248 -431.0844]\n",
      "Weights: [-4.8239  1.0007 -1.6194  0.3202  0.1092]\n",
      "MSE loss: 88.8738\n",
      "Iteration: 153400\n",
      "Gradient: [  -5.3287  -22.1812  -96.443  -273.1198 -452.6774]\n",
      "Weights: [-4.8526  1.0015 -1.6216  0.321   0.1094]\n",
      "MSE loss: 89.9603\n",
      "Iteration: 153500\n",
      "Gradient: [  14.37    -24.6495  -74.3299 -305.785     9.3267]\n",
      "Weights: [-4.8368  1.0126 -1.6225  0.3213  0.1091]\n",
      "MSE loss: 88.7482\n",
      "Iteration: 153600\n",
      "Gradient: [  -9.01    -18.0657   41.3618  117.4477 -364.1124]\n",
      "Weights: [-4.8256  1.0043 -1.6197  0.322   0.1089]\n",
      "MSE loss: 88.9784\n",
      "Iteration: 153700\n",
      "Gradient: [ -12.3106  -17.5799  -22.3036 -104.2559 -357.6385]\n",
      "Weights: [-4.822   0.9988 -1.6215  0.3213  0.1091]\n",
      "MSE loss: 88.8687\n",
      "Iteration: 153800\n",
      "Gradient: [ 10.7114  15.932   90.9808  93.8958 676.7916]\n",
      "Weights: [-4.8147  0.9949 -1.6192  0.322   0.1093]\n",
      "MSE loss: 89.1979\n",
      "Iteration: 153900\n",
      "Gradient: [ -0.8808  17.2979 -30.5108 208.6646 333.6236]\n",
      "Weights: [-4.8329  1.0001 -1.621   0.3224  0.1092]\n",
      "MSE loss: 88.7636\n",
      "Iteration: 154000\n",
      "Gradient: [ -9.5811  11.2454  49.2676 123.529   65.6653]\n",
      "Weights: [-4.8537  1.0113 -1.6174  0.3206  0.109 ]\n",
      "MSE loss: 88.9636\n",
      "Iteration: 154100\n",
      "Gradient: [ 21.2033  -4.1463  47.3106 -36.0199   5.8379]\n",
      "Weights: [-4.8299  1.0116 -1.6168  0.3214  0.1088]\n",
      "MSE loss: 89.9218\n",
      "Iteration: 154200\n",
      "Gradient: [ -5.5006  12.1561 -38.4055 204.8747 -78.0096]\n",
      "Weights: [-4.8474  0.9981 -1.6157  0.3204  0.1091]\n",
      "MSE loss: 89.2414\n",
      "Iteration: 154300\n",
      "Gradient: [ -6.3843  -0.803  -33.3943 -73.4537 202.1061]\n",
      "Weights: [-4.8372  0.9911 -1.6129  0.3202  0.1091]\n",
      "MSE loss: 88.9038\n",
      "Iteration: 154400\n",
      "Gradient: [  -4.6767   -6.1558   -1.2964   48.3502 -843.594 ]\n",
      "Weights: [-4.8341  0.985  -1.6132  0.3209  0.1091]\n",
      "MSE loss: 89.0373\n",
      "Iteration: 154500\n",
      "Gradient: [ -3.9964 -14.9654  -7.8953  55.0327 -53.3407]\n",
      "Weights: [-4.8246  0.9948 -1.6144  0.3208  0.109 ]\n",
      "MSE loss: 88.8254\n",
      "Iteration: 154600\n",
      "Gradient: [   1.4541  -47.4435  -56.7386  -11.4164 -101.6548]\n",
      "Weights: [-4.8302  0.9831 -1.6117  0.3207  0.1091]\n",
      "MSE loss: 88.8606\n",
      "Iteration: 154700\n",
      "Gradient: [  7.4673  26.6898  47.747   71.0852 555.2247]\n",
      "Weights: [-4.8245  1.0016 -1.614   0.32    0.1091]\n",
      "MSE loss: 89.258\n",
      "Iteration: 154800\n",
      "Gradient: [ -8.138   44.6982 -14.3605 145.8959 195.9336]\n",
      "Weights: [-4.837   0.9949 -1.6132  0.3205  0.1092]\n",
      "MSE loss: 88.8702\n",
      "Iteration: 154900\n",
      "Gradient: [  -1.6503  -38.2721  -81.9747 -215.6431 -430.04  ]\n",
      "Weights: [-4.8244  0.9891 -1.6141  0.3202  0.1089]\n",
      "MSE loss: 89.0582\n",
      "Iteration: 155000\n",
      "Gradient: [  8.989   26.5152  19.5945 -58.402  365.8551]\n",
      "Weights: [-4.8288  0.9841 -1.6132  0.3216  0.1088]\n",
      "MSE loss: 88.8483\n",
      "Iteration: 155100\n",
      "Gradient: [ 12.0809  49.6451  67.0386   7.9085 367.9784]\n",
      "Weights: [-4.8     0.9861 -1.6142  0.3222  0.109 ]\n",
      "MSE loss: 90.6198\n",
      "Iteration: 155200\n",
      "Gradient: [ -8.9519  13.5338  91.4895 236.2163 218.0271]\n",
      "Weights: [-4.8256  0.9914 -1.6173  0.3215  0.1089]\n",
      "MSE loss: 88.8224\n",
      "Iteration: 155300\n",
      "Gradient: [-15.4046   7.7324  11.9158  78.2127 299.4467]\n",
      "Weights: [-4.8326  0.991  -1.6162  0.3211  0.1091]\n",
      "MSE loss: 88.8834\n",
      "Iteration: 155400\n",
      "Gradient: [  14.3076  -21.6007  -44.2236 -132.6372 -175.0758]\n",
      "Weights: [-4.8042  0.9901 -1.6159  0.3213  0.109 ]\n",
      "MSE loss: 89.456\n",
      "Iteration: 155500\n",
      "Gradient: [ -20.1143   -3.0335   18.4147  -45.3787 -290.7946]\n",
      "Weights: [-4.8485  0.9945 -1.6168  0.3212  0.1089]\n",
      "MSE loss: 89.8603\n",
      "Iteration: 155600\n",
      "Gradient: [   2.3133   23.018    68.5799 -185.5887  -18.7176]\n",
      "Weights: [-4.8136  0.9935 -1.6168  0.3216  0.1091]\n",
      "MSE loss: 89.226\n",
      "Iteration: 155700\n",
      "Gradient: [  9.5298  31.1999  45.1201 115.0625 300.9687]\n",
      "Weights: [-4.8318  1.0029 -1.6204  0.3209  0.1093]\n",
      "MSE loss: 88.742\n",
      "Iteration: 155800\n",
      "Gradient: [ 25.8383  28.1304  50.2889 123.1364 485.3094]\n",
      "Weights: [-4.8169  1.0018 -1.6186  0.3211  0.1093]\n",
      "MSE loss: 89.5924\n",
      "Iteration: 155900\n",
      "Gradient: [-10.7793  18.8621  15.0426 -37.8189  66.9116]\n",
      "Weights: [-4.8227  1.0015 -1.6196  0.321   0.1095]\n",
      "MSE loss: 89.1162\n",
      "Iteration: 156000\n",
      "Gradient: [ 19.1321   6.6559  14.2571 250.1194  68.1672]\n",
      "Weights: [-4.8136  0.9898 -1.6177  0.3212  0.1094]\n",
      "MSE loss: 88.9978\n",
      "Iteration: 156100\n",
      "Gradient: [   5.267   -21.5157  -38.9758   80.9082 -249.7267]\n",
      "Weights: [-4.82    0.9894 -1.617   0.321   0.1093]\n",
      "MSE loss: 88.7673\n",
      "Iteration: 156200\n",
      "Gradient: [  13.1385  -22.5845  -41.2886    9.1261 -145.1827]\n",
      "Weights: [-4.8073  0.9823 -1.6139  0.3205  0.1095]\n",
      "MSE loss: 89.0844\n",
      "Iteration: 156300\n",
      "Gradient: [ 16.4527  -7.6998  -6.4139 -69.5136  65.0461]\n",
      "Weights: [-4.8159  0.9842 -1.6156  0.3208  0.1094]\n",
      "MSE loss: 88.7967\n",
      "Iteration: 156400\n",
      "Gradient: [ -13.1821  -15.377   -21.2285  -14.7197 -605.6795]\n",
      "Weights: [-4.8238  0.9877 -1.6167  0.3202  0.1098]\n",
      "MSE loss: 88.8036\n",
      "Iteration: 156500\n",
      "Gradient: [  -3.157    10.21     18.5399   81.4976 -432.2604]\n",
      "Weights: [-4.8435  1.0009 -1.6167  0.3193  0.1097]\n",
      "MSE loss: 88.9233\n",
      "Iteration: 156600\n",
      "Gradient: [  12.9718  -20.5363   21.4963  -35.2362 -337.9589]\n",
      "Weights: [-4.8147  0.9983 -1.6197  0.3199  0.1098]\n",
      "MSE loss: 89.1199\n",
      "Iteration: 156700\n",
      "Gradient: [ -0.8923  39.8453  26.1823 257.5489  60.9569]\n",
      "Weights: [-4.8373  1.0056 -1.6239  0.3207  0.1098]\n",
      "MSE loss: 88.8134\n",
      "Iteration: 156800\n",
      "Gradient: [-16.0547  16.0025  58.5414 196.1772 314.2267]\n",
      "Weights: [-4.8461  1.0011 -1.6273  0.322   0.1097]\n",
      "MSE loss: 89.9881\n",
      "Iteration: 156900\n",
      "Gradient: [   6.8301   13.5432   30.1784  121.7485 -327.7972]\n",
      "Weights: [-4.8181  0.9953 -1.6282  0.3226  0.1097]\n",
      "MSE loss: 88.8972\n",
      "Iteration: 157000\n",
      "Gradient: [ -3.3155 -11.8176 -15.0508 -58.5211 131.781 ]\n",
      "Weights: [-4.8453  1.0073 -1.6252  0.3213  0.1097]\n",
      "MSE loss: 89.1047\n",
      "Iteration: 157100\n",
      "Gradient: [-17.5043  -1.2337 107.6605 189.1984 487.3132]\n",
      "Weights: [-4.8441  1.0194 -1.6247  0.3202  0.1097]\n",
      "MSE loss: 88.8148\n",
      "Iteration: 157200\n",
      "Gradient: [  -6.4638   11.4229  -54.5426 -136.0249  -75.0194]\n",
      "Weights: [-4.8495  1.0127 -1.6213  0.3193  0.1099]\n",
      "MSE loss: 88.9208\n",
      "Iteration: 157300\n",
      "Gradient: [  0.4144   9.7773 -49.6677 -93.4569 262.534 ]\n",
      "Weights: [-4.8322  1.0007 -1.624   0.3209  0.1097]\n",
      "MSE loss: 88.9674\n",
      "Iteration: 157400\n",
      "Gradient: [ -15.1655   14.2084  -44.4391   -0.6754 -234.4029]\n",
      "Weights: [-4.8422  0.9947 -1.6203  0.3214  0.1097]\n",
      "MSE loss: 89.1713\n",
      "Iteration: 157500\n",
      "Gradient: [  19.3      -5.6896  -11.6863 -127.2007 -147.3377]\n",
      "Weights: [-4.82    1.0031 -1.625   0.322   0.1097]\n",
      "MSE loss: 89.0114\n",
      "Iteration: 157600\n",
      "Gradient: [ -8.4943   9.2108 -23.0523  43.4161 417.1039]\n",
      "Weights: [-4.847   1.0048 -1.6263  0.3223  0.1093]\n",
      "MSE loss: 89.6362\n",
      "Iteration: 157700\n",
      "Gradient: [   4.0448  -21.816   -28.8646 -124.0113 -555.3085]\n",
      "Weights: [-4.8403  1.0123 -1.6265  0.3227  0.109 ]\n",
      "MSE loss: 88.7715\n",
      "Iteration: 157800\n",
      "Gradient: [  -3.7421  -29.8126  -39.0687 -164.8204   83.4449]\n",
      "Weights: [-4.8368  1.0192 -1.6299  0.3236  0.1089]\n",
      "MSE loss: 88.7184\n",
      "Iteration: 157900\n",
      "Gradient: [  4.7921 -24.9706  40.0184  55.6342 124.2825]\n",
      "Weights: [-4.8223  1.0148 -1.6318  0.3251  0.1088]\n",
      "MSE loss: 89.041\n",
      "Iteration: 158000\n",
      "Gradient: [  3.6156  15.3479  -4.1012 141.5901 534.4248]\n",
      "Weights: [-4.8313  1.0112 -1.6301  0.3255  0.1087]\n",
      "MSE loss: 88.738\n",
      "Iteration: 158100\n",
      "Gradient: [ 14.5945  20.2081  58.7689 100.9867 -22.4854]\n",
      "Weights: [-4.8368  1.0077 -1.6295  0.3256  0.1086]\n",
      "MSE loss: 88.7478\n",
      "Iteration: 158200\n",
      "Gradient: [  0.5845  28.5175  92.2232 145.6864 270.4349]\n",
      "Weights: [-4.8308  1.0111 -1.6279  0.3267  0.108 ]\n",
      "MSE loss: 88.8622\n",
      "Iteration: 158300\n",
      "Gradient: [ 11.7518 -13.1513 -10.915    9.8659 204.6099]\n",
      "Weights: [-4.8314  1.0154 -1.6327  0.3271  0.1083]\n",
      "MSE loss: 88.8014\n",
      "Iteration: 158400\n",
      "Gradient: [  -8.7648  -29.0645  -36.481  -117.1979 -351.8634]\n",
      "Weights: [-4.832   1.0026 -1.6315  0.3272  0.1082]\n",
      "MSE loss: 88.9292\n",
      "Iteration: 158500\n",
      "Gradient: [-15.8261 -23.2991 -57.2264  15.5837 214.7383]\n",
      "Weights: [-4.8355  1.0014 -1.6346  0.3294  0.1082]\n",
      "MSE loss: 88.8794\n",
      "Iteration: 158600\n",
      "Gradient: [  -5.9229  -13.4191    1.2186 -114.9217 -325.2175]\n",
      "Weights: [-4.8267  1.0017 -1.6388  0.3306  0.1082]\n",
      "MSE loss: 88.7377\n",
      "Iteration: 158700\n",
      "Gradient: [   0.7242   13.979    12.6508 -136.1028  318.7476]\n",
      "Weights: [-4.8352  1.0151 -1.6387  0.3303  0.1079]\n",
      "MSE loss: 88.6376\n",
      "Iteration: 158800\n",
      "Gradient: [  4.4683  -8.5482   9.4723 140.0001 148.3923]\n",
      "Weights: [-4.8492  1.0256 -1.6379  0.3297  0.1076]\n",
      "MSE loss: 88.7104\n",
      "Iteration: 158900\n",
      "Gradient: [-31.5436  14.1035  37.6437 -82.149  -27.0834]\n",
      "Weights: [-4.8519  1.0147 -1.6389  0.3306  0.1076]\n",
      "MSE loss: 89.543\n",
      "Iteration: 159000\n",
      "Gradient: [ -7.1389  -9.9579  28.3518  96.8359 358.5313]\n",
      "Weights: [-4.8428  1.0198 -1.64    0.3316  0.1076]\n",
      "MSE loss: 88.6921\n",
      "Iteration: 159100\n",
      "Gradient: [  -9.9583   13.7168  -98.3702 -188.0862 -294.0584]\n",
      "Weights: [-4.8466  1.0223 -1.6429  0.3305  0.1079]\n",
      "MSE loss: 88.8936\n",
      "Iteration: 159200\n",
      "Gradient: [-10.0671   9.5887  17.4502 -50.1272 170.7267]\n",
      "Weights: [-4.8472  1.0336 -1.6444  0.3306  0.1079]\n",
      "MSE loss: 88.7123\n",
      "Iteration: 159300\n",
      "Gradient: [   4.6378  -29.953   -87.2592  -49.055  -565.8443]\n",
      "Weights: [-4.8094  1.0247 -1.6473  0.3312  0.108 ]\n",
      "MSE loss: 89.6516\n",
      "Iteration: 159400\n",
      "Gradient: [ -0.7083  -1.5338  18.0049 -88.2353 396.903 ]\n",
      "Weights: [-4.8172  1.0074 -1.6482  0.3324  0.1083]\n",
      "MSE loss: 88.7959\n",
      "Iteration: 159500\n",
      "Gradient: [  -3.1928   -0.3404   52.8297  144.6553 -112.1877]\n",
      "Weights: [-4.8342  1.0295 -1.6479  0.3319  0.1082]\n",
      "MSE loss: 88.9191\n",
      "Iteration: 159600\n",
      "Gradient: [  2.8112  12.1307 -49.03    -8.073  254.7613]\n",
      "Weights: [-4.8386  1.03   -1.6512  0.3312  0.1085]\n",
      "MSE loss: 88.6431\n",
      "Iteration: 159700\n",
      "Gradient: [ -0.4855  25.1075   7.0819 118.9469 272.1869]\n",
      "Weights: [-4.8313  1.0422 -1.6526  0.3297  0.1088]\n",
      "MSE loss: 88.9795\n",
      "Iteration: 159800\n",
      "Gradient: [  8.6801  31.2249  41.962  -99.5712 289.0697]\n",
      "Weights: [-4.8235  1.0352 -1.6538  0.3314  0.1086]\n",
      "MSE loss: 89.0621\n",
      "Iteration: 159900\n",
      "Gradient: [  6.6087  22.2817  68.2901 436.2931 525.5505]\n",
      "Weights: [-4.8322  1.0444 -1.6554  0.3311  0.1087]\n",
      "MSE loss: 89.0996\n",
      "Iteration: 160000\n",
      "Gradient: [  -0.6062    0.5046   -6.7164  -28.0044 -225.1877]\n",
      "Weights: [-4.8464  1.0273 -1.651   0.3309  0.1084]\n",
      "MSE loss: 89.3415\n",
      "Iteration: 160100\n",
      "Gradient: [  11.5291    6.4361  -68.9286  -57.61   -432.6995]\n",
      "Weights: [-4.8363  1.0351 -1.6521  0.3324  0.1081]\n",
      "MSE loss: 88.7103\n",
      "Iteration: 160200\n",
      "Gradient: [-1.900000e-02  4.278900e+00  3.629380e+01  4.297300e+00  3.914909e+02]\n",
      "Weights: [-4.8363  1.0267 -1.6468  0.3312  0.1082]\n",
      "MSE loss: 88.6556\n",
      "Iteration: 160300\n",
      "Gradient: [  -2.0575    4.1094  -13.6008   27.2954 -221.8963]\n",
      "Weights: [-4.8488  1.0362 -1.6463  0.3302  0.1079]\n",
      "MSE loss: 88.7182\n",
      "Iteration: 160400\n",
      "Gradient: [-12.8925  35.7811  40.6555 164.2015  43.8712]\n",
      "Weights: [-4.8419  1.0289 -1.6472  0.3308  0.1081]\n",
      "MSE loss: 88.6546\n",
      "Iteration: 160500\n",
      "Gradient: [ 16.2343 -18.4622  16.8789   3.159  277.361 ]\n",
      "Weights: [-4.8279  1.0199 -1.6479  0.3316  0.108 ]\n",
      "MSE loss: 88.71\n",
      "Iteration: 160600\n",
      "Gradient: [ 22.9898  22.2937 103.5333 -35.1499 691.2729]\n",
      "Weights: [-4.8199  1.0227 -1.6433  0.3322  0.1079]\n",
      "MSE loss: 90.4116\n",
      "Iteration: 160700\n",
      "Gradient: [4.821000e-01 1.955940e+01 4.242910e+01 1.231878e+02 4.999926e+02]\n",
      "Weights: [-4.8454  1.0347 -1.6506  0.3314  0.1078]\n",
      "MSE loss: 88.92\n",
      "Iteration: 160800\n",
      "Gradient: [ -2.6177  68.458  100.6748 100.791  302.7141]\n",
      "Weights: [-4.8372  1.0401 -1.65    0.333   0.1077]\n",
      "MSE loss: 89.3144\n",
      "Iteration: 160900\n",
      "Gradient: [ -11.9508   13.064    59.3883  114.1743 -184.9176]\n",
      "Weights: [-4.828   1.0144 -1.6487  0.3338  0.1076]\n",
      "MSE loss: 88.7074\n",
      "Iteration: 161000\n",
      "Gradient: [  2.7391 -15.2235 -35.2119 -73.6849  77.5163]\n",
      "Weights: [-4.8308  1.0205 -1.6469  0.3338  0.1074]\n",
      "MSE loss: 88.6139\n",
      "Iteration: 161100\n",
      "Gradient: [  4.3828  56.6743  51.1296  70.4361 332.4775]\n",
      "Weights: [-4.8308  1.0295 -1.6474  0.3328  0.1075]\n",
      "MSE loss: 88.8872\n",
      "Iteration: 161200\n",
      "Gradient: [ 14.7922 -13.3384  82.6791   8.874  281.4533]\n",
      "Weights: [-4.8248  1.0384 -1.6508  0.3332  0.1076]\n",
      "MSE loss: 89.7436\n",
      "Iteration: 161300\n",
      "Gradient: [  -4.4668  -23.1478   -1.6044 -143.5956  299.0692]\n",
      "Weights: [-4.8494  1.0362 -1.6467  0.3319  0.1074]\n",
      "MSE loss: 88.6898\n",
      "Iteration: 161400\n",
      "Gradient: [ -6.0598   0.7611 121.3878 120.095  321.0839]\n",
      "Weights: [-4.8452  1.0258 -1.6431  0.3323  0.1075]\n",
      "MSE loss: 88.7195\n",
      "Iteration: 161500\n",
      "Gradient: [-25.5429  23.7879  71.3356 267.0337 749.3247]\n",
      "Weights: [-4.8563  1.0262 -1.6451  0.3318  0.1077]\n",
      "MSE loss: 89.2408\n",
      "Iteration: 161600\n",
      "Gradient: [   2.3336   20.7715   53.0047   93.8354 -248.3068]\n",
      "Weights: [-4.8175  1.0199 -1.6497  0.3318  0.1082]\n",
      "MSE loss: 88.7727\n",
      "Iteration: 161700\n",
      "Gradient: [   5.0639  -38.7142  -42.2275 -128.8136 -145.7652]\n",
      "Weights: [-4.8371  1.0366 -1.6521  0.3319  0.108 ]\n",
      "MSE loss: 88.6213\n",
      "Iteration: 161800\n",
      "Gradient: [ -11.2487  -11.1752   41.5694 -257.289  -195.6505]\n",
      "Weights: [-4.8555  1.0376 -1.6513  0.3311  0.1083]\n",
      "MSE loss: 88.9198\n",
      "Iteration: 161900\n",
      "Gradient: [  -7.7115   -3.6892  -23.5903 -155.8603 -334.3452]\n",
      "Weights: [-4.84    1.035  -1.6523  0.3306  0.1083]\n",
      "MSE loss: 88.821\n",
      "Iteration: 162000\n",
      "Gradient: [  0.851    8.3464 -49.8207 -92.9056 -72.2923]\n",
      "Weights: [-4.8233  1.0328 -1.6526  0.3314  0.1085]\n",
      "MSE loss: 89.0435\n",
      "Iteration: 162100\n",
      "Gradient: [-15.4442  -5.4355   6.1958  -4.3484 117.3259]\n",
      "Weights: [-4.8389  1.0328 -1.6496  0.3314  0.1081]\n",
      "MSE loss: 88.6192\n",
      "Iteration: 162200\n",
      "Gradient: [ 13.3332  -1.6882  73.5443 -80.9395  49.0431]\n",
      "Weights: [-4.8355  1.046  -1.6537  0.3322  0.1081]\n",
      "MSE loss: 89.3331\n",
      "Iteration: 162300\n",
      "Gradient: [ 12.078   -2.167   -7.6143  68.0758 183.165 ]\n",
      "Weights: [-4.8233  1.0429 -1.6534  0.3315  0.1079]\n",
      "MSE loss: 89.232\n",
      "Iteration: 162400\n",
      "Gradient: [ -6.5456  11.1983  53.6377 208.7975 616.8126]\n",
      "Weights: [-4.862   1.052  -1.6558  0.3327  0.1079]\n",
      "MSE loss: 88.7729\n",
      "Iteration: 162500\n",
      "Gradient: [  16.9142  -13.6685  -25.3805   53.9414 -128.241 ]\n",
      "Weights: [-4.8166  1.0324 -1.6585  0.3328  0.1083]\n",
      "MSE loss: 88.911\n",
      "Iteration: 162600\n",
      "Gradient: [  18.081    15.8676  -21.1324   34.6187 -194.8943]\n",
      "Weights: [-4.8285  1.0388 -1.6581  0.3329  0.1084]\n",
      "MSE loss: 88.8197\n",
      "Iteration: 162700\n",
      "Gradient: [   6.5504   16.897   -37.144    -4.4342 -132.9072]\n",
      "Weights: [-4.8223  1.0248 -1.6536  0.3328  0.1084]\n",
      "MSE loss: 88.7629\n",
      "Iteration: 162800\n",
      "Gradient: [ -5.1585   4.4635  83.2243 -67.941   96.2166]\n",
      "Weights: [-4.833   1.0283 -1.6586  0.3335  0.1084]\n",
      "MSE loss: 88.822\n",
      "Iteration: 162900\n",
      "Gradient: [   0.3434   33.4      -6.8773   36.3954 -146.7709]\n",
      "Weights: [-4.8393  1.0284 -1.6551  0.333   0.1082]\n",
      "MSE loss: 88.826\n",
      "Iteration: 163000\n",
      "Gradient: [ 15.2658 -10.5429 -34.4771  72.9082 240.8079]\n",
      "Weights: [-4.8255  1.0426 -1.6532  0.3317  0.1083]\n",
      "MSE loss: 89.7998\n",
      "Iteration: 163100\n",
      "Gradient: [  13.0787  -21.1485   10.4589  -52.1568 -487.6945]\n",
      "Weights: [-4.8362  1.0456 -1.653   0.3307  0.108 ]\n",
      "MSE loss: 88.7798\n",
      "Iteration: 163200\n",
      "Gradient: [   3.5243   -9.3349   -7.3334    7.7894 -191.3728]\n",
      "Weights: [-4.8694  1.0647 -1.6568  0.3312  0.1082]\n",
      "MSE loss: 88.8575\n",
      "Iteration: 163300\n",
      "Gradient: [   7.2176  -20.4833   19.5093 -167.6799   26.8763]\n",
      "Weights: [-4.8305  1.0486 -1.6555  0.332   0.1082]\n",
      "MSE loss: 89.5364\n",
      "Iteration: 163400\n",
      "Gradient: [  2.3096  12.3802  47.5716 167.4638 501.8514]\n",
      "Weights: [-4.8422  1.0571 -1.6603  0.3317  0.1084]\n",
      "MSE loss: 88.8053\n",
      "Iteration: 163500\n",
      "Gradient: [ 12.1888  31.0615  89.2786 136.677  511.9363]\n",
      "Weights: [-4.8428  1.0622 -1.6596  0.3307  0.1087]\n",
      "MSE loss: 89.3377\n",
      "Iteration: 163600\n",
      "Gradient: [ -4.0947  31.4432 -14.4335  61.7576 534.866 ]\n",
      "Weights: [-4.8733  1.0696 -1.6595  0.3312  0.1086]\n",
      "MSE loss: 89.0417\n",
      "Iteration: 163700\n",
      "Gradient: [  4.1845  38.5692  89.3389 180.9048 287.1882]\n",
      "Weights: [-4.8718  1.0677 -1.6621  0.3315  0.1086]\n",
      "MSE loss: 88.9126\n",
      "Iteration: 163800\n",
      "Gradient: [  8.0291  28.834  -49.7052  24.954  255.6235]\n",
      "Weights: [-4.8555  1.065  -1.6634  0.3311  0.1087]\n",
      "MSE loss: 88.6669\n",
      "Iteration: 163900\n",
      "Gradient: [ -1.0081  16.9838 -15.3663 164.2721 187.9935]\n",
      "Weights: [-4.859   1.0674 -1.663   0.3312  0.1088]\n",
      "MSE loss: 88.7913\n",
      "Iteration: 164000\n",
      "Gradient: [ -7.8102  47.5843  38.67    -8.3376 263.4383]\n",
      "Weights: [-4.8559  1.0607 -1.6663  0.3321  0.1091]\n",
      "MSE loss: 88.7193\n",
      "Iteration: 164100\n",
      "Gradient: [  -0.4594   -1.5322  -37.8454 -141.3818   28.2608]\n",
      "Weights: [-4.8337  1.0403 -1.6607  0.3325  0.109 ]\n",
      "MSE loss: 88.743\n",
      "Iteration: 164200\n",
      "Gradient: [ -25.2087  -15.5028  -59.3607  -17.819  -431.5088]\n",
      "Weights: [-4.8523  1.0367 -1.6616  0.3339  0.1084]\n",
      "MSE loss: 89.4671\n",
      "Iteration: 164300\n",
      "Gradient: [  32.9949  -24.1579  -73.6606   23.9708 -611.3736]\n",
      "Weights: [-4.8245  1.0466 -1.6621  0.3328  0.1085]\n",
      "MSE loss: 89.0464\n",
      "Iteration: 164400\n",
      "Gradient: [  -1.5249   -7.4862  -51.4033 -128.7087  285.6821]\n",
      "Weights: [-4.8306  1.0423 -1.659   0.3334  0.1083]\n",
      "MSE loss: 88.919\n",
      "Iteration: 164500\n",
      "Gradient: [  7.0796  -2.7551   5.1509 -24.3808 187.3452]\n",
      "Weights: [-4.8393  1.0474 -1.66    0.3339  0.108 ]\n",
      "MSE loss: 88.6893\n",
      "Iteration: 164600\n",
      "Gradient: [  -4.2366   -6.3156  -45.4664 -236.069  -558.2778]\n",
      "Weights: [-4.841   1.0345 -1.6649  0.3349  0.1081]\n",
      "MSE loss: 89.8939\n",
      "Iteration: 164700\n",
      "Gradient: [  8.8479  13.1656  47.8777 117.7603 623.4054]\n",
      "Weights: [-4.8382  1.0326 -1.6622  0.3356  0.1081]\n",
      "MSE loss: 88.7453\n",
      "Iteration: 164800\n",
      "Gradient: [-10.9216  -5.3017  13.6903 212.6091 544.3505]\n",
      "Weights: [-4.8455  1.0357 -1.6584  0.334   0.1078]\n",
      "MSE loss: 89.1118\n",
      "Iteration: 164900\n",
      "Gradient: [ 14.0638  -9.5313  44.9426 109.9353 296.5295]\n",
      "Weights: [-4.8322  1.0393 -1.6599  0.334   0.1079]\n",
      "MSE loss: 88.6431\n",
      "Iteration: 165000\n",
      "Gradient: [ 10.1763  13.69    57.7257  -3.4796 548.4323]\n",
      "Weights: [-4.8405  1.0566 -1.6581  0.3339  0.1075]\n",
      "MSE loss: 89.2428\n",
      "Iteration: 165100\n",
      "Gradient: [  9.1813  11.3555 -45.1649 -54.2321  92.7599]\n",
      "Weights: [-4.8477  1.0523 -1.6596  0.3334  0.1078]\n",
      "MSE loss: 88.6159\n",
      "Iteration: 165200\n",
      "Gradient: [  6.1555  -1.2289   2.4512  99.4331 118.0983]\n",
      "Weights: [-4.8486  1.0544 -1.6605  0.3335  0.1078]\n",
      "MSE loss: 88.6157\n",
      "Iteration: 165300\n",
      "Gradient: [ -16.5445   31.8303   46.2319  -83.8179 -520.64  ]\n",
      "Weights: [-4.8559  1.0549 -1.6633  0.3344  0.1082]\n",
      "MSE loss: 88.7217\n",
      "Iteration: 165400\n",
      "Gradient: [ -23.2534  -36.4911  -74.4109 -198.9204 -231.6258]\n",
      "Weights: [-4.8503  1.0505 -1.6672  0.3333  0.1086]\n",
      "MSE loss: 89.1827\n",
      "Iteration: 165500\n",
      "Gradient: [-26.2776  -4.1156 -51.0102 -35.7699 341.9923]\n",
      "Weights: [-4.8548  1.0475 -1.6669  0.3333  0.1088]\n",
      "MSE loss: 89.5335\n",
      "Iteration: 165600\n",
      "Gradient: [ -14.9936    1.8502  -34.5264 -125.2043 -216.8151]\n",
      "Weights: [-4.8487  1.0437 -1.6621  0.3325  0.109 ]\n",
      "MSE loss: 88.8009\n",
      "Iteration: 165700\n",
      "Gradient: [  2.549    2.2482   8.3632 -38.8492  47.9363]\n",
      "Weights: [-4.8425  1.0473 -1.6584  0.3322  0.1084]\n",
      "MSE loss: 88.6389\n",
      "Iteration: 165800\n",
      "Gradient: [   9.0458   -3.4984   53.0506   27.8891 -440.7181]\n",
      "Weights: [-4.8462  1.0571 -1.6598  0.3329  0.1083]\n",
      "MSE loss: 89.1023\n",
      "Iteration: 165900\n",
      "Gradient: [   6.3081  -26.0051    5.9407 -112.1037 -567.2238]\n",
      "Weights: [-4.8373  1.0511 -1.6577  0.3316  0.1084]\n",
      "MSE loss: 88.8822\n",
      "Iteration: 166000\n",
      "Gradient: [  -7.8745   28.7286  -27.7213  102.0442 -406.9412]\n",
      "Weights: [-4.8399  1.0413 -1.6583  0.3321  0.1086]\n",
      "MSE loss: 88.6167\n",
      "Iteration: 166100\n",
      "Gradient: [  10.8918   -4.2278   48.8908 -107.1396  -46.0598]\n",
      "Weights: [-4.8294  1.0304 -1.6597  0.333   0.1087]\n",
      "MSE loss: 88.7074\n",
      "Iteration: 166200\n",
      "Gradient: [ -2.7066 -12.7412 -38.6447 167.1909 123.0193]\n",
      "Weights: [-4.8344  1.037  -1.6574  0.3328  0.1083]\n",
      "MSE loss: 88.6115\n",
      "Iteration: 166300\n",
      "Gradient: [  2.3691 -21.2842  16.8583 -84.4752  13.2269]\n",
      "Weights: [-4.8237  1.0256 -1.6555  0.3336  0.108 ]\n",
      "MSE loss: 88.7015\n",
      "Iteration: 166400\n",
      "Gradient: [  1.2233  17.2705  30.9969  37.2954 141.7453]\n",
      "Weights: [-4.8396  1.0297 -1.6548  0.3349  0.1079]\n",
      "MSE loss: 88.6688\n",
      "Iteration: 166500\n",
      "Gradient: [-15.5016   0.3763 -11.4549 168.5068 162.6936]\n",
      "Weights: [-4.8533  1.0365 -1.6538  0.3344  0.1076]\n",
      "MSE loss: 88.7531\n",
      "Iteration: 166600\n",
      "Gradient: [   8.3205  -10.8211    7.0382 -209.4132 -285.8699]\n",
      "Weights: [-4.8347  1.0419 -1.6581  0.3347  0.1079]\n",
      "MSE loss: 88.9171\n",
      "Iteration: 166700\n",
      "Gradient: [  7.3481  45.0995  47.4475 121.7548 173.0063]\n",
      "Weights: [-4.8215  1.034  -1.659   0.3346  0.1082]\n",
      "MSE loss: 89.0584\n",
      "Iteration: 166800\n",
      "Gradient: [ -19.0372   -2.583   -52.4334 -136.8931 -491.8201]\n",
      "Weights: [-4.8276  1.0239 -1.6581  0.3354  0.1078]\n",
      "MSE loss: 88.6954\n",
      "Iteration: 166900\n",
      "Gradient: [ 12.3036  14.7982 -29.2193  20.3317  60.7876]\n",
      "Weights: [-4.8283  1.0466 -1.6593  0.3343  0.1075]\n",
      "MSE loss: 88.9572\n",
      "Iteration: 167000\n",
      "Gradient: [  2.7532   5.4067 -22.4098 -19.3051  70.4327]\n",
      "Weights: [-4.841   1.0462 -1.6586  0.3334  0.1077]\n",
      "MSE loss: 88.6599\n",
      "Iteration: 167100\n",
      "Gradient: [  -9.3151  -66.1095  -84.6701 -227.6    -175.5809]\n",
      "Weights: [-4.8333  1.0405 -1.658   0.3344  0.1078]\n",
      "MSE loss: 88.6922\n",
      "Iteration: 167200\n",
      "Gradient: [ 12.0834  22.1893  -4.7008 -28.1573 200.6475]\n",
      "Weights: [-4.8272  1.0407 -1.6619  0.3351  0.108 ]\n",
      "MSE loss: 88.8388\n",
      "Iteration: 167300\n",
      "Gradient: [  15.8999   14.3261   46.449   -93.4049 -306.1573]\n",
      "Weights: [-4.7962  1.0244 -1.6654  0.3365  0.1084]\n",
      "MSE loss: 89.7993\n",
      "Iteration: 167400\n",
      "Gradient: [ -4.8243  21.9088  70.5938 221.2602 735.9725]\n",
      "Weights: [-4.8319  1.0309 -1.664   0.3356  0.1084]\n",
      "MSE loss: 88.7366\n",
      "Iteration: 167500\n",
      "Gradient: [ 43.7664  19.8314  35.5838 200.7556 378.2559]\n",
      "Weights: [-4.8088  1.0431 -1.6677  0.3358  0.1085]\n",
      "MSE loss: 90.3603\n",
      "Iteration: 167600\n",
      "Gradient: [-10.912   -1.6546 -75.1743 -16.3373 766.1558]\n",
      "Weights: [-4.8322  1.0433 -1.6676  0.3367  0.1083]\n",
      "MSE loss: 88.8098\n",
      "Iteration: 167700\n",
      "Gradient: [ -8.1227  14.7699  55.9628 -50.2261  37.0921]\n",
      "Weights: [-4.8536  1.0545 -1.6678  0.3368  0.1076]\n",
      "MSE loss: 88.6491\n",
      "Iteration: 167800\n",
      "Gradient: [ -7.2693  20.1958  30.1347 101.3257 252.5161]\n",
      "Weights: [-4.8327  1.0462 -1.6683  0.3386  0.1075]\n",
      "MSE loss: 88.7427\n",
      "Iteration: 167900\n",
      "Gradient: [  -4.483   -10.081   -37.7292  157.7881 -177.8469]\n",
      "Weights: [-4.8599  1.0413 -1.6697  0.3385  0.1075]\n",
      "MSE loss: 90.3668\n",
      "Iteration: 168000\n",
      "Gradient: [ -27.4327  -30.095   -20.5903 -111.6231 -246.1542]\n",
      "Weights: [-4.8606  1.0258 -1.6648  0.3393  0.1072]\n",
      "MSE loss: 91.4469\n",
      "Iteration: 168100\n",
      "Gradient: [   1.9495  -34.6156  -82.1386   70.9194 -480.2633]\n",
      "Weights: [-4.8444  1.0332 -1.6645  0.3392  0.1069]\n",
      "MSE loss: 89.1698\n",
      "Iteration: 168200\n",
      "Gradient: [-13.6179 -24.7794 -32.1648   7.6231  25.1661]\n",
      "Weights: [-4.8538  1.0401 -1.6654  0.339   0.107 ]\n",
      "MSE loss: 89.1015\n",
      "Iteration: 168300\n",
      "Gradient: [   2.2166   10.2443  -20.3799 -149.6748 -179.1869]\n",
      "Weights: [-4.845   1.0463 -1.6685  0.34    0.1067]\n",
      "MSE loss: 88.6129\n",
      "Iteration: 168400\n",
      "Gradient: [  7.2245  12.4773  23.1326  -3.6157 227.2151]\n",
      "Weights: [-4.8484  1.055  -1.6693  0.3402  0.1067]\n",
      "MSE loss: 88.5516\n",
      "Iteration: 168500\n",
      "Gradient: [  -8.5991   18.4352  -28.8273   48.2296 -284.4864]\n",
      "Weights: [-4.8664  1.0549 -1.6698  0.3402  0.1068]\n",
      "MSE loss: 89.0276\n",
      "Iteration: 168600\n",
      "Gradient: [  8.9622  42.0045  34.1488 181.5289 184.5232]\n",
      "Weights: [-4.8432  1.0679 -1.6693  0.3393  0.1067]\n",
      "MSE loss: 89.5254\n",
      "Iteration: 168700\n",
      "Gradient: [  1.4765  12.4072  47.0545 186.5409  73.2928]\n",
      "Weights: [-4.8416  1.0535 -1.6683  0.3401  0.1066]\n",
      "MSE loss: 88.6398\n",
      "Iteration: 168800\n",
      "Gradient: [  5.1758  30.7173  91.8066  88.6028 368.5768]\n",
      "Weights: [-4.8387  1.0434 -1.6661  0.3399  0.1067]\n",
      "MSE loss: 88.5142\n",
      "Iteration: 168900\n",
      "Gradient: [ -6.5166  45.9251  50.5806  35.3911 406.5258]\n",
      "Weights: [-4.8491  1.0507 -1.6672  0.3403  0.1066]\n",
      "MSE loss: 88.5674\n",
      "Iteration: 169000\n",
      "Gradient: [ -9.9777  12.9523  -2.0009  10.8792 647.8495]\n",
      "Weights: [-4.8421  1.0313 -1.6644  0.3399  0.1066]\n",
      "MSE loss: 89.1648\n",
      "Iteration: 169100\n",
      "Gradient: [ 13.3385  -7.6078  61.221  126.6045  91.1066]\n",
      "Weights: [-4.8361  1.0419 -1.6617  0.3404  0.1063]\n",
      "MSE loss: 88.9035\n",
      "Iteration: 169200\n",
      "Gradient: [2.272000e-01 7.887200e+00 5.614660e+01 3.072960e+01 3.734481e+02]\n",
      "Weights: [-4.8523  1.0569 -1.665   0.3407  0.1061]\n",
      "MSE loss: 88.8338\n",
      "Iteration: 169300\n",
      "Gradient: [  9.1857  27.8845  39.8108 150.0884  16.5168]\n",
      "Weights: [-4.8463  1.0532 -1.6699  0.3414  0.1062]\n",
      "MSE loss: 88.5118\n",
      "Iteration: 169400\n",
      "Gradient: [  7.3263  26.1245  33.2327 -77.2922 -30.7159]\n",
      "Weights: [-4.8375  1.0456 -1.6664  0.341   0.1061]\n",
      "MSE loss: 88.5491\n",
      "Iteration: 169500\n",
      "Gradient: [   9.4381    4.9501    4.5539 -101.4394  -91.8024]\n",
      "Weights: [-4.8411  1.0548 -1.6733  0.3423  0.1064]\n",
      "MSE loss: 88.5739\n",
      "Iteration: 169600\n",
      "Gradient: [ -22.6606  -12.1247    7.9441  -93.2138 -254.5428]\n",
      "Weights: [-4.8505  1.0526 -1.6808  0.3429  0.1068]\n",
      "MSE loss: 89.1896\n",
      "Iteration: 169700\n",
      "Gradient: [  17.1437   -3.1439   20.2746 -156.6222  525.0875]\n",
      "Weights: [-4.8308  1.0537 -1.6792  0.3437  0.1066]\n",
      "MSE loss: 88.707\n",
      "Iteration: 169800\n",
      "Gradient: [-17.2033  -1.7715 -12.0332  30.3071 543.5539]\n",
      "Weights: [-4.8577  1.0615 -1.6796  0.3445  0.1062]\n",
      "MSE loss: 88.591\n",
      "Iteration: 169900\n",
      "Gradient: [   1.7537  -12.7396  -80.0071 -284.6183 -260.0243]\n",
      "Weights: [-4.8367  1.0561 -1.6805  0.3443  0.1062]\n",
      "MSE loss: 88.5081\n",
      "Iteration: 170000\n",
      "Gradient: [  -1.642   -20.1237  -80.9584   26.9274 -130.2298]\n",
      "Weights: [-4.8308  1.0499 -1.6807  0.3448  0.1065]\n",
      "MSE loss: 88.5313\n",
      "Iteration: 170100\n",
      "Gradient: [   2.1177   36.2864   58.5955 -153.9034  101.3958]\n",
      "Weights: [-4.8201  1.0498 -1.6762  0.3451  0.1062]\n",
      "MSE loss: 90.2537\n",
      "Iteration: 170200\n",
      "Gradient: [  7.7525  26.2236  79.1929  28.2714 192.8859]\n",
      "Weights: [-4.8483  1.0527 -1.678   0.3462  0.1057]\n",
      "MSE loss: 88.4707\n",
      "Iteration: 170300\n",
      "Gradient: [   0.3754  -45.9469   26.5085  -43.2854 -160.6394]\n",
      "Weights: [-4.8559  1.0628 -1.6801  0.347   0.1056]\n",
      "MSE loss: 88.6275\n",
      "Iteration: 170400\n",
      "Gradient: [ -4.8658  -4.7402 -40.9764   5.1071 -91.0461]\n",
      "Weights: [-4.8611  1.0547 -1.6823  0.3479  0.1054]\n",
      "MSE loss: 89.1628\n",
      "Iteration: 170500\n",
      "Gradient: [   3.7826   10.8938   24.55   -111.663    -7.0702]\n",
      "Weights: [-4.8476  1.0764 -1.6854  0.3478  0.1052]\n",
      "MSE loss: 88.8889\n",
      "Iteration: 170600\n",
      "Gradient: [ 11.165  -13.2079 -24.0204 131.24   -59.7715]\n",
      "Weights: [-4.8471  1.0677 -1.6878  0.3484  0.1054]\n",
      "MSE loss: 88.4299\n",
      "Iteration: 170700\n",
      "Gradient: [ 10.0568  26.2252  48.7346 122.571   -4.3484]\n",
      "Weights: [-4.8299  1.0533 -1.6889  0.3497  0.1057]\n",
      "MSE loss: 88.5298\n",
      "Iteration: 170800\n",
      "Gradient: [  14.1717   19.5228   28.8033  108.622  -203.557 ]\n",
      "Weights: [-4.8459  1.0647 -1.6916  0.3496  0.1057]\n",
      "MSE loss: 88.4149\n",
      "Iteration: 170900\n",
      "Gradient: [  -8.9935  -16.8243  -10.8123   34.8994 -339.6322]\n",
      "Weights: [-4.8449  1.0636 -1.691   0.3493  0.1056]\n",
      "MSE loss: 88.4273\n",
      "Iteration: 171000\n",
      "Gradient: [  5.7447  22.4766  59.1267 286.8371  81.5127]\n",
      "Weights: [-4.8309  1.0534 -1.6892  0.3501  0.1057]\n",
      "MSE loss: 88.5867\n",
      "Iteration: 171100\n",
      "Gradient: [  -9.9115   -8.1808  -23.8608 -171.7323 -277.2009]\n",
      "Weights: [-4.8456  1.0609 -1.6904  0.3489  0.1056]\n",
      "MSE loss: 88.6178\n",
      "Iteration: 171200\n",
      "Gradient: [  -0.795     3.4667  -22.7738  -32.1147 -400.4487]\n",
      "Weights: [-4.8337  1.0583 -1.6879  0.3488  0.1057]\n",
      "MSE loss: 88.526\n",
      "Iteration: 171300\n",
      "Gradient: [  4.8868  -0.701   18.7788 -18.489   79.2505]\n",
      "Weights: [-4.8291  1.0659 -1.6884  0.3479  0.1056]\n",
      "MSE loss: 88.8654\n",
      "Iteration: 171400\n",
      "Gradient: [  -8.458   -47.1285 -134.9128  -99.6956 -268.9062]\n",
      "Weights: [-4.8514  1.0587 -1.688   0.3492  0.1053]\n",
      "MSE loss: 88.8116\n",
      "Iteration: 171500\n",
      "Gradient: [   3.1857  -32.8142  -42.9562  -56.9333 -438.9284]\n",
      "Weights: [-4.8272  1.049  -1.6862  0.3502  0.105 ]\n",
      "MSE loss: 88.4974\n",
      "Iteration: 171600\n",
      "Gradient: [  1.9403  16.1368  -5.4277 115.5864 -29.1267]\n",
      "Weights: [-4.8278  1.0491 -1.6902  0.3521  0.1053]\n",
      "MSE loss: 88.669\n",
      "Iteration: 171700\n",
      "Gradient: [  5.0706  25.8923  61.2299 -37.0348 248.5857]\n",
      "Weights: [-4.839   1.0589 -1.6921  0.3524  0.1048]\n",
      "MSE loss: 88.3718\n",
      "Iteration: 171800\n",
      "Gradient: [   8.2346  -13.6052   28.4543    4.6502 -282.9175]\n",
      "Weights: [-4.8328  1.0613 -1.6926  0.3513  0.1052]\n",
      "MSE loss: 88.4962\n",
      "Iteration: 171900\n",
      "Gradient: [ -12.8393   22.816   -15.0581 -153.3489   93.422 ]\n",
      "Weights: [-4.8443  1.0619 -1.6906  0.3512  0.1052]\n",
      "MSE loss: 88.481\n",
      "Iteration: 172000\n",
      "Gradient: [  4.9839   7.8962 -17.8694  19.4814 244.3277]\n",
      "Weights: [-4.848   1.0624 -1.6923  0.3515  0.1051]\n",
      "MSE loss: 88.4339\n",
      "Iteration: 172100\n",
      "Gradient: [  -4.7561   -6.5773  -45.6702  -50.0434 -368.4881]\n",
      "Weights: [-4.8512  1.0711 -1.6904  0.3509  0.1049]\n",
      "MSE loss: 88.4588\n",
      "Iteration: 172200\n",
      "Gradient: [ -1.6296  10.0155  33.5855  20.0627 199.8324]\n",
      "Weights: [-4.835   1.0644 -1.6908  0.351   0.1048]\n",
      "MSE loss: 88.5214\n",
      "Iteration: 172300\n",
      "Gradient: [  11.6555   25.8983  -18.9816   39.7943 -171.6613]\n",
      "Weights: [-4.8354  1.0589 -1.6876  0.3502  0.1051]\n",
      "MSE loss: 88.5062\n",
      "Iteration: 172400\n",
      "Gradient: [ -7.3283  22.5951 -12.9983   2.5103  -6.2191]\n",
      "Weights: [-4.8267  1.0414 -1.6854  0.3513  0.1052]\n",
      "MSE loss: 88.5456\n",
      "Iteration: 172500\n",
      "Gradient: [   3.1341  -10.7098    4.7843 -265.95   -218.409 ]\n",
      "Weights: [-4.8225  1.0362 -1.6817  0.3508  0.1048]\n",
      "MSE loss: 88.477\n",
      "Iteration: 172600\n",
      "Gradient: [  11.9224   22.6087   44.7336 -106.0099 -308.6652]\n",
      "Weights: [-4.8302  1.0385 -1.6839  0.351   0.1049]\n",
      "MSE loss: 88.4732\n",
      "Iteration: 172700\n",
      "Gradient: [ -20.0154  -33.1619  -18.3412 -304.2632 -200.1125]\n",
      "Weights: [-4.8484  1.0346 -1.6842  0.3518  0.1048]\n",
      "MSE loss: 89.7567\n",
      "Iteration: 172800\n",
      "Gradient: [ 10.8782  37.7763 107.9326  98.8988 486.0204]\n",
      "Weights: [-4.8355  1.0539 -1.6826  0.3507  0.1048]\n",
      "MSE loss: 88.929\n",
      "Iteration: 172900\n",
      "Gradient: [ -2.5093   4.651   -5.1403  35.3543 321.3592]\n",
      "Weights: [-4.8495  1.0503 -1.684   0.3513  0.1047]\n",
      "MSE loss: 88.496\n",
      "Iteration: 173000\n",
      "Gradient: [ 15.8384   5.6904  10.8268 102.6661 124.3192]\n",
      "Weights: [-4.8089  1.0488 -1.6867  0.3528  0.1044]\n",
      "MSE loss: 89.8826\n",
      "Iteration: 173100\n",
      "Gradient: [-17.9859 -28.5755 -29.5612 -19.6194 -67.3766]\n",
      "Weights: [-4.8509  1.0399 -1.685   0.353   0.1043]\n",
      "MSE loss: 89.3445\n",
      "Iteration: 173200\n",
      "Gradient: [  1.7602 -11.3687 -34.6653  49.2573 117.4921]\n",
      "Weights: [-4.8308  1.0353 -1.6807  0.353   0.1041]\n",
      "MSE loss: 88.3785\n",
      "Iteration: 173300\n",
      "Gradient: [   2.4111    5.0633  -28.1122 -307.1227 -214.2117]\n",
      "Weights: [-4.8457  1.0471 -1.6821  0.3521  0.1038]\n",
      "MSE loss: 88.7404\n",
      "Iteration: 173400\n",
      "Gradient: [   7.0368   40.0504  -18.0499  -81.5187 -524.3322]\n",
      "Weights: [-4.8223  1.0453 -1.6858  0.3538  0.1041]\n",
      "MSE loss: 88.6945\n",
      "Iteration: 173500\n",
      "Gradient: [-25.3905 -13.2621 -30.4235  78.6588 -57.4582]\n",
      "Weights: [-4.8723  1.0512 -1.6862  0.3543  0.104 ]\n",
      "MSE loss: 89.8373\n",
      "Iteration: 173600\n",
      "Gradient: [  3.4603  -2.6108 -20.7253  19.0534  69.6452]\n",
      "Weights: [-4.8448  1.0511 -1.6838  0.3536  0.1037]\n",
      "MSE loss: 88.3735\n",
      "Iteration: 173700\n",
      "Gradient: [  -2.8894   11.7684   -4.8411 -110.7331 -571.7534]\n",
      "Weights: [-4.8439  1.0489 -1.6866  0.354   0.1038]\n",
      "MSE loss: 88.5614\n",
      "Iteration: 173800\n",
      "Gradient: [  -8.8188  -46.8369   -8.4782  -98.2501 -365.7732]\n",
      "Weights: [-4.8461  1.0494 -1.6883  0.3537  0.104 ]\n",
      "MSE loss: 88.867\n",
      "Iteration: 173900\n",
      "Gradient: [ 11.9864   4.3183 -15.649  -93.9142 248.0026]\n",
      "Weights: [-4.8329  1.0616 -1.6878  0.3546  0.1037]\n",
      "MSE loss: 89.3588\n",
      "Iteration: 174000\n",
      "Gradient: [  4.9948  -1.53    11.6209 174.2872 -35.6753]\n",
      "Weights: [-4.8505  1.0608 -1.6899  0.3551  0.1036]\n",
      "MSE loss: 88.371\n",
      "Iteration: 174100\n",
      "Gradient: [-21.7104  44.8521  25.2796 220.4571 328.0215]\n",
      "Weights: [-4.8664  1.0655 -1.6886  0.3556  0.1035]\n",
      "MSE loss: 88.6692\n",
      "Iteration: 174200\n",
      "Gradient: [ -6.347   -1.5366  52.5535 -67.4777 244.7748]\n",
      "Weights: [-4.852   1.0623 -1.6877  0.3557  0.1031]\n",
      "MSE loss: 88.4221\n",
      "Iteration: 174300\n",
      "Gradient: [  -9.5283  -40.849   -12.9594 -182.6964 -324.9558]\n",
      "Weights: [-4.8507  1.0573 -1.6855  0.3556  0.1031]\n",
      "MSE loss: 88.4011\n",
      "Iteration: 174400\n",
      "Gradient: [  -5.0732  -35.1681  -18.1911  -48.4976 -405.8255]\n",
      "Weights: [-4.8324  1.046  -1.6875  0.3562  0.1034]\n",
      "MSE loss: 88.3611\n",
      "Iteration: 174500\n",
      "Gradient: [  4.9266  -2.8003  42.6972   6.63   170.6891]\n",
      "Weights: [-4.832   1.0475 -1.6871  0.3565  0.1032]\n",
      "MSE loss: 88.4134\n",
      "Iteration: 174600\n",
      "Gradient: [-6.9214 15.508  31.8923 74.4878 89.2101]\n",
      "Weights: [-4.8269  1.0316 -1.6848  0.3573  0.1034]\n",
      "MSE loss: 88.4079\n",
      "Iteration: 174700\n",
      "Gradient: [-15.4545   9.9767 -71.5443  51.1874 -94.8411]\n",
      "Weights: [-4.8251  1.0217 -1.6859  0.3571  0.1037]\n",
      "MSE loss: 88.6022\n",
      "Iteration: 174800\n",
      "Gradient: [ -5.291  -18.9062 -36.3056 -41.1612 299.1462]\n",
      "Weights: [-4.819   1.0164 -1.6842  0.3559  0.1036]\n",
      "MSE loss: 89.4435\n",
      "Iteration: 174900\n",
      "Gradient: [-12.9467   6.7992   5.8446 -82.3897  15.3368]\n",
      "Weights: [-4.8434  1.0434 -1.6835  0.3554  0.1033]\n",
      "MSE loss: 88.3887\n",
      "Iteration: 175000\n",
      "Gradient: [ -2.4699   4.9005  68.8708  97.986  125.2906]\n",
      "Weights: [-4.8371  1.0384 -1.6866  0.3558  0.1035]\n",
      "MSE loss: 88.6197\n",
      "Iteration: 175100\n",
      "Gradient: [  16.3306   -8.8674  -91.9336 -219.3827   62.2426]\n",
      "Weights: [-4.8315  1.0467 -1.6873  0.3559  0.1034]\n",
      "MSE loss: 88.3932\n",
      "Iteration: 175200\n",
      "Gradient: [ 20.4094 -28.8867   7.1195 -84.1223  47.9758]\n",
      "Weights: [-4.8264  1.0535 -1.6867  0.356   0.1032]\n",
      "MSE loss: 89.1814\n",
      "Iteration: 175300\n",
      "Gradient: [  -6.0475  -38.8098  -39.6971  -26.771  -644.7735]\n",
      "Weights: [-4.815   1.0402 -1.6884  0.3565  0.1034]\n",
      "MSE loss: 88.6839\n",
      "Iteration: 175400\n",
      "Gradient: [   1.061   -50.6932  -91.2069  -65.6125 -689.8298]\n",
      "Weights: [-4.8455  1.0551 -1.6877  0.3562  0.1028]\n",
      "MSE loss: 88.4998\n",
      "Iteration: 175500\n",
      "Gradient: [ -5.2214  13.3836  59.2131  -1.1551 575.0046]\n",
      "Weights: [-4.8445  1.0543 -1.6891  0.3562  0.103 ]\n",
      "MSE loss: 88.5054\n",
      "Iteration: 175600\n",
      "Gradient: [-10.8906 -41.0199 -74.4956   2.2375 161.272 ]\n",
      "Weights: [-4.8508  1.0486 -1.6903  0.3571  0.103 ]\n",
      "MSE loss: 89.1632\n",
      "Iteration: 175700\n",
      "Gradient: [ -23.0699   -3.8312   83.6871  -19.7121 -294.5513]\n",
      "Weights: [-4.8554  1.0378 -1.6882  0.3586  0.1029]\n",
      "MSE loss: 89.5452\n",
      "Iteration: 175800\n",
      "Gradient: [  4.404  -28.9323 -52.7801  21.5292 -25.3364]\n",
      "Weights: [-4.8258  1.0282 -1.6848  0.3582  0.1028]\n",
      "MSE loss: 88.394\n",
      "Iteration: 175900\n",
      "Gradient: [ -1.8963   8.4266 -25.101  -47.2322  44.1153]\n",
      "Weights: [-4.8234  1.0315 -1.6841  0.358   0.1027]\n",
      "MSE loss: 88.3809\n",
      "Iteration: 176000\n",
      "Gradient: [ 11.225   47.3448  88.1756 123.9056 512.3686]\n",
      "Weights: [-4.8228  1.0352 -1.6871  0.3591  0.1026]\n",
      "MSE loss: 88.3992\n",
      "Iteration: 176100\n",
      "Gradient: [  -0.7536  -13.0657  -12.2129 -103.393   251.1646]\n",
      "Weights: [-4.8413  1.0455 -1.6896  0.3597  0.1025]\n",
      "MSE loss: 88.3103\n",
      "Iteration: 176200\n",
      "Gradient: [  1.8223  -7.458  -21.3834 -46.4244 126.5546]\n",
      "Weights: [-4.8257  1.0444 -1.6914  0.3597  0.1026]\n",
      "MSE loss: 88.4079\n",
      "Iteration: 176300\n",
      "Gradient: [  -7.706    -6.4685   34.5004  -89.3921 -232.6969]\n",
      "Weights: [-4.8254  1.0447 -1.695   0.3606  0.1025]\n",
      "MSE loss: 88.3701\n",
      "Iteration: 176400\n",
      "Gradient: [  -2.8103    2.3168  -22.3064  -39.4563 -110.3589]\n",
      "Weights: [-4.8496  1.0578 -1.7025  0.361   0.103 ]\n",
      "MSE loss: 88.706\n",
      "Iteration: 176500\n",
      "Gradient: [  1.053   -6.9171  65.8225  81.3052 592.1109]\n",
      "Weights: [-4.8433  1.0672 -1.7013  0.3614  0.1029]\n",
      "MSE loss: 88.6641\n",
      "Iteration: 176600\n",
      "Gradient: [  -4.4529  -22.6912  -38.8992 -132.3354 -446.3516]\n",
      "Weights: [-4.8411  1.0593 -1.7013  0.361   0.1029]\n",
      "MSE loss: 88.2711\n",
      "Iteration: 176700\n",
      "Gradient: [ -16.0845  -10.4614   42.0033  -53.9294 -400.7148]\n",
      "Weights: [-4.8482  1.0582 -1.7069  0.3622  0.1032]\n",
      "MSE loss: 88.6634\n",
      "Iteration: 176800\n",
      "Gradient: [-3.038000e-01  8.711000e-01  9.752800e+00 -1.381142e+02  7.489911e+02]\n",
      "Weights: [-4.8412  1.0561 -1.7068  0.3623  0.1032]\n",
      "MSE loss: 88.4284\n",
      "Iteration: 176900\n",
      "Gradient: [   1.7707  -10.4958   -6.2133 -273.5106 -164.0022]\n",
      "Weights: [-4.8417  1.0577 -1.7048  0.3625  0.103 ]\n",
      "MSE loss: 88.3039\n",
      "Iteration: 177000\n",
      "Gradient: [ -10.6801   14.7628  -60.076    -8.3447 -123.6887]\n",
      "Weights: [-4.8398  1.0657 -1.7091  0.3629  0.1028]\n",
      "MSE loss: 88.2719\n",
      "Iteration: 177100\n",
      "Gradient: [ -4.2046  -9.7045 -33.9313 107.3672 -69.7221]\n",
      "Weights: [-4.8437  1.0802 -1.7122  0.3634  0.1029]\n",
      "MSE loss: 88.647\n",
      "Iteration: 177200\n",
      "Gradient: [ -9.5456   0.5529  63.7475 140.961  149.0846]\n",
      "Weights: [-4.8605  1.0771 -1.7114  0.3628  0.1031]\n",
      "MSE loss: 88.4579\n",
      "Iteration: 177300\n",
      "Gradient: [ 23.1265  15.4158  67.4844 217.3761   1.9036]\n",
      "Weights: [-4.828   1.0731 -1.7135  0.3627  0.1034]\n",
      "MSE loss: 88.7193\n",
      "Iteration: 177400\n",
      "Gradient: [   2.0028    0.8955   81.414    35.3424 -577.3834]\n",
      "Weights: [-4.8221  1.06   -1.712   0.3635  0.1033]\n",
      "MSE loss: 88.5101\n",
      "Iteration: 177500\n",
      "Gradient: [  -6.636   -22.2473  -39.4737 -116.3515 -319.8475]\n",
      "Weights: [-4.842   1.0641 -1.7131  0.3637  0.1031]\n",
      "MSE loss: 88.4291\n",
      "Iteration: 177600\n",
      "Gradient: [   6.0409   19.9627   -0.286  -119.1136 -128.9635]\n",
      "Weights: [-4.8039  1.0568 -1.7142  0.3634  0.1032]\n",
      "MSE loss: 89.0182\n",
      "Iteration: 177700\n",
      "Gradient: [  19.3866   16.8148   -8.1487  -84.0803 -492.2645]\n",
      "Weights: [-4.8201  1.0657 -1.7135  0.3634  0.1031]\n",
      "MSE loss: 88.603\n",
      "Iteration: 177800\n",
      "Gradient: [  2.238   -1.777  -27.3984  97.1534  88.6603]\n",
      "Weights: [-4.8432  1.0727 -1.7113  0.363   0.1033]\n",
      "MSE loss: 88.4669\n",
      "Iteration: 177900\n",
      "Gradient: [  -1.3246   11.2961   46.5486 -135.0112  -24.8148]\n",
      "Weights: [-4.8323  1.0602 -1.7104  0.3641  0.1029]\n",
      "MSE loss: 88.2841\n",
      "Iteration: 178000\n",
      "Gradient: [-11.0931  16.2425   9.5328  55.9921 393.4716]\n",
      "Weights: [-4.8608  1.0844 -1.7103  0.3634  0.1024]\n",
      "MSE loss: 88.3318\n",
      "Iteration: 178100\n",
      "Gradient: [   4.5107  -20.3828  -35.0464 -228.2825   34.8886]\n",
      "Weights: [-4.8534  1.0788 -1.7081  0.3642  0.1023]\n",
      "MSE loss: 88.5607\n",
      "Iteration: 178200\n",
      "Gradient: [ 4.857100e+00 -4.005000e+00 -2.150000e-02 -1.204851e+02 -8.144070e+01]\n",
      "Weights: [-4.8444  1.0669 -1.7102  0.3638  0.1026]\n",
      "MSE loss: 88.3364\n",
      "Iteration: 178300\n",
      "Gradient: [  -4.9447   -1.6147    0.3201   71.0329 -168.2564]\n",
      "Weights: [-4.8518  1.08   -1.7101  0.3638  0.1023]\n",
      "MSE loss: 88.2796\n",
      "Iteration: 178400\n",
      "Gradient: [ -6.3227  15.2514 -44.4149 176.634  567.5948]\n",
      "Weights: [-4.8537  1.0768 -1.71    0.3641  0.1024]\n",
      "MSE loss: 88.2782\n",
      "Iteration: 178500\n",
      "Gradient: [  7.4904  15.9417  39.3604 -88.4525 537.3341]\n",
      "Weights: [-4.8388  1.064  -1.7061  0.3642  0.1021]\n",
      "MSE loss: 88.2568\n",
      "Iteration: 178600\n",
      "Gradient: [ 20.5972 -33.7587 -30.5846 -18.2696 -10.8033]\n",
      "Weights: [-4.8296  1.0539 -1.7042  0.3649  0.1022]\n",
      "MSE loss: 88.4042\n",
      "Iteration: 178700\n",
      "Gradient: [  9.9165   0.1496 -53.5152 -23.7052 123.0284]\n",
      "Weights: [-4.8345  1.0571 -1.7014  0.3633  0.1023]\n",
      "MSE loss: 88.3561\n",
      "Iteration: 178800\n",
      "Gradient: [ -9.1521  -5.7472  58.2559 -43.0468 332.4706]\n",
      "Weights: [-4.841   1.0548 -1.6995  0.3617  0.1023]\n",
      "MSE loss: 88.4701\n",
      "Iteration: 178900\n",
      "Gradient: [ -14.3567   19.3462  -33.2005 -104.1757  456.5701]\n",
      "Weights: [-4.8555  1.0666 -1.7018  0.3613  0.1027]\n",
      "MSE loss: 88.3861\n",
      "Iteration: 179000\n",
      "Gradient: [   3.1824   11.4177  -35.2005   35.5923 -266.9311]\n",
      "Weights: [-4.8384  1.0578 -1.7004  0.3618  0.1025]\n",
      "MSE loss: 88.2699\n",
      "Iteration: 179100\n",
      "Gradient: [  -3.4681   11.2028 -123.4921 -127.6436 -497.1529]\n",
      "Weights: [-4.8503  1.0467 -1.6974  0.3625  0.1025]\n",
      "MSE loss: 88.6581\n",
      "Iteration: 179200\n",
      "Gradient: [  2.6984  23.3276  29.6736 -33.0018 -79.4083]\n",
      "Weights: [-4.8334  1.0448 -1.7007  0.3629  0.1025]\n",
      "MSE loss: 88.3908\n",
      "Iteration: 179300\n",
      "Gradient: [  1.6962   5.2167 -47.7693 -32.9884  77.0767]\n",
      "Weights: [-4.8235  1.0461 -1.6994  0.362   0.1029]\n",
      "MSE loss: 88.4888\n",
      "Iteration: 179400\n",
      "Gradient: [-11.2612   0.6633 -12.2669 -84.6024 112.0174]\n",
      "Weights: [-4.8366  1.0387 -1.6973  0.3632  0.1024]\n",
      "MSE loss: 88.428\n",
      "Iteration: 179500\n",
      "Gradient: [ 24.3951  17.8382  66.0165  46.3932 410.0261]\n",
      "Weights: [-4.8227  1.0402 -1.6936  0.3627  0.1022]\n",
      "MSE loss: 88.5408\n",
      "Iteration: 179600\n",
      "Gradient: [  13.7111  -34.3096  -37.1758 -244.2995 -355.4564]\n",
      "Weights: [-4.8326  1.0438 -1.6942  0.363   0.1018]\n",
      "MSE loss: 88.2791\n",
      "Iteration: 179700\n",
      "Gradient: [   6.653    13.56    -67.2514  -56.7282 -409.3096]\n",
      "Weights: [-4.8216  1.0372 -1.6914  0.3632  0.1017]\n",
      "MSE loss: 88.4915\n",
      "Iteration: 179800\n",
      "Gradient: [  -9.1365   -3.7043   -5.0351  179.676  -219.8444]\n",
      "Weights: [-4.8246  1.0243 -1.6918  0.3643  0.1019]\n",
      "MSE loss: 88.344\n",
      "Iteration: 179900\n",
      "Gradient: [ -16.3563  -43.5693   -4.071  -156.3805 -877.5223]\n",
      "Weights: [-4.8226  1.0241 -1.6905  0.3629  0.1021]\n",
      "MSE loss: 88.3713\n",
      "Iteration: 180000\n",
      "Gradient: [ -16.4527  -39.0081  -28.6553 -118.6702 -364.9556]\n",
      "Weights: [-4.8401  1.0412 -1.6922  0.3623  0.1019]\n",
      "MSE loss: 88.396\n",
      "Iteration: 180100\n",
      "Gradient: [  5.2495  -7.7158  29.2988  14.0706 271.1797]\n",
      "Weights: [-4.824   1.0485 -1.6962  0.3633  0.1016]\n",
      "MSE loss: 88.4945\n",
      "Iteration: 180200\n",
      "Gradient: [  4.3943 -18.9188 -33.9966 -70.528  108.7839]\n",
      "Weights: [-4.8328  1.043  -1.6937  0.3634  0.1016]\n",
      "MSE loss: 88.2718\n",
      "Iteration: 180300\n",
      "Gradient: [ 14.1567  13.052   28.5956  23.6808 -73.9373]\n",
      "Weights: [-4.8336  1.0485 -1.6917  0.3631  0.1016]\n",
      "MSE loss: 88.5909\n",
      "Iteration: 180400\n",
      "Gradient: [-20.0619 -27.8219  28.5768 -76.5893 661.5496]\n",
      "Weights: [-4.8418  1.0381 -1.6908  0.3644  0.1014]\n",
      "MSE loss: 88.3255\n",
      "Iteration: 180500\n",
      "Gradient: [ -5.1436  15.2163  -3.5153  36.042  -49.7959]\n",
      "Weights: [-4.8343  1.0427 -1.6983  0.365   0.1016]\n",
      "MSE loss: 88.3316\n",
      "Iteration: 180600\n",
      "Gradient: [ -0.7906   4.2729   6.9843  -8.9428 173.7378]\n",
      "Weights: [-4.83    1.0408 -1.701   0.3657  0.1014]\n",
      "MSE loss: 88.7129\n",
      "Iteration: 180700\n",
      "Gradient: [  8.8783  21.7212 -32.0383  56.1668 664.0403]\n",
      "Weights: [-4.838   1.0391 -1.6976  0.3672  0.1012]\n",
      "MSE loss: 88.2872\n",
      "Iteration: 180800\n",
      "Gradient: [   1.3937  -16.2345  -64.7662 -197.9594   -1.6727]\n",
      "Weights: [-4.8317  1.0443 -1.6957  0.3656  0.1012]\n",
      "MSE loss: 88.3159\n",
      "Iteration: 180900\n",
      "Gradient: [  8.5331  -1.0259   4.3432  27.5311 119.7556]\n",
      "Weights: [-4.8254  1.038  -1.6934  0.3655  0.101 ]\n",
      "MSE loss: 88.336\n",
      "Iteration: 181000\n",
      "Gradient: [  7.2882   2.8625  38.3267  83.0266 143.0801]\n",
      "Weights: [-4.8479  1.0464 -1.6904  0.3648  0.1007]\n",
      "MSE loss: 88.3455\n",
      "Iteration: 181100\n",
      "Gradient: [-12.6147 -20.0901   4.5684 -89.5441  -9.5993]\n",
      "Weights: [-4.8501  1.0436 -1.692   0.3642  0.101 ]\n",
      "MSE loss: 88.8172\n",
      "Iteration: 181200\n",
      "Gradient: [-10.1225  44.6037   3.8147 189.2897 719.0467]\n",
      "Weights: [-4.8522  1.0498 -1.6932  0.3654  0.101 ]\n",
      "MSE loss: 88.3909\n",
      "Iteration: 181300\n",
      "Gradient: [-12.9839 -30.7903 -28.6363   5.2242 320.8012]\n",
      "Weights: [-4.8589  1.0505 -1.6968  0.3654  0.1011]\n",
      "MSE loss: 88.9632\n",
      "Iteration: 181400\n",
      "Gradient: [  12.0206  -14.1923  -10.0799 -118.793  -103.0117]\n",
      "Weights: [-4.8367  1.0668 -1.704   0.3654  0.1013]\n",
      "MSE loss: 88.4913\n",
      "Iteration: 181500\n",
      "Gradient: [-1.295100e+00 -1.110780e+01  2.000000e-04  1.776005e+02  3.017831e+02]\n",
      "Weights: [-4.8421  1.0693 -1.7057  0.3656  0.1016]\n",
      "MSE loss: 88.5018\n",
      "Iteration: 181600\n",
      "Gradient: [  1.039   19.4367  39.6149 -19.8201 541.4285]\n",
      "Weights: [-4.843   1.0636 -1.7047  0.3659  0.1017]\n",
      "MSE loss: 88.4478\n",
      "Iteration: 181700\n",
      "Gradient: [  -4.5823   16.5003  -19.0014 -141.867  -490.8535]\n",
      "Weights: [-4.8351  1.0553 -1.7045  0.3662  0.1016]\n",
      "MSE loss: 88.2325\n",
      "Iteration: 181800\n",
      "Gradient: [  11.8717   -1.804   -42.4513 -113.4046   16.4424]\n",
      "Weights: [-4.8214  1.0506 -1.7071  0.3677  0.1017]\n",
      "MSE loss: 88.6008\n",
      "Iteration: 181900\n",
      "Gradient: [ -1.8433  -9.7234 -73.5408  75.4777 158.8314]\n",
      "Weights: [-4.8365  1.0703 -1.7098  0.3675  0.1014]\n",
      "MSE loss: 88.6365\n",
      "Iteration: 182000\n",
      "Gradient: [   4.9927  -17.2648   -7.0573 -142.9639 -404.2441]\n",
      "Weights: [-4.8591  1.0694 -1.7117  0.3672  0.1015]\n",
      "MSE loss: 88.7673\n",
      "Iteration: 182100\n",
      "Gradient: [ -10.8443  -59.0588  -71.1286 -121.8394  246.8867]\n",
      "Weights: [-4.865   1.0706 -1.7166  0.3679  0.1019]\n",
      "MSE loss: 89.354\n",
      "Iteration: 182200\n",
      "Gradient: [  23.2544    3.5202  107.9117 -122.7525   28.3571]\n",
      "Weights: [-4.8429  1.0765 -1.7133  0.3662  0.1021]\n",
      "MSE loss: 88.4322\n",
      "Iteration: 182300\n",
      "Gradient: [ -40.9744  -44.5708  -51.4897 -212.1048 -721.2622]\n",
      "Weights: [-4.8703  1.0621 -1.7133  0.3662  0.1023]\n",
      "MSE loss: 90.9616\n",
      "Iteration: 182400\n",
      "Gradient: [ 18.3168  27.3063 139.8338 150.0388 520.5954]\n",
      "Weights: [-4.8235  1.0743 -1.7121  0.3655  0.1025]\n",
      "MSE loss: 90.2832\n",
      "Iteration: 182500\n",
      "Gradient: [ -3.459  -14.6006 -26.2935  25.4956 -90.4372]\n",
      "Weights: [-4.8335  1.0742 -1.7163  0.3665  0.102 ]\n",
      "MSE loss: 88.3372\n",
      "Iteration: 182600\n",
      "Gradient: [ -5.6739  31.2262  23.3335  29.0129 406.3926]\n",
      "Weights: [-4.8504  1.0772 -1.7126  0.366   0.1021]\n",
      "MSE loss: 88.303\n",
      "Iteration: 182700\n",
      "Gradient: [ -12.4856  -10.5709  -11.5996   45.576  -153.8435]\n",
      "Weights: [-4.8598  1.0838 -1.7128  0.3649  0.102 ]\n",
      "MSE loss: 88.3594\n",
      "Iteration: 182800\n",
      "Gradient: [   4.5477    0.9595  -36.2747 -161.6524 -322.2576]\n",
      "Weights: [-4.8591  1.0889 -1.7164  0.3655  0.1019]\n",
      "MSE loss: 88.3573\n",
      "Iteration: 182900\n",
      "Gradient: [-11.2689 -16.1907 -12.8938 -39.6636 -75.7675]\n",
      "Weights: [-4.8613  1.0858 -1.718   0.3664  0.1018]\n",
      "MSE loss: 88.6947\n",
      "Iteration: 183000\n",
      "Gradient: [  2.6318  25.6277 -23.6903 116.6613 -26.2138]\n",
      "Weights: [-4.8496  1.0926 -1.7174  0.3664  0.1017]\n",
      "MSE loss: 88.4488\n",
      "Iteration: 183100\n",
      "Gradient: [ -10.3753    3.5501   16.234    40.503  -280.5109]\n",
      "Weights: [-4.8668  1.0919 -1.7176  0.3655  0.102 ]\n",
      "MSE loss: 88.5807\n",
      "Iteration: 183200\n",
      "Gradient: [   9.2575  -12.6656    4.487  -178.9261  269.272 ]\n",
      "Weights: [-4.8383  1.0877 -1.7207  0.3659  0.1024]\n",
      "MSE loss: 88.4531\n",
      "Iteration: 183300\n",
      "Gradient: [ 15.2479  20.5444   3.7159  92.288  133.0467]\n",
      "Weights: [-4.8526  1.0949 -1.724   0.3659  0.1024]\n",
      "MSE loss: 88.3166\n",
      "Iteration: 183400\n",
      "Gradient: [   3.1414   -6.2422  -40.6314 -170.6314 -499.8995]\n",
      "Weights: [-4.8459  1.0936 -1.725   0.3661  0.1027]\n",
      "MSE loss: 88.295\n",
      "Iteration: 183500\n",
      "Gradient: [ -13.182    15.7428   -2.9174  -71.9378 -167.9915]\n",
      "Weights: [-4.8608  1.0849 -1.7254  0.3661  0.1026]\n",
      "MSE loss: 89.7077\n",
      "Iteration: 183600\n",
      "Gradient: [ -8.0289 -35.115   14.7261 -89.8144 -48.2233]\n",
      "Weights: [-4.8434  1.0656 -1.7193  0.3656  0.103 ]\n",
      "MSE loss: 88.8784\n",
      "Iteration: 183700\n",
      "Gradient: [   0.9714   -3.441   -96.4975    7.3672 -306.0717]\n",
      "Weights: [-4.8287  1.0695 -1.7203  0.3659  0.103 ]\n",
      "MSE loss: 88.324\n",
      "Iteration: 183800\n",
      "Gradient: [  -8.0242   14.0852  -18.4754  -23.6126 -220.2222]\n",
      "Weights: [-4.8301  1.075  -1.7251  0.3663  0.1031]\n",
      "MSE loss: 88.3714\n",
      "Iteration: 183900\n",
      "Gradient: [ -8.7739  19.6911 -49.03   -49.2116 461.2557]\n",
      "Weights: [-4.8528  1.0862 -1.7219  0.3646  0.1031]\n",
      "MSE loss: 88.3198\n",
      "Iteration: 184000\n",
      "Gradient: [ -15.8662  -11.9237   -9.0972 -210.9239 -784.8134]\n",
      "Weights: [-4.8617  1.089  -1.7227  0.3644  0.1031]\n",
      "MSE loss: 88.6522\n",
      "Iteration: 184100\n",
      "Gradient: [   3.2944  -15.2062  -86.1766 -161.692  -293.7948]\n",
      "Weights: [-4.8444  1.085  -1.7237  0.3644  0.1034]\n",
      "MSE loss: 88.2874\n",
      "Iteration: 184200\n",
      "Gradient: [ 2.61820e+00  4.55300e-01  4.64390e+00 -4.47960e+01 -4.58584e+02]\n",
      "Weights: [-4.8481  1.0888 -1.7228  0.3646  0.1033]\n",
      "MSE loss: 88.2965\n",
      "Iteration: 184300\n",
      "Gradient: [ -16.8814    7.789   -44.8759   45.2315 -616.1808]\n",
      "Weights: [-4.8554  1.0917 -1.7231  0.3641  0.1031]\n",
      "MSE loss: 88.4691\n",
      "Iteration: 184400\n",
      "Gradient: [-11.8152  -4.2062  19.0077 -44.851  -29.0999]\n",
      "Weights: [-4.8488  1.0823 -1.7218  0.3648  0.1032]\n",
      "MSE loss: 88.3278\n",
      "Iteration: 184500\n",
      "Gradient: [  -1.6216  -32.4999  -62.2469 -219.0183 -468.2832]\n",
      "Weights: [-4.8372  1.0805 -1.7253  0.3642  0.1032]\n",
      "MSE loss: 89.0207\n",
      "Iteration: 184600\n",
      "Gradient: [ 9.1341 -6.2841 43.4651 26.1543 61.0645]\n",
      "Weights: [-4.8635  1.0889 -1.7236  0.3655  0.1032]\n",
      "MSE loss: 88.504\n",
      "Iteration: 184700\n",
      "Gradient: [  -4.0868  -39.8985  -40.075   -27.7616 -472.383 ]\n",
      "Weights: [-4.848   1.0861 -1.7229  0.365   0.103 ]\n",
      "MSE loss: 88.2657\n",
      "Iteration: 184800\n",
      "Gradient: [ -1.5529  -7.06    38.2213 135.1686 224.0716]\n",
      "Weights: [-4.8445  1.08   -1.7203  0.3653  0.103 ]\n",
      "MSE loss: 88.237\n",
      "Iteration: 184900\n",
      "Gradient: [   3.8423   29.9762   31.8382  194.172  -292.833 ]\n",
      "Weights: [-4.8704  1.0933 -1.7215  0.3644  0.1031]\n",
      "MSE loss: 88.6311\n",
      "Iteration: 185000\n",
      "Gradient: [  11.0717   -8.2537  -56.1575  -41.897  -266.9118]\n",
      "Weights: [-4.83    1.079  -1.7196  0.3642  0.103 ]\n",
      "MSE loss: 88.4305\n",
      "Iteration: 185100\n",
      "Gradient: [ 2.127000e-01  3.198270e+01  5.693170e+01 -3.561000e+00  2.776846e+02]\n",
      "Weights: [-4.8395  1.0872 -1.7179  0.3645  0.1026]\n",
      "MSE loss: 88.4601\n",
      "Iteration: 185200\n",
      "Gradient: [  -6.9332  -20.1339   58.3932 -150.9511  232.3725]\n",
      "Weights: [-4.8441  1.0824 -1.7196  0.3661  0.1024]\n",
      "MSE loss: 88.2302\n",
      "Iteration: 185300\n",
      "Gradient: [ -18.489    -9.7577   43.484   -11.6449 -335.4171]\n",
      "Weights: [-4.8351  1.0811 -1.7232  0.3678  0.102 ]\n",
      "MSE loss: 88.3284\n",
      "Iteration: 185400\n",
      "Gradient: [ -16.4633  -10.0461   -4.6765 -226.7076  -10.6826]\n",
      "Weights: [-4.836   1.0777 -1.7271  0.3685  0.1026]\n",
      "MSE loss: 88.2428\n",
      "Iteration: 185500\n",
      "Gradient: [   3.9912  -35.5766  -40.0984 -281.8742 -201.7007]\n",
      "Weights: [-4.8235  1.0625 -1.7231  0.3688  0.1024]\n",
      "MSE loss: 88.3785\n",
      "Iteration: 185600\n",
      "Gradient: [  6.9762  39.1869  40.5773 151.8847  84.3389]\n",
      "Weights: [-4.8424  1.0601 -1.7189  0.3696  0.102 ]\n",
      "MSE loss: 88.4525\n",
      "Iteration: 185700\n",
      "Gradient: [ -21.1068   -6.2343   -9.4772  -86.3544 -250.6764]\n",
      "Weights: [-4.8682  1.0804 -1.7239  0.3702  0.1014]\n",
      "MSE loss: 89.714\n",
      "Iteration: 185800\n",
      "Gradient: [   4.2965   -2.7073  -61.9989 -151.6229    5.6327]\n",
      "Weights: [-4.8425  1.0732 -1.7229  0.3701  0.1016]\n",
      "MSE loss: 88.3089\n",
      "Iteration: 185900\n",
      "Gradient: [-11.1566 -22.0721 -18.1623 -67.8597 156.078 ]\n",
      "Weights: [-4.8312  1.0658 -1.7211  0.3694  0.102 ]\n",
      "MSE loss: 88.2318\n",
      "Iteration: 186000\n",
      "Gradient: [   3.3242    4.7467   45.5385 -127.3076  134.2378]\n",
      "Weights: [-4.8472  1.0773 -1.7227  0.3694  0.102 ]\n",
      "MSE loss: 88.2008\n",
      "Iteration: 186100\n",
      "Gradient: [  1.6064   7.4662 -31.9943 -18.0982 -50.7139]\n",
      "Weights: [-4.8301  1.0712 -1.7253  0.3701  0.1021]\n",
      "MSE loss: 88.2491\n",
      "Iteration: 186200\n",
      "Gradient: [ 20.0081 -18.9214 -17.6578  50.8113 112.0017]\n",
      "Weights: [-4.8276  1.0816 -1.7261  0.3711  0.1015]\n",
      "MSE loss: 88.7135\n",
      "Iteration: 186300\n",
      "Gradient: [  0.8561 -10.1759 -13.3221  -0.5296  70.7785]\n",
      "Weights: [-4.8342  1.0833 -1.7318  0.3721  0.1017]\n",
      "MSE loss: 88.245\n",
      "Iteration: 186400\n",
      "Gradient: [   1.5921    5.6849   41.5676 -229.5174 -342.8016]\n",
      "Weights: [-4.8482  1.0886 -1.7298  0.3715  0.1016]\n",
      "MSE loss: 88.1522\n",
      "Iteration: 186500\n",
      "Gradient: [ -3.1177  13.7495 -21.5294  94.4219  86.1415]\n",
      "Weights: [-4.8685  1.0884 -1.7305  0.3728  0.1015]\n",
      "MSE loss: 88.6792\n",
      "Iteration: 186600\n",
      "Gradient: [  4.5019 -21.9159  18.8165 -19.3187 126.3887]\n",
      "Weights: [-4.8348  1.0859 -1.7306  0.3726  0.1012]\n",
      "MSE loss: 88.3014\n",
      "Iteration: 186700\n",
      "Gradient: [  4.8253 -13.9607   1.4153  29.6555 196.7245]\n",
      "Weights: [-4.8593  1.0911 -1.7306  0.3728  0.1013]\n",
      "MSE loss: 88.2344\n",
      "Iteration: 186800\n",
      "Gradient: [ -23.8694   20.4173   24.8576  124.1547 -267.3812]\n",
      "Weights: [-4.8557  1.083  -1.7267  0.373   0.101 ]\n",
      "MSE loss: 88.2314\n",
      "Iteration: 186900\n",
      "Gradient: [ -10.8713  -20.4143  -39.1408 -150.6251 -707.4385]\n",
      "Weights: [-4.8265  1.0776 -1.7232  0.3726  0.1007]\n",
      "MSE loss: 88.7572\n",
      "Iteration: 187000\n",
      "Gradient: [  3.2042  27.7537 -15.2991  24.5275 104.4086]\n",
      "Weights: [-4.8448  1.0849 -1.7283  0.3732  0.1007]\n",
      "MSE loss: 88.1731\n",
      "Iteration: 187100\n",
      "Gradient: [ -11.0889    3.2523  -40.4717 -248.9311   72.2419]\n",
      "Weights: [-4.8597  1.0913 -1.7288  0.3741  0.1006]\n",
      "MSE loss: 88.1981\n",
      "Iteration: 187200\n",
      "Gradient: [  11.535    -0.8797   -4.3293  -66.1185 -349.9929]\n",
      "Weights: [-4.8434  1.0924 -1.7291  0.3741  0.1003]\n",
      "MSE loss: 88.3487\n",
      "Iteration: 187300\n",
      "Gradient: [-16.6241 -10.2517 -67.9812 -34.4627 -27.2689]\n",
      "Weights: [-4.8604  1.0956 -1.7361  0.3758  0.1003]\n",
      "MSE loss: 88.564\n",
      "Iteration: 187400\n",
      "Gradient: [  7.5952 -18.7396 -54.5657  63.433  -77.9733]\n",
      "Weights: [-4.8584  1.0916 -1.7326  0.3763  0.1003]\n",
      "MSE loss: 88.1656\n",
      "Iteration: 187500\n",
      "Gradient: [  -8.6431   -5.7352   48.9556 -103.8178   36.1272]\n",
      "Weights: [-4.8402  1.0745 -1.7316  0.3777  0.1001]\n",
      "MSE loss: 88.0929\n",
      "Iteration: 187600\n",
      "Gradient: [   6.0078  -30.0415   24.5559  -34.0269 -264.1596]\n",
      "Weights: [-4.8448  1.0698 -1.732   0.3784  0.1   ]\n",
      "MSE loss: 88.3816\n",
      "Iteration: 187700\n",
      "Gradient: [-20.6066 -41.3171 -34.1395  84.6058 -66.7977]\n",
      "Weights: [-4.8561  1.0796 -1.7295  0.3773  0.0995]\n",
      "MSE loss: 88.6866\n",
      "Iteration: 187800\n",
      "Gradient: [  4.0957  20.0645  91.7853 -25.14   401.0497]\n",
      "Weights: [-4.8574  1.0812 -1.7306  0.3777  0.0999]\n",
      "MSE loss: 88.2533\n",
      "Iteration: 187900\n",
      "Gradient: [ 17.0322  -3.2735  23.1918  50.4969 -27.4684]\n",
      "Weights: [-4.8328  1.0644 -1.7313  0.3785  0.1001]\n",
      "MSE loss: 88.1644\n",
      "Iteration: 188000\n",
      "Gradient: [-15.5751   5.4167   0.3647  52.707  279.0604]\n",
      "Weights: [-4.8607  1.0874 -1.7317  0.3783  0.0994]\n",
      "MSE loss: 88.3104\n",
      "Iteration: 188100\n",
      "Gradient: [   7.9897  -10.3485  -36.6812 -170.1152 -238.1163]\n",
      "Weights: [-4.8333  1.0924 -1.7329  0.3782  0.0993]\n",
      "MSE loss: 88.9416\n",
      "Iteration: 188200\n",
      "Gradient: [-0.1926 -8.4004 23.0819 54.6104 84.2653]\n",
      "Weights: [-4.8434  1.0889 -1.7352  0.3783  0.0998]\n",
      "MSE loss: 88.1619\n",
      "Iteration: 188300\n",
      "Gradient: [ 14.3416  26.3473  97.4502  -1.8934 805.4981]\n",
      "Weights: [-4.8539  1.0878 -1.7325  0.3782  0.0999]\n",
      "MSE loss: 88.2505\n",
      "Iteration: 188400\n",
      "Gradient: [  6.1399  -7.3975 -24.7491  30.5408 -20.5364]\n",
      "Weights: [-4.8397  1.0856 -1.7336  0.3772  0.0997]\n",
      "MSE loss: 88.2774\n",
      "Iteration: 188500\n",
      "Gradient: [  -7.6858  -27.3216  -41.3061  160.3909 -229.9266]\n",
      "Weights: [-4.842   1.0676 -1.7304  0.3788  0.0998]\n",
      "MSE loss: 88.2061\n",
      "Iteration: 188600\n",
      "Gradient: [ 15.6267   7.055   28.7078 323.8302 288.2105]\n",
      "Weights: [-4.837   1.0729 -1.7292  0.3789  0.0998]\n",
      "MSE loss: 88.5217\n",
      "Iteration: 188700\n",
      "Gradient: [  11.5992   15.5154   17.8779  -75.1636 -361.6869]\n",
      "Weights: [-4.8219  1.0702 -1.7294  0.3778  0.0999]\n",
      "MSE loss: 88.5574\n",
      "Iteration: 188800\n",
      "Gradient: [ 16.8253  -9.1345   3.4683  32.7677 -53.6897]\n",
      "Weights: [-4.8242  1.075  -1.7262  0.3777  0.0997]\n",
      "MSE loss: 89.5361\n",
      "Iteration: 188900\n",
      "Gradient: [ -15.775    -7.8602   10.1739 -152.0358  133.9863]\n",
      "Weights: [-4.8544  1.0598 -1.7288  0.3782  0.1001]\n",
      "MSE loss: 89.4876\n",
      "Iteration: 189000\n",
      "Gradient: [ -2.8481  28.1568 141.8235 186.2948 558.1989]\n",
      "Weights: [-4.8372  1.0731 -1.7287  0.3771  0.1003]\n",
      "MSE loss: 88.3442\n",
      "Iteration: 189100\n",
      "Gradient: [ -1.2468  14.5742 -24.6994 -79.36   -80.7634]\n",
      "Weights: [-4.8336  1.0737 -1.7288  0.3775  0.1001]\n",
      "MSE loss: 88.4076\n",
      "Iteration: 189200\n",
      "Gradient: [  -4.3111    1.5813   -5.8267 -133.0958 -475.2929]\n",
      "Weights: [-4.856   1.0826 -1.7283  0.377   0.0999]\n",
      "MSE loss: 88.1736\n",
      "Iteration: 189300\n",
      "Gradient: [ 2.94800e-01  2.24369e+01  8.12750e+00 -1.66307e+01 -2.24000e-02]\n",
      "Weights: [-4.8423  1.0912 -1.7317  0.3771  0.1   ]\n",
      "MSE loss: 88.7051\n",
      "Iteration: 189400\n",
      "Gradient: [ -10.8548  -12.4434 -108.8968 -231.4214 -444.7171]\n",
      "Weights: [-4.8411  1.0766 -1.7292  0.3772  0.1   ]\n",
      "MSE loss: 88.1301\n",
      "Iteration: 189500\n",
      "Gradient: [ -3.3525  18.9019  56.3136 -12.9128 524.1531]\n",
      "Weights: [-4.8408  1.0718 -1.7306  0.3777  0.1001]\n",
      "MSE loss: 88.1074\n",
      "Iteration: 189600\n",
      "Gradient: [ -5.9844  25.236  -17.1805 106.2688  24.3841]\n",
      "Weights: [-4.8333  1.067  -1.7303  0.3792  0.0998]\n",
      "MSE loss: 88.1154\n",
      "Iteration: 189700\n",
      "Gradient: [-24.76    23.0786 -17.7433  15.2152 214.6695]\n",
      "Weights: [-4.8449  1.06   -1.7296  0.3792  0.0997]\n",
      "MSE loss: 88.8806\n",
      "Iteration: 189800\n",
      "Gradient: [   7.2539   -2.802     3.1496 -176.4395  118.439 ]\n",
      "Weights: [-4.8373  1.0704 -1.7307  0.3789  0.0996]\n",
      "MSE loss: 88.0873\n",
      "Iteration: 189900\n",
      "Gradient: [  1.4553  35.3624  10.0353 114.9455 -39.5062]\n",
      "Weights: [-4.8382  1.0722 -1.7286  0.3787  0.0996]\n",
      "MSE loss: 88.1393\n",
      "Iteration: 190000\n",
      "Gradient: [ -2.1317  -5.7227 -49.6201  15.674   -6.9704]\n",
      "Weights: [-4.8479  1.0765 -1.7282  0.3793  0.0992]\n",
      "MSE loss: 88.1345\n",
      "Iteration: 190100\n",
      "Gradient: [ 15.7303 -29.651   12.1798  48.3723 320.0956]\n",
      "Weights: [-4.8374  1.0712 -1.7314  0.3794  0.0995]\n",
      "MSE loss: 88.0919\n",
      "Iteration: 190200\n",
      "Gradient: [ -13.9611   -4.7513  -41.8711 -202.7176 -535.3908]\n",
      "Weights: [-4.8502  1.0739 -1.7323  0.3789  0.0994]\n",
      "MSE loss: 88.8556\n",
      "Iteration: 190300\n",
      "Gradient: [  30.5168   29.4207   22.5258 -106.0275  133.6724]\n",
      "Weights: [-4.84    1.0803 -1.7324  0.3796  0.0995]\n",
      "MSE loss: 88.3006\n",
      "Iteration: 190400\n",
      "Gradient: [ -17.6712    0.5783  -43.7058 -116.1075   87.3325]\n",
      "Weights: [-4.8574  1.0843 -1.733   0.3793  0.0993]\n",
      "MSE loss: 88.3102\n",
      "Iteration: 190500\n",
      "Gradient: [-15.1472   1.2427  20.0487 158.5973 402.839 ]\n",
      "Weights: [-4.8556  1.0866 -1.731   0.3788  0.0993]\n",
      "MSE loss: 88.1369\n",
      "Iteration: 190600\n",
      "Gradient: [ 15.9667   0.3997  71.0156 159.9937 337.1557]\n",
      "Weights: [-4.8415  1.085  -1.7294  0.378   0.0996]\n",
      "MSE loss: 88.5216\n",
      "Iteration: 190700\n",
      "Gradient: [ -28.2512   -3.584   -43.6675  -45.9488 -631.1663]\n",
      "Weights: [-4.8675  1.0885 -1.732   0.3776  0.0996]\n",
      "MSE loss: 88.7167\n",
      "Iteration: 190800\n",
      "Gradient: [ -4.0751 -12.2648  70.7367 -46.2731  96.3644]\n",
      "Weights: [-4.8409  1.0644 -1.7302  0.3777  0.0999]\n",
      "MSE loss: 88.7983\n",
      "Iteration: 190900\n",
      "Gradient: [  -2.1365  -18.0243   -7.4529  -41.8542 -849.6405]\n",
      "Weights: [-4.8412  1.0649 -1.7294  0.3777  0.1   ]\n",
      "MSE loss: 88.3885\n",
      "Iteration: 191000\n",
      "Gradient: [  7.8047  22.4559 -12.7649 -24.333  352.5394]\n",
      "Weights: [-4.8422  1.0751 -1.732   0.3782  0.1001]\n",
      "MSE loss: 88.1123\n",
      "Iteration: 191100\n",
      "Gradient: [ -4.267   26.9274  -4.3083 243.193  138.9792]\n",
      "Weights: [-4.8387  1.0734 -1.7294  0.3777  0.1001]\n",
      "MSE loss: 88.1843\n",
      "Iteration: 191200\n",
      "Gradient: [-8.038900e+00 -2.019000e-01 -1.823090e+01 -1.425472e+02 -4.257581e+02]\n",
      "Weights: [-4.8476  1.0906 -1.7286  0.3763  0.0998]\n",
      "MSE loss: 88.4158\n",
      "Iteration: 191300\n",
      "Gradient: [   8.5044  -19.1905   37.7515  -84.9105 -101.2404]\n",
      "Weights: [-4.8274  1.0777 -1.7306  0.3772  0.0996]\n",
      "MSE loss: 88.3941\n",
      "Iteration: 191400\n",
      "Gradient: [ 15.3619 -13.2504  15.8751  36.7224  11.131 ]\n",
      "Weights: [-4.8223  1.0655 -1.7293  0.3782  0.0997]\n",
      "MSE loss: 88.274\n",
      "Iteration: 191500\n",
      "Gradient: [  -0.4515   -9.9197  -25.4366 -214.9779 -246.1927]\n",
      "Weights: [-4.8441  1.0677 -1.7276  0.3788  0.0997]\n",
      "MSE loss: 88.1556\n",
      "Iteration: 191600\n",
      "Gradient: [ -11.4118  -19.9316  -57.3044  -66.5584 -419.2101]\n",
      "Weights: [-4.8446  1.0669 -1.7272  0.3785  0.0997]\n",
      "MSE loss: 88.1478\n",
      "Iteration: 191700\n",
      "Gradient: [-32.7864   2.1006  24.2105 -50.1717 180.3382]\n",
      "Weights: [-4.8715  1.0731 -1.7279  0.3774  0.0995]\n",
      "MSE loss: 90.6427\n",
      "Iteration: 191800\n",
      "Gradient: [   8.8755   11.8111  -10.9798   -1.6185 -295.0125]\n",
      "Weights: [-4.833   1.069  -1.7255  0.3771  0.0997]\n",
      "MSE loss: 88.1677\n",
      "Iteration: 191900\n",
      "Gradient: [ -7.9935  34.9336  91.0002 215.9014 433.2267]\n",
      "Weights: [-4.8163  1.0554 -1.7261  0.3781  0.1   ]\n",
      "MSE loss: 88.4875\n",
      "Iteration: 192000\n",
      "Gradient: [ -7.0487 -26.3987 -77.1918 -29.9787 119.5401]\n",
      "Weights: [-4.8389  1.0523 -1.723   0.3775  0.1   ]\n",
      "MSE loss: 88.3903\n",
      "Iteration: 192100\n",
      "Gradient: [   5.7828    9.1293  -36.0014 -147.8104 -408.8655]\n",
      "Weights: [-4.8132  1.0434 -1.7215  0.3776  0.1   ]\n",
      "MSE loss: 88.323\n",
      "Iteration: 192200\n",
      "Gradient: [ -11.5046  -29.1357  -65.3129 -179.6129   24.1746]\n",
      "Weights: [-4.8315  1.0509 -1.7199  0.3771  0.0998]\n",
      "MSE loss: 88.1469\n",
      "Iteration: 192300\n",
      "Gradient: [-10.9049  16.2797 -18.8347 106.5642 -87.2196]\n",
      "Weights: [-4.8172  1.0527 -1.7229  0.3781  0.0996]\n",
      "MSE loss: 88.3706\n",
      "Iteration: 192400\n",
      "Gradient: [  -0.8046  -20.0956  -34.4744 -175.1987 -349.509 ]\n",
      "Weights: [-4.824   1.0478 -1.7229  0.3788  0.0996]\n",
      "MSE loss: 88.1696\n",
      "Iteration: 192500\n",
      "Gradient: [  4.3725  35.7244  60.5767  76.3452 221.4805]\n",
      "Weights: [-4.8231  1.0413 -1.7188  0.38    0.0992]\n",
      "MSE loss: 88.3031\n",
      "Iteration: 192600\n",
      "Gradient: [ 16.5799  13.5952  -4.7306  14.6314 191.0121]\n",
      "Weights: [-4.8151  1.0416 -1.7234  0.3802  0.0991]\n",
      "MSE loss: 88.2921\n",
      "Iteration: 192700\n",
      "Gradient: [ 13.732   24.565   -9.9723 111.7977 160.4846]\n",
      "Weights: [-4.8377  1.0557 -1.7227  0.3804  0.0991]\n",
      "MSE loss: 88.2084\n",
      "Iteration: 192800\n",
      "Gradient: [ -19.5812   -0.6232 -100.3312 -210.0417 -110.196 ]\n",
      "Weights: [-4.8558  1.0448 -1.7231  0.3807  0.099 ]\n",
      "MSE loss: 90.444\n",
      "Iteration: 192900\n",
      "Gradient: [  8.3189   9.8532  37.2455  19.9059 499.9756]\n",
      "Weights: [-4.8093  1.0485 -1.7263  0.3813  0.099 ]\n",
      "MSE loss: 88.4792\n",
      "Iteration: 193000\n",
      "Gradient: [  7.56     5.5511  -1.5044 -35.6303 271.3741]\n",
      "Weights: [-4.8193  1.0559 -1.7278  0.3805  0.0992]\n",
      "MSE loss: 88.2412\n",
      "Iteration: 193100\n",
      "Gradient: [ -1.0604  45.5701  98.2067 218.4932 523.9521]\n",
      "Weights: [-4.8345  1.0699 -1.7278  0.3813  0.099 ]\n",
      "MSE loss: 88.902\n",
      "Iteration: 193200\n",
      "Gradient: [-16.4703  37.5073  32.7938 -60.5322 476.3684]\n",
      "Weights: [-4.8718  1.0705 -1.7286  0.3817  0.0985]\n",
      "MSE loss: 89.6243\n",
      "Iteration: 193300\n",
      "Gradient: [  16.1079    5.0117   -5.2309 -184.4191   93.1161]\n",
      "Weights: [-4.8106  1.0589 -1.7305  0.3829  0.0985]\n",
      "MSE loss: 88.7735\n",
      "Iteration: 193400\n",
      "Gradient: [   0.9645  -32.1613  -23.9618   36.6407 -193.7037]\n",
      "Weights: [-4.8221  1.0535 -1.7335  0.3846  0.0987]\n",
      "MSE loss: 88.1407\n",
      "Iteration: 193500\n",
      "Gradient: [  2.129   -2.3666  34.0609 268.0701  90.2683]\n",
      "Weights: [-4.8347  1.0634 -1.7312  0.384   0.0984]\n",
      "MSE loss: 88.1083\n",
      "Iteration: 193600\n",
      "Gradient: [  -4.435   -13.7446  -76.0941 -187.9899 -530.7503]\n",
      "Weights: [-4.8404  1.0602 -1.7337  0.3847  0.0983]\n",
      "MSE loss: 88.3235\n",
      "Iteration: 193700\n",
      "Gradient: [   4.0078   39.3335   -4.232    17.4364 -349.8634]\n",
      "Weights: [-4.8128  1.0508 -1.7278  0.3849  0.0981]\n",
      "MSE loss: 88.8029\n",
      "Iteration: 193800\n",
      "Gradient: [ -12.8857    6.1266  -10.1597  -57.8933 -128.7851]\n",
      "Weights: [-4.8432  1.0465 -1.7247  0.3854  0.0976]\n",
      "MSE loss: 88.4417\n",
      "Iteration: 193900\n",
      "Gradient: [  3.992  -20.4143 -52.6096 -74.4442  -0.2632]\n",
      "Weights: [-4.821   1.039  -1.7216  0.3847  0.0976]\n",
      "MSE loss: 88.1144\n",
      "Iteration: 194000\n",
      "Gradient: [   2.1793  -23.7147 -103.8685 -186.7476 -513.8663]\n",
      "Weights: [-4.8227  1.0543 -1.727   0.3853  0.0976]\n",
      "MSE loss: 88.2719\n",
      "Iteration: 194100\n",
      "Gradient: [  -1.659    -4.8366  -21.7121 -182.3339  274.264 ]\n",
      "Weights: [-4.8349  1.0525 -1.7275  0.3844  0.0978]\n",
      "MSE loss: 88.2857\n",
      "Iteration: 194200\n",
      "Gradient: [ -9.6533 -23.1038  30.2394 -37.0367 -78.6987]\n",
      "Weights: [-4.8381  1.0556 -1.729   0.3852  0.0976]\n",
      "MSE loss: 88.2954\n",
      "Iteration: 194300\n",
      "Gradient: [   2.2508   -7.7516   12.0004   36.1936 -191.4212]\n",
      "Weights: [-4.836   1.0605 -1.7326  0.3859  0.0978]\n",
      "MSE loss: 88.0502\n",
      "Iteration: 194400\n",
      "Gradient: [   2.8895  -40.9841  -59.5662 -113.5288   35.5237]\n",
      "Weights: [-4.8398  1.0686 -1.7342  0.3855  0.0978]\n",
      "MSE loss: 88.05\n",
      "Iteration: 194500\n",
      "Gradient: [  8.4393 -12.3992 -94.3419  36.1825  91.6883]\n",
      "Weights: [-4.8387  1.0817 -1.738   0.3856  0.0979]\n",
      "MSE loss: 88.2464\n",
      "Iteration: 194600\n",
      "Gradient: [  -6.8323   -7.2769 -115.8483 -211.5013 -393.8856]\n",
      "Weights: [-4.8533  1.0863 -1.7376  0.385   0.098 ]\n",
      "MSE loss: 88.0853\n",
      "Iteration: 194700\n",
      "Gradient: [  10.52    -15.1788   24.1011   60.6829 -314.1447]\n",
      "Weights: [-4.8457  1.0818 -1.7422  0.3864  0.0982]\n",
      "MSE loss: 88.0156\n",
      "Iteration: 194800\n",
      "Gradient: [ 13.9903  -9.462   55.5933 172.6117   4.5988]\n",
      "Weights: [-4.8336  1.0825 -1.7404  0.3858  0.0981]\n",
      "MSE loss: 88.4233\n",
      "Iteration: 194900\n",
      "Gradient: [  1.6078 -23.3953   1.5741  95.9516 344.3027]\n",
      "Weights: [-4.854   1.0782 -1.7405  0.3854  0.0984]\n",
      "MSE loss: 88.2981\n",
      "Iteration: 195000\n",
      "Gradient: [  0.2128  23.4464 -14.6415  87.0029  47.3948]\n",
      "Weights: [-4.8416  1.0906 -1.739   0.3849  0.0982]\n",
      "MSE loss: 88.7442\n",
      "Iteration: 195100\n",
      "Gradient: [-17.5189  37.7813 101.98   139.3397 628.0077]\n",
      "Weights: [-4.858   1.0766 -1.7386  0.3855  0.0982]\n",
      "MSE loss: 88.4019\n",
      "Iteration: 195200\n",
      "Gradient: [ -0.7788  16.7038 -43.257   23.3248 -71.1188]\n",
      "Weights: [-4.8348  1.0826 -1.7386  0.3848  0.0982]\n",
      "MSE loss: 88.3247\n",
      "Iteration: 195300\n",
      "Gradient: [ 13.2091  19.9933  48.8476  13.5295 481.2138]\n",
      "Weights: [-4.8508  1.0832 -1.7392  0.3844  0.0985]\n",
      "MSE loss: 88.0692\n",
      "Iteration: 195400\n",
      "Gradient: [-22.6117  -1.9277   6.9029  91.1518 -61.3986]\n",
      "Weights: [-4.856   1.0701 -1.7366  0.3846  0.0986]\n",
      "MSE loss: 88.5733\n",
      "Iteration: 195500\n",
      "Gradient: [   5.4765  -12.6628  -40.5175   70.0284 -193.212 ]\n",
      "Weights: [-4.8516  1.0889 -1.7385  0.3845  0.0984]\n",
      "MSE loss: 88.3845\n",
      "Iteration: 195600\n",
      "Gradient: [ 10.4653  23.1013  43.1479  19.5822 289.6905]\n",
      "Weights: [-4.8096  1.0687 -1.7391  0.3853  0.0985]\n",
      "MSE loss: 89.2161\n",
      "Iteration: 195700\n",
      "Gradient: [-14.4844   5.2094  47.924  -52.7483 248.3365]\n",
      "Weights: [-4.8337  1.0607 -1.737   0.3855  0.0986]\n",
      "MSE loss: 88.0977\n",
      "Iteration: 195800\n",
      "Gradient: [  20.0216    8.2441    3.0477 -101.5689 -281.3273]\n",
      "Weights: [-4.8483  1.0773 -1.7415  0.3846  0.0988]\n",
      "MSE loss: 88.2046\n",
      "Iteration: 195900\n",
      "Gradient: [   0.6506   11.8349    8.398   185.1201 -388.5556]\n",
      "Weights: [-4.8357  1.0778 -1.7428  0.3848  0.0989]\n",
      "MSE loss: 88.0414\n",
      "Iteration: 196000\n",
      "Gradient: [  -8.0999  -41.0636  -70.1024 -167.9475 -454.6134]\n",
      "Weights: [-4.8332  1.0803 -1.7409  0.3832  0.0988]\n",
      "MSE loss: 88.1282\n",
      "Iteration: 196100\n",
      "Gradient: [ -2.9545  20.8001   4.4898  -3.1402 402.0243]\n",
      "Weights: [-4.831   1.078  -1.7401  0.3837  0.0989]\n",
      "MSE loss: 88.1855\n",
      "Iteration: 196200\n",
      "Gradient: [  -7.8752   -1.2305   56.8001   80.4436 -159.3435]\n",
      "Weights: [-4.8515  1.0965 -1.7429  0.3836  0.0988]\n",
      "MSE loss: 88.1519\n",
      "Iteration: 196300\n",
      "Gradient: [  7.7889   2.3944  54.9085 209.1753  66.3951]\n",
      "Weights: [-4.8338  1.0809 -1.7419  0.3837  0.0987]\n",
      "MSE loss: 88.1062\n",
      "Iteration: 196400\n",
      "Gradient: [ -14.1123   -9.8734  -58.8133  -34.2876 -229.5904]\n",
      "Weights: [-4.8576  1.0887 -1.7451  0.3842  0.0988]\n",
      "MSE loss: 88.4832\n",
      "Iteration: 196500\n",
      "Gradient: [ -20.3337    5.0419   59.0193  124.6401 -134.5813]\n",
      "Weights: [-4.8607  1.0952 -1.7428  0.3837  0.0988]\n",
      "MSE loss: 88.1287\n",
      "Iteration: 196600\n",
      "Gradient: [-17.9924  23.9325  72.8197  22.6235 351.733 ]\n",
      "Weights: [-4.8622  1.0923 -1.7449  0.3841  0.099 ]\n",
      "MSE loss: 88.2725\n",
      "Iteration: 196700\n",
      "Gradient: [  7.3058  16.5095 -16.521   89.159  128.3167]\n",
      "Weights: [-4.8388  1.0894 -1.7422  0.3836  0.099 ]\n",
      "MSE loss: 88.3905\n",
      "Iteration: 196800\n",
      "Gradient: [   0.9215  -13.9514   -0.7883  -55.7886 -287.438 ]\n",
      "Weights: [-4.8484  1.0692 -1.7387  0.3835  0.099 ]\n",
      "MSE loss: 88.6462\n",
      "Iteration: 196900\n",
      "Gradient: [   4.7548  -20.1281   25.7918   69.4504 -319.0131]\n",
      "Weights: [-4.8383  1.079  -1.7425  0.3843  0.0989]\n",
      "MSE loss: 88.0263\n",
      "Iteration: 197000\n",
      "Gradient: [  -2.0701    0.2512  -21.8855  -31.3837 -193.9089]\n",
      "Weights: [-4.8651  1.0937 -1.7444  0.383   0.099 ]\n",
      "MSE loss: 88.6192\n",
      "Iteration: 197100\n",
      "Gradient: [-24.8833 -25.6976 -41.2138 -30.2072 173.1949]\n",
      "Weights: [-4.8766  1.1011 -1.7475  0.3821  0.0992]\n",
      "MSE loss: 90.1707\n",
      "Iteration: 197200\n",
      "Gradient: [-18.7105 -36.9503 -54.8778  56.2927  16.9715]\n",
      "Weights: [-4.8629  1.1076 -1.749   0.3825  0.099 ]\n",
      "MSE loss: 88.4594\n",
      "Iteration: 197300\n",
      "Gradient: [ -0.7051  40.9192  12.1308 162.0769 272.1858]\n",
      "Weights: [-4.8618  1.1159 -1.7474  0.3828  0.0991]\n",
      "MSE loss: 88.5564\n",
      "Iteration: 197400\n",
      "Gradient: [  0.837  -12.5957  65.0914  52.6615 121.5621]\n",
      "Weights: [-4.8647  1.1003 -1.7429  0.3809  0.0994]\n",
      "MSE loss: 88.2907\n",
      "Iteration: 197500\n",
      "Gradient: [ -1.2993 -10.125   -4.2598  70.1988 406.2009]\n",
      "Weights: [-4.8546  1.1019 -1.743   0.3807  0.0994]\n",
      "MSE loss: 88.09\n",
      "Iteration: 197600\n",
      "Gradient: [  8.9883  22.0262  26.4048 184.5224 132.271 ]\n",
      "Weights: [-4.8453  1.1004 -1.748   0.3827  0.0995]\n",
      "MSE loss: 88.1861\n",
      "Iteration: 197700\n",
      "Gradient: [ -4.4878 -27.4268 -60.1103 115.6052  -2.0089]\n",
      "Weights: [-4.8365  1.0881 -1.7483  0.383   0.0997]\n",
      "MSE loss: 88.0714\n",
      "Iteration: 197800\n",
      "Gradient: [   8.2615    4.4645   58.9953  -65.8878 -159.1916]\n",
      "Weights: [-4.8477  1.0815 -1.7446  0.3826  0.0999]\n",
      "MSE loss: 88.2154\n",
      "Iteration: 197900\n",
      "Gradient: [ 16.6951  17.5049  11.9486 215.1158 656.5947]\n",
      "Weights: [-4.8418  1.0972 -1.7443  0.3819  0.0996]\n",
      "MSE loss: 88.5119\n",
      "Iteration: 198000\n",
      "Gradient: [ -1.2449  -4.8389  10.5601   8.4386 457.5642]\n",
      "Weights: [-4.8415  1.0882 -1.7427  0.3826  0.0993]\n",
      "MSE loss: 88.1064\n",
      "Iteration: 198100\n",
      "Gradient: [   9.2008   27.3474   16.0817 -151.9709 -223.2394]\n",
      "Weights: [-4.8311  1.0759 -1.7399  0.3827  0.0992]\n",
      "MSE loss: 88.1019\n",
      "Iteration: 198200\n",
      "Gradient: [ -14.8753  -14.5444  -93.6452 -200.6941   92.9895]\n",
      "Weights: [-4.8345  1.065  -1.7396  0.3839  0.0991]\n",
      "MSE loss: 88.2125\n",
      "Iteration: 198300\n",
      "Gradient: [-14.0443  -0.8391  14.6891  70.1731 551.4683]\n",
      "Weights: [-4.8388  1.0764 -1.7403  0.3831  0.099 ]\n",
      "MSE loss: 88.0928\n",
      "Iteration: 198400\n",
      "Gradient: [  5.5424  24.69     3.7919 -14.4278 252.6102]\n",
      "Weights: [-4.8607  1.0894 -1.7384  0.3839  0.0985]\n",
      "MSE loss: 88.1841\n",
      "Iteration: 198500\n",
      "Gradient: [ -1.5626  35.4704 100.8154 114.8541 227.9358]\n",
      "Weights: [-4.8655  1.088  -1.7368  0.383   0.0984]\n",
      "MSE loss: 88.4249\n",
      "Iteration: 198600\n",
      "Gradient: [ 18.8572  17.435  -54.3135 271.73   558.5603]\n",
      "Weights: [-4.8248  1.0734 -1.7367  0.3848  0.0985]\n",
      "MSE loss: 88.7874\n",
      "Iteration: 198700\n",
      "Gradient: [  -6.6746   -6.733    -9.729    -1.1407 -182.2933]\n",
      "Weights: [-4.8539  1.0672 -1.7331  0.3844  0.0981]\n",
      "MSE loss: 88.6849\n",
      "Iteration: 198800\n",
      "Gradient: [  10.693   -13.9945   28.0594  -22.5963 -414.862 ]\n",
      "Weights: [-4.8194  1.0598 -1.7293  0.3847  0.0978]\n",
      "MSE loss: 88.4372\n",
      "Iteration: 198900\n",
      "Gradient: [ 14.1618   0.6499 -34.238   41.6972 -91.1787]\n",
      "Weights: [-4.8451  1.0774 -1.7319  0.3845  0.0976]\n",
      "MSE loss: 88.1116\n",
      "Iteration: 199000\n",
      "Gradient: [  -8.9492  -43.339    31.781   -35.0484 -233.572 ]\n",
      "Weights: [-4.8515  1.0788 -1.7336  0.3843  0.0979]\n",
      "MSE loss: 88.0895\n",
      "Iteration: 199100\n",
      "Gradient: [  6.4899 -19.0546  29.069  105.0796 392.3722]\n",
      "Weights: [-4.8276  1.0605 -1.7306  0.3848  0.0981]\n",
      "MSE loss: 88.2187\n",
      "Iteration: 199200\n",
      "Gradient: [ 23.3382  23.3648  22.2249 181.8171 162.6851]\n",
      "Weights: [-4.8314  1.0592 -1.7322  0.3844  0.0983]\n",
      "MSE loss: 88.0689\n",
      "Iteration: 199300\n",
      "Gradient: [  -1.2054   -8.9938  -16.8246    6.3872 -134.1657]\n",
      "Weights: [-4.8177  1.0637 -1.7366  0.385   0.0984]\n",
      "MSE loss: 88.3609\n",
      "Iteration: 199400\n",
      "Gradient: [   5.1624   33.5976   58.4152 -180.1304  -59.4614]\n",
      "Weights: [-4.8312  1.0827 -1.737   0.3837  0.0985]\n",
      "MSE loss: 88.7888\n",
      "Iteration: 199500\n",
      "Gradient: [ -5.4478   7.5536 -15.3523 -49.9279 292.5565]\n",
      "Weights: [-4.8314  1.0633 -1.7396  0.3853  0.0989]\n",
      "MSE loss: 88.1127\n",
      "Iteration: 199600\n",
      "Gradient: [   1.8768  -10.3843  -61.1321  -31.2617 -216.1222]\n",
      "Weights: [-4.8378  1.0696 -1.739   0.3862  0.0985]\n",
      "MSE loss: 88.1342\n",
      "Iteration: 199700\n",
      "Gradient: [ 8.1842 21.7457 32.807  34.0958  4.2327]\n",
      "Weights: [-4.8299  1.0636 -1.7366  0.3867  0.0981]\n",
      "MSE loss: 88.1752\n",
      "Iteration: 199800\n",
      "Gradient: [  -6.7264  -27.4091  -70.7997   -5.6279 -291.151 ]\n",
      "Weights: [-4.8328  1.0528 -1.7327  0.3852  0.0981]\n",
      "MSE loss: 88.37\n",
      "Iteration: 199900\n",
      "Gradient: [ 12.3764 -25.1353  46.1559 129.3264 527.0797]\n",
      "Weights: [-4.8463  1.0719 -1.7345  0.3845  0.0983]\n",
      "MSE loss: 88.0681\n"
     ]
    }
   ],
   "source": [
    "weights_6, losses_6, iter_final_6, fit_time_6 = model_fit(X_train, y_train,\n",
    "                                                          learning_rate=[1e-3, 1e-4, 1e-5, 1e-6, 1e-7],\n",
    "                                                          tolerance=(0.2**2 * N_points),\n",
    "                                                          beta=0,\n",
    "                                                          batch_ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be7e7b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses_1, color='magenta', linestyle='--', linewidth=2)\n",
    "plt.plot(losses_2, color='orange', linestyle='-.', linewidth=2)\n",
    "plt.plot(losses_3, color='blue', linestyle='--', linewidth=2)\n",
    "plt.plot(losses_4, color='green', linestyle='-', linewidth=2)\n",
    "plt.plot(losses_5, color='brown', linestyle='--', linewidth=1)\n",
    "plt.plot(losses_6, color='red', linestyle=':', linewidth=2)\n",
    "plt.ylim(0,800)\n",
    "\n",
    "plt.legend(['lr=[5e-7, 5e-7, 5e-7, 5e-7, 5e-7], batch=1.0, beta=0.0',\n",
    "            'lr=[1e-5, 1e-6, 1e-7, 1e-8, 1e-9], batch=1.0, beta=0.0',\n",
    "            'lr=[1e-3, 1e-4, 1e-5, 1e-6, 1e-7], batch=0.1, beta=0.0',\n",
    "            'lr=[1e-3, 1e-4, 1e-5, 1e-6, 1e-7], batch=0.1, beta=0.8',\n",
    "            'lr=[1e-3, 1e-4, 1e-5, 1e-6, 1e-7], batch=0.1, beta=0.4',\n",
    "            'lr=[1e-3, 1e-4, 1e-5, 1e-6, 1e-7], batch=0.2, beta=0.0'])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc74e00d",
   "metadata": {},
   "source": [
    "Кривые для batch=0.1, beta=[0.0, 0.4, 0.8] практически накладываются друг на друга. Следовательно, в рассмотренной частной задачи оптимизации параметр beta незначим. Фильтрация градиента (Momentum) не требуется."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11143885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "legend_list =  ['lr=[5e-7, 5e-7, 5e-7, 5e-7, 5e-7], batch=1.0, beta=0.0',\n",
    "                'lr=[1e-5, 1e-6, 1e-7, 1e-8, 1e-9], batch=1.0, beta=0.0',\n",
    "                'lr=[1e-3, 1e-4, 1e-5, 1e-6, 1e-7], batch=0.1, beta=0.0',\n",
    "                'lr=[1e-3, 1e-4, 1e-5, 1e-6, 1e-7], batch=0.2, beta=0.0']\n",
    "\n",
    "plt.plot(losses_1, color='magenta', linestyle='--', linewidth=2)\n",
    "plt.plot(losses_2, color='orange', linestyle='-.', linewidth=2)\n",
    "plt.plot(losses_3, color='blue', linestyle='--', linewidth=2)\n",
    "plt.plot(losses_6, color='red', linestyle=':', linewidth=2)\n",
    "plt.ylim(0,800)\n",
    "plt.legend(legend_list)\n",
    "plt.savefig('plots/MSE_plot.png')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6a399437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_train, y_train)\n",
    "plt.plot(np.sort(X_train), f_trend(np.sort(X_train)), color='red')\n",
    "plt.plot(np.sort(X_train), f_poly(np.sort(X_train), weights_1), '.', color='black')\n",
    "plt.plot(np.sort(X_train), f_poly(np.sort(X_train), weights_2), '.', color='green')\n",
    "plt.plot(np.sort(X_train), f_poly(np.sort(X_train), weights_3), '--', color='blue')\n",
    "plt.plot(np.sort(X_train), f_poly(np.sort(X_train), weights_6), '--', color='yellow')\n",
    "plt.legend(['dataset', 'trend line'] + legend_list)\n",
    "plt.savefig('plots/trend_line_plot.png')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "75f829b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fit_time = [fit_time_1, fit_time_2, fit_time_3, fit_time_6]\n",
    "bar_colors = ['magenta', 'orange', 'blue', 'red']\n",
    "plt.bar(['1', '2', '3', '4'], fit_time, color=bar_colors)\n",
    "plt.savefig('plots/time_plot.png')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b8df437f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17800.521367788315, 17925.368300437927, 3114.852860212326, 5091.532459259033]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

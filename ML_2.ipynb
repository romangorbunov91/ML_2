{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "453d2819",
   "metadata": {},
   "source": [
    "ДЗ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3d6c553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cbd143",
   "metadata": {},
   "source": [
    "Проверим как численно находить градиент функции f(x). Для этого помимо самой функции f(x) определим ее производную."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7601ac54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**3 - 3*x**2 + 2*x - 5\n",
    "    #return 2*x - 5\n",
    "\n",
    "def f_deriv(x):\n",
    "    return 3*x**2 - 6*x + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a770ef8",
   "metadata": {},
   "source": [
    "Формируем тренировочный датасет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "674758b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005\n",
      "0.005\n"
     ]
    }
   ],
   "source": [
    "x_min = 0\n",
    "x_max = 5\n",
    "points = 1000\n",
    "\n",
    "delta_x = np.abs(x_max - x_min) / points\n",
    "\n",
    "X_train = np.linspace(x_min, x_max, points + 1)\n",
    "y_train = f(X_train)\n",
    "\n",
    "# Проверка шага.\n",
    "print(delta_x)\n",
    "print(X_train[1] - X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2b7f618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(f_deriv(X_train))\n",
    "plt.plot(np.gradient(y_train, X_train), '--')\n",
    "plt.legend(['function', 'numerical'])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee933d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_targ(X, w_coeff):\n",
    "    Y = np.zeros_like(X)\n",
    "    for n, x in enumerate(X):\n",
    "        for idx, w in enumerate(w_coeff.tolist()[0]):\n",
    "            Y[n] = Y[n] + w * x**idx\n",
    "    return Y\n",
    "\n",
    "def loss_func(X, y, w_coeff):\n",
    "    #print(np.sum((y - func_targ(X, w_coeff))**2))\n",
    "    return np.sum((y - func_targ(X, w_coeff))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "551da401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0\n",
      "Gradient [[  -12550.13759888   -71108.66261646  -334709.89606576 -1533669.27115712]]\n",
      "Loss 304505.80716036935\n",
      "Weights [[-1.e-04 -1.e-05 -1.e-06 -1.e-07]]\n",
      "Iter 1\n",
      "Gradient [[  -11294.04193053   -65174.72084059  -292769.01777394 -1190143.04098254]]\n",
      "Loss 82868.20665780018\n",
      "Weights [[1.25491376 0.71107663 0.3347089  0.15336683]]\n",
      "Iter 2\n",
      "Gradient [[ 9843.23120478 11879.49092222 11579.07733414 26495.56395849]]\n",
      "Loss 193686.41343551694\n",
      "Weights [[2.38431795 1.36282383 0.62747791 0.27238113]]\n",
      "Iter 3\n",
      "Gradient [[ 25590.5356638   70142.77978276 230828.57079045 827105.12934969]]\n",
      "Loss 156436.1099730426\n",
      "Weights [[1.39999483 1.24402893 0.61589884 0.26973157]]\n",
      "Iter 4\n",
      "Gradient [[ 21089.84424905  56981.61052049 173814.03030388 532387.64366863]]\n",
      "Loss 55068.19678064712\n",
      "Weights [[-1.15905873  0.54260113  0.38507027  0.18702106]]\n",
      "Iter 5\n",
      "Gradient [[   3873.82556534   -1621.05805639  -49950.15011419 -349358.62236058]]\n",
      "Loss 117742.12472590743\n",
      "Weights [[-3.26804316 -0.02721498  0.21125624  0.1337823 ]]\n",
      "Iter 6\n",
      "Gradient [[  -7712.2956398   -41024.92886422 -191982.67620468 -846842.03229449]]\n",
      "Loss 81703.89133686821\n",
      "Weights [[-3.65542572 -0.0110044   0.26120639  0.16871816]]\n",
      "Iter 7\n",
      "Gradient [[  -4225.31860169  -27520.6674887  -130615.50490645 -546906.93394135]]\n",
      "Loss 34535.35052254316\n",
      "Weights [[-2.88419615  0.39924489  0.45318906  0.25340236]]\n",
      "Iter 8\n",
      "Gradient [[  7530.80023038  15306.98137974  36852.71781031 116987.30001494]]\n",
      "Loss 69578.76394476472\n",
      "Weights [[-2.46166429  0.67445157  0.58380457  0.30809306]]\n",
      "Iter 9\n",
      "Gradient [[ 14182.53057699  40327.76971323 130030.50943058 445188.91752752]]\n",
      "Loss 45569.531856976\n",
      "Weights [[-3.21474431  0.52138175  0.54695185  0.29639433]]\n",
      "Iter 10\n",
      "Gradient [[  9895.14445905  26675.51544092  74738.98051857 195235.1375081 ]]\n",
      "Loss 26094.65939046619\n",
      "Weights [[-4.63299737  0.11810406  0.41692134  0.25187543]]\n",
      "Iter 11\n",
      "Gradient [[    508.13893185   -5315.56098656  -46362.63199273 -273962.45065695]]\n",
      "Loss 43667.339308836206\n",
      "Weights [[-5.62251182 -0.1486511   0.34218236  0.23235192]]\n",
      "Iter 12\n",
      "Gradient [[  -4338.51932491  -21624.36997191 -103538.8161484  -463238.70686203]]\n",
      "Loss 26953.35373062311\n",
      "Weights [[-5.67332571 -0.09549549  0.38854499  0.25974817]]\n",
      "Iter 13\n",
      "Gradient [[  -1199.49287532   -9860.67258599  -53650.11541224 -239529.02679525]]\n",
      "Loss 19051.73196820385\n",
      "Weights [[-5.23947378  0.12074821  0.49208381  0.30607204]]\n",
      "Iter 14\n",
      "Gradient [[  5066.43342902  13032.51104963  35230.56068278 107431.29087742]]\n",
      "Loss 28020.398906398812\n",
      "Weights [[-5.11952449  0.21935494  0.54573392  0.33002494]]\n",
      "Iter 15\n",
      "Gradient [[  7568.44166006  22731.18879504  70742.02735547 225299.2137978 ]]\n",
      "Loss 18313.904418530063\n",
      "Weights [[-5.62616783  0.08902983  0.51050336  0.31928181]]\n",
      "Iter 16\n",
      "Gradient [[ 4390.56728606 12311.8413222  29619.61140683 49568.76211467]]\n",
      "Loss 15758.553224364168\n",
      "Weights [[-6.383012   -0.13828206  0.43976133  0.29675189]]\n",
      "Iter 17\n",
      "Gradient [[   -535.96617263   -4477.5928951   -33364.52817365 -190279.06685889]]\n",
      "Loss 19635.221148317763\n",
      "Weights [[-6.82206873 -0.26140048  0.41014172  0.29179501]]\n",
      "Iter 18\n",
      "Gradient [[  -2342.84501204  -10426.15464629  -53113.04892147 -248059.43582253]]\n",
      "Loss 13626.778198091626\n",
      "Weights [[-6.76847211 -0.21662455  0.44350625  0.31082292]]\n",
      "Iter 19\n",
      "Gradient [[-8.20387818e+01 -2.05572832e+03 -1.86973490e+04 -1.00435680e+05]]\n",
      "Loss 12974.190779760629\n",
      "Weights [[-6.53418761 -0.112363    0.4966193   0.33562886]]\n",
      "Iter 20\n",
      "Gradient [[ 3122.54064408  9701.79123032 26658.50082995 73920.82639046]]\n",
      "Loss 14720.70400532624\n",
      "Weights [[-6.52598373 -0.09180572  0.51531665  0.34567243]]\n",
      "Iter 21\n",
      "Gradient [[  3862.14475874  12792.81972767  37578.66729028 105060.88517167]]\n",
      "Loss 11477.91002473774\n",
      "Weights [[-6.8382378  -0.18882363  0.48865815  0.33828035]]\n",
      "Iter 22\n",
      "Gradient [[ 1769.49842973  5830.49038642 10517.93200455 -6753.09295279]]\n",
      "Loss 11518.363947145375\n",
      "Weights [[-7.22445227 -0.31675183  0.45107948  0.32777426]]\n",
      "Iter 23\n",
      "Gradient [[   -719.70503217   -2643.24346444  -20943.96206786 -124284.41606503]]\n",
      "Loss 12032.520982951435\n",
      "Weights [[-7.40140212 -0.37505673  0.44056155  0.32844957]]\n",
      "Iter 24\n",
      "Gradient [[  -1249.87601097   -4284.316353    -25536.82623947 -131615.75756046]]\n",
      "Loss 10138.363147337363\n",
      "Weights [[-7.32943161 -0.3486243   0.46150551  0.34087801]]\n",
      "Iter 25\n",
      "Gradient [[   207.49512727   1080.55919032  -3870.38706444 -41281.13237247]]\n",
      "Loss 10312.528314125457\n",
      "Weights [[-7.20444401 -0.30578113  0.48704234  0.35403959]]\n",
      "Iter 26\n",
      "Gradient [[ 1776.76439564  6871.37324765 18335.15763457 42692.80597877]]\n",
      "Loss 10445.928836601723\n",
      "Weights [[-7.22519352 -0.31658672  0.49091272  0.3581677 ]]\n",
      "Iter 27\n",
      "Gradient [[ 1847.18462242  7380.85990901 19812.52212749 42412.75901656]]\n",
      "Loss 9463.417045803553\n",
      "Weights [[-7.40286996 -0.38530046  0.47257757  0.35389842]]\n",
      "Iter 28\n",
      "Gradient [[   567.10663808   3083.8904881    3302.69397532 -24155.44771209]]\n",
      "Loss 9604.276738796145\n",
      "Weights [[-7.58758843 -0.45910906  0.45276504  0.34965714]]\n",
      "Iter 29\n",
      "Gradient [[  -640.29030424  -1017.72845491 -11730.7257906  -79011.34395482]]\n",
      "Loss 9502.53543446349\n",
      "Weights [[-7.64429909 -0.48994796  0.44946235  0.35207269]]\n",
      "Iter 30\n",
      "Gradient [[  -691.16719038  -1075.43835491 -11028.72327493 -70633.5205829 ]]\n",
      "Loss 8931.82437613652\n",
      "Weights [[-7.58027006 -0.47977068  0.46119308  0.35997382]]\n",
      "Iter 31\n",
      "Gradient [[   183.66351175   2134.58177873   1784.48972451 -18296.91876233]]\n",
      "Loss 9012.789866424291\n",
      "Weights [[-7.51115334 -0.46901629  0.4722218   0.36703717]]\n",
      "Iter 32\n",
      "Gradient [[  914.72104832  4853.04660553 12149.82718372 20169.67971679]]\n",
      "Loss 8897.210992776989\n",
      "Weights [[-7.52951969 -0.49036211  0.47043731  0.36886687]]\n",
      "Iter 33\n",
      "Gradient [[  782.73538142  4524.76041617 10671.98344506 11385.76515267]]\n",
      "Loss 8592.781058857536\n",
      "Weights [[-7.6209918  -0.53889258  0.45828748  0.3668499 ]]\n",
      "Iter 34\n",
      "Gradient [[    40.81129825   2017.89887134   1138.75784955 -26268.02266966]]\n",
      "Loss 8609.134665410904\n",
      "Weights [[-7.69926533 -0.58414018  0.4476155   0.36571132]]\n",
      "Iter 35\n",
      "Gradient [[  -517.52668792    126.41025684  -5671.00684792 -50326.56382398]]\n",
      "Loss 8467.803360486281\n",
      "Weights [[-7.70334646 -0.60431917  0.44647674  0.36833812]]\n",
      "Iter 36\n",
      "Gradient [[  -425.3074357     514.05227789  -3690.42181399 -39672.37469166]]\n",
      "Loss 8273.214526840813\n",
      "Weights [[-7.65159379 -0.60558327  0.45214775  0.37337078]]\n",
      "Iter 37\n",
      "Gradient [[    72.57245043   2336.08118386   3525.4489464  -10663.71058422]]\n",
      "Loss 8256.087038261294\n",
      "Weights [[-7.60906305 -0.61072379  0.45583817  0.37733802]]\n",
      "Iter 38\n",
      "Gradient [[ 392.20072194 3536.48348866 8079.05291093 5842.26350711]]\n",
      "Loss 8144.150671711737\n",
      "Weights [[-7.6163203  -0.63408461  0.45231272  0.37840439]]\n",
      "Iter 39\n",
      "Gradient [[  236.69197027  3056.52450279  6154.89184965 -3070.41759162]]\n",
      "Loss 8020.5106132975725\n",
      "Weights [[-7.65554037 -0.66944944  0.44423367  0.37782016]]\n",
      "Iter 40\n",
      "Gradient [[  -174.70132405   1657.61314207    891.42764373 -23451.73501418]]\n",
      "Loss 7974.345288518756\n",
      "Weights [[-7.67920957 -0.70001469  0.43807878  0.3781272 ]]\n",
      "Iter 41\n",
      "Gradient [[  -417.39258416    837.21652564  -1979.99918938 -33084.17846845]]\n",
      "Loss 7867.207299809429\n",
      "Weights [[-7.66173943 -0.71659082  0.43718735  0.38047238]]\n",
      "Iter 42\n",
      "Gradient [[  -309.10030512   1248.3744824    -139.83899631 -24412.03227747]]\n",
      "Loss 7769.954417146837\n",
      "Weights [[-7.62000017 -0.72496298  0.43916735  0.3837808 ]]\n",
      "Iter 43\n",
      "Gradient [[  -38.04167661  2236.39060869  3754.81044365 -8948.75476048]]\n",
      "Loss 7715.015195850616\n",
      "Weights [[-7.58909014 -0.73744673  0.43930719  0.386222  ]]\n",
      "Iter 44\n",
      "Gradient [[   89.44074471  2720.84748165  5586.50861103 -2522.95950615]]\n",
      "Loss 7627.554035255912\n",
      "Weights [[-7.58528598 -0.75981063  0.43555238  0.38711687]]\n",
      "Iter 45\n",
      "Gradient [[  -34.26809497  2315.29760562  4013.84030517 -9243.06924453]]\n",
      "Loss 7548.3461685332495\n",
      "Weights [[-7.59423005 -0.78701911  0.42996587  0.38736917]]\n",
      "Iter 46\n",
      "Gradient [[  -253.41743011   1563.60035447   1221.16792514 -19823.53289034]]\n",
      "Loss 7484.203010613874\n",
      "Weights [[-7.59080324 -0.81017208  0.42595203  0.38829348]]\n",
      "Iter 47\n",
      "Gradient [[  -349.61369317   1237.31282356    138.10806264 -23094.47853261]]\n",
      "Loss 7401.590940892127\n",
      "Weights [[-7.5654615  -0.82580809  0.42473086  0.39027583]]\n",
      "Iter 48\n",
      "Gradient [[  -263.74681498   1550.74098638   1482.71530541 -17088.62295786]]\n",
      "Loss 7328.935318996228\n",
      "Weights [[-7.53050013 -0.83818122  0.42459275  0.39258528]]\n",
      "Iter 49\n",
      "Gradient [[ -121.95287368  2063.19350386  3501.19931043 -9143.61321333]]\n",
      "Loss 7265.181462200734\n",
      "Weights [[-7.50412545 -0.85368863  0.42311004  0.39429414]]\n",
      "Iter 50\n",
      "Gradient [[  -78.69031513  2229.01487108  4131.14610795 -7051.65636295]]\n",
      "Loss 7192.498489055199\n",
      "Weights [[-7.49193016 -0.87432056  0.41960884  0.3952085 ]]\n",
      "Iter 51\n",
      "Gradient [[  -163.0477129    1941.73090972   3041.51925462 -11489.1652883 ]]\n",
      "Loss 7124.978000021465\n",
      "Weights [[-7.48406113 -0.89661071  0.41547769  0.39591367]]\n",
      "Iter 52\n",
      "Gradient [[  -275.20663676   1551.05089525   1614.87150929 -16745.9665284 ]]\n",
      "Loss 7059.4393953992\n",
      "Weights [[-7.46775636 -0.91602802  0.41243617  0.39706258]]\n",
      "Iter 53\n",
      "Gradient [[  -307.33808987   1438.62673222   1287.89172612 -17444.54674627]]\n",
      "Loss 6989.616445809575\n",
      "Weights [[-7.44023569 -0.93153853  0.4108213   0.39873718]]\n",
      "Iter 54\n",
      "Gradient [[  -248.73865039   1645.41907202   2161.76697358 -13637.05068486]]\n",
      "Loss 6924.734188775328\n",
      "Weights [[-7.40950188 -0.9459248   0.40953341  0.40048164]]\n",
      "Iter 55\n",
      "Gradient [[ -177.32912374  1898.4027146   3164.1350098  -9701.32676507]]\n",
      "Loss 6861.718893382231\n",
      "Weights [[-7.38462802 -0.96237899  0.40737164  0.40184534]]\n",
      "Iter 56\n",
      "Gradient [[ -167.71436071  1933.53316069  3306.26479959 -9298.32897371]]\n",
      "Loss 6796.951262929544\n",
      "Weights [[-7.36689511 -0.98136301  0.40420751  0.40281547]]\n",
      "Iter 57\n",
      "Gradient [[  -220.25065752   1747.46499939   2616.14666148 -11995.70116276]]\n",
      "Loss 6734.379470117164\n",
      "Weights [[-7.35012367 -1.00069834  0.40090124  0.40374531]]\n",
      "Iter 58\n",
      "Gradient [[  -275.14494808   1550.18062972   1914.54433527 -14475.89666755]]\n",
      "Loss 6671.953946304055\n",
      "Weights [[-7.32809861 -1.01817299  0.39828509  0.40494488]]\n",
      "Iter 59\n",
      "Gradient [[  -281.5611875    1522.00635031   1873.37935215 -14279.96160951]]\n",
      "Loss 6609.174119390927\n",
      "Weights [[-7.30058411 -1.0336748   0.39637055  0.40639247]]\n",
      "Iter 60\n",
      "Gradient [[  -244.7230023    1646.22096851   2399.59918315 -12001.10992241]]\n",
      "Loss 6548.631342233238\n",
      "Weights [[-7.27242799 -1.04889486  0.39449717  0.40782046]]\n",
      "Iter 61\n",
      "Gradient [[  -210.05986321   1763.29887301   2872.91823327 -10122.79876832]]\n",
      "Loss 6488.776877643299\n",
      "Weights [[-7.24795569 -1.06535707  0.39209757  0.40902057]]\n",
      "Iter 62\n",
      "Gradient [[  -211.82576164   1751.73301882   2843.61065667 -10267.48101373]]\n",
      "Loss 6429.006906930959\n",
      "Weights [[-7.22694971 -1.08299006  0.38922465  0.41003285]]\n",
      "Iter 63\n",
      "Gradient [[  -242.40784609   1637.29551259   2431.23923043 -11805.51589273]]\n",
      "Loss 6370.2170356016895\n",
      "Weights [[-7.20576713 -1.10050739  0.38638104  0.4110596 ]]\n",
      "Iter 64\n",
      "Gradient [[  -267.77776043   1539.71936785   2099.52281806 -12895.962025  ]]\n",
      "Loss 6311.6590898028\n",
      "Weights [[-7.18152634 -1.11688035  0.3839498   0.41224015]]\n",
      "Iter 65\n",
      "Gradient [[  -265.30415964   1539.51572074   2144.45625221 -12488.46481616]]\n",
      "Loss 6253.6230000267315\n",
      "Weights [[-7.15474857 -1.13227754  0.38185028  0.41352975]]\n",
      "Iter 66\n",
      "Gradient [[  -243.28912356   1608.27337098   2442.71018336 -11177.98429014]]\n",
      "Loss 6196.742866141687\n",
      "Weights [[-7.12821815 -1.1476727   0.37970582  0.4147786 ]]\n",
      "Iter 67\n",
      "Gradient [[  -227.01717558   1657.04470343   2651.38198146 -10314.25119291]]\n",
      "Loss 6140.418878711146\n",
      "Weights [[-7.10388924 -1.16375543  0.37726311  0.41589639]]\n",
      "Iter 68\n",
      "Gradient [[  -231.20432528   1633.46658854   2579.80498477 -10559.34138659]]\n",
      "Loss 6084.561292829043\n",
      "Weights [[-7.08118752 -1.18032588  0.37461173  0.41692782]]\n",
      "Iter 69\n",
      "Gradient [[  -247.93438422   1564.92634756   2343.1022439  -11385.16094239]]\n",
      "Loss 6029.312804421572\n",
      "Weights [[-7.05806709 -1.19666055  0.37203193  0.41798375]]\n",
      "Iter 70\n",
      "Gradient [[  -258.6609271    1516.79492036   2192.39261827 -11811.00262261]]\n",
      "Loss 5974.478911567729\n",
      "Weights [[-7.03327365 -1.21230981  0.36968883  0.41912227]]\n",
      "Iter 71\n",
      "Gradient [[  -254.08595972   1522.29477648   2247.36047901 -11432.61976507]]\n",
      "Loss 5920.276657422883\n",
      "Weights [[-7.00740756 -1.22747776  0.36749643  0.42030337]]\n",
      "Iter 72\n",
      "Gradient [[  -241.30380943   1556.83210352   2406.86442751 -10698.34112087]]\n",
      "Loss 5866.832176921653\n",
      "Weights [[-6.98199896 -1.24270071  0.36524907  0.42144663]]\n",
      "Iter 73\n",
      "Gradient [[  -233.82150509   1572.8126193    2488.87843336 -10313.15946018]]\n",
      "Loss 5813.929231901031\n",
      "Weights [[-6.95786858 -1.25826903  0.36284221  0.42251647]]\n",
      "Iter 74\n",
      "Gradient [[  -237.35707855   1549.78769793   2420.94558568 -10511.98921224]]\n",
      "Loss 5761.539968971216\n",
      "Weights [[-6.93448643 -1.27399715  0.36035333  0.42354778]]\n",
      "Iter 75\n",
      "Gradient [[  -245.86869827   1508.81603422   2288.50760348 -10925.15808087]]\n",
      "Loss 5709.6658313511125\n",
      "Weights [[-6.91075072 -1.28949503  0.35793238  0.42459898]]\n",
      "Iter 76\n",
      "Gradient [[  -249.64291106   1484.19245379   2222.27308694 -11050.41780643]]\n",
      "Loss 5658.280872625066\n",
      "Weights [[-6.88616385 -1.30458319  0.35564388  0.4256915 ]]\n",
      "Iter 77\n",
      "Gradient [[  -245.33859586   1487.95241042   2263.96825499 -10760.71159029]]\n",
      "Loss 5607.486406219212\n",
      "Weights [[-6.86119956 -1.31942512  0.3534216   0.42679654]]\n",
      "Iter 78\n",
      "Gradient [[  -237.96492888   1502.62987252   2343.54590557 -10353.29035675]]\n",
      "Loss 5557.2937536454665\n",
      "Weights [[-6.8366657  -1.33430464  0.35115763  0.42787261]]\n",
      "Iter 79\n",
      "Gradient [[  -234.45866293   1503.74233124   2368.11889311 -10181.96518485]]\n",
      "Loss 5507.620969280122\n",
      "Weights [[-6.81286921 -1.34933094  0.34881409  0.42890794]]\n",
      "Iter 80\n",
      "Gradient [[  -236.65584755   1484.65190152   2315.57368995 -10306.38243573]]\n",
      "Loss 5458.446760261322\n",
      "Weights [[-6.78942334 -1.36436836  0.34644597  0.42992613]]\n",
      "Iter 81\n",
      "Gradient [[  -240.52283904   1459.4742688    2242.08458195 -10491.72672149]]\n",
      "Loss 5409.760504273649\n",
      "Weights [[-6.76575776 -1.37921488  0.3441304   0.43095677]]\n",
      "Iter 82\n",
      "Gradient [[  -241.17109924   1445.4997267    2213.0110806  -10491.18128528]]\n",
      "Loss 5361.571415715882\n",
      "Weights [[-6.74170548 -1.39380962  0.34188831  0.43200595]]\n",
      "Iter 83\n",
      "Gradient [[  -237.71986617   1445.9756496    2238.37680715 -10285.14198304]]\n",
      "Loss 5313.918572588764\n",
      "Weights [[-6.71758837 -1.40826462  0.3396753   0.43305506]]\n",
      "Iter 84\n",
      "Gradient [[  -233.36791521   1449.70965937   2273.98379872 -10055.91758582]]\n",
      "Loss 5266.792712287829\n",
      "Weights [[-6.69381638 -1.42272438  0.33743692  0.43408358]]\n",
      "Iter 85\n",
      "Gradient [[ -231.5346018   1444.61257404  2274.3645325  -9973.49877239]]\n",
      "Loss 5220.15791802219\n",
      "Weights [[-6.67047959 -1.43722147  0.33516294  0.43508917]]\n",
      "Iter 86\n",
      "Gradient [[  -232.53718009   1429.47626125   2236.6837522  -10034.70136876]]\n",
      "Loss 5173.998127372065\n",
      "Weights [[-6.64732613 -1.4516676   0.33288858  0.43608652]]\n",
      "Iter 87\n",
      "Gradient [[  -233.89030927   1413.0215532    2195.19801706 -10099.76065344]]\n",
      "Loss 5128.307455115515\n",
      "Weights [[-6.62407241 -1.46596236  0.33065189  0.43708999]]\n",
      "Iter 88\n",
      "Gradient [[  -233.23782269   1403.5988247    2181.17986102 -10052.95497861]]\n",
      "Loss 5083.095370233507\n",
      "Weights [[-6.60068338 -1.48009258  0.32845669  0.43809997]]\n",
      "Iter 89\n",
      "Gradient [[ -230.59174306  1401.24162393  2193.47777514 -9909.00284347]]\n",
      "Loss 5038.374081579814\n",
      "Weights [[-6.5773596  -1.49412857  0.32627551  0.43910526]]\n",
      "Iter 90\n",
      "Gradient [[ -227.86337668  1399.24580612  2205.91290756 -9773.60411478]]\n",
      "Loss 4994.133177973132\n",
      "Weights [[-6.55430042 -1.50814098  0.32408204  0.44009616]]\n",
      "Iter 91\n",
      "Gradient [[ -226.66178837  1391.91095728  2197.26808668 -9724.29108691]]\n",
      "Loss 4950.353895497687\n",
      "Weights [[-6.53151408 -1.52213344  0.32187612  0.44107352]]\n",
      "Iter 92\n",
      "Gradient [[ -226.80919643  1379.81709612  2170.69769164 -9741.42935213]]\n",
      "Loss 4907.02532860855\n",
      "Weights [[-6.50884791 -1.53605255  0.31967886  0.44204595]]\n",
      "Iter 93\n",
      "Gradient [[ -226.85954534  1368.04880121  2145.97581531 -9745.88454515]]\n",
      "Loss 4864.144442509538\n",
      "Weights [[-6.48616699 -1.54985072  0.31750816  0.44302009]]\n",
      "Iter 94\n",
      "Gradient [[ -225.73396814  1360.43928193  2137.29194539 -9686.50885278]]\n",
      "Loss 4821.715581754877\n",
      "Weights [[-6.46348103 -1.56353121  0.31536218  0.44399468]]\n",
      "Iter 95\n",
      "Gradient [[ -223.68646793  1356.13207069  2140.71365884 -9583.79622668]]\n",
      "Loss 4779.740373774175\n",
      "Weights [[-6.44090763 -1.5771356   0.31322489  0.44496333]]\n",
      "Iter 96\n",
      "Gradient [[ -221.80262864  1351.31178115  2141.47793837 -9496.29547427]]\n",
      "Loss 4738.210220046807\n",
      "Weights [[-6.41853899 -1.59069692  0.31108418  0.44592171]]\n",
      "Iter 97\n",
      "Gradient [[ -220.79709311  1343.43995965  2130.26607638 -9457.0043214 ]]\n",
      "Loss 4697.1137761904165\n",
      "Weights [[-6.39635872 -1.60421004  0.3089427   0.44687134]]\n",
      "Iter 98\n",
      "Gradient [[ -220.40025961  1333.44368524  2111.09566679 -9446.62928779]]\n",
      "Loss 4656.443691909547\n",
      "Weights [[-6.37427902 -1.61764444  0.30681243  0.44781704]]\n",
      "Iter 99\n",
      "Gradient [[ -219.80873493  1324.15278994  2094.87875069 -9422.233595  ]]\n",
      "Loss 4616.197549346647\n",
      "Weights [[-6.35223899 -1.63097888  0.30470134  0.44876171]]\n",
      "Iter 100\n",
      "Gradient [[ -218.56081883  1317.21347872  2087.61013017 -9363.0292506 ]]\n",
      "Loss 4576.375774105885\n",
      "Weights [[-6.33025812 -1.6442204   0.30260646  0.44970393]]\n",
      "Iter 101\n",
      "Gradient [[ -216.91411213  1311.73682336  2085.56677304 -9286.0267476 ]]\n",
      "Loss 4536.97608964223\n",
      "Weights [[-6.30840203 -1.65739254  0.30051885  0.45064023]]\n",
      "Iter 102\n",
      "Gradient [[ -215.45928677  1305.64318886  2080.76745607 -9222.36326837]]\n",
      "Loss 4497.991616773012\n",
      "Weights [[-6.28671062 -1.67050991  0.29843328  0.45156883]]\n",
      "Iter 103\n",
      "Gradient [[ -214.48893977  1297.88910217  2069.46156526 -9184.66018398]]\n",
      "Loss 4459.414683635519\n",
      "Weights [[-6.26516469 -1.68356634  0.29635251  0.45249107]]\n",
      "Iter 104\n",
      "Gradient [[ -213.77495041  1289.26757262  2054.91013302 -9158.48719369]]\n",
      "Loss 4421.239946822058\n",
      "Weights [[-6.2437158  -1.69654523  0.29428305  0.45340954]]\n",
      "Iter 105\n",
      "Gradient [[ -212.88887264  1281.29016372  2042.89608527 -9121.45297098]]\n",
      "Loss 4383.464772139883\n",
      "Weights [[-6.22233831 -1.70943791  0.29222814  0.45432539]]\n",
      "Iter 106\n",
      "Gradient [[ -211.65247476  1274.59604627  2035.68396118 -9066.2372966 ]]\n",
      "Loss 4346.087394127983\n",
      "Weights [[-6.20104942 -1.72225081  0.29018525  0.45523753]]\n",
      "Iter 107\n",
      "Gradient [[ -210.26065986  1268.50711737  2030.52865146 -9004.68870446]]\n",
      "Loss 4309.104252594021\n",
      "Weights [[-6.17988417 -1.73499677  0.28814956  0.45614415]]\n",
      "Iter 108\n",
      "Gradient [[ -209.02229455  1261.93403821  2023.28267643 -8952.72548169]]\n",
      "Loss 4272.5097310718875\n",
      "Weights [[-6.1588581  -1.74768184  0.28611903  0.45704462]]\n",
      "Iter 109\n",
      "Gradient [[ -208.04185707  1254.50139373  2012.65020166 -8914.25809549]]\n",
      "Loss 4236.298127709582\n",
      "Weights [[-6.13795587 -1.76030118  0.28409575  0.4579399 ]]\n",
      "Iter 110\n",
      "Gradient [[ -207.15901169  1246.76929156  2000.87839191 -8879.75774289]]\n",
      "Loss 4200.465129012762\n",
      "Weights [[-6.11715169 -1.77284619  0.2820831   0.45883132]]\n",
      "Iter 111\n",
      "Gradient [[ -206.1541055   1239.51387447  1990.91325714 -8838.00504593]]\n",
      "Loss 4165.007786507032\n",
      "Weights [[-6.09643579 -1.78531389  0.28008222  0.4597193 ]]\n",
      "Iter 112\n",
      "Gradient [[ -204.97073898  1232.93997107  1983.43607769 -8787.18384327]]\n",
      "Loss 4129.923299719446\n",
      "Weights [[-6.07582038 -1.79770902  0.27809131  0.4606031 ]]\n",
      "Iter 113\n",
      "Gradient [[ -203.73842667  1226.59492321  1976.64869675 -8734.79664943]]\n",
      "Loss 4095.207828559385\n",
      "Weights [[-6.0553233  -1.81003842  0.27610787  0.46148182]]\n",
      "Iter 114\n",
      "Gradient [[ -202.61236139  1219.93034542  1968.48075055 -8688.65060948]]\n",
      "Loss 4060.856640727495\n",
      "Weights [[-6.03494946 -1.82230437  0.27413122  0.4623553 ]]\n",
      "Iter 115\n",
      "Gradient [[ -201.61958244  1212.84686145  1958.63016742 -8649.2992952 ]]\n",
      "Loss 4026.865142337008\n",
      "Weights [[-6.01468822 -1.83450368  0.27216274  0.46322416]]\n",
      "Iter 116\n",
      "Gradient [[ -200.65799084  1205.7021538   1948.51032763 -8610.91411495]]\n",
      "Loss 3993.2295487392826\n",
      "Weights [[-5.99452627 -1.84663215  0.27020411  0.46408909]]\n",
      "Iter 117\n",
      "Gradient [[ -199.61951847  1198.87867147  1939.56055476 -8568.11406354]]\n",
      "Loss 3959.9467232330294\n",
      "Weights [[-5.97446047 -1.85868917  0.2682556   0.46495018]]\n",
      "Iter 118\n",
      "Gradient [[ -198.49513025  1192.41140039  1931.85930523 -8521.04157449]]\n",
      "Loss 3927.013470204247\n",
      "Weights [[-5.95449852 -1.87067795  0.26631604  0.46580699]]\n",
      "Iter 119\n",
      "Gradient [[ -197.36437797  1186.02140059  1924.31274132 -8474.15159072]]\n",
      "Loss 3894.426037553364\n",
      "Weights [[-5.934649   -1.88260207  0.26438418  0.4666591 ]]\n",
      "Iter 120\n",
      "Gradient [[ -196.30192334  1179.44422644  1915.93215579 -8431.09549661]]\n",
      "Loss 3862.1803122118317\n",
      "Weights [[-5.91491256 -1.89446228  0.26245987  0.46750651]]\n",
      "Iter 121\n",
      "Gradient [[ -195.30687325  1172.6807519   1906.76273226 -8391.36876554]]\n",
      "Loss 3830.272355747761\n",
      "Weights [[-5.89528237 -1.90625672  0.26054394  0.46834962]]\n",
      "Iter 122\n",
      "Gradient [[ -194.31834487  1165.94464722  1897.63961654 -8351.59865331]]\n",
      "Loss 3798.698688606686\n",
      "Weights [[-5.87575169 -1.91798353  0.25863717  0.46918876]]\n",
      "Iter 123\n",
      "Gradient [[ -193.28559666  1159.41575578  1889.23288724 -8309.33590115]]\n",
      "Loss 3767.4561273753525\n",
      "Weights [[-5.85631985 -1.92964298  0.25673954  0.47002392]]\n",
      "Iter 124\n",
      "Gradient [[ -192.21481669  1153.07377651  1881.43618209 -8265.22470418]]\n",
      "Loss 3736.541404282135\n",
      "Weights [[-5.83699129 -1.94123714  0.2548503   0.47085485]]\n",
      "Iter 125\n",
      "Gradient [[ -191.15217723  1146.75634867  1873.61927764 -8221.78218187]]\n",
      "Loss 3705.950974405569\n",
      "Weights [[-5.81776981 -1.95276787  0.25296887  0.47168138]]\n",
      "Iter 126\n",
      "Gradient [[ -190.13178357  1140.34225944  1865.33460167 -8180.61657163]]\n",
      "Loss 3675.681169014685\n",
      "Weights [[-5.79865459 -1.96423544  0.25109525  0.47250355]]\n",
      "Iter 127\n",
      "Gradient [[ -189.1452961   1133.85962499  1856.71137839 -8141.0575209 ]]\n",
      "Loss 3645.7284681951223\n",
      "Weights [[-5.77964141 -1.97563886  0.24922991  0.47332162]]\n",
      "Iter 128\n",
      "Gradient [[ -188.15818704  1127.42973664  1848.21958984 -8101.23959873]]\n",
      "Loss 3616.08960836761\n",
      "Weights [[-5.76072688 -1.98697746  0.2473732   0.47413572]]\n",
      "Iter 129\n",
      "Gradient [[ -187.1479819   1121.13234902  1840.15256387 -8060.12308619]]\n",
      "Loss 3586.7614606623465\n",
      "Weights [[-5.74191106 -1.99825175  0.24552498  0.47494585]]\n",
      "Iter 130\n",
      "Gradient [[ -186.12350981  1114.93697155  1832.37948534 -8018.33025912]]\n",
      "Loss 3557.7408367953644\n",
      "Weights [[-5.72319627 -2.00946308  0.24368483  0.47575186]]\n",
      "Iter 131\n",
      "Gradient [[ -185.1103649   1108.75332597  1824.55308487 -7977.2239983 ]]\n",
      "Loss 3529.0244236764247\n",
      "Weights [[-5.70458392 -2.02061245  0.24185245  0.47655369]]\n",
      "Iter 132\n",
      "Gradient [[ -184.12308895  1102.52942032  1816.48485023 -7937.45413218]]\n",
      "Loss 3500.608885355313\n",
      "Weights [[-5.68607288 -2.03169998  0.2400279   0.47735141]]\n",
      "Iter 133\n",
      "Gradient [[ -183.15335615  1096.29385115  1808.29506014 -7898.47028107]]\n",
      "Loss 3472.490997570852\n",
      "Weights [[-5.66766057 -2.04272527  0.23821141  0.47814516]]\n",
      "Iter 134\n",
      "Gradient [[ -182.18240013  1090.11253766  1800.23727021 -7859.28145957]]\n",
      "Loss 3444.667680471798\n",
      "Weights [[-5.64934523 -2.05368821  0.23640312  0.47893501]]\n",
      "Iter 135\n",
      "Gradient [[ -181.20105213  1084.01801747  1792.42888073 -7819.49141956]]\n",
      "Loss 3417.135919446569\n",
      "Weights [[-5.63112699 -2.06458934  0.23460288  0.47972093]]\n",
      "Iter 136\n",
      "Gradient [[ -180.21667724  1077.98450306  1784.76466275 -7779.56229634]]\n",
      "Loss 3389.8926704356772\n",
      "Weights [[-5.61300689 -2.07542952  0.23281045  0.48050288]]\n",
      "Iter 137\n",
      "Gradient [[ -179.24289076  1071.96377137  1777.0607814  -7740.2030929 ]]\n",
      "Loss 3362.934843881467\n",
      "Weights [[-5.59498522 -2.08620936  0.23102569  0.48128084]]\n",
      "Iter 138\n",
      "Gradient [[ -178.28526658  1065.93563466  1769.24635189 -7701.6397848 ]]\n",
      "Loss 3336.2593667437227\n",
      "Weights [[-5.57706093 -2.096929    0.22924862  0.48205486]]\n",
      "Iter 139\n",
      "Gradient [[ -177.33753556  1059.92171117  1761.40941115 -7663.49072127]]\n",
      "Loss 3309.863246359774\n",
      "Weights [[-5.55923241 -2.10758836  0.22747938  0.48282502]]\n",
      "Iter 140\n",
      "Gradient [[ -176.38988939  1053.95639217  1753.68140289 -7625.24955993]]\n",
      "Loss 3283.7435749213323\n",
      "Weights [[-5.54149865 -2.11818757  0.22571797  0.48359137]]\n",
      "Iter 141\n",
      "Gradient [[ -175.43907069  1048.05117832  1746.10238978 -7586.79428744]]\n",
      "Loss 3257.897482036403\n",
      "Weights [[-5.52385966 -2.12872714  0.22396429  0.4843539 ]]\n",
      "Iter 142\n",
      "Gradient [[ -174.49021622  1042.18790924  1738.60013091 -7548.42858934]]\n",
      "Loss 3232.32209052196\n",
      "Weights [[-5.50631576 -2.13920765  0.22221819  0.48511258]]\n",
      "Iter 143\n",
      "Gradient [[ -173.55028304  1036.34178317  1731.08103044 -7510.50653746]]\n",
      "Loss 3207.014517043497\n",
      "Weights [[-5.48886673 -2.14962953  0.22047959  0.48586742]]\n",
      "Iter 144\n",
      "Gradient [[ -172.62100104  1030.50629005  1723.5239499  -7473.08170804]]\n",
      "Loss 3181.9719075852827\n",
      "Weights [[-5.47151171 -2.15999295  0.2187485   0.48661847]]\n",
      "Iter 145\n",
      "Gradient [[ -171.69820958  1024.69575045  1715.98626638 -7435.91284902]]\n",
      "Loss 3157.1914663073135\n",
      "Weights [[-5.45424961 -2.17029801  0.21702498  0.48736578]]\n",
      "Iter 146\n",
      "Gradient [[ -170.77697714  1018.92736548  1708.53337779 -7398.7515124 ]]\n",
      "Loss 3132.670451412476\n",
      "Weights [[-5.43707979 -2.18054497  0.21530899  0.48810937]]\n",
      "Iter 147\n",
      "Gradient [[ -169.85646763  1013.20398344  1701.17417783 -7361.58083551]]\n",
      "Loss 3108.4061483040286\n",
      "Weights [[-5.42000209 -2.19073424  0.21360046  0.48884925]]\n",
      "Iter 148\n",
      "Gradient [[ -168.93993073  1007.51398882  1693.86328908 -7324.58570564]]\n",
      "Loss 3084.3958499163664\n",
      "Weights [[-5.40301644 -2.20086628  0.21189929  0.4895854 ]]\n",
      "Iter 149\n",
      "Gradient [[ -168.03077113  1001.84509978  1686.55496488 -7287.93503983]]\n",
      "Loss 3060.6368613791497\n",
      "Weights [[-5.38612245 -2.21094142  0.21020542  0.49031786]]\n",
      "Iter 150\n",
      "Gradient [[ -167.12923077   996.19613088  1679.24689296 -7251.62163167]]\n",
      "Loss 3037.1265193075055\n",
      "Weights [[-5.36931937 -2.22095987  0.20851887  0.49104666]]\n",
      "Iter 151\n",
      "Gradient [[ -166.23275043   990.57582305  1671.97380629 -7215.50215302]]\n",
      "Loss 3013.8622039995807\n",
      "Weights [[-5.35260645 -2.23092183  0.20683962  0.49177182]]\n",
      "Iter 152\n",
      "Gradient [[ -165.33895005   984.99237195  1664.76676846 -7179.46009286]]\n",
      "Loss 2990.8413340806474\n",
      "Weights [[-5.33598317 -2.24082759  0.20516765  0.49249337]]\n",
      "Iter 153\n",
      "Gradient [[ -164.44785883   979.44551152  1657.62387726 -7143.51136615]]\n",
      "Loss 2968.0613519569074\n",
      "Weights [[-5.31944928 -2.25067752  0.20350288  0.49321131]]\n",
      "Iter 154\n",
      "Gradient [[ -163.56139962   973.92825728  1650.51829793 -7107.76232157]]\n",
      "Loss 2945.519715557531\n",
      "Weights [[-5.30300449 -2.26047197  0.20184526  0.49392566]]\n",
      "Iter 155\n",
      "Gradient [[ -162.68115516   968.43476246  1643.42867496 -7072.28908417]]\n",
      "Loss 2923.2139027396497\n",
      "Weights [[-5.28664835 -2.27021125  0.20019474  0.49463644]]\n",
      "Iter 156\n",
      "Gradient [[ -161.80689375   962.96555946  1636.35838657 -7037.06932193]]\n",
      "Loss 2901.1414212949526\n",
      "Weights [[-5.27038024 -2.2798956   0.19855131  0.49534367]]\n",
      "Iter 157\n",
      "Gradient [[ -160.93712265   957.52566164  1629.32732061 -7002.02177598]]\n",
      "Loss 2879.2998135865537\n",
      "Weights [[-5.25419955 -2.28952526  0.19691495  0.49604738]]\n",
      "Iter 158\n",
      "Gradient [[ -160.07074462   952.11873231  1622.34939603 -6967.09470618]]\n",
      "Loss 2857.6866522144296\n",
      "Weights [[-5.23810583 -2.29910051  0.19528562  0.49674758]]\n",
      "Iter 159\n",
      "Gradient [[ -159.20801344   946.74368147  1615.42017425 -6932.30885553]]\n",
      "Loss 2836.2995323941345\n",
      "Weights [[-5.22209876 -2.3086217   0.19366327  0.49744429]]\n",
      "Iter 160\n",
      "Gradient [[ -158.35000703   941.3964868   1608.52449721 -6897.72250534]]\n",
      "Loss 2815.136068740255\n",
      "Weights [[-5.20617796 -2.31808914  0.19204785  0.49813752]]\n",
      "Iter 161\n",
      "Gradient [[ -157.49741144   936.07447747  1601.65293605 -6863.36727727]]\n",
      "Loss 2794.1938983868768\n",
      "Weights [[-5.19034296 -2.3275031   0.19043933  0.49882729]]\n",
      "Iter 162\n",
      "Gradient [[ -156.64991685   930.77849073  1594.80955031 -6829.22198658]]\n",
      "Loss 2773.4706859189037\n",
      "Weights [[-5.17459322 -2.33686385  0.18883768  0.49951363]]\n",
      "Iter 163\n",
      "Gradient [[ -155.80668669   925.5112432   1588.00519399 -6795.24226922]]\n",
      "Loss 2752.964124781757\n",
      "Weights [[-5.15892823 -2.34617163  0.18724287  0.50019655]]\n",
      "Iter 164\n",
      "Gradient [[ -154.96724146   920.27421468  1581.24559737 -6761.40672384]]\n",
      "Loss 2732.6719342859537\n",
      "Weights [[-5.14334756 -2.35542674  0.18565486  0.50087607]]\n",
      "Iter 165\n",
      "Gradient [[ -154.13182839   915.06632444  1574.5266797  -6727.73206815]]\n",
      "Loss 2712.5918557107325\n",
      "Weights [[-5.12785083 -2.36462949  0.18407362  0.50155222]]\n",
      "Iter 166\n",
      "Gradient [[ -153.30102168   909.8853232   1567.84019046 -6694.24876747]]\n",
      "Loss 2692.721651170613\n",
      "Weights [[-5.11243765 -2.37378015  0.18249909  0.50222499]]\n",
      "Iter 167\n",
      "Gradient [[ -152.47508642   904.7300379   1561.1822605  -6660.96819794]]\n",
      "Loss 2673.0591055286095\n",
      "Weights [[-5.09710755 -2.382879    0.18093125  0.50289441]]\n",
      "Iter 168\n",
      "Gradient [[ -151.65376259   899.60114871  1554.55608949 -6627.87451005]]\n",
      "Loss 2653.6020286694006\n",
      "Weights [[-5.08186004 -2.3919263   0.17937007  0.50356051]]\n",
      "Iter 169\n",
      "Gradient [[ -150.83659514   894.50003703  1547.96732665 -6594.94421377]]\n",
      "Loss 2634.3482556498384\n",
      "Weights [[-5.06669466 -2.40092231  0.17781551  0.5042233 ]]\n",
      "Iter 170\n",
      "Gradient [[ -150.0233873    889.42718447  1541.41799644 -6562.16930697]]\n",
      "Loss 2615.295644777658\n",
      "Weights [[-5.051611   -2.40986731  0.17626754  0.50488279]]\n",
      "Iter 171\n",
      "Gradient [[ -149.21431808   884.38174688  1534.90508278 -6529.56105588]]\n",
      "Loss 2596.4420756412474\n",
      "Weights [[-5.03660866 -2.41876159  0.17472613  0.50553901]]\n",
      "Iter 172\n",
      "Gradient [[ -148.40967629   879.36248456  1528.42422778 -6497.13454998]]\n",
      "Loss 2577.7854487548793\n",
      "Weights [[-5.02168723 -2.4276054   0.17319122  0.50619197]]\n",
      "Iter 173\n",
      "Gradient [[ -147.60954076   874.36889258  1521.97400213 -6464.89258731]]\n",
      "Loss 2559.3236866046823\n",
      "Weights [[-5.00684626 -2.43639903  0.1716628   0.50684168]]\n",
      "Iter 174\n",
      "Gradient [[ -146.81372423   869.40140722  1515.5565367  -6432.82449037]]\n",
      "Loss 2541.05473458664\n",
      "Weights [[-4.99208531 -2.44514272  0.17014082  0.50748817]]\n",
      "Iter 175\n",
      "Gradient [[ -146.02198365   864.46066961  1509.17461458 -6400.91809416]]\n",
      "Loss 2522.976560739307\n",
      "Weights [[-4.97740394 -2.45383673  0.16862527  0.50813145]]\n",
      "Iter 176\n",
      "Gradient [[ -145.2342433    859.54673617  1502.82870171 -6369.17084279]]\n",
      "Loss 2505.087154550205\n",
      "Weights [[-4.96280174 -2.46248134  0.16711609  0.50877154]]\n",
      "Iter 177\n",
      "Gradient [[ -144.45061411   854.65900429  1496.51677868 -6337.5895145 ]]\n",
      "Loss 2487.3845259463087\n",
      "Weights [[-4.94827832 -2.47107681  0.16561326  0.50940846]]\n",
      "Iter 178\n",
      "Gradient [[ -143.67123009   849.7967858   1490.23659201 -6306.18104411]]\n",
      "Loss 2469.8667051811653\n",
      "Weights [[-4.93383325 -2.4796234   0.16411675  0.51004222]]\n",
      "Iter 179\n",
      "Gradient [[ -142.89609441   844.95985214  1483.9876944  -6274.94497992]]\n",
      "Loss 2452.5317433318796\n",
      "Weights [[-4.91946613 -2.48812136  0.16262651  0.51067284]]\n",
      "Iter 180\n",
      "Gradient [[ -142.12508102   840.14843324  1477.77135617 -6243.87449967]]\n",
      "Loss 2435.377712596257\n",
      "Weights [[-4.90517652 -2.49657096  0.16114252  0.51130033]]\n",
      "Iter 181\n",
      "Gradient [[ -141.3580601    835.3627772   1471.58884574 -6212.96335038]]\n",
      "Loss 2418.4027059376285\n",
      "Weights [[-4.89096401 -2.50497245  0.15966475  0.51192472]]\n",
      "Iter 182\n",
      "Gradient [[ -140.59500288   830.6027786   1465.44004626 -6182.21090074]]\n",
      "Loss 2401.604836342063\n",
      "Weights [[-4.87682821 -2.51332607  0.15819316  0.51254601]]\n",
      "Iter 183\n",
      "Gradient [[ -139.83596921   825.86801899  1459.32367115 -6151.62082157]]\n",
      "Loss 2384.9822362720306\n",
      "Weights [[-4.86276871 -2.5216321   0.15672772  0.51316424]]\n",
      "Iter 184\n",
      "Gradient [[ -139.08101255   821.15810073  1453.23856175 -6121.19589673]]\n",
      "Loss 2368.5330575968546\n",
      "Weights [[-4.84878511 -2.52989078  0.1552684   0.5137794 ]]\n",
      "Iter 185\n",
      "Gradient [[ -138.33010962   816.47289683  1447.18461131 -6090.93469652]]\n",
      "Loss 2352.255471769798\n",
      "Weights [[-4.83487701 -2.53810236  0.15381516  0.51439152]]\n",
      "Iter 186\n",
      "Gradient [[ -137.58317745   811.81249316  1441.16250074 -6060.83294592]]\n",
      "Loss 2336.147669834462\n",
      "Weights [[-4.821044   -2.54626709  0.15236797  0.51500061]]\n",
      "Iter 187\n",
      "Gradient [[ -136.84014449   807.17693823  1435.17273034 -6030.88736372]]\n",
      "Loss 2320.2078620929124\n",
      "Weights [[-4.80728568 -2.55438522  0.15092681  0.51560669]]\n",
      "Iter 188\n",
      "Gradient [[ -136.10099713   802.56607802  1429.21501341 -6001.09780762]]\n",
      "Loss 2304.4342776272447\n",
      "Weights [[-4.79360167 -2.56245699  0.14949164  0.51620978]]\n",
      "Iter 189\n",
      "Gradient [[ -135.36576158   797.97961794  1423.28854391 -5971.46599436]]\n",
      "Loss 2288.825163969745\n",
      "Weights [[-4.77999157 -2.57048265  0.14806242  0.51680989]]\n",
      "Iter 190\n",
      "Gradient [[ -134.63445122   793.41730841  1417.39271158 -5941.99268782]]\n",
      "Loss 2273.3787870171336\n",
      "Weights [[-4.76645499 -2.57846244  0.14663914  0.51740704]]\n",
      "Iter 191\n",
      "Gradient [[ -133.90703649   788.87905191  1411.52749126 -5912.67635153]]\n",
      "Loss 2258.093431033596\n",
      "Weights [[-4.75299155 -2.58639662  0.14522174  0.51800124]]\n",
      "Iter 192\n",
      "Gradient [[ -133.183462     784.36484391  1405.69319583 -5883.51427531]]\n",
      "Loss 2242.9673985358795\n",
      "Weights [[-4.73960084 -2.59428541  0.14381022  0.51859251]]\n",
      "Iter 193\n",
      "Gradient [[ -132.46368518   779.87463672  1399.88995332 -5854.50461455]]\n",
      "Loss 2227.999010010195\n",
      "Weights [[-4.7262825  -2.60212906  0.14240452  0.51918086]]\n",
      "Iter 194\n",
      "Gradient [[ -131.74769524   775.40827171  1394.11746376 -5825.64720737]]\n",
      "Loss 2213.1866035836138\n",
      "Weights [[-4.71303613 -2.6099278   0.14100463  0.51976631]]\n",
      "Iter 195\n",
      "Gradient [[ -131.03549803   770.96553196  1388.37521478 -5796.9426221 ]]\n",
      "Loss 2198.5285347932368\n",
      "Weights [[-4.69986136 -2.61768188  0.13961052  0.52034887]]\n",
      "Iter 196\n",
      "Gradient [[ -130.32708795   766.54624121  1382.66286023 -5768.39069269]]\n",
      "Loss 2184.0231764759264\n",
      "Weights [[-4.68675781 -2.62539154  0.13822214  0.52092857]]\n",
      "Iter 197\n",
      "Gradient [[ -129.62243643   762.15030524  1376.98036656 -5739.99004736]]\n",
      "Loss 2169.6689186833023\n",
      "Weights [[-4.6737251  -2.633057    0.13683948  0.52150541]]\n",
      "Iter 198\n",
      "Gradient [[ -128.92150474   757.77766717  1371.32783147 -5711.73889037]]\n",
      "Loss 2155.4641685237507\n",
      "Weights [[-4.66076286 -2.64067851  0.1354625   0.52207941]]\n",
      "Iter 199\n",
      "Gradient [[ -128.22426422   753.42823612  1365.70521261 -5683.63604333]]\n",
      "Loss 2141.4073499246333\n",
      "Weights [[-4.6478707  -2.64825628  0.13409117  0.52265058]]\n",
      "Iter 200\n",
      "Gradient [[ -127.53070297   749.10186304  1360.11224406 -5655.68119605]]\n",
      "Loss 2127.4969033873945\n",
      "Weights [[-4.63504828 -2.65579056  0.13272546  0.52321894]]\n",
      "Iter 201\n",
      "Gradient [[ -126.84081529   744.79837768  1354.54858449 -5627.87427929]]\n",
      "Loss 2113.731285802177\n",
      "Weights [[-4.62229521 -2.66328158  0.13136535  0.52378451]]\n",
      "Iter 202\n",
      "Gradient [[ -126.15458726   740.51763935  1349.01400972 -5600.21473191]]\n",
      "Loss 2100.1089703199495\n",
      "Weights [[-4.60961113 -2.67072957  0.1300108   0.5243473 ]]\n",
      "Iter 203\n",
      "Gradient [[ -125.47199316   736.25955012  1343.50845632 -5572.7013864 ]]\n",
      "Loss 2086.6284462273306\n",
      "Weights [[-4.59699567 -2.67813474  0.12866179  0.52490732]]\n",
      "Iter 204\n",
      "Gradient [[ -124.79300397   732.02402496  1338.03190314 -5545.33296274]]\n",
      "Loss 2073.288218779566\n",
      "Weights [[-4.58444847 -2.68549734  0.12731828  0.52546459]]\n",
      "Iter 205\n",
      "Gradient [[ -124.11759749   727.81095596  1332.58423607 -5518.10857849]]\n",
      "Loss 2060.0868089978185\n",
      "Weights [[-4.57196917 -2.69281758  0.12598025  0.52601912]]\n",
      "Iter 206\n",
      "Gradient [[ -123.44576006   723.62020605  1327.16522899 -5491.02778222]]\n",
      "Loss 2047.0227534715466\n",
      "Weights [[-4.55955741 -2.70009569  0.12464766  0.52657093]]\n",
      "Iter 207\n",
      "Gradient [[ -122.77747984   719.45163249  1321.77463676 -5464.09017061]]\n",
      "Loss 2034.0946041953202\n",
      "Weights [[-4.54721283 -2.70733189  0.1233205   0.52712004]]\n",
      "Iter 208\n",
      "Gradient [[ -122.1127397    715.30511192  1316.41228917 -5437.29503753]]\n",
      "Loss 2021.3009284322252\n",
      "Weights [[-4.53493508 -2.71452641  0.12199872  0.52766645]]\n",
      "Iter 209\n",
      "Gradient [[ -121.45151675   711.1805425   1311.07809552 -5410.64138572]]\n",
      "Loss 2008.6403085738443\n",
      "Weights [[-4.52272381 -2.72167946  0.12068231  0.52821018]]\n",
      "Iter 210\n",
      "Gradient [[ -120.79378747   707.07782559  1305.77197274 -5384.12821961]]\n",
      "Loss 1996.1113419777582\n",
      "Weights [[-4.51057866 -2.72879126  0.11937123  0.52875124]]\n",
      "Iter 211\n",
      "Gradient [[ -120.13953265   702.99684849  1300.49378087 -5357.75478353]]\n",
      "Loss 1983.712640790498\n",
      "Weights [[-4.49849928 -2.73586204  0.11806546  0.52928965]]\n",
      "Iter 212\n",
      "Gradient [[ -119.48873724   698.93748463  1295.24332643 -5331.52052725]]\n",
      "Loss 1971.4428317777736\n",
      "Weights [[-4.48648533 -2.74289201  0.11676497  0.52982543]]\n",
      "Iter 213\n",
      "Gradient [[ -118.8413864    694.89960762  1290.02041724 -5305.42488475]]\n",
      "Loss 1959.3005561740304\n",
      "Weights [[-4.47453645 -2.74988139  0.11546972  0.53035858]]\n",
      "Iter 214\n",
      "Gradient [[ -118.19746216   690.88310296  1284.82490617 -5279.46711483]]\n",
      "Loss 1947.2844695440324\n",
      "Weights [[-4.46265231 -2.75683038  0.1141797   0.53088912]]\n",
      "Iter 215\n",
      "Gradient [[ -117.55694383   686.88786671  1279.6566838  -5253.64634455]]\n",
      "Loss 1935.3932416407065\n",
      "Weights [[-4.45083257 -2.76373921  0.11289488  0.53141707]]\n",
      "Iter 216\n",
      "Gradient [[ -116.91981105   682.91379476  1274.51563714 -5227.96173444]]\n",
      "Loss 1923.6255562517672\n",
      "Weights [[-4.43907687 -2.77060809  0.11161522  0.53194243]]\n",
      "Iter 217\n",
      "Gradient [[ -116.28604603   678.96077502  1269.40162044 -5202.41258327]]\n",
      "Loss 1911.980111041421\n",
      "Weights [[-4.42738489 -2.77743723  0.11034071  0.53246523]]\n",
      "Iter 218\n",
      "Gradient [[ -115.65563292   675.02868927  1264.31446387 -5176.99828343]]\n",
      "Loss 1900.4556173983638\n",
      "Weights [[-4.41575629 -2.78422684  0.10907131  0.53298547]]\n",
      "Iter 219\n",
      "Gradient [[ -115.02855563   671.11742117  1259.25400438 -5151.7181988 ]]\n",
      "Loss 1889.050800294502\n",
      "Weights [[-4.40419072 -2.79097712  0.10780699  0.53350317]]\n",
      "Iter 220\n",
      "Gradient [[ -114.40479627   667.22686143  1254.22010474 -5126.57159727]]\n",
      "Loss 1877.7643981491528\n",
      "Weights [[-4.39268787 -2.7976883   0.10654774  0.53401834]]\n",
      "Iter 221\n",
      "Gradient [[ -113.78433584   663.35690578  1249.21264487 -5101.55769216]]\n",
      "Loss 1866.5951626907495\n",
      "Weights [[-4.38124739 -2.80436057  0.10529352  0.534531  ]]\n",
      "Iter 222\n",
      "Gradient [[ -113.16715583   659.50744912  1244.23149907 -5076.67573157]]\n",
      "Loss 1855.5418588135108\n",
      "Weights [[-4.36986896 -2.81099414  0.1040443   0.53504116]]\n",
      "Iter 223\n",
      "Gradient [[ -112.55323919   655.67838207  1239.27652377 -5051.92504061]]\n",
      "Loss 1844.6032644333168\n",
      "Weights [[-4.35855224 -2.81758921  0.10280007  0.53554882]]\n",
      "Iter 224\n",
      "Gradient [[ -111.94256979   651.86959292  1234.34756556 -5027.3049851 ]]\n",
      "Loss 1833.7781703483115\n",
      "Weights [[-4.34729692 -2.82414599  0.1015608   0.53605402]]\n",
      "Iter 225\n",
      "Gradient [[ -111.33513116   648.08097198  1229.44447774 -5002.81490669]]\n",
      "Loss 1823.0653801055234\n",
      "Weights [[-4.33610266 -2.83066469  0.10032645  0.53655675]]\n",
      "Iter 226\n",
      "Gradient [[ -110.73090593   644.31241365  1224.56712801 -4978.4540974 ]]\n",
      "Loss 1812.4637098702003\n",
      "Weights [[-4.32496915 -2.8371455   0.099097    0.53705703]]\n",
      "Iter 227\n",
      "Gradient [[ -110.1298763    640.56381476  1219.71539145 -4954.22183015]]\n",
      "Loss 1801.9719882939148\n",
      "Weights [[-4.31389606 -2.84358862  0.09787244  0.53755487]]\n",
      "Iter 228\n",
      "Gradient [[ -109.53202492   636.83507141  1214.88913854 -4930.11740526]]\n",
      "Loss 1791.5890563809062\n",
      "Weights [[-4.30288307 -2.84999426  0.09665272  0.5380503 ]]\n",
      "Iter 229\n",
      "Gradient [[ -108.93733527   633.12607765  1210.08823057 -4906.14016507]]\n",
      "Loss 1781.3137673552017\n",
      "Weights [[-4.29192987 -2.85636261  0.09543783  0.53854331]]\n",
      "Iter 230\n",
      "Gradient [[ -108.34579123   629.43672697  1205.31252547 -4882.28946882]]\n",
      "Loss 1771.14498653115\n",
      "Weights [[-4.28103613 -2.86269387  0.09422774  0.53903392]]\n",
      "Iter 231\n",
      "Gradient [[ -107.75737648   625.7669145   1200.56188639 -4858.56465965]]\n",
      "Loss 1761.0815911875059\n",
      "Weights [[-4.27020155 -2.86898824  0.09302243  0.53952215]]\n",
      "Iter 232\n",
      "Gradient [[ -107.17207421   622.1165378   1195.83618431 -4834.96505682]]\n",
      "Loss 1751.122470443075\n",
      "Weights [[-4.25942582 -2.87524591  0.09182187  0.54000801]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m     w_coeff_u[\u001b[32m0\u001b[39m,idx] = w + w_step\n\u001b[32m     20\u001b[39m     w_coeff_lr[\u001b[32m0\u001b[39m,idx] = w_coeff_u[\u001b[32m0\u001b[39m,idx]\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     w_grad[\u001b[32m0\u001b[39m,idx] = np.gradient([\u001b[43mloss_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_coeff\u001b[49m\u001b[43m)\u001b[49m, loss_func(X_train, y_train, w_coeff_u)], w_step)[\u001b[32m0\u001b[39m]\n\u001b[32m     22\u001b[39m     \u001b[38;5;66;03m#print(w_grad[0,idx])\u001b[39;00m\n\u001b[32m     23\u001b[39m     \u001b[38;5;66;03m#print(loss_func(X_train, y_train, w_coeff_u) - loss_func(X_train, y_train, w_coeff))\u001b[39;00m\n\u001b[32m     24\u001b[39m w_coeff = w_coeff_lr.copy()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mloss_func\u001b[39m\u001b[34m(X, y, w_coeff)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mloss_func\u001b[39m(X, y, w_coeff):\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m#print(np.sum((y - func_targ(X, w_coeff))**2))\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc_targ\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_coeff\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\git\\ITMO\\ML\\ML_2\\venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:2333\u001b[39m, in \u001b[36m_sum_dispatcher\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, initial, where)\u001b[39m\n\u001b[32m   2327\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mPassing `min` or `max` keyword argument when \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2328\u001b[39m                          \u001b[33m\"\u001b[39m\u001b[33m`a_min` and `a_max` are provided is forbidden.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2330\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[33m'\u001b[39m\u001b[33mclip\u001b[39m\u001b[33m'\u001b[39m, a_min, a_max, out=out, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m2333\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_sum_dispatcher\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   2334\u001b[39m                     initial=\u001b[38;5;28;01mNone\u001b[39;00m, where=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   2335\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, out)\n\u001b[32m   2338\u001b[39m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_sum_dispatcher)\n\u001b[32m   2339\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msum\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=np._NoValue,\n\u001b[32m   2340\u001b[39m         initial=np._NoValue, where=np._NoValue):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "max_iterations = 50000\n",
    "lr = [1e-4, 1e-5, 1e-6, 1e-7]\n",
    "#lr = [1e-4, 1e-6]\n",
    "tol = 1e-4\n",
    "# Weights.\n",
    "K = 4\n",
    "w_coeff = np.zeros((1,K))\n",
    "#w_coeff = np.array([[-5, 2]])\n",
    "#w_coeff[0,0] = -5\n",
    "#w_coeff[0,1] = 1\n",
    "\n",
    "w_grad = np.ones_like(w_coeff)\n",
    "w_coeff_lr = w_coeff.copy()\n",
    "\n",
    "for i in range(max_iterations):\n",
    "    for idx, w in enumerate(w_coeff.tolist()[0]):\n",
    "        w_coeff_u = w_coeff.copy()\n",
    "        w_step = - lr[idx] * w_grad[0,idx]\n",
    "        w_coeff_u[0,idx] = w + w_step\n",
    "        w_coeff_lr[0,idx] = w_coeff_u[0,idx]\n",
    "        w_grad[0,idx] = np.gradient([loss_func(X_train, y_train, w_coeff), loss_func(X_train, y_train, w_coeff_u)], w_step)[0]\n",
    "        #print(w_grad[0,idx])\n",
    "        #print(loss_func(X_train, y_train, w_coeff_u) - loss_func(X_train, y_train, w_coeff))\n",
    "    w_coeff = w_coeff_lr.copy()\n",
    "    print('Iter', i)\n",
    "    print('Gradient', w_grad)\n",
    "    print('Loss', loss_func(X_train, y_train, w_coeff))\n",
    "    print('Weights', w_coeff)\n",
    "    if loss_func(X_train, y_train, w_coeff) < tol:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b89dd3",
   "metadata": {},
   "source": [
    "f1(x)=x^3−3x^2+2x−5\n",
    "\n",
    "f2(x)=w3.x^3 + w2.x^2 + w1.x + w0\n",
    "\n",
    "где ожидаемые значения весов:\n",
    "\n",
    "w0 = -5\n",
    "\n",
    "w1 = 2\n",
    "\n",
    "w2 = -3\n",
    "\n",
    "w3 = 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

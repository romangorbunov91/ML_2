{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "453d2819",
   "metadata": {},
   "source": [
    "# Градиентный спуск\n",
    "## Задача поиска оптимальных коэффициентов полиномиальной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3d6c553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a770ef8",
   "metadata": {},
   "source": [
    "Формируем синтетический датасет на основе полиномиальной функции с добавлением случайного шума ${\\epsilon}$:\n",
    "\n",
    "${f(x) = \\sum_{k=0}^{K-1}{w_k \\cdot x^k} + \\epsilon}$,\n",
    "\n",
    "где ${w}$ - массив весов размера ${K}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05952c8",
   "metadata": {},
   "source": [
    "Определим полиномиальную функцию, в частном и в общем виде:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee933d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_trend(x):\n",
    "    return x**3 - 3*x**2 + 2*x - 5\n",
    "\n",
    "def f_poly(X, w_coeff):\n",
    "    Y = np.zeros_like(X)\n",
    "    for n, x in enumerate(X):\n",
    "        for idx, w in enumerate(w_coeff.tolist()[0]):\n",
    "            Y[n] = Y[n] + w * x**idx\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaf5f32",
   "metadata": {},
   "source": [
    "Сгенерируем синтетический датасет и построим графики."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37d9e5cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_min = 0\n",
    "x_max = 3\n",
    "N_points = 2000\n",
    "# Случайный набор X-ов.\n",
    "X_train = np.random.uniform(low=x_min, high=x_max, size=(N_points,))\n",
    "# Отклики с добавлением шума.\n",
    "y_train = f_trend(X_train) + np.random.normal(0,0.2,N_points)\n",
    "\n",
    "plt.scatter(X_train, y_train)\n",
    "plt.plot(np.sort(X_train), f_trend(np.sort(X_train)), color='yellow')\n",
    "plt.plot(np.sort(X_train), f_poly(np.sort(X_train), np.array([[-5, 2, -3, 1]])), '--', color='black')\n",
    "plt.legend(['train dataset', 'exact trend line', 'poly line'])\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "#plt.savefig('plots/dataset_plot.png')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a4f4da",
   "metadata": {},
   "source": [
    "Функция ошибки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "858c2928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(X, y, w_coeff):\n",
    "    return np.sum((y - f_poly(X, w_coeff))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca17f1a9",
   "metadata": {},
   "source": [
    "Функция поиска параметров методом градиентного спуска."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551da401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit(X_train, y_train, learning_rate, tolerance, batch_ratio, beta):\n",
    "    t_init = time.time()\n",
    "\n",
    "    batch_size = int(batch_ratio * len(X_train))\n",
    "    \n",
    "    iteration_max = 200000\n",
    "    \n",
    "    # Коэффициенты (веса) инициилизируются нулями.\n",
    "    w_coeff = np.zeros((1,len(learning_rate)))\n",
    "    # Буфер коэффициентов.\n",
    "    w_coeff_buff = copy.deepcopy(w_coeff)\n",
    "    \n",
    "    # Градиенты инициализируются ненулевыми значениями.\n",
    "    grad = np.ones_like(w_coeff)\n",
    "    \n",
    "    # Аккумулятор для фильтра градиента.\n",
    "    grad_filter = np.zeros_like(grad)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for i in range(iteration_max):\n",
    "        for k, w in enumerate(w_coeff.tolist()[0]):\n",
    "            # Накопление \"момента\".\n",
    "            grad_filter[0,k] = beta * grad_filter[0,k] + (1 - beta) * grad[0,k]\n",
    "            # Шаг градиентного спуска.\n",
    "            w_step = - learning_rate[k] * grad_filter[0,k]\n",
    "            # Сбрасываем коэффициенты до необновленных значений.\n",
    "            w_coeff_1 = copy.deepcopy(w_coeff)\n",
    "            w_coeff_2 = copy.deepcopy(w_coeff)\n",
    "            # Модификация только k-го коэффициента:\n",
    "            w_coeff_1[0,k] = w + w_step\n",
    "            w_coeff_2[0,k] = w + 2*w_step\n",
    "\n",
    "            # Выборка случайных элементов (по индексам).\n",
    "            batch_indices = np.random.choice(len(X_train), size=batch_size, replace=True)\n",
    "            X_train_batch = [X_train[idx] for idx in batch_indices]\n",
    "            y_train_batch = [y_train[idx] for idx in batch_indices]\n",
    "            \n",
    "            # Формируем массив значений функции потерь, для вычисления градиента. Массив состоит из 3х элементов.\n",
    "            loss_func_grad = [loss_func(X_train_batch, y_train_batch, w_coeff  ),\n",
    "                              loss_func(X_train_batch, y_train_batch, w_coeff_1),\n",
    "                              loss_func(X_train_batch, y_train_batch, w_coeff_2)]\n",
    "            # Массив градиента состоит из 3х чисел с индексами [0, 1, 2]. Берем предпоследнее число.\n",
    "            grad[0,k] = np.gradient(loss_func_grad, w_step)[1]\n",
    "            \n",
    "            # Обновление одного коэффициента в итоговом массиве.\n",
    "            w_coeff_buff[0,k] = w_coeff_1[0,k]\n",
    "        # Обновление всех коэффициентов.\n",
    "        w_coeff = copy.deepcopy(w_coeff_buff)\n",
    "        # Вычисление ошибки на полном датасете, при обновленных коэффициентах.\n",
    "        loss = loss_func(X_train, y_train, w_coeff)\n",
    "        # Накопление ошибки в отдельный массив для дальнейшей визуализации.\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            #print('\\x1b[2K', end='')  # Очистить строку\n",
    "            clear_output(wait=True)\n",
    "            print('Iteration:', i)\n",
    "            print('Gradient:', np.round(grad[0],4))\n",
    "            print('Weights:', np.round(w_coeff[0],4))\n",
    "            print('Loss:', np.round(loss,4))\n",
    "        if (loss < tolerance):\n",
    "            break\n",
    "    iter_final = i\n",
    "    fit_time = time.time() - t_init\n",
    "    return w_coeff, losses, iter_final, fit_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec579397",
   "metadata": {},
   "source": [
    "В ходе обучения заложим среднеквадратическое отклонение до 0.2. Соответствующая сумма квадратов отклонений: (0.2**2 * N_points)=80."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b052a6",
   "metadata": {},
   "source": [
    "Сначала попробуем обучение на полном датасете, причем learning rate зададим одинаковыми для каждого искомого параметра модели.\n",
    "\n",
    "Установлено, что большое значение параметра learning rate приводит к неустойчивости решателя. Функция ошибки \"улетает\" в бесконечность.\n",
    "\n",
    "Напротив, при малом значении параметра learning rate оптимизация выполняется медленно.\n",
    "\n",
    "В результате проб подобрано наибольшее значение learning rate, при котором сходимость устойчива (функция ошибки не \"улетает\" в бесконечность)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "add33c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1900\n",
      "Gradient: [ 3943.8777  1467.8529   411.5195   -15.9331 -2687.6957]\n",
      "Weights: [-1.3021 -0.9574 -0.8986 -0.6409  0.3956]\n",
      "Loss: 5651.9913\n"
     ]
    }
   ],
   "source": [
    "# Обучение на полном датасете.\n",
    "# lr - const.\n",
    "weights_1, losses_1, iter_final_1, fit_time_1 = model_fit(X_train, y_train,\n",
    "                                                          learning_rate=5*[1e-7],\n",
    "                                                          tolerance=(0.2**2 * N_points),\n",
    "                                                          batch_ratio=1.0,\n",
    "                                                          beta=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f5b334",
   "metadata": {},
   "source": [
    "В заданное максимальное число итераций не достигнут желаемый уровень ошибки."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b50ece9",
   "metadata": {},
   "source": [
    "Попробуем задать разный learning rate для параметров модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91fc4314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 900\n",
      "Gradient: [   -28.6441    366.7832     54.1271  -2805.0979 -10503.08  ]\n",
      "Weights: [-5.1044 -0.3284  0.1273  0.0748  0.0283]\n",
      "Loss: 742.8744\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Обучение на полном датасете.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# lr - индивидуальный для каждого из параметров модели.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m weights_2, losses_2, iter_final_2, fit_time_2 = \u001b[43mmodel_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m                                                          \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1e-6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1e-7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1e-8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1e-9\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m                                                          \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_points\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m                                                          \u001b[49m\u001b[43mbatch_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m                                                          \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mmodel_fit\u001b[39m\u001b[34m(X_train, y_train, learning_rate, tolerance, batch_ratio, beta)\u001b[39m\n\u001b[32m     38\u001b[39m y_train_batch = [y_train[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m batch_indices]\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Формируем массив значений функции потерь, для вычисления градиента. Массив состоит из 3х элементов.\u001b[39;00m\n\u001b[32m     41\u001b[39m loss_func_grad = [loss_func(X_train_batch, y_train_batch, w_coeff  ),\n\u001b[32m     42\u001b[39m                   loss_func(X_train_batch, y_train_batch, w_coeff_1),\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m                   \u001b[43mloss_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_coeff_2\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Массив градиента состоит из 3х чисел с индексами [0, 1, 2]. Берем предпоследнее число.\u001b[39;00m\n\u001b[32m     45\u001b[39m grad[\u001b[32m0\u001b[39m,k] = np.gradient(loss_func_grad, w_step)[\u001b[32m1\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mloss_func\u001b[39m\u001b[34m(X, y, w_coeff)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mloss_func\u001b[39m(X, y, w_coeff):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.sum((y - \u001b[43mf_poly\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_coeff\u001b[49m\u001b[43m)\u001b[49m)**\u001b[32m2\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mf_poly\u001b[39m\u001b[34m(X, w_coeff)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m n, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(X):\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m idx, w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(w_coeff.tolist()[\u001b[32m0\u001b[39m]):\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m         Y[n] = Y[n] + w * x**idx\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Y\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Обучение на полном датасете.\n",
    "# lr - индивидуальный для каждого из параметров модели.\n",
    "weights_2, losses_2, iter_final_2, fit_time_2 = model_fit(X_train, y_train,\n",
    "                                                          learning_rate=[1e-5, 1e-6, 1e-7, 1e-8, 1e-9],\n",
    "                                                          tolerance=(0.2**2 * N_points),\n",
    "                                                          batch_ratio=1.0,\n",
    "                                                          beta=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb1858e",
   "metadata": {},
   "source": [
    "В заданное максимальное число итераций также не достигнут желаемый уровень ошибки. Но скорость сходимости улучшилась значительно."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80ba27d",
   "metadata": {},
   "source": [
    "Попробуем обучать не на полном датасете, а на случайно выбранных значениях.\n",
    "\n",
    "В таком случае оказалось, что обучение на батчах устойчиво при бОльших значениях learning rate. Получается, разбиение на батчи не только снижает объем вычислений, но и позволяет улучшить сходимость за счет большего learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dbe209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Gradient: [ 1639.6891  2135.7205  3786.9637  7360.9314 15840.172 ]\n",
      "Weights: [-0.001  -0.0001 -0.     -0.     -0.    ]\n",
      "MSE loss: 40427.5608\n",
      "Iteration: 100\n",
      "Gradient: [  -22.8278   -49.941   -187.1791    27.9543 -2555.2576]\n",
      "Weights: [-5.092  -0.357   0.1177  0.0761  0.0293]\n",
      "MSE loss: 719.4556\n",
      "Iteration: 200\n",
      "Gradient: [  -17.7646    55.9927    57.1689  -126.6707 -2144.7199]\n",
      "Weights: [-4.8595 -0.6474  0.1039  0.0893  0.0388]\n",
      "MSE loss: 487.5871\n",
      "Iteration: 300\n",
      "Gradient: [ -22.2885   44.0813   -4.3774 -293.5782  -22.9731]\n",
      "Weights: [-4.6588 -0.8482  0.0831  0.1     0.0457]\n",
      "MSE loss: 362.1784\n",
      "Iteration: 400\n",
      "Gradient: [   8.6767   65.6788  107.0816   65.8841 -618.6707]\n",
      "Weights: [-4.4662 -1.024   0.0766  0.1066  0.0508]\n",
      "MSE loss: 294.3935\n",
      "Iteration: 500\n",
      "Gradient: [  17.4356   -6.5911   35.3054   -1.6997 -523.3908]\n",
      "Weights: [-4.3972 -1.1312  0.0662  0.1103  0.0553]\n",
      "MSE loss: 256.2042\n",
      "Iteration: 600\n",
      "Gradient: [  18.9748   10.5005   22.4872  112.2367 -191.0203]\n",
      "Weights: [-4.322  -1.188   0.0493  0.1143  0.0585]\n",
      "MSE loss: 237.9259\n",
      "Iteration: 700\n",
      "Gradient: [   7.9946  -16.5645  -40.2053 -149.6196 -677.357 ]\n",
      "Weights: [-4.2916 -1.2304  0.0344  0.1168  0.0605]\n",
      "MSE loss: 226.8909\n",
      "Iteration: 800\n",
      "Gradient: [ -22.851   -17.8842   15.8974  -42.0274 -132.3966]\n",
      "Weights: [-4.292  -1.2528  0.0212  0.1197  0.0625]\n",
      "MSE loss: 221.3704\n",
      "Iteration: 900\n",
      "Gradient: [   1.8814   -5.0709   13.2122 -170.6594 -157.0236]\n",
      "Weights: [-4.2351 -1.2734  0.0101  0.1194  0.0645]\n",
      "MSE loss: 214.5521\n",
      "Iteration: 1000\n",
      "Gradient: [ -19.8493   15.1598   48.26   -105.6885  239.6976]\n",
      "Weights: [-4.2165e+00 -1.2712e+00 -2.2000e-03  1.1970e-01  6.5500e-02]\n",
      "MSE loss: 211.6201\n",
      "Iteration: 1100\n",
      "Gradient: [  -5.9733  -32.8696  -17.3947 -144.789  -795.96  ]\n",
      "Weights: [-4.2244 -1.2756 -0.0122  0.1212  0.0668]\n",
      "MSE loss: 209.3397\n",
      "Iteration: 1200\n",
      "Gradient: [   1.8266   -5.3539   82.0942  117.9038 -488.4135]\n",
      "Weights: [-4.1787 -1.267  -0.0285  0.1216  0.0682]\n",
      "MSE loss: 205.922\n",
      "Iteration: 1300\n",
      "Gradient: [-12.881  -10.5111  -3.8272 -30.1452  -4.0407]\n",
      "Weights: [-4.2211 -1.2413 -0.0408  0.122   0.0692]\n",
      "MSE loss: 202.7919\n",
      "Iteration: 1400\n",
      "Gradient: [ -20.2829  -60.1952  -35.1586 -116.9425  -75.7598]\n",
      "Weights: [-4.2257 -1.2307 -0.05    0.1205  0.07  ]\n",
      "MSE loss: 201.3485\n",
      "Iteration: 1500\n",
      "Gradient: [ -6.7264  20.6572  40.1868  16.6297 118.9853]\n",
      "Weights: [-4.1959 -1.2201 -0.0585  0.1196  0.071 ]\n",
      "MSE loss: 199.06\n",
      "Iteration: 1600\n",
      "Gradient: [   3.0642   -1.4522  -16.192     3.0279 -205.7211]\n",
      "Weights: [-4.2079 -1.2012 -0.0704  0.1195  0.0715]\n",
      "MSE loss: 196.9784\n",
      "Iteration: 1700\n",
      "Gradient: [-12.043  -30.0179 -42.6981 -38.3541  72.8378]\n",
      "Weights: [-4.2234 -1.168  -0.0812  0.1191  0.0722]\n",
      "MSE loss: 194.7984\n",
      "Iteration: 1800\n",
      "Gradient: [   3.3353  -20.0697  -32.2159 -102.7877   85.6657]\n",
      "Weights: [-4.2185 -1.1557 -0.088   0.1181  0.0731]\n",
      "MSE loss: 193.7811\n",
      "Iteration: 1900\n",
      "Gradient: [ -3.0078   4.9619  -0.8737 -43.0421   8.7238]\n",
      "Weights: [-4.2566 -1.1342 -0.1005  0.1185  0.0738]\n",
      "MSE loss: 190.9001\n",
      "Iteration: 2000\n",
      "Gradient: [  26.7289   16.8499   62.5514 -113.4842    4.7726]\n",
      "Weights: [-4.2426 -1.1221 -0.1071  0.1178  0.0746]\n",
      "MSE loss: 189.6332\n",
      "Iteration: 2100\n",
      "Gradient: [ -13.4281   15.1932   14.8109 -106.2918   50.0218]\n",
      "Weights: [-4.2418 -1.1103 -0.1196  0.1175  0.0754]\n",
      "MSE loss: 187.0818\n",
      "Iteration: 2200\n",
      "Gradient: [ 14.4114 -19.0656 -10.1914  32.3317 -76.6231]\n",
      "Weights: [-4.2413 -1.1065 -0.1276  0.1172  0.0763]\n",
      "MSE loss: 185.4457\n",
      "Iteration: 2300\n",
      "Gradient: [ -13.4093   -7.0669    3.4579   48.999  -554.7839]\n",
      "Weights: [-4.2549 -1.0958 -0.1394  0.1164  0.0772]\n",
      "MSE loss: 184.9689\n",
      "Iteration: 2400\n",
      "Gradient: [  -4.738     7.4839   49.6379   62.0556 -254.0472]\n",
      "Weights: [-4.2638 -1.0624 -0.1469  0.1159  0.0779]\n",
      "MSE loss: 181.6853\n",
      "Iteration: 2500\n",
      "Gradient: [ -6.5756  13.6646  76.2841 102.8145 126.3761]\n",
      "Weights: [-4.2722 -1.0358 -0.1555  0.1145  0.0786]\n",
      "MSE loss: 180.208\n",
      "Iteration: 2600\n",
      "Gradient: [22.8891 -8.8614 28.6516 65.8248 70.1287]\n",
      "Weights: [-4.2595 -1.0253 -0.168   0.1137  0.0794]\n",
      "MSE loss: 177.8682\n",
      "Iteration: 2700\n",
      "Gradient: [  -3.1642    3.1121  -17.4943   93.7152 -490.0846]\n",
      "Weights: [-4.2884 -0.9966 -0.1783  0.1132  0.0802]\n",
      "MSE loss: 175.7954\n",
      "Iteration: 2800\n",
      "Gradient: [ -3.0339  -3.9789  40.6178  31.1552 144.4192]\n",
      "Weights: [-4.29   -0.9764 -0.1887  0.1119  0.0809]\n",
      "MSE loss: 173.8977\n",
      "Iteration: 2900\n",
      "Gradient: [ -1.2783   0.6272  97.3422  44.4701 300.328 ]\n",
      "Weights: [-4.2904 -0.9713 -0.1943  0.1121  0.0815]\n",
      "MSE loss: 172.8819\n",
      "Iteration: 3000\n",
      "Gradient: [ -4.3478 -30.7094 -27.6027 -71.7141 -74.6438]\n",
      "Weights: [-4.2896 -0.9734 -0.2013  0.1117  0.0826]\n",
      "MSE loss: 171.5348\n",
      "Iteration: 3100\n",
      "Gradient: [   8.2723   -3.4947   -7.2924  -11.4154 -198.3093]\n",
      "Weights: [-4.2796 -0.949  -0.2058  0.1091  0.0828]\n",
      "MSE loss: 170.7365\n",
      "Iteration: 3200\n",
      "Gradient: [  -2.421     7.0708   67.2931  163.6155 -102.9462]\n",
      "Weights: [-4.2898 -0.9381 -0.2124  0.1085  0.0837]\n",
      "MSE loss: 169.1034\n",
      "Iteration: 3300\n",
      "Gradient: [  2.2173  11.3367 -30.0244 -61.5845 -97.339 ]\n",
      "Weights: [-4.286  -0.9276 -0.2233  0.1084  0.0844]\n",
      "MSE loss: 167.4702\n",
      "Iteration: 3400\n",
      "Gradient: [ 20.8993  13.2638  -0.1463  59.3487 -17.2065]\n",
      "Weights: [-4.3039 -0.9111 -0.2336  0.109   0.0851]\n",
      "MSE loss: 165.7407\n",
      "Iteration: 3500\n",
      "Gradient: [   3.7806    3.5043   59.3993  -89.1717 -184.5942]\n",
      "Weights: [-4.2888 -0.9127 -0.2382  0.1087  0.0858]\n",
      "MSE loss: 164.9983\n",
      "Iteration: 3600\n",
      "Gradient: [  14.2017   -3.5917   42.4022 -111.5742  411.9345]\n",
      "Weights: [-4.2738 -0.8969 -0.249   0.1078  0.0865]\n",
      "MSE loss: 164.0283\n",
      "Iteration: 3700\n",
      "Gradient: [  12.7212    1.829    32.1725   66.2443 -327.6078]\n",
      "Weights: [-4.2893 -0.8855 -0.2588  0.1084  0.0874]\n",
      "MSE loss: 161.9194\n",
      "Iteration: 3800\n",
      "Gradient: [  -6.4471   14.6582   42.0121  -63.2669 -214.9843]\n",
      "Weights: [-4.3052 -0.8627 -0.2707  0.1071  0.088 ]\n",
      "MSE loss: 160.3027\n",
      "Iteration: 3900\n",
      "Gradient: [   2.7101  -11.318    11.9882  -88.0923 -151.8314]\n",
      "Weights: [-4.3294 -0.8543 -0.2715  0.1078  0.0885]\n",
      "MSE loss: 159.7144\n",
      "Iteration: 4000\n",
      "Gradient: [ -4.6535   9.9342  38.995   76.5557 151.9488]\n",
      "Weights: [-4.322  -0.8322 -0.2821  0.1065  0.0887]\n",
      "MSE loss: 158.1374\n",
      "Iteration: 4100\n",
      "Gradient: [ -3.4494  29.084   26.7214  98.8088 330.3296]\n",
      "Weights: [-4.355  -0.7916 -0.2926  0.1066  0.0892]\n",
      "MSE loss: 156.339\n",
      "Iteration: 4200\n",
      "Gradient: [   0.8139   12.3622   19.749  -107.0833  102.9874]\n",
      "Weights: [-4.3529 -0.784  -0.3038  0.107   0.09  ]\n",
      "MSE loss: 154.7574\n",
      "Iteration: 4300\n",
      "Gradient: [  -2.4816  -19.7108  -20.9259 -214.5198   62.0283]\n",
      "Weights: [-4.3284 -0.774  -0.3133  0.1062  0.0905]\n",
      "MSE loss: 153.6692\n",
      "Iteration: 4400\n",
      "Gradient: [  5.507  -23.4882 -51.4668 154.2913  48.1804]\n",
      "Weights: [-4.3613 -0.7543 -0.32    0.1063  0.0912]\n",
      "MSE loss: 152.2766\n",
      "Iteration: 4500\n",
      "Gradient: [  8.6528   8.4213  -8.4514  42.6863 -78.5563]\n",
      "Weights: [-4.3546 -0.7461 -0.3244  0.1054  0.0915]\n",
      "MSE loss: 151.4956\n",
      "Iteration: 4600\n",
      "Gradient: [  21.0641    5.934    -9.9374   40.8579 -150.4687]\n",
      "Weights: [-4.3403 -0.7385 -0.3333  0.1053  0.0922]\n",
      "MSE loss: 150.5415\n",
      "Iteration: 4700\n",
      "Gradient: [  3.7661  -2.1589 -11.1572  23.1679 -47.9793]\n",
      "Weights: [-4.3434 -0.7399 -0.3383  0.1047  0.0931]\n",
      "MSE loss: 149.4415\n",
      "Iteration: 4800\n",
      "Gradient: [  -0.5692   17.1116   45.2622  123.5114 -113.0965]\n",
      "Weights: [-4.3684 -0.7114 -0.347   0.1036  0.0936]\n",
      "MSE loss: 147.9809\n",
      "Iteration: 4900\n",
      "Gradient: [ 14.9955  23.5911  28.6556  25.9571 -75.3449]\n",
      "Weights: [-4.3409 -0.7117 -0.3497  0.1026  0.0942]\n",
      "MSE loss: 147.908\n",
      "Iteration: 5000\n",
      "Gradient: [  -1.5658  -25.5085  -12.482   166.8181 -161.4048]\n",
      "Weights: [-4.3681 -0.713  -0.3528  0.1022  0.0948]\n",
      "MSE loss: 147.2884\n",
      "Iteration: 5100\n",
      "Gradient: [ -6.7696  -8.4428  29.6457 -17.2923 -44.4914]\n",
      "Weights: [-4.3689 -0.6876 -0.3589  0.1013  0.0952]\n",
      "MSE loss: 145.7232\n",
      "Iteration: 5200\n",
      "Gradient: [   1.7831    5.9301   -3.1652   20.7524 -131.6993]\n",
      "Weights: [-4.353  -0.6807 -0.3655  0.1002  0.0956]\n",
      "MSE loss: 145.0669\n",
      "Iteration: 5300\n",
      "Gradient: [  6.2515 -25.3154  -3.697  -65.4525  85.6404]\n",
      "Weights: [-4.3637 -0.6831 -0.3703  0.1007  0.0962]\n",
      "MSE loss: 144.4598\n",
      "Iteration: 5400\n",
      "Gradient: [  1.541    8.974   42.7495  37.6779 257.8005]\n",
      "Weights: [-4.3444 -0.6662 -0.3751  0.1008  0.0964]\n",
      "MSE loss: 145.0852\n",
      "Iteration: 5500\n",
      "Gradient: [ 7.210000e-02 -3.337200e+00  3.225600e+01  7.440040e+01  1.924438e+02]\n",
      "Weights: [-4.3953 -0.6467 -0.3814  0.1012  0.0967]\n",
      "MSE loss: 142.7919\n",
      "Iteration: 5600\n",
      "Gradient: [  6.9762  -7.8488 -14.0182  14.4846  92.4352]\n",
      "Weights: [-4.3951 -0.631  -0.3865  0.1007  0.0969]\n",
      "MSE loss: 142.0258\n",
      "Iteration: 5700\n",
      "Gradient: [  0.8053  23.338   20.6279  16.9961 -39.8088]\n",
      "Weights: [-4.3854 -0.608  -0.3968  0.0994  0.0973]\n",
      "MSE loss: 140.9266\n",
      "Iteration: 5800\n",
      "Gradient: [   2.0467   12.5764  -31.0265 -133.3338  -49.4158]\n",
      "Weights: [-4.4064 -0.605  -0.4036  0.0997  0.098 ]\n",
      "MSE loss: 139.846\n",
      "Iteration: 5900\n",
      "Gradient: [   7.4052  -16.8059    0.5458 -107.8604  -65.5948]\n",
      "Weights: [-4.3941 -0.5891 -0.4122  0.0994  0.0986]\n",
      "MSE loss: 138.6569\n",
      "Iteration: 6000\n",
      "Gradient: [  0.4935 -19.5487  19.4043 -58.3692 335.2005]\n",
      "Weights: [-4.3974 -0.5821 -0.4165  0.0988  0.0988]\n",
      "MSE loss: 138.1055\n",
      "Iteration: 6100\n",
      "Gradient: [   2.4417   -4.7334    7.3853   66.0398 -299.5577]\n",
      "Weights: [-4.3983 -0.564  -0.4239  0.0986  0.0991]\n",
      "MSE loss: 137.3361\n",
      "Iteration: 6200\n",
      "Gradient: [  6.734   14.3962  19.0635  72.7121 -48.9967]\n",
      "Weights: [-4.4092 -0.5524 -0.4284  0.0986  0.0998]\n",
      "MSE loss: 136.6997\n",
      "Iteration: 6300\n",
      "Gradient: [   9.2119    6.0071   16.7343   79.5067 -251.7077]\n",
      "Weights: [-4.415  -0.5343 -0.4381  0.0978  0.1002]\n",
      "MSE loss: 135.2375\n",
      "Iteration: 6400\n",
      "Gradient: [   2.4555   -6.0893   -5.5999 -114.3412  -15.6115]\n",
      "Weights: [-4.4177 -0.5311 -0.4447  0.0978  0.1008]\n",
      "MSE loss: 134.3744\n",
      "Iteration: 6500\n",
      "Gradient: [ 6.191   8.629   8.7241 14.7875 -6.5185]\n",
      "Weights: [-4.4211 -0.5264 -0.4503  0.0985  0.1015]\n",
      "MSE loss: 133.6407\n",
      "Iteration: 6600\n",
      "Gradient: [ -4.5871  -9.0112 -45.3404  -6.9738  52.1454]\n",
      "Weights: [-4.4146 -0.5242 -0.4542  0.0975  0.1019]\n",
      "MSE loss: 133.0746\n",
      "Iteration: 6700\n",
      "Gradient: [  6.5402   2.4965  70.5228  77.5833 113.3956]\n",
      "Weights: [-4.4049 -0.5094 -0.4585  0.0968  0.1025]\n",
      "MSE loss: 133.4784\n",
      "Iteration: 6800\n",
      "Gradient: [ 1.35780e+00 -1.49300e-01 -5.01110e+00 -1.51817e+02  7.66871e+01]\n",
      "Weights: [-4.4194 -0.505  -0.4639  0.0966  0.1028]\n",
      "MSE loss: 131.7521\n",
      "Iteration: 6900\n",
      "Gradient: [ -7.718  -10.2846 -16.9383  33.4226 -49.1485]\n",
      "Weights: [-4.4267 -0.4974 -0.4689  0.096   0.1032]\n",
      "MSE loss: 131.1246\n",
      "Iteration: 7000\n",
      "Gradient: [  -1.7651   -8.5271  -40.1694  -24.0976 -554.3215]\n",
      "Weights: [-4.4466 -0.4803 -0.4771  0.0966  0.1036]\n",
      "MSE loss: 130.6041\n",
      "Iteration: 7100\n",
      "Gradient: [ -2.0401 -18.4556   2.0913   9.7442  10.8554]\n",
      "Weights: [-4.4418 -0.4693 -0.4837  0.0966  0.1038]\n",
      "MSE loss: 129.7457\n",
      "Iteration: 7200\n",
      "Gradient: [  4.7454  14.1329   6.8463  10.5897 201.4979]\n",
      "Weights: [-4.4207 -0.4647 -0.4882  0.0966  0.1042]\n",
      "MSE loss: 129.3125\n",
      "Iteration: 7300\n",
      "Gradient: [  3.1932  14.1466  18.4301 -26.2605 120.8399]\n",
      "Weights: [-4.4468 -0.4493 -0.4916  0.0964  0.1046]\n",
      "MSE loss: 128.528\n",
      "Iteration: 7400\n",
      "Gradient: [  8.4096  26.6521  43.9946 146.1537 199.5987]\n",
      "Weights: [-4.4663 -0.4194 -0.5011  0.0954  0.1049]\n",
      "MSE loss: 127.3217\n",
      "Iteration: 7500\n",
      "Gradient: [  3.4393 -10.467   -7.2687  50.9963 268.7134]\n",
      "Weights: [-4.4806 -0.4161 -0.5032  0.0951  0.1052]\n",
      "MSE loss: 127.4176\n",
      "Iteration: 7600\n",
      "Gradient: [  6.8772   6.4808 -32.1488  55.7703 184.0036]\n",
      "Weights: [-4.4642 -0.4116 -0.5087  0.0949  0.1056]\n",
      "MSE loss: 126.3757\n",
      "Iteration: 7700\n",
      "Gradient: [  0.4116  22.5168  35.7022   7.5638 374.1376]\n",
      "Weights: [-4.4644 -0.405  -0.5134  0.0938  0.1061]\n",
      "MSE loss: 125.7494\n",
      "Iteration: 7800\n",
      "Gradient: [ -0.1953 -30.4761 -31.4581 -29.5211  39.5627]\n",
      "Weights: [-4.465  -0.3995 -0.5169  0.0932  0.1064]\n",
      "MSE loss: 125.4742\n",
      "Iteration: 7900\n",
      "Gradient: [ -1.5489  -6.9981 -24.816   71.8717 192.9373]\n",
      "Weights: [-4.4868 -0.3771 -0.521   0.0923  0.1068]\n",
      "MSE loss: 124.7933\n",
      "Iteration: 8000\n",
      "Gradient: [  13.7609  -15.5894   -3.3435  -16.9699 -122.6235]\n",
      "Weights: [-4.4702 -0.3649 -0.5286  0.0923  0.1071]\n",
      "MSE loss: 124.0432\n",
      "Iteration: 8100\n",
      "Gradient: [  -6.1822   -1.7249    3.8784 -125.8471    3.7883]\n",
      "Weights: [-4.4826 -0.3688 -0.5336  0.0924  0.1079]\n",
      "MSE loss: 123.5065\n",
      "Iteration: 8200\n",
      "Gradient: [-10.6055 -14.3216  -5.2871  98.4283 -37.3837]\n",
      "Weights: [-4.4777 -0.3546 -0.5408  0.0928  0.1083]\n",
      "MSE loss: 122.5471\n",
      "Iteration: 8300\n",
      "Gradient: [ 8.8154 25.029  42.8397 24.4852 40.2344]\n",
      "Weights: [-4.468  -0.3436 -0.5443  0.0916  0.1086]\n",
      "MSE loss: 122.5162\n",
      "Iteration: 8400\n",
      "Gradient: [  7.1409 -21.5909  22.5998  32.1203 -83.7363]\n",
      "Weights: [-4.4976 -0.3236 -0.5499  0.0898  0.1092]\n",
      "MSE loss: 121.2245\n",
      "Iteration: 8500\n",
      "Gradient: [ -1.9967  13.0753 -10.9289  30.6199 -62.2206]\n",
      "Weights: [-4.486  -0.3263 -0.5567  0.0902  0.1097]\n",
      "MSE loss: 120.6938\n",
      "Iteration: 8600\n",
      "Gradient: [  -5.0165    5.1581   42.2299  -87.9079 -321.9181]\n",
      "Weights: [-4.4962 -0.3194 -0.5601  0.0891  0.1103]\n",
      "MSE loss: 120.37\n",
      "Iteration: 8700\n",
      "Gradient: [   2.1317  -15.0189   23.4029   37.9064 -517.0999]\n",
      "Weights: [-4.481  -0.3158 -0.5648  0.0893  0.1108]\n",
      "MSE loss: 119.5651\n",
      "Iteration: 8800\n",
      "Gradient: [  -4.9417   -5.3239   14.1154 -136.8198 -511.4496]\n",
      "Weights: [-4.5044 -0.2977 -0.5697  0.0885  0.1111]\n",
      "MSE loss: 119.0477\n",
      "Iteration: 8900\n",
      "Gradient: [  10.865     9.266     9.4455   31.409  -203.8699]\n",
      "Weights: [-4.4892 -0.3051 -0.5727  0.0892  0.1115]\n",
      "MSE loss: 118.7343\n",
      "Iteration: 9000\n",
      "Gradient: [ -7.5928  11.8759  29.6934 -12.1483 230.9196]\n",
      "Weights: [-4.4876 -0.2899 -0.5773  0.0886  0.1118]\n",
      "MSE loss: 118.3011\n",
      "Iteration: 9100\n",
      "Gradient: [  9.1168 -10.5315  30.2802 125.1169  41.4956]\n",
      "Weights: [-4.4931 -0.2874 -0.5822  0.0888  0.1122]\n",
      "MSE loss: 117.7379\n",
      "Iteration: 9200\n",
      "Gradient: [  3.358   12.3088 -22.0201  34.2634  39.5571]\n",
      "Weights: [-4.4963 -0.2756 -0.5877  0.0886  0.1125]\n",
      "MSE loss: 117.1881\n",
      "Iteration: 9300\n",
      "Gradient: [  4.7231  -6.7847  15.3228 -91.5651 -11.4031]\n",
      "Weights: [-4.5293 -0.251  -0.5926  0.0881  0.1126]\n",
      "MSE loss: 116.9358\n",
      "Iteration: 9400\n",
      "Gradient: [ -4.4116  16.4633   1.7653   8.376  -96.6548]\n",
      "Weights: [-4.5202 -0.2327 -0.6006  0.0872  0.1129]\n",
      "MSE loss: 115.8909\n",
      "Iteration: 9500\n",
      "Gradient: [   1.7419   -2.9677  -28.8609  -26.7578 -169.1869]\n",
      "Weights: [-4.5338 -0.219  -0.6065  0.0867  0.1133]\n",
      "MSE loss: 115.45\n",
      "Iteration: 9600\n",
      "Gradient: [   0.9283    2.1274  -41.0772  -87.9834 -353.2448]\n",
      "Weights: [-4.5298 -0.2127 -0.6129  0.0862  0.1139]\n",
      "MSE loss: 114.7513\n",
      "Iteration: 9700\n",
      "Gradient: [  4.3804  23.7345  64.2962  52.2979 -22.1884]\n",
      "Weights: [-4.531  -0.2036 -0.6163  0.0872  0.1143]\n",
      "MSE loss: 114.6222\n",
      "Iteration: 9800\n",
      "Gradient: [  -0.8638   11.3253   49.8451   30.8415 -147.1193]\n",
      "Weights: [-4.5332 -0.2005 -0.6212  0.0859  0.1146]\n",
      "MSE loss: 113.9826\n",
      "Iteration: 9900\n",
      "Gradient: [ 5.955700e+00 -1.273040e+01 -2.972670e+01  1.387000e-01  1.880973e+02]\n",
      "Weights: [-4.5484 -0.1915 -0.6229  0.0859  0.115 ]\n",
      "MSE loss: 113.7613\n",
      "Iteration: 10000\n",
      "Gradient: [ -7.4084  15.4801  20.2906  25.2569 319.7616]\n",
      "Weights: [-4.542  -0.1681 -0.6304  0.0851  0.1152]\n",
      "MSE loss: 113.0916\n",
      "Iteration: 10100\n",
      "Gradient: [  -1.6939  -20.0302  -14.7321  -30.2791 -123.6322]\n",
      "Weights: [-4.5404 -0.1714 -0.6346  0.0853  0.1156]\n",
      "MSE loss: 112.5175\n",
      "Iteration: 10200\n",
      "Gradient: [-10.1245   3.3687 -32.1918 -31.3352 226.3972]\n",
      "Weights: [-4.513  -0.1708 -0.6404  0.0854  0.1159]\n",
      "MSE loss: 112.7302\n",
      "Iteration: 10300\n",
      "Gradient: [ -7.0663 -17.7713   1.4713  32.8384 191.015 ]\n",
      "Weights: [-4.5297 -0.1688 -0.6453  0.086   0.1164]\n",
      "MSE loss: 111.6529\n",
      "Iteration: 10400\n",
      "Gradient: [ -7.3474 -12.3463  -5.7742  48.5207 -22.0164]\n",
      "Weights: [-4.538  -0.1575 -0.6469  0.0854  0.1166]\n",
      "MSE loss: 111.4125\n",
      "Iteration: 10500\n",
      "Gradient: [  -8.4024   14.4484   11.6884   59.3004 -407.7007]\n",
      "Weights: [-4.5573 -0.1508 -0.6506  0.0855  0.1169]\n",
      "MSE loss: 111.3348\n",
      "Iteration: 10600\n",
      "Gradient: [ -0.2249   9.461   -1.3974 -21.4253  39.4888]\n",
      "Weights: [-4.5386 -0.1476 -0.6556  0.085   0.1173]\n",
      "MSE loss: 110.6102\n",
      "Iteration: 10700\n",
      "Gradient: [  4.2037  14.2529  -7.6517 109.2033  41.1928]\n",
      "Weights: [-4.5384 -0.1372 -0.6623  0.0849  0.1177]\n",
      "MSE loss: 110.1183\n",
      "Iteration: 10800\n",
      "Gradient: [-9.9487 -4.4397 29.8867 40.4202 68.9558]\n",
      "Weights: [-4.5493 -0.1245 -0.6636  0.0844  0.1177]\n",
      "MSE loss: 109.9112\n",
      "Iteration: 10900\n",
      "Gradient: [-9.0675 -8.888  42.9616  9.8941 61.1237]\n",
      "Weights: [-4.5569 -0.1114 -0.6702  0.0839  0.1184]\n",
      "MSE loss: 109.2667\n",
      "Iteration: 11000\n",
      "Gradient: [  1.4816  -6.7718   5.2243  37.7349 -51.137 ]\n",
      "Weights: [-4.5566 -0.1114 -0.6731  0.0835  0.1185]\n",
      "MSE loss: 109.1514\n",
      "Iteration: 11100\n",
      "Gradient: [  5.9965  -8.5356 -18.8699  50.1197  14.9555]\n",
      "Weights: [-4.5568 -0.1051 -0.6738  0.0831  0.1188]\n",
      "MSE loss: 108.8816\n",
      "Iteration: 11200\n",
      "Gradient: [14.5912 -8.7207 35.1666 -0.428  59.2376]\n",
      "Weights: [-4.5656 -0.0931 -0.6767  0.0828  0.1189]\n",
      "MSE loss: 108.6553\n",
      "Iteration: 11300\n",
      "Gradient: [ -2.486    0.6591 -22.0624 -69.1529 160.8858]\n",
      "Weights: [-4.5773 -0.0849 -0.6803  0.0823  0.1192]\n",
      "MSE loss: 108.3446\n",
      "Iteration: 11400\n",
      "Gradient: [-23.6462   8.9008 -23.8235 -38.9606 177.1144]\n",
      "Weights: [-4.5622 -0.0845 -0.6846  0.0816  0.1197]\n",
      "MSE loss: 107.8408\n",
      "Iteration: 11500\n",
      "Gradient: [  4.5441  18.1373  62.1222  97.6919 -25.6733]\n",
      "Weights: [-4.5502 -0.087  -0.6855  0.0808  0.1204]\n",
      "MSE loss: 107.8638\n",
      "Iteration: 11600\n",
      "Gradient: [ -10.7735  -15.1176  -61.9696   40.4545 -177.2497]\n",
      "Weights: [-4.5782 -0.0878 -0.6877  0.081   0.1208]\n",
      "MSE loss: 107.9827\n",
      "Iteration: 11700\n",
      "Gradient: [ -3.0667   4.8201  41.5655  31.2632 266.4809]\n",
      "Weights: [-4.5578 -0.0746 -0.6922  0.0806  0.121 ]\n",
      "MSE loss: 107.2596\n",
      "Iteration: 11800\n",
      "Gradient: [ 10.3264  19.4916  47.5438   0.437  245.4208]\n",
      "Weights: [-4.5576 -0.0715 -0.6968  0.0809  0.1214]\n",
      "MSE loss: 106.9398\n",
      "Iteration: 11900\n",
      "Gradient: [-1.223000e-01 -1.493120e+01 -1.095830e+01 -5.655110e+01  2.318939e+02]\n",
      "Weights: [-4.5628 -0.0679 -0.7001  0.0803  0.1216]\n",
      "MSE loss: 106.3922\n",
      "Iteration: 12000\n",
      "Gradient: [  4.8322   8.1856  -1.6948   9.2654 167.0623]\n",
      "Weights: [-4.5562 -0.0587 -0.7042  0.0801  0.1217]\n",
      "MSE loss: 106.254\n",
      "Iteration: 12100\n",
      "Gradient: [-11.0287  -7.7064  27.3214  98.8303 120.4664]\n",
      "Weights: [-4.563  -0.0467 -0.7101  0.0803  0.1219]\n",
      "MSE loss: 105.7989\n",
      "Iteration: 12200\n",
      "Gradient: [  -2.1322    1.8824  -19.3055   46.3762 -103.1131]\n",
      "Weights: [-4.572  -0.0365 -0.7161  0.0805  0.1223]\n",
      "MSE loss: 105.2678\n",
      "Iteration: 12300\n",
      "Gradient: [  12.1053    8.3798   31.9096  -40.8332 -106.0162]\n",
      "Weights: [-4.5787 -0.0204 -0.7233  0.0799  0.1227]\n",
      "MSE loss: 104.6806\n",
      "Iteration: 12400\n",
      "Gradient: [   4.1703    6.7552  -42.3593 -135.7317 -187.1603]\n",
      "Weights: [-4.5711 -0.0146 -0.7278  0.0799  0.1229]\n",
      "MSE loss: 104.5415\n",
      "Iteration: 12500\n",
      "Gradient: [   2.9755   -6.754    -2.1009    9.0618 -146.0192]\n",
      "Weights: [-4.5988e+00 -4.4000e-03 -7.3170e-01  7.9500e-02  1.2340e-01]\n",
      "MSE loss: 104.2523\n",
      "Iteration: 12600\n",
      "Gradient: [  0.8637  16.9101   5.852  -49.0031 -72.2074]\n",
      "Weights: [-4.6069  0.0104 -0.7359  0.079   0.1234]\n",
      "MSE loss: 104.1391\n",
      "Iteration: 12700\n",
      "Gradient: [-4.1688 -2.4637  5.5461 11.6283  1.001 ]\n",
      "Weights: [-4.5912  0.0095 -0.7397  0.0791  0.1239]\n",
      "MSE loss: 103.4399\n",
      "Iteration: 12800\n",
      "Gradient: [   4.0918   -0.8635   14.9946    8.2842 -294.8532]\n",
      "Weights: [-4.5998  0.0281 -0.7449  0.079   0.1242]\n",
      "MSE loss: 103.1663\n",
      "Iteration: 12900\n",
      "Gradient: [  -5.9165  -13.5189  -18.7173   -7.0966 -153.0802]\n",
      "Weights: [-4.6029  0.029  -0.7482  0.079   0.1243]\n",
      "MSE loss: 102.8985\n",
      "Iteration: 13000\n",
      "Gradient: [  -5.3976   -4.982   -25.0368  -23.1936 -102.4162]\n",
      "Weights: [-4.6142  0.037  -0.7509  0.0787  0.1245]\n",
      "MSE loss: 102.8637\n",
      "Iteration: 13100\n",
      "Gradient: [-13.1037 -15.1045 -15.6931 -30.0912 110.7023]\n",
      "Weights: [-4.6263  0.0414 -0.7528  0.0789  0.1246]\n",
      "MSE loss: 103.1715\n",
      "Iteration: 13200\n",
      "Gradient: [  -6.151     4.5578  -42.0128  -95.4266 -220.6683]\n",
      "Weights: [-4.6093  0.0393 -0.7576  0.0793  0.1249]\n",
      "MSE loss: 102.5543\n",
      "Iteration: 13300\n",
      "Gradient: [  1.4308  -3.5039 -12.8972   5.0627  -7.729 ]\n",
      "Weights: [-4.601   0.0444 -0.7593  0.0787  0.1252]\n",
      "MSE loss: 102.061\n",
      "Iteration: 13400\n",
      "Gradient: [  5.4162  -1.3132  12.836  -78.9328  -9.3792]\n",
      "Weights: [-4.5878  0.0512 -0.7655  0.0787  0.1256]\n",
      "MSE loss: 101.9397\n",
      "Iteration: 13500\n",
      "Gradient: [-6.7835  8.6168 -3.8174 26.6907 56.1394]\n",
      "Weights: [-4.6008  0.0518 -0.7669  0.079   0.1259]\n",
      "MSE loss: 101.5578\n",
      "Iteration: 13600\n",
      "Gradient: [  -2.0177  -25.2224   16.6291  -20.9595 -326.1666]\n",
      "Weights: [-4.5947  0.0504 -0.768   0.0788  0.126 ]\n",
      "MSE loss: 101.5083\n",
      "Iteration: 13700\n",
      "Gradient: [ -6.5619  11.6526  29.112   -9.3773 -67.0501]\n",
      "Weights: [-4.6035  0.0606 -0.7717  0.0793  0.1261]\n",
      "MSE loss: 101.2869\n",
      "Iteration: 13800\n",
      "Gradient: [  2.98     3.2344  38.5227 -23.3934 174.4202]\n",
      "Weights: [-4.6173  0.0667 -0.7727  0.0789  0.1264]\n",
      "MSE loss: 101.227\n",
      "Iteration: 13900\n",
      "Gradient: [-17.5427  -4.5725 -29.2651 -17.4299 -56.668 ]\n",
      "Weights: [-4.627   0.0728 -0.7761  0.0783  0.1266]\n",
      "MSE loss: 101.2612\n",
      "Iteration: 14000\n",
      "Gradient: [   1.9875    9.25      4.989   -82.0065 -142.8191]\n",
      "Weights: [-4.6138  0.0825 -0.7803  0.0773  0.1267]\n",
      "MSE loss: 100.6603\n",
      "Iteration: 14100\n",
      "Gradient: [  1.3892   9.5433  10.1445  57.2076 -75.3821]\n",
      "Weights: [-4.6124  0.091  -0.7827  0.0769  0.1268]\n",
      "MSE loss: 100.487\n",
      "Iteration: 14200\n",
      "Gradient: [   0.662    -3.3215  -16.3043   -6.0208 -163.7828]\n",
      "Weights: [-4.6022  0.0934 -0.785   0.0773  0.1271]\n",
      "MSE loss: 100.7656\n",
      "Iteration: 14300\n",
      "Gradient: [   3.363    13.8795  -55.8983  -34.6638 -165.5107]\n",
      "Weights: [-4.6363  0.0986 -0.7872  0.077   0.1275]\n",
      "MSE loss: 100.375\n",
      "Iteration: 14400\n",
      "Gradient: [  11.2242  -10.1324  -22.6577  -42.5086 -321.5324]\n",
      "Weights: [-4.6274  0.1084 -0.7916  0.0767  0.1275]\n",
      "MSE loss: 99.8732\n",
      "Iteration: 14500\n",
      "Gradient: [  0.8671 -11.0644  24.3093 -10.635   78.2687]\n",
      "Weights: [-4.6224  0.1134 -0.7939  0.0773  0.1275]\n",
      "MSE loss: 99.8046\n",
      "Iteration: 14600\n",
      "Gradient: [ 1.2545  1.262  25.4761 16.7827 54.3041]\n",
      "Weights: [-4.6466  0.1247 -0.7949  0.0776  0.1276]\n",
      "MSE loss: 99.9418\n",
      "Iteration: 14700\n",
      "Gradient: [ 14.2739  -3.3502  -2.6714 -20.2169  27.5686]\n",
      "Weights: [-4.6209  0.123  -0.8021  0.0774  0.128 ]\n",
      "MSE loss: 99.3348\n",
      "Iteration: 14800\n",
      "Gradient: [ -3.7761  14.5373  30.2263 105.8246  14.2416]\n",
      "Weights: [-4.6184  0.1161 -0.803   0.0779  0.1282]\n",
      "MSE loss: 99.2324\n",
      "Iteration: 14900\n",
      "Gradient: [  9.2714 -12.1951 -69.7528  -9.2871  26.9057]\n",
      "Weights: [-4.6246  0.1236 -0.8053  0.0779  0.1282]\n",
      "MSE loss: 99.0882\n",
      "Iteration: 15000\n",
      "Gradient: [  8.4359   0.2094  -9.309  -51.9459 -57.9011]\n",
      "Weights: [-4.6225  0.12   -0.8048  0.0773  0.1286]\n",
      "MSE loss: 99.0305\n",
      "Iteration: 15100\n",
      "Gradient: [ -12.4134   -7.7652   10.0809  -81.2171 -349.8413]\n",
      "Weights: [-4.6329  0.1305 -0.8106  0.0773  0.1288]\n",
      "MSE loss: 98.8377\n",
      "Iteration: 15200\n",
      "Gradient: [  -9.3443    5.9236  -31.9209   27.3476 -331.0759]\n",
      "Weights: [-4.6387  0.1509 -0.8148  0.076   0.1291]\n",
      "MSE loss: 98.3827\n",
      "Iteration: 15300\n",
      "Gradient: [  6.307  -17.1595   3.2895 -45.1181  34.9282]\n",
      "Weights: [-4.6616  0.174  -0.8216  0.0755  0.1293]\n",
      "MSE loss: 98.2094\n",
      "Iteration: 15400\n",
      "Gradient: [-3.4077 -4.014  20.5717 64.6833 12.4178]\n",
      "Weights: [-4.654   0.1707 -0.8236  0.0756  0.1296]\n",
      "MSE loss: 98.0064\n",
      "Iteration: 15500\n",
      "Gradient: [  4.5804  -7.7184  26.5897 -21.0159 -29.9127]\n",
      "Weights: [-4.6618  0.1763 -0.8248  0.0764  0.1297]\n",
      "MSE loss: 98.0238\n",
      "Iteration: 15600\n",
      "Gradient: [   3.0595   12.2343   -1.8218   54.8854 -216.2967]\n",
      "Weights: [-4.626   0.166  -0.8288  0.0762  0.1301]\n",
      "MSE loss: 97.8482\n",
      "Iteration: 15700\n",
      "Gradient: [ -4.3252  -8.8918  27.2468 -54.295  -24.1024]\n",
      "Weights: [-4.6452  0.17   -0.8323  0.0763  0.1303]\n",
      "MSE loss: 97.6078\n",
      "Iteration: 15800\n",
      "Gradient: [  6.1653  -5.5455  10.3877  53.8716 126.0994]\n",
      "Weights: [-4.6286  0.1781 -0.8372  0.0769  0.1304]\n",
      "MSE loss: 97.4634\n",
      "Iteration: 15900\n",
      "Gradient: [ -3.1502  13.4957  27.4351 -23.0306 -90.6273]\n",
      "Weights: [-4.6437  0.1843 -0.8378  0.0764  0.1307]\n",
      "MSE loss: 97.1563\n",
      "Iteration: 16000\n",
      "Gradient: [   7.2888    7.0105   20.0651   74.3855 -125.7176]\n",
      "Weights: [-4.6385  0.1942 -0.841   0.0759  0.1308]\n",
      "MSE loss: 97.2966\n",
      "Iteration: 16100\n",
      "Gradient: [ -3.7559   1.6918 -10.6807  27.0197 208.3005]\n",
      "Weights: [-4.6522  0.2072 -0.8453  0.0755  0.1311]\n",
      "MSE loss: 96.7921\n",
      "Iteration: 16200\n",
      "Gradient: [   2.8437   -5.5378   22.0858   37.3128 -140.0991]\n",
      "Weights: [-4.653   0.214  -0.8511  0.076   0.1314]\n",
      "MSE loss: 96.6027\n",
      "Iteration: 16300\n",
      "Gradient: [ -3.0018  14.2848   8.0472  77.5376 -39.7287]\n",
      "Weights: [-4.6724  0.2244 -0.8516  0.0751  0.1315]\n",
      "MSE loss: 96.3953\n",
      "Iteration: 16400\n",
      "Gradient: [ -0.2333  -4.7034   1.0006 -16.0602 150.0316]\n",
      "Weights: [-4.6668  0.2277 -0.8562  0.0747  0.132 ]\n",
      "MSE loss: 96.0611\n",
      "Iteration: 16500\n",
      "Gradient: [   1.9295   -6.7189   -8.2136    9.1975 -132.2672]\n",
      "Weights: [-4.6621  0.2297 -0.8577  0.0744  0.132 ]\n",
      "MSE loss: 95.9786\n",
      "Iteration: 16600\n",
      "Gradient: [  -1.6121   10.7142   11.8442 -145.0389  278.6616]\n",
      "Weights: [-4.679   0.2383 -0.8602  0.0734  0.1323]\n",
      "MSE loss: 96.0218\n",
      "Iteration: 16700\n",
      "Gradient: [  4.7329   0.845  -21.0311  31.1381  75.431 ]\n",
      "Weights: [-4.6549  0.2374 -0.8627  0.0737  0.1324]\n",
      "MSE loss: 95.8542\n",
      "Iteration: 16800\n",
      "Gradient: [   3.6617   -1.1262   -2.0577   49.029  -199.1535]\n",
      "Weights: [-4.6673  0.2458 -0.8669  0.0739  0.1327]\n",
      "MSE loss: 95.4817\n",
      "Iteration: 16900\n",
      "Gradient: [  4.1402   0.7954 -12.8886  20.0463 122.5269]\n",
      "Weights: [-4.6542  0.2423 -0.8693  0.074   0.133 ]\n",
      "MSE loss: 95.4797\n",
      "Iteration: 17000\n",
      "Gradient: [  -2.9797  -12.9396  -18.5507   54.5374 -373.0463]\n",
      "Weights: [-4.6778  0.2473 -0.8724  0.0746  0.1331]\n",
      "MSE loss: 95.5129\n",
      "Iteration: 17100\n",
      "Gradient: [  7.3766   6.5173  14.7612 152.5187 431.8112]\n",
      "Weights: [-4.6643  0.253  -0.874   0.0747  0.1333]\n",
      "MSE loss: 95.2813\n",
      "Iteration: 17200\n",
      "Gradient: [  6.2619  -5.4406  -6.8277 -85.4854 -78.3101]\n",
      "Weights: [-4.6683  0.2476 -0.8775  0.0745  0.1335]\n",
      "MSE loss: 95.3213\n",
      "Iteration: 17300\n",
      "Gradient: [ -1.3389  13.7206   1.974   74.0561 -47.6   ]\n",
      "Weights: [-4.6513  0.2493 -0.8779  0.0748  0.1337]\n",
      "MSE loss: 95.255\n",
      "Iteration: 17400\n",
      "Gradient: [   4.3632   -7.8024   -5.1974 -101.4295  -50.6242]\n",
      "Weights: [-4.6654  0.2544 -0.8779  0.0737  0.1341]\n",
      "MSE loss: 94.9452\n",
      "Iteration: 17500\n",
      "Gradient: [  14.6039   13.6435   42.9089   31.303  -211.6348]\n",
      "Weights: [-4.6493  0.2535 -0.8822  0.0734  0.1344]\n",
      "MSE loss: 94.8554\n",
      "Iteration: 17600\n",
      "Gradient: [ -1.3502 -13.4597 -61.655  -39.2012 -12.1109]\n",
      "Weights: [-4.6758  0.2707 -0.8865  0.0727  0.1345]\n",
      "MSE loss: 94.5683\n",
      "Iteration: 17700\n",
      "Gradient: [ -5.3897   1.3454 -25.7834 -37.8059  52.8681]\n",
      "Weights: [-4.6711  0.2749 -0.8891  0.0723  0.1349]\n",
      "MSE loss: 94.2809\n",
      "Iteration: 17800\n",
      "Gradient: [   9.3946  -15.708    -8.2148  -13.8675 -121.5525]\n",
      "Weights: [-4.6778  0.2856 -0.8924  0.0718  0.1352]\n",
      "MSE loss: 94.0598\n",
      "Iteration: 17900\n",
      "Gradient: [  -3.8894  -20.9624   19.4465   43.9989 -251.2655]\n",
      "Weights: [-4.6863  0.298  -0.8955  0.0713  0.1353]\n",
      "MSE loss: 93.9181\n",
      "Iteration: 18000\n",
      "Gradient: [ 18.1124  -0.7132  20.81   -28.9501 158.0504]\n",
      "Weights: [-4.6719  0.2916 -0.897   0.0714  0.1357]\n",
      "MSE loss: 93.9163\n",
      "Iteration: 18100\n",
      "Gradient: [   3.772    -0.7034   28.3089   78.1576 -385.3655]\n",
      "Weights: [-4.6899  0.3012 -0.9028  0.071   0.1362]\n",
      "MSE loss: 93.716\n",
      "Iteration: 18200\n",
      "Gradient: [  9.3911  -5.6141  -2.0492  78.0924 109.124 ]\n",
      "Weights: [-4.6802  0.3192 -0.9055  0.0702  0.1362]\n",
      "MSE loss: 93.8335\n",
      "Iteration: 18300\n",
      "Gradient: [  9.9196   5.4074   0.8465 -30.8394 -81.385 ]\n",
      "Weights: [-4.6974  0.3265 -0.911   0.0701  0.1364]\n",
      "MSE loss: 93.294\n",
      "Iteration: 18400\n",
      "Gradient: [  -4.3651  -11.2033   11.1986   66.2747 -178.3455]\n",
      "Weights: [-4.7224  0.3282 -0.9107  0.0701  0.1366]\n",
      "MSE loss: 94.3928\n",
      "Iteration: 18500\n",
      "Gradient: [14.3671  3.0549 41.1668 42.687  66.7145]\n",
      "Weights: [-4.6906  0.3333 -0.9131  0.0705  0.1367]\n",
      "MSE loss: 93.4745\n",
      "Iteration: 18600\n",
      "Gradient: [  4.8686   2.0352  10.42   -77.636  -27.2776]\n",
      "Weights: [-4.6817  0.3298 -0.9171  0.071   0.1367]\n",
      "MSE loss: 93.144\n",
      "Iteration: 18700\n",
      "Gradient: [ -7.6102  -5.0499   2.7386 105.4673 120.625 ]\n",
      "Weights: [-4.7033  0.3295 -0.9163  0.0709  0.1369]\n",
      "MSE loss: 93.1621\n",
      "Iteration: 18800\n",
      "Gradient: [  15.1316    6.3351   -9.0829   19.3599 -122.3282]\n",
      "Weights: [-4.6678  0.3219 -0.9173  0.0709  0.137 ]\n",
      "MSE loss: 93.3733\n",
      "Iteration: 18900\n",
      "Gradient: [  -0.2579    9.6698   -6.6064 -119.0449  -20.0994]\n",
      "Weights: [-4.69    0.3184 -0.9158  0.0702  0.1374]\n",
      "MSE loss: 93.0884\n",
      "Iteration: 19000\n",
      "Gradient: [ -6.5741 -21.8255  12.8417  16.5935 266.465 ]\n",
      "Weights: [-4.6929  0.3173 -0.9142  0.071   0.1372]\n",
      "MSE loss: 93.1511\n",
      "Iteration: 19100\n",
      "Gradient: [  3.4001  12.621  -27.1353 -61.3825 -63.9937]\n",
      "Weights: [-4.6865  0.3184 -0.9171  0.0702  0.1375]\n",
      "MSE loss: 93.0573\n",
      "Iteration: 19200\n",
      "Gradient: [  5.3026   5.5674   6.745  -39.2958 -17.8543]\n",
      "Weights: [-4.6818  0.3249 -0.9188  0.0697  0.1376]\n",
      "MSE loss: 92.8638\n",
      "Iteration: 19300\n",
      "Gradient: [  -1.3461  -20.2107   27.0441  -46.8057 -219.8476]\n",
      "Weights: [-4.7066  0.3375 -0.923   0.0696  0.1377]\n",
      "MSE loss: 93.2142\n",
      "Iteration: 19400\n",
      "Gradient: [   0.6948    3.3295    9.602   -45.4834 -421.7161]\n",
      "Weights: [-4.6937  0.3433 -0.9246  0.069   0.138 ]\n",
      "MSE loss: 92.5394\n",
      "Iteration: 19500\n",
      "Gradient: [  7.2504 -15.2519  35.7916  27.8633 -20.3441]\n",
      "Weights: [-4.6998  0.3495 -0.926   0.0689  0.1381]\n",
      "MSE loss: 92.4799\n",
      "Iteration: 19600\n",
      "Gradient: [  -1.8761    7.4675  -23.1141   -8.2227 -268.7466]\n",
      "Weights: [-4.6985  0.3504 -0.9288  0.0692  0.1381]\n",
      "MSE loss: 92.4243\n",
      "Iteration: 19700\n",
      "Gradient: [  -0.9734   -2.5245   -9.2571   77.9462 -158.856 ]\n",
      "Weights: [-4.688   0.3403 -0.9288  0.0697  0.1381]\n",
      "MSE loss: 92.5418\n",
      "Iteration: 19800\n",
      "Gradient: [-10.0759   0.6025  10.0749  74.3498 128.038 ]\n",
      "Weights: [-4.6996  0.346  -0.9293  0.0698  0.1384]\n",
      "MSE loss: 92.4385\n",
      "Iteration: 19900\n",
      "Gradient: [ -1.7648  15.0441  28.3529  19.5139 -76.226 ]\n",
      "Weights: [-4.6932  0.3491 -0.9311  0.0696  0.1384]\n",
      "MSE loss: 92.3277\n",
      "Iteration: 20000\n",
      "Gradient: [ -2.2324 -11.4687  -8.8967 133.1536 -38.4997]\n",
      "Weights: [-4.6948  0.3568 -0.9309  0.0694  0.1382]\n",
      "MSE loss: 92.4075\n",
      "Iteration: 20100\n",
      "Gradient: [   1.8396   -9.7856   -5.0932   59.8552 -187.9698]\n",
      "Weights: [-4.6973  0.3555 -0.9339  0.0689  0.1387]\n",
      "MSE loss: 92.1823\n",
      "Iteration: 20200\n",
      "Gradient: [ 1.078000e-01  5.257000e-01  4.263400e+00  1.471813e+02 -2.911921e+02]\n",
      "Weights: [-4.7166  0.3694 -0.9368  0.0687  0.1387]\n",
      "MSE loss: 92.2901\n",
      "Iteration: 20300\n",
      "Gradient: [   0.703   -14.6509   21.4367  -59.8364 -466.8936]\n",
      "Weights: [-4.6955  0.3743 -0.9414  0.068   0.1391]\n",
      "MSE loss: 91.9874\n",
      "Iteration: 20400\n",
      "Gradient: [  3.1111   0.3196   3.2185  37.9636 -35.5906]\n",
      "Weights: [-4.6909  0.3784 -0.9443  0.0678  0.1395]\n",
      "MSE loss: 92.1105\n",
      "Iteration: 20500\n",
      "Gradient: [  -8.2374  -26.7731  -25.7778    1.2509 -181.8819]\n",
      "Weights: [-4.6998  0.3734 -0.9466  0.0686  0.1397]\n",
      "MSE loss: 91.7255\n",
      "Iteration: 20600\n",
      "Gradient: [ -7.3589   3.4324  45.0769  61.4918 259.076 ]\n",
      "Weights: [-4.7021  0.382  -0.9489  0.0685  0.1397]\n",
      "MSE loss: 91.6272\n",
      "Iteration: 20700\n",
      "Gradient: [ -17.9651  -24.5461   16.8976   19.8875 -213.1882]\n",
      "Weights: [-4.7171  0.3833 -0.9494  0.068   0.1399]\n",
      "MSE loss: 91.9013\n",
      "Iteration: 20800\n",
      "Gradient: [  -1.1604   -6.8769  -29.7596  -74.272  -252.3401]\n",
      "Weights: [-4.7148  0.3891 -0.9502  0.0673  0.1401]\n",
      "MSE loss: 91.5783\n",
      "Iteration: 20900\n",
      "Gradient: [ -8.4264 -14.063   18.449   61.5076 -90.1011]\n",
      "Weights: [-4.7093  0.3953 -0.9543  0.0673  0.1403]\n",
      "MSE loss: 91.372\n",
      "Iteration: 21000\n",
      "Gradient: [  1.0868  -8.1107  -9.5774 -50.0625 -69.5987]\n",
      "Weights: [-4.7249  0.4133 -0.9589  0.0669  0.1404]\n",
      "MSE loss: 91.2464\n",
      "Iteration: 21100\n",
      "Gradient: [-4.828000e+00 -1.176100e+00  8.910000e-02 -1.012572e+02 -1.617325e+02]\n",
      "Weights: [-4.7429  0.4214 -0.9602  0.0666  0.1406]\n",
      "MSE loss: 91.5391\n",
      "Iteration: 21200\n",
      "Gradient: [  3.2896  14.0822  30.5205  27.2741 279.0092]\n",
      "Weights: [-4.7239  0.4206 -0.9611  0.0664  0.1407]\n",
      "MSE loss: 91.1613\n",
      "Iteration: 21300\n",
      "Gradient: [  2.9561   0.7809 -13.1943 -33.3679  67.3946]\n",
      "Weights: [-4.7274  0.4204 -0.9632  0.0665  0.1408]\n",
      "MSE loss: 91.0919\n",
      "Iteration: 21400\n",
      "Gradient: [  -7.8196  -11.8444   -2.1486  -61.841  -318.9729]\n",
      "Weights: [-4.7214  0.4142 -0.9651  0.067   0.141 ]\n",
      "MSE loss: 91.1113\n",
      "Iteration: 21500\n",
      "Gradient: [  1.2913 -11.4422  43.7028  12.4777 -12.9528]\n",
      "Weights: [-4.717   0.4186 -0.966   0.0671  0.1411]\n",
      "MSE loss: 91.0084\n",
      "Iteration: 21600\n",
      "Gradient: [  1.8514 -15.1727 -13.3243 -92.1944   1.8015]\n",
      "Weights: [-4.7176  0.4116 -0.9673  0.0674  0.1411]\n",
      "MSE loss: 91.1445\n",
      "Iteration: 21700\n",
      "Gradient: [ -10.6039    4.0218    1.0912 -110.254   186.3033]\n",
      "Weights: [-4.707   0.4201 -0.9692  0.0672  0.1412]\n",
      "MSE loss: 91.0443\n",
      "Iteration: 21800\n",
      "Gradient: [ -3.9409   1.1166  39.1849  36.6196 115.2277]\n",
      "Weights: [-4.7185  0.4192 -0.9704  0.0684  0.1414]\n",
      "MSE loss: 91.0081\n",
      "Iteration: 21900\n",
      "Gradient: [ 2.3941 14.242  48.5323 23.7828 44.9029]\n",
      "Weights: [-4.7113  0.4119 -0.9721  0.0687  0.1417]\n",
      "MSE loss: 90.9694\n",
      "Iteration: 22000\n",
      "Gradient: [  5.4014  -3.8113  -0.6501 -50.1504 -52.6901]\n",
      "Weights: [-4.7019  0.4097 -0.9745  0.0683  0.1418]\n",
      "MSE loss: 90.9276\n",
      "Iteration: 22100\n",
      "Gradient: [ -1.8908  11.4165  32.1925  18.6482 -79.0218]\n",
      "Weights: [-4.7154  0.4285 -0.9759  0.0676  0.1419]\n",
      "MSE loss: 90.7726\n",
      "Iteration: 22200\n",
      "Gradient: [ -5.8676   3.7497  26.2873  -8.756  186.4704]\n",
      "Weights: [-4.7197  0.4356 -0.978   0.0671  0.142 ]\n",
      "MSE loss: 90.6564\n",
      "Iteration: 22300\n",
      "Gradient: [  1.8084  14.4491  31.1225 -42.7252  26.1366]\n",
      "Weights: [-4.7287  0.4469 -0.9797  0.0658  0.142 ]\n",
      "MSE loss: 90.5217\n",
      "Iteration: 22400\n",
      "Gradient: [  4.191  -19.7323   6.4872 -12.1655  28.5275]\n",
      "Weights: [-4.7365  0.4478 -0.98    0.0651  0.1423]\n",
      "MSE loss: 90.5768\n",
      "Iteration: 22500\n",
      "Gradient: [  2.4159  -6.7618  18.2478  21.3967 210.8312]\n",
      "Weights: [-4.7272  0.4543 -0.9832  0.0651  0.1424]\n",
      "MSE loss: 90.4162\n",
      "Iteration: 22600\n",
      "Gradient: [  -0.2714   -8.2619   -8.2332  -12.43   -241.6317]\n",
      "Weights: [-4.7193  0.4457 -0.9856  0.0654  0.1427]\n",
      "MSE loss: 90.4243\n",
      "Iteration: 22700\n",
      "Gradient: [ -6.1286   3.9879   3.638  215.2144 -20.184 ]\n",
      "Weights: [-4.7295  0.4499 -0.9862  0.0653  0.1431]\n",
      "MSE loss: 90.3187\n",
      "Iteration: 22800\n",
      "Gradient: [ -6.7924  -2.9304 -10.447  -31.2069  18.8666]\n",
      "Weights: [-4.7006  0.4438 -0.9916  0.0658  0.1433]\n",
      "MSE loss: 90.5492\n",
      "Iteration: 22900\n",
      "Gradient: [  8.5105 -17.6982  27.0539 -23.4672  59.806 ]\n",
      "Weights: [-4.7321  0.4667 -0.9967  0.0661  0.1434]\n",
      "MSE loss: 90.0379\n",
      "Iteration: 23000\n",
      "Gradient: [  -0.375     9.0875  -11.5915 -107.8773  217.3912]\n",
      "Weights: [-4.7278  0.4773 -1.0005  0.0651  0.1437]\n",
      "MSE loss: 89.9695\n",
      "Iteration: 23100\n",
      "Gradient: [-11.0213 -18.5403  26.6018 -37.6475   9.3705]\n",
      "Weights: [-4.7381  0.4706 -1.0012  0.0657  0.1438]\n",
      "MSE loss: 90.2605\n",
      "Iteration: 23200\n",
      "Gradient: [  -0.6679   -3.3872   20.0864   60.7608 -148.434 ]\n",
      "Weights: [-4.7414  0.4905 -1.0018  0.0647  0.1437]\n",
      "MSE loss: 89.9201\n",
      "Iteration: 23300\n",
      "Gradient: [ -1.2543  -5.6233  28.3767  98.8745 -12.6014]\n",
      "Weights: [-4.7329  0.4878 -1.0041  0.0649  0.1439]\n",
      "MSE loss: 89.8989\n",
      "Iteration: 23400\n",
      "Gradient: [   7.2145   -1.2614   22.6329  -28.9064 -160.7152]\n",
      "Weights: [-4.7323  0.4853 -1.0023  0.0647  0.1439]\n",
      "MSE loss: 90.0101\n",
      "Iteration: 23500\n",
      "Gradient: [  -3.8897   -1.6576   22.7444 -108.5722   34.6668]\n",
      "Weights: [-4.7325  0.4823 -1.0052  0.0655  0.1441]\n",
      "MSE loss: 89.8072\n",
      "Iteration: 23600\n",
      "Gradient: [   2.6498   18.3576   -6.4942   77.2895 -280.0531]\n",
      "Weights: [-4.7232  0.4777 -1.0069  0.0657  0.1443]\n",
      "MSE loss: 89.8527\n",
      "Iteration: 23700\n",
      "Gradient: [   6.5674   11.1144   -8.4045  -69.1116 -101.8219]\n",
      "Weights: [-4.7308  0.4853 -1.0091  0.0657  0.1444]\n",
      "MSE loss: 89.7461\n",
      "Iteration: 23800\n",
      "Gradient: [-1.773000e-01  1.535910e+01  7.365400e+00 -6.398700e+00  3.060956e+02]\n",
      "Weights: [-4.7327  0.5023 -1.0137  0.0648  0.1445]\n",
      "MSE loss: 89.7568\n",
      "Iteration: 23900\n",
      "Gradient: [  0.5653   1.7246  38.0106  66.5366 208.9392]\n",
      "Weights: [-4.7393  0.5011 -1.0128  0.0642  0.1448]\n",
      "MSE loss: 89.5882\n",
      "Iteration: 24000\n",
      "Gradient: [  3.031    2.3055 -16.2336 -20.7501 -24.1817]\n",
      "Weights: [-4.7252  0.4886 -1.0137  0.065   0.1448]\n",
      "MSE loss: 89.6863\n",
      "Iteration: 24100\n",
      "Gradient: [ 11.6347   4.1845 -40.4454 -83.1734 174.3607]\n",
      "Weights: [-4.736   0.5032 -1.016   0.0653  0.1447]\n",
      "MSE loss: 89.6392\n",
      "Iteration: 24200\n",
      "Gradient: [  -2.1858   14.6689  -17.9532    1.4369 -168.3343]\n",
      "Weights: [-4.7407  0.5083 -1.0184  0.065   0.1448]\n",
      "MSE loss: 89.4794\n",
      "Iteration: 24300\n",
      "Gradient: [ -9.4884  17.4687 -19.0141  37.0566 180.203 ]\n",
      "Weights: [-4.7491  0.514  -1.0219  0.0651  0.145 ]\n",
      "MSE loss: 89.4009\n",
      "Iteration: 24400\n",
      "Gradient: [ -0.761    3.2424  18.6258  24.5183 170.2482]\n",
      "Weights: [-4.7523  0.5268 -1.0225  0.0648  0.1449]\n",
      "MSE loss: 89.4498\n",
      "Iteration: 24500\n",
      "Gradient: [  -7.362     5.976   -15.9412 -113.4823 -274.9752]\n",
      "Weights: [-4.7477  0.5179 -1.0262  0.0657  0.1451]\n",
      "MSE loss: 89.3246\n",
      "Iteration: 24600\n",
      "Gradient: [   4.6984   -1.1011  -37.7776   70.8789 -150.7777]\n",
      "Weights: [-4.7365  0.5171 -1.0283  0.0648  0.1455]\n",
      "MSE loss: 89.3631\n",
      "Iteration: 24700\n",
      "Gradient: [  0.2269 -16.7936 -15.6465   0.2921  31.2242]\n",
      "Weights: [-4.748   0.5213 -1.0265  0.0644  0.1457]\n",
      "MSE loss: 89.2541\n",
      "Iteration: 24800\n",
      "Gradient: [ 9.7658  7.0067 -8.6155 62.0213 91.0282]\n",
      "Weights: [-4.748   0.5207 -1.0239  0.0639  0.1457]\n",
      "MSE loss: 89.3485\n",
      "Iteration: 24900\n",
      "Gradient: [ -13.4447    5.2726   35.2587  -88.1283 -105.3004]\n",
      "Weights: [-4.7453  0.5163 -1.0258  0.0641  0.1458]\n",
      "MSE loss: 89.2977\n",
      "Iteration: 25000\n",
      "Gradient: [ -8.4916 -19.2288 -41.4476  20.2041  -5.6478]\n",
      "Weights: [-4.7477  0.5154 -1.0267  0.0644  0.1457]\n",
      "MSE loss: 89.4284\n",
      "Iteration: 25100\n",
      "Gradient: [ 14.6421 -20.1883  14.4112  34.0064 118.651 ]\n",
      "Weights: [-4.7468  0.5233 -1.0289  0.0645  0.1457]\n",
      "MSE loss: 89.2142\n",
      "Iteration: 25200\n",
      "Gradient: [ -3.247   -5.3588 -48.3585 -99.9375   8.7541]\n",
      "Weights: [-4.7498  0.5278 -1.0329  0.064   0.146 ]\n",
      "MSE loss: 89.3576\n",
      "Iteration: 25300\n",
      "Gradient: [  4.5564  18.2101  -7.6411  30.1078 -35.2338]\n",
      "Weights: [-4.7353  0.5237 -1.0305  0.0642  0.1462]\n",
      "MSE loss: 89.481\n",
      "Iteration: 25400\n",
      "Gradient: [   3.967    12.5875    3.1093   98.5959 -226.6447]\n",
      "Weights: [-4.7473  0.5278 -1.0331  0.0647  0.1463]\n",
      "MSE loss: 89.2314\n",
      "Iteration: 25500\n",
      "Gradient: [ -8.6206   5.2564  -7.3042 -82.7683 -23.1428]\n",
      "Weights: [-4.7536  0.5394 -1.034   0.0641  0.1461]\n",
      "MSE loss: 89.1509\n",
      "Iteration: 25600\n",
      "Gradient: [  3.9567   5.2234 -21.9226  69.6288 -76.7279]\n",
      "Weights: [-4.769   0.5456 -1.0366  0.0636  0.1462]\n",
      "MSE loss: 89.2589\n",
      "Iteration: 25700\n",
      "Gradient: [ -11.6633   -3.6968  -26.0343  -16.5912 -187.3904]\n",
      "Weights: [-4.7751  0.5506 -1.0385  0.0635  0.1462]\n",
      "MSE loss: 89.4955\n",
      "Iteration: 25800\n",
      "Gradient: [  8.7435  -7.7689   3.2916 -40.0516 -22.6866]\n",
      "Weights: [-4.7525  0.5519 -1.0392  0.064   0.1463]\n",
      "MSE loss: 89.2249\n",
      "Iteration: 25900\n",
      "Gradient: [-5.670000e-02 -1.273280e+01 -4.696230e+01 -2.897730e+01 -3.823131e+02]\n",
      "Weights: [-4.7311  0.5254 -1.0376  0.0643  0.1463]\n",
      "MSE loss: 89.3847\n",
      "Iteration: 26000\n",
      "Gradient: [  2.2439 -10.2742   9.4267  13.2899 -41.8336]\n",
      "Weights: [-4.7404  0.5346 -1.0379  0.0643  0.1464]\n",
      "MSE loss: 89.1143\n",
      "Iteration: 26100\n",
      "Gradient: [-1.37400e-01 -1.35605e+01  9.74040e+00 -7.59400e+00  2.12275e+02]\n",
      "Weights: [-4.7498  0.5432 -1.0421  0.0645  0.1466]\n",
      "MSE loss: 88.9547\n",
      "Iteration: 26200\n",
      "Gradient: [  -3.9963   15.5609   -9.0729   68.1847 -346.4831]\n",
      "Weights: [-4.7633  0.556  -1.0457  0.0639  0.1469]\n",
      "MSE loss: 88.8836\n",
      "Iteration: 26300\n",
      "Gradient: [  6.1815  14.9592  65.8997  12.8749 311.2069]\n",
      "Weights: [-4.7512  0.5628 -1.0492  0.0639  0.1472]\n",
      "MSE loss: 89.1103\n",
      "Iteration: 26400\n",
      "Gradient: [  -4.7375   -6.6255   -9.0707   32.8302 -136.063 ]\n",
      "Weights: [-4.743   0.5511 -1.0506  0.065   0.1472]\n",
      "MSE loss: 88.9523\n",
      "Iteration: 26500\n",
      "Gradient: [ 3.2629 -9.1445 18.7569 44.2676 89.1837]\n",
      "Weights: [-4.7489  0.5543 -1.0519  0.065   0.1471]\n",
      "MSE loss: 88.8266\n",
      "Iteration: 26600\n",
      "Gradient: [ 10.6496  -5.1936 -15.7606 -70.6417 181.0307]\n",
      "Weights: [-4.7436  0.555  -1.0507  0.0645  0.1474]\n",
      "MSE loss: 89.0362\n",
      "Iteration: 26700\n",
      "Gradient: [ 3.0508  6.5329  7.6749 46.9672 16.7157]\n",
      "Weights: [-4.7655  0.566  -1.0496  0.064   0.1472]\n",
      "MSE loss: 88.8639\n",
      "Iteration: 26800\n",
      "Gradient: [   8.305     3.7752   -7.8565  -63.4448 -170.5263]\n",
      "Weights: [-4.7814  0.5847 -1.051   0.0632  0.1469]\n",
      "MSE loss: 88.8293\n",
      "Iteration: 26900\n",
      "Gradient: [ 10.9044  13.1026 -14.3329 166.0494 127.0849]\n",
      "Weights: [-4.7676  0.5758 -1.0509  0.0631  0.147 ]\n",
      "MSE loss: 88.7538\n",
      "Iteration: 27000\n",
      "Gradient: [ -1.2515   0.4712 -46.1688   2.8843  -4.2696]\n",
      "Weights: [-4.7727  0.5795 -1.0492  0.0627  0.1471]\n",
      "MSE loss: 88.8824\n",
      "Iteration: 27100\n",
      "Gradient: [ -7.9994   4.4598  -7.1122 -36.1825 227.4191]\n",
      "Weights: [-4.7701  0.567  -1.0506  0.0632  0.1473]\n",
      "MSE loss: 88.8263\n",
      "Iteration: 27200\n",
      "Gradient: [   3.9445   -5.1938    8.1344 -106.1095 -163.2528]\n",
      "Weights: [-4.7721  0.5776 -1.0517  0.063   0.1471]\n",
      "MSE loss: 88.7255\n",
      "Iteration: 27300\n",
      "Gradient: [  -3.1721   -9.3753   25.885   -53.6245 -117.7208]\n",
      "Weights: [-4.7807  0.573  -1.0511  0.0639  0.1468]\n",
      "MSE loss: 89.1736\n",
      "Iteration: 27400\n",
      "Gradient: [ -1.3878 -18.1481 -10.0544 -35.5108 -83.6929]\n",
      "Weights: [-4.7649  0.5638 -1.0495  0.0644  0.1469]\n",
      "MSE loss: 88.7869\n",
      "Iteration: 27500\n",
      "Gradient: [  1.4213 -13.8123  21.1245 -29.8464  -6.0809]\n",
      "Weights: [-4.7523  0.5543 -1.0518  0.0657  0.1469]\n",
      "MSE loss: 88.8323\n",
      "Iteration: 27600\n",
      "Gradient: [ -0.3502  -4.9226  -4.2399 -70.2359 -74.5532]\n",
      "Weights: [-4.7526  0.5661 -1.0552  0.0659  0.1468]\n",
      "MSE loss: 88.8015\n",
      "Iteration: 27700\n",
      "Gradient: [   3.0899   -5.053   -21.3841  -39.234  -137.1651]\n",
      "Weights: [-4.7625  0.5714 -1.0577  0.0657  0.1468]\n",
      "MSE loss: 88.7895\n",
      "Iteration: 27800\n",
      "Gradient: [  -0.4892   21.4332  -20.6439  -36.2899 -179.1126]\n",
      "Weights: [-4.7673  0.5821 -1.0609  0.0654  0.147 ]\n",
      "MSE loss: 88.6703\n",
      "Iteration: 27900\n",
      "Gradient: [ -7.0933   7.382  -18.9831  63.3405 196.9588]\n",
      "Weights: [-4.7868  0.5829 -1.0582  0.0652  0.1472]\n",
      "MSE loss: 88.9334\n",
      "Iteration: 28000\n",
      "Gradient: [ -9.0098  10.2966 -55.8682 153.5625  91.7112]\n",
      "Weights: [-4.7619  0.5791 -1.0614  0.0656  0.1471]\n",
      "MSE loss: 88.6476\n",
      "Iteration: 28100\n",
      "Gradient: [ 12.2394   4.4546   9.9356  52.33   169.0003]\n",
      "Weights: [-4.748   0.5614 -1.0573  0.0661  0.1471]\n",
      "MSE loss: 88.7789\n",
      "Iteration: 28200\n",
      "Gradient: [ -4.7453   8.9266 -18.7622  15.0368 -68.0903]\n",
      "Weights: [-4.7679  0.5701 -1.0574  0.0657  0.147 ]\n",
      "MSE loss: 88.8001\n",
      "Iteration: 28300\n",
      "Gradient: [ -3.7098  17.3869  27.8706  -4.3659 -41.8724]\n",
      "Weights: [-4.7723  0.5651 -1.0548  0.0663  0.147 ]\n",
      "MSE loss: 88.8939\n",
      "Iteration: 28400\n",
      "Gradient: [ -1.9592  -1.9705  -4.5346 -29.2281 -60.4435]\n",
      "Weights: [-4.7742  0.5634 -1.055   0.0658  0.1473]\n",
      "MSE loss: 89.0733\n",
      "Iteration: 28500\n",
      "Gradient: [ -0.6729  -3.7212   1.5831  24.4948 -18.8828]\n",
      "Weights: [-4.7587  0.5656 -1.0575  0.0661  0.1472]\n",
      "MSE loss: 88.6951\n",
      "Iteration: 28600\n",
      "Gradient: [ -2.8628   6.7385 -60.7519  -2.5778 317.6503]\n",
      "Weights: [-4.7463  0.5647 -1.0595  0.0658  0.1475]\n",
      "MSE loss: 88.7988\n",
      "Iteration: 28700\n",
      "Gradient: [   0.2866  -10.2525   -9.3349  -35.4727 -144.5337]\n",
      "Weights: [-4.7414  0.5641 -1.0617  0.0665  0.1472]\n",
      "MSE loss: 88.8766\n",
      "Iteration: 28800\n",
      "Gradient: [ -0.1607   8.2891 -18.3468  -5.4601 -58.2424]\n",
      "Weights: [-4.7656  0.5655 -1.0607  0.0665  0.1474]\n",
      "MSE loss: 88.9905\n",
      "Iteration: 28900\n",
      "Gradient: [  2.5744 -24.2789 -26.8613 -95.0208 -77.2847]\n",
      "Weights: [-4.7678  0.5717 -1.0608  0.0661  0.1475]\n",
      "MSE loss: 88.7071\n",
      "Iteration: 29000\n",
      "Gradient: [  5.0887 -16.4373 -18.9527  18.0928 248.9008]\n",
      "Weights: [-4.773   0.5876 -1.0633  0.0661  0.1471]\n",
      "MSE loss: 88.5608\n",
      "Iteration: 29100\n",
      "Gradient: [   8.1063   -5.0479    9.5372   55.6581 -327.4831]\n",
      "Weights: [-4.7552  0.584  -1.0649  0.0664  0.147 ]\n",
      "MSE loss: 88.7339\n",
      "Iteration: 29200\n",
      "Gradient: [  5.6153  -4.0775   1.9437 100.4386 -62.6563]\n",
      "Weights: [-4.7731  0.585  -1.0625  0.0663  0.1471]\n",
      "MSE loss: 88.5833\n",
      "Iteration: 29300\n",
      "Gradient: [-3.192   6.5733  8.3749 52.5749 13.8932]\n",
      "Weights: [-4.7806  0.5925 -1.0665  0.067   0.1471]\n",
      "MSE loss: 88.5729\n",
      "Iteration: 29400\n",
      "Gradient: [ -7.4041  -2.374   41.2903 -57.186  115.799 ]\n",
      "Weights: [-4.7621  0.5867 -1.0709  0.0675  0.1473]\n",
      "MSE loss: 88.5287\n",
      "Iteration: 29500\n",
      "Gradient: [ -3.8    -16.7648  17.3729  53.5663 206.0813]\n",
      "Weights: [-4.7758  0.5883 -1.0725  0.0689  0.1473]\n",
      "MSE loss: 88.6132\n",
      "Iteration: 29600\n",
      "Gradient: [ 10.5385  12.0195 -13.2594  33.836  163.7646]\n",
      "Weights: [-4.782   0.6056 -1.0742  0.0689  0.1471]\n",
      "MSE loss: 88.5319\n",
      "Iteration: 29700\n",
      "Gradient: [  -2.3767  -23.7106  -23.565    39.3949 -361.884 ]\n",
      "Weights: [-4.7821  0.6042 -1.0738  0.0678  0.1472]\n",
      "MSE loss: 88.4548\n",
      "Iteration: 29800\n",
      "Gradient: [ -4.4595 -18.8338  15.0924 -64.684   71.6413]\n",
      "Weights: [-4.7743  0.6061 -1.077   0.0682  0.1472]\n",
      "MSE loss: 88.3858\n",
      "Iteration: 29900\n",
      "Gradient: [  -5.4946  -10.5191  -45.3016   49.5434 -286.5994]\n",
      "Weights: [-4.8005  0.6203 -1.0791  0.0677  0.1473]\n",
      "MSE loss: 88.6789\n",
      "Iteration: 30000\n",
      "Gradient: [ -5.0448   8.3486  20.0323 -40.4328 -62.9236]\n",
      "Weights: [-4.7875  0.6288 -1.0806  0.0675  0.1473]\n",
      "MSE loss: 88.5585\n",
      "Iteration: 30100\n",
      "Gradient: [  0.5852  16.4996 -24.8397 102.6923  -2.4343]\n",
      "Weights: [-4.7719  0.6228 -1.0813  0.0679  0.1474]\n",
      "MSE loss: 88.8544\n",
      "Iteration: 30200\n",
      "Gradient: [  -4.0394   16.1883  -41.8555   45.3172 -113.4338]\n",
      "Weights: [-4.7743  0.6192 -1.0831  0.0677  0.1476]\n",
      "MSE loss: 88.3579\n",
      "Iteration: 30300\n",
      "Gradient: [  4.7804  -8.8255 -10.2939  34.7003 -99.7253]\n",
      "Weights: [-4.7653  0.6048 -1.0787  0.0675  0.148 ]\n",
      "MSE loss: 88.5466\n",
      "Iteration: 30400\n",
      "Gradient: [  -6.3834    4.5424    0.3629    1.5219 -186.5767]\n",
      "Weights: [-4.76    0.6016 -1.0802  0.0684  0.1479]\n",
      "MSE loss: 88.5871\n",
      "Iteration: 30500\n",
      "Gradient: [   0.4167  -10.3845    6.6305   44.7392 -172.8886]\n",
      "Weights: [-4.762   0.5973 -1.0793  0.0685  0.148 ]\n",
      "MSE loss: 88.5263\n",
      "Iteration: 30600\n",
      "Gradient: [ 9.5335  3.2597 34.9476 82.5477 -0.1496]\n",
      "Weights: [-4.7532  0.6012 -1.0788  0.0677  0.148 ]\n",
      "MSE loss: 88.9316\n",
      "Iteration: 30700\n",
      "Gradient: [  13.3316    8.3931   21.7032   35.2529 -170.0788]\n",
      "Weights: [-4.778   0.6199 -1.0823  0.0676  0.1478]\n",
      "MSE loss: 88.3989\n",
      "Iteration: 30800\n",
      "Gradient: [  -7.2599  -12.5437  -26.3675 -162.0931 -100.7375]\n",
      "Weights: [-4.7805  0.6134 -1.0816  0.0675  0.1479]\n",
      "MSE loss: 88.3069\n",
      "Iteration: 30900\n",
      "Gradient: [  -1.2726  -11.8253  -45.4014  -41.4046 -243.3043]\n",
      "Weights: [-4.7828  0.627  -1.083   0.0669  0.1476]\n",
      "MSE loss: 88.3533\n",
      "Iteration: 31000\n",
      "Gradient: [ -2.0625   0.3251  31.772   13.6414 -15.407 ]\n",
      "Weights: [-4.7767  0.6178 -1.0835  0.0679  0.1479]\n",
      "MSE loss: 88.3003\n",
      "Iteration: 31100\n",
      "Gradient: [9.535000e+00 6.910000e-02 2.661220e+01 1.963604e+02 1.260482e+02]\n",
      "Weights: [-4.7572  0.62   -1.0864  0.0675  0.1483]\n",
      "MSE loss: 89.2204\n",
      "Iteration: 31200\n",
      "Gradient: [  1.3878 -12.5674  -2.9648  41.911  181.1234]\n",
      "Weights: [-4.7851  0.6293 -1.0883  0.0671  0.1485]\n",
      "MSE loss: 88.2721\n",
      "Iteration: 31300\n",
      "Gradient: [  0.2459 -14.3371  10.7935 -33.6637  71.0351]\n",
      "Weights: [-4.799   0.6338 -1.0917  0.0671  0.1487]\n",
      "MSE loss: 88.4018\n",
      "Iteration: 31400\n",
      "Gradient: [-10.0288   4.4517 -30.7123  70.787  -77.3683]\n",
      "Weights: [-4.7914  0.6456 -1.0948  0.0664  0.1487]\n",
      "MSE loss: 88.1248\n",
      "Iteration: 31500\n",
      "Gradient: [  2.3134 -20.2608 -11.0361 -33.5701 177.186 ]\n",
      "Weights: [-4.7791  0.6439 -1.0992  0.0666  0.1489]\n",
      "MSE loss: 88.1497\n",
      "Iteration: 31600\n",
      "Gradient: [  3.6941   8.1778   9.808  -48.5032 -15.5699]\n",
      "Weights: [-4.7781  0.6486 -1.1015  0.0672  0.1492]\n",
      "MSE loss: 88.3215\n",
      "Iteration: 31700\n",
      "Gradient: [  -0.9243    0.6644   -7.8265   65.5196 -257.1163]\n",
      "Weights: [-4.8025  0.6449 -1.0991  0.0672  0.1493]\n",
      "MSE loss: 88.2911\n",
      "Iteration: 31800\n",
      "Gradient: [  7.9719  19.5793   1.0482  73.2173 218.2823]\n",
      "Weights: [-4.7956  0.6473 -1.1014  0.0673  0.1494]\n",
      "MSE loss: 88.0899\n",
      "Iteration: 31900\n",
      "Gradient: [ -1.8633  -7.9961   0.3123 102.0941  90.4015]\n",
      "Weights: [-4.797   0.662  -1.1043  0.0674  0.1493]\n",
      "MSE loss: 88.2937\n",
      "Iteration: 32000\n",
      "Gradient: [ 6.424200e+00  6.710000e-02  3.740970e+01 -9.537520e+01  2.804635e+02]\n",
      "Weights: [-4.7929  0.6622 -1.1059  0.0666  0.1495]\n",
      "MSE loss: 88.0249\n",
      "Iteration: 32100\n",
      "Gradient: [   0.9523   -5.8999    6.6369   94.6492 -170.9097]\n",
      "Weights: [-4.8046  0.6695 -1.1057  0.0653  0.1496]\n",
      "MSE loss: 88.0176\n",
      "Iteration: 32200\n",
      "Gradient: [  2.9265   5.8895  13.9715 -25.1356 122.0344]\n",
      "Weights: [-4.793   0.6625 -1.103   0.0653  0.1496]\n",
      "MSE loss: 88.1154\n",
      "Iteration: 32300\n",
      "Gradient: [  4.8646  -9.8842 -17.4942 -44.3064  71.9889]\n",
      "Weights: [-4.781   0.6538 -1.1046  0.0654  0.1497]\n",
      "MSE loss: 88.0823\n",
      "Iteration: 32400\n",
      "Gradient: [  2.698    2.6391  42.3587  59.7388 212.1399]\n",
      "Weights: [-4.801   0.6571 -1.1035  0.0659  0.1497]\n",
      "MSE loss: 88.0576\n",
      "Iteration: 32500\n",
      "Gradient: [   2.2454    5.0026   16.1439 -106.6993   51.5972]\n",
      "Weights: [-4.8034  0.6814 -1.11    0.0646  0.1498]\n",
      "MSE loss: 88.0188\n",
      "Iteration: 32600\n",
      "Gradient: [   8.8034  -12.6963   20.2223   21.4973 -117.2925]\n",
      "Weights: [-4.7969  0.6825 -1.1123  0.0651  0.1499]\n",
      "MSE loss: 88.1025\n",
      "Iteration: 32700\n",
      "Gradient: [ 8.600000e-03 -2.579000e+00 -7.170000e+00  6.035700e+00  1.202146e+02]\n",
      "Weights: [-4.8033  0.6813 -1.1123  0.0654  0.1498]\n",
      "MSE loss: 87.9642\n",
      "Iteration: 32800\n",
      "Gradient: [  9.0867 -12.4399  -0.6146  -8.5281 -16.8364]\n",
      "Weights: [-4.8151  0.6888 -1.1141  0.065   0.1499]\n",
      "MSE loss: 88.0346\n",
      "Iteration: 32900\n",
      "Gradient: [ 2.580000e-02 -1.588600e+01 -2.153200e+00 -9.821120e+01 -3.455165e+02]\n",
      "Weights: [-4.7975  0.6755 -1.1129  0.0659  0.1499]\n",
      "MSE loss: 87.9273\n",
      "Iteration: 33000\n",
      "Gradient: [  5.1894  -3.3391   7.707  -31.6218 -13.2075]\n",
      "Weights: [-4.7966  0.6724 -1.1142  0.067   0.1498]\n",
      "MSE loss: 87.8925\n",
      "Iteration: 33100\n",
      "Gradient: [  1.8705  -6.513  -19.5867 120.2711 -23.6942]\n",
      "Weights: [-4.804   0.6812 -1.1138  0.0672  0.1496]\n",
      "MSE loss: 87.9808\n",
      "Iteration: 33200\n",
      "Gradient: [ -2.8633 -12.7404  17.4269 125.1431 200.9585]\n",
      "Weights: [-4.7957  0.6818 -1.1151  0.0671  0.1497]\n",
      "MSE loss: 88.1552\n",
      "Iteration: 33300\n",
      "Gradient: [ 13.5403  -7.7535  -4.3738 -93.1329 -81.5271]\n",
      "Weights: [-4.7913  0.6807 -1.1155  0.0671  0.1497]\n",
      "MSE loss: 88.2002\n",
      "Iteration: 33400\n",
      "Gradient: [  4.6022   8.0781 -16.5837  28.5722 -35.6255]\n",
      "Weights: [-4.7957  0.684  -1.1194  0.0677  0.1497]\n",
      "MSE loss: 87.9373\n",
      "Iteration: 33500\n",
      "Gradient: [  4.2706  -7.9211  19.6391 156.4292 -29.3031]\n",
      "Weights: [-4.8021  0.6803 -1.1199  0.069   0.1497]\n",
      "MSE loss: 87.8552\n",
      "Iteration: 33600\n",
      "Gradient: [-8.9755  1.4152 18.0638 15.566  16.5312]\n",
      "Weights: [-4.8007  0.674  -1.1196  0.0688  0.1499]\n",
      "MSE loss: 87.8847\n",
      "Iteration: 33700\n",
      "Gradient: [ -9.2923  -9.1719   6.4793 -32.6541 140.8231]\n",
      "Weights: [-4.7825  0.6687 -1.1209  0.069   0.1499]\n",
      "MSE loss: 87.931\n",
      "Iteration: 33800\n",
      "Gradient: [ -2.2273   7.0754  -7.2558 -27.6526 151.2212]\n",
      "Weights: [-4.7827  0.6748 -1.1202  0.0693  0.1497]\n",
      "MSE loss: 88.1797\n",
      "Iteration: 33900\n",
      "Gradient: [ -12.6151   11.6421    7.0514  -63.5258 -201.3415]\n",
      "Weights: [-4.8012  0.6892 -1.1242  0.0705  0.1494]\n",
      "MSE loss: 87.9677\n",
      "Iteration: 34000\n",
      "Gradient: [   8.9289    1.2497  -39.2023  -14.2872 -220.7667]\n",
      "Weights: [-4.8004  0.6977 -1.1282  0.0701  0.1494]\n",
      "MSE loss: 87.9072\n",
      "Iteration: 34100\n",
      "Gradient: [  7.7498  10.8007 -36.8258 -15.3385 194.73  ]\n",
      "Weights: [-4.8195  0.706  -1.1308  0.0702  0.1497]\n",
      "MSE loss: 87.84\n",
      "Iteration: 34200\n",
      "Gradient: [  7.4546  18.0619  -8.4422  40.5569 -51.6417]\n",
      "Weights: [-4.8118  0.7131 -1.1327  0.0702  0.1497]\n",
      "MSE loss: 88.1057\n",
      "Iteration: 34300\n",
      "Gradient: [  6.1967  16.8891  50.6335  92.7616 325.7566]\n",
      "Weights: [-4.8019  0.7168 -1.1365  0.0703  0.1499]\n",
      "MSE loss: 88.4627\n",
      "Iteration: 34400\n",
      "Gradient: [ -6.4209  10.1866 -25.1768  84.5696 378.4451]\n",
      "Weights: [-4.8131  0.7076 -1.1398  0.0715  0.1503]\n",
      "MSE loss: 87.7075\n",
      "Iteration: 34500\n",
      "Gradient: [   5.6019    8.892   -10.6829    6.4953 -179.3754]\n",
      "Weights: [-4.8025  0.7112 -1.1379  0.07    0.1503]\n",
      "MSE loss: 87.8853\n",
      "Iteration: 34600\n",
      "Gradient: [-2.2415 -6.8769 24.8797 47.8025 60.2683]\n",
      "Weights: [-4.8261  0.7225 -1.1382  0.0701  0.1503]\n",
      "MSE loss: 87.9936\n",
      "Iteration: 34700\n",
      "Gradient: [  6.3725  11.1884  14.2951  84.4119 173.402 ]\n",
      "Weights: [-4.8156  0.7133 -1.1392  0.0699  0.1502]\n",
      "MSE loss: 87.8073\n",
      "Iteration: 34800\n",
      "Gradient: [  -3.4644   -3.7809   -2.7667 -149.3907 -397.1769]\n",
      "Weights: [-4.8007  0.7049 -1.1366  0.0713  0.15  ]\n",
      "MSE loss: 87.9309\n",
      "Iteration: 34900\n",
      "Gradient: [ -3.5811   8.4792  15.0986  45.1708 -40.9403]\n",
      "Weights: [-4.8119  0.7041 -1.1381  0.0723  0.1498]\n",
      "MSE loss: 87.713\n",
      "Iteration: 35000\n",
      "Gradient: [  -5.9543  -10.5218  -24.2486 -107.9152   13.3038]\n",
      "Weights: [-4.8176  0.6975 -1.1398  0.0726  0.15  ]\n",
      "MSE loss: 88.3983\n",
      "Iteration: 35100\n",
      "Gradient: [  -7.3884  -14.7901  -14.401   -19.392  -137.0601]\n",
      "Weights: [-4.8104  0.7066 -1.1433  0.0727  0.1499]\n",
      "MSE loss: 87.8471\n",
      "Iteration: 35200\n",
      "Gradient: [ -7.3089   1.9889 -22.3739 -69.6641  29.4074]\n",
      "Weights: [-4.805   0.7057 -1.1424  0.0735  0.1499]\n",
      "MSE loss: 87.6443\n",
      "Iteration: 35300\n",
      "Gradient: [-0.9117 -1.4904  7.6434 61.8983 31.2986]\n",
      "Weights: [-4.8033  0.7095 -1.1443  0.0728  0.1499]\n",
      "MSE loss: 87.6797\n",
      "Iteration: 35400\n",
      "Gradient: [ -1.1689 -14.1871  -1.5908  20.9264 -18.2924]\n",
      "Weights: [-4.8015  0.7141 -1.1479  0.0738  0.15  ]\n",
      "MSE loss: 87.6572\n",
      "Iteration: 35500\n",
      "Gradient: [  9.764   13.685  -35.4903  68.0325   5.6075]\n",
      "Weights: [-4.8085  0.7157 -1.149   0.0742  0.1498]\n",
      "MSE loss: 87.6418\n",
      "Iteration: 35600\n",
      "Gradient: [  -3.8417   10.324   -11.0363  -35.2946 -240.5766]\n",
      "Weights: [-4.8168  0.7115 -1.1473  0.0741  0.1501]\n",
      "MSE loss: 87.7732\n",
      "Iteration: 35700\n",
      "Gradient: [-2.606000e-01  8.266900e+00  3.778000e+00  4.688170e+01 -2.660659e+02]\n",
      "Weights: [-4.8019  0.7162 -1.1491  0.0741  0.15  ]\n",
      "MSE loss: 87.6753\n",
      "Iteration: 35800\n",
      "Gradient: [  7.5114 -13.2657  14.5095  24.6807 162.8399]\n",
      "Weights: [-4.8047  0.7136 -1.152   0.074   0.1502]\n",
      "MSE loss: 87.7876\n",
      "Iteration: 35900\n",
      "Gradient: [ -6.0086  -4.8821 -44.0551 -48.6844  29.4621]\n",
      "Weights: [-4.7953  0.7033 -1.1516  0.0749  0.1502]\n",
      "MSE loss: 87.7855\n",
      "Iteration: 36000\n",
      "Gradient: [ 14.0901  27.5096 -21.2438  30.014  103.4712]\n",
      "Weights: [-4.779   0.7017 -1.1484  0.0757  0.1501]\n",
      "MSE loss: 88.6161\n",
      "Iteration: 36100\n",
      "Gradient: [ -10.4361   -2.4016  -53.4215   80.371  -129.1777]\n",
      "Weights: [-4.8012  0.6979 -1.1466  0.0753  0.15  ]\n",
      "MSE loss: 87.6997\n",
      "Iteration: 36200\n",
      "Gradient: [  5.4879 -17.7941  -9.8568  28.8546 122.461 ]\n",
      "Weights: [-4.7995  0.7069 -1.1446  0.0748  0.1498]\n",
      "MSE loss: 87.8672\n",
      "Iteration: 36300\n",
      "Gradient: [ -8.4083 -16.8388 -22.7822  11.8106  16.877 ]\n",
      "Weights: [-4.805   0.7009 -1.1447  0.0749  0.1498]\n",
      "MSE loss: 87.6664\n",
      "Iteration: 36400\n",
      "Gradient: [ -1.0274  -1.6466  13.8431 107.0514  97.1209]\n",
      "Weights: [-4.811   0.7121 -1.1467  0.0753  0.1496]\n",
      "MSE loss: 87.6218\n",
      "Iteration: 36500\n",
      "Gradient: [   0.4996   -7.1754   38.5132  110.7696 -296.9046]\n",
      "Weights: [-4.8004  0.7059 -1.1482  0.0757  0.1496]\n",
      "MSE loss: 87.6244\n",
      "Iteration: 36600\n",
      "Gradient: [  5.2412 -15.3504 -24.8381  73.3734  36.8509]\n",
      "Weights: [-4.7969  0.712  -1.1496  0.076   0.1496]\n",
      "MSE loss: 87.8197\n",
      "Iteration: 36700\n",
      "Gradient: [-1.890000e-01 -1.943580e+01 -2.582400e+01 -5.915850e+01  2.075544e+02]\n",
      "Weights: [-4.8089  0.7161 -1.1513  0.0752  0.1498]\n",
      "MSE loss: 87.6185\n",
      "Iteration: 36800\n",
      "Gradient: [  2.6484  15.0681  -2.4085   1.2715 221.0495]\n",
      "Weights: [-4.8014  0.7243 -1.1544  0.0762  0.1498]\n",
      "MSE loss: 87.9455\n",
      "Iteration: 36900\n",
      "Gradient: [   2.9009   14.7084  -20.5196   51.8111 -168.1139]\n",
      "Weights: [-4.8084  0.7304 -1.1546  0.0755  0.1496]\n",
      "MSE loss: 87.6974\n",
      "Iteration: 37000\n",
      "Gradient: [   5.6805   -1.8287   36.7054   47.4381 -140.4354]\n",
      "Weights: [-4.8004  0.7202 -1.1525  0.0752  0.1499]\n",
      "MSE loss: 87.7334\n",
      "Iteration: 37100\n",
      "Gradient: [  -5.5113   -8.5101  -28.3435  -97.6212 -313.0125]\n",
      "Weights: [-4.8186  0.7214 -1.1507  0.0747  0.15  ]\n",
      "MSE loss: 87.6314\n",
      "Iteration: 37200\n",
      "Gradient: [  3.4431  -6.4009  72.3372  44.5207 189.4417]\n",
      "Weights: [-4.8026  0.7135 -1.1507  0.0759  0.1499]\n",
      "MSE loss: 87.7412\n",
      "Iteration: 37300\n",
      "Gradient: [ -6.1414 -14.824  -40.0833 -22.7505 -96.3314]\n",
      "Weights: [-4.8096  0.7104 -1.1505  0.0757  0.1497]\n",
      "MSE loss: 87.7251\n",
      "Iteration: 37400\n",
      "Gradient: [ 7.4123 29.538  -0.353  95.8437 13.2092]\n",
      "Weights: [-4.8004  0.7132 -1.1506  0.0756  0.1497]\n",
      "MSE loss: 87.6248\n",
      "Iteration: 37500\n",
      "Gradient: [  -4.6352   -9.3459  -14.9275  -15.8549 -222.4333]\n",
      "Weights: [-4.809   0.7227 -1.1539  0.0756  0.1498]\n",
      "MSE loss: 87.5672\n",
      "Iteration: 37600\n",
      "Gradient: [   0.5757    3.557    34.3937   65.8743 -150.7578]\n",
      "Weights: [-4.7999  0.718  -1.1537  0.0766  0.1496]\n",
      "MSE loss: 87.6804\n",
      "Iteration: 37700\n",
      "Gradient: [ 9.4881 15.821  35.4192  0.2333 50.133 ]\n",
      "Weights: [-4.8063  0.7208 -1.1532  0.0764  0.1497]\n",
      "MSE loss: 87.6784\n",
      "Iteration: 37800\n",
      "Gradient: [ -2.2006  15.4848  -6.6039  27.9096 362.501 ]\n",
      "Weights: [-4.8213  0.7342 -1.1539  0.0759  0.1495]\n",
      "MSE loss: 87.6865\n",
      "Iteration: 37900\n",
      "Gradient: [ -5.5735   5.3255  -8.1224  32.4196 -57.9291]\n",
      "Weights: [-4.8306  0.7307 -1.1536  0.0764  0.1495]\n",
      "MSE loss: 87.7565\n",
      "Iteration: 38000\n",
      "Gradient: [ -8.6464  -9.9334  26.4024 -41.2911 -10.9175]\n",
      "Weights: [-4.8168  0.7282 -1.1566  0.0763  0.1494]\n",
      "MSE loss: 87.7459\n",
      "Iteration: 38100\n",
      "Gradient: [ -0.5831  15.1369 -23.2713   3.1644 141.2108]\n",
      "Weights: [-4.8013  0.7263 -1.158   0.077   0.1495]\n",
      "MSE loss: 87.6675\n",
      "Iteration: 38200\n",
      "Gradient: [  10.4041  -18.5181  -24.2182   39.8342 -186.4028]\n",
      "Weights: [-4.8194  0.731  -1.1587  0.0779  0.1495]\n",
      "MSE loss: 87.5662\n",
      "Iteration: 38300\n",
      "Gradient: [ -4.3563   8.9502   4.7838  45.4573 162.1719]\n",
      "Weights: [-4.8172  0.717  -1.1581  0.0789  0.1494]\n",
      "MSE loss: 87.7873\n",
      "Iteration: 38400\n",
      "Gradient: [ -6.0211   4.9317  26.0889  34.6084 182.3658]\n",
      "Weights: [-4.8121  0.7112 -1.1549  0.0792  0.1494]\n",
      "MSE loss: 87.6485\n",
      "Iteration: 38500\n",
      "Gradient: [   8.0354  -12.4234   15.5654   72.9662 -105.6171]\n",
      "Weights: [-4.7941  0.7085 -1.1574  0.0794  0.1493]\n",
      "MSE loss: 87.5907\n",
      "Iteration: 38600\n",
      "Gradient: [10.9349 11.5198 14.2184 32.2196 29.2073]\n",
      "Weights: [-4.8044  0.7209 -1.1578  0.0792  0.1492]\n",
      "MSE loss: 87.6175\n",
      "Iteration: 38700\n",
      "Gradient: [ -0.5842   5.0297  41.5414 -88.8102 -18.9791]\n",
      "Weights: [-4.8108  0.7133 -1.1583  0.0799  0.1493]\n",
      "MSE loss: 87.6281\n",
      "Iteration: 38800\n",
      "Gradient: [   4.7541  -10.2514  -21.6737 -114.2289  114.8113]\n",
      "Weights: [-4.8091  0.7274 -1.1622  0.0791  0.1494]\n",
      "MSE loss: 87.4927\n",
      "Iteration: 38900\n",
      "Gradient: [  6.5347 -17.1589 -23.2652 -14.7192  55.4406]\n",
      "Weights: [-4.8123  0.7291 -1.1615  0.079   0.1492]\n",
      "MSE loss: 87.517\n",
      "Iteration: 39000\n",
      "Gradient: [  -2.6504   -1.4252  -28.5917    4.0433 -220.2537]\n",
      "Weights: [-4.8264  0.7391 -1.1626  0.0789  0.1492]\n",
      "MSE loss: 87.574\n",
      "Iteration: 39100\n",
      "Gradient: [   6.2047   -5.6539   11.2636   74.2598 -196.3279]\n",
      "Weights: [-4.8235  0.7326 -1.1609  0.0792  0.1493]\n",
      "MSE loss: 87.5743\n",
      "Iteration: 39200\n",
      "Gradient: [ -8.5149  11.3028 -20.4355  45.1562 -17.9348]\n",
      "Weights: [-4.8271  0.7425 -1.1625  0.0786  0.1493]\n",
      "MSE loss: 87.5986\n",
      "Iteration: 39300\n",
      "Gradient: [   7.7053   -4.8042  -12.7257  -56.1689 -240.3571]\n",
      "Weights: [-4.8223  0.7477 -1.1649  0.078   0.1495]\n",
      "MSE loss: 87.5631\n",
      "Iteration: 39400\n",
      "Gradient: [ 10.1033  18.0538 -16.3609  15.5175 162.5477]\n",
      "Weights: [-4.8186  0.7489 -1.166   0.0782  0.1494]\n",
      "MSE loss: 87.6093\n",
      "Iteration: 39500\n",
      "Gradient: [  9.7694 -11.9807  29.1353  88.023  217.6243]\n",
      "Weights: [-4.8139  0.7456 -1.1661  0.0777  0.1497]\n",
      "MSE loss: 87.6101\n",
      "Iteration: 39600\n",
      "Gradient: [  -4.2668   -3.9599  -14.4417   37.7478 -203.2284]\n",
      "Weights: [-4.841   0.7579 -1.1703  0.0777  0.1498]\n",
      "MSE loss: 87.8373\n",
      "Iteration: 39700\n",
      "Gradient: [ -7.6003  -5.7157  25.9414 -53.7572 423.5021]\n",
      "Weights: [-4.8321  0.7535 -1.1711  0.078   0.15  ]\n",
      "MSE loss: 87.574\n",
      "Iteration: 39800\n",
      "Gradient: [ -11.5407    6.4213  -19.853  -104.6742  -15.2351]\n",
      "Weights: [-4.8296  0.7537 -1.1694  0.0784  0.1498]\n",
      "MSE loss: 87.539\n",
      "Iteration: 39900\n",
      "Gradient: [-1.58000e-02  8.29640e+00  6.47782e+01  5.02810e+00  2.81237e+01]\n",
      "Weights: [-4.8217  0.7485 -1.1701  0.0789  0.1498]\n",
      "MSE loss: 87.4736\n",
      "Iteration: 40000\n",
      "Gradient: [ 2.489100e+00  1.076090e+01  7.230000e-02  2.863480e+01 -2.514311e+02]\n",
      "Weights: [-4.8133  0.7398 -1.1699  0.0792  0.1499]\n",
      "MSE loss: 87.4458\n",
      "Iteration: 40100\n",
      "Gradient: [   3.9178   -9.9492  -36.8677 -104.1487  -17.9117]\n",
      "Weights: [-4.8035  0.7379 -1.1708  0.0785  0.1502]\n",
      "MSE loss: 87.5207\n",
      "Iteration: 40200\n",
      "Gradient: [ 10.7972   6.1035   7.807   33.2855 231.5675]\n",
      "Weights: [-4.8054  0.7441 -1.1717  0.0787  0.1499]\n",
      "MSE loss: 87.5713\n",
      "Iteration: 40300\n",
      "Gradient: [   1.6467    5.4067    6.4371  -10.5073 -140.1728]\n",
      "Weights: [-4.8195  0.7388 -1.1712  0.0789  0.1501]\n",
      "MSE loss: 87.6278\n",
      "Iteration: 40400\n",
      "Gradient: [  -7.3778    7.8583   -6.4581  -38.8536 -342.6781]\n",
      "Weights: [-4.797   0.7355 -1.1702  0.0793  0.15  ]\n",
      "MSE loss: 87.7197\n",
      "Iteration: 40500\n",
      "Gradient: [ 11.0851 -10.7446   0.6403  92.7193 306.325 ]\n",
      "Weights: [-4.8103  0.7429 -1.1729  0.0801  0.1501]\n",
      "MSE loss: 87.5958\n",
      "Iteration: 40600\n",
      "Gradient: [7.66200e+00 3.89000e-02 2.33642e+01 9.11744e+01 7.06623e+01]\n",
      "Weights: [-4.8037  0.743  -1.176   0.0798  0.1502]\n",
      "MSE loss: 87.5048\n",
      "Iteration: 40700\n",
      "Gradient: [-0.9964  8.7231  2.7463 74.9348 52.2256]\n",
      "Weights: [-4.8025  0.7391 -1.1736  0.0798  0.1501]\n",
      "MSE loss: 87.5456\n",
      "Iteration: 40800\n",
      "Gradient: [ -5.816   -6.2531  -9.688  -21.4232  29.7953]\n",
      "Weights: [-4.8109  0.7353 -1.1727  0.0803  0.1501]\n",
      "MSE loss: 87.4605\n",
      "Iteration: 40900\n",
      "Gradient: [ -9.601   24.7058 -10.1706  37.0285 295.4138]\n",
      "Weights: [-4.8236  0.7312 -1.1684  0.0798  0.1501]\n",
      "MSE loss: 87.7567\n",
      "Iteration: 41000\n",
      "Gradient: [-0.8417 15.0422 21.0849 51.1755 91.9336]\n",
      "Weights: [-4.8284  0.7469 -1.1702  0.0793  0.1499]\n",
      "MSE loss: 87.5445\n",
      "Iteration: 41100\n",
      "Gradient: [   4.0489   -1.3226  -34.5821 -123.0216   26.3482]\n",
      "Weights: [-4.8128  0.7447 -1.1733  0.0794  0.1499]\n",
      "MSE loss: 87.4631\n",
      "Iteration: 41200\n",
      "Gradient: [  2.916  -29.7413  31.4176 -68.5134  -1.0499]\n",
      "Weights: [-4.815   0.7429 -1.1718  0.0794  0.1499]\n",
      "MSE loss: 87.4493\n",
      "Iteration: 41300\n",
      "Gradient: [  4.6255   3.429  -16.8349 -49.2739 123.7255]\n",
      "Weights: [-4.8059  0.7479 -1.1735  0.0786  0.15  ]\n",
      "MSE loss: 87.59\n",
      "Iteration: 41400\n",
      "Gradient: [  0.9557   3.1254  11.552  -32.3378 141.2337]\n",
      "Weights: [-4.8264  0.7563 -1.1727  0.0783  0.1502]\n",
      "MSE loss: 87.5094\n",
      "Iteration: 41500\n",
      "Gradient: [  1.7147  -5.2384  13.5675 -32.8306 105.8083]\n",
      "Weights: [-4.827   0.7509 -1.1754  0.0784  0.1502]\n",
      "MSE loss: 87.7934\n",
      "Iteration: 41600\n",
      "Gradient: [  -7.8832    0.5689  -19.3216  -49.5561 -237.338 ]\n",
      "Weights: [-4.8267  0.7517 -1.1761  0.0788  0.1504]\n",
      "MSE loss: 87.5599\n",
      "Iteration: 41700\n",
      "Gradient: [  -5.2591  -11.7099   17.3683   -5.632  -190.9364]\n",
      "Weights: [-4.8182  0.7537 -1.1779  0.0789  0.1504]\n",
      "MSE loss: 87.4176\n",
      "Iteration: 41800\n",
      "Gradient: [  4.2386   1.643    1.8005  80.4669 343.6227]\n",
      "Weights: [-4.8217  0.7609 -1.1796  0.0789  0.1503]\n",
      "MSE loss: 87.4211\n",
      "Iteration: 41900\n",
      "Gradient: [ 17.054    1.8035  55.8894 114.655  143.6987]\n",
      "Weights: [-4.8174  0.7686 -1.1811  0.0797  0.1502]\n",
      "MSE loss: 87.8517\n",
      "Iteration: 42000\n",
      "Gradient: [  -7.2957  -11.9474   -7.9076   23.3537 -155.2568]\n",
      "Weights: [-4.8285  0.7632 -1.1803  0.0797  0.1503]\n",
      "MSE loss: 87.4242\n",
      "Iteration: 42100\n",
      "Gradient: [ 3.1021  3.407  26.7346 61.2827 28.1823]\n",
      "Weights: [-4.8274  0.7656 -1.1824  0.0804  0.1502]\n",
      "MSE loss: 87.42\n",
      "Iteration: 42200\n",
      "Gradient: [   6.7491    7.7702    1.0757    9.3482 -262.6199]\n",
      "Weights: [-4.8071  0.7571 -1.1843  0.081   0.1501]\n",
      "MSE loss: 87.4952\n",
      "Iteration: 42300\n",
      "Gradient: [  -0.971     8.9978   22.1044  -86.8027 -107.1801]\n",
      "Weights: [-4.825   0.7653 -1.1857  0.0816  0.1501]\n",
      "MSE loss: 87.3647\n",
      "Iteration: 42400\n",
      "Gradient: [-3.5931  1.1722  8.9211 85.8245 28.0435]\n",
      "Weights: [-4.8311  0.7694 -1.1894  0.0824  0.1501]\n",
      "MSE loss: 87.4207\n",
      "Iteration: 42500\n",
      "Gradient: [-3.2489 -9.4221 34.2675 42.8066 18.9795]\n",
      "Weights: [-4.8162  0.7602 -1.1893  0.0819  0.1504]\n",
      "MSE loss: 87.4018\n",
      "Iteration: 42600\n",
      "Gradient: [ 10.9573  17.2251  -1.361  -97.6235 130.9492]\n",
      "Weights: [-4.805   0.7593 -1.1893  0.0818  0.1507]\n",
      "MSE loss: 87.6089\n",
      "Iteration: 42700\n",
      "Gradient: [  1.2508  -0.5961  -2.318  -37.4776  38.0625]\n",
      "Weights: [-4.8073  0.7577 -1.1888  0.082   0.1505]\n",
      "MSE loss: 87.4381\n",
      "Iteration: 42800\n",
      "Gradient: [  9.0613   7.4805  31.3266  66.4105 476.6296]\n",
      "Weights: [-4.8112  0.762  -1.1876  0.0817  0.1506]\n",
      "MSE loss: 87.6106\n",
      "Iteration: 42900\n",
      "Gradient: [ -4.2319 -15.0095   4.6093   0.3176  29.0341]\n",
      "Weights: [-4.8198  0.761  -1.1871  0.0818  0.1504]\n",
      "MSE loss: 87.3641\n",
      "Iteration: 43000\n",
      "Gradient: [  -7.0149  -17.3966   24.0395 -122.1929 -100.2931]\n",
      "Weights: [-4.8241  0.7617 -1.1863  0.0815  0.1506]\n",
      "MSE loss: 87.4408\n",
      "Iteration: 43100\n",
      "Gradient: [  4.0044   0.4158  27.9292 -14.5101 -43.1659]\n",
      "Weights: [-4.8439  0.7704 -1.1879  0.081   0.1505]\n",
      "MSE loss: 87.8945\n",
      "Iteration: 43200\n",
      "Gradient: [  9.4202   9.296  -21.6332 -52.6218 -50.615 ]\n",
      "Weights: [-4.8153  0.7617 -1.1866  0.0805  0.1505]\n",
      "MSE loss: 87.3972\n",
      "Iteration: 43300\n",
      "Gradient: [  6.3744  18.0085  18.8695 -10.6143   0.4739]\n",
      "Weights: [-4.812   0.7579 -1.1869  0.0814  0.1503]\n",
      "MSE loss: 87.408\n",
      "Iteration: 43400\n",
      "Gradient: [  11.978    -4.7204  -34.4351 -101.494  -320.7756]\n",
      "Weights: [-4.8115  0.7645 -1.1879  0.0809  0.1504]\n",
      "MSE loss: 87.4456\n",
      "Iteration: 43500\n",
      "Gradient: [ -0.9897 -10.4305 -34.7136 -14.4124 -18.8217]\n",
      "Weights: [-4.8246  0.7694 -1.1864  0.0803  0.1504]\n",
      "MSE loss: 87.3815\n",
      "Iteration: 43600\n",
      "Gradient: [  -1.3447   -9.9987   14.6193 -133.9131  -24.8838]\n",
      "Weights: [-4.8224  0.7636 -1.1871  0.0803  0.1507]\n",
      "MSE loss: 87.4141\n",
      "Iteration: 43700\n",
      "Gradient: [  -0.6413  -12.0879    9.5598   24.7576 -201.1955]\n",
      "Weights: [-4.8289  0.7643 -1.1866  0.0805  0.1506]\n",
      "MSE loss: 87.5311\n",
      "Iteration: 43800\n",
      "Gradient: [   0.6941  -22.4457  -23.2522  -51.4314 -200.5037]\n",
      "Weights: [-4.8229  0.7682 -1.1869  0.0806  0.1506]\n",
      "MSE loss: 87.4385\n",
      "Iteration: 43900\n",
      "Gradient: [ -1.4141   5.0106 -23.5197  74.7912 193.0625]\n",
      "Weights: [-4.8078  0.7555 -1.1871  0.0813  0.1506]\n",
      "MSE loss: 87.4219\n",
      "Iteration: 44000\n",
      "Gradient: [  2.6327   4.8861  16.3672  99.9356 452.305 ]\n",
      "Weights: [-4.811   0.764  -1.1865  0.0808  0.1507]\n",
      "MSE loss: 87.7257\n",
      "Iteration: 44100\n",
      "Gradient: [ 18.1418  16.6426   7.5871  12.1502 381.5062]\n",
      "Weights: [-4.8188  0.7715 -1.1883  0.0811  0.1504]\n",
      "MSE loss: 87.514\n",
      "Iteration: 44200\n",
      "Gradient: [   5.5397   18.8206    0.1551  -52.4692 -127.5471]\n",
      "Weights: [-4.8167  0.7634 -1.1869  0.0811  0.1503]\n",
      "MSE loss: 87.3738\n",
      "Iteration: 44300\n",
      "Gradient: [-5.130000e-02  3.306000e-01  6.051700e+00 -5.099500e+01  3.199699e+02]\n",
      "Weights: [-4.8062  0.758  -1.1874  0.0818  0.1505]\n",
      "MSE loss: 87.5802\n",
      "Iteration: 44400\n",
      "Gradient: [  7.8912  -1.4515  -6.8863 103.0773 162.663 ]\n",
      "Weights: [-4.8215  0.7669 -1.1889  0.0811  0.1505]\n",
      "MSE loss: 87.3586\n",
      "Iteration: 44500\n",
      "Gradient: [  4.1685   4.8299  22.0098  21.7018 173.707 ]\n",
      "Weights: [-4.8159  0.7626 -1.1903  0.0819  0.1507]\n",
      "MSE loss: 87.3838\n",
      "Iteration: 44600\n",
      "Gradient: [ 4.36000e-02 -2.16761e+01 -1.75826e+01 -3.33412e+01  6.34764e+01]\n",
      "Weights: [-4.834   0.7681 -1.1921  0.0825  0.1506]\n",
      "MSE loss: 87.6013\n",
      "Iteration: 44700\n",
      "Gradient: [   3.6362   -4.2853   -4.8405  -41.2471 -136.6193]\n",
      "Weights: [-4.8393  0.7789 -1.1948  0.0821  0.1505]\n",
      "MSE loss: 87.6553\n",
      "Iteration: 44800\n",
      "Gradient: [ -5.3475   2.3756 -28.2266  80.9359 -96.7804]\n",
      "Weights: [-4.8154  0.7698 -1.1941  0.0826  0.1505]\n",
      "MSE loss: 87.3623\n",
      "Iteration: 44900\n",
      "Gradient: [-1.398070e+01 -2.024000e+00 -3.140910e+01 -2.732000e-01 -2.735018e+02]\n",
      "Weights: [-4.8274  0.7801 -1.1979  0.0834  0.1504]\n",
      "MSE loss: 87.3092\n",
      "Iteration: 45000\n",
      "Gradient: [ 16.5669  31.0584  22.8269   4.6465 156.5688]\n",
      "Weights: [-4.7972  0.7711 -1.1979  0.0834  0.1504]\n",
      "MSE loss: 87.9284\n",
      "Iteration: 45100\n",
      "Gradient: [   3.7656    7.8387  -15.0822   -2.6102 -106.246 ]\n",
      "Weights: [-4.8212  0.7747 -1.1976  0.0838  0.1503]\n",
      "MSE loss: 87.3139\n",
      "Iteration: 45200\n",
      "Gradient: [ 13.5209  10.9623 -26.8672 -50.9174 222.9826]\n",
      "Weights: [-4.8211  0.7837 -1.2011  0.0845  0.1502]\n",
      "MSE loss: 87.342\n",
      "Iteration: 45300\n",
      "Gradient: [   5.5001   -5.502    52.1242   82.0872 -165.6766]\n",
      "Weights: [-4.8367  0.7875 -1.2007  0.085   0.15  ]\n",
      "MSE loss: 87.3278\n",
      "Iteration: 45400\n",
      "Gradient: [  4.4985  -4.585    7.2757 -60.8729 214.41  ]\n",
      "Weights: [-4.8157  0.7831 -1.2038  0.085   0.1502]\n",
      "MSE loss: 87.3697\n",
      "Iteration: 45500\n",
      "Gradient: [  -2.9977    8.2168  -24.0227 -123.6203  352.1202]\n",
      "Weights: [-4.8095  0.772  -1.2051  0.086   0.1505]\n",
      "MSE loss: 87.3628\n",
      "Iteration: 45600\n",
      "Gradient: [-6.3035 -1.4026 46.6721 18.9332 71.7312]\n",
      "Weights: [-4.8073  0.77   -1.2043  0.0865  0.1505]\n",
      "MSE loss: 87.4308\n",
      "Iteration: 45700\n",
      "Gradient: [ -6.1484   5.2599 -27.6358  60.9843 -31.3371]\n",
      "Weights: [-4.8219  0.7837 -1.2058  0.0858  0.1503]\n",
      "MSE loss: 87.2726\n",
      "Iteration: 45800\n",
      "Gradient: [ -2.4208   1.1299 -12.0965 -25.9404 -23.7283]\n",
      "Weights: [-4.8222  0.7828 -1.2067  0.0861  0.1506]\n",
      "MSE loss: 87.33\n",
      "Iteration: 45900\n",
      "Gradient: [-5.0829  5.9519  0.2205 33.1201 52.9035]\n",
      "Weights: [-4.8271  0.7705 -1.2033  0.0864  0.1504]\n",
      "MSE loss: 87.5542\n",
      "Iteration: 46000\n",
      "Gradient: [ -7.5476  -1.6616 -59.0289 -17.1128  40.1182]\n",
      "Weights: [-4.8222  0.7722 -1.203   0.0866  0.1503]\n",
      "MSE loss: 87.3363\n",
      "Iteration: 46100\n",
      "Gradient: [ 6.8669 12.0206 28.5851 34.724  50.6075]\n",
      "Weights: [-4.8059  0.7753 -1.2018  0.086   0.1502]\n",
      "MSE loss: 87.8354\n",
      "Iteration: 46200\n",
      "Gradient: [ -0.4965  -3.765   -1.1063  54.7624 -22.6517]\n",
      "Weights: [-4.8153  0.7764 -1.2038  0.0867  0.1501]\n",
      "MSE loss: 87.3261\n",
      "Iteration: 46300\n",
      "Gradient: [ -7.7013   3.0078 -40.1389   7.6962 235.8037]\n",
      "Weights: [-4.8308  0.7789 -1.2038  0.0866  0.1501]\n",
      "MSE loss: 87.3743\n",
      "Iteration: 46400\n",
      "Gradient: [  9.5562 -10.9416   5.7816 120.9164 369.2112]\n",
      "Weights: [-4.8279  0.7863 -1.2038  0.0861  0.1501]\n",
      "MSE loss: 87.3609\n",
      "Iteration: 46500\n",
      "Gradient: [ 13.9045  11.2437  42.4072 -77.9119 133.4677]\n",
      "Weights: [-4.8309  0.7813 -1.2017  0.0858  0.1501]\n",
      "MSE loss: 87.3182\n",
      "Iteration: 46600\n",
      "Gradient: [   1.6966   -4.4891  -40.0178   10.3873 -334.1447]\n",
      "Weights: [-4.817   0.7765 -1.2025  0.085   0.1501]\n",
      "MSE loss: 87.4213\n",
      "Iteration: 46700\n",
      "Gradient: [11.3115 -6.0871 37.0922 50.6192 50.6011]\n",
      "Weights: [-4.811   0.7673 -1.2014  0.086   0.1501]\n",
      "MSE loss: 87.3632\n",
      "Iteration: 46800\n",
      "Gradient: [ 20.1574  27.6609  45.3977 169.4951  19.8469]\n",
      "Weights: [-4.793   0.7734 -1.2009  0.0857  0.1501]\n",
      "MSE loss: 88.5667\n",
      "Iteration: 46900\n",
      "Gradient: [ -2.6316  -9.2767  21.6497 -51.1963 157.1561]\n",
      "Weights: [-4.824   0.7699 -1.2001  0.0858  0.1502]\n",
      "MSE loss: 87.3698\n",
      "Iteration: 47000\n",
      "Gradient: [   3.587     0.7828    6.4055   83.7008 -329.926 ]\n",
      "Weights: [-4.8165  0.7777 -1.202   0.085   0.1502]\n",
      "MSE loss: 87.3186\n",
      "Iteration: 47100\n",
      "Gradient: [ -3.5645   9.8861  -0.3387  34.3139 -45.5339]\n",
      "Weights: [-4.827   0.7816 -1.2013  0.0848  0.1502]\n",
      "MSE loss: 87.2856\n",
      "Iteration: 47200\n",
      "Gradient: [   0.689   -14.1412    4.1021   29.6493 -211.3841]\n",
      "Weights: [-4.8114  0.7653 -1.2015  0.0865  0.1502]\n",
      "MSE loss: 87.3458\n",
      "Iteration: 47300\n",
      "Gradient: [  5.7768  10.6853  44.9104  71.8783 196.9186]\n",
      "Weights: [-4.8178  0.7703 -1.2027  0.0869  0.1501]\n",
      "MSE loss: 87.3072\n",
      "Iteration: 47400\n",
      "Gradient: [  3.0017   2.3892 -12.0128 -38.9053  30.2647]\n",
      "Weights: [-4.8024  0.7753 -1.2054  0.0868  0.15  ]\n",
      "MSE loss: 87.5888\n",
      "Iteration: 47500\n",
      "Gradient: [  -4.3228   10.0489  -29.8931 -111.4461  -72.4749]\n",
      "Weights: [-4.8216  0.7847 -1.2098  0.0877  0.1498]\n",
      "MSE loss: 87.341\n",
      "Iteration: 47600\n",
      "Gradient: [  3.1381 -16.8664 -19.4549  51.5795 157.3692]\n",
      "Weights: [-4.8484  0.8017 -1.2102  0.0878  0.1498]\n",
      "MSE loss: 87.4102\n",
      "Iteration: 47700\n",
      "Gradient: [   1.2054  -15.8887  -37.1846  -68.3614 -143.5539]\n",
      "Weights: [-4.8415  0.7993 -1.2117  0.0882  0.1497]\n",
      "MSE loss: 87.3059\n",
      "Iteration: 47800\n",
      "Gradient: [   5.0577   -3.6327   14.4549  -60.7903 -131.6894]\n",
      "Weights: [-4.8211  0.7863 -1.2118  0.0886  0.1501]\n",
      "MSE loss: 87.2519\n",
      "Iteration: 47900\n",
      "Gradient: [ 9.9323 15.4609 11.8644 -8.7475 19.5813]\n",
      "Weights: [-4.8075  0.7762 -1.2132  0.0892  0.1504]\n",
      "MSE loss: 87.4218\n",
      "Iteration: 48000\n",
      "Gradient: [  4.5213  -5.32   -22.4319  19.4385  61.9987]\n",
      "Weights: [-4.836   0.789  -1.2137  0.0881  0.1504]\n",
      "MSE loss: 87.5005\n",
      "Iteration: 48100\n",
      "Gradient: [  1.4041   3.3898 -37.6664  53.7597 296.601 ]\n",
      "Weights: [-4.8197  0.7869 -1.2098  0.0874  0.1503]\n",
      "MSE loss: 87.3458\n",
      "Iteration: 48200\n",
      "Gradient: [   3.0697   11.9596  -24.0989   16.8035 -107.6858]\n",
      "Weights: [-4.8232  0.7936 -1.2111  0.0867  0.1502]\n",
      "MSE loss: 87.2716\n",
      "Iteration: 48300\n",
      "Gradient: [  7.6704   7.5978   6.4596  99.3882 172.4601]\n",
      "Weights: [-4.8182  0.7993 -1.2126  0.087   0.1502]\n",
      "MSE loss: 87.6047\n",
      "Iteration: 48400\n",
      "Gradient: [-11.8002  13.6982  -7.1887  -3.6826 435.8675]\n",
      "Weights: [-4.8374  0.8013 -1.2098  0.087   0.15  ]\n",
      "MSE loss: 87.3273\n",
      "Iteration: 48500\n",
      "Gradient: [  -3.3691   -6.1613  -39.1982  -37.6181 -101.4426]\n",
      "Weights: [-4.8325  0.7963 -1.21    0.0869  0.15  ]\n",
      "MSE loss: 87.2452\n",
      "Iteration: 48600\n",
      "Gradient: [  4.5222   1.1955  -1.5141  77.6324 -35.5099]\n",
      "Weights: [-4.8456  0.8116 -1.2122  0.087   0.15  ]\n",
      "MSE loss: 87.3955\n",
      "Iteration: 48700\n",
      "Gradient: [ -5.9035   4.7041  18.3352 -85.8396 -83.4511]\n",
      "Weights: [-4.8397  0.8071 -1.2096  0.0858  0.15  ]\n",
      "MSE loss: 87.3236\n",
      "Iteration: 48800\n",
      "Gradient: [  -1.5205    9.2189   -9.7767   16.9626 -145.3886]\n",
      "Weights: [-4.8393  0.803  -1.2104  0.0865  0.1499]\n",
      "MSE loss: 87.2832\n",
      "Iteration: 48900\n",
      "Gradient: [  2.0585 -10.4065  -9.5721  97.7044 -54.8191]\n",
      "Weights: [-4.8399  0.8083 -1.2109  0.0869  0.1497]\n",
      "MSE loss: 87.3343\n",
      "Iteration: 49000\n",
      "Gradient: [ -3.0854 -10.789    6.666  -22.0862 -52.8084]\n",
      "Weights: [-4.8523  0.8081 -1.2123  0.087   0.1498]\n",
      "MSE loss: 87.6252\n",
      "Iteration: 49100\n",
      "Gradient: [  3.2482  -0.4355 -13.0921  28.6095 218.2889]\n",
      "Weights: [-4.8361  0.81   -1.2129  0.0868  0.1499]\n",
      "MSE loss: 87.3379\n",
      "Iteration: 49200\n",
      "Gradient: [  -2.5548   -3.6989  -49.9909  -72.8578 -122.7959]\n",
      "Weights: [-4.8535  0.8037 -1.213   0.0873  0.1498]\n",
      "MSE loss: 88.0961\n",
      "Iteration: 49300\n",
      "Gradient: [  7.6607   4.8114   0.8155 -40.3364 135.7632]\n",
      "Weights: [-4.8383  0.8038 -1.2114  0.0872  0.1498]\n",
      "MSE loss: 87.2619\n",
      "Iteration: 49400\n",
      "Gradient: [ -16.1797    2.6627   14.1348   86.2334 -227.8333]\n",
      "Weights: [-4.811   0.7848 -1.2089  0.0865  0.1502]\n",
      "MSE loss: 87.4309\n",
      "Iteration: 49500\n",
      "Gradient: [ 3.4009  2.5874 -1.1958 21.7128 29.5345]\n",
      "Weights: [-4.8298  0.7901 -1.2093  0.0868  0.1503]\n",
      "MSE loss: 87.2646\n",
      "Iteration: 49600\n",
      "Gradient: [ -4.8938   4.2406   2.4345 -22.5683 -59.1063]\n",
      "Weights: [-4.837   0.7814 -1.2078  0.0874  0.1504]\n",
      "MSE loss: 87.5964\n",
      "Iteration: 49700\n",
      "Gradient: [  -5.2922   -3.6702    6.0718  -56.8918 -201.9997]\n",
      "Weights: [-4.8238  0.7825 -1.2111  0.0877  0.1504]\n",
      "MSE loss: 87.3137\n",
      "Iteration: 49800\n",
      "Gradient: [ -4.7704  14.7764 -20.3198 -66.1314  77.795 ]\n",
      "Weights: [-4.8142  0.782  -1.2123  0.0878  0.1502]\n",
      "MSE loss: 87.355\n",
      "Iteration: 49900\n",
      "Gradient: [  1.3462  -5.5966  -0.7733  46.7475 -62.4645]\n",
      "Weights: [-4.8193  0.7799 -1.2096  0.0882  0.1502]\n",
      "MSE loss: 87.2754\n",
      "Iteration: 50000\n",
      "Gradient: [-10.3218  -9.4145  25.7963 -45.4714  23.262 ]\n",
      "Weights: [-4.8206  0.7782 -1.2099  0.0877  0.1503]\n",
      "MSE loss: 87.3795\n",
      "Iteration: 50100\n",
      "Gradient: [   9.1809   -2.8637    0.5688  -75.0649 -135.0448]\n",
      "Weights: [-4.824   0.7895 -1.2127  0.088   0.15  ]\n",
      "MSE loss: 87.2738\n",
      "Iteration: 50200\n",
      "Gradient: [ 3.4852  4.4945 -1.1266 58.9796 40.0432]\n",
      "Weights: [-4.8273  0.7904 -1.2106  0.0875  0.1501]\n",
      "MSE loss: 87.235\n",
      "Iteration: 50300\n",
      "Gradient: [ -0.68    14.1534 -27.6533 -42.5085 177.5256]\n",
      "Weights: [-4.8245  0.789  -1.2134  0.0874  0.1503]\n",
      "MSE loss: 87.3851\n",
      "Iteration: 50400\n",
      "Gradient: [  -2.0231    6.5512   26.7516 -188.9671   58.4824]\n",
      "Weights: [-4.8451  0.8045 -1.2163  0.0872  0.1503]\n",
      "MSE loss: 87.5648\n",
      "Iteration: 50500\n",
      "Gradient: [ -0.8164  -6.9282 -33.005   73.0216  64.3931]\n",
      "Weights: [-4.8146  0.7952 -1.2171  0.0879  0.1504]\n",
      "MSE loss: 87.3766\n",
      "Iteration: 50600\n",
      "Gradient: [   7.4977   -5.3998  -23.0242  111.6586 -111.9844]\n",
      "Weights: [-4.7988  0.7885 -1.2174  0.0879  0.1505]\n",
      "MSE loss: 87.796\n",
      "Iteration: 50700\n",
      "Gradient: [  -5.1966   13.0543   -6.6027  -41.3402 -178.6582]\n",
      "Weights: [-4.8253  0.7887 -1.2153  0.088   0.1506]\n",
      "MSE loss: 87.3022\n",
      "Iteration: 50800\n",
      "Gradient: [ -0.7081  12.1775 -10.7991  85.5857 -59.0569]\n",
      "Weights: [-4.8147  0.7879 -1.214   0.0873  0.1507]\n",
      "MSE loss: 87.3887\n",
      "Iteration: 50900\n",
      "Gradient: [  9.1867   3.9814 -11.1261 -97.4559  90.9514]\n",
      "Weights: [-4.8204  0.7967 -1.2163  0.0862  0.1509]\n",
      "MSE loss: 87.2979\n",
      "Iteration: 51000\n",
      "Gradient: [-5.0448  8.351  23.775  63.9372 33.0684]\n",
      "Weights: [-4.8452  0.8033 -1.217   0.0869  0.1508]\n",
      "MSE loss: 87.5211\n",
      "Iteration: 51100\n",
      "Gradient: [  4.8587 -12.9954 -48.7834  49.6285 -80.8473]\n",
      "Weights: [-4.8333  0.8087 -1.2196  0.0876  0.1507]\n",
      "MSE loss: 87.3387\n",
      "Iteration: 51200\n",
      "Gradient: [-11.3373   8.8483 -27.2008 -45.554   63.9582]\n",
      "Weights: [-4.8322  0.8096 -1.2201  0.0873  0.1507]\n",
      "MSE loss: 87.2567\n",
      "Iteration: 51300\n",
      "Gradient: [-5.77540e+00  7.94570e+00 -1.02022e+01  2.00000e-03  9.31002e+01]\n",
      "Weights: [-4.844   0.8186 -1.2211  0.0869  0.1507]\n",
      "MSE loss: 87.285\n",
      "Iteration: 51400\n",
      "Gradient: [ -2.6982   9.8788  92.9081  42.5401 144.849 ]\n",
      "Weights: [-4.8355  0.8207 -1.2187  0.0865  0.1503]\n",
      "MSE loss: 87.4409\n",
      "Iteration: 51500\n",
      "Gradient: [ -9.5777   1.3612  25.9062  15.6816 239.3572]\n",
      "Weights: [-4.8642  0.8296 -1.2183  0.0867  0.1503]\n",
      "MSE loss: 87.5784\n",
      "Iteration: 51600\n",
      "Gradient: [   6.2476    1.1065  -42.8918   -8.3957 -367.9884]\n",
      "Weights: [-4.8498  0.8249 -1.2206  0.0879  0.1503]\n",
      "MSE loss: 87.4758\n",
      "Iteration: 51700\n",
      "Gradient: [   4.0424    1.2517   -6.4321   17.0815 -210.4611]\n",
      "Weights: [-4.8453  0.823  -1.219   0.0871  0.1501]\n",
      "MSE loss: 87.3276\n",
      "Iteration: 51800\n",
      "Gradient: [  0.5665 -11.5233  31.6521  -2.6707 -52.6653]\n",
      "Weights: [-4.8501  0.8244 -1.2202  0.087   0.1501]\n",
      "MSE loss: 87.3305\n",
      "Iteration: 51900\n",
      "Gradient: [ 5.5202 15.5283 20.7092 -1.8471 93.0736]\n",
      "Weights: [-4.8373  0.8221 -1.2246  0.0885  0.1502]\n",
      "MSE loss: 87.2617\n",
      "Iteration: 52000\n",
      "Gradient: [-10.7343   5.5668   1.0768 -27.1277  59.0365]\n",
      "Weights: [-4.8493  0.8181 -1.2237  0.0885  0.1504]\n",
      "MSE loss: 87.3736\n",
      "Iteration: 52100\n",
      "Gradient: [ -8.8347  11.1145  17.8073 -77.055   24.4608]\n",
      "Weights: [-4.8462  0.8118 -1.2243  0.0882  0.1504]\n",
      "MSE loss: 88.009\n",
      "Iteration: 52200\n",
      "Gradient: [  -6.3268   -5.7407   -5.6613  -29.6986 -263.262 ]\n",
      "Weights: [-4.8368  0.8145 -1.2252  0.088   0.1506]\n",
      "MSE loss: 87.3217\n",
      "Iteration: 52300\n",
      "Gradient: [ 13.8758  -4.6473  -4.2488   0.5856 -46.7966]\n",
      "Weights: [-4.8196  0.8069 -1.2233  0.0882  0.1507]\n",
      "MSE loss: 87.3733\n",
      "Iteration: 52400\n",
      "Gradient: [-12.0516  14.3351  13.6477 102.6343 244.9269]\n",
      "Weights: [-4.8498  0.813  -1.223   0.088   0.1507]\n",
      "MSE loss: 87.5289\n",
      "Iteration: 52500\n",
      "Gradient: [  -0.8144    0.6343   -2.9968  -35.1444 -207.349 ]\n",
      "Weights: [-4.8424  0.8117 -1.2223  0.0882  0.1507]\n",
      "MSE loss: 87.2905\n",
      "Iteration: 52600\n",
      "Gradient: [ -14.6169   -1.9276  -15.9496  -84.5475 -109.3023]\n",
      "Weights: [-4.8334  0.7986 -1.2195  0.0873  0.1507]\n",
      "MSE loss: 87.5806\n",
      "Iteration: 52700\n",
      "Gradient: [  6.8434   7.9171 -46.423  -44.8417   7.9626]\n",
      "Weights: [-4.8234  0.799  -1.218   0.0876  0.1506]\n",
      "MSE loss: 87.258\n",
      "Iteration: 52800\n",
      "Gradient: [   8.3261   14.5872    1.8731  -96.0117 -145.326 ]\n",
      "Weights: [-4.8207  0.7995 -1.2169  0.0879  0.1505]\n",
      "MSE loss: 87.478\n",
      "Iteration: 52900\n",
      "Gradient: [  5.3136 -14.3962   5.2248  30.2247 120.2019]\n",
      "Weights: [-4.8246  0.7959 -1.2161  0.0883  0.1503]\n",
      "MSE loss: 87.2568\n",
      "Iteration: 53000\n",
      "Gradient: [  5.3646  19.07   -27.8451 -20.0067 -83.1693]\n",
      "Weights: [-4.8349  0.8014 -1.2215  0.089   0.1504]\n",
      "MSE loss: 87.389\n",
      "Iteration: 53100\n",
      "Gradient: [ -5.9026  -3.6085   8.6646 124.9337  84.4508]\n",
      "Weights: [-4.8452  0.8056 -1.2203  0.0891  0.1505]\n",
      "MSE loss: 87.4364\n",
      "Iteration: 53200\n",
      "Gradient: [   5.9951    2.5038   18.8651  -38.3361 -186.7997]\n",
      "Weights: [-4.8358  0.8021 -1.2208  0.0902  0.1503]\n",
      "MSE loss: 87.3147\n",
      "Iteration: 53300\n",
      "Gradient: [  2.3538 -15.5513  -6.0817  35.2136 -80.3968]\n",
      "Weights: [-4.8405  0.7989 -1.2216  0.0905  0.1502]\n",
      "MSE loss: 87.5242\n",
      "Iteration: 53400\n",
      "Gradient: [ -6.962   13.763  -10.0897  30.3116 179.5923]\n",
      "Weights: [-4.8283  0.801  -1.2216  0.0901  0.1503]\n",
      "MSE loss: 87.2471\n",
      "Iteration: 53500\n",
      "Gradient: [ -7.1691  17.5328 -12.4818 -61.6915   4.4016]\n",
      "Weights: [-4.8245  0.7977 -1.2186  0.0898  0.1501]\n",
      "MSE loss: 87.2539\n",
      "Iteration: 53600\n",
      "Gradient: [-5.322800e+00  8.620500e+00 -2.455220e+01 -9.940000e-02  1.444816e+02]\n",
      "Weights: [-4.8395  0.8082 -1.2219  0.0898  0.15  ]\n",
      "MSE loss: 87.273\n",
      "Iteration: 53700\n",
      "Gradient: [  14.0346   10.9516   12.5642  -41.3035 -218.614 ]\n",
      "Weights: [-4.8337  0.8151 -1.2204  0.0895  0.1499]\n",
      "MSE loss: 87.4975\n",
      "Iteration: 53800\n",
      "Gradient: [  3.399    9.0224   8.2763  75.5802 271.5807]\n",
      "Weights: [-4.8485  0.8197 -1.2231  0.0901  0.15  ]\n",
      "MSE loss: 87.3364\n",
      "Iteration: 53900\n",
      "Gradient: [-4.24000e-02 -1.46949e+01  4.37241e+01 -4.23541e+01  1.59245e+01]\n",
      "Weights: [-4.8462  0.8184 -1.2259  0.0905  0.1499]\n",
      "MSE loss: 87.2841\n",
      "Iteration: 54000\n",
      "Gradient: [  0.8395  20.8642  27.0545  -5.8923 131.645 ]\n",
      "Weights: [-4.8402  0.8157 -1.2234  0.0902  0.15  ]\n",
      "MSE loss: 87.2503\n",
      "Iteration: 54100\n",
      "Gradient: [  -3.0265  -14.9511  -36.7005  -19.016  -247.4308]\n",
      "Weights: [-4.8339  0.8146 -1.2266  0.0908  0.1498]\n",
      "MSE loss: 87.1927\n",
      "Iteration: 54200\n",
      "Gradient: [ 4.8537 -7.2575  3.4854 61.5862 16.2855]\n",
      "Weights: [-4.8409  0.8228 -1.2278  0.0907  0.1497]\n",
      "MSE loss: 87.2209\n",
      "Iteration: 54300\n",
      "Gradient: [ -5.157   -2.8009 -29.2698 -99.9712 -62.7972]\n",
      "Weights: [-4.8449  0.8226 -1.2281  0.0903  0.15  ]\n",
      "MSE loss: 87.2554\n",
      "Iteration: 54400\n",
      "Gradient: [ 11.0888  -4.6383  10.8576 -81.3871 -51.1466]\n",
      "Weights: [-4.8332  0.8319 -1.2332  0.0904  0.1503]\n",
      "MSE loss: 87.4056\n",
      "Iteration: 54500\n",
      "Gradient: [ 17.9821   1.3353  24.6788  61.9277 106.8656]\n",
      "Weights: [-4.835   0.829  -1.2322  0.0906  0.1502]\n",
      "MSE loss: 87.2905\n",
      "Iteration: 54600\n",
      "Gradient: [-2.4386  3.9033  9.9644 75.9234 88.3726]\n",
      "Weights: [-4.841   0.8241 -1.2292  0.0904  0.1501]\n",
      "MSE loss: 87.1911\n",
      "Iteration: 54700\n",
      "Gradient: [ -0.2243 -15.7929  27.0731  79.5972 161.1722]\n",
      "Weights: [-4.8411  0.8219 -1.2292  0.091   0.1502]\n",
      "MSE loss: 87.2526\n",
      "Iteration: 54800\n",
      "Gradient: [  1.2222  12.4851  -7.2071  62.7894 146.1976]\n",
      "Weights: [-4.8312  0.8194 -1.2285  0.0906  0.1502]\n",
      "MSE loss: 87.3172\n",
      "Iteration: 54900\n",
      "Gradient: [   6.2724   -2.3219  -18.433    18.2257 -107.8753]\n",
      "Weights: [-4.8152  0.8162 -1.2293  0.091   0.1501]\n",
      "MSE loss: 87.8251\n",
      "Iteration: 55000\n",
      "Gradient: [  4.0385  -1.1389   2.3254  44.6242 -78.6943]\n",
      "Weights: [-4.8331  0.8128 -1.2306  0.0919  0.15  ]\n",
      "MSE loss: 87.2286\n",
      "Iteration: 55100\n",
      "Gradient: [ -9.008    6.4185  17.5708 -53.9776  35.818 ]\n",
      "Weights: [-4.8251  0.8092 -1.2296  0.0925  0.15  ]\n",
      "MSE loss: 87.2193\n",
      "Iteration: 55200\n",
      "Gradient: [ -2.3147  -4.5666   0.22    83.6696 150.6299]\n",
      "Weights: [-4.8321  0.8096 -1.2282  0.0923  0.1499]\n",
      "MSE loss: 87.1613\n",
      "Iteration: 55300\n",
      "Gradient: [  0.9047  -1.7011 -21.1547 -46.6826  95.6274]\n",
      "Weights: [-4.8342  0.8012 -1.228   0.0929  0.1499]\n",
      "MSE loss: 87.3777\n",
      "Iteration: 55400\n",
      "Gradient: [   5.3184    1.5559   20.4031  -11.251  -309.8698]\n",
      "Weights: [-4.8125  0.7944 -1.2261  0.093   0.1499]\n",
      "MSE loss: 87.3204\n",
      "Iteration: 55500\n",
      "Gradient: [   1.5074   11.7915   36.0732  -64.4068 -101.5292]\n",
      "Weights: [-4.8334  0.8065 -1.2291  0.0941  0.1495]\n",
      "MSE loss: 87.1565\n",
      "Iteration: 55600\n",
      "Gradient: [ -7.8264   1.7629 -49.0638 -41.4398 -52.7361]\n",
      "Weights: [-4.8419  0.8143 -1.229   0.0937  0.1495]\n",
      "MSE loss: 87.1836\n",
      "Iteration: 55700\n",
      "Gradient: [ 9.70360e+00 -5.09890e+00  5.06000e-02  6.71455e+01  6.02049e+01]\n",
      "Weights: [-4.8375  0.8096 -1.2272  0.0942  0.1493]\n",
      "MSE loss: 87.1783\n",
      "Iteration: 55800\n",
      "Gradient: [  -1.1867    3.3866  -32.0619  -22.9106 -132.9224]\n",
      "Weights: [-4.8351  0.8094 -1.229   0.0936  0.1494]\n",
      "MSE loss: 87.1659\n",
      "Iteration: 55900\n",
      "Gradient: [   7.0926   19.1813   15.536    45.233  -323.5516]\n",
      "Weights: [-4.8333  0.7999 -1.2278  0.0946  0.1494]\n",
      "MSE loss: 87.2493\n",
      "Iteration: 56000\n",
      "Gradient: [  -0.4577  -10.7167   18.506   -38.5689 -282.0416]\n",
      "Weights: [-4.8238  0.7987 -1.227   0.0944  0.1494]\n",
      "MSE loss: 87.154\n",
      "Iteration: 56100\n",
      "Gradient: [ 3.213  14.9689  2.2374 99.922  36.3769]\n",
      "Weights: [-4.8286  0.8008 -1.2243  0.0942  0.1493]\n",
      "MSE loss: 87.2388\n",
      "Iteration: 56200\n",
      "Gradient: [ -6.7662 -23.6198  17.3024  69.4515 -12.6388]\n",
      "Weights: [-4.8369  0.8093 -1.2256  0.0934  0.1492]\n",
      "MSE loss: 87.1435\n",
      "Iteration: 56300\n",
      "Gradient: [  -1.8768    3.7219    0.8491  117.9609 -161.0539]\n",
      "Weights: [-4.8249  0.8102 -1.2262  0.0928  0.1491]\n",
      "MSE loss: 87.2471\n",
      "Iteration: 56400\n",
      "Gradient: [ -12.8314  -13.0735    7.6425 -139.706  -105.697 ]\n",
      "Weights: [-4.8548  0.8204 -1.2249  0.092   0.1492]\n",
      "MSE loss: 87.3953\n",
      "Iteration: 56500\n",
      "Gradient: [ -1.7554   2.631  -16.9047 -63.5237  49.2296]\n",
      "Weights: [-4.8378  0.8193 -1.2281  0.0923  0.1491]\n",
      "MSE loss: 87.2785\n",
      "Iteration: 56600\n",
      "Gradient: [   5.3583  -13.3438  -13.4612   -8.1644 -123.8726]\n",
      "Weights: [-4.8482  0.8166 -1.2257  0.0926  0.1495]\n",
      "MSE loss: 87.3124\n",
      "Iteration: 56700\n",
      "Gradient: [ -12.1609   -2.3777   19.295     7.9904 -165.4745]\n",
      "Weights: [-4.8498  0.8123 -1.2255  0.092   0.1495]\n",
      "MSE loss: 87.5316\n",
      "Iteration: 56800\n",
      "Gradient: [ -6.4761   2.0482   1.8534  -3.9791 179.6037]\n",
      "Weights: [-4.8311  0.8029 -1.2251  0.093   0.1494]\n",
      "MSE loss: 87.1605\n",
      "Iteration: 56900\n",
      "Gradient: [  5.3154 -18.8983 -11.1652 -88.7532 210.1313]\n",
      "Weights: [-4.8282  0.8097 -1.2294  0.0933  0.1496]\n",
      "MSE loss: 87.1432\n",
      "Iteration: 57000\n",
      "Gradient: [ -14.9426   -8.6227   10.1006  -27.5182 -421.1769]\n",
      "Weights: [-4.8461  0.8107 -1.2309  0.094   0.1495]\n",
      "MSE loss: 87.6111\n",
      "Iteration: 57100\n",
      "Gradient: [ -3.7522 -14.6472  51.1259 -76.5768  97.2682]\n",
      "Weights: [-4.8384  0.8167 -1.2346  0.0947  0.1494]\n",
      "MSE loss: 87.1955\n",
      "Iteration: 57200\n",
      "Gradient: [   5.5024   -1.9951  -40.7747  101.4277 -548.6809]\n",
      "Weights: [-4.8332  0.8116 -1.2342  0.0948  0.1496]\n",
      "MSE loss: 87.1545\n",
      "Iteration: 57300\n",
      "Gradient: [ 11.6021 -10.5603   7.519  -11.1722 175.1988]\n",
      "Weights: [-4.8269  0.8136 -1.2361  0.0945  0.1499]\n",
      "MSE loss: 87.168\n",
      "Iteration: 57400\n",
      "Gradient: [  9.4176 -12.6448 -33.1675 -27.6202  15.3765]\n",
      "Weights: [-4.8274  0.8157 -1.237   0.0947  0.1499]\n",
      "MSE loss: 87.1906\n",
      "Iteration: 57500\n",
      "Gradient: [ 5.3981  7.1988 19.6539 58.8456  7.6967]\n",
      "Weights: [-4.8228  0.8165 -1.2367  0.0945  0.1499]\n",
      "MSE loss: 87.3011\n",
      "Iteration: 57600\n",
      "Gradient: [  -9.2043  -25.0184  -47.8785   22.9778 -131.9291]\n",
      "Weights: [-4.8372  0.8196 -1.2366  0.0942  0.15  ]\n",
      "MSE loss: 87.1535\n",
      "Iteration: 57700\n",
      "Gradient: [  3.4792   7.1367  40.3348 -15.9459 -16.2858]\n",
      "Weights: [-4.8316  0.8155 -1.2365  0.0941  0.1498]\n",
      "MSE loss: 87.2219\n",
      "Iteration: 57800\n",
      "Gradient: [  4.1361   1.0205 -31.0078  14.132  275.697 ]\n",
      "Weights: [-4.8225  0.8083 -1.2336  0.0938  0.15  ]\n",
      "MSE loss: 87.1843\n",
      "Iteration: 57900\n",
      "Gradient: [ -4.2452  17.2949  17.892    9.9103 179.1519]\n",
      "Weights: [-4.8461  0.8227 -1.2356  0.0943  0.1499]\n",
      "MSE loss: 87.2553\n",
      "Iteration: 58000\n",
      "Gradient: [  -9.8498   -8.3771   -3.6966  -26.8944 -132.9413]\n",
      "Weights: [-4.8548  0.8168 -1.2347  0.0948  0.1497]\n",
      "MSE loss: 87.8222\n",
      "Iteration: 58100\n",
      "Gradient: [ -3.0225   4.1114  18.0647  29.4108 141.6904]\n",
      "Weights: [-4.829   0.8078 -1.234   0.095   0.1497]\n",
      "MSE loss: 87.1581\n",
      "Iteration: 58200\n",
      "Gradient: [ -2.8313   2.0302   4.9407 -37.0347 -64.9034]\n",
      "Weights: [-4.8183  0.8084 -1.2351  0.0952  0.1497]\n",
      "MSE loss: 87.2608\n",
      "Iteration: 58300\n",
      "Gradient: [  7.1944   6.3979 -10.6225  38.5422 -92.1964]\n",
      "Weights: [-4.8306  0.8142 -1.2352  0.0954  0.1496]\n",
      "MSE loss: 87.1561\n",
      "Iteration: 58400\n",
      "Gradient: [  4.949   -9.434   34.0038 137.7285 -33.5498]\n",
      "Weights: [-4.835   0.8196 -1.2371  0.0956  0.1497]\n",
      "MSE loss: 87.2402\n",
      "Iteration: 58500\n",
      "Gradient: [  -4.153     6.3322   49.6629  -31.9584 -165.703 ]\n",
      "Weights: [-4.8233  0.8035 -1.2353  0.0966  0.1495]\n",
      "MSE loss: 87.1608\n",
      "Iteration: 58600\n",
      "Gradient: [  -1.3957  -17.6626  -22.0415   35.0521 -144.7256]\n",
      "Weights: [-4.8202  0.8122 -1.2348  0.0959  0.1491]\n",
      "MSE loss: 87.2163\n",
      "Iteration: 58700\n",
      "Gradient: [ -13.1481   12.6346   -0.1991   14.5307 -117.2397]\n",
      "Weights: [-4.8431  0.8103 -1.2349  0.0968  0.1489]\n",
      "MSE loss: 87.6741\n",
      "Iteration: 58800\n",
      "Gradient: [ -2.0078 -12.3527   8.4429  25.6255 -71.8912]\n",
      "Weights: [-4.8396  0.8032 -1.2309  0.0972  0.1487]\n",
      "MSE loss: 87.333\n",
      "Iteration: 58900\n",
      "Gradient: [   1.0136   -2.4518   15.9998   92.1643 -212.6171]\n",
      "Weights: [-4.8253  0.8033 -1.2305  0.0976  0.1485]\n",
      "MSE loss: 87.1103\n",
      "Iteration: 59000\n",
      "Gradient: [  6.8276 -18.1096 -14.1285  65.8008 203.8343]\n",
      "Weights: [-4.8327  0.801  -1.2278  0.0971  0.1484]\n",
      "MSE loss: 87.1119\n",
      "Iteration: 59100\n",
      "Gradient: [-6.51000e-02 -8.48700e+00  1.39547e+01  7.46527e+01 -4.13792e+02]\n",
      "Weights: [-4.8361  0.8036 -1.2289  0.0976  0.1483]\n",
      "MSE loss: 87.1332\n",
      "Iteration: 59200\n",
      "Gradient: [  8.816   -1.0254 -12.5469  -5.2197 -55.0616]\n",
      "Weights: [-4.8295  0.8044 -1.2283  0.0978  0.1482]\n",
      "MSE loss: 87.1192\n",
      "Iteration: 59300\n",
      "Gradient: [ -6.8128   9.5328   1.7322 -61.4353 112.4043]\n",
      "Weights: [-4.8459  0.7965 -1.2265  0.0979  0.1484]\n",
      "MSE loss: 87.5976\n",
      "Iteration: 59400\n",
      "Gradient: [   7.3902    5.3276   12.3661  102.9549 -200.7737]\n",
      "Weights: [-4.8304  0.7967 -1.2259  0.098   0.1483]\n",
      "MSE loss: 87.1639\n",
      "Iteration: 59500\n",
      "Gradient: [-11.4266   2.9518 -41.4843   7.7575  70.8354]\n",
      "Weights: [-4.8331  0.8022 -1.227   0.0977  0.1481]\n",
      "MSE loss: 87.0817\n",
      "Iteration: 59600\n",
      "Gradient: [ -1.8821   8.9035   3.8921 104.1224  19.2782]\n",
      "Weights: [-4.8431  0.8053 -1.2276  0.0964  0.1482]\n",
      "MSE loss: 87.6388\n",
      "Iteration: 59700\n",
      "Gradient: [ 12.3718   0.5343  11.9479  -3.7222 -48.5236]\n",
      "Weights: [-4.813   0.803  -1.2295  0.097   0.1483]\n",
      "MSE loss: 87.3706\n",
      "Iteration: 59800\n",
      "Gradient: [ -7.6921 -16.5341  12.5854   6.3936 137.9855]\n",
      "Weights: [-4.8118  0.7926 -1.2286  0.0974  0.1484]\n",
      "MSE loss: 87.206\n",
      "Iteration: 59900\n",
      "Gradient: [ -5.6066   9.4711  10.7967 -60.0046 -98.3406]\n",
      "Weights: [-4.8369  0.8    -1.2267  0.0981  0.1483]\n",
      "MSE loss: 87.2066\n",
      "Iteration: 60000\n",
      "Gradient: [ -0.5641 -12.3261  28.5076  24.9786  71.2497]\n",
      "Weights: [-4.8322  0.7996 -1.2295  0.0984  0.1483]\n",
      "MSE loss: 87.1165\n",
      "Iteration: 60100\n",
      "Gradient: [ -4.7635 -18.4242 -23.1506   4.7858 -29.6509]\n",
      "Weights: [-4.8381  0.8034 -1.2264  0.098   0.1481]\n",
      "MSE loss: 87.1433\n",
      "Iteration: 60200\n",
      "Gradient: [  -0.6967    9.5267   11.6658   31.8669 -165.7534]\n",
      "Weights: [-4.8429  0.8044 -1.225   0.0977  0.148 ]\n",
      "MSE loss: 87.2137\n",
      "Iteration: 60300\n",
      "Gradient: [  1.1324   0.9741 -21.9128  -1.6204 111.7139]\n",
      "Weights: [-4.8415  0.7997 -1.2242  0.0976  0.148 ]\n",
      "MSE loss: 87.2198\n",
      "Iteration: 60400\n",
      "Gradient: [ -7.8113  -7.7189  30.5308  24.9405 -45.1748]\n",
      "Weights: [-4.8218  0.7879 -1.2234  0.0974  0.1481]\n",
      "MSE loss: 87.1488\n",
      "Iteration: 60500\n",
      "Gradient: [  6.8149  -1.6315  16.4834 -38.8889  58.0232]\n",
      "Weights: [-4.828   0.7984 -1.2251  0.0978  0.1481]\n",
      "MSE loss: 87.1345\n",
      "Iteration: 60600\n",
      "Gradient: [  -8.2856  -23.4728    4.2921  -37.1937 -362.7718]\n",
      "Weights: [-4.8355  0.7983 -1.2258  0.0969  0.1481]\n",
      "MSE loss: 87.4142\n",
      "Iteration: 60700\n",
      "Gradient: [ -0.47    -9.2013   0.2595   7.0871 131.7999]\n",
      "Weights: [-4.8311  0.8084 -1.2282  0.096   0.1481]\n",
      "MSE loss: 87.314\n",
      "Iteration: 60800\n",
      "Gradient: [  6.5159  -9.4107 -28.5681  24.8265 247.8223]\n",
      "Weights: [-4.825   0.8095 -1.2283  0.0964  0.1482]\n",
      "MSE loss: 87.2135\n",
      "Iteration: 60900\n",
      "Gradient: [ -0.6517 -11.1806  19.7842 -48.3197 250.1184]\n",
      "Weights: [-4.8469  0.8171 -1.2286  0.097   0.1481]\n",
      "MSE loss: 87.1735\n",
      "Iteration: 61000\n",
      "Gradient: [  8.0057  -7.4234   4.2841   5.2583 -82.4717]\n",
      "Weights: [-4.8263  0.8116 -1.2306  0.0974  0.1481]\n",
      "MSE loss: 87.184\n",
      "Iteration: 61100\n",
      "Gradient: [  5.1854   1.691  -44.4723  61.7741 -15.4707]\n",
      "Weights: [-4.824   0.8136 -1.2328  0.0979  0.1479]\n",
      "MSE loss: 87.2381\n",
      "Iteration: 61200\n",
      "Gradient: [  0.835   11.6015 -21.269  139.5731  51.4323]\n",
      "Weights: [-4.8357  0.8091 -1.2291  0.0985  0.1478]\n",
      "MSE loss: 87.0889\n",
      "Iteration: 61300\n",
      "Gradient: [  5.5155 -15.9991  17.4144 -68.793  360.75  ]\n",
      "Weights: [-4.8302  0.803  -1.2281  0.0987  0.1478]\n",
      "MSE loss: 87.0651\n",
      "Iteration: 61400\n",
      "Gradient: [-7.640000e-02  2.044600e+00  2.491260e+01  1.877190e+02 -3.295947e+02]\n",
      "Weights: [-4.8355  0.7986 -1.2291  0.0996  0.1477]\n",
      "MSE loss: 87.2251\n",
      "Iteration: 61500\n",
      "Gradient: [  6.5949   0.9533  50.9278  36.0539 -45.4363]\n",
      "Weights: [-4.8132  0.8016 -1.2311  0.0997  0.1477]\n",
      "MSE loss: 87.3992\n",
      "Iteration: 61600\n",
      "Gradient: [  -0.727    -6.7457   -3.9461 -128.3852  -54.0035]\n",
      "Weights: [-4.85    0.8129 -1.231   0.0997  0.1476]\n",
      "MSE loss: 87.2482\n",
      "Iteration: 61700\n",
      "Gradient: [  -6.752     0.7409    2.1834   -8.4137 -325.3539]\n",
      "Weights: [-4.8374  0.8087 -1.2305  0.0995  0.1475]\n",
      "MSE loss: 87.0875\n",
      "Iteration: 61800\n",
      "Gradient: [ -2.9866  -5.6086 -22.06    44.4638 -66.3948]\n",
      "Weights: [-4.8372  0.8013 -1.2301  0.1     0.1476]\n",
      "MSE loss: 87.1731\n",
      "Iteration: 61900\n",
      "Gradient: [-3.43   -6.5875 12.5315 -7.8553 13.1271]\n",
      "Weights: [-4.8272  0.8029 -1.2298  0.0993  0.1477]\n",
      "MSE loss: 87.0619\n",
      "Iteration: 62000\n",
      "Gradient: [  5.9833  -4.4833 -49.4684   1.5581 123.1416]\n",
      "Weights: [-4.8242  0.8    -1.231   0.0996  0.1476]\n",
      "MSE loss: 87.1519\n",
      "Iteration: 62100\n",
      "Gradient: [  4.3134  -6.148  -38.1639  56.5743  51.1791]\n",
      "Weights: [-4.8198  0.7911 -1.2302  0.1005  0.1477]\n",
      "MSE loss: 87.0784\n",
      "Iteration: 62200\n",
      "Gradient: [ -3.7603  -1.993  -17.3034   0.5117 -13.7881]\n",
      "Weights: [-4.8344  0.8076 -1.2304  0.0993  0.1477]\n",
      "MSE loss: 87.0555\n",
      "Iteration: 62300\n",
      "Gradient: [   7.3562    0.7413   25.7913  -34.5553 -281.6331]\n",
      "Weights: [-4.839   0.8084 -1.2297  0.0989  0.1475]\n",
      "MSE loss: 87.2435\n",
      "Iteration: 62400\n",
      "Gradient: [  8.3259  13.056  -15.8295 -11.7016  52.7815]\n",
      "Weights: [-4.8202  0.8053 -1.2309  0.0993  0.1477]\n",
      "MSE loss: 87.226\n",
      "Iteration: 62500\n",
      "Gradient: [  8.1666   7.4208 -24.373   52.3783 395.1616]\n",
      "Weights: [-4.8109  0.7962 -1.2293  0.0998  0.1478]\n",
      "MSE loss: 87.5164\n",
      "Iteration: 62600\n",
      "Gradient: [  2.1325 -15.5519 -50.1      5.6168 -31.9536]\n",
      "Weights: [-4.8242  0.8019 -1.23    0.1007  0.1474]\n",
      "MSE loss: 87.1369\n",
      "Iteration: 62700\n",
      "Gradient: [ 7.8061 -7.0983 29.589  -2.6019 -9.5555]\n",
      "Weights: [-4.8248  0.7941 -1.2319  0.1008  0.1479]\n",
      "MSE loss: 87.074\n",
      "Iteration: 62800\n",
      "Gradient: [-10.0672  21.4175 -33.0901  -5.5567 -69.1293]\n",
      "Weights: [-4.8252  0.7896 -1.2322  0.1012  0.1477]\n",
      "MSE loss: 87.3382\n",
      "Iteration: 62900\n",
      "Gradient: [-5.4182 -1.6935 -4.1599 24.1759 17.9931]\n",
      "Weights: [-4.8199  0.7908 -1.2307  0.1005  0.1478]\n",
      "MSE loss: 87.0832\n",
      "Iteration: 63000\n",
      "Gradient: [ 2.158000e-01  1.378140e+01 -3.155500e+00  6.170620e+01  2.579948e+02]\n",
      "Weights: [-4.8162  0.7942 -1.2291  0.1001  0.1475]\n",
      "MSE loss: 87.1246\n",
      "Iteration: 63100\n",
      "Gradient: [ -0.9862   2.9245 -31.5268 -63.1329 -40.4964]\n",
      "Weights: [-4.8232  0.798  -1.2303  0.0999  0.1478]\n",
      "MSE loss: 87.0589\n",
      "Iteration: 63200\n",
      "Gradient: [   2.4096   15.0346    2.4893   48.1137 -207.8484]\n",
      "Weights: [-4.8175  0.8042 -1.2311  0.0993  0.1479]\n",
      "MSE loss: 87.3774\n",
      "Iteration: 63300\n",
      "Gradient: [ 2.300000e-03 -1.281820e+01 -5.024330e+01 -1.702510e+01  1.217954e+02]\n",
      "Weights: [-4.8263  0.8001 -1.2291  0.0987  0.148 ]\n",
      "MSE loss: 87.0644\n",
      "Iteration: 63400\n",
      "Gradient: [ 7.9414 10.109  25.622  -0.8153 14.3679]\n",
      "Weights: [-4.8151  0.7943 -1.229   0.0988  0.1481]\n",
      "MSE loss: 87.1514\n",
      "Iteration: 63500\n",
      "Gradient: [ -2.5056   8.2472  15.9891 -17.201  285.8904]\n",
      "Weights: [-4.8394  0.8053 -1.2303  0.0999  0.1479]\n",
      "MSE loss: 87.1556\n",
      "Iteration: 63600\n",
      "Gradient: [  7.1498  -5.1419  12.7971 -35.7266 160.3432]\n",
      "Weights: [-4.8257  0.8022 -1.231   0.1001  0.1479]\n",
      "MSE loss: 87.2079\n",
      "Iteration: 63700\n",
      "Gradient: [ 10.3817  14.633   34.4236 -24.2679 -63.6574]\n",
      "Weights: [-4.8192  0.8049 -1.233   0.0998  0.148 ]\n",
      "MSE loss: 87.2844\n",
      "Iteration: 63800\n",
      "Gradient: [  3.8251   6.1499  37.7993 -68.4931 310.3735]\n",
      "Weights: [-4.8217  0.806  -1.2343  0.1003  0.1478]\n",
      "MSE loss: 87.1328\n",
      "Iteration: 63900\n",
      "Gradient: [ -0.8894  -9.7288  30.2016  -2.626  -44.3323]\n",
      "Weights: [-4.8233  0.7965 -1.2334  0.1001  0.1481]\n",
      "MSE loss: 87.1039\n",
      "Iteration: 64000\n",
      "Gradient: [  13.0718   -4.2162  -19.5069   76.0436 -201.0716]\n",
      "Weights: [-4.8004  0.7926 -1.2333  0.0993  0.1483]\n",
      "MSE loss: 87.48\n",
      "Iteration: 64100\n",
      "Gradient: [  -1.2562    1.7632   14.2964   -7.7792 -119.1572]\n",
      "Weights: [-4.8167  0.7912 -1.231   0.0999  0.1482]\n",
      "MSE loss: 87.1459\n",
      "Iteration: 64200\n",
      "Gradient: [   2.4192  -15.6528    3.8319   22.1547 -112.74  ]\n",
      "Weights: [-4.8183  0.7974 -1.232   0.0994  0.1482]\n",
      "MSE loss: 87.0999\n",
      "Iteration: 64300\n",
      "Gradient: [ -2.4676  -5.5019 -18.7166 -86.8228 -46.5237]\n",
      "Weights: [-4.8181  0.8055 -1.2344  0.0989  0.1484]\n",
      "MSE loss: 87.22\n",
      "Iteration: 64400\n",
      "Gradient: [  -1.3148  -17.5754   -9.1368   19.5715 -164.1958]\n",
      "Weights: [-4.8329  0.8104 -1.2336  0.099   0.1479]\n",
      "MSE loss: 87.0833\n",
      "Iteration: 64500\n",
      "Gradient: [  -3.9631    2.0832   41.2208 -113.797   -35.8706]\n",
      "Weights: [-4.836   0.8211 -1.2366  0.099   0.1481]\n",
      "MSE loss: 87.1001\n",
      "Iteration: 64600\n",
      "Gradient: [  2.6722   7.971  -20.0494 -81.7171 -92.6033]\n",
      "Weights: [-4.8317  0.8236 -1.2373  0.0987  0.1479]\n",
      "MSE loss: 87.1718\n",
      "Iteration: 64700\n",
      "Gradient: [   1.5925    8.4408   -0.4949    1.6947 -183.1507]\n",
      "Weights: [-4.84    0.8273 -1.2395  0.099   0.148 ]\n",
      "MSE loss: 87.0961\n",
      "Iteration: 64800\n",
      "Gradient: [ -0.8027  -4.1943  -4.3761  68.0427 294.1441]\n",
      "Weights: [-4.8393  0.8263 -1.2401  0.0994  0.1481]\n",
      "MSE loss: 87.0571\n",
      "Iteration: 64900\n",
      "Gradient: [-4.8959  1.5617 -5.9772 12.7003 30.5322]\n",
      "Weights: [-4.8302  0.8154 -1.2375  0.0991  0.1483]\n",
      "MSE loss: 87.0517\n",
      "Iteration: 65000\n",
      "Gradient: [ -6.4013 -16.8623  -9.4579  92.1297  69.4448]\n",
      "Weights: [-4.851   0.8266 -1.2369  0.0983  0.1481]\n",
      "MSE loss: 87.226\n",
      "Iteration: 65100\n",
      "Gradient: [ -5.2896  -2.6751 -26.9995  79.3498  53.6026]\n",
      "Weights: [-4.8415  0.8253 -1.2346  0.0977  0.1482]\n",
      "MSE loss: 87.1328\n",
      "Iteration: 65200\n",
      "Gradient: [ -8.9241   0.5823   2.5691 -60.1605 272.5348]\n",
      "Weights: [-4.8567  0.8301 -1.2366  0.0974  0.1483]\n",
      "MSE loss: 87.357\n",
      "Iteration: 65300\n",
      "Gradient: [  2.5294 -13.4256  -3.459   27.423  184.5218]\n",
      "Weights: [-4.8544  0.8347 -1.2382  0.0985  0.1482]\n",
      "MSE loss: 87.2156\n",
      "Iteration: 65400\n",
      "Gradient: [ 11.1688   9.7109  63.8072  22.7145 252.132 ]\n",
      "Weights: [-4.8534  0.8394 -1.2416  0.098   0.1484]\n",
      "MSE loss: 87.1483\n",
      "Iteration: 65500\n",
      "Gradient: [ -1.9948  -9.6061 -61.6768  -3.1122 -17.7663]\n",
      "Weights: [-4.8466  0.8338 -1.2461  0.0991  0.1486]\n",
      "MSE loss: 87.1464\n",
      "Iteration: 65600\n",
      "Gradient: [  8.4981   0.8864  -5.7997  87.3251 166.9115]\n",
      "Weights: [-4.848   0.8359 -1.2461  0.0997  0.1487]\n",
      "MSE loss: 87.0904\n",
      "Iteration: 65700\n",
      "Gradient: [-10.5516 -20.566   -1.3834 -36.0964  91.4099]\n",
      "Weights: [-4.8275  0.8213 -1.2452  0.0997  0.1487]\n",
      "MSE loss: 87.063\n",
      "Iteration: 65800\n",
      "Gradient: [  0.9574  10.7106  43.438  -17.2663 398.2179]\n",
      "Weights: [-4.8345  0.8357 -1.2467  0.0992  0.1488]\n",
      "MSE loss: 87.2268\n",
      "Iteration: 65900\n",
      "Gradient: [   4.0651    2.3383    0.5774 -106.8336  -72.0332]\n",
      "Weights: [-4.8253  0.8266 -1.2479  0.0995  0.1489]\n",
      "MSE loss: 87.1184\n",
      "Iteration: 66000\n",
      "Gradient: [ -4.5587 -10.2553  22.9476  57.9404 103.6583]\n",
      "Weights: [-4.8261  0.8281 -1.2495  0.0996  0.1493]\n",
      "MSE loss: 87.2586\n",
      "Iteration: 66100\n",
      "Gradient: [  2.6778  -5.3205  40.6116  25.445  149.8663]\n",
      "Weights: [-4.8302  0.8304 -1.2493  0.1001  0.1492]\n",
      "MSE loss: 87.3353\n",
      "Iteration: 66200\n",
      "Gradient: [  3.6533 -24.3549 -28.2882 -55.3469 -88.7249]\n",
      "Weights: [-4.8301  0.8328 -1.2502  0.1003  0.1489]\n",
      "MSE loss: 87.1855\n",
      "Iteration: 66300\n",
      "Gradient: [ -11.2634   -3.3752   -7.8358  -37.8486 -301.1659]\n",
      "Weights: [-4.8434  0.8376 -1.2516  0.1     0.1489]\n",
      "MSE loss: 87.0568\n",
      "Iteration: 66400\n",
      "Gradient: [ -8.6398  -2.7906 -34.7408 -60.1592  19.5012]\n",
      "Weights: [-4.8374  0.8192 -1.2504  0.1006  0.1491]\n",
      "MSE loss: 87.5437\n",
      "Iteration: 66500\n",
      "Gradient: [ -1.2374 -13.0866  30.899   87.1441 236.428 ]\n",
      "Weights: [-4.8274  0.8174 -1.2497  0.1015  0.149 ]\n",
      "MSE loss: 87.09\n",
      "Iteration: 66600\n",
      "Gradient: [  3.4481 -23.5415 -27.5935 -48.9362   3.7398]\n",
      "Weights: [-4.8228  0.821  -1.2526  0.1014  0.1487]\n",
      "MSE loss: 87.2847\n",
      "Iteration: 66700\n",
      "Gradient: [  -0.5526    5.3007   -9.0358  -84.741  -280.9653]\n",
      "Weights: [-4.8267  0.8216 -1.2534  0.1026  0.1487]\n",
      "MSE loss: 87.0782\n",
      "Iteration: 66800\n",
      "Gradient: [ -2.7594 -11.0202  14.7523 -25.143   20.4736]\n",
      "Weights: [-4.8375  0.8215 -1.2499  0.1021  0.1485]\n",
      "MSE loss: 87.1799\n",
      "Iteration: 66900\n",
      "Gradient: [ 4.069   5.1824 15.7285 24.0889 67.4608]\n",
      "Weights: [-4.827   0.8247 -1.25    0.1019  0.1484]\n",
      "MSE loss: 87.0385\n",
      "Iteration: 67000\n",
      "Gradient: [ -7.2505  33.5067  -8.2967  72.9238 260.5168]\n",
      "Weights: [-4.824   0.8247 -1.249   0.1016  0.1486]\n",
      "MSE loss: 87.2197\n",
      "Iteration: 67100\n",
      "Gradient: [  -3.2357   -3.2532  -10.2584 -190.2294   25.9447]\n",
      "Weights: [-4.852   0.8432 -1.2513  0.1012  0.1484]\n",
      "MSE loss: 87.0681\n",
      "Iteration: 67200\n",
      "Gradient: [   7.9523    2.4496   41.6545  -53.2326 -221.692 ]\n",
      "Weights: [-4.8341  0.836  -1.2523  0.1014  0.1483]\n",
      "MSE loss: 87.0928\n",
      "Iteration: 67300\n",
      "Gradient: [ -3.5238 -12.294   30.254   -5.2719   6.1399]\n",
      "Weights: [-4.8609  0.8432 -1.2529  0.1026  0.1483]\n",
      "MSE loss: 87.2838\n",
      "Iteration: 67400\n",
      "Gradient: [ -3.0724  -9.2314   1.1245  73.4171 472.1064]\n",
      "Weights: [-4.8437  0.8423 -1.2549  0.1024  0.1482]\n",
      "MSE loss: 86.9956\n",
      "Iteration: 67500\n",
      "Gradient: [ 13.9354   9.6297 -16.3501 -14.5562 142.1262]\n",
      "Weights: [-4.8196  0.8434 -1.2557  0.1028  0.1481]\n",
      "MSE loss: 87.9483\n",
      "Iteration: 67600\n",
      "Gradient: [   3.4156    8.5328   -7.8011  -56.6104 -132.3381]\n",
      "Weights: [-4.8581  0.8494 -1.2539  0.1032  0.1477]\n",
      "MSE loss: 87.0984\n",
      "Iteration: 67700\n",
      "Gradient: [ -7.8999  11.0463 -18.6803 -87.5666  63.9005]\n",
      "Weights: [-4.8519  0.8365 -1.2509  0.1034  0.1476]\n",
      "MSE loss: 87.2076\n",
      "Iteration: 67800\n",
      "Gradient: [ -6.8384 -19.4543 -28.3343 -24.6057 -34.9455]\n",
      "Weights: [-4.8504  0.8303 -1.2488  0.1037  0.1475]\n",
      "MSE loss: 87.1844\n",
      "Iteration: 67900\n",
      "Gradient: [   1.5357    1.4527   12.2769 -155.3363  -47.1326]\n",
      "Weights: [-4.8431  0.8315 -1.2494  0.1044  0.1473]\n",
      "MSE loss: 86.9776\n",
      "Iteration: 68000\n",
      "Gradient: [  3.7507  -4.8528 -18.0834 -21.9854 -70.6455]\n",
      "Weights: [-4.8425  0.8252 -1.249   0.1048  0.1474]\n",
      "MSE loss: 87.0218\n",
      "Iteration: 68100\n",
      "Gradient: [  6.6089   3.0524 -15.8742 -70.8124 261.9388]\n",
      "Weights: [-4.8407  0.8365 -1.25    0.1045  0.1474]\n",
      "MSE loss: 87.1461\n",
      "Iteration: 68200\n",
      "Gradient: [ -2.4464   2.6586 -25.8097 -65.3828   2.5987]\n",
      "Weights: [-4.8377  0.8287 -1.2488  0.1051  0.1473]\n",
      "MSE loss: 87.0796\n",
      "Iteration: 68300\n",
      "Gradient: [  1.5752   5.9055  25.7728 -50.9926 214.9325]\n",
      "Weights: [-4.826   0.8261 -1.2501  0.1052  0.1474]\n",
      "MSE loss: 87.1699\n",
      "Iteration: 68400\n",
      "Gradient: [  -3.2393    7.0434  -22.4294   -5.2566 -196.422 ]\n",
      "Weights: [-4.8196  0.824  -1.2538  0.1058  0.1474]\n",
      "MSE loss: 87.1049\n",
      "Iteration: 68500\n",
      "Gradient: [  11.3004    1.604   -20.4552   -9.3119 -152.3332]\n",
      "Weights: [-4.8331  0.819  -1.2522  0.1068  0.1474]\n",
      "MSE loss: 86.9641\n",
      "Iteration: 68600\n",
      "Gradient: [  4.5582  15.6139  -2.0182  79.5599 142.0222]\n",
      "Weights: [-4.8201  0.819  -1.2505  0.1058  0.1475]\n",
      "MSE loss: 87.2453\n",
      "Iteration: 68700\n",
      "Gradient: [   2.4948  -16.6336   -0.2874 -136.0207   14.3018]\n",
      "Weights: [-4.8227  0.8134 -1.2517  0.1065  0.1474]\n",
      "MSE loss: 86.9783\n",
      "Iteration: 68800\n",
      "Gradient: [ 6.9863 -0.5171 18.0445 26.988   7.0615]\n",
      "Weights: [-4.8326  0.8204 -1.2531  0.1062  0.1474]\n",
      "MSE loss: 87.0053\n",
      "Iteration: 68900\n",
      "Gradient: [ -8.9743   8.3764  -3.0552 -99.048  143.9055]\n",
      "Weights: [-4.8305  0.8175 -1.2526  0.1069  0.1476]\n",
      "MSE loss: 87.0524\n",
      "Iteration: 69000\n",
      "Gradient: [   5.0394   -5.4766  -11.8318   98.3764 -190.5403]\n",
      "Weights: [-4.835   0.8194 -1.2532  0.106   0.1476]\n",
      "MSE loss: 87.1289\n",
      "Iteration: 69100\n",
      "Gradient: [-22.1451 -14.6341   9.2682 -14.0271 263.0937]\n",
      "Weights: [-4.8407  0.8211 -1.2514  0.1055  0.1475]\n",
      "MSE loss: 87.2119\n",
      "Iteration: 69200\n",
      "Gradient: [  6.632   12.677   20.4289 -13.7422  71.1345]\n",
      "Weights: [-4.8422  0.8304 -1.2507  0.1061  0.1474]\n",
      "MSE loss: 87.2188\n",
      "Iteration: 69300\n",
      "Gradient: [  -7.769     5.1162   -1.2295 -101.3884 -310.6389]\n",
      "Weights: [-4.8411  0.8312 -1.252   0.105   0.1474]\n",
      "MSE loss: 86.9636\n",
      "Iteration: 69400\n",
      "Gradient: [ -12.5587  -12.3377  -17.4488  -76.8831 -141.3727]\n",
      "Weights: [-4.8418  0.8198 -1.2489  0.1053  0.1476]\n",
      "MSE loss: 87.0776\n",
      "Iteration: 69500\n",
      "Gradient: [   3.1467    4.1473  -31.5596   36.5924 -353.5588]\n",
      "Weights: [-4.8376  0.8271 -1.2538  0.1061  0.1476]\n",
      "MSE loss: 86.9625\n",
      "Iteration: 69600\n",
      "Gradient: [  6.9572   1.9676   2.9246 -32.015   -8.0624]\n",
      "Weights: [-4.8137  0.8179 -1.2529  0.107   0.1474]\n",
      "MSE loss: 87.4513\n",
      "Iteration: 69700\n",
      "Gradient: [ -1.3057  -3.4156  50.4263 -99.4209 167.9178]\n",
      "Weights: [-4.8256  0.8134 -1.2519  0.1076  0.1473]\n",
      "MSE loss: 86.9917\n",
      "Iteration: 69800\n",
      "Gradient: [  3.0223  17.384  -17.0266 -89.8061 -15.657 ]\n",
      "Weights: [-4.8197  0.8166 -1.2535  0.1078  0.1474]\n",
      "MSE loss: 87.1753\n",
      "Iteration: 69900\n",
      "Gradient: [  -1.6755   -4.9086    0.3016   41.5953 -202.4572]\n",
      "Weights: [-4.8235  0.8203 -1.2533  0.1084  0.147 ]\n",
      "MSE loss: 87.2413\n",
      "Iteration: 70000\n",
      "Gradient: [ -5.2567  -3.0279 -36.8124 114.9127  51.9421]\n",
      "Weights: [-4.8222  0.8153 -1.2556  0.1081  0.1471]\n",
      "MSE loss: 87.0168\n",
      "Iteration: 70100\n",
      "Gradient: [-13.5788 -12.0647   1.2157 -43.2821  37.3761]\n",
      "Weights: [-4.8388  0.8179 -1.2554  0.1077  0.1471]\n",
      "MSE loss: 87.5872\n",
      "Iteration: 70200\n",
      "Gradient: [  2.7119  -4.7774  45.7924  13.3481 260.5028]\n",
      "Weights: [-4.8276  0.818  -1.2529  0.1077  0.1472]\n",
      "MSE loss: 86.985\n",
      "Iteration: 70300\n",
      "Gradient: [ -9.4267  14.3305  30.8961 -61.0174  72.5034]\n",
      "Weights: [-4.8384  0.8251 -1.2513  0.1077  0.1469]\n",
      "MSE loss: 87.0153\n",
      "Iteration: 70400\n",
      "Gradient: [  -1.3907   -0.8264  -35.8412  -48.4234 -442.3572]\n",
      "Weights: [-4.8342  0.8235 -1.2528  0.1074  0.1469]\n",
      "MSE loss: 86.9179\n",
      "Iteration: 70500\n",
      "Gradient: [  -6.9763    4.9795    2.6779   68.8851 -161.6054]\n",
      "Weights: [-4.836   0.8254 -1.2526  0.1072  0.1472]\n",
      "MSE loss: 86.9944\n",
      "Iteration: 70600\n",
      "Gradient: [ -1.5697  -8.3713 -15.3186  31.8558 152.7825]\n",
      "Weights: [-4.8313  0.8195 -1.2548  0.107   0.1473]\n",
      "MSE loss: 87.1129\n",
      "Iteration: 70700\n",
      "Gradient: [ -0.3141  -1.0082   9.1142 126.9959  47.9517]\n",
      "Weights: [-4.8467  0.8289 -1.2576  0.1082  0.1472]\n",
      "MSE loss: 87.1284\n",
      "Iteration: 70800\n",
      "Gradient: [ -1.6503  -2.4763  14.8533  28.4957 345.5513]\n",
      "Weights: [-4.8266  0.8264 -1.2569  0.1079  0.1471]\n",
      "MSE loss: 86.9626\n",
      "Iteration: 70900\n",
      "Gradient: [   2.7162   -2.3079    4.4712  107.9204 -149.7707]\n",
      "Weights: [-4.8313  0.8317 -1.2558  0.1071  0.1469]\n",
      "MSE loss: 86.9585\n",
      "Iteration: 71000\n",
      "Gradient: [  -1.7895  -19.2973    1.3754   83.7115 -163.6312]\n",
      "Weights: [-4.8481  0.8314 -1.2576  0.1082  0.1471]\n",
      "MSE loss: 87.111\n",
      "Iteration: 71100\n",
      "Gradient: [  9.8585   5.6442  16.8174 -76.2726  62.9732]\n",
      "Weights: [-4.8224  0.8286 -1.2576  0.1084  0.147 ]\n",
      "MSE loss: 87.1958\n",
      "Iteration: 71200\n",
      "Gradient: [ -5.0596  12.1501  -0.5188  79.6293 -62.802 ]\n",
      "Weights: [-4.8375  0.8323 -1.2584  0.1087  0.1469]\n",
      "MSE loss: 86.888\n",
      "Iteration: 71300\n",
      "Gradient: [11.1631  4.2699 19.7442 -3.0421 10.3863]\n",
      "Weights: [-4.8339  0.8274 -1.2566  0.1076  0.1471]\n",
      "MSE loss: 86.9331\n",
      "Iteration: 71400\n",
      "Gradient: [ -7.1874 -15.0791  -4.9565  96.4468 -83.119 ]\n",
      "Weights: [-4.8537  0.8348 -1.2572  0.1072  0.1472]\n",
      "MSE loss: 87.3138\n",
      "Iteration: 71500\n",
      "Gradient: [  4.1073  -1.8512 -24.7011 -71.6752  -4.8222]\n",
      "Weights: [-4.8411  0.8336 -1.2541  0.1063  0.1471]\n",
      "MSE loss: 86.9314\n",
      "Iteration: 71600\n",
      "Gradient: [  -6.7403   -8.371   -30.2022 -127.4884   89.7263]\n",
      "Weights: [-4.8303  0.83   -1.2561  0.1069  0.1474]\n",
      "MSE loss: 86.9995\n",
      "Iteration: 71700\n",
      "Gradient: [   1.9205    3.5124  -12.6145  -98.3096 -361.4872]\n",
      "Weights: [-4.8463  0.8328 -1.258   0.1062  0.1476]\n",
      "MSE loss: 87.2348\n",
      "Iteration: 71800\n",
      "Gradient: [ -7.984   -4.6808 -17.327  -73.1741  57.4803]\n",
      "Weights: [-4.8475  0.84   -1.2575  0.1055  0.1477]\n",
      "MSE loss: 86.9927\n",
      "Iteration: 71900\n",
      "Gradient: [ -3.6558  13.6132   9.2976 -89.6866 -60.1886]\n",
      "Weights: [-4.8432  0.8461 -1.2588  0.1043  0.1478]\n",
      "MSE loss: 87.0084\n",
      "Iteration: 72000\n",
      "Gradient: [   0.9196  -32.1902  -13.1264    7.8247 -105.7357]\n",
      "Weights: [-4.8541  0.8478 -1.2612  0.1047  0.148 ]\n",
      "MSE loss: 87.2101\n",
      "Iteration: 72100\n",
      "Gradient: [-2.8608 -0.6562  4.3248 90.0831 38.6898]\n",
      "Weights: [-4.856   0.8498 -1.2609  0.1047  0.1482]\n",
      "MSE loss: 87.0875\n",
      "Iteration: 72200\n",
      "Gradient: [  7.1352  -1.5761  23.8649 116.1631 -33.518 ]\n",
      "Weights: [-4.8349  0.8412 -1.2613  0.1043  0.1483]\n",
      "MSE loss: 86.989\n",
      "Iteration: 72300\n",
      "Gradient: [-10.9988  13.2221  17.2314 130.1478  68.9523]\n",
      "Weights: [-4.8463  0.8529 -1.2626  0.1041  0.1483]\n",
      "MSE loss: 86.9742\n",
      "Iteration: 72400\n",
      "Gradient: [-10.293   -5.8091   9.5342 -80.2357 143.079 ]\n",
      "Weights: [-4.8412  0.8463 -1.2633  0.1053  0.1482]\n",
      "MSE loss: 86.9484\n",
      "Iteration: 72500\n",
      "Gradient: [ -4.3809 -11.3713  11.515   31.3921 -40.1328]\n",
      "Weights: [-4.8259  0.8418 -1.2658  0.1063  0.1482]\n",
      "MSE loss: 87.0998\n",
      "Iteration: 72600\n",
      "Gradient: [ -9.2992  15.0901   8.9111 114.8854  59.6687]\n",
      "Weights: [-4.8577  0.849  -1.2633  0.1064  0.1479]\n",
      "MSE loss: 87.1524\n",
      "Iteration: 72700\n",
      "Gradient: [ 1.6808 -2.1317 21.1303 12.0509 54.1427]\n",
      "Weights: [-4.8359  0.8336 -1.2608  0.1067  0.1479]\n",
      "MSE loss: 86.9451\n",
      "Iteration: 72800\n",
      "Gradient: [ -0.2451  20.811  -22.389  -39.6466 -15.2016]\n",
      "Weights: [-4.8232  0.836  -1.2608  0.106   0.1479]\n",
      "MSE loss: 87.1882\n",
      "Iteration: 72900\n",
      "Gradient: [ 11.666   14.4046 -15.2643 -74.4049  -4.1567]\n",
      "Weights: [-4.8172  0.8382 -1.2604  0.1067  0.1476]\n",
      "MSE loss: 87.811\n",
      "Iteration: 73000\n",
      "Gradient: [ 8.2399 23.6629 25.0631  9.2223 62.4963]\n",
      "Weights: [-4.8387  0.8463 -1.2613  0.1069  0.1473]\n",
      "MSE loss: 86.9827\n",
      "Iteration: 73100\n",
      "Gradient: [ -3.5995  17.6398 -25.256   -9.0003 -50.0216]\n",
      "Weights: [-4.8447  0.8462 -1.2604  0.1072  0.1473]\n",
      "MSE loss: 87.0041\n",
      "Iteration: 73200\n",
      "Gradient: [ -0.2851  -7.9268 -26.341   46.2937  86.996 ]\n",
      "Weights: [-4.8554  0.8487 -1.2631  0.1078  0.1473]\n",
      "MSE loss: 87.0384\n",
      "Iteration: 73300\n",
      "Gradient: [ -8.4918  -1.74   -20.2829 -38.185  288.0568]\n",
      "Weights: [-4.8382  0.8444 -1.2639  0.1074  0.1473]\n",
      "MSE loss: 86.9474\n",
      "Iteration: 73400\n",
      "Gradient: [ -9.5547 -11.0751   3.5058  34.554  -18.6432]\n",
      "Weights: [-4.8374  0.8468 -1.2655  0.1077  0.1473]\n",
      "MSE loss: 86.954\n",
      "Iteration: 73500\n",
      "Gradient: [  4.8508  -7.7558  -4.7095 -37.7608  80.2193]\n",
      "Weights: [-4.848   0.846  -1.2672  0.1082  0.1477]\n",
      "MSE loss: 86.9884\n",
      "Iteration: 73600\n",
      "Gradient: [-16.6031  12.5598  22.4595 -15.5115  -4.0871]\n",
      "Weights: [-4.8501  0.848  -1.2678  0.1082  0.1476]\n",
      "MSE loss: 87.0971\n",
      "Iteration: 73700\n",
      "Gradient: [ -9.4155 -15.7948 -23.4626 -15.1326 131.3117]\n",
      "Weights: [-4.8565  0.8479 -1.2694  0.1098  0.1474]\n",
      "MSE loss: 87.2301\n",
      "Iteration: 73800\n",
      "Gradient: [  5.5364  16.6141  -1.2819  -1.0569 287.9359]\n",
      "Weights: [-4.8381  0.8446 -1.2659  0.1095  0.1472]\n",
      "MSE loss: 86.9928\n",
      "Iteration: 73900\n",
      "Gradient: [  2.2642  -8.5467 -24.0457 -50.942  -21.7865]\n",
      "Weights: [-4.8385  0.8378 -1.2649  0.11    0.1471]\n",
      "MSE loss: 86.8917\n",
      "Iteration: 74000\n",
      "Gradient: [  -5.6001    2.9391  -65.8755 -135.563    -5.2009]\n",
      "Weights: [-4.8532  0.8355 -1.2625  0.11    0.1469]\n",
      "MSE loss: 87.2826\n",
      "Iteration: 74100\n",
      "Gradient: [   0.67      1.2713  -35.3731 -110.5323 -183.1085]\n",
      "Weights: [-4.8126  0.8269 -1.2635  0.1099  0.147 ]\n",
      "MSE loss: 87.2184\n",
      "Iteration: 74200\n",
      "Gradient: [  3.3968  25.1882 -38.1176 -24.4019 -48.9614]\n",
      "Weights: [-4.8359  0.8374 -1.2632  0.1105  0.1468]\n",
      "MSE loss: 86.9444\n",
      "Iteration: 74300\n",
      "Gradient: [ -1.3072   8.035   22.5208 -65.7759 134.4666]\n",
      "Weights: [-4.8373  0.8351 -1.2608  0.1107  0.1467]\n",
      "MSE loss: 87.1522\n",
      "Iteration: 74400\n",
      "Gradient: [  -0.2871  -19.6437   25.7868 -164.24   -173.6273]\n",
      "Weights: [-4.8428  0.8264 -1.2628  0.111   0.1468]\n",
      "MSE loss: 87.2287\n",
      "Iteration: 74500\n",
      "Gradient: [   0.8302   20.5574  -32.8233  -99.0286 -112.7121]\n",
      "Weights: [-4.8265  0.817  -1.2593  0.1114  0.1467]\n",
      "MSE loss: 86.9187\n",
      "Iteration: 74600\n",
      "Gradient: [ 12.3392 -18.7396 -37.1942 -97.6433 -55.7318]\n",
      "Weights: [-4.8039  0.8114 -1.2578  0.1108  0.1466]\n",
      "MSE loss: 87.3578\n",
      "Iteration: 74700\n",
      "Gradient: [-1.42000e-02  8.48200e-01 -1.18818e+01  1.79613e+01  9.03954e+01]\n",
      "Weights: [-4.8227  0.8166 -1.2574  0.1114  0.1465]\n",
      "MSE loss: 87.0039\n",
      "Iteration: 74800\n",
      "Gradient: [  2.2372   1.2021 -27.4665  86.6154  36.1074]\n",
      "Weights: [-4.834   0.8099 -1.2538  0.1114  0.1463]\n",
      "MSE loss: 87.0365\n",
      "Iteration: 74900\n",
      "Gradient: [ 10.0903  31.1201 -29.8514 -50.3726 -64.5719]\n",
      "Weights: [-4.814   0.8149 -1.2542  0.1103  0.146 ]\n",
      "MSE loss: 87.1013\n",
      "Iteration: 75000\n",
      "Gradient: [ -0.94     4.9146  46.7053  70.7593 -89.2572]\n",
      "Weights: [-4.842   0.8214 -1.253   0.1107  0.1461]\n",
      "MSE loss: 86.9418\n",
      "Iteration: 75100\n",
      "Gradient: [ -4.2883 -17.1406  -0.2546  31.5325 198.1386]\n",
      "Weights: [-4.8408  0.8273 -1.2544  0.1099  0.1462]\n",
      "MSE loss: 86.8945\n",
      "Iteration: 75200\n",
      "Gradient: [  -1.2368    7.3428   11.9059   -2.0792 -368.7503]\n",
      "Weights: [-4.8452  0.8335 -1.256   0.11    0.1461]\n",
      "MSE loss: 86.9107\n",
      "Iteration: 75300\n",
      "Gradient: [  4.9363  12.777   19.6985  41.9681 162.2137]\n",
      "Weights: [-4.8341  0.8278 -1.2574  0.1101  0.1463]\n",
      "MSE loss: 86.8903\n",
      "Iteration: 75400\n",
      "Gradient: [ -6.3922  -0.9882   5.7234 -35.2202  55.8591]\n",
      "Weights: [-4.8324  0.8247 -1.2581  0.1101  0.1463]\n",
      "MSE loss: 86.9917\n",
      "Iteration: 75500\n",
      "Gradient: [   5.8207   -4.0227  -69.378  -116.5351 -140.0533]\n",
      "Weights: [-4.8244  0.8179 -1.2563  0.1108  0.1464]\n",
      "MSE loss: 86.8938\n",
      "Iteration: 75600\n",
      "Gradient: [ -20.9458   -3.3648  -11.0931   33.4102 -131.9053]\n",
      "Weights: [-4.8365  0.8172 -1.2549  0.1107  0.1462]\n",
      "MSE loss: 87.064\n",
      "Iteration: 75700\n",
      "Gradient: [ 11.9289  -5.3563  21.4616   2.2525 109.1337]\n",
      "Weights: [-4.834   0.8165 -1.255   0.1114  0.1462]\n",
      "MSE loss: 86.8993\n",
      "Iteration: 75800\n",
      "Gradient: [ 1.531000e-01 -2.176360e+01 -3.649490e+01 -1.289570e+01  1.770861e+02]\n",
      "Weights: [-4.8434  0.8296 -1.2587  0.1113  0.146 ]\n",
      "MSE loss: 86.9839\n",
      "Iteration: 75900\n",
      "Gradient: [   6.1023   -1.9792  -24.1855  -37.5232 -207.2557]\n",
      "Weights: [-4.8411  0.833  -1.2614  0.1118  0.1458]\n",
      "MSE loss: 87.0632\n",
      "Iteration: 76000\n",
      "Gradient: [  4.2818  15.5865   7.4939 -64.1917  70.6649]\n",
      "Weights: [-4.8401  0.8382 -1.2595  0.1119  0.1457]\n",
      "MSE loss: 86.953\n",
      "Iteration: 76100\n",
      "Gradient: [  0.313   17.6771 -45.9529  14.4023 168.3451]\n",
      "Weights: [-4.8396  0.8389 -1.2589  0.1116  0.1457]\n",
      "MSE loss: 86.9857\n",
      "Iteration: 76200\n",
      "Gradient: [ -5.9288   4.7757 -35.7877  91.9601 136.3707]\n",
      "Weights: [-4.8446  0.8382 -1.2589  0.1115  0.1458]\n",
      "MSE loss: 86.913\n",
      "Iteration: 76300\n",
      "Gradient: [ -1.3173  11.1167  29.0557 -32.152  147.2553]\n",
      "Weights: [-4.8388  0.8398 -1.26    0.111   0.1458]\n",
      "MSE loss: 86.9319\n",
      "Iteration: 76400\n",
      "Gradient: [   5.3873   11.1605   30.909    68.7892 -166.9486]\n",
      "Weights: [-4.8293  0.8377 -1.2616  0.111   0.1461]\n",
      "MSE loss: 87.0597\n",
      "Iteration: 76500\n",
      "Gradient: [ -0.3876  -1.3913  12.0153  33.6847 150.1493]\n",
      "Weights: [-4.8228  0.8342 -1.2608  0.1115  0.1461]\n",
      "MSE loss: 87.3441\n",
      "Iteration: 76600\n",
      "Gradient: [  3.1266  13.431   24.5637  24.5835 121.2268]\n",
      "Weights: [-4.8345  0.835  -1.2596  0.1119  0.146 ]\n",
      "MSE loss: 87.0904\n",
      "Iteration: 76700\n",
      "Gradient: [  -4.1229    9.8141  -86.1451  -20.5553 -193.0016]\n",
      "Weights: [-4.8477  0.8328 -1.2612  0.112   0.1459]\n",
      "MSE loss: 87.1287\n",
      "Iteration: 76800\n",
      "Gradient: [ -1.7505   6.7168  -6.8841  84.4762 -82.1633]\n",
      "Weights: [-4.8414  0.8303 -1.2613  0.112   0.1459]\n",
      "MSE loss: 87.0288\n",
      "Iteration: 76900\n",
      "Gradient: [ -7.0137   9.8453  35.3076  55.369  186.3724]\n",
      "Weights: [-4.8336  0.833  -1.2613  0.1121  0.146 ]\n",
      "MSE loss: 86.89\n",
      "Iteration: 77000\n",
      "Gradient: [ 14.469  -11.1523  -4.7443 -48.4567 -44.2942]\n",
      "Weights: [-4.8323  0.8361 -1.2634  0.1126  0.1459]\n",
      "MSE loss: 86.8984\n",
      "Iteration: 77100\n",
      "Gradient: [ -1.1171  13.3467  -3.9351 121.8158 -99.7812]\n",
      "Weights: [-4.8459  0.8474 -1.2654  0.1125  0.1461]\n",
      "MSE loss: 87.0079\n",
      "Iteration: 77200\n",
      "Gradient: [  6.6905  21.986   -6.701   70.0092 213.9922]\n",
      "Weights: [-4.849   0.8575 -1.2674  0.1124  0.146 ]\n",
      "MSE loss: 87.1501\n",
      "Iteration: 77300\n",
      "Gradient: [ 1.47430e+00  4.63000e-02 -1.12025e+01 -8.35900e+01  1.79602e+01]\n",
      "Weights: [-4.845   0.8351 -1.2646  0.113   0.1461]\n",
      "MSE loss: 86.9162\n",
      "Iteration: 77400\n",
      "Gradient: [   1.8257  -34.9389  -14.5763    3.5921 -115.84  ]\n",
      "Weights: [-4.8381  0.8349 -1.2654  0.1134  0.1462]\n",
      "MSE loss: 86.9067\n",
      "Iteration: 77500\n",
      "Gradient: [  0.2117  -7.4879   8.9811  22.6676 131.9178]\n",
      "Weights: [-4.8413  0.8455 -1.2667  0.1129  0.1462]\n",
      "MSE loss: 87.0145\n",
      "Iteration: 77600\n",
      "Gradient: [-10.5693   0.2416  22.1848 -10.5037 125.1767]\n",
      "Weights: [-4.8467  0.8336 -1.264   0.1135  0.146 ]\n",
      "MSE loss: 86.9402\n",
      "Iteration: 77700\n",
      "Gradient: [ -0.3094   1.2405 -20.8126 -69.264  -79.3563]\n",
      "Weights: [-4.8353  0.8334 -1.2658  0.1135  0.1458]\n",
      "MSE loss: 86.8582\n",
      "Iteration: 77800\n",
      "Gradient: [   6.6346    0.3112   12.5594   73.5117 -106.9304]\n",
      "Weights: [-4.8257  0.8268 -1.2668  0.1149  0.1458]\n",
      "MSE loss: 86.8299\n",
      "Iteration: 77900\n",
      "Gradient: [  2.2138  19.8094   6.7522  27.8019 206.0161]\n",
      "Weights: [-4.8247  0.8277 -1.2667  0.1153  0.1458]\n",
      "MSE loss: 86.9115\n",
      "Iteration: 78000\n",
      "Gradient: [  1.151   12.0903  13.333   10.5054 231.6804]\n",
      "Weights: [-4.8191  0.8186 -1.2665  0.116   0.1458]\n",
      "MSE loss: 86.9149\n",
      "Iteration: 78100\n",
      "Gradient: [-5.69000e-02 -1.61975e+01 -2.45821e+01 -6.01576e+01  8.26470e+01]\n",
      "Weights: [-4.8147  0.8049 -1.2642  0.1163  0.1457]\n",
      "MSE loss: 86.9963\n",
      "Iteration: 78200\n",
      "Gradient: [  8.5915  27.8044  18.9387 127.8611  67.8387]\n",
      "Weights: [-4.7956  0.8029 -1.2612  0.1168  0.1454]\n",
      "MSE loss: 87.6873\n",
      "Iteration: 78300\n",
      "Gradient: [ -0.8531  15.9905  -2.1541   2.6786 375.3772]\n",
      "Weights: [-4.8275  0.8058 -1.2599  0.1174  0.1452]\n",
      "MSE loss: 86.9208\n",
      "Iteration: 78400\n",
      "Gradient: [  -9.8402   -1.3853  -19.7516  -24.972  -307.3208]\n",
      "Weights: [-4.823   0.8071 -1.2625  0.1179  0.145 ]\n",
      "MSE loss: 86.8657\n",
      "Iteration: 78500\n",
      "Gradient: [  -2.9588   -4.4564  -11.7    -106.087   -47.3716]\n",
      "Weights: [-4.827   0.8192 -1.263   0.1172  0.1447]\n",
      "MSE loss: 86.7919\n",
      "Iteration: 78600\n",
      "Gradient: [ -2.3201  -4.0782   1.6761 116.8996 186.6953]\n",
      "Weights: [-4.836   0.8232 -1.264   0.1172  0.145 ]\n",
      "MSE loss: 86.7997\n",
      "Iteration: 78700\n",
      "Gradient: [  -0.6966    1.2491   61.6305  -60.1936 -103.973 ]\n",
      "Weights: [-4.838   0.8265 -1.2637  0.1176  0.1447]\n",
      "MSE loss: 86.787\n",
      "Iteration: 78800\n",
      "Gradient: [  3.9366  -3.7082 -18.466   69.1838 116.6569]\n",
      "Weights: [-4.8326  0.8219 -1.2654  0.118   0.145 ]\n",
      "MSE loss: 86.7903\n",
      "Iteration: 78900\n",
      "Gradient: [  2.3129  13.2986  10.1482 -29.4243 -61.2659]\n",
      "Weights: [-4.8313  0.8185 -1.2645  0.1182  0.1447]\n",
      "MSE loss: 86.8038\n",
      "Iteration: 79000\n",
      "Gradient: [   9.1562    3.9275   34.3084  -10.3456 -139.8157]\n",
      "Weights: [-4.8169  0.8147 -1.2632  0.1177  0.1448]\n",
      "MSE loss: 86.8688\n",
      "Iteration: 79100\n",
      "Gradient: [  7.6877   2.8794  25.9319 -29.9211 200.1241]\n",
      "Weights: [-4.8245  0.8143 -1.2623  0.1183  0.1447]\n",
      "MSE loss: 86.8263\n",
      "Iteration: 79200\n",
      "Gradient: [ -6.0025   5.2859 -18.714   27.6178 -11.3367]\n",
      "Weights: [-4.8313  0.8213 -1.2635  0.119   0.1445]\n",
      "MSE loss: 86.8842\n",
      "Iteration: 79300\n",
      "Gradient: [-9.9669 18.1973  6.6614 10.3985 44.8717]\n",
      "Weights: [-4.8175  0.8063 -1.2603  0.1188  0.1442]\n",
      "MSE loss: 86.8396\n",
      "Iteration: 79400\n",
      "Gradient: [   2.7838   -3.8657   19.3369   54.3629 -267.1699]\n",
      "Weights: [-4.8223  0.8122 -1.2614  0.1193  0.1441]\n",
      "MSE loss: 86.7772\n",
      "Iteration: 79500\n",
      "Gradient: [  0.3761   7.0683   9.8461  41.3241 296.5336]\n",
      "Weights: [-4.8143  0.7996 -1.262   0.1206  0.1442]\n",
      "MSE loss: 86.8256\n",
      "Iteration: 79600\n",
      "Gradient: [  -5.3709   21.3651    2.08    -53.5133 -119.2427]\n",
      "Weights: [-4.8183  0.8003 -1.2593  0.1213  0.1439]\n",
      "MSE loss: 86.9101\n",
      "Iteration: 79700\n",
      "Gradient: [  10.9372   -6.5845    2.8505  -28.4147 -101.7018]\n",
      "Weights: [-4.825   0.8033 -1.2585  0.121   0.1436]\n",
      "MSE loss: 86.7663\n",
      "Iteration: 79800\n",
      "Gradient: [  1.1018  11.1976  42.5639 -24.5791 149.5077]\n",
      "Weights: [-4.831   0.817  -1.2634  0.1211  0.1435]\n",
      "MSE loss: 86.7744\n",
      "Iteration: 79900\n",
      "Gradient: [  -4.7488   15.9605   72.8352   59.9008 -106.2001]\n",
      "Weights: [-4.8102  0.8192 -1.2649  0.1208  0.1436]\n",
      "MSE loss: 87.3447\n",
      "Iteration: 80000\n",
      "Gradient: [ -1.3718   1.3996 -18.5693 -92.8077 -26.0339]\n",
      "Weights: [-4.8252  0.8225 -1.2645  0.1199  0.1436]\n",
      "MSE loss: 86.8681\n",
      "Iteration: 80100\n",
      "Gradient: [  -0.5805  -24.7471    1.7559  -54.9194 -234.3244]\n",
      "Weights: [-4.8344  0.8117 -1.2607  0.1192  0.1441]\n",
      "MSE loss: 86.9437\n",
      "Iteration: 80200\n",
      "Gradient: [-4.8827  9.9165 14.1541 57.1107 70.3379]\n",
      "Weights: [-4.8378  0.8244 -1.262   0.1187  0.1438]\n",
      "MSE loss: 86.8755\n",
      "Iteration: 80300\n",
      "Gradient: [ -0.4164 -12.5624 -21.0842 -15.8155 -80.6294]\n",
      "Weights: [-4.83    0.8238 -1.2596  0.1186  0.1438]\n",
      "MSE loss: 86.9504\n",
      "Iteration: 80400\n",
      "Gradient: [   1.0018  -10.7697   20.4252 -140.911   -43.6976]\n",
      "Weights: [-4.8414  0.8255 -1.2613  0.1189  0.1439]\n",
      "MSE loss: 86.8101\n",
      "Iteration: 80500\n",
      "Gradient: [ -0.6271  15.3027 -48.858  -97.973  -64.9072]\n",
      "Weights: [-4.8433  0.8291 -1.2605  0.118   0.1439]\n",
      "MSE loss: 86.8439\n",
      "Iteration: 80600\n",
      "Gradient: [ 13.0169  -8.9252 -14.1291 -13.6495 259.8879]\n",
      "Weights: [-4.8365  0.8257 -1.263   0.1189  0.1439]\n",
      "MSE loss: 86.7779\n",
      "Iteration: 80700\n",
      "Gradient: [ -4.0347   9.8573 -23.9201  64.755  302.0432]\n",
      "Weights: [-4.8349  0.8262 -1.263   0.1189  0.144 ]\n",
      "MSE loss: 86.7809\n",
      "Iteration: 80800\n",
      "Gradient: [-4.0768 -7.8618 20.6885 50.0543 81.4915]\n",
      "Weights: [-4.847   0.8209 -1.2633  0.1193  0.1443]\n",
      "MSE loss: 87.0515\n",
      "Iteration: 80900\n",
      "Gradient: [  6.8655  22.4638 -18.5298  92.8869 121.3659]\n",
      "Weights: [-4.8087  0.8088 -1.2652  0.1203  0.1443]\n",
      "MSE loss: 86.9713\n",
      "Iteration: 81000\n",
      "Gradient: [ 1.142600e+00 -2.520000e-02  1.993160e+01 -5.160940e+01  2.504752e+02]\n",
      "Weights: [-4.8213  0.8182 -1.2662  0.1201  0.1444]\n",
      "MSE loss: 86.92\n",
      "Iteration: 81100\n",
      "Gradient: [  5.4093   1.2249 -14.6691 -48.2288  72.2697]\n",
      "Weights: [-4.8251  0.8256 -1.2686  0.1205  0.1442]\n",
      "MSE loss: 86.8233\n",
      "Iteration: 81200\n",
      "Gradient: [  6.0043  -3.0621  23.2423 -73.4593   4.5186]\n",
      "Weights: [-4.8295  0.8281 -1.2704  0.1205  0.1442]\n",
      "MSE loss: 86.729\n",
      "Iteration: 81300\n",
      "Gradient: [ -2.9377 -12.1652 -16.1338 -64.6241  39.9049]\n",
      "Weights: [-4.8327  0.829  -1.2703  0.1209  0.1441]\n",
      "MSE loss: 86.7242\n",
      "Iteration: 81400\n",
      "Gradient: [  5.3094 -14.2394  46.3847 -34.4907   8.8051]\n",
      "Weights: [-4.8105  0.8223 -1.2715  0.1202  0.1443]\n",
      "MSE loss: 87.0383\n",
      "Iteration: 81500\n",
      "Gradient: [  4.7326  -8.858   16.47   -30.529  380.4235]\n",
      "Weights: [-4.8251  0.8195 -1.2697  0.1213  0.1444]\n",
      "MSE loss: 86.7949\n",
      "Iteration: 81600\n",
      "Gradient: [  7.2674 -16.908   17.3748  37.5578  -9.5198]\n",
      "Weights: [-4.8174  0.8049 -1.2682  0.1218  0.1444]\n",
      "MSE loss: 86.8422\n",
      "Iteration: 81700\n",
      "Gradient: [   1.9829    0.5756   30.9296  143.6039 -209.2069]\n",
      "Weights: [-4.8282  0.8145 -1.2695  0.1213  0.1445]\n",
      "MSE loss: 86.8142\n",
      "Iteration: 81800\n",
      "Gradient: [ -5.7529  10.8909   5.1287 -51.6065  20.8392]\n",
      "Weights: [-4.8153  0.812  -1.2682  0.1203  0.1443]\n",
      "MSE loss: 86.8797\n",
      "Iteration: 81900\n",
      "Gradient: [ -7.9319  21.4174 -22.221  111.5852  55.4995]\n",
      "Weights: [-4.8248  0.8147 -1.2663  0.1199  0.1445]\n",
      "MSE loss: 86.7653\n",
      "Iteration: 82000\n",
      "Gradient: [ -0.2641   7.8852 -10.5026  40.0018 193.905 ]\n",
      "Weights: [-4.8233  0.826  -1.2708  0.1196  0.1445]\n",
      "MSE loss: 86.7922\n",
      "Iteration: 82100\n",
      "Gradient: [  6.7566   5.184    0.6186   7.7631 408.1418]\n",
      "Weights: [-4.8108  0.8175 -1.2704  0.1198  0.1446]\n",
      "MSE loss: 86.9458\n",
      "Iteration: 82200\n",
      "Gradient: [  -5.8743  -13.4454    1.6413   -7.6901 -149.9318]\n",
      "Weights: [-4.823   0.833  -1.2708  0.119   0.1444]\n",
      "MSE loss: 86.9186\n",
      "Iteration: 82300\n",
      "Gradient: [  10.5789   23.8295   -2.1393   27.9586 -129.304 ]\n",
      "Weights: [-4.8251  0.8259 -1.2708  0.1199  0.1447]\n",
      "MSE loss: 86.7932\n",
      "Iteration: 82400\n",
      "Gradient: [  -0.4998  -18.6685    0.4987   10.7515 -113.9427]\n",
      "Weights: [-4.8264  0.8185 -1.2695  0.1206  0.1443]\n",
      "MSE loss: 86.8239\n",
      "Iteration: 82500\n",
      "Gradient: [ -3.5674  -0.5493  43.1708 -63.0023  49.9999]\n",
      "Weights: [-4.8156  0.804  -1.2648  0.1211  0.1441]\n",
      "MSE loss: 86.8219\n",
      "Iteration: 82600\n",
      "Gradient: [   5.4836   -0.3805   -3.6445   53.3524 -162.7242]\n",
      "Weights: [-4.8164  0.8171 -1.2658  0.1216  0.1439]\n",
      "MSE loss: 87.149\n",
      "Iteration: 82700\n",
      "Gradient: [   1.3086    0.9423   13.8406 -207.1786 -252.3718]\n",
      "Weights: [-4.8418  0.83   -1.2671  0.1207  0.1437]\n",
      "MSE loss: 86.7801\n",
      "Iteration: 82800\n",
      "Gradient: [ 1.6547 14.7521 13.9506 57.6124 59.2111]\n",
      "Weights: [-4.8135  0.823  -1.268   0.1217  0.1437]\n",
      "MSE loss: 87.2924\n",
      "Iteration: 82900\n",
      "Gradient: [  4.2807  10.433   27.2441 -49.8151 -62.6998]\n",
      "Weights: [-4.8324  0.8263 -1.266   0.1206  0.1436]\n",
      "MSE loss: 86.7592\n",
      "Iteration: 83000\n",
      "Gradient: [ 6.4704 -3.0663 17.6218 67.6022 21.3485]\n",
      "Weights: [-4.8286  0.8197 -1.2623  0.12    0.1439]\n",
      "MSE loss: 86.8666\n",
      "Iteration: 83100\n",
      "Gradient: [ -7.8254  -6.7462 -22.8055 -42.4279  22.2119]\n",
      "Weights: [-4.841   0.8181 -1.2623  0.1197  0.1438]\n",
      "MSE loss: 87.0774\n",
      "Iteration: 83200\n",
      "Gradient: [ 16.0265  35.6392   3.9456  38.5833 104.6591]\n",
      "Weights: [-4.8165  0.8228 -1.2662  0.1202  0.144 ]\n",
      "MSE loss: 87.127\n",
      "Iteration: 83300\n",
      "Gradient: [  -4.5822    3.8011   -2.9293  -55.9511 -154.8179]\n",
      "Weights: [-4.829   0.821  -1.2653  0.1207  0.144 ]\n",
      "MSE loss: 86.779\n",
      "Iteration: 83400\n",
      "Gradient: [ -1.8107  -7.1422 -26.2268  47.7735  36.8159]\n",
      "Weights: [-4.8269  0.8172 -1.2671  0.1214  0.1441]\n",
      "MSE loss: 86.781\n",
      "Iteration: 83500\n",
      "Gradient: [  8.7792   8.4109  27.4567  72.2171 -88.4304]\n",
      "Weights: [-4.8366  0.815  -1.2654  0.1213  0.1439]\n",
      "MSE loss: 86.8781\n",
      "Iteration: 83600\n",
      "Gradient: [   6.2721  -17.6503   -9.8238 -105.3745 -193.5638]\n",
      "Weights: [-4.8363  0.8192 -1.2659  0.121   0.1438]\n",
      "MSE loss: 86.8923\n",
      "Iteration: 83700\n",
      "Gradient: [ 3.463  -4.2961 -1.9928 -0.2891 46.8689]\n",
      "Weights: [-4.8274  0.8206 -1.2678  0.1211  0.1438]\n",
      "MSE loss: 86.7464\n",
      "Iteration: 83800\n",
      "Gradient: [  -4.9764   10.78    -10.4413  -32.349  -276.3653]\n",
      "Weights: [-4.8237  0.8252 -1.2678  0.1206  0.1439]\n",
      "MSE loss: 86.8298\n",
      "Iteration: 83900\n",
      "Gradient: [ -8.6629  10.7134  35.6762 236.9085 -87.0974]\n",
      "Weights: [-4.8327  0.8264 -1.2676  0.121   0.144 ]\n",
      "MSE loss: 86.7936\n",
      "Iteration: 84000\n",
      "Gradient: [ 7.400000e-02  8.113900e+00 -1.016000e+01 -1.806650e+01 -1.111479e+02]\n",
      "Weights: [-4.8513  0.8285 -1.2691  0.1215  0.1437]\n",
      "MSE loss: 87.3221\n",
      "Iteration: 84100\n",
      "Gradient: [  3.3121  -4.5098 -17.3617  22.5054 100.2715]\n",
      "Weights: [-4.8531  0.8458 -1.2685  0.1201  0.1436]\n",
      "MSE loss: 86.88\n",
      "Iteration: 84200\n",
      "Gradient: [-10.7315  -1.8955  -8.5218  28.4163  50.1997]\n",
      "Weights: [-4.858   0.839  -1.2672  0.12    0.1438]\n",
      "MSE loss: 87.0527\n",
      "Iteration: 84300\n",
      "Gradient: [  -4.2734    8.5384   14.6731   42.0324 -177.4421]\n",
      "Weights: [-4.8368  0.8322 -1.2667  0.1194  0.144 ]\n",
      "MSE loss: 86.7661\n",
      "Iteration: 84400\n",
      "Gradient: [  -1.566    12.8488   -8.1008 -149.5019   -8.6722]\n",
      "Weights: [-4.8398  0.8358 -1.2669  0.1185  0.144 ]\n",
      "MSE loss: 86.8933\n",
      "Iteration: 84500\n",
      "Gradient: [  6.2689 -21.8757  -3.2056  32.022   22.2207]\n",
      "Weights: [-4.8489  0.8376 -1.2661  0.1191  0.1441]\n",
      "MSE loss: 86.8525\n",
      "Iteration: 84600\n",
      "Gradient: [  12.7045   -7.366     6.9645  -16.7931 -107.8777]\n",
      "Weights: [-4.8273  0.8309 -1.2686  0.1196  0.1442]\n",
      "MSE loss: 86.8491\n",
      "Iteration: 84700\n",
      "Gradient: [ -5.3457   6.7062 -28.465   39.8311 -24.6683]\n",
      "Weights: [-4.828   0.8309 -1.2657  0.1188  0.1443]\n",
      "MSE loss: 86.9953\n",
      "Iteration: 84800\n",
      "Gradient: [  -0.8282   24.9674   29.1226  114.3406 -162.1707]\n",
      "Weights: [-4.8603  0.8457 -1.2679  0.1193  0.144 ]\n",
      "MSE loss: 87.0019\n",
      "Iteration: 84900\n",
      "Gradient: [ 6.29990e+00 -6.27790e+00  1.45386e+01  2.44000e-02  9.10536e+01]\n",
      "Weights: [-4.8383  0.8474 -1.2718  0.1189  0.1441]\n",
      "MSE loss: 86.9012\n",
      "Iteration: 85000\n",
      "Gradient: [  3.3372  13.757    7.3901 -19.6841 186.6655]\n",
      "Weights: [-4.8354  0.8422 -1.271   0.1194  0.1442]\n",
      "MSE loss: 86.8774\n",
      "Iteration: 85100\n",
      "Gradient: [ -7.9428   8.4357   4.9574 120.4377 -54.3925]\n",
      "Weights: [-4.8431  0.8465 -1.2709  0.1188  0.1444]\n",
      "MSE loss: 86.8845\n",
      "Iteration: 85200\n",
      "Gradient: [  0.2023   3.1736  47.3303  89.3991 171.2905]\n",
      "Weights: [-4.8276  0.8408 -1.2726  0.1194  0.1446]\n",
      "MSE loss: 87.1749\n",
      "Iteration: 85300\n",
      "Gradient: [-3.178800e+00 -2.130500e+00  2.010000e-01  4.198010e+01  4.468252e+02]\n",
      "Weights: [-4.8343  0.8343 -1.2739  0.1189  0.145 ]\n",
      "MSE loss: 86.7401\n",
      "Iteration: 85400\n",
      "Gradient: [  0.8779   8.497  -19.0796 -13.1305  73.5231]\n",
      "Weights: [-4.8493  0.8417 -1.274   0.1191  0.145 ]\n",
      "MSE loss: 86.8504\n",
      "Iteration: 85500\n",
      "Gradient: [   1.2726  -12.1757   -3.0081  -52.3616 -272.8295]\n",
      "Weights: [-4.8257  0.8352 -1.2743  0.1192  0.1447]\n",
      "MSE loss: 86.8172\n",
      "Iteration: 85600\n",
      "Gradient: [ -6.4346  -8.3774 -36.3224 -16.5598 -10.3738]\n",
      "Weights: [-4.8392  0.8356 -1.2755  0.1197  0.1446]\n",
      "MSE loss: 86.9848\n",
      "Iteration: 85700\n",
      "Gradient: [   2.4658    5.0302   24.6132   40.4788 -131.7298]\n",
      "Weights: [-4.8255  0.8393 -1.2773  0.1208  0.1448]\n",
      "MSE loss: 87.069\n",
      "Iteration: 85800\n",
      "Gradient: [   0.5353   -6.443   -35.3368 -117.7917  173.4776]\n",
      "Weights: [-4.8155  0.8314 -1.2791  0.121   0.1449]\n",
      "MSE loss: 86.9439\n",
      "Iteration: 85900\n",
      "Gradient: [  -0.8429   -1.6566    5.1831  -60.3038 -110.2105]\n",
      "Weights: [-4.8337  0.8311 -1.2778  0.1215  0.1447]\n",
      "MSE loss: 86.7976\n",
      "Iteration: 86000\n",
      "Gradient: [  3.1999   9.4281  40.1025  87.0545 120.5984]\n",
      "Weights: [-4.8352  0.8425 -1.2794  0.1215  0.1445]\n",
      "MSE loss: 86.7022\n",
      "Iteration: 86100\n",
      "Gradient: [  3.0137  17.7819 -16.1477  35.0624 129.2495]\n",
      "Weights: [-4.8177  0.8366 -1.2812  0.1221  0.1448]\n",
      "MSE loss: 87.101\n",
      "Iteration: 86200\n",
      "Gradient: [  -0.6387  -11.9042   10.7439   89.0793 -176.7691]\n",
      "Weights: [-4.8425  0.8423 -1.2791  0.122   0.1444]\n",
      "MSE loss: 86.7161\n",
      "Iteration: 86300\n",
      "Gradient: [  10.4564   -4.2871  -45.1351  -97.6013 -354.7614]\n",
      "Weights: [-4.8385  0.8414 -1.2804  0.1216  0.1445]\n",
      "MSE loss: 86.7407\n",
      "Iteration: 86400\n",
      "Gradient: [ -2.0564  15.5572  31.8827   1.1824 251.5938]\n",
      "Weights: [-4.8377  0.8412 -1.2792  0.1222  0.1444]\n",
      "MSE loss: 86.6929\n",
      "Iteration: 86500\n",
      "Gradient: [ 9.5213 -5.6795 11.0244 29.0993 38.0759]\n",
      "Weights: [-4.8369  0.8411 -1.2791  0.123   0.1443]\n",
      "MSE loss: 86.8067\n",
      "Iteration: 86600\n",
      "Gradient: [  -4.5682   12.3862   28.9541   39.7206 -299.5552]\n",
      "Weights: [-4.8323  0.8454 -1.2787  0.1225  0.1439]\n",
      "MSE loss: 86.8529\n",
      "Iteration: 86700\n",
      "Gradient: [ -6.9322   2.875   34.7602 -57.8261 -93.3585]\n",
      "Weights: [-4.8358  0.8416 -1.2789  0.1223  0.1438]\n",
      "MSE loss: 86.8582\n",
      "Iteration: 86800\n",
      "Gradient: [ 1.476500e+00  1.098000e-01 -2.785260e+01 -2.301600e+01 -3.226458e+02]\n",
      "Weights: [-4.823   0.8341 -1.2786  0.123   0.1439]\n",
      "MSE loss: 86.7817\n",
      "Iteration: 86900\n",
      "Gradient: [  7.3458  15.744  -34.9694  17.6399 106.9697]\n",
      "Weights: [-4.8123  0.8259 -1.278   0.1238  0.144 ]\n",
      "MSE loss: 86.9866\n",
      "Iteration: 87000\n",
      "Gradient: [  -0.727    -3.5219  -15.1712  -81.544  -442.955 ]\n",
      "Weights: [-4.8286  0.8309 -1.2791  0.1234  0.1442]\n",
      "MSE loss: 86.6866\n",
      "Iteration: 87100\n",
      "Gradient: [-1.2012 14.9624  3.0669 55.5353 56.939 ]\n",
      "Weights: [-4.8405  0.8319 -1.2819  0.1242  0.1443]\n",
      "MSE loss: 86.971\n",
      "Iteration: 87200\n",
      "Gradient: [  8.3556   7.7975 -15.4049  72.4943  62.7708]\n",
      "Weights: [-4.829   0.835  -1.2815  0.1235  0.1442]\n",
      "MSE loss: 86.6855\n",
      "Iteration: 87300\n",
      "Gradient: [ -4.7869  19.2146   4.9724 116.3265 192.5744]\n",
      "Weights: [-4.8172  0.8352 -1.283   0.124   0.1442]\n",
      "MSE loss: 86.93\n",
      "Iteration: 87400\n",
      "Gradient: [  -9.1355  -20.156    -6.3547 -136.257   142.9687]\n",
      "Weights: [-4.8375  0.8426 -1.2857  0.1237  0.1442]\n",
      "MSE loss: 86.8441\n",
      "Iteration: 87500\n",
      "Gradient: [-7.5954 -0.9023 37.1559 -9.0921 56.5379]\n",
      "Weights: [-4.8404  0.8504 -1.2893  0.1244  0.1444]\n",
      "MSE loss: 86.6729\n",
      "Iteration: 87600\n",
      "Gradient: [  1.1453   1.3388 -40.6644  21.5423  32.2184]\n",
      "Weights: [-4.8521  0.8606 -1.2886  0.1242  0.1441]\n",
      "MSE loss: 86.7007\n",
      "Iteration: 87700\n",
      "Gradient: [ -7.5111   8.2851  28.5473  31.1713 -53.5948]\n",
      "Weights: [-4.85    0.8591 -1.2859  0.1232  0.1441]\n",
      "MSE loss: 86.7019\n",
      "Iteration: 87800\n",
      "Gradient: [ -0.9297  -5.6241  -8.4019  74.4004 165.0085]\n",
      "Weights: [-4.8577  0.8514 -1.2818  0.1232  0.1441]\n",
      "MSE loss: 86.9043\n",
      "Iteration: 87900\n",
      "Gradient: [  -6.9419   11.2085  -27.5269   51.4644 -197.5274]\n",
      "Weights: [-4.8497  0.8519 -1.2798  0.1228  0.1439]\n",
      "MSE loss: 86.7458\n",
      "Iteration: 88000\n",
      "Gradient: [  1.4078  -0.9935 -19.9587 -74.4218 250.811 ]\n",
      "Weights: [-4.8385  0.8428 -1.28    0.123   0.144 ]\n",
      "MSE loss: 86.677\n",
      "Iteration: 88100\n",
      "Gradient: [  0.4271 -18.9403   9.4137 -45.1191 149.6053]\n",
      "Weights: [-4.8251  0.8366 -1.2763  0.1232  0.1436]\n",
      "MSE loss: 86.885\n",
      "Iteration: 88200\n",
      "Gradient: [  0.4749  -7.1218  57.1718 -10.1742 119.3298]\n",
      "Weights: [-4.8219  0.8411 -1.2809  0.1239  0.1438]\n",
      "MSE loss: 87.0641\n",
      "Iteration: 88300\n",
      "Gradient: [-3.181000e-01 -4.975400e+00 -1.314590e+01  3.110740e+01 -3.953055e+02]\n",
      "Weights: [-4.8304  0.8357 -1.2815  0.1239  0.1438]\n",
      "MSE loss: 86.8042\n",
      "Iteration: 88400\n",
      "Gradient: [  2.2174  -5.33     1.9884 -34.5805 -19.6164]\n",
      "Weights: [-4.8318  0.8355 -1.28    0.1239  0.144 ]\n",
      "MSE loss: 86.6646\n",
      "Iteration: 88500\n",
      "Gradient: [  -3.7933   23.9245   22.9202 -136.4769 -274.2085]\n",
      "Weights: [-4.8483  0.8444 -1.2802  0.1234  0.1441]\n",
      "MSE loss: 86.7615\n",
      "Iteration: 88600\n",
      "Gradient: [  2.0788  -3.6172  34.1215  10.0116 160.4776]\n",
      "Weights: [-4.8333  0.845  -1.2841  0.1237  0.1439]\n",
      "MSE loss: 86.7551\n",
      "Iteration: 88700\n",
      "Gradient: [  1.3965 -16.089   14.7587  10.1701 -93.6814]\n",
      "Weights: [-4.8444  0.8544 -1.2855  0.1237  0.1439]\n",
      "MSE loss: 86.6984\n",
      "Iteration: 88800\n",
      "Gradient: [  5.8072  14.1683  -3.1416 -41.2802 -65.9105]\n",
      "Weights: [-4.8195  0.8447 -1.2864  0.1247  0.144 ]\n",
      "MSE loss: 87.0223\n",
      "Iteration: 88900\n",
      "Gradient: [ -4.5039  24.816   38.9825 -66.9237  63.7134]\n",
      "Weights: [-4.8378  0.8457 -1.2871  0.1255  0.144 ]\n",
      "MSE loss: 86.6421\n",
      "Iteration: 89000\n",
      "Gradient: [ 8.5912 11.0528  0.9051 57.2837 21.9414]\n",
      "Weights: [-4.8281  0.8418 -1.2879  0.1259  0.1439]\n",
      "MSE loss: 86.6585\n",
      "Iteration: 89100\n",
      "Gradient: [  4.0476 -21.1527 -34.9075 -60.2959 -14.4303]\n",
      "Weights: [-4.8403  0.8445 -1.2884  0.1265  0.1437]\n",
      "MSE loss: 86.6829\n",
      "Iteration: 89200\n",
      "Gradient: [-1.3501  8.8171 -3.1791 32.3269  4.245 ]\n",
      "Weights: [-4.8393  0.8448 -1.2868  0.1261  0.1437]\n",
      "MSE loss: 86.6399\n",
      "Iteration: 89300\n",
      "Gradient: [   2.2715  -16.1625   21.4203  -67.6192 -115.6908]\n",
      "Weights: [-4.8232  0.8355 -1.2848  0.1259  0.1437]\n",
      "MSE loss: 86.6997\n",
      "Iteration: 89400\n",
      "Gradient: [  0.6247  -1.0535  -5.3128 -80.114  -15.011 ]\n",
      "Weights: [-4.8355  0.8478 -1.2885  0.1259  0.1437]\n",
      "MSE loss: 86.6276\n",
      "Iteration: 89500\n",
      "Gradient: [-10.7831  13.8665  46.9938 -69.4199 200.5783]\n",
      "Weights: [-4.8462  0.8444 -1.2889  0.1266  0.1438]\n",
      "MSE loss: 86.8754\n",
      "Iteration: 89600\n",
      "Gradient: [-5.600000e-03  1.176670e+01  3.127120e+01  3.239060e+01  3.789266e+02]\n",
      "Weights: [-4.8412  0.8554 -1.291   0.1263  0.1439]\n",
      "MSE loss: 86.6843\n",
      "Iteration: 89700\n",
      "Gradient: [ 1.242330e+01  1.167000e-01 -1.296560e+01 -7.717040e+01 -2.938532e+02]\n",
      "Weights: [-4.8362  0.8479 -1.2913  0.1264  0.144 ]\n",
      "MSE loss: 86.6228\n",
      "Iteration: 89800\n",
      "Gradient: [ -1.1885  14.1659 -35.2768  -6.8176 -71.8352]\n",
      "Weights: [-4.8337  0.852  -1.2914  0.1265  0.1439]\n",
      "MSE loss: 86.6635\n",
      "Iteration: 89900\n",
      "Gradient: [ -9.5557  -8.5547  19.8429  43.1836 -61.9621]\n",
      "Weights: [-4.8353  0.8484 -1.2892  0.1268  0.1435]\n",
      "MSE loss: 86.6183\n",
      "Iteration: 90000\n",
      "Gradient: [-10.9617  11.5574 -18.1255  -3.0959 284.6734]\n",
      "Weights: [-4.8333  0.8424 -1.2865  0.1266  0.1435]\n",
      "MSE loss: 86.62\n",
      "Iteration: 90100\n",
      "Gradient: [  10.5556  -21.1104   18.5245   46.9579 -286.6548]\n",
      "Weights: [-4.83    0.8365 -1.2827  0.1265  0.1433]\n",
      "MSE loss: 86.6429\n",
      "Iteration: 90200\n",
      "Gradient: [ -8.2584 -13.9315 -53.6633 -49.7787  32.1889]\n",
      "Weights: [-4.8527  0.8406 -1.2834  0.1273  0.1431]\n",
      "MSE loss: 86.9743\n",
      "Iteration: 90300\n",
      "Gradient: [ -0.9725   4.1249   9.539   46.0954 111.7667]\n",
      "Weights: [-4.8242  0.8285 -1.285   0.1281  0.1431]\n",
      "MSE loss: 86.7228\n",
      "Iteration: 90400\n",
      "Gradient: [  2.9449   7.6839  30.8298  39.6533 -43.453 ]\n",
      "Weights: [-4.8411  0.8409 -1.2851  0.1284  0.1434]\n",
      "MSE loss: 86.9843\n",
      "Iteration: 90500\n",
      "Gradient: [-4.290000e-02 -2.176920e+01 -2.297440e+01 -5.217120e+01  2.440083e+02]\n",
      "Weights: [-4.826   0.8351 -1.2876  0.1281  0.1434]\n",
      "MSE loss: 86.6279\n",
      "Iteration: 90600\n",
      "Gradient: [  -0.4661   14.0258   14.3365  135.2758 -212.8778]\n",
      "Weights: [-4.8191  0.8376 -1.2867  0.1282  0.143 ]\n",
      "MSE loss: 86.8363\n",
      "Iteration: 90700\n",
      "Gradient: [  -6.4818   -1.1583   26.2594  -93.8757 -142.3822]\n",
      "Weights: [-4.8411  0.8371 -1.2869  0.1285  0.1433]\n",
      "MSE loss: 86.7132\n",
      "Iteration: 90800\n",
      "Gradient: [   4.2183   -1.1417  -48.6092   25.1552 -126.3398]\n",
      "Weights: [-4.8258  0.8269 -1.2885  0.1292  0.1433]\n",
      "MSE loss: 86.8007\n",
      "Iteration: 90900\n",
      "Gradient: [ -2.0362  -5.0858  17.0982  78.8036 277.5877]\n",
      "Weights: [-4.8223  0.8327 -1.2914  0.13    0.1434]\n",
      "MSE loss: 86.6611\n",
      "Iteration: 91000\n",
      "Gradient: [  4.7698  -7.4056 -35.1142 -38.9985  -7.8253]\n",
      "Weights: [-4.8293  0.845  -1.2909  0.1297  0.143 ]\n",
      "MSE loss: 86.7442\n",
      "Iteration: 91100\n",
      "Gradient: [ 11.7269   4.7779 -42.6334   0.9444 267.1034]\n",
      "Weights: [-4.8248  0.8359 -1.2915  0.13    0.1432]\n",
      "MSE loss: 86.6219\n",
      "Iteration: 91200\n",
      "Gradient: [  9.8298   4.9571 -22.2578 -27.5563 107.6002]\n",
      "Weights: [-4.8197  0.8338 -1.29    0.129   0.1432]\n",
      "MSE loss: 86.6916\n",
      "Iteration: 91300\n",
      "Gradient: [  6.7129  -8.0048  79.5862 -80.3154   1.9611]\n",
      "Weights: [-4.8393  0.8454 -1.2883  0.1283  0.143 ]\n",
      "MSE loss: 86.6218\n",
      "Iteration: 91400\n",
      "Gradient: [  4.4985 -11.6118   3.3712 153.9311 107.1463]\n",
      "Weights: [-4.8265  0.8506 -1.2898  0.1283  0.1431]\n",
      "MSE loss: 87.0103\n",
      "Iteration: 91500\n",
      "Gradient: [  3.0308 -10.0962  26.8739  56.8124  -6.7669]\n",
      "Weights: [-4.8422  0.8566 -1.2938  0.1287  0.1433]\n",
      "MSE loss: 86.596\n",
      "Iteration: 91600\n",
      "Gradient: [  2.7739  13.3791  15.0686 -56.1232  -7.5114]\n",
      "Weights: [-4.8353  0.8529 -1.2928  0.129   0.1431]\n",
      "MSE loss: 86.6391\n",
      "Iteration: 91700\n",
      "Gradient: [  3.4227   3.8391  56.865  -35.7037 455.6716]\n",
      "Weights: [-4.8328  0.8529 -1.2914  0.1285  0.1431]\n",
      "MSE loss: 86.7685\n",
      "Iteration: 91800\n",
      "Gradient: [  2.7682   3.5305 -15.4053 -62.0019 -66.0696]\n",
      "Weights: [-4.8388  0.8524 -1.293   0.1284  0.1433]\n",
      "MSE loss: 86.5993\n",
      "Iteration: 91900\n",
      "Gradient: [ -9.9755  -8.8968  28.1524  -7.291  -81.1492]\n",
      "Weights: [-4.8557  0.8595 -1.2938  0.1281  0.1434]\n",
      "MSE loss: 86.768\n",
      "Iteration: 92000\n",
      "Gradient: [  5.2303  10.2661  54.7092 -31.3665 174.347 ]\n",
      "Weights: [-4.8475  0.8541 -1.295   0.1288  0.1436]\n",
      "MSE loss: 86.6693\n",
      "Iteration: 92100\n",
      "Gradient: [  2.8041   7.0892  -2.0438  77.3987 332.795 ]\n",
      "Weights: [-4.833   0.8616 -1.2981  0.1286  0.1436]\n",
      "MSE loss: 86.7488\n",
      "Iteration: 92200\n",
      "Gradient: [ -5.0721   8.3795  -9.6163  14.6637 -45.3678]\n",
      "Weights: [-4.8434  0.863  -1.2996  0.1287  0.1437]\n",
      "MSE loss: 86.5797\n",
      "Iteration: 92300\n",
      "Gradient: [  3.3715   5.7145 -39.1648  17.1857 -72.6291]\n",
      "Weights: [-4.8277  0.8564 -1.2979  0.1295  0.1436]\n",
      "MSE loss: 87.0293\n",
      "Iteration: 92400\n",
      "Gradient: [-10.4087   7.7795 -24.2658 -94.7091 -62.6884]\n",
      "Weights: [-4.8311  0.8359 -1.2946  0.1306  0.1433]\n",
      "MSE loss: 86.7231\n",
      "Iteration: 92500\n",
      "Gradient: [  3.7165  -4.1999   2.765  -30.9388  21.0206]\n",
      "Weights: [-4.8059  0.8299 -1.2924  0.1305  0.1434]\n",
      "MSE loss: 87.2224\n",
      "Iteration: 92600\n",
      "Gradient: [  -7.6105    2.7366   -6.5769   26.413  -126.6491]\n",
      "Weights: [-4.8366  0.8398 -1.2935  0.1297  0.1434]\n",
      "MSE loss: 86.7608\n",
      "Iteration: 92700\n",
      "Gradient: [  1.581   12.0662 -32.7135 -37.2825 -38.7479]\n",
      "Weights: [-4.8287  0.8374 -1.2897  0.1295  0.1433]\n",
      "MSE loss: 86.6855\n",
      "Iteration: 92800\n",
      "Gradient: [ -3.7366  17.0001  10.9324 -28.4202 143.6488]\n",
      "Weights: [-4.8419  0.8494 -1.2913  0.1289  0.143 ]\n",
      "MSE loss: 86.6121\n",
      "Iteration: 92900\n",
      "Gradient: [ -10.971   -25.7082  -14.491    48.2072 -130.4604]\n",
      "Weights: [-4.8459  0.8541 -1.2937  0.1296  0.143 ]\n",
      "MSE loss: 86.5975\n",
      "Iteration: 93000\n",
      "Gradient: [  1.807    2.1332 -28.9596 -35.1954 178.1159]\n",
      "Weights: [-4.8479  0.8546 -1.2952  0.1299  0.1429]\n",
      "MSE loss: 86.7751\n",
      "Iteration: 93100\n",
      "Gradient: [  3.1425  11.7651  13.5218 -58.5653 246.8164]\n",
      "Weights: [-4.8395  0.8568 -1.296   0.131   0.1427]\n",
      "MSE loss: 86.6076\n",
      "Iteration: 93200\n",
      "Gradient: [  17.742     3.1977   -8.3817   -7.5411 -106.2938]\n",
      "Weights: [-4.8344  0.8592 -1.2974  0.1307  0.1427]\n",
      "MSE loss: 86.6638\n",
      "Iteration: 93300\n",
      "Gradient: [  -1.5087    9.46     27.6643  -96.0691 -147.3373]\n",
      "Weights: [-4.8316  0.8576 -1.2992  0.1315  0.1426]\n",
      "MSE loss: 86.6485\n",
      "Iteration: 93400\n",
      "Gradient: [ -6.4433   0.5758 -46.9951 -93.2246 -55.8637]\n",
      "Weights: [-4.8532  0.8599 -1.2968  0.1307  0.1427]\n",
      "MSE loss: 86.7066\n",
      "Iteration: 93500\n",
      "Gradient: [  -3.1386   -9.4714   11.1722  -74.7521 -252.2403]\n",
      "Weights: [-4.8453  0.8669 -1.3004  0.1307  0.1428]\n",
      "MSE loss: 86.5845\n",
      "Iteration: 93600\n",
      "Gradient: [ -0.6656   0.8428  -8.2652   7.4306 198.768 ]\n",
      "Weights: [-4.8376  0.8681 -1.3026  0.1306  0.1431]\n",
      "MSE loss: 86.6506\n",
      "Iteration: 93700\n",
      "Gradient: [  3.6298   2.9763  10.0584  28.9344 -69.3104]\n",
      "Weights: [-4.8377  0.8671 -1.3052  0.1307  0.1434]\n",
      "MSE loss: 86.5727\n",
      "Iteration: 93800\n",
      "Gradient: [   7.9794    6.7797   20.5393  -92.114  -192.0637]\n",
      "Weights: [-4.8299  0.8687 -1.3054  0.131   0.1434]\n",
      "MSE loss: 87.0224\n",
      "Iteration: 93900\n",
      "Gradient: [  8.6894   4.8179 -18.0466  10.8662 140.7664]\n",
      "Weights: [-4.8527  0.8644 -1.3042  0.1304  0.1437]\n",
      "MSE loss: 86.7634\n",
      "Iteration: 94000\n",
      "Gradient: [  3.1636  -7.6    -11.1354 -15.3966 179.4348]\n",
      "Weights: [-4.8422  0.8675 -1.308   0.1309  0.1438]\n",
      "MSE loss: 86.5567\n",
      "Iteration: 94100\n",
      "Gradient: [   2.4319   -1.5765    2.0927 -130.6595   82.3039]\n",
      "Weights: [-4.8408  0.8786 -1.3107  0.1307  0.1436]\n",
      "MSE loss: 86.5994\n",
      "Iteration: 94200\n",
      "Gradient: [ -5.0615  -1.1369  12.3395 148.1023 132.321 ]\n",
      "Weights: [-4.8447  0.8809 -1.3125  0.1313  0.1436]\n",
      "MSE loss: 86.553\n",
      "Iteration: 94300\n",
      "Gradient: [  -5.1262   -1.7202  -16.2632  -94.8583 -142.4469]\n",
      "Weights: [-4.8453  0.8681 -1.3094  0.1322  0.1433]\n",
      "MSE loss: 86.6748\n",
      "Iteration: 94400\n",
      "Gradient: [  -0.7543   -8.0443  -22.3016 -107.9548  -46.6434]\n",
      "Weights: [-4.8511  0.8605 -1.3089  0.1335  0.1433]\n",
      "MSE loss: 86.9115\n",
      "Iteration: 94500\n",
      "Gradient: [  0.7715  -2.5251   6.2874 -26.6288 401.0002]\n",
      "Weights: [-4.8377  0.863  -1.3109  0.1346  0.1429]\n",
      "MSE loss: 86.5103\n",
      "Iteration: 94600\n",
      "Gradient: [ 0.6303  4.8145  3.0092 41.0697 14.0054]\n",
      "Weights: [-4.8346  0.8796 -1.3132  0.135   0.1425]\n",
      "MSE loss: 86.9208\n",
      "Iteration: 94700\n",
      "Gradient: [ -1.3592   1.6721 -29.6117 -41.4716  11.8858]\n",
      "Weights: [-4.8476  0.8724 -1.3157  0.1362  0.1428]\n",
      "MSE loss: 86.5036\n",
      "Iteration: 94800\n",
      "Gradient: [ -5.6048 -14.9504 -28.1837 -66.7182  55.3993]\n",
      "Weights: [-4.8438  0.8717 -1.3143  0.1364  0.1426]\n",
      "MSE loss: 86.5016\n",
      "Iteration: 94900\n",
      "Gradient: [  1.5562  14.8029 -21.8613  95.5067 106.6683]\n",
      "Weights: [-4.8468  0.8716 -1.3132  0.1368  0.1422]\n",
      "MSE loss: 86.4632\n",
      "Iteration: 95000\n",
      "Gradient: [   5.4679   -5.6648  -16.2621 -117.4587 -235.8291]\n",
      "Weights: [-4.8409  0.8631 -1.3131  0.138   0.1419]\n",
      "MSE loss: 86.4753\n",
      "Iteration: 95100\n",
      "Gradient: [  10.5424   -0.9092   19.6691  -60.7245 -178.4605]\n",
      "Weights: [-4.8457  0.8675 -1.315   0.1379  0.1421]\n",
      "MSE loss: 86.5347\n",
      "Iteration: 95200\n",
      "Gradient: [  8.845   16.5032  -3.4701 -30.893  -13.9179]\n",
      "Weights: [-4.8378  0.8644 -1.3141  0.1373  0.1423]\n",
      "MSE loss: 86.4601\n",
      "Iteration: 95300\n",
      "Gradient: [ -5.9471   4.5608   8.3458 -58.6871 121.5948]\n",
      "Weights: [-4.836   0.8575 -1.3124  0.1374  0.1423]\n",
      "MSE loss: 86.5138\n",
      "Iteration: 95400\n",
      "Gradient: [  7.7139  -4.2777  10.1268 -22.7548  58.3481]\n",
      "Weights: [-4.8282  0.8606 -1.3123  0.1375  0.1422]\n",
      "MSE loss: 86.6269\n",
      "Iteration: 95500\n",
      "Gradient: [  5.3739   3.4331 -26.1341   8.5382 156.9078]\n",
      "Weights: [-4.8472  0.8576 -1.3091  0.1376  0.1421]\n",
      "MSE loss: 86.5912\n",
      "Iteration: 95600\n",
      "Gradient: [  0.3661  -1.5673  28.0016 -22.2069  95.7142]\n",
      "Weights: [-4.8349  0.8643 -1.3099  0.1369  0.1421]\n",
      "MSE loss: 86.6188\n",
      "Iteration: 95700\n",
      "Gradient: [ -1.1485 -17.1143   1.5219 -55.2724 -28.1063]\n",
      "Weights: [-4.8519  0.8684 -1.3112  0.1365  0.1419]\n",
      "MSE loss: 86.761\n",
      "Iteration: 95800\n",
      "Gradient: [ -13.1115   -5.8146    8.8289 -171.0195 -156.481 ]\n",
      "Weights: [-4.8577  0.8723 -1.3132  0.1363  0.1422]\n",
      "MSE loss: 86.8951\n",
      "Iteration: 95900\n",
      "Gradient: [  -3.543    -0.4349  -33.9569   81.215  -112.2905]\n",
      "Weights: [-4.8472  0.8582 -1.3099  0.1364  0.1423]\n",
      "MSE loss: 86.7688\n",
      "Iteration: 96000\n",
      "Gradient: [ -6.4292  14.3935  14.7763 -30.0985  88.1636]\n",
      "Weights: [-4.8248  0.8628 -1.3115  0.1361  0.1423]\n",
      "MSE loss: 86.6751\n",
      "Iteration: 96100\n",
      "Gradient: [ -3.0579  -8.517   42.5984 -65.1968 -74.5563]\n",
      "Weights: [-4.8549  0.8801 -1.3144  0.136   0.1423]\n",
      "MSE loss: 86.5267\n",
      "Iteration: 96200\n",
      "Gradient: [  4.1687  21.3602  52.1014  25.3099 239.5154]\n",
      "Weights: [-4.8445  0.8767 -1.3156  0.1361  0.1422]\n",
      "MSE loss: 86.5737\n",
      "Iteration: 96300\n",
      "Gradient: [ -5.2041  -9.9001  -5.6085 -67.6272  -7.8341]\n",
      "Weights: [-4.8361  0.8744 -1.3186  0.1369  0.1423]\n",
      "MSE loss: 86.5256\n",
      "Iteration: 96400\n",
      "Gradient: [  7.9834  -4.4203 -23.6886  61.5504  46.0563]\n",
      "Weights: [-4.8289  0.8769 -1.3211  0.1376  0.1425]\n",
      "MSE loss: 86.7239\n",
      "Iteration: 96500\n",
      "Gradient: [-13.5761 -11.4002 -11.8026 -82.8358  67.5454]\n",
      "Weights: [-4.8442  0.8673 -1.3202  0.1381  0.1427]\n",
      "MSE loss: 86.6485\n",
      "Iteration: 96600\n",
      "Gradient: [  -6.0404  -10.3693  -18.7685 -111.8902  288.1067]\n",
      "Weights: [-4.858   0.8729 -1.3197  0.1381  0.1426]\n",
      "MSE loss: 86.9116\n",
      "Iteration: 96700\n",
      "Gradient: [  10.0974  -11.5184   32.6543  -66.0533 -274.4605]\n",
      "Weights: [-4.8436  0.8756 -1.3221  0.1381  0.1424]\n",
      "MSE loss: 86.5353\n",
      "Iteration: 96800\n",
      "Gradient: [7.38450e+00 1.82797e+01 3.24492e+01 8.45484e+01 6.40000e-03]\n",
      "Weights: [-4.8447  0.8828 -1.3221  0.1375  0.1424]\n",
      "MSE loss: 86.4574\n",
      "Iteration: 96900\n",
      "Gradient: [   7.2307    2.5556  -54.6593   35.8707 -142.8086]\n",
      "Weights: [-4.8478  0.8832 -1.3208  0.1372  0.1424]\n",
      "MSE loss: 86.454\n",
      "Iteration: 97000\n",
      "Gradient: [  0.4011  -1.5132 -14.3083 -41.7279  -7.6069]\n",
      "Weights: [-4.8353  0.8729 -1.3201  0.1383  0.1424]\n",
      "MSE loss: 86.4734\n",
      "Iteration: 97100\n",
      "Gradient: [  0.3338  -8.1291 -13.8708  30.1939 140.9855]\n",
      "Weights: [-4.8548  0.8816 -1.3192  0.1375  0.1423]\n",
      "MSE loss: 86.5228\n",
      "Iteration: 97200\n",
      "Gradient: [ -7.5271  26.9343   2.5155 -20.7758  -5.416 ]\n",
      "Weights: [-4.8479  0.889  -1.3201  0.1378  0.1423]\n",
      "MSE loss: 86.7841\n",
      "Iteration: 97300\n",
      "Gradient: [  -7.6335    5.316    -8.2775   26.2983 -311.469 ]\n",
      "Weights: [-4.8319  0.8742 -1.3193  0.1371  0.1424]\n",
      "MSE loss: 86.5586\n",
      "Iteration: 97400\n",
      "Gradient: [ 11.9907  -1.4374  29.3034 -95.3335 147.5285]\n",
      "Weights: [-4.849   0.8843 -1.3191  0.138   0.1422]\n",
      "MSE loss: 86.5601\n",
      "Iteration: 97500\n",
      "Gradient: [  -6.44     -0.8341   -6.5939  -32.8504 -115.814 ]\n",
      "Weights: [-4.8494  0.8799 -1.3215  0.1389  0.1421]\n",
      "MSE loss: 86.4553\n",
      "Iteration: 97600\n",
      "Gradient: [  -1.6395  -12.8863  -11.3578 -110.3889 -121.8436]\n",
      "Weights: [-4.8531  0.8708 -1.3184  0.1397  0.1421]\n",
      "MSE loss: 86.5936\n",
      "Iteration: 97700\n",
      "Gradient: [  -5.3938  -25.3805  -34.3385   -0.4999 -393.1845]\n",
      "Weights: [-4.8375  0.8716 -1.3224  0.14    0.1419]\n",
      "MSE loss: 86.4555\n",
      "Iteration: 97800\n",
      "Gradient: [  -1.7295   -6.9827  -12.8022 -112.6154  -22.6644]\n",
      "Weights: [-4.8558  0.8736 -1.3194  0.1396  0.1418]\n",
      "MSE loss: 86.7581\n",
      "Iteration: 97900\n",
      "Gradient: [  4.9853  -2.8013  23.8203  34.3916 225.6182]\n",
      "Weights: [-4.8386  0.8721 -1.3178  0.1394  0.1418]\n",
      "MSE loss: 86.4636\n",
      "Iteration: 98000\n",
      "Gradient: [ 5.356000e+00 -5.020000e-02  1.519790e+01  1.703866e+02 -1.391080e+01]\n",
      "Weights: [-4.8623  0.8836 -1.3179  0.1389  0.1419]\n",
      "MSE loss: 86.6252\n",
      "Iteration: 98100\n",
      "Gradient: [   4.955     2.7344   30.273    11.5419 -104.4442]\n",
      "Weights: [-4.8336  0.8709 -1.3189  0.1397  0.1417]\n",
      "MSE loss: 86.4818\n",
      "Iteration: 98200\n",
      "Gradient: [ -3.5498 -16.9768   3.3601 -86.9925 -86.4533]\n",
      "Weights: [-4.852   0.8837 -1.322   0.1394  0.1418]\n",
      "MSE loss: 86.4327\n",
      "Iteration: 98300\n",
      "Gradient: [  2.3558  -9.3536 -68.3724 -53.432  324.3517]\n",
      "Weights: [-4.847   0.8871 -1.3218  0.1391  0.1417]\n",
      "MSE loss: 86.4752\n",
      "Iteration: 98400\n",
      "Gradient: [ -3.2887  -7.8343 -26.3226  13.8976 113.5352]\n",
      "Weights: [-4.8537  0.8836 -1.3225  0.1396  0.1417]\n",
      "MSE loss: 86.5489\n",
      "Iteration: 98500\n",
      "Gradient: [   7.847    -7.9221    1.9824   77.1634 -417.7957]\n",
      "Weights: [-4.8422  0.8849 -1.3239  0.1393  0.1418]\n",
      "MSE loss: 86.4996\n",
      "Iteration: 98600\n",
      "Gradient: [ 10.089   -0.7999  37.0535  90.3562 278.3569]\n",
      "Weights: [-4.845   0.881  -1.3213  0.1395  0.1419]\n",
      "MSE loss: 86.4467\n",
      "Iteration: 98700\n",
      "Gradient: [ -2.5559  12.2811  49.5152 136.0867 209.9217]\n",
      "Weights: [-4.8489  0.8851 -1.3232  0.1409  0.1416]\n",
      "MSE loss: 86.4775\n",
      "Iteration: 98800\n",
      "Gradient: [  3.2686   9.362   14.2775 -94.938  -48.6713]\n",
      "Weights: [-4.8449  0.8816 -1.3242  0.1407  0.1416]\n",
      "MSE loss: 86.4179\n",
      "Iteration: 98900\n",
      "Gradient: [  8.932  -15.0394 -27.4417 -75.8314 -81.8763]\n",
      "Weights: [-4.8279  0.8816 -1.3258  0.1408  0.1417]\n",
      "MSE loss: 86.7838\n",
      "Iteration: 99000\n",
      "Gradient: [ -6.2203   1.7624   5.5181 -79.4189   8.5992]\n",
      "Weights: [-4.848   0.8845 -1.3254  0.1414  0.1415]\n",
      "MSE loss: 86.387\n",
      "Iteration: 99100\n",
      "Gradient: [  6.9655  15.466   45.6346  88.5292 -46.4885]\n",
      "Weights: [-4.8341  0.8734 -1.325   0.1417  0.1418]\n",
      "MSE loss: 86.4319\n",
      "Iteration: 99200\n",
      "Gradient: [  -4.9949   -7.2261    2.107  -119.3344  109.7468]\n",
      "Weights: [-4.8372  0.8734 -1.3248  0.1414  0.1417]\n",
      "MSE loss: 86.4013\n",
      "Iteration: 99300\n",
      "Gradient: [ -2.9839   9.3496 -15.3362 -22.3744  -6.5619]\n",
      "Weights: [-4.8478  0.8843 -1.3267  0.1419  0.1419]\n",
      "MSE loss: 86.5464\n",
      "Iteration: 99400\n",
      "Gradient: [ 7.9827  9.1915 -1.9913 22.6499 57.112 ]\n",
      "Weights: [-4.8458  0.8868 -1.3271  0.1418  0.1417]\n",
      "MSE loss: 86.5481\n",
      "Iteration: 99500\n",
      "Gradient: [  6.0608  -1.9214 -34.3543 -47.7018 148.2473]\n",
      "Weights: [-4.8414  0.88   -1.3307  0.1425  0.1419]\n",
      "MSE loss: 86.3894\n",
      "Iteration: 99600\n",
      "Gradient: [ -10.5648  -31.193   -41.5455 -125.0942 -270.3365]\n",
      "Weights: [-4.8652  0.8867 -1.3304  0.1421  0.1419]\n",
      "MSE loss: 87.0063\n",
      "Iteration: 99700\n",
      "Gradient: [  6.9022  -1.3111  23.0739 -51.7989 194.2542]\n",
      "Weights: [-4.851   0.8876 -1.3307  0.1417  0.142 ]\n",
      "MSE loss: 86.4202\n",
      "Iteration: 99800\n",
      "Gradient: [   9.5479    8.1401  -20.5793 -148.8103  -79.6768]\n",
      "Weights: [-4.8345  0.8854 -1.3291  0.1418  0.1418]\n",
      "MSE loss: 86.6074\n",
      "Iteration: 99900\n",
      "Gradient: [  -0.7499   -5.496     8.031    32.9351 -139.6539]\n",
      "Weights: [-4.8458  0.8762 -1.3261  0.1424  0.1416]\n",
      "MSE loss: 86.4125\n",
      "Iteration: 100000\n",
      "Gradient: [-13.8937  10.7254  23.9854  15.4096 -35.9922]\n",
      "Weights: [-4.8342  0.8666 -1.3245  0.1429  0.1416]\n",
      "MSE loss: 86.4098\n",
      "Iteration: 100100\n",
      "Gradient: [   2.9211   -1.2647    5.6637   18.952  -134.5689]\n",
      "Weights: [-4.8342  0.8687 -1.3257  0.1431  0.1415]\n",
      "MSE loss: 86.3893\n",
      "Iteration: 100200\n",
      "Gradient: [  4.5087 -14.6063 -27.8699  19.3422 -97.1977]\n",
      "Weights: [-4.8269  0.867  -1.3276  0.1432  0.1415]\n",
      "MSE loss: 86.4662\n",
      "Iteration: 100300\n",
      "Gradient: [   0.6555   -8.3212  -30.65     -7.013  -118.277 ]\n",
      "Weights: [-4.8461  0.8789 -1.3276  0.1424  0.1416]\n",
      "MSE loss: 86.4119\n",
      "Iteration: 100400\n",
      "Gradient: [ 13.0444  24.4697 -31.641    1.0516 -99.715 ]\n",
      "Weights: [-4.8279  0.8668 -1.3254  0.1432  0.1416]\n",
      "MSE loss: 86.5063\n",
      "Iteration: 100500\n",
      "Gradient: [  -4.4684   13.2546  -24.2703   28.5918 -230.087 ]\n",
      "Weights: [-4.8395  0.8755 -1.3264  0.1428  0.1415]\n",
      "MSE loss: 86.3675\n",
      "Iteration: 100600\n",
      "Gradient: [ 14.0499 -12.5969 -62.3448 -23.0227 -92.2391]\n",
      "Weights: [-4.8325  0.8703 -1.3262  0.1436  0.1414]\n",
      "MSE loss: 86.407\n",
      "Iteration: 100700\n",
      "Gradient: [ -6.1604  -1.2288   6.8712  26.4092 -31.8039]\n",
      "Weights: [-4.8317  0.8654 -1.3248  0.1432  0.1416]\n",
      "MSE loss: 86.445\n",
      "Iteration: 100800\n",
      "Gradient: [  -9.2285   -5.2359  -29.8641 -142.2236 -229.3609]\n",
      "Weights: [-4.8459  0.8671 -1.3265  0.1436  0.1414]\n",
      "MSE loss: 86.7499\n",
      "Iteration: 100900\n",
      "Gradient: [ -9.5172   4.3205  30.7704  -4.974  -56.0899]\n",
      "Weights: [-4.8281  0.8717 -1.3254  0.1431  0.1411]\n",
      "MSE loss: 86.4981\n",
      "Iteration: 101000\n",
      "Gradient: [-1.4188 17.0314 34.0357 54.9315 28.3714]\n",
      "Weights: [-4.8432  0.8697 -1.324   0.1439  0.1411]\n",
      "MSE loss: 86.3813\n",
      "Iteration: 101100\n",
      "Gradient: [  -2.6728   -0.3201   14.2601    5.7499 -238.3941]\n",
      "Weights: [-4.843   0.8798 -1.3266  0.1436  0.1411]\n",
      "MSE loss: 86.3629\n",
      "Iteration: 101200\n",
      "Gradient: [ -7.2469   2.5897  45.3992 -10.3374  50.3609]\n",
      "Weights: [-4.8465  0.877  -1.3268  0.1433  0.1413]\n",
      "MSE loss: 86.4041\n",
      "Iteration: 101300\n",
      "Gradient: [   1.967    14.1116  -38.3508 -118.3779  114.3154]\n",
      "Weights: [-4.8436  0.884  -1.3267  0.1433  0.1411]\n",
      "MSE loss: 86.4425\n",
      "Iteration: 101400\n",
      "Gradient: [  -0.7875   -3.1574   29.2353  -35.6266 -145.5529]\n",
      "Weights: [-4.8475  0.8942 -1.3314  0.1438  0.1412]\n",
      "MSE loss: 86.5482\n",
      "Iteration: 101500\n",
      "Gradient: [  9.8828  -3.6054  11.6475 -67.8408 -95.1482]\n",
      "Weights: [-4.8442  0.8832 -1.3311  0.1448  0.1412]\n",
      "MSE loss: 86.3443\n",
      "Iteration: 101600\n",
      "Gradient: [ -6.8615  19.1558  17.1359 -31.2984 -34.1303]\n",
      "Weights: [-4.8477  0.8832 -1.3311  0.1445  0.1413]\n",
      "MSE loss: 86.3523\n",
      "Iteration: 101700\n",
      "Gradient: [-1.061650e+01  2.430100e+00 -5.280000e-02 -1.081818e+02 -3.067100e+01]\n",
      "Weights: [-4.8501  0.8726 -1.3299  0.1451  0.141 ]\n",
      "MSE loss: 86.927\n",
      "Iteration: 101800\n",
      "Gradient: [  3.5514 -13.1387  65.1382 -12.7145 140.4429]\n",
      "Weights: [-4.8499  0.8858 -1.3296  0.1449  0.1411]\n",
      "MSE loss: 86.5306\n",
      "Iteration: 101900\n",
      "Gradient: [   6.1179   10.3546  -11.6522   30.3168 -121.7076]\n",
      "Weights: [-4.8526  0.8888 -1.3307  0.145   0.1407]\n",
      "MSE loss: 86.3578\n",
      "Iteration: 102000\n",
      "Gradient: [  10.9984    9.6037    5.4652 -117.887    74.0781]\n",
      "Weights: [-4.839   0.8828 -1.3315  0.1453  0.1408]\n",
      "MSE loss: 86.3578\n",
      "Iteration: 102100\n",
      "Gradient: [ -4.3435  -6.9806 -12.1808  -7.6443 272.2297]\n",
      "Weights: [-4.8567  0.8885 -1.332   0.1442  0.141 ]\n",
      "MSE loss: 86.6435\n",
      "Iteration: 102200\n",
      "Gradient: [  10.9961   15.723   -35.5529  107.0932 -187.1599]\n",
      "Weights: [-4.8584  0.8904 -1.3292  0.1441  0.141 ]\n",
      "MSE loss: 86.4199\n",
      "Iteration: 102300\n",
      "Gradient: [ -3.9603  14.9735  36.2752 107.3199  -6.1345]\n",
      "Weights: [-4.8505  0.8764 -1.3245  0.1434  0.1409]\n",
      "MSE loss: 86.4565\n",
      "Iteration: 102400\n",
      "Gradient: [   3.2773    3.0793   -2.5445   10.7577 -165.3731]\n",
      "Weights: [-4.8388  0.875  -1.3236  0.1429  0.141 ]\n",
      "MSE loss: 86.3884\n",
      "Iteration: 102500\n",
      "Gradient: [  9.0709   4.4334 -17.0936  27.602  248.0255]\n",
      "Weights: [-4.8563  0.8778 -1.3239  0.1432  0.1412]\n",
      "MSE loss: 86.5093\n",
      "Iteration: 102600\n",
      "Gradient: [   0.6149   -7.6233   -3.1666  -17.5432 -154.0498]\n",
      "Weights: [-4.8478  0.8707 -1.3221  0.1427  0.1411]\n",
      "MSE loss: 86.5061\n",
      "Iteration: 102700\n",
      "Gradient: [   8.346    -7.3627   -6.0182 -131.2161 -177.0968]\n",
      "Weights: [-4.838   0.8749 -1.3203  0.1418  0.1408]\n",
      "MSE loss: 86.4657\n",
      "Iteration: 102800\n",
      "Gradient: [   6.4866  -11.0859   18.874    57.2015 -130.2075]\n",
      "Weights: [-4.8347  0.8701 -1.3204  0.1423  0.141 ]\n",
      "MSE loss: 86.4254\n",
      "Iteration: 102900\n",
      "Gradient: [   4.667    -9.5686    6.3829   -4.4129 -154.4778]\n",
      "Weights: [-4.8454  0.8736 -1.318   0.1423  0.1406]\n",
      "MSE loss: 86.4003\n",
      "Iteration: 103000\n",
      "Gradient: [-10.0682   9.7314  -9.7731 -56.7339 -33.9314]\n",
      "Weights: [-4.8487  0.8694 -1.3204  0.143   0.1409]\n",
      "MSE loss: 86.4527\n",
      "Iteration: 103100\n",
      "Gradient: [  6.7026 -13.1155  -6.233  -35.1847  45.058 ]\n",
      "Weights: [-4.8524  0.8687 -1.3213  0.1428  0.141 ]\n",
      "MSE loss: 86.726\n",
      "Iteration: 103200\n",
      "Gradient: [ 5.1674 -4.9991  5.5051 72.2351 43.7198]\n",
      "Weights: [-4.8349  0.8612 -1.3182  0.1425  0.141 ]\n",
      "MSE loss: 86.3815\n",
      "Iteration: 103300\n",
      "Gradient: [  6.6654 -13.7069 -17.5934 -26.3597 -80.7895]\n",
      "Weights: [-4.8254  0.8632 -1.318   0.1413  0.1412]\n",
      "MSE loss: 86.5441\n",
      "Iteration: 103400\n",
      "Gradient: [   7.5907   13.5754  -13.3465   46.5523 -200.7467]\n",
      "Weights: [-4.8345  0.8588 -1.3128  0.1413  0.1412]\n",
      "MSE loss: 86.5721\n",
      "Iteration: 103500\n",
      "Gradient: [  6.5435 -18.9026  39.1555  58.9538 -24.0649]\n",
      "Weights: [-4.8472  0.8648 -1.3142  0.1412  0.1409]\n",
      "MSE loss: 86.4594\n",
      "Iteration: 103600\n",
      "Gradient: [  -5.1626   -3.5007    1.29      0.6161 -119.9941]\n",
      "Weights: [-4.8223  0.8608 -1.3158  0.1416  0.141 ]\n",
      "MSE loss: 86.7236\n",
      "Iteration: 103700\n",
      "Gradient: [ 7.6184 -7.8187 13.4462 53.5266 13.353 ]\n",
      "Weights: [-4.8284  0.8636 -1.3147  0.1417  0.1408]\n",
      "MSE loss: 86.6473\n",
      "Iteration: 103800\n",
      "Gradient: [ 6.1702 14.4248 24.4658 39.1186 55.9683]\n",
      "Weights: [-4.8415  0.8677 -1.314   0.1414  0.1407]\n",
      "MSE loss: 86.4334\n",
      "Iteration: 103900\n",
      "Gradient: [  -4.0174   -0.6728   25.5572   76.6289 -336.8219]\n",
      "Weights: [-4.8516  0.8648 -1.3127  0.1412  0.1408]\n",
      "MSE loss: 86.545\n",
      "Iteration: 104000\n",
      "Gradient: [ -6.117    1.2034 -10.5727 117.287  191.5952]\n",
      "Weights: [-4.8309  0.8577 -1.3103  0.1416  0.1407]\n",
      "MSE loss: 86.6286\n",
      "Iteration: 104100\n",
      "Gradient: [  -6.6295  -20.9294   16.0864 -164.1947   57.0134]\n",
      "Weights: [-4.8475  0.8604 -1.3105  0.1413  0.1407]\n",
      "MSE loss: 86.4537\n",
      "Iteration: 104200\n",
      "Gradient: [  1.3827   3.9878  25.4876  -9.8975 105.0611]\n",
      "Weights: [-4.8423  0.8604 -1.3101  0.1413  0.1408]\n",
      "MSE loss: 86.451\n",
      "Iteration: 104300\n",
      "Gradient: [  0.7281   4.602  -47.3622 119.3959 -60.9413]\n",
      "Weights: [-4.8252  0.8504 -1.3089  0.1412  0.1409]\n",
      "MSE loss: 86.6069\n",
      "Iteration: 104400\n",
      "Gradient: [ -7.7768   1.3909   9.7375 -13.3979 221.8798]\n",
      "Weights: [-4.8475  0.8623 -1.3119  0.1413  0.1409]\n",
      "MSE loss: 86.4446\n",
      "Iteration: 104500\n",
      "Gradient: [-1.81000e-02  6.60880e+00  4.64140e+00 -1.03717e+01 -8.41338e+01]\n",
      "Weights: [-4.8599  0.8658 -1.31    0.1413  0.1404]\n",
      "MSE loss: 86.7351\n",
      "Iteration: 104600\n",
      "Gradient: [   3.7521   -9.2309   -3.8588  -80.7064 -399.9741]\n",
      "Weights: [-4.8586  0.8659 -1.3112  0.1411  0.1404]\n",
      "MSE loss: 86.924\n",
      "Iteration: 104700\n",
      "Gradient: [  -6.1915   -1.0236   11.8932   31.8279 -187.6555]\n",
      "Weights: [-4.8431  0.8585 -1.3128  0.1424  0.1404]\n",
      "MSE loss: 86.5264\n",
      "Iteration: 104800\n",
      "Gradient: [  0.5653   0.3663  16.1396 -28.0862 250.5622]\n",
      "Weights: [-4.8161  0.85   -1.3112  0.1422  0.1403]\n",
      "MSE loss: 86.7279\n",
      "Iteration: 104900\n",
      "Gradient: [ -5.3917  -8.528   31.5721 -19.3849 137.5673]\n",
      "Weights: [-4.8235  0.8553 -1.3137  0.1425  0.1406]\n",
      "MSE loss: 86.5502\n",
      "Iteration: 105000\n",
      "Gradient: [   4.2565   -1.718   -10.4088   41.9188 -248.2954]\n",
      "Weights: [-4.8462  0.8614 -1.3122  0.1421  0.1407]\n",
      "MSE loss: 86.4269\n",
      "Iteration: 105100\n",
      "Gradient: [ -1.017   -7.2833   3.9542 -63.8472 -53.6556]\n",
      "Weights: [-4.8286  0.8596 -1.3119  0.1417  0.1406]\n",
      "MSE loss: 86.6192\n",
      "Iteration: 105200\n",
      "Gradient: [ 19.2175  -8.7441  40.3184  74.6145 398.4957]\n",
      "Weights: [-4.8317  0.8692 -1.313   0.1416  0.1406]\n",
      "MSE loss: 87.1198\n",
      "Iteration: 105300\n",
      "Gradient: [  2.8785   7.7336  20.8893  75.6518 467.4921]\n",
      "Weights: [-4.8455  0.8618 -1.3126  0.1413  0.1408]\n",
      "MSE loss: 86.4661\n",
      "Iteration: 105400\n",
      "Gradient: [   7.0538  -23.7148   15.55     30.4131 -218.5607]\n",
      "Weights: [-4.8359  0.8555 -1.3129  0.1419  0.1408]\n",
      "MSE loss: 86.3915\n",
      "Iteration: 105500\n",
      "Gradient: [-13.6636   5.1089  41.8785  -6.0321  -8.0425]\n",
      "Weights: [-4.8222  0.8473 -1.3105  0.1414  0.1408]\n",
      "MSE loss: 86.4937\n",
      "Iteration: 105600\n",
      "Gradient: [  5.2301  -5.7763 -34.1833  31.8719 155.5197]\n",
      "Weights: [-4.838   0.8557 -1.3111  0.1427  0.1408]\n",
      "MSE loss: 86.6562\n",
      "Iteration: 105700\n",
      "Gradient: [  -3.1271    0.5866  -10.1796  -71.5546 -280.0875]\n",
      "Weights: [-4.831   0.8481 -1.312   0.1424  0.1408]\n",
      "MSE loss: 86.4114\n",
      "Iteration: 105800\n",
      "Gradient: [ -4.0337   3.2491  18.884   22.6258 -20.1073]\n",
      "Weights: [-4.8308  0.8544 -1.3122  0.1419  0.1408]\n",
      "MSE loss: 86.4141\n",
      "Iteration: 105900\n",
      "Gradient: [   0.5518  -16.5005  -31.3293  -67.021  -268.4771]\n",
      "Weights: [-4.845   0.8618 -1.3144  0.1424  0.1408]\n",
      "MSE loss: 86.4132\n",
      "Iteration: 106000\n",
      "Gradient: [ 1.498000e-01  3.765000e-01 -1.810580e+01  5.117980e+01 -2.641426e+02]\n",
      "Weights: [-4.855   0.8741 -1.3164  0.142   0.1405]\n",
      "MSE loss: 86.5468\n",
      "Iteration: 106100\n",
      "Gradient: [  8.0236   3.9943 -27.7918   8.0602 -68.3717]\n",
      "Weights: [-4.8504  0.8725 -1.3133  0.1418  0.1403]\n",
      "MSE loss: 86.4653\n",
      "Iteration: 106200\n",
      "Gradient: [11.1332  8.0088  9.7483 53.5849 13.5227]\n",
      "Weights: [-4.8468  0.8732 -1.3109  0.1416  0.1403]\n",
      "MSE loss: 86.7237\n",
      "Iteration: 106300\n",
      "Gradient: [  -1.843    -9.9319   13.5157 -179.8767  -40.5812]\n",
      "Weights: [-4.8388  0.8604 -1.3123  0.1417  0.1406]\n",
      "MSE loss: 86.4179\n",
      "Iteration: 106400\n",
      "Gradient: [  8.0275   5.4965  22.5071 -42.0503  29.758 ]\n",
      "Weights: [-4.8408  0.872  -1.3141  0.1414  0.1406]\n",
      "MSE loss: 86.5435\n",
      "Iteration: 106500\n",
      "Gradient: [-2.1484  6.0977 45.8361 -4.7709 74.4749]\n",
      "Weights: [-4.8497  0.8707 -1.3173  0.1418  0.141 ]\n",
      "MSE loss: 86.4328\n",
      "Iteration: 106600\n",
      "Gradient: [ -1.1976  13.737  -47.7043 125.3301  21.1476]\n",
      "Weights: [-4.8571  0.8784 -1.3191  0.1408  0.1411]\n",
      "MSE loss: 86.5831\n",
      "Iteration: 106700\n",
      "Gradient: [ -6.5848  18.6727 -62.1374 -18.5383 267.7575]\n",
      "Weights: [-4.8484  0.8713 -1.3194  0.1415  0.1412]\n",
      "MSE loss: 86.4753\n",
      "Iteration: 106800\n",
      "Gradient: [ -1.6179  14.9609   8.9781 -83.2636 121.4618]\n",
      "Weights: [-4.8502  0.8691 -1.3156  0.1412  0.1414]\n",
      "MSE loss: 86.5536\n",
      "Iteration: 106900\n",
      "Gradient: [  8.425   -3.7428  -6.5754  14.0103 120.6166]\n",
      "Weights: [-4.8372  0.8693 -1.3197  0.1414  0.1412]\n",
      "MSE loss: 86.4008\n",
      "Iteration: 107000\n",
      "Gradient: [ -1.73     4.6421 -35.3382   6.661  -55.7034]\n",
      "Weights: [-4.8636  0.8725 -1.3175  0.1411  0.1413]\n",
      "MSE loss: 86.8828\n",
      "Iteration: 107100\n",
      "Gradient: [ -3.1739  -9.5812 -16.6407 -72.8232 -13.9315]\n",
      "Weights: [-4.827   0.8667 -1.3204  0.141   0.1414]\n",
      "MSE loss: 86.5332\n",
      "Iteration: 107200\n",
      "Gradient: [   2.1869   12.8834   14.1007  116.0075 -163.4189]\n",
      "Weights: [-4.8423  0.8705 -1.3182  0.1416  0.1412]\n",
      "MSE loss: 86.446\n",
      "Iteration: 107300\n",
      "Gradient: [-1.5484 12.7372 -3.3626 81.582  35.2193]\n",
      "Weights: [-4.8409  0.8625 -1.315   0.141   0.1412]\n",
      "MSE loss: 86.4085\n",
      "Iteration: 107400\n",
      "Gradient: [-1.7541  6.5916 37.9018 55.0425 13.712 ]\n",
      "Weights: [-4.8357  0.8623 -1.3137  0.1413  0.1411]\n",
      "MSE loss: 86.5071\n",
      "Iteration: 107500\n",
      "Gradient: [  2.8807 -17.7855  34.5642  97.6761 -66.4596]\n",
      "Weights: [-4.828   0.8477 -1.3117  0.1418  0.141 ]\n",
      "MSE loss: 86.4225\n",
      "Iteration: 107600\n",
      "Gradient: [  7.7719  -2.6212 -37.6596 -92.1822 351.7862]\n",
      "Weights: [-4.8257  0.8588 -1.3138  0.1419  0.1411]\n",
      "MSE loss: 86.8978\n",
      "Iteration: 107700\n",
      "Gradient: [ -3.0785   8.4794 -13.2727  44.7122  82.8213]\n",
      "Weights: [-4.8309  0.8465 -1.3128  0.1424  0.141 ]\n",
      "MSE loss: 86.4497\n",
      "Iteration: 107800\n",
      "Gradient: [  -1.9136    8.1003  -15.0358  -24.7718 -154.8835]\n",
      "Weights: [-4.8181  0.8555 -1.3137  0.1423  0.1408]\n",
      "MSE loss: 86.9012\n",
      "Iteration: 107900\n",
      "Gradient: [ -2.842  -16.0924   8.0125  63.531  263.2765]\n",
      "Weights: [-4.8114  0.8485 -1.3137  0.1428  0.1409]\n",
      "MSE loss: 86.9998\n",
      "Iteration: 108000\n",
      "Gradient: [-0.1678 -1.0977 24.2135  7.2944 13.1921]\n",
      "Weights: [-4.836   0.8456 -1.3103  0.1424  0.1408]\n",
      "MSE loss: 86.4702\n",
      "Iteration: 108100\n",
      "Gradient: [  1.7469  -2.6554  19.6597 -92.1298  22.2345]\n",
      "Weights: [-4.8218  0.8433 -1.3089  0.1413  0.1408]\n",
      "MSE loss: 86.4793\n",
      "Iteration: 108200\n",
      "Gradient: [ 3.89530e+00 -9.91920e+00  1.29000e-02  4.65665e+01  5.05060e+01]\n",
      "Weights: [-4.8264  0.8436 -1.3072  0.1417  0.141 ]\n",
      "MSE loss: 86.7213\n",
      "Iteration: 108300\n",
      "Gradient: [   1.3959  -11.4822  -38.424   -19.4381 -281.8566]\n",
      "Weights: [-4.8237  0.8362 -1.3084  0.1408  0.1411]\n",
      "MSE loss: 86.7401\n",
      "Iteration: 108400\n",
      "Gradient: [  4.4051  21.3843  18.1766  92.564  396.6263]\n",
      "Weights: [-4.813   0.8301 -1.3039  0.1413  0.141 ]\n",
      "MSE loss: 86.7093\n",
      "Iteration: 108500\n",
      "Gradient: [ -8.9715  -1.582  -30.1347 -36.366    1.8292]\n",
      "Weights: [-4.811   0.8254 -1.3043  0.1422  0.1408]\n",
      "MSE loss: 86.6134\n",
      "Iteration: 108600\n",
      "Gradient: [  -1.5759   -5.6607  -32.8191   59.4279 -156.3078]\n",
      "Weights: [-4.831   0.8318 -1.3051  0.1425  0.1408]\n",
      "MSE loss: 86.5653\n",
      "Iteration: 108700\n",
      "Gradient: [  5.873   11.451  -19.2113 -81.7897 -74.0055]\n",
      "Weights: [-4.8098  0.8254 -1.305   0.1419  0.1408]\n",
      "MSE loss: 86.6025\n",
      "Iteration: 108800\n",
      "Gradient: [  -6.2669    8.2168    6.276     2.4195 -199.201 ]\n",
      "Weights: [-4.8176  0.8311 -1.3087  0.1416  0.1409]\n",
      "MSE loss: 86.8544\n",
      "Iteration: 108900\n",
      "Gradient: [   2.6044    3.7446   -6.0184   42.0413 -114.8488]\n",
      "Weights: [-4.8222  0.8442 -1.3077  0.1415  0.1408]\n",
      "MSE loss: 86.5589\n",
      "Iteration: 109000\n",
      "Gradient: [  1.0796  -5.242   -7.325   30.2922 163.8076]\n",
      "Weights: [-4.8338  0.8397 -1.3058  0.1413  0.1405]\n",
      "MSE loss: 86.7339\n",
      "Iteration: 109100\n",
      "Gradient: [  -6.1166   -3.0163   -3.2059   53.1413 -119.7551]\n",
      "Weights: [-4.8313  0.8477 -1.3089  0.1417  0.1407]\n",
      "MSE loss: 86.4079\n",
      "Iteration: 109200\n",
      "Gradient: [ -5.268  -11.1969  -8.9324 -11.1983 -76.0454]\n",
      "Weights: [-4.8247  0.8453 -1.3072  0.1413  0.1408]\n",
      "MSE loss: 86.5101\n",
      "Iteration: 109300\n",
      "Gradient: [ -2.0238  -5.8439 -58.5477  88.7786 -72.2179]\n",
      "Weights: [-4.8258  0.8391 -1.3066  0.1417  0.1407]\n",
      "MSE loss: 86.4389\n",
      "Iteration: 109400\n",
      "Gradient: [ -8.6886 -16.6717 -39.9153  20.9028  34.3118]\n",
      "Weights: [-4.8435  0.8477 -1.3076  0.1415  0.1406]\n",
      "MSE loss: 86.6459\n",
      "Iteration: 109500\n",
      "Gradient: [  3.0109  15.6492  52.5108 -29.2983 182.2392]\n",
      "Weights: [-4.8304  0.8501 -1.3059  0.1408  0.1404]\n",
      "MSE loss: 86.4653\n",
      "Iteration: 109600\n",
      "Gradient: [  1.4083   6.8127  26.7991  41.1782 -25.9208]\n",
      "Weights: [-4.8405  0.848  -1.3059  0.1415  0.1405]\n",
      "MSE loss: 86.4464\n",
      "Iteration: 109700\n",
      "Gradient: [ -2.5891   3.0329 -12.346  170.9154 224.5374]\n",
      "Weights: [-4.8339  0.8605 -1.3079  0.1415  0.1403]\n",
      "MSE loss: 86.7965\n",
      "Iteration: 109800\n",
      "Gradient: [   0.4392    9.8884   49.7352  125.019  -153.2027]\n",
      "Weights: [-4.8363  0.8584 -1.3077  0.1422  0.1403]\n",
      "MSE loss: 86.7203\n",
      "Iteration: 109900\n",
      "Gradient: [  4.1564  -0.3841 -20.6873  26.2741  -0.6075]\n",
      "Weights: [-4.8503  0.8485 -1.309   0.1418  0.1404]\n",
      "MSE loss: 87.4096\n",
      "Iteration: 110000\n",
      "Gradient: [ -2.3964   5.162    4.1368  64.1391 136.9742]\n",
      "Weights: [-4.8399  0.851  -1.3091  0.1418  0.1408]\n",
      "MSE loss: 86.4328\n",
      "Iteration: 110100\n",
      "Gradient: [ 8.8682  7.893  18.0455 85.4152 74.8733]\n",
      "Weights: [-4.8337  0.8526 -1.3078  0.1407  0.1406]\n",
      "MSE loss: 86.4336\n",
      "Iteration: 110200\n",
      "Gradient: [ -3.3088  13.3451  -5.3939 -69.0151 388.2282]\n",
      "Weights: [-4.821   0.847  -1.3114  0.1419  0.1409]\n",
      "MSE loss: 86.4989\n",
      "Iteration: 110300\n",
      "Gradient: [  7.2157   7.0472   4.7678  81.0344 165.0182]\n",
      "Weights: [-4.8231  0.8465 -1.3081  0.1418  0.1406]\n",
      "MSE loss: 86.5481\n",
      "Iteration: 110400\n",
      "Gradient: [ 1.5746  0.4357  7.6471 76.3218 13.2983]\n",
      "Weights: [-4.8358  0.8457 -1.3069  0.1417  0.1406]\n",
      "MSE loss: 86.4279\n",
      "Iteration: 110500\n",
      "Gradient: [ -2.6591   0.7211  21.8456   3.5992 221.4304]\n",
      "Weights: [-4.8399  0.8561 -1.307   0.1404  0.1407]\n",
      "MSE loss: 86.4278\n",
      "Iteration: 110600\n",
      "Gradient: [ 7.4268 10.4786  3.6299 89.9191  7.026 ]\n",
      "Weights: [-4.8419  0.8525 -1.3094  0.1414  0.1409]\n",
      "MSE loss: 86.4556\n",
      "Iteration: 110700\n",
      "Gradient: [  12.6882   28.9482   11.3752  -24.1254 -180.9481]\n",
      "Weights: [-4.8369  0.8588 -1.3095  0.1413  0.1407]\n",
      "MSE loss: 86.5076\n",
      "Iteration: 110800\n",
      "Gradient: [  -9.4432  -29.5475  -10.4698 -103.655  -186.9233]\n",
      "Weights: [-4.8378  0.8522 -1.3093  0.141   0.1407]\n",
      "MSE loss: 86.5141\n",
      "Iteration: 110900\n",
      "Gradient: [ 12.8977  -3.5518  26.9164  24.3981 218.0722]\n",
      "Weights: [-4.8425  0.8646 -1.3125  0.1415  0.1408]\n",
      "MSE loss: 86.4382\n",
      "Iteration: 111000\n",
      "Gradient: [  6.1108   3.4808 -10.4347 114.4066 200.136 ]\n",
      "Weights: [-4.8286  0.8625 -1.3123  0.1412  0.1407]\n",
      "MSE loss: 86.7172\n",
      "Iteration: 111100\n",
      "Gradient: [-5.1234 13.5748 15.4401 77.1633 10.8829]\n",
      "Weights: [-4.8688  0.865  -1.3119  0.1426  0.1407]\n",
      "MSE loss: 87.0878\n",
      "Iteration: 111200\n",
      "Gradient: [  -7.6709    9.9541  -22.8345  -35.2573 -241.4299]\n",
      "Weights: [-4.8385  0.8692 -1.3143  0.1417  0.1403]\n",
      "MSE loss: 86.5044\n",
      "Iteration: 111300\n",
      "Gradient: [-10.8606   1.2016  -2.2779  10.4872 244.8365]\n",
      "Weights: [-4.8426  0.8615 -1.3126  0.1419  0.1406]\n",
      "MSE loss: 86.3976\n",
      "Iteration: 111400\n",
      "Gradient: [  0.3958 -10.4941 -14.7704  32.7614 145.1062]\n",
      "Weights: [-4.8391  0.8626 -1.3118  0.1418  0.1407]\n",
      "MSE loss: 86.5326\n",
      "Iteration: 111500\n",
      "Gradient: [-19.2739  -4.3958  25.4135  21.6158 214.1097]\n",
      "Weights: [-4.8517  0.8551 -1.3123  0.1429  0.1405]\n",
      "MSE loss: 86.8271\n",
      "Iteration: 111600\n",
      "Gradient: [  4.4424  -1.1821  15.2568 123.3841 -89.6497]\n",
      "Weights: [-4.832   0.8679 -1.3173  0.1431  0.1404]\n",
      "MSE loss: 86.5622\n",
      "Iteration: 111700\n",
      "Gradient: [  3.6351   4.6657  22.8053  39.7487 -41.5581]\n",
      "Weights: [-4.8261  0.8627 -1.3156  0.1429  0.1405]\n",
      "MSE loss: 86.7093\n",
      "Iteration: 111800\n",
      "Gradient: [  4.6723 -16.18    15.3857  61.6343 -47.5328]\n",
      "Weights: [-4.8307  0.8611 -1.319   0.1439  0.1408]\n",
      "MSE loss: 86.4834\n",
      "Iteration: 111900\n",
      "Gradient: [  -4.6426   -3.0673    4.208  -141.9695   87.1335]\n",
      "Weights: [-4.8373  0.8574 -1.3196  0.1445  0.1408]\n",
      "MSE loss: 86.3938\n",
      "Iteration: 112000\n",
      "Gradient: [  8.8273   8.4101   2.7587  10.7235 120.9688]\n",
      "Weights: [-4.8233  0.8508 -1.3178  0.1443  0.1406]\n",
      "MSE loss: 86.4274\n",
      "Iteration: 112100\n",
      "Gradient: [  3.6459   3.3389  23.8239 -69.0454  90.2388]\n",
      "Weights: [-4.8424  0.8559 -1.3169  0.1442  0.1409]\n",
      "MSE loss: 86.5232\n",
      "Iteration: 112200\n",
      "Gradient: [ 18.4546 -12.842   19.669   -9.002  129.7739]\n",
      "Weights: [-4.8175  0.8479 -1.3156  0.1441  0.1409]\n",
      "MSE loss: 86.7628\n",
      "Iteration: 112300\n",
      "Gradient: [ -0.9603   9.5179  -9.3222   0.2935 -36.0317]\n",
      "Weights: [-4.8263  0.855  -1.3154  0.1439  0.1406]\n",
      "MSE loss: 86.6154\n",
      "Iteration: 112400\n",
      "Gradient: [  9.5786  -6.4372  26.5345 161.405   84.7092]\n",
      "Weights: [-4.8477  0.8601 -1.3164  0.1434  0.1409]\n",
      "MSE loss: 86.5042\n",
      "Iteration: 112500\n",
      "Gradient: [   4.8104  -14.1187  -28.3939 -125.7132   14.3558]\n",
      "Weights: [-4.8246  0.8522 -1.3179  0.1441  0.1408]\n",
      "MSE loss: 86.425\n",
      "Iteration: 112600\n",
      "Gradient: [ -4.8443 -19.0954  -6.5427  98.5702  41.1977]\n",
      "Weights: [-4.8215  0.847  -1.3169  0.1448  0.1406]\n",
      "MSE loss: 86.4362\n",
      "Iteration: 112700\n",
      "Gradient: [ -2.8843  13.439  -15.7631  46.2291 345.5338]\n",
      "Weights: [-4.8333  0.8537 -1.3148  0.1443  0.1404]\n",
      "MSE loss: 86.3781\n",
      "Iteration: 112800\n",
      "Gradient: [  8.7202  -4.6389  17.8221  29.4584 156.2514]\n",
      "Weights: [-4.8303  0.8579 -1.3179  0.1445  0.1405]\n",
      "MSE loss: 86.4012\n",
      "Iteration: 112900\n",
      "Gradient: [ -1.7464  19.718  -37.1738  53.3518 119.0419]\n",
      "Weights: [-4.8411  0.8601 -1.317   0.1434  0.1407]\n",
      "MSE loss: 86.4119\n",
      "Iteration: 113000\n",
      "Gradient: [  0.1931  -9.4028  24.3936 -32.8162 113.8759]\n",
      "Weights: [-4.8467  0.8617 -1.316   0.143   0.1407]\n",
      "MSE loss: 86.4572\n",
      "Iteration: 113100\n",
      "Gradient: [ -3.2637  -4.4343 -25.1538 -71.2916 -54.0091]\n",
      "Weights: [-4.831   0.8461 -1.3132  0.1436  0.1405]\n",
      "MSE loss: 86.4623\n",
      "Iteration: 113200\n",
      "Gradient: [ -0.8001  -1.571   12.6213 -11.3141  79.8616]\n",
      "Weights: [-4.8474  0.8509 -1.3109  0.1438  0.1404]\n",
      "MSE loss: 86.57\n",
      "Iteration: 113300\n",
      "Gradient: [ -4.8934   3.9676  -5.588    4.476  -70.1065]\n",
      "Weights: [-4.8369  0.8585 -1.3129  0.1427  0.1404]\n",
      "MSE loss: 86.3904\n",
      "Iteration: 113400\n",
      "Gradient: [  5.9981 -21.5742 -16.0004  45.7998 128.5766]\n",
      "Weights: [-4.8364  0.8588 -1.3106  0.1429  0.1403]\n",
      "MSE loss: 86.5571\n",
      "Iteration: 113500\n",
      "Gradient: [  11.933    37.4876   20.3748  165.8526 -201.2719]\n",
      "Weights: [-4.8291  0.8586 -1.3108  0.1425  0.1402]\n",
      "MSE loss: 86.6386\n",
      "Iteration: 113600\n",
      "Gradient: [  7.0199 -10.8867 -33.2597 -27.7979  -7.4247]\n",
      "Weights: [-4.8411  0.8597 -1.3103  0.1428  0.1403]\n",
      "MSE loss: 86.5316\n",
      "Iteration: 113700\n",
      "Gradient: [  -5.001   -15.8422  -31.2194   21.774  -266.8205]\n",
      "Weights: [-4.8477  0.8554 -1.3091  0.1422  0.1403]\n",
      "MSE loss: 86.54\n",
      "Iteration: 113800\n",
      "Gradient: [ 2.0262  2.382  -4.9738  9.3203 89.7379]\n",
      "Weights: [-4.8201  0.8471 -1.3089  0.143   0.1401]\n",
      "MSE loss: 86.5914\n",
      "Iteration: 113900\n",
      "Gradient: [  1.3852   5.4051  37.731   13.2505 101.8118]\n",
      "Weights: [-4.8218  0.8474 -1.3097  0.1424  0.1404]\n",
      "MSE loss: 86.5181\n",
      "Iteration: 114000\n",
      "Gradient: [-11.1026  -8.1348 -16.356  -25.6835 -12.495 ]\n",
      "Weights: [-4.84    0.8496 -1.3106  0.1429  0.1405]\n",
      "MSE loss: 86.4773\n",
      "Iteration: 114100\n",
      "Gradient: [  2.2386   5.2337 -35.0554 -33.2952 -95.3075]\n",
      "Weights: [-4.8328  0.8436 -1.3094  0.1428  0.1404]\n",
      "MSE loss: 86.52\n",
      "Iteration: 114200\n",
      "Gradient: [ 3.695800e+00 -7.240000e-02 -3.293140e+01  1.366396e+02 -4.655660e+01]\n",
      "Weights: [-4.8352  0.8592 -1.3108  0.1423  0.1401]\n",
      "MSE loss: 86.4595\n",
      "Iteration: 114300\n",
      "Gradient: [  -8.2431   20.5742    2.64   -100.3541   48.9947]\n",
      "Weights: [-4.8544  0.8668 -1.3135  0.1425  0.1403]\n",
      "MSE loss: 86.5627\n",
      "Iteration: 114400\n",
      "Gradient: [-1.024460e+01  3.147000e-01  1.046340e+01 -4.964340e+01 -3.490906e+02]\n",
      "Weights: [-4.8454  0.8573 -1.3103  0.1434  0.1402]\n",
      "MSE loss: 86.4446\n",
      "Iteration: 114500\n",
      "Gradient: [ -2.4948   9.987  -33.544    3.5653 -43.9746]\n",
      "Weights: [-4.827   0.8429 -1.3089  0.1433  0.1403]\n",
      "MSE loss: 86.4035\n",
      "Iteration: 114600\n",
      "Gradient: [  8.9209 -18.543    3.6919  65.798  -63.9189]\n",
      "Weights: [-4.8241  0.8418 -1.3095  0.1441  0.1402]\n",
      "MSE loss: 86.448\n",
      "Iteration: 114700\n",
      "Gradient: [  1.278  -18.3489 -29.171  -38.2076 112.1325]\n",
      "Weights: [-4.837   0.8464 -1.3081  0.1435  0.1403]\n",
      "MSE loss: 86.4512\n",
      "Iteration: 114800\n",
      "Gradient: [  -8.9504    7.9349   36.6134   29.9609 -233.2482]\n",
      "Weights: [-4.8307  0.839  -1.3088  0.1441  0.1403]\n",
      "MSE loss: 86.4293\n",
      "Iteration: 114900\n",
      "Gradient: [ -8.9248   0.1697 -13.4834 -14.1976  16.7002]\n",
      "Weights: [-4.8329  0.8356 -1.3104  0.1441  0.1405]\n",
      "MSE loss: 86.7425\n",
      "Iteration: 115000\n",
      "Gradient: [  1.1546  11.5618 -13.2474   4.9159 163.0281]\n",
      "Weights: [-4.8138  0.8351 -1.3089  0.1441  0.1404]\n",
      "MSE loss: 86.6321\n",
      "Iteration: 115100\n",
      "Gradient: [-3.00000e-04 -1.95460e+00 -3.98980e+00 -8.92240e+00 -7.02183e+01]\n",
      "Weights: [-4.823   0.8301 -1.305   0.144   0.1399]\n",
      "MSE loss: 86.4743\n",
      "Iteration: 115200\n",
      "Gradient: [  5.8528   7.6308 -21.4871  38.1313 -94.3082]\n",
      "Weights: [-4.8216  0.8344 -1.3043  0.1436  0.1397]\n",
      "MSE loss: 86.4758\n",
      "Iteration: 115300\n",
      "Gradient: [ -0.9243 -12.5404  29.2165  26.592  240.5579]\n",
      "Weights: [-4.8327  0.8375 -1.3023  0.1436  0.1399]\n",
      "MSE loss: 86.4965\n",
      "Iteration: 115400\n",
      "Gradient: [ -5.887    3.2364   6.0323 -22.8038  45.8313]\n",
      "Weights: [-4.8052  0.8312 -1.3066  0.1438  0.14  ]\n",
      "MSE loss: 86.775\n",
      "Iteration: 115500\n",
      "Gradient: [  3.4126 -11.5401  40.1379   6.0848  81.9027]\n",
      "Weights: [-4.8242  0.8478 -1.3095  0.1433  0.1399]\n",
      "MSE loss: 86.5116\n",
      "Iteration: 115600\n",
      "Gradient: [  -0.663    -6.1982   41.9125  -64.6633 -136.2178]\n",
      "Weights: [-4.8294  0.8484 -1.3095  0.1436  0.14  ]\n",
      "MSE loss: 86.4114\n",
      "Iteration: 115700\n",
      "Gradient: [  -4.456     2.6201   24.034    78.5948 -161.7037]\n",
      "Weights: [-4.8295  0.8524 -1.3143  0.1433  0.1402]\n",
      "MSE loss: 86.5682\n",
      "Iteration: 115800\n",
      "Gradient: [ -1.6578  12.0669 -17.278  -12.2152 233.6213]\n",
      "Weights: [-4.8305  0.8523 -1.3106  0.1428  0.1404]\n",
      "MSE loss: 86.4471\n",
      "Iteration: 115900\n",
      "Gradient: [  5.5457   2.0936  53.8011  22.1044 162.2421]\n",
      "Weights: [-4.8321  0.8504 -1.3105  0.1438  0.1403]\n",
      "MSE loss: 86.4995\n",
      "Iteration: 116000\n",
      "Gradient: [  -0.3424   -0.6976    5.4306   17.1039 -170.0414]\n",
      "Weights: [-4.8326  0.8462 -1.3123  0.1441  0.1402]\n",
      "MSE loss: 86.45\n",
      "Iteration: 116100\n",
      "Gradient: [  1.599  -11.3235  23.6657 -16.4623  51.3365]\n",
      "Weights: [-4.836   0.8475 -1.3109  0.1437  0.14  ]\n",
      "MSE loss: 86.5709\n",
      "Iteration: 116200\n",
      "Gradient: [  1.8298 -20.4389  -4.8454 -98.4486  29.0563]\n",
      "Weights: [-4.8303  0.8476 -1.3109  0.1442  0.14  ]\n",
      "MSE loss: 86.379\n",
      "Iteration: 116300\n",
      "Gradient: [ -8.769  -16.3763 -25.5026  33.4023 204.0499]\n",
      "Weights: [-4.8276  0.8345 -1.3096  0.1449  0.1402]\n",
      "MSE loss: 86.4692\n",
      "Iteration: 116400\n",
      "Gradient: [  0.3411  11.8775 -19.8514 -75.6949 -40.8127]\n",
      "Weights: [-4.8323  0.8404 -1.3056  0.143   0.14  ]\n",
      "MSE loss: 86.4792\n",
      "Iteration: 116500\n",
      "Gradient: [  7.9684   7.0124 -41.5604  13.5774  66.1298]\n",
      "Weights: [-4.8341  0.8472 -1.3065  0.1429  0.1401]\n",
      "MSE loss: 86.4056\n",
      "Iteration: 116600\n",
      "Gradient: [  9.5556   2.1843   5.3303 -15.1882 178.3105]\n",
      "Weights: [-4.8431  0.8498 -1.3041  0.1426  0.14  ]\n",
      "MSE loss: 86.5068\n",
      "Iteration: 116700\n",
      "Gradient: [-15.5149   2.1266 -32.7164 -30.0812 -22.3257]\n",
      "Weights: [-4.835   0.8372 -1.3033  0.1417  0.1403]\n",
      "MSE loss: 86.6552\n",
      "Iteration: 116800\n",
      "Gradient: [ -1.3307  -7.6479  -5.3104  36.7535 131.678 ]\n",
      "Weights: [-4.8389  0.8373 -1.3013  0.1416  0.1404]\n",
      "MSE loss: 86.5217\n",
      "Iteration: 116900\n",
      "Gradient: [  4.944  -16.7038 -13.5417 108.7869 -75.1033]\n",
      "Weights: [-4.822   0.8414 -1.3036  0.1415  0.1402]\n",
      "MSE loss: 86.5374\n",
      "Iteration: 117000\n",
      "Gradient: [-15.8869  -4.1903   4.9154  74.5248 260.3034]\n",
      "Weights: [-4.8493  0.8492 -1.3044  0.1416  0.1402]\n",
      "MSE loss: 86.647\n",
      "Iteration: 117100\n",
      "Gradient: [  5.3836  -2.9246  44.6703   7.0286 300.9893]\n",
      "Weights: [-4.836   0.8427 -1.3029  0.1422  0.1403]\n",
      "MSE loss: 86.5412\n",
      "Iteration: 117200\n",
      "Gradient: [   6.3164   16.9409  -11.848  -102.7046   43.215 ]\n",
      "Weights: [-4.8382  0.8457 -1.3034  0.1423  0.1404]\n",
      "MSE loss: 86.7194\n",
      "Iteration: 117300\n",
      "Gradient: [  -5.8199   -0.2494  -20.4221   -7.9644 -116.4308]\n",
      "Weights: [-4.8357  0.8317 -1.3029  0.1432  0.1403]\n",
      "MSE loss: 86.573\n",
      "Iteration: 117400\n",
      "Gradient: [ -7.4176 -16.7249   9.5368 -16.8197  72.6088]\n",
      "Weights: [-4.8081  0.8234 -1.2991  0.1419  0.1403]\n",
      "MSE loss: 86.7092\n",
      "Iteration: 117500\n",
      "Gradient: [ -10.9555  -11.8371   12.5348 -124.2961 -169.7007]\n",
      "Weights: [-4.8354  0.8326 -1.3007  0.1418  0.1401]\n",
      "MSE loss: 86.7294\n",
      "Iteration: 117600\n",
      "Gradient: [  7.5455   1.7755 -22.0482  34.701  -39.769 ]\n",
      "Weights: [-4.8231  0.8299 -1.3005  0.1426  0.14  ]\n",
      "MSE loss: 86.4449\n",
      "Iteration: 117700\n",
      "Gradient: [ -6.8487 -11.4418 -16.3399 -66.3102 129.8302]\n",
      "Weights: [-4.8428  0.8309 -1.2956  0.1422  0.1398]\n",
      "MSE loss: 86.6257\n",
      "Iteration: 117800\n",
      "Gradient: [  4.8299 -18.5967  14.7061 -26.1711 157.2878]\n",
      "Weights: [-4.832   0.8261 -1.2992  0.1433  0.1398]\n",
      "MSE loss: 86.569\n",
      "Iteration: 117900\n",
      "Gradient: [ 18.5938   8.2243  47.2485  73.3157 210.998 ]\n",
      "Weights: [-4.8175  0.8355 -1.3024  0.143   0.1399]\n",
      "MSE loss: 86.679\n",
      "Iteration: 118000\n",
      "Gradient: [-4.6978  3.8923 37.9965 -1.4214 64.6253]\n",
      "Weights: [-4.8393  0.8357 -1.3025  0.1433  0.14  ]\n",
      "MSE loss: 86.5407\n",
      "Iteration: 118100\n",
      "Gradient: [ -2.3409  -0.7563 -54.0328  86.8837 -58.1492]\n",
      "Weights: [-4.831   0.8426 -1.3067  0.1431  0.1401]\n",
      "MSE loss: 86.4045\n",
      "Iteration: 118200\n",
      "Gradient: [-10.2887   0.7304  -4.7892  -7.0597 170.6938]\n",
      "Weights: [-4.8494  0.8467 -1.3084  0.1436  0.1402]\n",
      "MSE loss: 86.727\n",
      "Iteration: 118300\n",
      "Gradient: [-13.4382  15.5785  12.5676 -21.8209  99.6962]\n",
      "Weights: [-4.8317  0.8385 -1.3083  0.1441  0.1401]\n",
      "MSE loss: 86.4493\n",
      "Iteration: 118400\n",
      "Gradient: [  7.4426   5.3348  29.7653  39.1227 -67.6509]\n",
      "Weights: [-4.8329  0.8416 -1.3067  0.1448  0.1398]\n",
      "MSE loss: 86.4414\n",
      "Iteration: 118500\n",
      "Gradient: [  6.5229  -1.3075  -4.7639  19.589  -35.2523]\n",
      "Weights: [-4.8188  0.8354 -1.3054  0.145   0.1395]\n",
      "MSE loss: 86.5128\n",
      "Iteration: 118600\n",
      "Gradient: [  6.6847   5.378  -28.8559  -8.991  -63.4465]\n",
      "Weights: [-4.8205  0.83   -1.3064  0.145   0.1396]\n",
      "MSE loss: 86.531\n",
      "Iteration: 118700\n",
      "Gradient: [ -9.9735  -3.9089  31.1004 -27.2829 -77.0842]\n",
      "Weights: [-4.8293  0.8336 -1.3072  0.1451  0.1397]\n",
      "MSE loss: 86.4929\n",
      "Iteration: 118800\n",
      "Gradient: [  5.5529  14.0811  10.8678 117.1865  82.1788]\n",
      "Weights: [-4.8198  0.828  -1.3063  0.1454  0.1396]\n",
      "MSE loss: 86.5114\n",
      "Iteration: 118900\n",
      "Gradient: [  7.9501  15.6537  10.8109 -77.4813  79.2851]\n",
      "Weights: [-4.8344  0.8308 -1.3019  0.146   0.1393]\n",
      "MSE loss: 86.4972\n",
      "Iteration: 119000\n",
      "Gradient: [ -1.3272 -14.4498  -6.0182 -85.8857  46.6103]\n",
      "Weights: [-4.8306  0.8271 -1.3002  0.1448  0.1393]\n",
      "MSE loss: 86.4965\n",
      "Iteration: 119100\n",
      "Gradient: [ -2.6883  -8.623  -26.2531 -41.1258  16.4428]\n",
      "Weights: [-4.8314  0.8276 -1.3016  0.1449  0.1392]\n",
      "MSE loss: 86.7493\n",
      "Iteration: 119200\n",
      "Gradient: [  8.183    0.8114  19.0645  15.8819 118.2683]\n",
      "Weights: [-4.8305  0.83   -1.3012  0.144   0.1394]\n",
      "MSE loss: 86.588\n",
      "Iteration: 119300\n",
      "Gradient: [  -2.692    -0.8809  -26.4995   61.0235 -213.6591]\n",
      "Weights: [-4.8293  0.8372 -1.3021  0.1439  0.1393]\n",
      "MSE loss: 86.4538\n",
      "Iteration: 119400\n",
      "Gradient: [  2.8066  14.6996   4.3864 -44.64    17.516 ]\n",
      "Weights: [-4.8373  0.8427 -1.3011  0.1443  0.1394]\n",
      "MSE loss: 86.6258\n",
      "Iteration: 119500\n",
      "Gradient: [  5.0026 -34.254   23.362   -9.262  164.8   ]\n",
      "Weights: [-4.8562  0.859  -1.3054  0.1439  0.1393]\n",
      "MSE loss: 86.5827\n",
      "Iteration: 119600\n",
      "Gradient: [  -4.7815   -1.8018   25.0774   99.0557 -174.4886]\n",
      "Weights: [-4.8513  0.8549 -1.3047  0.1439  0.1392]\n",
      "MSE loss: 86.5326\n",
      "Iteration: 119700\n",
      "Gradient: [  0.4998   4.9278 -11.6484  32.676   44.1877]\n",
      "Weights: [-4.8452  0.8545 -1.3035  0.1436  0.1394]\n",
      "MSE loss: 86.5695\n",
      "Iteration: 119800\n",
      "Gradient: [  5.2165   6.9042  22.1092 -29.9917  85.3399]\n",
      "Weights: [-4.8279  0.8543 -1.3043  0.1436  0.1392]\n",
      "MSE loss: 87.0\n",
      "Iteration: 119900\n",
      "Gradient: [   8.6978  -44.3253   -3.6947 -176.0548   -8.1462]\n",
      "Weights: [-4.8406  0.8472 -1.3056  0.1442  0.1392]\n",
      "MSE loss: 86.5404\n",
      "Iteration: 120000\n",
      "Gradient: [   8.6268   -3.3415   20.5551 -100.487  -287.4611]\n",
      "Weights: [-4.8232  0.8357 -1.3029  0.1451  0.1392]\n",
      "MSE loss: 86.4879\n",
      "Iteration: 120100\n",
      "Gradient: [ 10.675    7.4316 -21.4824  42.2583 232.2438]\n",
      "Weights: [-4.8325  0.8382 -1.3025  0.1454  0.1391]\n",
      "MSE loss: 86.4545\n",
      "Iteration: 120200\n",
      "Gradient: [  -2.4962  -16.088    57.0577   66.7202 -213.1353]\n",
      "Weights: [-4.8156  0.8255 -1.303   0.1454  0.1394]\n",
      "MSE loss: 86.4654\n",
      "Iteration: 120300\n",
      "Gradient: [  5.2919  -6.7868   7.8783 107.559  199.1231]\n",
      "Weights: [-4.8318  0.8409 -1.3069  0.1455  0.1394]\n",
      "MSE loss: 86.3746\n",
      "Iteration: 120400\n",
      "Gradient: [  9.5974 -12.4736  12.2411 -28.1658 -24.5346]\n",
      "Weights: [-4.8241  0.8355 -1.3041  0.1452  0.1392]\n",
      "MSE loss: 86.4264\n",
      "Iteration: 120500\n",
      "Gradient: [ -2.4807 -11.0514 -26.9967 120.7361 126.4942]\n",
      "Weights: [-4.8271  0.8237 -1.3001  0.1461  0.1392]\n",
      "MSE loss: 86.4485\n",
      "Iteration: 120600\n",
      "Gradient: [-6.0368 -2.2235 18.6304 41.4084 18.2125]\n",
      "Weights: [-4.8364  0.8237 -1.2967  0.1452  0.139 ]\n",
      "MSE loss: 86.559\n",
      "Iteration: 120700\n",
      "Gradient: [ 6.6471  6.9324 22.7438 44.2955 26.8405]\n",
      "Weights: [-4.8245  0.8269 -1.2953  0.1447  0.139 ]\n",
      "MSE loss: 86.6255\n",
      "Iteration: 120800\n",
      "Gradient: [ -3.6887   1.0205  45.11   147.5607  59.0292]\n",
      "Weights: [-4.82    0.8144 -1.2931  0.1448  0.1391]\n",
      "MSE loss: 86.4913\n",
      "Iteration: 120900\n",
      "Gradient: [  8.6127   6.6124  -3.4801  99.6438 132.9774]\n",
      "Weights: [-4.8105  0.8128 -1.2945  0.1449  0.1391]\n",
      "MSE loss: 86.5714\n",
      "Iteration: 121000\n",
      "Gradient: [  8.548    0.3739  -1.1195 -44.5602 194.1274]\n",
      "Weights: [-4.8015  0.8136 -1.2952  0.1445  0.1392]\n",
      "MSE loss: 86.9041\n",
      "Iteration: 121100\n",
      "Gradient: [  3.338   -1.5791 -38.2938 -67.1829  80.9005]\n",
      "Weights: [-4.8183  0.818  -1.2938  0.1451  0.139 ]\n",
      "MSE loss: 86.6699\n",
      "Iteration: 121200\n",
      "Gradient: [   2.9021   -1.0821  -31.7177   -6.2268 -143.7299]\n",
      "Weights: [-4.8225  0.8203 -1.2955  0.1451  0.1389]\n",
      "MSE loss: 86.4428\n",
      "Iteration: 121300\n",
      "Gradient: [  0.2304 -12.3878 -38.1883 -21.3394  44.2962]\n",
      "Weights: [-4.8131  0.819  -1.297   0.1453  0.1389]\n",
      "MSE loss: 86.5273\n",
      "Iteration: 121400\n",
      "Gradient: [ -1.397   -9.1174  21.4741  17.7993 151.6766]\n",
      "Weights: [-4.8281  0.8308 -1.2999  0.1452  0.1389]\n",
      "MSE loss: 86.4197\n",
      "Iteration: 121500\n",
      "Gradient: [ 1.1611 -3.7423 -0.352   9.523  -9.0239]\n",
      "Weights: [-4.8308  0.8322 -1.2989  0.1449  0.139 ]\n",
      "MSE loss: 86.4341\n",
      "Iteration: 121600\n",
      "Gradient: [  2.2988   7.1195   0.276   -2.11   -21.9901]\n",
      "Weights: [-4.8221  0.828  -1.2989  0.1447  0.1392]\n",
      "MSE loss: 86.5037\n",
      "Iteration: 121700\n",
      "Gradient: [ -0.289  -35.5649 -39.9662 -76.8519   2.9537]\n",
      "Weights: [-4.8308  0.8239 -1.3007  0.1451  0.1393]\n",
      "MSE loss: 86.6022\n",
      "Iteration: 121800\n",
      "Gradient: [  -4.6964  -18.9945  -16.0854   56.6854 -184.2022]\n",
      "Weights: [-4.8532  0.8413 -1.3006  0.1447  0.139 ]\n",
      "MSE loss: 86.8396\n",
      "Iteration: 121900\n",
      "Gradient: [ 11.2139   9.8287  94.6045  81.327  307.2675]\n",
      "Weights: [-4.8311  0.8473 -1.3028  0.1447  0.1389]\n",
      "MSE loss: 86.6506\n",
      "Iteration: 122000\n",
      "Gradient: [  6.6706  11.0381  -5.6093  85.1621 308.4086]\n",
      "Weights: [-4.8329  0.853  -1.307   0.1453  0.139 ]\n",
      "MSE loss: 86.6034\n",
      "Iteration: 122100\n",
      "Gradient: [ 8.1815  5.9174 11.1036 81.7827 16.0989]\n",
      "Weights: [-4.8342  0.8537 -1.3098  0.1457  0.1393]\n",
      "MSE loss: 86.5004\n",
      "Iteration: 122200\n",
      "Gradient: [  -1.7947   14.6485  -31.4032  -17.2267 -184.5897]\n",
      "Weights: [-4.8505  0.8518 -1.3105  0.1456  0.1394]\n",
      "MSE loss: 86.6955\n",
      "Iteration: 122300\n",
      "Gradient: [ -7.5775  -3.1841  17.2341 -31.1762 140.0329]\n",
      "Weights: [-4.8541  0.8447 -1.3094  0.1469  0.1391]\n",
      "MSE loss: 87.1972\n",
      "Iteration: 122400\n",
      "Gradient: [ -15.9014   -0.3572  -13.7287   88.5983 -172.2611]\n",
      "Weights: [-4.8427  0.8501 -1.3114  0.1471  0.139 ]\n",
      "MSE loss: 86.4583\n",
      "Iteration: 122500\n",
      "Gradient: [ 4.9066  6.5588 -9.4392 28.8362 88.8722]\n",
      "Weights: [-4.8267  0.8432 -1.3095  0.1473  0.139 ]\n",
      "MSE loss: 86.4475\n",
      "Iteration: 122600\n",
      "Gradient: [  10.997    19.7824   17.074    31.1412 -140.8458]\n",
      "Weights: [-4.8294  0.838  -1.3085  0.1481  0.1388]\n",
      "MSE loss: 86.3523\n",
      "Iteration: 122700\n",
      "Gradient: [ -4.3954 -19.603  -29.463   24.8187  61.4117]\n",
      "Weights: [-4.8231  0.8401 -1.309   0.1476  0.1389]\n",
      "MSE loss: 86.4512\n",
      "Iteration: 122800\n",
      "Gradient: [ -2.0226   9.5948  -0.4966 -64.1672  -6.6085]\n",
      "Weights: [-4.8118  0.8289 -1.3078  0.1482  0.1387]\n",
      "MSE loss: 86.519\n",
      "Iteration: 122900\n",
      "Gradient: [ -12.0861   10.7345    9.7591  -42.9697 -116.5013]\n",
      "Weights: [-4.8395  0.8396 -1.3071  0.1476  0.1388]\n",
      "MSE loss: 86.4285\n",
      "Iteration: 123000\n",
      "Gradient: [-1.92400e-01 -4.34490e+00 -9.54160e+00  5.08737e+01 -2.23703e+02]\n",
      "Weights: [-4.8402  0.8362 -1.305   0.1476  0.1388]\n",
      "MSE loss: 86.4706\n",
      "Iteration: 123100\n",
      "Gradient: [ -4.6076   7.7005  -9.335  -51.0374  63.8545]\n",
      "Weights: [-4.8335  0.8319 -1.3059  0.1482  0.1388]\n",
      "MSE loss: 86.413\n",
      "Iteration: 123200\n",
      "Gradient: [ -1.0907  -4.3175  30.6221 -22.538  332.4096]\n",
      "Weights: [-4.8269  0.8275 -1.3041  0.148   0.1388]\n",
      "MSE loss: 86.4058\n",
      "Iteration: 123300\n",
      "Gradient: [ 11.3558 -11.4891 -31.1635  -0.3252  91.0858]\n",
      "Weights: [-4.8413  0.8372 -1.3087  0.1484  0.1388]\n",
      "MSE loss: 86.6003\n",
      "Iteration: 123400\n",
      "Gradient: [ -8.908   -1.8743  -4.9784 -50.6597 276.1361]\n",
      "Weights: [-4.8359  0.8393 -1.3047  0.1473  0.1387]\n",
      "MSE loss: 86.4161\n",
      "Iteration: 123500\n",
      "Gradient: [  5.5709   6.1399 -26.2544  28.2274 -28.4793]\n",
      "Weights: [-4.8094  0.8276 -1.3058  0.1473  0.1388]\n",
      "MSE loss: 86.6116\n",
      "Iteration: 123600\n",
      "Gradient: [   8.5997    1.1034   12.4386   26.9574 -181.5482]\n",
      "Weights: [-4.8401  0.8364 -1.3028  0.1472  0.1388]\n",
      "MSE loss: 86.5362\n",
      "Iteration: 123700\n",
      "Gradient: [  -7.7327  -15.0243  -11.5146   10.1785 -301.4646]\n",
      "Weights: [-4.828   0.8295 -1.3045  0.1468  0.1389]\n",
      "MSE loss: 86.4322\n",
      "Iteration: 123800\n",
      "Gradient: [-0.8096  5.2771 52.8557 55.3085 53.8681]\n",
      "Weights: [-4.8136  0.8179 -1.3017  0.1469  0.139 ]\n",
      "MSE loss: 86.4557\n",
      "Iteration: 123900\n",
      "Gradient: [   6.1042    4.7325    3.8643 -196.7923  190.1639]\n",
      "Weights: [-4.8261  0.8395 -1.3063  0.147   0.1389]\n",
      "MSE loss: 86.4595\n",
      "Iteration: 124000\n",
      "Gradient: [   5.6793  -22.6488   13.41     75.3664 -119.5874]\n",
      "Weights: [-4.8382  0.8401 -1.3072  0.1471  0.139 ]\n",
      "MSE loss: 86.407\n",
      "Iteration: 124100\n",
      "Gradient: [-10.5367 -13.6437   4.8137  -8.0912 -32.4037]\n",
      "Weights: [-4.8363  0.8391 -1.3076  0.1465  0.139 ]\n",
      "MSE loss: 86.5101\n",
      "Iteration: 124200\n",
      "Gradient: [   7.9067   -0.6333   -8.8237   25.7041 -138.2755]\n",
      "Weights: [-4.8243  0.8355 -1.306   0.1464  0.1391]\n",
      "MSE loss: 86.4086\n",
      "Iteration: 124300\n",
      "Gradient: [ -2.0649   3.2435 -14.6096 -74.4384 -73.0021]\n",
      "Weights: [-4.8374  0.8439 -1.3086  0.1465  0.1391]\n",
      "MSE loss: 86.4055\n",
      "Iteration: 124400\n",
      "Gradient: [ -8.2205  -0.693  -12.4718  94.1518  -1.2934]\n",
      "Weights: [-4.8327  0.8379 -1.307   0.1464  0.1394]\n",
      "MSE loss: 86.3946\n",
      "Iteration: 124500\n",
      "Gradient: [ -9.4845   7.1714  -8.0432  27.0811 -42.1243]\n",
      "Weights: [-4.8348  0.836  -1.3065  0.147   0.1391]\n",
      "MSE loss: 86.4049\n",
      "Iteration: 124600\n",
      "Gradient: [   5.9457    2.986    12.313   -92.752  -419.438 ]\n",
      "Weights: [-4.8196  0.8491 -1.3107  0.1461  0.1391]\n",
      "MSE loss: 86.7162\n",
      "Iteration: 124700\n",
      "Gradient: [   0.9805    1.5592   -1.6189   41.6088 -121.3164]\n",
      "Weights: [-4.8419  0.8537 -1.3103  0.1461  0.1393]\n",
      "MSE loss: 86.4155\n",
      "Iteration: 124800\n",
      "Gradient: [  -5.5194    3.7065   -7.6668    3.614  -169.7938]\n",
      "Weights: [-4.8437  0.8519 -1.3107  0.1461  0.1392]\n",
      "MSE loss: 86.4276\n",
      "Iteration: 124900\n",
      "Gradient: [  6.9302   3.5929  -0.5906  36.0928 -18.4183]\n",
      "Weights: [-4.8411  0.8545 -1.3113  0.1459  0.1392]\n",
      "MSE loss: 86.3875\n",
      "Iteration: 125000\n",
      "Gradient: [ -3.3605  -1.8729  -1.8271 -37.9786 388.3363]\n",
      "Weights: [-4.8388  0.8573 -1.3112  0.1452  0.1395]\n",
      "MSE loss: 86.4125\n",
      "Iteration: 125100\n",
      "Gradient: [ -4.7637 -10.8748 -21.9611 -38.8942 -36.074 ]\n",
      "Weights: [-4.8438  0.8546 -1.313   0.1455  0.1396]\n",
      "MSE loss: 86.4313\n",
      "Iteration: 125200\n",
      "Gradient: [  3.5492  -9.8226  23.7684  35.5833 -93.7979]\n",
      "Weights: [-4.8355  0.8567 -1.3135  0.1451  0.1396]\n",
      "MSE loss: 86.3872\n",
      "Iteration: 125300\n",
      "Gradient: [ -8.6838  12.9955  -1.5338  26.5937 136.1292]\n",
      "Weights: [-4.837   0.865  -1.3144  0.1451  0.1396]\n",
      "MSE loss: 86.5792\n",
      "Iteration: 125400\n",
      "Gradient: [  0.052   -0.8457  12.917  -40.5628 -20.9687]\n",
      "Weights: [-4.8342  0.8589 -1.3167  0.146   0.1397]\n",
      "MSE loss: 86.3715\n",
      "Iteration: 125500\n",
      "Gradient: [  -2.4542  -18.5355  -57.518    14.8114 -173.0795]\n",
      "Weights: [-4.8516  0.8625 -1.3175  0.1462  0.1396]\n",
      "MSE loss: 86.5938\n",
      "Iteration: 125600\n",
      "Gradient: [ 4.73820e+00 -8.24760e+00 -3.87000e-02  5.22750e+01 -8.01825e+01]\n",
      "Weights: [-4.8416  0.8585 -1.3165  0.1456  0.1397]\n",
      "MSE loss: 86.4329\n",
      "Iteration: 125700\n",
      "Gradient: [  1.784    2.5297  11.0895 -79.1959 -84.9155]\n",
      "Weights: [-4.8388  0.8535 -1.3151  0.1457  0.1398]\n",
      "MSE loss: 86.3817\n",
      "Iteration: 125800\n",
      "Gradient: [  0.9536  10.6686 -12.9895 -10.8932  70.099 ]\n",
      "Weights: [-4.8445  0.858  -1.3135  0.1463  0.1395]\n",
      "MSE loss: 86.4365\n",
      "Iteration: 125900\n",
      "Gradient: [-16.4107  -6.6611  23.3886  -6.7358  80.6765]\n",
      "Weights: [-4.8494  0.8575 -1.3157  0.1463  0.1396]\n",
      "MSE loss: 86.5692\n",
      "Iteration: 126000\n",
      "Gradient: [  5.3764  28.3986   2.5881  21.3809 -12.4371]\n",
      "Weights: [-4.8333  0.861  -1.3151  0.1462  0.1393]\n",
      "MSE loss: 86.4742\n",
      "Iteration: 126100\n",
      "Gradient: [ -14.4227   -8.7646  -28.2899  -26.5625 -297.8469]\n",
      "Weights: [-4.8512  0.8599 -1.3137  0.1464  0.1394]\n",
      "MSE loss: 86.4564\n",
      "Iteration: 126200\n",
      "Gradient: [ 2.637000e-01  1.056150e+01  3.739500e+00 -1.328000e-01 -1.474832e+02]\n",
      "Weights: [-4.8225  0.8494 -1.3127  0.1464  0.1394]\n",
      "MSE loss: 86.599\n",
      "Iteration: 126300\n",
      "Gradient: [ -7.0889 -19.5831   0.9886  -6.6109 251.8998]\n",
      "Weights: [-4.8209  0.8418 -1.314   0.1474  0.1395]\n",
      "MSE loss: 86.4331\n",
      "Iteration: 126400\n",
      "Gradient: [  4.3209 -21.2343  -8.7739  80.3828  -4.2732]\n",
      "Weights: [-4.8239  0.8399 -1.3167  0.1478  0.1397]\n",
      "MSE loss: 86.4339\n",
      "Iteration: 126500\n",
      "Gradient: [  -4.1347    2.1459  -26.6415  -80.2741 -128.641 ]\n",
      "Weights: [-4.8175  0.8443 -1.3168  0.1473  0.1398]\n",
      "MSE loss: 86.4904\n",
      "Iteration: 126600\n",
      "Gradient: [  3.3309  -2.6131  22.9628 -27.6588 164.1963]\n",
      "Weights: [-4.824   0.8477 -1.3142  0.1472  0.1398]\n",
      "MSE loss: 86.895\n",
      "Iteration: 126700\n",
      "Gradient: [  -0.3395   -5.2101  -22.4778   -1.0036 -106.5697]\n",
      "Weights: [-4.8391  0.8398 -1.3131  0.1481  0.1395]\n",
      "MSE loss: 86.5391\n",
      "Iteration: 126800\n",
      "Gradient: [  -8.4269   -5.8371   20.356  -126.2985  241.8784]\n",
      "Weights: [-4.8297  0.8422 -1.3134  0.1485  0.1393]\n",
      "MSE loss: 86.4088\n",
      "Iteration: 126900\n",
      "Gradient: [ -9.2384   5.5096 -11.3528  25.5014 -81.9789]\n",
      "Weights: [-4.8171  0.8401 -1.3158  0.1487  0.1392]\n",
      "MSE loss: 86.4273\n",
      "Iteration: 127000\n",
      "Gradient: [  5.9228 -10.7323  13.2428  24.3638 -95.2014]\n",
      "Weights: [-4.8091  0.8251 -1.3105  0.1491  0.1391]\n",
      "MSE loss: 86.521\n",
      "Iteration: 127100\n",
      "Gradient: [ -6.332  -18.8823 -38.2739 -47.2907 315.8998]\n",
      "Weights: [-4.8292  0.8299 -1.3124  0.1506  0.139 ]\n",
      "MSE loss: 86.5039\n",
      "Iteration: 127200\n",
      "Gradient: [-3.2683  3.1096 -8.3706 13.1521 57.9163]\n",
      "Weights: [-4.8218  0.8358 -1.3137  0.1492  0.1392]\n",
      "MSE loss: 86.4017\n",
      "Iteration: 127300\n",
      "Gradient: [  -6.8436  -14.0784  -15.6851  -14.1591 -105.0301]\n",
      "Weights: [-4.8195  0.834  -1.3157  0.1495  0.139 ]\n",
      "MSE loss: 86.4567\n",
      "Iteration: 127400\n",
      "Gradient: [  3.9978  10.0138   0.5206  44.5772 148.6126]\n",
      "Weights: [-4.8235  0.8414 -1.3192  0.1494  0.1393]\n",
      "MSE loss: 86.4063\n",
      "Iteration: 127500\n",
      "Gradient: [  2.2099 -11.9422 -15.4907 -13.1608 151.2389]\n",
      "Weights: [-4.8333  0.8476 -1.3194  0.1496  0.1394]\n",
      "MSE loss: 86.3419\n",
      "Iteration: 127600\n",
      "Gradient: [  1.7402 -16.9527 -16.3951 -46.9995 167.4211]\n",
      "Weights: [-4.8341  0.8556 -1.3222  0.1491  0.1391]\n",
      "MSE loss: 86.4793\n",
      "Iteration: 127700\n",
      "Gradient: [  -1.258   -14.8321  -27.1352   17.8589 -110.7531]\n",
      "Weights: [-4.8276  0.8598 -1.3206  0.1487  0.139 ]\n",
      "MSE loss: 86.4448\n",
      "Iteration: 127800\n",
      "Gradient: [   2.7464   11.8674  -36.8795  138.8122 -202.9681]\n",
      "Weights: [-4.8208  0.8414 -1.3189  0.1482  0.1395]\n",
      "MSE loss: 86.5635\n",
      "Iteration: 127900\n",
      "Gradient: [ 2.0339 16.0424 18.5939 43.506  85.7754]\n",
      "Weights: [-4.8222  0.8415 -1.317   0.1486  0.1396]\n",
      "MSE loss: 86.3929\n",
      "Iteration: 128000\n",
      "Gradient: [  4.3522   7.3136 -17.4145 -45.1168  84.2085]\n",
      "Weights: [-4.8203  0.8433 -1.315   0.1476  0.1395]\n",
      "MSE loss: 86.4588\n",
      "Iteration: 128100\n",
      "Gradient: [ -5.572  -18.1668 -15.4388  36.5696 -63.9161]\n",
      "Weights: [-4.8333  0.845  -1.3163  0.1477  0.1394]\n",
      "MSE loss: 86.5342\n",
      "Iteration: 128200\n",
      "Gradient: [  6.198  -12.4718  -4.9656 -33.7555  41.2268]\n",
      "Weights: [-4.8165  0.8382 -1.314   0.148   0.1394]\n",
      "MSE loss: 86.4659\n",
      "Iteration: 128300\n",
      "Gradient: [  -3.4755   -2.2537   31.0068  148.1475 -131.8582]\n",
      "Weights: [-4.7978  0.8297 -1.3126  0.1475  0.1395]\n",
      "MSE loss: 86.9453\n",
      "Iteration: 128400\n",
      "Gradient: [  1.8018  -3.4929 -16.1749 -77.0062  68.9229]\n",
      "Weights: [-4.8292  0.8435 -1.3135  0.1475  0.1395]\n",
      "MSE loss: 86.3671\n",
      "Iteration: 128500\n",
      "Gradient: [   6.3891   25.1575   -8.0766 -168.7632  -44.6136]\n",
      "Weights: [-4.8151  0.8411 -1.3136  0.1469  0.1395]\n",
      "MSE loss: 86.5322\n",
      "Iteration: 128600\n",
      "Gradient: [ 10.6233  14.4926  15.1062 -20.4568  35.4075]\n",
      "Weights: [-4.8147  0.8384 -1.3122  0.1478  0.1395]\n",
      "MSE loss: 86.8979\n",
      "Iteration: 128700\n",
      "Gradient: [ -2.1514   4.6802 -17.9883  -6.9276 119.0993]\n",
      "Weights: [-4.823   0.8368 -1.3134  0.1474  0.1395]\n",
      "MSE loss: 86.4097\n",
      "Iteration: 128800\n",
      "Gradient: [ 9.3827 15.5349 21.4452 -2.4642 80.722 ]\n",
      "Weights: [-4.8347  0.8427 -1.3128  0.1481  0.1394]\n",
      "MSE loss: 86.4191\n",
      "Iteration: 128900\n",
      "Gradient: [-9.00000e-03  1.34717e+01  2.34533e+01  5.35300e+00  5.02456e+01]\n",
      "Weights: [-4.8413  0.8596 -1.3158  0.1482  0.1391]\n",
      "MSE loss: 86.4966\n",
      "Iteration: 129000\n",
      "Gradient: [ -6.5334  -6.9545 -35.9968  38.8627  23.3776]\n",
      "Weights: [-4.8374  0.8453 -1.3156  0.1491  0.139 ]\n",
      "MSE loss: 86.4192\n",
      "Iteration: 129100\n",
      "Gradient: [ 2.9584 23.6348 15.9599  4.3465 77.4614]\n",
      "Weights: [-4.8376  0.8501 -1.3165  0.1491  0.1389]\n",
      "MSE loss: 86.3467\n",
      "Iteration: 129200\n",
      "Gradient: [ -12.5066    3.236    -0.9815  -62.8401 -118.6571]\n",
      "Weights: [-4.8377  0.8397 -1.314   0.1491  0.1392]\n",
      "MSE loss: 86.489\n",
      "Iteration: 129300\n",
      "Gradient: [ -0.5192  -5.1459   6.7706 133.9942 -89.5899]\n",
      "Weights: [-4.8438  0.8404 -1.3127  0.1491  0.1392]\n",
      "MSE loss: 86.6111\n",
      "Iteration: 129400\n",
      "Gradient: [  0.1609   7.8354 -20.0148 -35.8402 -54.2349]\n",
      "Weights: [-4.8154  0.8395 -1.3167  0.1484  0.1393]\n",
      "MSE loss: 86.4765\n",
      "Iteration: 129500\n",
      "Gradient: [   0.9378   14.4803  -37.7473  -40.891  -394.8075]\n",
      "Weights: [-4.8359  0.8455 -1.3169  0.1488  0.1393]\n",
      "MSE loss: 86.4114\n",
      "Iteration: 129600\n",
      "Gradient: [ -2.129  -16.2753  -8.7038 -56.1329  -6.9017]\n",
      "Weights: [-4.8313  0.8531 -1.3212  0.1491  0.1393]\n",
      "MSE loss: 86.3091\n",
      "Iteration: 129700\n",
      "Gradient: [-2.8816  6.1654  0.2039  6.8581 89.6014]\n",
      "Weights: [-4.8301  0.8565 -1.3196  0.1496  0.139 ]\n",
      "MSE loss: 86.4212\n",
      "Iteration: 129800\n",
      "Gradient: [ -4.3936   3.2168 -14.5255  41.3392 141.5238]\n",
      "Weights: [-4.8331  0.8568 -1.3211  0.1501  0.139 ]\n",
      "MSE loss: 86.3429\n",
      "Iteration: 129900\n",
      "Gradient: [  2.4169 -14.3185 -22.3032 -48.8726 -49.0072]\n",
      "Weights: [-4.8368  0.8613 -1.3222  0.1502  0.139 ]\n",
      "MSE loss: 86.3334\n",
      "Iteration: 130000\n",
      "Gradient: [  -9.6485   -7.8128    5.8334  -64.1192 -326.3586]\n",
      "Weights: [-4.8429  0.8653 -1.3222  0.1496  0.139 ]\n",
      "MSE loss: 86.3273\n",
      "Iteration: 130100\n",
      "Gradient: [ -4.2235   4.4819 -24.8832 -54.8047 159.5016]\n",
      "Weights: [-4.8421  0.86   -1.3219  0.1499  0.1392]\n",
      "MSE loss: 86.3258\n",
      "Iteration: 130200\n",
      "Gradient: [   5.9201    8.3682  -10.9399   -0.2726 -193.943 ]\n",
      "Weights: [-4.8333  0.8575 -1.3225  0.1495  0.1391]\n",
      "MSE loss: 86.3041\n",
      "Iteration: 130300\n",
      "Gradient: [   1.6148   -5.6598   -0.8332  -33.2036 -237.8652]\n",
      "Weights: [-4.8448  0.8654 -1.323   0.15    0.1389]\n",
      "MSE loss: 86.308\n",
      "Iteration: 130400\n",
      "Gradient: [ 11.2586  -6.0825  -6.0305 134.9267 -19.8412]\n",
      "Weights: [-4.8427  0.8718 -1.3275  0.1507  0.1388]\n",
      "MSE loss: 86.2883\n",
      "Iteration: 130500\n",
      "Gradient: [ -6.3351  27.5314  17.3612 -19.1658 423.3023]\n",
      "Weights: [-4.8527  0.873  -1.3254  0.1502  0.139 ]\n",
      "MSE loss: 86.3535\n",
      "Iteration: 130600\n",
      "Gradient: [  -7.1319  -21.9434   22.0852    0.8343 -119.8875]\n",
      "Weights: [-4.8494  0.8679 -1.327   0.1504  0.1389]\n",
      "MSE loss: 86.5748\n",
      "Iteration: 130700\n",
      "Gradient: [  -9.0232   13.3425   -8.7987  -43.6805 -237.5522]\n",
      "Weights: [-4.8389  0.8676 -1.3257  0.1502  0.1393]\n",
      "MSE loss: 86.4316\n",
      "Iteration: 130800\n",
      "Gradient: [   4.6927  -16.751    15.1531  -64.6551 -168.7043]\n",
      "Weights: [-4.8378  0.87   -1.3266  0.1501  0.1392]\n",
      "MSE loss: 86.3976\n",
      "Iteration: 130900\n",
      "Gradient: [ -6.6508  12.1559   7.0603  31.1494 228.5998]\n",
      "Weights: [-4.847   0.8686 -1.3272  0.1494  0.1394]\n",
      "MSE loss: 86.3847\n",
      "Iteration: 131000\n",
      "Gradient: [ -0.2354   0.9161  44.9649 -36.0402  67.7941]\n",
      "Weights: [-4.8525  0.8734 -1.3266  0.1493  0.1393]\n",
      "MSE loss: 86.3748\n",
      "Iteration: 131100\n",
      "Gradient: [  -8.8869   -2.3392   13.2877    9.0541 -234.3003]\n",
      "Weights: [-4.8573  0.8794 -1.3298  0.149   0.1396]\n",
      "MSE loss: 86.4692\n",
      "Iteration: 131200\n",
      "Gradient: [ -9.0775   7.544   -5.4207 -18.4459 179.9765]\n",
      "Weights: [-4.8657  0.8852 -1.3279  0.1489  0.1395]\n",
      "MSE loss: 86.5447\n",
      "Iteration: 131300\n",
      "Gradient: [   8.6261   10.1814   11.2616   42.8196 -157.3575]\n",
      "Weights: [-4.8299  0.8685 -1.328   0.1488  0.1395]\n",
      "MSE loss: 86.3761\n",
      "Iteration: 131400\n",
      "Gradient: [ -4.7169  -6.1725 -18.4876  12.8363   1.8157]\n",
      "Weights: [-4.8237  0.8557 -1.33    0.15    0.1397]\n",
      "MSE loss: 86.463\n",
      "Iteration: 131500\n",
      "Gradient: [  1.8138  -2.5273   7.3518 -17.511  173.8121]\n",
      "Weights: [-4.8344  0.8699 -1.3287  0.1496  0.1396]\n",
      "MSE loss: 86.3412\n",
      "Iteration: 131600\n",
      "Gradient: [ 10.677  -19.729   16.1207  76.2155 255.5814]\n",
      "Weights: [-4.8286  0.8723 -1.3314  0.1507  0.1395]\n",
      "MSE loss: 86.6704\n",
      "Iteration: 131700\n",
      "Gradient: [ -10.9828  -14.4009   52.9081  -70.0333 -157.253 ]\n",
      "Weights: [-4.8335  0.8689 -1.331   0.1505  0.1395]\n",
      "MSE loss: 86.2813\n",
      "Iteration: 131800\n",
      "Gradient: [-2.1593 -4.0415  8.4939 14.1377 21.1825]\n",
      "Weights: [-4.8378  0.876  -1.3349  0.1516  0.1394]\n",
      "MSE loss: 86.2823\n",
      "Iteration: 131900\n",
      "Gradient: [ -1.8304   2.756    3.2511 116.3583  91.3117]\n",
      "Weights: [-4.8461  0.8851 -1.3342  0.1503  0.1393]\n",
      "MSE loss: 86.2852\n",
      "Iteration: 132000\n",
      "Gradient: [-6.077000e-01  2.030160e+01  1.000860e+01 -2.381000e-01  2.455758e+02]\n",
      "Weights: [-4.8497  0.8829 -1.3326  0.1501  0.1394]\n",
      "MSE loss: 86.2834\n",
      "Iteration: 132100\n",
      "Gradient: [  4.1229  -5.1138  -4.1912 -55.3443  -0.3134]\n",
      "Weights: [-4.848   0.8869 -1.3328  0.1504  0.1394]\n",
      "MSE loss: 86.4683\n",
      "Iteration: 132200\n",
      "Gradient: [  11.7554    9.6832  -10.3293   35.9663 -288.4078]\n",
      "Weights: [-4.8293  0.8773 -1.3337  0.15    0.1393]\n",
      "MSE loss: 86.4849\n",
      "Iteration: 132300\n",
      "Gradient: [-10.3483   7.9881  30.5435 -31.8317  -2.4809]\n",
      "Weights: [-4.8443  0.8759 -1.3326  0.1505  0.1397]\n",
      "MSE loss: 86.3106\n",
      "Iteration: 132400\n",
      "Gradient: [-11.7956  22.5356  15.9185  99.5434 180.4591]\n",
      "Weights: [-4.8406  0.873  -1.33    0.1507  0.1393]\n",
      "MSE loss: 86.3131\n",
      "Iteration: 132500\n",
      "Gradient: [ 13.0206  -1.9616   2.0336  66.0167 206.854 ]\n",
      "Weights: [-4.8387  0.8834 -1.3316  0.1505  0.1392]\n",
      "MSE loss: 86.6852\n",
      "Iteration: 132600\n",
      "Gradient: [ -1.5944   5.228    1.8917 -73.7834  55.9252]\n",
      "Weights: [-4.8486  0.8854 -1.3327  0.1501  0.1394]\n",
      "MSE loss: 86.3449\n",
      "Iteration: 132700\n",
      "Gradient: [ 18.5289  19.2347  51.2387  86.3213 -45.8074]\n",
      "Weights: [-4.8317  0.8913 -1.3352  0.1509  0.1393]\n",
      "MSE loss: 87.5338\n",
      "Iteration: 132800\n",
      "Gradient: [ -2.9192   2.3486 -17.2667 -96.0206 -93.1182]\n",
      "Weights: [-4.8357  0.8757 -1.3349  0.1515  0.1392]\n",
      "MSE loss: 86.2807\n",
      "Iteration: 132900\n",
      "Gradient: [   0.9547   -1.2911   -0.5976  -49.5706 -142.9014]\n",
      "Weights: [-4.8525  0.8741 -1.3301  0.1511  0.1391]\n",
      "MSE loss: 86.3667\n",
      "Iteration: 133000\n",
      "Gradient: [ -1.3841  -8.5922  14.4249 -77.0193  84.2754]\n",
      "Weights: [-4.8324  0.8537 -1.3266  0.1516  0.139 ]\n",
      "MSE loss: 86.3497\n",
      "Iteration: 133100\n",
      "Gradient: [   6.106   -12.2178   -5.5418  -66.6764 -289.4695]\n",
      "Weights: [-4.8252  0.8474 -1.3228  0.1512  0.139 ]\n",
      "MSE loss: 86.3079\n",
      "Iteration: 133200\n",
      "Gradient: [ 11.0725  -6.7557  13.1297 131.1543  88.8689]\n",
      "Weights: [-4.8133  0.8457 -1.3203  0.1509  0.139 ]\n",
      "MSE loss: 86.7426\n",
      "Iteration: 133300\n",
      "Gradient: [   5.5217   -0.457     3.7375 -154.8898   -7.3781]\n",
      "Weights: [-4.8497  0.8615 -1.3226  0.1512  0.1388]\n",
      "MSE loss: 86.4082\n",
      "Iteration: 133400\n",
      "Gradient: [  3.9533 -27.8633  21.3422 -47.4867 248.856 ]\n",
      "Weights: [-4.8434  0.8586 -1.325   0.1518  0.1387]\n",
      "MSE loss: 86.3777\n",
      "Iteration: 133500\n",
      "Gradient: [  -4.2215   -8.0449  -15.2667  -81.3992 -196.182 ]\n",
      "Weights: [-4.8356  0.863  -1.3245  0.1514  0.1385]\n",
      "MSE loss: 86.2923\n",
      "Iteration: 133600\n",
      "Gradient: [  3.5688  -8.7229  -5.253   46.0597 332.3962]\n",
      "Weights: [-4.8155  0.8483 -1.3246  0.1529  0.1386]\n",
      "MSE loss: 86.5411\n",
      "Iteration: 133700\n",
      "Gradient: [   2.083    22.9934   10.2881   33.288  -187.0077]\n",
      "Weights: [-4.8174  0.8542 -1.3245  0.1523  0.1388]\n",
      "MSE loss: 86.8876\n",
      "Iteration: 133800\n",
      "Gradient: [ 2.4088 -4.5509 10.9418 31.5543 99.8518]\n",
      "Weights: [-4.838   0.8519 -1.3243  0.1523  0.1388]\n",
      "MSE loss: 86.349\n",
      "Iteration: 133900\n",
      "Gradient: [ -4.0693   8.9732 -41.8758 -43.0245 160.3269]\n",
      "Weights: [-4.8295  0.8478 -1.3245  0.1521  0.1387]\n",
      "MSE loss: 86.4091\n",
      "Iteration: 134000\n",
      "Gradient: [-18.069   -4.9438   5.8341   0.2439  27.2645]\n",
      "Weights: [-4.83    0.8483 -1.3223  0.1518  0.1388]\n",
      "MSE loss: 86.2828\n",
      "Iteration: 134100\n",
      "Gradient: [ 14.6585   3.8489   2.2063  48.9816 -45.2035]\n",
      "Weights: [-4.8222  0.854  -1.3238  0.1519  0.1387]\n",
      "MSE loss: 86.4957\n",
      "Iteration: 134200\n",
      "Gradient: [  2.6937   4.473   -5.0403 -74.388  314.575 ]\n",
      "Weights: [-4.8191  0.8514 -1.3221  0.1516  0.1387]\n",
      "MSE loss: 86.6677\n",
      "Iteration: 134300\n",
      "Gradient: [ -3.7786  18.211  -10.2038 189.1966 -74.8319]\n",
      "Weights: [-4.8307  0.8569 -1.3262  0.1524  0.1388]\n",
      "MSE loss: 86.2763\n",
      "Iteration: 134400\n",
      "Gradient: [   2.048    16.5525   -8.0846 -114.771  -112.5762]\n",
      "Weights: [-4.8403  0.8559 -1.3237  0.1522  0.1387]\n",
      "MSE loss: 86.3036\n",
      "Iteration: 134500\n",
      "Gradient: [  -0.9386   -1.1921   27.1691  -48.3835 -136.1099]\n",
      "Weights: [-4.833   0.8467 -1.3228  0.1521  0.1386]\n",
      "MSE loss: 86.4125\n",
      "Iteration: 134600\n",
      "Gradient: [ -9.6822 -18.2415  -8.6077  17.0342  54.2182]\n",
      "Weights: [-4.8302  0.8498 -1.324   0.1524  0.1386]\n",
      "MSE loss: 86.2836\n",
      "Iteration: 134700\n",
      "Gradient: [ 10.7845 -16.3692  33.1121  79.1936  68.3661]\n",
      "Weights: [-4.8308  0.8476 -1.3221  0.1535  0.1384]\n",
      "MSE loss: 86.3098\n",
      "Iteration: 134800\n",
      "Gradient: [ -0.1232  -1.9442   4.5963 -30.3682 -56.011 ]\n",
      "Weights: [-4.834   0.8463 -1.3235  0.1535  0.1384]\n",
      "MSE loss: 86.336\n",
      "Iteration: 134900\n",
      "Gradient: [  2.1471  -8.5293 -40.5635 -39.8944 -32.7594]\n",
      "Weights: [-4.8226  0.8487 -1.3245  0.1532  0.1386]\n",
      "MSE loss: 86.3522\n",
      "Iteration: 135000\n",
      "Gradient: [   2.2771   12.8946    3.949    32.6759 -267.0828]\n",
      "Weights: [-4.8376  0.8612 -1.3264  0.1532  0.1382]\n",
      "MSE loss: 86.2506\n",
      "Iteration: 135100\n",
      "Gradient: [ -3.1209   7.3158  -6.7704 -66.8466  15.6396]\n",
      "Weights: [-4.8414  0.8559 -1.3248  0.1538  0.1384]\n",
      "MSE loss: 86.3185\n",
      "Iteration: 135200\n",
      "Gradient: [  7.1569 -14.3763 -54.6028  53.7532   1.1755]\n",
      "Weights: [-4.8335  0.8648 -1.3275  0.1531  0.1384]\n",
      "MSE loss: 86.3435\n",
      "Iteration: 135300\n",
      "Gradient: [  -2.4673   -8.5032    8.5114 -133.2649 -695.7347]\n",
      "Weights: [-4.8407  0.864  -1.3277  0.1531  0.1383]\n",
      "MSE loss: 86.2569\n",
      "Iteration: 135400\n",
      "Gradient: [ 7.1604  5.738  29.9189  7.2139 -1.2968]\n",
      "Weights: [-4.8295  0.8567 -1.3259  0.1528  0.1384]\n",
      "MSE loss: 86.2732\n",
      "Iteration: 135500\n",
      "Gradient: [  -9.1647    3.1874    6.7238   14.6145 -209.6494]\n",
      "Weights: [-4.8386  0.8557 -1.3286  0.1526  0.1387]\n",
      "MSE loss: 86.6192\n",
      "Iteration: 135600\n",
      "Gradient: [ -1.5191   9.7053  22.009   58.6394 228.2064]\n",
      "Weights: [-4.819   0.8486 -1.327   0.1527  0.1391]\n",
      "MSE loss: 86.4136\n",
      "Iteration: 135700\n",
      "Gradient: [ -4.1519   6.1696  16.1107  42.2699 146.0537]\n",
      "Weights: [-4.817   0.8431 -1.3291  0.1533  0.1392]\n",
      "MSE loss: 86.3866\n",
      "Iteration: 135800\n",
      "Gradient: [  2.4023  -2.7142  18.8798  44.3498 -32.1592]\n",
      "Weights: [-4.8302  0.8408 -1.3267  0.1531  0.1392]\n",
      "MSE loss: 86.581\n",
      "Iteration: 135900\n",
      "Gradient: [  -2.8349   -1.0907  -29.2206   26.5104 -267.7454]\n",
      "Weights: [-4.8208  0.8376 -1.3248  0.1527  0.1392]\n",
      "MSE loss: 86.407\n",
      "Iteration: 136000\n",
      "Gradient: [ -7.6477 -14.2995  24.0143  31.3325 125.8579]\n",
      "Weights: [-4.836   0.8532 -1.3257  0.1514  0.1392]\n",
      "MSE loss: 86.3514\n",
      "Iteration: 136100\n",
      "Gradient: [-0.1416 18.7818 -1.1292 51.9296  1.6841]\n",
      "Weights: [-4.8333  0.8668 -1.3285  0.1515  0.1392]\n",
      "MSE loss: 86.4966\n",
      "Iteration: 136200\n",
      "Gradient: [ -3.6788  -9.44    25.6935 -65.1131 225.8122]\n",
      "Weights: [-4.829   0.8628 -1.3281  0.1512  0.1392]\n",
      "MSE loss: 86.3671\n",
      "Iteration: 136300\n",
      "Gradient: [ -7.1263 -11.0927  -6.6549 -14.0454 -14.4704]\n",
      "Weights: [-4.833   0.8593 -1.3289  0.1516  0.1391]\n",
      "MSE loss: 86.296\n",
      "Iteration: 136400\n",
      "Gradient: [  6.4788   6.7691 -21.7197  83.9293  14.4052]\n",
      "Weights: [-4.8273  0.8637 -1.3297  0.1517  0.139 ]\n",
      "MSE loss: 86.3339\n",
      "Iteration: 136500\n",
      "Gradient: [   0.8651   -6.904   -67.5078    4.7887 -193.0328]\n",
      "Weights: [-4.8191  0.8591 -1.3316  0.1528  0.1389]\n",
      "MSE loss: 86.4111\n",
      "Iteration: 136600\n",
      "Gradient: [-1.291000e-01 -1.426410e+01 -5.373900e+00  8.766690e+01  1.321629e+02]\n",
      "Weights: [-4.8419  0.8698 -1.3292  0.1516  0.1389]\n",
      "MSE loss: 86.2533\n",
      "Iteration: 136700\n",
      "Gradient: [   4.9101  -11.5345  -31.2862  -77.0243 -275.1902]\n",
      "Weights: [-4.8321  0.8753 -1.3308  0.1511  0.1389]\n",
      "MSE loss: 86.4597\n",
      "Iteration: 136800\n",
      "Gradient: [-11.467    8.8703   7.9606  57.4153  92.9653]\n",
      "Weights: [-4.845   0.8831 -1.3298  0.1499  0.139 ]\n",
      "MSE loss: 86.372\n",
      "Iteration: 136900\n",
      "Gradient: [ -3.8888   5.75    27.2331 -46.9427 -39.0236]\n",
      "Weights: [-4.8518  0.8766 -1.3293  0.1505  0.139 ]\n",
      "MSE loss: 86.3662\n",
      "Iteration: 137000\n",
      "Gradient: [ -10.6372    4.1219    6.3864    9.2423 -334.11  ]\n",
      "Weights: [-4.8594  0.8705 -1.3282  0.151   0.1389]\n",
      "MSE loss: 86.8462\n",
      "Iteration: 137100\n",
      "Gradient: [  3.562  -23.627  -37.3167 -34.0839 101.5571]\n",
      "Weights: [-4.853   0.876  -1.3286  0.1514  0.1388]\n",
      "MSE loss: 86.3328\n",
      "Iteration: 137200\n",
      "Gradient: [   8.0078    6.0483   -6.3943   27.8912 -228.2982]\n",
      "Weights: [-4.8318  0.8645 -1.3274  0.1509  0.139 ]\n",
      "MSE loss: 86.3029\n",
      "Iteration: 137300\n",
      "Gradient: [ 13.6432  -2.9616  12.1462  20.9732 331.0689]\n",
      "Weights: [-4.8191  0.8585 -1.3287  0.1513  0.1391]\n",
      "MSE loss: 86.4528\n",
      "Iteration: 137400\n",
      "Gradient: [ -5.7085   9.4912  28.224   40.5452 256.7515]\n",
      "Weights: [-4.8432  0.8653 -1.3263  0.151   0.1392]\n",
      "MSE loss: 86.3111\n",
      "Iteration: 137500\n",
      "Gradient: [ -2.3272   6.2521  -1.5287  -1.1249 344.6962]\n",
      "Weights: [-4.853   0.8735 -1.3283  0.1509  0.1391]\n",
      "MSE loss: 86.3612\n",
      "Iteration: 137600\n",
      "Gradient: [ -8.688  -37.789   20.7686 -12.7693 105.293 ]\n",
      "Weights: [-4.8441  0.8659 -1.3274  0.1509  0.1389]\n",
      "MSE loss: 86.4064\n",
      "Iteration: 137700\n",
      "Gradient: [  3.159    5.4032 -14.0986 -85.427  -27.6626]\n",
      "Weights: [-4.8418  0.8715 -1.329   0.1512  0.1389]\n",
      "MSE loss: 86.2614\n",
      "Iteration: 137800\n",
      "Gradient: [  -2.7131  -19.2375   10.0478  -98.4506 -153.0412]\n",
      "Weights: [-4.846   0.8725 -1.3288  0.1507  0.139 ]\n",
      "MSE loss: 86.2951\n",
      "Iteration: 137900\n",
      "Gradient: [  7.225   13.0742  34.754  102.1685 -14.8363]\n",
      "Weights: [-4.8378  0.8866 -1.333   0.1513  0.139 ]\n",
      "MSE loss: 86.8962\n",
      "Iteration: 138000\n",
      "Gradient: [  3.247   -5.9031 -14.8766 -45.7009  75.7267]\n",
      "Weights: [-4.8531  0.8827 -1.3319  0.151   0.1387]\n",
      "MSE loss: 86.449\n",
      "Iteration: 138100\n",
      "Gradient: [ -2.2308  -6.6855 -28.8602 -21.2351 138.4763]\n",
      "Weights: [-4.8522  0.8867 -1.331   0.1513  0.1385]\n",
      "MSE loss: 86.3605\n",
      "Iteration: 138200\n",
      "Gradient: [   2.7405    2.8184   -1.9168  -61.5372 -214.5081]\n",
      "Weights: [-4.8425  0.8807 -1.3326  0.1514  0.1387]\n",
      "MSE loss: 86.3432\n",
      "Iteration: 138300\n",
      "Gradient: [ -7.9804  -5.1199 -61.1073  35.7204 -21.3686]\n",
      "Weights: [-4.8302  0.8709 -1.3336  0.1524  0.1388]\n",
      "MSE loss: 86.3419\n",
      "Iteration: 138400\n",
      "Gradient: [  -5.2258   -9.1946  -44.3272    4.6526 -381.2144]\n",
      "Weights: [-4.8419  0.8708 -1.3329  0.1523  0.1389]\n",
      "MSE loss: 86.3194\n",
      "Iteration: 138500\n",
      "Gradient: [  -6.9143   11.8299  -25.3456  -19.874  -305.5398]\n",
      "Weights: [-4.8311  0.8651 -1.3319  0.1524  0.1389]\n",
      "MSE loss: 86.2687\n",
      "Iteration: 138600\n",
      "Gradient: [ 10.4203   8.7319 -17.4638 -75.632   79.0304]\n",
      "Weights: [-4.8255  0.8648 -1.3325  0.1524  0.1391]\n",
      "MSE loss: 86.3395\n",
      "Iteration: 138700\n",
      "Gradient: [ -6.5738 -20.6563 -31.0951  16.0435 -73.2381]\n",
      "Weights: [-4.8351  0.8587 -1.3297  0.152   0.1391]\n",
      "MSE loss: 86.3477\n",
      "Iteration: 138800\n",
      "Gradient: [  2.5957 -17.0854  28.6125 -70.6362 171.339 ]\n",
      "Weights: [-4.8427  0.8645 -1.3274  0.1505  0.1391]\n",
      "MSE loss: 86.4577\n",
      "Iteration: 138900\n",
      "Gradient: [  9.72    -8.5083  26.0441 130.1642  15.9966]\n",
      "Weights: [-4.8272  0.8537 -1.3231  0.1513  0.139 ]\n",
      "MSE loss: 86.4496\n",
      "Iteration: 139000\n",
      "Gradient: [ -1.1224   4.8202  20.5114 -18.6947 206.9913]\n",
      "Weights: [-4.8354  0.8521 -1.325   0.1517  0.1391]\n",
      "MSE loss: 86.3263\n",
      "Iteration: 139100\n",
      "Gradient: [ -11.4502   10.0252    1.4513   -2.6008 -197.0555]\n",
      "Weights: [-4.8404  0.8686 -1.3301  0.1509  0.1391]\n",
      "MSE loss: 86.3462\n",
      "Iteration: 139200\n",
      "Gradient: [   8.7281    4.2195   15.7977   16.9406 -154.3658]\n",
      "Weights: [-4.8514  0.8836 -1.3341  0.1508  0.1393]\n",
      "MSE loss: 86.284\n",
      "Iteration: 139300\n",
      "Gradient: [-11.6048   5.3816  13.4265  20.553  -64.7231]\n",
      "Weights: [-4.8509  0.8829 -1.3328  0.1505  0.1394]\n",
      "MSE loss: 86.2913\n",
      "Iteration: 139400\n",
      "Gradient: [  6.0703  -1.8334  22.376   60.8442 215.8753]\n",
      "Weights: [-4.8516  0.8774 -1.3352  0.1511  0.1396]\n",
      "MSE loss: 86.3954\n",
      "Iteration: 139500\n",
      "Gradient: [ -2.08     5.4254  30.5392 -50.1216 174.609 ]\n",
      "Weights: [-4.8303  0.8703 -1.3331  0.151   0.1395]\n",
      "MSE loss: 86.3343\n",
      "Iteration: 139600\n",
      "Gradient: [  -5.0411    2.252     6.595     7.0049 -303.6841]\n",
      "Weights: [-4.8163  0.8624 -1.3331  0.1511  0.1395]\n",
      "MSE loss: 86.5254\n",
      "Iteration: 139700\n",
      "Gradient: [   7.4424    2.6599  -19.2997 -111.1232 -286.1973]\n",
      "Weights: [-4.8366  0.8707 -1.3363  0.1517  0.1397]\n",
      "MSE loss: 86.2666\n",
      "Iteration: 139800\n",
      "Gradient: [  -5.172     3.1254  -16.2148 -138.6992  197.6393]\n",
      "Weights: [-4.8447  0.8683 -1.3367  0.1527  0.1395]\n",
      "MSE loss: 86.4969\n",
      "Iteration: 139900\n",
      "Gradient: [ -6.239   17.0387 -29.514   57.2865  99.8695]\n",
      "Weights: [-4.8304  0.8721 -1.3384  0.1528  0.1394]\n",
      "MSE loss: 86.2653\n",
      "Iteration: 140000\n",
      "Gradient: [  -4.9311  -15.5162   11.8902 -168.3011 -103.2435]\n",
      "Weights: [-4.8537  0.884  -1.3387  0.1526  0.1394]\n",
      "MSE loss: 86.3146\n",
      "Iteration: 140100\n",
      "Gradient: [  -5.5696    5.3549  -13.3716  -80.8579 -151.6721]\n",
      "Weights: [-4.8461  0.8914 -1.3407  0.1525  0.1393]\n",
      "MSE loss: 86.283\n",
      "Iteration: 140200\n",
      "Gradient: [ -1.5738  -7.6159  32.3919 140.757  130.1628]\n",
      "Weights: [-4.8635  0.8941 -1.3402  0.1528  0.1391]\n",
      "MSE loss: 86.3906\n",
      "Iteration: 140300\n",
      "Gradient: [ 9.2317  4.4093 -3.0383 69.0537 55.6212]\n",
      "Weights: [-4.8514  0.8944 -1.341   0.1535  0.1388]\n",
      "MSE loss: 86.2644\n",
      "Iteration: 140400\n",
      "Gradient: [ 1.976000e-01 -1.223040e+01 -3.733090e+01 -2.392870e+01 -3.376297e+02]\n",
      "Weights: [-4.8529  0.891  -1.3418  0.1542  0.1387]\n",
      "MSE loss: 86.2379\n",
      "Iteration: 140500\n",
      "Gradient: [ 13.6147   1.7093  12.2094  48.6043 186.9864]\n",
      "Weights: [-4.8315  0.8871 -1.3419  0.1545  0.1387]\n",
      "MSE loss: 86.6035\n",
      "Iteration: 140600\n",
      "Gradient: [  -6.1696  -12.4202  -15.3465  144.6983 -116.5836]\n",
      "Weights: [-4.8376  0.8826 -1.3413  0.1539  0.1389]\n",
      "MSE loss: 86.2328\n",
      "Iteration: 140700\n",
      "Gradient: [  4.2744  -5.8313 -23.6763 112.3669  90.079 ]\n",
      "Weights: [-4.8311  0.8759 -1.3417  0.1539  0.1393]\n",
      "MSE loss: 86.2518\n",
      "Iteration: 140800\n",
      "Gradient: [   2.6479  -16.8028  -26.1565  -45.7286 -272.1763]\n",
      "Weights: [-4.8397  0.8809 -1.3433  0.1544  0.1391]\n",
      "MSE loss: 86.2072\n",
      "Iteration: 140900\n",
      "Gradient: [ -2.0209  -2.8199 -29.1636   4.3759 -30.6899]\n",
      "Weights: [-4.8581  0.8966 -1.3443  0.1539  0.1391]\n",
      "MSE loss: 86.2689\n",
      "Iteration: 141000\n",
      "Gradient: [ -5.4513  25.2847  16.9401  -2.1887 -90.2697]\n",
      "Weights: [-4.8447  0.8844 -1.3422  0.154   0.1393]\n",
      "MSE loss: 86.2371\n",
      "Iteration: 141100\n",
      "Gradient: [  5.2352  -0.3205 -27.559   37.092  -35.3025]\n",
      "Weights: [-4.8336  0.8764 -1.3441  0.1554  0.1391]\n",
      "MSE loss: 86.2055\n",
      "Iteration: 141200\n",
      "Gradient: [  6.275   10.6675  -3.9544  35.9682 171.0939]\n",
      "Weights: [-4.8234  0.8817 -1.3457  0.1558  0.139 ]\n",
      "MSE loss: 86.7328\n",
      "Iteration: 141300\n",
      "Gradient: [ 3.809   9.4583 -9.002  38.7282 34.6864]\n",
      "Weights: [-4.8285  0.8778 -1.3431  0.1552  0.1389]\n",
      "MSE loss: 86.3169\n",
      "Iteration: 141400\n",
      "Gradient: [  3.9858 -17.477    7.392  -33.0769 113.1734]\n",
      "Weights: [-4.853   0.8884 -1.3445  0.156   0.1387]\n",
      "MSE loss: 86.2244\n",
      "Iteration: 141500\n",
      "Gradient: [-5.3154  3.1611 36.8092 30.2438  2.965 ]\n",
      "Weights: [-4.8559  0.8825 -1.3399  0.1558  0.1385]\n",
      "MSE loss: 86.3149\n",
      "Iteration: 141600\n",
      "Gradient: [-3.26000e-02 -9.36330e+00 -1.01387e+01  3.56552e+01  1.50792e+01]\n",
      "Weights: [-4.8539  0.8944 -1.3422  0.155   0.1383]\n",
      "MSE loss: 86.2737\n",
      "Iteration: 141700\n",
      "Gradient: [ -1.0773  -7.8576  12.9068 -75.2784  62.1538]\n",
      "Weights: [-4.8645  0.8904 -1.3389  0.1553  0.1385]\n",
      "MSE loss: 86.4658\n",
      "Iteration: 141800\n",
      "Gradient: [-9.73000e-02 -1.50887e+01 -3.80670e+00 -1.01547e+01  2.12971e+02]\n",
      "Weights: [-4.8395  0.8787 -1.3407  0.1554  0.1385]\n",
      "MSE loss: 86.2142\n",
      "Iteration: 141900\n",
      "Gradient: [   8.7895   -1.905     0.7669   11.2333 -191.608 ]\n",
      "Weights: [-4.8399  0.8862 -1.3429  0.1551  0.1386]\n",
      "MSE loss: 86.2272\n",
      "Iteration: 142000\n",
      "Gradient: [  6.9476  -2.418   20.1308 -78.3867  -5.9452]\n",
      "Weights: [-4.8455  0.8843 -1.3409  0.1543  0.1387]\n",
      "MSE loss: 86.2425\n",
      "Iteration: 142100\n",
      "Gradient: [  1.9868  12.0125 -20.8137 -23.2855   6.4848]\n",
      "Weights: [-4.8637  0.8855 -1.341   0.1552  0.1386]\n",
      "MSE loss: 86.6061\n",
      "Iteration: 142200\n",
      "Gradient: [ -4.1069   2.3115  -5.3771  68.3798 254.9392]\n",
      "Weights: [-4.8361  0.8853 -1.3419  0.1555  0.1386]\n",
      "MSE loss: 86.453\n",
      "Iteration: 142300\n",
      "Gradient: [ -1.637   -1.7055  13.9646 -46.5208  -9.2171]\n",
      "Weights: [-4.8435  0.8846 -1.3445  0.156   0.1387]\n",
      "MSE loss: 86.1812\n",
      "Iteration: 142400\n",
      "Gradient: [ -6.0229  19.8099  11.2292  70.0507 163.0916]\n",
      "Weights: [-4.8359  0.8801 -1.3433  0.1552  0.139 ]\n",
      "MSE loss: 86.2116\n",
      "Iteration: 142500\n",
      "Gradient: [-9.6848 16.3719  6.1259 58.2635 59.8773]\n",
      "Weights: [-4.8324  0.8788 -1.3439  0.1566  0.1387]\n",
      "MSE loss: 86.3576\n",
      "Iteration: 142600\n",
      "Gradient: [ -0.2308 -15.57    40.4236  18.5653   6.3716]\n",
      "Weights: [-4.8107  0.8706 -1.3442  0.1568  0.1388]\n",
      "MSE loss: 86.992\n",
      "Iteration: 142700\n",
      "Gradient: [  1.1486   4.7767 -25.1385 116.7469 253.068 ]\n",
      "Weights: [-4.8258  0.8719 -1.3423  0.1562  0.1387]\n",
      "MSE loss: 86.2908\n",
      "Iteration: 142800\n",
      "Gradient: [-11.6256 -13.8455  -5.5152 -18.8735  -0.8612]\n",
      "Weights: [-4.8451  0.8732 -1.3411  0.1573  0.1385]\n",
      "MSE loss: 86.2694\n",
      "Iteration: 142900\n",
      "Gradient: [-11.0501 -11.6004 -14.6195 -10.6183 -49.4721]\n",
      "Weights: [-4.8331  0.8606 -1.343   0.1574  0.1386]\n",
      "MSE loss: 86.6117\n",
      "Iteration: 143000\n",
      "Gradient: [ 11.3655   4.2414 -25.6218 -44.6046 207.6976]\n",
      "Weights: [-4.8208  0.8635 -1.3439  0.158   0.1385]\n",
      "MSE loss: 86.2795\n",
      "Iteration: 143100\n",
      "Gradient: [  2.4959 -16.8617   3.4915 -36.2657 285.2602]\n",
      "Weights: [-4.8277  0.8679 -1.3437  0.1582  0.1384]\n",
      "MSE loss: 86.2201\n",
      "Iteration: 143200\n",
      "Gradient: [ -18.122    -2.6854    3.4074  -42.0968 -157.3551]\n",
      "Weights: [-4.8451  0.8757 -1.3456  0.1581  0.1384]\n",
      "MSE loss: 86.3038\n",
      "Iteration: 143300\n",
      "Gradient: [   4.6147   -6.6207    1.6999 -166.1359 -162.7705]\n",
      "Weights: [-4.8222  0.8653 -1.3457  0.1586  0.1383]\n",
      "MSE loss: 86.2789\n",
      "Iteration: 143400\n",
      "Gradient: [-20.7921  -6.5009 -20.6391  37.8056 131.6671]\n",
      "Weights: [-4.8482  0.8825 -1.3488  0.1589  0.1384]\n",
      "MSE loss: 86.221\n",
      "Iteration: 143500\n",
      "Gradient: [ -7.5419 -13.7619 -62.8928 -11.5042 -73.6618]\n",
      "Weights: [-4.8338  0.8797 -1.3516  0.1593  0.1384]\n",
      "MSE loss: 86.1576\n",
      "Iteration: 143600\n",
      "Gradient: [-10.8163   8.8253   6.7693 -29.3751 130.723 ]\n",
      "Weights: [-4.8415  0.8771 -1.3506  0.1596  0.1384]\n",
      "MSE loss: 86.2773\n",
      "Iteration: 143700\n",
      "Gradient: [  2.6923  -6.5001 -35.327   33.0082 -84.8724]\n",
      "Weights: [-4.8316  0.8766 -1.3498  0.1593  0.1383]\n",
      "MSE loss: 86.1803\n",
      "Iteration: 143800\n",
      "Gradient: [   3.7429   -8.6905  -21.2835 -116.9455 -316.4326]\n",
      "Weights: [-4.8329  0.8772 -1.3501  0.1594  0.1383]\n",
      "MSE loss: 86.1539\n",
      "Iteration: 143900\n",
      "Gradient: [  -2.2673    1.5837  -20.5242  -98.4075 -208.6556]\n",
      "Weights: [-4.841   0.8782 -1.3493  0.1593  0.1384]\n",
      "MSE loss: 86.1788\n",
      "Iteration: 144000\n",
      "Gradient: [-15.0008   8.6206 -83.2227 -51.3434  66.1872]\n",
      "Weights: [-4.8489  0.8841 -1.3493  0.1588  0.1382]\n",
      "MSE loss: 86.2673\n",
      "Iteration: 144100\n",
      "Gradient: [ -3.5711 -22.5457   8.7665  90.9335 225.9382]\n",
      "Weights: [-4.8467  0.8867 -1.3494  0.1585  0.1382]\n",
      "MSE loss: 86.1941\n",
      "Iteration: 144200\n",
      "Gradient: [ 13.7731  15.1178  -0.222  -42.3892  98.6768]\n",
      "Weights: [-4.832   0.8865 -1.3516  0.1591  0.1384]\n",
      "MSE loss: 86.3185\n",
      "Iteration: 144300\n",
      "Gradient: [  5.8936  -1.5058  22.3037  42.327  110.2369]\n",
      "Weights: [-4.8271  0.8793 -1.3524  0.1598  0.1385]\n",
      "MSE loss: 86.2964\n",
      "Iteration: 144400\n",
      "Gradient: [ -1.9489 -13.2572  10.2177  46.9165 -58.0876]\n",
      "Weights: [-4.8463  0.8932 -1.3577  0.1598  0.1384]\n",
      "MSE loss: 86.2869\n",
      "Iteration: 144500\n",
      "Gradient: [-11.1779   5.055  -10.1486  -3.7304 141.6903]\n",
      "Weights: [-4.8431  0.8981 -1.355   0.1596  0.1382]\n",
      "MSE loss: 86.1938\n",
      "Iteration: 144600\n",
      "Gradient: [ -1.6781   0.4473 -12.7248 -23.469  -26.7355]\n",
      "Weights: [-4.8454  0.8993 -1.3567  0.1591  0.1385]\n",
      "MSE loss: 86.1209\n",
      "Iteration: 144700\n",
      "Gradient: [  6.29    11.7212  26.6385  88.03   203.3201]\n",
      "Weights: [-4.8561  0.9126 -1.3594  0.1598  0.1383]\n",
      "MSE loss: 86.1931\n",
      "Iteration: 144800\n",
      "Gradient: [ -3.1437   8.0893  25.8281 -17.2038 135.8959]\n",
      "Weights: [-4.8549  0.9182 -1.3607  0.1588  0.1385]\n",
      "MSE loss: 86.2523\n",
      "Iteration: 144900\n",
      "Gradient: [  0.8226  -8.5777   8.2293  20.7629 -13.7118]\n",
      "Weights: [-4.8548  0.9157 -1.3602  0.1588  0.1385]\n",
      "MSE loss: 86.1962\n",
      "Iteration: 145000\n",
      "Gradient: [ -8.4792  12.7027 -18.6735 -78.9039  71.7778]\n",
      "Weights: [-4.8583  0.9072 -1.3617  0.1596  0.1386]\n",
      "MSE loss: 86.2835\n",
      "Iteration: 145100\n",
      "Gradient: [  -0.8185   20.7911  -33.3933  -14.276  -128.1153]\n",
      "Weights: [-4.8369  0.895  -1.3614  0.1601  0.1388]\n",
      "MSE loss: 86.1436\n",
      "Iteration: 145200\n",
      "Gradient: [  -8.1047    3.5157  -27.6912  -78.9723 -144.1007]\n",
      "Weights: [-4.8537  0.899  -1.3584  0.16    0.1385]\n",
      "MSE loss: 86.1789\n",
      "Iteration: 145300\n",
      "Gradient: [  2.2648  -2.8583   7.1378 -23.1466 -74.7514]\n",
      "Weights: [-4.8513  0.9099 -1.3613  0.16    0.1384]\n",
      "MSE loss: 86.1307\n",
      "Iteration: 145400\n",
      "Gradient: [  5.4573   2.3583   5.2052  73.7414 166.259 ]\n",
      "Weights: [-4.8367  0.9021 -1.3589  0.1594  0.1385]\n",
      "MSE loss: 86.3182\n",
      "Iteration: 145500\n",
      "Gradient: [  -1.5479   -3.9087   -5.9209 -186.4068   84.4806]\n",
      "Weights: [-4.8603  0.909  -1.3578  0.1593  0.1384]\n",
      "MSE loss: 86.1896\n",
      "Iteration: 145600\n",
      "Gradient: [ -4.6757 -19.4244  39.4215 -78.9666 181.0598]\n",
      "Weights: [-4.8773  0.9146 -1.3574  0.1591  0.1384]\n",
      "MSE loss: 86.541\n",
      "Iteration: 145700\n",
      "Gradient: [   9.0857    3.3196  -28.9173 -152.837    87.4457]\n",
      "Weights: [-4.8428  0.9133 -1.3601  0.1589  0.1385]\n",
      "MSE loss: 86.5464\n",
      "Iteration: 145800\n",
      "Gradient: [ -3.5758   3.4796   1.8799 -41.0328 -59.8114]\n",
      "Weights: [-4.8539  0.9143 -1.3615  0.159   0.1385]\n",
      "MSE loss: 86.1441\n",
      "Iteration: 145900\n",
      "Gradient: [  6.4941   8.5643  30.8925  82.6788 -41.6085]\n",
      "Weights: [-4.869   0.9193 -1.3632  0.1586  0.1387]\n",
      "MSE loss: 86.3777\n",
      "Iteration: 146000\n",
      "Gradient: [  6.2901   1.9445   0.7651 -66.462  237.3103]\n",
      "Weights: [-4.8459  0.9199 -1.3675  0.1596  0.1387]\n",
      "MSE loss: 86.2426\n",
      "Iteration: 146100\n",
      "Gradient: [ -3.67    -8.0751  -7.747  -33.1957 -20.8624]\n",
      "Weights: [-4.8482  0.9178 -1.3652  0.1592  0.1385]\n",
      "MSE loss: 86.2349\n",
      "Iteration: 146200\n",
      "Gradient: [  5.8397  -5.6151 -10.558  -27.6642  10.4487]\n",
      "Weights: [-4.8569  0.914  -1.3637  0.1602  0.1385]\n",
      "MSE loss: 86.1209\n",
      "Iteration: 146300\n",
      "Gradient: [  3.7126  -4.2122 -26.8876  25.9941 -24.3088]\n",
      "Weights: [-4.8434  0.9073 -1.3634  0.1607  0.1383]\n",
      "MSE loss: 86.1347\n",
      "Iteration: 146400\n",
      "Gradient: [   3.0895   -2.748    28.5076   67.6987 -201.7132]\n",
      "Weights: [-4.8515  0.9078 -1.3644  0.1617  0.1384]\n",
      "MSE loss: 86.0952\n",
      "Iteration: 146500\n",
      "Gradient: [ -3.3656   7.1545  31.015  -63.2977 173.2653]\n",
      "Weights: [-4.8655  0.9059 -1.364   0.1615  0.1383]\n",
      "MSE loss: 86.7138\n",
      "Iteration: 146600\n",
      "Gradient: [   1.9181  -18.3746  -43.1181  -31.9528 -151.1803]\n",
      "Weights: [-4.8359  0.8957 -1.3656  0.1621  0.1383]\n",
      "MSE loss: 86.2387\n",
      "Iteration: 146700\n",
      "Gradient: [  -5.9068   -3.9724  -38.6774  -44.101  -202.5944]\n",
      "Weights: [-4.8496  0.8992 -1.3645  0.1627  0.1383]\n",
      "MSE loss: 86.1328\n",
      "Iteration: 146800\n",
      "Gradient: [-10.5714   5.8178 -25.4608 -71.7885 -15.2658]\n",
      "Weights: [-4.8547  0.9029 -1.3636  0.162   0.138 ]\n",
      "MSE loss: 86.381\n",
      "Iteration: 146900\n",
      "Gradient: [  -5.8066  -19.0037   -5.4491 -186.7667 -227.1592]\n",
      "Weights: [-4.8599  0.9099 -1.3636  0.1622  0.138 ]\n",
      "MSE loss: 86.1409\n",
      "Iteration: 147000\n",
      "Gradient: [  0.7834  -2.2993 -21.2468 -14.9283  70.1251]\n",
      "Weights: [-4.8586  0.902  -1.3646  0.1627  0.1381]\n",
      "MSE loss: 86.4052\n",
      "Iteration: 147100\n",
      "Gradient: [ 16.2006  -8.1471 -19.8768  39.0044 -74.5993]\n",
      "Weights: [-4.85    0.9057 -1.3627  0.1633  0.1378]\n",
      "MSE loss: 86.1935\n",
      "Iteration: 147200\n",
      "Gradient: [  6.1822  -9.7688 -12.3988 -27.2328 113.8326]\n",
      "Weights: [-4.866   0.9082 -1.3623  0.163   0.1375]\n",
      "MSE loss: 86.4015\n",
      "Iteration: 147300\n",
      "Gradient: [-4.2469 -3.2961 -6.1898 62.4381 16.0561]\n",
      "Weights: [-4.8683  0.9121 -1.3607  0.1623  0.1374]\n",
      "MSE loss: 86.3137\n",
      "Iteration: 147400\n",
      "Gradient: [ -7.8944 -11.7687 -24.0678  27.8666  43.4016]\n",
      "Weights: [-4.873   0.9096 -1.361   0.1623  0.1378]\n",
      "MSE loss: 86.5193\n",
      "Iteration: 147500\n",
      "Gradient: [  3.8541   8.5859 -11.6656 -36.6777  -9.3634]\n",
      "Weights: [-4.8536  0.9097 -1.3639  0.1623  0.1378]\n",
      "MSE loss: 86.0884\n",
      "Iteration: 147600\n",
      "Gradient: [ -3.0576  21.3885   0.8303   3.9171 -54.2211]\n",
      "Weights: [-4.8579  0.9166 -1.3682  0.1632  0.1378]\n",
      "MSE loss: 86.0922\n",
      "Iteration: 147700\n",
      "Gradient: [   1.9945    5.2836    5.8411  -75.3132 -118.9752]\n",
      "Weights: [-4.8635  0.9221 -1.3674  0.1633  0.1377]\n",
      "MSE loss: 86.1664\n",
      "Iteration: 147800\n",
      "Gradient: [ -1.3491  -2.6508  34.4986  31.5838 114.099 ]\n",
      "Weights: [-4.8598  0.9232 -1.3675  0.1629  0.1377]\n",
      "MSE loss: 86.1587\n",
      "Iteration: 147900\n",
      "Gradient: [ -11.4958   -1.4115   27.084   167.2097 -114.5428]\n",
      "Weights: [-4.8538  0.9266 -1.3734  0.1636  0.1378]\n",
      "MSE loss: 86.1254\n",
      "Iteration: 148000\n",
      "Gradient: [ 5.5788 -5.7765 28.5984 84.5535  1.5992]\n",
      "Weights: [-4.8463  0.9286 -1.3771  0.1644  0.138 ]\n",
      "MSE loss: 86.3512\n",
      "Iteration: 148100\n",
      "Gradient: [ -9.8845 -10.7976 -34.5376 -30.4681 -16.2833]\n",
      "Weights: [-4.8654  0.9296 -1.3794  0.1651  0.1379]\n",
      "MSE loss: 86.2632\n",
      "Iteration: 148200\n",
      "Gradient: [  7.8566   6.1204  20.8937  99.2452 213.4724]\n",
      "Weights: [-4.8626  0.9368 -1.3807  0.1665  0.1378]\n",
      "MSE loss: 86.3323\n",
      "Iteration: 148300\n",
      "Gradient: [  -6.0632    3.7121   -3.0734 -144.2982 -404.3798]\n",
      "Weights: [-4.8723  0.9353 -1.3841  0.1669  0.1377]\n",
      "MSE loss: 86.3713\n",
      "Iteration: 148400\n",
      "Gradient: [ -0.3992  19.9785  -1.0507  37.9307 278.2515]\n",
      "Weights: [-4.8695  0.9426 -1.3839  0.1667  0.1377]\n",
      "MSE loss: 86.0823\n",
      "Iteration: 148500\n",
      "Gradient: [ 8.8996 -0.3272 26.6332 43.0098  6.9499]\n",
      "Weights: [-4.8515  0.9344 -1.383   0.1666  0.1376]\n",
      "MSE loss: 86.1097\n",
      "Iteration: 148600\n",
      "Gradient: [  6.2414   8.3743  22.0482  79.583  -65.2133]\n",
      "Weights: [-4.8483  0.9279 -1.3831  0.1671  0.1377]\n",
      "MSE loss: 86.0456\n",
      "Iteration: 148700\n",
      "Gradient: [ -6.0444  -7.2541 -27.1503  69.0875 -90.0036]\n",
      "Weights: [-4.8455  0.9267 -1.3837  0.1674  0.1378]\n",
      "MSE loss: 86.0903\n",
      "Iteration: 148800\n",
      "Gradient: [-19.5164  -5.6946 -11.0087 -94.4788 133.0137]\n",
      "Weights: [-4.8687  0.9207 -1.3832  0.1681  0.138 ]\n",
      "MSE loss: 86.5426\n",
      "Iteration: 148900\n",
      "Gradient: [  9.1664  -7.9708   1.9852 -14.3088 -31.0034]\n",
      "Weights: [-4.8474  0.9226 -1.3816  0.1678  0.1377]\n",
      "MSE loss: 86.0663\n",
      "Iteration: 149000\n",
      "Gradient: [   0.4199  -11.038   -15.8053   -7.7919 -120.4718]\n",
      "Weights: [-4.8573  0.9232 -1.3797  0.1666  0.1378]\n",
      "MSE loss: 86.0468\n",
      "Iteration: 149100\n",
      "Gradient: [ -3.1555  -7.4594  -6.6732 -51.2214  92.0209]\n",
      "Weights: [-4.8575  0.9207 -1.3806  0.1669  0.1378]\n",
      "MSE loss: 86.1927\n",
      "Iteration: 149200\n",
      "Gradient: [  1.0411  12.0724  34.3362 169.314  -67.6075]\n",
      "Weights: [-4.8433  0.9198 -1.3804  0.167   0.1378]\n",
      "MSE loss: 86.0728\n",
      "Iteration: 149300\n",
      "Gradient: [ -5.238    1.7557  25.5061 -32.0651  42.2031]\n",
      "Weights: [-4.8413  0.9088 -1.3778  0.1667  0.1381]\n",
      "MSE loss: 86.0588\n",
      "Iteration: 149400\n",
      "Gradient: [ -1.8767  -0.4848  39.4731  74.2214 151.1692]\n",
      "Weights: [-4.8475  0.9116 -1.3759  0.1666  0.1381]\n",
      "MSE loss: 86.1371\n",
      "Iteration: 149500\n",
      "Gradient: [ -6.8124 -14.6023 -13.4151 -43.2266  35.1492]\n",
      "Weights: [-4.8362  0.902  -1.3766  0.1667  0.1378]\n",
      "MSE loss: 86.1816\n",
      "Iteration: 149600\n",
      "Gradient: [  0.4003  -0.4031  -0.8701  62.449  -97.4605]\n",
      "Weights: [-4.8405  0.9075 -1.3761  0.1674  0.1377]\n",
      "MSE loss: 86.0681\n",
      "Iteration: 149700\n",
      "Gradient: [  -0.9531   20.6882   34.9072  -62.7627 -132.7492]\n",
      "Weights: [-4.8381  0.9058 -1.3748  0.1676  0.1374]\n",
      "MSE loss: 86.038\n",
      "Iteration: 149800\n",
      "Gradient: [   6.9949    4.8889  -33.7717 -115.4613  105.2788]\n",
      "Weights: [-4.8283  0.8934 -1.3745  0.1689  0.1375]\n",
      "MSE loss: 86.1682\n",
      "Iteration: 149900\n",
      "Gradient: [ -1.2765   5.213   50.5292 121.946   35.5966]\n",
      "Weights: [-4.8507  0.9012 -1.374   0.1691  0.1372]\n",
      "MSE loss: 86.1026\n",
      "Iteration: 150000\n",
      "Gradient: [  1.0176 -20.9748   0.5783  22.3182 -39.1467]\n",
      "Weights: [-4.8482  0.9049 -1.3749  0.1688  0.137 ]\n",
      "MSE loss: 86.0661\n",
      "Iteration: 150100\n",
      "Gradient: [   4.9247   -4.2645   18.2648  -60.7091 -493.3155]\n",
      "Weights: [-4.8512  0.9121 -1.3749  0.1686  0.1369]\n",
      "MSE loss: 85.9825\n",
      "Iteration: 150200\n",
      "Gradient: [  -3.8212    1.3773  -16.801    81.6512 -104.1878]\n",
      "Weights: [-4.8529  0.9142 -1.3732  0.1674  0.1369]\n",
      "MSE loss: 86.0154\n",
      "Iteration: 150300\n",
      "Gradient: [ 2.360000e-02  2.751850e+01 -1.450200e+01 -1.276590e+01 -2.699542e+02]\n",
      "Weights: [-4.8555  0.9199 -1.3709  0.167   0.1367]\n",
      "MSE loss: 86.114\n",
      "Iteration: 150400\n",
      "Gradient: [  7.1982  -8.575  -24.4474   1.6823  55.3629]\n",
      "Weights: [-4.8472  0.9062 -1.3672  0.1669  0.1368]\n",
      "MSE loss: 86.0503\n",
      "Iteration: 150500\n",
      "Gradient: [   1.1783   16.1138  -21.6744 -113.056  -223.0249]\n",
      "Weights: [-4.8395  0.8992 -1.3662  0.1671  0.1367]\n",
      "MSE loss: 86.0496\n",
      "Iteration: 150600\n",
      "Gradient: [  8.1348  14.0969   9.6205 -22.7145  89.5522]\n",
      "Weights: [-4.8377  0.8945 -1.3646  0.1669  0.1369]\n",
      "MSE loss: 86.0747\n",
      "Iteration: 150700\n",
      "Gradient: [-5.704  -8.1141 22.0716 10.8118 45.8813]\n",
      "Weights: [-4.8219  0.8895 -1.3658  0.1675  0.1367]\n",
      "MSE loss: 86.3218\n",
      "Iteration: 150800\n",
      "Gradient: [ -9.1535  -2.8651  -7.9875 -62.4157 120.2089]\n",
      "Weights: [-4.8506  0.8956 -1.3639  0.1672  0.1366]\n",
      "MSE loss: 86.0975\n",
      "Iteration: 150900\n",
      "Gradient: [  -6.3005    3.1005  -24.9484   63.8826 -160.8928]\n",
      "Weights: [-4.8476  0.8838 -1.3632  0.1678  0.1368]\n",
      "MSE loss: 86.3226\n",
      "Iteration: 151000\n",
      "Gradient: [ -7.4076  -6.5894  16.8545  37.4841 -33.6049]\n",
      "Weights: [-4.8429  0.8859 -1.3635  0.1675  0.1368]\n",
      "MSE loss: 86.1217\n",
      "Iteration: 151100\n",
      "Gradient: [ -3.5474  -7.3658  -5.3705 122.5776  47.5501]\n",
      "Weights: [-4.8554  0.8953 -1.3623  0.1672  0.1366]\n",
      "MSE loss: 86.1298\n",
      "Iteration: 151200\n",
      "Gradient: [   8.3132   -8.8779  -11.3842   -1.3143 -100.8053]\n",
      "Weights: [-4.8447  0.8993 -1.3638  0.1673  0.1365]\n",
      "MSE loss: 86.0408\n",
      "Iteration: 151300\n",
      "Gradient: [   3.9711  -18.972   -17.154   -70.3513 -142.5888]\n",
      "Weights: [-4.8559  0.895  -1.3615  0.1676  0.1363]\n",
      "MSE loss: 86.1293\n",
      "Iteration: 151400\n",
      "Gradient: [ -2.6495  10.396    3.8895  84.2581 -55.2049]\n",
      "Weights: [-4.8274  0.887  -1.3623  0.1674  0.1365]\n",
      "MSE loss: 86.1773\n",
      "Iteration: 151500\n",
      "Gradient: [ 4.142  17.2189 24.4668 83.0219 31.1907]\n",
      "Weights: [-4.8353  0.8927 -1.3647  0.1679  0.1367]\n",
      "MSE loss: 86.1801\n",
      "Iteration: 151600\n",
      "Gradient: [   3.2612   17.024     7.7746   29.8022 -109.1779]\n",
      "Weights: [-4.826   0.8906 -1.3692  0.1689  0.1367]\n",
      "MSE loss: 86.151\n",
      "Iteration: 151700\n",
      "Gradient: [  -4.301    -1.1935    9.1945   14.5641 -108.3083]\n",
      "Weights: [-4.8446  0.8922 -1.3697  0.1692  0.1368]\n",
      "MSE loss: 86.087\n",
      "Iteration: 151800\n",
      "Gradient: [ -2.9138 -13.8147  46.3212  16.6454 175.5802]\n",
      "Weights: [-4.8375  0.8974 -1.3717  0.1686  0.1369]\n",
      "MSE loss: 86.0072\n",
      "Iteration: 151900\n",
      "Gradient: [ -6.1893 -17.3987  -3.0531  29.1448 -97.1785]\n",
      "Weights: [-4.8499  0.9008 -1.3735  0.1692  0.1371]\n",
      "MSE loss: 86.0771\n",
      "Iteration: 152000\n",
      "Gradient: [  14.5227   -9.0885   21.8697  107.6982 -236.6378]\n",
      "Weights: [-4.8274  0.8985 -1.3713  0.1689  0.137 ]\n",
      "MSE loss: 86.6213\n",
      "Iteration: 152100\n",
      "Gradient: [ -0.6661   4.3618 -16.2351  26.2034   1.5348]\n",
      "Weights: [-4.8443  0.8965 -1.3715  0.1688  0.1369]\n",
      "MSE loss: 86.0966\n",
      "Iteration: 152200\n",
      "Gradient: [  -4.5366   -7.38    -33.5583 -112.6607 -219.0615]\n",
      "Weights: [-4.8358  0.8935 -1.3699  0.1692  0.1366]\n",
      "MSE loss: 86.0073\n",
      "Iteration: 152300\n",
      "Gradient: [  5.176   -5.9096  14.034  -15.7358  -3.5348]\n",
      "Weights: [-4.8427  0.9083 -1.3699  0.1689  0.1361]\n",
      "MSE loss: 86.1036\n",
      "Iteration: 152400\n",
      "Gradient: [  -2.7963    8.7656    8.8538   90.0333 -169.4835]\n",
      "Weights: [-4.837   0.8965 -1.3707  0.1701  0.1361]\n",
      "MSE loss: 86.0242\n",
      "Iteration: 152500\n",
      "Gradient: [  9.9329  -6.6783  20.6054  22.718  -50.2721]\n",
      "Weights: [-4.8334  0.8975 -1.3688  0.1697  0.1365]\n",
      "MSE loss: 86.401\n",
      "Iteration: 152600\n",
      "Gradient: [-6.8097  8.1661  2.926  45.235  20.3258]\n",
      "Weights: [-4.811   0.8815 -1.3688  0.1697  0.1366]\n",
      "MSE loss: 86.5171\n",
      "Iteration: 152700\n",
      "Gradient: [  -0.9086   -4.8839   18.8369   49.8176 -165.4054]\n",
      "Weights: [-4.8444  0.8941 -1.3716  0.1701  0.1366]\n",
      "MSE loss: 86.0478\n",
      "Iteration: 152800\n",
      "Gradient: [ -8.2699 -16.1815 -15.2755 -68.3463 256.0768]\n",
      "Weights: [-4.8526  0.8962 -1.3698  0.17    0.1366]\n",
      "MSE loss: 86.1165\n",
      "Iteration: 152900\n",
      "Gradient: [  -1.3806   -0.5228   25.4621  -86.1067 -126.6497]\n",
      "Weights: [-4.8518  0.8975 -1.3722  0.1705  0.1367]\n",
      "MSE loss: 86.1077\n",
      "Iteration: 153000\n",
      "Gradient: [ -7.9906  -0.2167  40.2277  94.7171 129.2976]\n",
      "Weights: [-4.8475  0.8928 -1.3702  0.1708  0.1364]\n",
      "MSE loss: 86.0545\n",
      "Iteration: 153100\n",
      "Gradient: [ -2.0382   1.0379 -53.6993 -52.1011 303.1175]\n",
      "Weights: [-4.8396  0.8861 -1.3692  0.1701  0.1365]\n",
      "MSE loss: 86.1655\n",
      "Iteration: 153200\n",
      "Gradient: [  -3.5528  -16.5478   22.7022  102.0807 -210.8874]\n",
      "Weights: [-4.8344  0.8825 -1.3699  0.1706  0.1364]\n",
      "MSE loss: 86.2419\n",
      "Iteration: 153300\n",
      "Gradient: [ -7.7503  14.7431 -38.3113  94.4979 214.055 ]\n",
      "Weights: [-4.8374  0.8896 -1.3718  0.1706  0.1365]\n",
      "MSE loss: 86.0704\n",
      "Iteration: 153400\n",
      "Gradient: [  9.5703   4.8431  28.3197   3.4589 463.3851]\n",
      "Weights: [-4.8313  0.8894 -1.3721  0.1708  0.1366]\n",
      "MSE loss: 86.0356\n",
      "Iteration: 153500\n",
      "Gradient: [ -8.9255  -2.1014 -10.4592  -0.2955 219.435 ]\n",
      "Weights: [-4.8481  0.8967 -1.3727  0.1706  0.1365]\n",
      "MSE loss: 86.0826\n",
      "Iteration: 153600\n",
      "Gradient: [ 4.3068 -0.8142 18.5881  6.2169  2.6409]\n",
      "Weights: [-4.8397  0.8903 -1.3713  0.1707  0.1362]\n",
      "MSE loss: 86.1883\n",
      "Iteration: 153700\n",
      "Gradient: [  4.1899   8.0951 -17.0012  45.0453 442.1915]\n",
      "Weights: [-4.8309  0.8947 -1.3725  0.1716  0.1364]\n",
      "MSE loss: 86.2774\n",
      "Iteration: 153800\n",
      "Gradient: [  7.8292 -22.6998  13.0666 -49.9088  -9.0769]\n",
      "Weights: [-4.8286  0.8984 -1.3786  0.1732  0.136 ]\n",
      "MSE loss: 86.0527\n",
      "Iteration: 153900\n",
      "Gradient: [ -4.0434   0.8253  44.4782  34.5097 -14.0513]\n",
      "Weights: [-4.842   0.9075 -1.3781  0.1734  0.1358]\n",
      "MSE loss: 85.9671\n",
      "Iteration: 154000\n",
      "Gradient: [  8.9521  22.9081   9.6896  13.3629 -48.3817]\n",
      "Weights: [-4.8425  0.9045 -1.3746  0.1732  0.1358]\n",
      "MSE loss: 86.1801\n",
      "Iteration: 154100\n",
      "Gradient: [-14.6379   9.4948  16.7259 -22.5679 167.8604]\n",
      "Weights: [-4.8417  0.8873 -1.3702  0.1733  0.1357]\n",
      "MSE loss: 85.9758\n",
      "Iteration: 154200\n",
      "Gradient: [  3.6473 -12.4837  24.2106 -62.7831  97.1339]\n",
      "Weights: [-4.8185  0.8895 -1.3726  0.173   0.1358]\n",
      "MSE loss: 86.5216\n",
      "Iteration: 154300\n",
      "Gradient: [   7.8812   14.0767   16.2143   31.1963 -156.817 ]\n",
      "Weights: [-4.8324  0.9042 -1.3782  0.1741  0.1358]\n",
      "MSE loss: 86.4026\n",
      "Iteration: 154400\n",
      "Gradient: [ -21.7729    6.9517  -25.5095  -91.9932 -136.5446]\n",
      "Weights: [-4.8619  0.9048 -1.3816  0.1738  0.1359]\n",
      "MSE loss: 87.0595\n",
      "Iteration: 154500\n",
      "Gradient: [ 1.364  -9.8913 11.8818 -6.0049 52.694 ]\n",
      "Weights: [-4.8265  0.8984 -1.3818  0.1738  0.1361]\n",
      "MSE loss: 86.0739\n",
      "Iteration: 154600\n",
      "Gradient: [ -6.3739   3.5406 -55.9957 -97.4843  43.3332]\n",
      "Weights: [-4.8436  0.8899 -1.3783  0.174   0.1361]\n",
      "MSE loss: 86.3371\n",
      "Iteration: 154700\n",
      "Gradient: [ 0.1368 11.0907 -7.3621 30.5045 81.7427]\n",
      "Weights: [-4.8355  0.8995 -1.3811  0.1737  0.1361]\n",
      "MSE loss: 85.9728\n",
      "Iteration: 154800\n",
      "Gradient: [  10.3681  -15.4918  -20.5776 -110.9479   27.3491]\n",
      "Weights: [-4.856   0.9028 -1.3773  0.1732  0.136 ]\n",
      "MSE loss: 86.1497\n",
      "Iteration: 154900\n",
      "Gradient: [  -0.5049   -5.2696   -6.1661  -77.8442 -264.109 ]\n",
      "Weights: [-4.8294  0.9008 -1.378   0.173   0.1362]\n",
      "MSE loss: 86.2572\n",
      "Iteration: 155000\n",
      "Gradient: [-2.718100e+00 -1.266580e+01  1.873610e+01  1.352000e-01 -2.190402e+02]\n",
      "Weights: [-4.8353  0.8934 -1.3769  0.173   0.1362]\n",
      "MSE loss: 85.9695\n",
      "Iteration: 155100\n",
      "Gradient: [ -0.8706   2.3648  10.6734  11.2071 536.5787]\n",
      "Weights: [-4.8542  0.9012 -1.3779  0.1736  0.1362]\n",
      "MSE loss: 86.1156\n",
      "Iteration: 155200\n",
      "Gradient: [ -0.427  -20.0479   4.9078  67.6199  61.9317]\n",
      "Weights: [-4.8705  0.9047 -1.3777  0.1733  0.136 ]\n",
      "MSE loss: 86.9611\n",
      "Iteration: 155300\n",
      "Gradient: [  -6.8538   -4.258    29.4947  -88.5609 -253.8924]\n",
      "Weights: [-4.8369  0.9002 -1.3782  0.1737  0.1359]\n",
      "MSE loss: 85.9407\n",
      "Iteration: 155400\n",
      "Gradient: [   5.1027   -6.264     4.339   -67.7773 -215.8129]\n",
      "Weights: [-4.8468  0.9082 -1.3784  0.1739  0.1359]\n",
      "MSE loss: 86.0699\n",
      "Iteration: 155500\n",
      "Gradient: [-12.1062 -10.396  -28.4578   2.2301 -26.708 ]\n",
      "Weights: [-4.852   0.9059 -1.3752  0.1737  0.1356]\n",
      "MSE loss: 86.0024\n",
      "Iteration: 155600\n",
      "Gradient: [  -0.6404   15.5653    3.5366  -39.5498 -243.329 ]\n",
      "Weights: [-4.8504  0.9014 -1.3748  0.1735  0.1357]\n",
      "MSE loss: 85.9658\n",
      "Iteration: 155700\n",
      "Gradient: [ -8.6865  -1.4704  -7.8787  23.595  -73.5143]\n",
      "Weights: [-4.8398  0.8983 -1.378   0.1732  0.1357]\n",
      "MSE loss: 86.2215\n",
      "Iteration: 155800\n",
      "Gradient: [-5.3862  2.5795 18.5134 73.5423 86.1356]\n",
      "Weights: [-4.8437  0.8934 -1.375   0.174   0.136 ]\n",
      "MSE loss: 86.0201\n",
      "Iteration: 155900\n",
      "Gradient: [ -13.5106   16.006    40.1488   25.5911 -239.8691]\n",
      "Weights: [-4.8693  0.9054 -1.3767  0.1733  0.1357]\n",
      "MSE loss: 86.9052\n",
      "Iteration: 156000\n",
      "Gradient: [  13.8374  -10.5754   11.7162 -105.4588   -9.7802]\n",
      "Weights: [-4.8171  0.8896 -1.3748  0.174   0.1356]\n",
      "MSE loss: 86.4592\n",
      "Iteration: 156100\n",
      "Gradient: [  1.6901  -0.1937 -52.8909   7.3753  28.5458]\n",
      "Weights: [-4.8401  0.8834 -1.373   0.1741  0.1357]\n",
      "MSE loss: 86.1699\n",
      "Iteration: 156200\n",
      "Gradient: [   3.4127    7.8322   -4.5383   23.1447 -279.1066]\n",
      "Weights: [-4.8235  0.8888 -1.374   0.1738  0.1356]\n",
      "MSE loss: 86.1042\n",
      "Iteration: 156300\n",
      "Gradient: [-3.72080e+00 -1.98258e+01 -4.21871e+01 -5.73858e+01 -4.07000e-02]\n",
      "Weights: [-4.8464  0.8964 -1.371   0.1732  0.1354]\n",
      "MSE loss: 85.9366\n",
      "Iteration: 156400\n",
      "Gradient: [ 10.1237  15.69    -3.8108  80.9779 150.9403]\n",
      "Weights: [-4.8293  0.8916 -1.3708  0.173   0.1355]\n",
      "MSE loss: 86.0989\n",
      "Iteration: 156500\n",
      "Gradient: [   9.2052   -8.3555   14.059  -108.8927  -14.2747]\n",
      "Weights: [-4.8353  0.8854 -1.3703  0.1738  0.1356]\n",
      "MSE loss: 85.9762\n",
      "Iteration: 156600\n",
      "Gradient: [  9.4469   3.1533  19.8855 -53.5352  47.4899]\n",
      "Weights: [-4.8389  0.8889 -1.37    0.1733  0.1357]\n",
      "MSE loss: 85.989\n",
      "Iteration: 156700\n",
      "Gradient: [ -8.6958  -6.7688   7.193   23.7576 164.2613]\n",
      "Weights: [-4.844   0.8941 -1.3697  0.1731  0.1355]\n",
      "MSE loss: 85.9769\n",
      "Iteration: 156800\n",
      "Gradient: [  5.9025   0.6217   0.9474  59.5788 457.7521]\n",
      "Weights: [-4.8281  0.8883 -1.3684  0.1727  0.1352]\n",
      "MSE loss: 86.0938\n",
      "Iteration: 156900\n",
      "Gradient: [  0.6029 -10.1642 -32.6366  24.4305 150.1263]\n",
      "Weights: [-4.852   0.889  -1.3679  0.1735  0.1353]\n",
      "MSE loss: 86.0891\n",
      "Iteration: 157000\n",
      "Gradient: [-1.3916 -1.1487  4.0265 80.2483 36.6609]\n",
      "Weights: [-4.8439  0.8806 -1.3647  0.1732  0.1352]\n",
      "MSE loss: 86.0688\n",
      "Iteration: 157100\n",
      "Gradient: [  -0.4652   17.3426   22.3075   30.9289 -132.7819]\n",
      "Weights: [-4.8531  0.8901 -1.3668  0.1735  0.135 ]\n",
      "MSE loss: 86.096\n",
      "Iteration: 157200\n",
      "Gradient: [  3.5226  11.5295 -11.4454   5.3022 190.5777]\n",
      "Weights: [-4.848   0.8945 -1.3669  0.1736  0.135 ]\n",
      "MSE loss: 86.0312\n",
      "Iteration: 157300\n",
      "Gradient: [   7.5058   11.3079   10.8877 -107.7147  -83.6987]\n",
      "Weights: [-4.83    0.8859 -1.3704  0.1744  0.135 ]\n",
      "MSE loss: 85.9824\n",
      "Iteration: 157400\n",
      "Gradient: [ -2.7044   2.309    1.4786   5.8712 -33.997 ]\n",
      "Weights: [-4.8283  0.8888 -1.3734  0.1757  0.135 ]\n",
      "MSE loss: 86.0259\n",
      "Iteration: 157500\n",
      "Gradient: [  6.7      7.4633 -28.6518  92.2605  42.1825]\n",
      "Weights: [-4.8352  0.8903 -1.3754  0.1765  0.135 ]\n",
      "MSE loss: 85.9221\n",
      "Iteration: 157600\n",
      "Gradient: [  1.9028 -15.8432  -4.7993 -66.9196 140.2927]\n",
      "Weights: [-4.8283  0.8895 -1.3767  0.1761  0.135 ]\n",
      "MSE loss: 86.0098\n",
      "Iteration: 157700\n",
      "Gradient: [ -9.3336  20.7483  29.1951 -51.2436 120.9622]\n",
      "Weights: [-4.8466  0.8938 -1.3735  0.1762  0.1349]\n",
      "MSE loss: 85.9512\n",
      "Iteration: 157800\n",
      "Gradient: [  4.5252  -2.1079  -1.3075  19.714  195.2566]\n",
      "Weights: [-4.8443  0.9009 -1.3764  0.1758  0.1348]\n",
      "MSE loss: 85.9099\n",
      "Iteration: 157900\n",
      "Gradient: [  4.8681  12.2857  12.2097 -38.6698 138.9073]\n",
      "Weights: [-4.83    0.8946 -1.3752  0.1756  0.1349]\n",
      "MSE loss: 86.043\n",
      "Iteration: 158000\n",
      "Gradient: [  6.1305   6.7333   5.9384  -8.6559 -21.0249]\n",
      "Weights: [-4.8356  0.8943 -1.3749  0.1759  0.1351]\n",
      "MSE loss: 86.0712\n",
      "Iteration: 158100\n",
      "Gradient: [ 5.7794 -2.406   7.1026 35.6348 74.3909]\n",
      "Weights: [-4.8363  0.8967 -1.3748  0.1753  0.1349]\n",
      "MSE loss: 85.9512\n",
      "Iteration: 158200\n",
      "Gradient: [  14.6268  -21.473     7.483    74.2825 -224.9658]\n",
      "Weights: [-4.8271  0.8967 -1.3738  0.1754  0.1349]\n",
      "MSE loss: 86.4493\n",
      "Iteration: 158300\n",
      "Gradient: [  8.5237  -1.6243 -10.6651 -27.6848  -6.651 ]\n",
      "Weights: [-4.8297  0.9029 -1.375   0.1748  0.1351]\n",
      "MSE loss: 86.5733\n",
      "Iteration: 158400\n",
      "Gradient: [  2.1575  12.5143   1.0461  33.1762 316.3354]\n",
      "Weights: [-4.8473  0.9066 -1.3755  0.1744  0.1352]\n",
      "MSE loss: 85.9554\n",
      "Iteration: 158500\n",
      "Gradient: [-10.1855 -10.9542  -1.7903  49.205  236.5458]\n",
      "Weights: [-4.8591  0.9099 -1.3757  0.1735  0.1354]\n",
      "MSE loss: 86.0176\n",
      "Iteration: 158600\n",
      "Gradient: [ -3.424  -13.7726  16.4653  14.8916  98.8664]\n",
      "Weights: [-4.8321  0.9    -1.3792  0.174   0.1356]\n",
      "MSE loss: 86.0042\n",
      "Iteration: 158700\n",
      "Gradient: [  -4.809     5.7631  -17.2697  -18.0279 -123.8229]\n",
      "Weights: [-4.8574  0.9094 -1.3813  0.1749  0.1357]\n",
      "MSE loss: 86.049\n",
      "Iteration: 158800\n",
      "Gradient: [  -1.4029   -5.6796    3.1107   32.882  -110.2196]\n",
      "Weights: [-4.8504  0.9146 -1.3828  0.1756  0.1353]\n",
      "MSE loss: 85.8885\n",
      "Iteration: 158900\n",
      "Gradient: [ -3.8339   1.6257  -6.516   14.2274 -30.0995]\n",
      "Weights: [-4.8604  0.9265 -1.3866  0.1757  0.1355]\n",
      "MSE loss: 85.9365\n",
      "Iteration: 159000\n",
      "Gradient: [  3.3002   7.3916  34.2286 -10.5865 110.545 ]\n",
      "Weights: [-4.8538  0.9215 -1.3859  0.1756  0.1355]\n",
      "MSE loss: 85.9023\n",
      "Iteration: 159100\n",
      "Gradient: [ 13.2994  14.8994 -16.345  -28.1698 264.2998]\n",
      "Weights: [-4.8448  0.9233 -1.3866  0.1759  0.1355]\n",
      "MSE loss: 86.278\n",
      "Iteration: 159200\n",
      "Gradient: [  2.6298 -22.2198  -2.785   64.064   25.3833]\n",
      "Weights: [-4.8445  0.9203 -1.3844  0.1746  0.1356]\n",
      "MSE loss: 86.0351\n",
      "Iteration: 159300\n",
      "Gradient: [ -3.0408   4.543  -12.1304 -51.1996 135.5219]\n",
      "Weights: [-4.8681  0.917  -1.3834  0.1745  0.1355]\n",
      "MSE loss: 86.651\n",
      "Iteration: 159400\n",
      "Gradient: [  9.3997  -8.8096   8.2815 -46.347   48.0722]\n",
      "Weights: [-4.8503  0.9027 -1.3764  0.1745  0.1355]\n",
      "MSE loss: 85.9332\n",
      "Iteration: 159500\n",
      "Gradient: [-14.8975  -8.5492 -62.0204  15.3028 154.9446]\n",
      "Weights: [-4.862   0.8973 -1.3753  0.175   0.1354]\n",
      "MSE loss: 86.4603\n",
      "Iteration: 159600\n",
      "Gradient: [  15.2453  -13.608     1.6156  -96.1676 -211.7957]\n",
      "Weights: [-4.8285  0.8942 -1.3757  0.1746  0.1354]\n",
      "MSE loss: 86.0742\n",
      "Iteration: 159700\n",
      "Gradient: [  -3.1672   -4.9188   -9.4853 -109.4291   79.2789]\n",
      "Weights: [-4.8445  0.8935 -1.3744  0.1748  0.1352]\n",
      "MSE loss: 86.0057\n",
      "Iteration: 159800\n",
      "Gradient: [  0.5376  11.99     2.4844 -13.5838  50.6207]\n",
      "Weights: [-4.8449  0.8912 -1.3747  0.1757  0.1353]\n",
      "MSE loss: 85.9745\n",
      "Iteration: 159900\n",
      "Gradient: [  7.3725 -11.5239  -3.8886  15.809   18.1013]\n",
      "Weights: [-4.8336  0.8907 -1.3741  0.1754  0.1353]\n",
      "MSE loss: 86.0242\n",
      "Iteration: 160000\n",
      "Gradient: [ 7.5284 12.1736  2.8911 26.726  50.3906]\n",
      "Weights: [-4.8312  0.8916 -1.3724  0.1745  0.1354]\n",
      "MSE loss: 86.1734\n",
      "Iteration: 160100\n",
      "Gradient: [  4.4944  13.7168   7.5298 -11.7507 200.2472]\n",
      "Weights: [-4.8306  0.8859 -1.3747  0.1751  0.1354]\n",
      "MSE loss: 85.9522\n",
      "Iteration: 160200\n",
      "Gradient: [  4.5726  -0.7862  -4.5545 175.6965  88.9788]\n",
      "Weights: [-4.8372  0.8969 -1.3752  0.1751  0.1353]\n",
      "MSE loss: 85.9899\n",
      "Iteration: 160300\n",
      "Gradient: [ 4.5209 -4.1093 10.996  34.9942 36.938 ]\n",
      "Weights: [-4.85    0.9032 -1.3757  0.1756  0.1351]\n",
      "MSE loss: 85.9714\n",
      "Iteration: 160400\n",
      "Gradient: [  0.3879   1.8557  33.8236  58.35   -80.0218]\n",
      "Weights: [-4.8561  0.9006 -1.3782  0.1762  0.135 ]\n",
      "MSE loss: 86.1712\n",
      "Iteration: 160500\n",
      "Gradient: [ -8.1226 -20.6284 -10.7932  28.715   83.7424]\n",
      "Weights: [-4.8578  0.9019 -1.38    0.1763  0.1351]\n",
      "MSE loss: 86.3425\n",
      "Iteration: 160600\n",
      "Gradient: [  -5.8566   -6.883   -38.6301  -69.4192 -146.522 ]\n",
      "Weights: [-4.8596  0.908  -1.3791  0.1761  0.135 ]\n",
      "MSE loss: 86.0547\n",
      "Iteration: 160700\n",
      "Gradient: [ 3.204  -1.0865 29.4249 43.7404 62.3809]\n",
      "Weights: [-4.861   0.9056 -1.3764  0.1765  0.1348]\n",
      "MSE loss: 86.0508\n",
      "Iteration: 160800\n",
      "Gradient: [   2.5168    5.1892    9.4925  103.4973 -227.6719]\n",
      "Weights: [-4.8342  0.8911 -1.3734  0.1768  0.1345]\n",
      "MSE loss: 85.9587\n",
      "Iteration: 160900\n",
      "Gradient: [-4.9746  6.3023 28.5864 62.4093 58.1196]\n",
      "Weights: [-4.8338  0.8864 -1.3763  0.1777  0.1347]\n",
      "MSE loss: 85.9032\n",
      "Iteration: 161000\n",
      "Gradient: [ 7.6221 -7.6803 15.0971 -5.134  49.2789]\n",
      "Weights: [-4.8044  0.8816 -1.378   0.1789  0.1347]\n",
      "MSE loss: 87.1386\n",
      "Iteration: 161100\n",
      "Gradient: [ -3.8146  -2.2216   4.5113  23.0515 -55.4596]\n",
      "Weights: [-4.8485  0.8961 -1.3787  0.1791  0.1346]\n",
      "MSE loss: 85.9808\n",
      "Iteration: 161200\n",
      "Gradient: [  -5.8059    4.6112  -19.2888   29.8791 -136.6973]\n",
      "Weights: [-4.8366  0.8806 -1.3756  0.1788  0.1345]\n",
      "MSE loss: 85.9801\n",
      "Iteration: 161300\n",
      "Gradient: [ -2.4141  18.9588  40.7227  82.8576 280.7466]\n",
      "Weights: [-4.8544  0.8899 -1.375   0.1779  0.1346]\n",
      "MSE loss: 86.2053\n",
      "Iteration: 161400\n",
      "Gradient: [ 2.3181  3.8049 48.9479 38.34   52.0031]\n",
      "Weights: [-4.8257  0.8913 -1.3785  0.1786  0.1345]\n",
      "MSE loss: 86.0943\n",
      "Iteration: 161500\n",
      "Gradient: [  -3.6884   -0.5262   22.001   -80.0824 -134.8083]\n",
      "Weights: [-4.8273  0.8903 -1.3799  0.1793  0.1345]\n",
      "MSE loss: 86.001\n",
      "Iteration: 161600\n",
      "Gradient: [-1.5814  4.5729 50.5736 14.7326 97.0565]\n",
      "Weights: [-4.8484  0.9085 -1.3814  0.1795  0.1343]\n",
      "MSE loss: 86.1028\n",
      "Iteration: 161700\n",
      "Gradient: [  9.1988  -2.4083  -4.5283   7.2781 330.1988]\n",
      "Weights: [-4.842   0.9069 -1.3833  0.1794  0.1343]\n",
      "MSE loss: 85.9097\n",
      "Iteration: 161800\n",
      "Gradient: [  -1.2387    1.8739  -22.3287  -76.1653 -482.1711]\n",
      "Weights: [-4.8609  0.9148 -1.3848  0.1788  0.1343]\n",
      "MSE loss: 86.0374\n",
      "Iteration: 161900\n",
      "Gradient: [  3.1935  15.5966  -6.2134   8.9723 215.4005]\n",
      "Weights: [-4.8618  0.9221 -1.385   0.179   0.1342]\n",
      "MSE loss: 85.9414\n",
      "Iteration: 162000\n",
      "Gradient: [  6.8823   1.3631  -8.0112  53.3723 140.0558]\n",
      "Weights: [-4.8512  0.9162 -1.3848  0.1793  0.1343]\n",
      "MSE loss: 85.9458\n",
      "Iteration: 162100\n",
      "Gradient: [ -1.8469  -4.1765 -50.4672   0.6724 112.1142]\n",
      "Weights: [-4.8589  0.9117 -1.384   0.1789  0.1342]\n",
      "MSE loss: 86.095\n",
      "Iteration: 162200\n",
      "Gradient: [-16.1693   9.8546  -4.6181   9.6235 258.6681]\n",
      "Weights: [-4.8698  0.9068 -1.3845  0.1803  0.1344]\n",
      "MSE loss: 86.5308\n",
      "Iteration: 162300\n",
      "Gradient: [  8.2338   9.8256 -40.632  159.2008 141.2994]\n",
      "Weights: [-4.8271  0.9024 -1.3864  0.1808  0.1343]\n",
      "MSE loss: 86.186\n",
      "Iteration: 162400\n",
      "Gradient: [ -4.4591  -6.7355 -14.3569 -23.1033 124.1635]\n",
      "Weights: [-4.8482  0.9011 -1.3858  0.1809  0.1343]\n",
      "MSE loss: 85.8946\n",
      "Iteration: 162500\n",
      "Gradient: [  -5.6724  -14.7531  -28.4766 -104.7399 -357.5626]\n",
      "Weights: [-4.8583  0.9063 -1.3871  0.1809  0.1342]\n",
      "MSE loss: 86.1985\n",
      "Iteration: 162600\n",
      "Gradient: [ -4.8183  -0.7829  10.6428  13.69   182.3216]\n",
      "Weights: [-4.8362  0.9062 -1.3892  0.1813  0.1342]\n",
      "MSE loss: 85.8578\n",
      "Iteration: 162700\n",
      "Gradient: [  3.3275 -15.0719 -16.5403  53.9195 -66.2111]\n",
      "Weights: [-4.8447  0.9021 -1.3896  0.1824  0.1342]\n",
      "MSE loss: 85.859\n",
      "Iteration: 162800\n",
      "Gradient: [  3.8842  -3.622   30.0984 119.2677  48.381 ]\n",
      "Weights: [-4.8342  0.9074 -1.3903  0.1816  0.1344]\n",
      "MSE loss: 85.9742\n",
      "Iteration: 162900\n",
      "Gradient: [  2.0673 -24.6593  26.126   75.9175 -41.335 ]\n",
      "Weights: [-4.8376  0.8965 -1.3896  0.1812  0.1345]\n",
      "MSE loss: 86.0798\n",
      "Iteration: 163000\n",
      "Gradient: [ -7.949   13.0086  30.6531  50.6728 240.3273]\n",
      "Weights: [-4.8335  0.9087 -1.3919  0.1817  0.1343]\n",
      "MSE loss: 85.9074\n",
      "Iteration: 163100\n",
      "Gradient: [  -1.299     8.7631   12.9054   39.5276 -188.5026]\n",
      "Weights: [-4.8488  0.9089 -1.3901  0.182   0.1343]\n",
      "MSE loss: 85.8543\n",
      "Iteration: 163200\n",
      "Gradient: [ -2.1902  -3.3729  -6.5416 -97.5549 232.254 ]\n",
      "Weights: [-4.8512  0.905  -1.3903  0.1817  0.1345]\n",
      "MSE loss: 85.974\n",
      "Iteration: 163300\n",
      "Gradient: [  0.9386  10.3038 -18.6035   0.6355 -23.921 ]\n",
      "Weights: [-4.8551  0.9109 -1.392   0.1815  0.1345]\n",
      "MSE loss: 85.9824\n",
      "Iteration: 163400\n",
      "Gradient: [ -6.0352   6.78     3.3126 -85.6894 136.5621]\n",
      "Weights: [-4.8554  0.9146 -1.3942  0.1824  0.1343]\n",
      "MSE loss: 85.9157\n",
      "Iteration: 163500\n",
      "Gradient: [ -0.9677  -6.0352  18.4072 123.6671  25.2787]\n",
      "Weights: [-4.875   0.9279 -1.3953  0.1826  0.1342]\n",
      "MSE loss: 86.2146\n",
      "Iteration: 163600\n",
      "Gradient: [   1.8267    0.666    -2.7078   67.7097 -231.8893]\n",
      "Weights: [-4.8479  0.9179 -1.3958  0.1826  0.134 ]\n",
      "MSE loss: 85.8575\n",
      "Iteration: 163700\n",
      "Gradient: [  3.29    10.5544  19.3992 121.7303 209.9285]\n",
      "Weights: [-4.845   0.9229 -1.3949  0.1819  0.1339]\n",
      "MSE loss: 85.9041\n",
      "Iteration: 163800\n",
      "Gradient: [ -1.8266  10.4057  31.3193  23.3307 167.9525]\n",
      "Weights: [-4.8712  0.9312 -1.3942  0.1819  0.1338]\n",
      "MSE loss: 86.1062\n",
      "Iteration: 163900\n",
      "Gradient: [ -4.9709  19.0304  -9.7984 127.1567  32.2695]\n",
      "Weights: [-4.8563  0.9278 -1.3937  0.1819  0.1338]\n",
      "MSE loss: 85.852\n",
      "Iteration: 164000\n",
      "Gradient: [  3.2459  13.2148   8.8651  17.9255 -73.0971]\n",
      "Weights: [-4.8664  0.9342 -1.3953  0.1817  0.134 ]\n",
      "MSE loss: 85.9011\n",
      "Iteration: 164100\n",
      "Gradient: [  2.2634  -2.0476  12.5153  36.2883 406.2482]\n",
      "Weights: [-4.8504  0.9294 -1.3947  0.1815  0.1342]\n",
      "MSE loss: 85.9954\n",
      "Iteration: 164200\n",
      "Gradient: [  2.8778  21.4032  -1.7578 -37.0423 357.5526]\n",
      "Weights: [-4.8539  0.9281 -1.3944  0.1817  0.1341]\n",
      "MSE loss: 85.882\n",
      "Iteration: 164300\n",
      "Gradient: [  3.2397   9.9657 -11.1741  68.3112  -8.6109]\n",
      "Weights: [-4.8614  0.9297 -1.395   0.1818  0.134 ]\n",
      "MSE loss: 85.8587\n",
      "Iteration: 164400\n",
      "Gradient: [-14.1487   0.7988 -13.5297 -65.7116 -17.2469]\n",
      "Weights: [-4.8599  0.9287 -1.3971  0.1821  0.1341]\n",
      "MSE loss: 85.8949\n",
      "Iteration: 164500\n",
      "Gradient: [  5.4321 -14.6672 -12.8702  55.4877 -40.9638]\n",
      "Weights: [-4.8501  0.9309 -1.4027  0.1833  0.1344]\n",
      "MSE loss: 85.7846\n",
      "Iteration: 164600\n",
      "Gradient: [  -4.331    16.2708    9.1695    5.8502 -107.4559]\n",
      "Weights: [-4.8402  0.9222 -1.4009  0.1827  0.1343]\n",
      "MSE loss: 85.9094\n",
      "Iteration: 164700\n",
      "Gradient: [  5.2952 -14.1423 -12.0451 -99.3241 -58.5978]\n",
      "Weights: [-4.8643  0.924  -1.399   0.1835  0.1343]\n",
      "MSE loss: 86.0126\n",
      "Iteration: 164800\n",
      "Gradient: [  5.2767  10.8154  19.4037  23.0211 231.389 ]\n",
      "Weights: [-4.8456  0.9226 -1.3994  0.1837  0.1342]\n",
      "MSE loss: 85.8185\n",
      "Iteration: 164900\n",
      "Gradient: [-10.0258  26.0908  17.5947  19.6374 164.3856]\n",
      "Weights: [-4.8521  0.9297 -1.4025  0.1837  0.1342]\n",
      "MSE loss: 85.761\n",
      "Iteration: 165000\n",
      "Gradient: [ -10.7273   -3.0031  -23.1865    2.4858 -165.5437]\n",
      "Weights: [-4.8495  0.9174 -1.4024  0.1847  0.1344]\n",
      "MSE loss: 85.8404\n",
      "Iteration: 165100\n",
      "Gradient: [  -0.5312   10.4032    2.2239   34.1958 -330.8714]\n",
      "Weights: [-4.8599  0.9245 -1.4     0.1847  0.1343]\n",
      "MSE loss: 85.972\n",
      "Iteration: 165200\n",
      "Gradient: [  5.5937  -0.804   20.8431 -27.3442   6.6807]\n",
      "Weights: [-4.8547  0.9286 -1.403   0.1854  0.1339]\n",
      "MSE loss: 85.7632\n",
      "Iteration: 165300\n",
      "Gradient: [ -14.4919   -8.7064  -33.4586  -69.4914 -200.7807]\n",
      "Weights: [-4.8665  0.9217 -1.3987  0.1845  0.1338]\n",
      "MSE loss: 86.2806\n",
      "Iteration: 165400\n",
      "Gradient: [  5.2746  -0.3868   7.7833  87.8    188.5799]\n",
      "Weights: [-4.847   0.9207 -1.3976  0.1842  0.134 ]\n",
      "MSE loss: 85.846\n",
      "Iteration: 165500\n",
      "Gradient: [  0.8541   3.7394   4.6347 -35.8309  73.6184]\n",
      "Weights: [-4.8414  0.9141 -1.3964  0.1842  0.1338]\n",
      "MSE loss: 85.7795\n",
      "Iteration: 165600\n",
      "Gradient: [  -9.9156  -11.5849    9.7215  -38.4057 -206.3693]\n",
      "Weights: [-4.8692  0.9263 -1.3978  0.1845  0.1338]\n",
      "MSE loss: 86.0276\n",
      "Iteration: 165700\n",
      "Gradient: [  4.8061 -11.2873   5.8305   3.8524 108.5474]\n",
      "Weights: [-4.864   0.925  -1.3973  0.1848  0.1338]\n",
      "MSE loss: 85.9153\n",
      "Iteration: 165800\n",
      "Gradient: [  5.4387  -6.8666 -40.1974  39.5159 -13.4027]\n",
      "Weights: [-4.8504  0.9285 -1.3991  0.1842  0.1337]\n",
      "MSE loss: 85.8307\n",
      "Iteration: 165900\n",
      "Gradient: [  -5.3418   -4.2118   27.2004  -30.3784 -228.7852]\n",
      "Weights: [-4.8631  0.9266 -1.3993  0.1846  0.1336]\n",
      "MSE loss: 85.9987\n",
      "Iteration: 166000\n",
      "Gradient: [   6.3731   -8.161   -48.915    27.4088 -235.22  ]\n",
      "Weights: [-4.8662  0.9302 -1.4025  0.1856  0.1335]\n",
      "MSE loss: 85.9996\n",
      "Iteration: 166100\n",
      "Gradient: [  6.118   -3.1008 -21.6834 -18.5095  71.6674]\n",
      "Weights: [-4.8476  0.9314 -1.4065  0.1865  0.1334]\n",
      "MSE loss: 85.8025\n",
      "Iteration: 166200\n",
      "Gradient: [  -5.3139   -9.4004  -49.7639  -39.1323 -127.4447]\n",
      "Weights: [-4.8692  0.9398 -1.4076  0.1872  0.1334]\n",
      "MSE loss: 85.8569\n",
      "Iteration: 166300\n",
      "Gradient: [ -7.7325  -8.2228   8.2075  16.7333 384.8007]\n",
      "Weights: [-4.8566  0.9239 -1.4057  0.1873  0.1335]\n",
      "MSE loss: 85.8943\n",
      "Iteration: 166400\n",
      "Gradient: [   6.3726   -6.1484  -16.6242    2.0807 -126.268 ]\n",
      "Weights: [-4.8489  0.9295 -1.4087  0.1877  0.1334]\n",
      "MSE loss: 85.7693\n",
      "Iteration: 166500\n",
      "Gradient: [-5.6284 -8.72   19.2962 36.7677 99.972 ]\n",
      "Weights: [-4.8557  0.933  -1.4107  0.1873  0.1337]\n",
      "MSE loss: 85.8117\n",
      "Iteration: 166600\n",
      "Gradient: [ -4.1105   4.5242  -0.0839 -25.573   57.7539]\n",
      "Weights: [-4.8536  0.9331 -1.4109  0.1872  0.1339]\n",
      "MSE loss: 85.722\n",
      "Iteration: 166700\n",
      "Gradient: [   1.6242    3.6605  -12.0182  -31.0793 -296.894 ]\n",
      "Weights: [-4.849   0.9345 -1.4127  0.1872  0.134 ]\n",
      "MSE loss: 85.7274\n",
      "Iteration: 166800\n",
      "Gradient: [   1.4938    2.8974   39.0392  -15.0673 -141.7091]\n",
      "Weights: [-4.8392  0.9321 -1.4112  0.1874  0.1339]\n",
      "MSE loss: 85.8794\n",
      "Iteration: 166900\n",
      "Gradient: [-1.6664 -1.7948 -6.8675 11.3056 50.6241]\n",
      "Weights: [-4.8379  0.9361 -1.4127  0.1875  0.1338]\n",
      "MSE loss: 86.0382\n",
      "Iteration: 167000\n",
      "Gradient: [ -9.4165  -8.8624 -22.921  -29.3781  58.547 ]\n",
      "Weights: [-4.8365  0.9269 -1.4117  0.188   0.1337]\n",
      "MSE loss: 85.7994\n",
      "Iteration: 167100\n",
      "Gradient: [ -10.329    -2.079     3.7391   23.9352 -147.2554]\n",
      "Weights: [-4.8518  0.9385 -1.415   0.188   0.1339]\n",
      "MSE loss: 85.6949\n",
      "Iteration: 167200\n",
      "Gradient: [  -5.923    -5.2212    8.4613  -72.5643 -215.6501]\n",
      "Weights: [-4.8432  0.9347 -1.4146  0.1881  0.1339]\n",
      "MSE loss: 85.7474\n",
      "Iteration: 167300\n",
      "Gradient: [   1.6158    2.7282  -23.0673  -52.3544 -327.7061]\n",
      "Weights: [-4.8686  0.9506 -1.4174  0.1879  0.1341]\n",
      "MSE loss: 85.7981\n",
      "Iteration: 167400\n",
      "Gradient: [   2.0139   -9.9153    7.9764 -108.4267   75.8872]\n",
      "Weights: [-4.8664  0.9506 -1.4187  0.1882  0.134 ]\n",
      "MSE loss: 85.7632\n",
      "Iteration: 167500\n",
      "Gradient: [   2.0069    8.2097    1.7549   25.881  -173.8114]\n",
      "Weights: [-4.8613  0.94   -1.4183  0.1893  0.1336]\n",
      "MSE loss: 86.0454\n",
      "Iteration: 167600\n",
      "Gradient: [ -2.1041   7.8151  15.6544 -38.2296 139.054 ]\n",
      "Weights: [-4.8458  0.9388 -1.4167  0.1894  0.1335]\n",
      "MSE loss: 85.7198\n",
      "Iteration: 167700\n",
      "Gradient: [  -1.675   -11.1102   22.3341   23.5213 -127.3675]\n",
      "Weights: [-4.858   0.9418 -1.4182  0.1901  0.1335]\n",
      "MSE loss: 85.683\n",
      "Iteration: 167800\n",
      "Gradient: [ -8.482   11.3383 -16.874  109.8285 162.6554]\n",
      "Weights: [-4.8525  0.9536 -1.4211  0.1903  0.1334]\n",
      "MSE loss: 85.9277\n",
      "Iteration: 167900\n",
      "Gradient: [ -2.3723  -3.339   42.5683 -11.9954  19.7911]\n",
      "Weights: [-4.8748  0.9517 -1.4201  0.1905  0.1333]\n",
      "MSE loss: 85.9441\n",
      "Iteration: 168000\n",
      "Gradient: [  -1.4743   -0.7495  -28.3795 -114.4786  173.8913]\n",
      "Weights: [-4.8739  0.96   -1.4204  0.1905  0.133 ]\n",
      "MSE loss: 85.784\n",
      "Iteration: 168100\n",
      "Gradient: [  3.443   15.1568 -14.8252  77.4461  -2.3019]\n",
      "Weights: [-4.8548  0.9617 -1.4248  0.1913  0.133 ]\n",
      "MSE loss: 85.927\n",
      "Iteration: 168200\n",
      "Gradient: [ -8.5563   2.0946  15.1666  19.8553 224.2268]\n",
      "Weights: [-4.8849  0.9592 -1.4238  0.1922  0.1331]\n",
      "MSE loss: 86.1393\n",
      "Iteration: 168300\n",
      "Gradient: [  -9.9194  -20.2741  -28.2712  -23.4879 -251.7721]\n",
      "Weights: [-4.8612  0.9513 -1.4268  0.1917  0.1333]\n",
      "MSE loss: 85.928\n",
      "Iteration: 168400\n",
      "Gradient: [ -4.6058  10.3217  39.3204 -17.6393 -96.2588]\n",
      "Weights: [-4.8574  0.9489 -1.4236  0.1915  0.1333]\n",
      "MSE loss: 85.6643\n",
      "Iteration: 168500\n",
      "Gradient: [ -5.8643   8.4555  16.763   73.7126 -55.1849]\n",
      "Weights: [-4.8531  0.9559 -1.425   0.1909  0.1332]\n",
      "MSE loss: 85.7472\n",
      "Iteration: 168600\n",
      "Gradient: [ 0.2967 10.7168 -0.5433  5.2917 64.1749]\n",
      "Weights: [-4.8754  0.9641 -1.4268  0.1909  0.1334]\n",
      "MSE loss: 85.8453\n",
      "Iteration: 168700\n",
      "Gradient: [   4.1948   -2.4618   -8.5052  -12.1641 -230.0184]\n",
      "Weights: [-4.8693  0.9686 -1.4309  0.1917  0.1335]\n",
      "MSE loss: 85.6787\n",
      "Iteration: 168800\n",
      "Gradient: [ 3.0724  4.3733 47.2876 27.6829  9.1307]\n",
      "Weights: [-4.8546  0.9632 -1.4278  0.1907  0.1335]\n",
      "MSE loss: 85.7954\n",
      "Iteration: 168900\n",
      "Gradient: [-0.7002  2.6848  7.652  64.2144 94.9123]\n",
      "Weights: [-4.8493  0.954  -1.4291  0.1915  0.1336]\n",
      "MSE loss: 85.7137\n",
      "Iteration: 169000\n",
      "Gradient: [ 10.253   -8.4579  17.3677  14.9803 -63.3823]\n",
      "Weights: [-4.8518  0.9471 -1.4263  0.1915  0.1338]\n",
      "MSE loss: 85.6497\n",
      "Iteration: 169100\n",
      "Gradient: [  5.4529   3.0272   8.8297 -73.2633  54.5427]\n",
      "Weights: [-4.8545  0.9505 -1.4251  0.1908  0.1338]\n",
      "MSE loss: 85.6832\n",
      "Iteration: 169200\n",
      "Gradient: [  1.2968   1.9241  22.4957  66.3874 173.9625]\n",
      "Weights: [-4.8514  0.9474 -1.4248  0.1908  0.1338]\n",
      "MSE loss: 85.6585\n",
      "Iteration: 169300\n",
      "Gradient: [  0.694    2.2913   6.2261  -5.4107 -90.7169]\n",
      "Weights: [-4.8403  0.9421 -1.4262  0.1919  0.1339]\n",
      "MSE loss: 85.841\n",
      "Iteration: 169400\n",
      "Gradient: [ -16.4043  -26.4094  -14.2504  -17.8102 -413.7771]\n",
      "Weights: [-4.8751  0.9494 -1.428   0.1922  0.1337]\n",
      "MSE loss: 86.5108\n",
      "Iteration: 169500\n",
      "Gradient: [ -0.8624  -6.549  -24.5177  14.6491 157.3615]\n",
      "Weights: [-4.8753  0.9666 -1.427   0.1914  0.1334]\n",
      "MSE loss: 85.7601\n",
      "Iteration: 169600\n",
      "Gradient: [  8.4745 -17.9248  34.4241   2.469    1.8731]\n",
      "Weights: [-4.8623  0.967  -1.4279  0.1914  0.1333]\n",
      "MSE loss: 85.7628\n",
      "Iteration: 169700\n",
      "Gradient: [ -9.9816   1.3803 -20.2116 -15.1332  16.2027]\n",
      "Weights: [-4.8713  0.9602 -1.4257  0.1913  0.1333]\n",
      "MSE loss: 85.7524\n",
      "Iteration: 169800\n",
      "Gradient: [  9.0418 -16.0793  57.2995  77.4506   0.8691]\n",
      "Weights: [-4.8249  0.9419 -1.4238  0.1913  0.1335]\n",
      "MSE loss: 86.6083\n",
      "Iteration: 169900\n",
      "Gradient: [ -3.3316 -13.3525  -3.8872 -35.7441  30.0146]\n",
      "Weights: [-4.8458  0.9395 -1.4247  0.1907  0.1336]\n",
      "MSE loss: 86.0085\n",
      "Iteration: 170000\n",
      "Gradient: [   2.4072   17.9048   13.8392  -16.4681 -257.2205]\n",
      "Weights: [-4.8443  0.9394 -1.4266  0.192   0.1338]\n",
      "MSE loss: 85.6893\n",
      "Iteration: 170100\n",
      "Gradient: [ -3.7087  11.1668  18.8673 124.3646 281.0899]\n",
      "Weights: [-4.8404  0.9406 -1.424   0.1924  0.1338]\n",
      "MSE loss: 86.3522\n",
      "Iteration: 170200\n",
      "Gradient: [ 2.8352  3.8391  0.6801 79.5909 36.3624]\n",
      "Weights: [-4.8463  0.9381 -1.4248  0.1923  0.1336]\n",
      "MSE loss: 85.664\n",
      "Iteration: 170300\n",
      "Gradient: [  -2.1179   -5.3383   24.993    29.5274 -211.9173]\n",
      "Weights: [-4.8542  0.9417 -1.4226  0.1913  0.1336]\n",
      "MSE loss: 85.6724\n",
      "Iteration: 170400\n",
      "Gradient: [  0.7874 -10.9463  41.2996  72.7885 319.1482]\n",
      "Weights: [-4.8477  0.9424 -1.423   0.1911  0.1334]\n",
      "MSE loss: 85.7201\n",
      "Iteration: 170500\n",
      "Gradient: [ -7.0381  -8.0934  20.5962 125.6941  48.1352]\n",
      "Weights: [-4.8692  0.9466 -1.4241  0.1924  0.1335]\n",
      "MSE loss: 85.9041\n",
      "Iteration: 170600\n",
      "Gradient: [  5.8105  24.9014  12.991   68.7158 305.8992]\n",
      "Weights: [-4.8397  0.9458 -1.4263  0.1922  0.1335]\n",
      "MSE loss: 85.8727\n",
      "Iteration: 170700\n",
      "Gradient: [ -12.0413   -4.9837  -76.1238  -36.2102 -208.4459]\n",
      "Weights: [-4.8561  0.943  -1.4269  0.1925  0.1336]\n",
      "MSE loss: 85.7784\n",
      "Iteration: 170800\n",
      "Gradient: [   0.9229   24.658   -40.137     0.4164 -234.4851]\n",
      "Weights: [-4.8373  0.9446 -1.4265  0.192   0.1334]\n",
      "MSE loss: 85.8468\n",
      "Iteration: 170900\n",
      "Gradient: [ -9.0338  21.9053 -25.9306  -8.2622  -8.9487]\n",
      "Weights: [-4.8497  0.9361 -1.4243  0.1922  0.1338]\n",
      "MSE loss: 85.7164\n",
      "Iteration: 171000\n",
      "Gradient: [-8.663   0.0932 19.0101  7.5948 42.6302]\n",
      "Weights: [-4.853   0.9296 -1.4215  0.1915  0.1337]\n",
      "MSE loss: 86.0561\n",
      "Iteration: 171100\n",
      "Gradient: [ -8.4395   8.6744   8.7885   0.9663 -33.6777]\n",
      "Weights: [-4.8527  0.9444 -1.422   0.1918  0.1334]\n",
      "MSE loss: 85.739\n",
      "Iteration: 171200\n",
      "Gradient: [   2.9803   -2.7341  -42.7107  124.5994 -196.7495]\n",
      "Weights: [-4.8505  0.9394 -1.4206  0.1925  0.1333]\n",
      "MSE loss: 85.7645\n",
      "Iteration: 171300\n",
      "Gradient: [ -11.7862  -21.8056  -40.389  -200.9693  109.5048]\n",
      "Weights: [-4.8659  0.9421 -1.4211  0.193   0.1331]\n",
      "MSE loss: 85.8022\n",
      "Iteration: 171400\n",
      "Gradient: [   7.2114    4.6193    6.5012 -135.2182 -103.7247]\n",
      "Weights: [-4.8398  0.9295 -1.4184  0.1933  0.1329]\n",
      "MSE loss: 85.7764\n",
      "Iteration: 171500\n",
      "Gradient: [ -1.2354   0.6075   7.4196 -54.8493 107.8254]\n",
      "Weights: [-4.8499  0.9377 -1.4187  0.1925  0.1329]\n",
      "MSE loss: 85.6833\n",
      "Iteration: 171600\n",
      "Gradient: [  4.631   13.2251  55.4788 -72.0024  36.1031]\n",
      "Weights: [-4.8402  0.9275 -1.4165  0.1924  0.1327]\n",
      "MSE loss: 85.6972\n",
      "Iteration: 171700\n",
      "Gradient: [ -4.9147   8.7264  -3.6389  32.5362 142.4555]\n",
      "Weights: [-4.8495  0.9326 -1.4159  0.1926  0.133 ]\n",
      "MSE loss: 85.8237\n",
      "Iteration: 171800\n",
      "Gradient: [   2.8093   10.9022    5.8971   94.39   -161.8826]\n",
      "Weights: [-4.8452  0.9319 -1.4153  0.1927  0.1325]\n",
      "MSE loss: 85.6816\n",
      "Iteration: 171900\n",
      "Gradient: [  2.4368 -19.7831 -15.6527 -57.8073 212.1563]\n",
      "Weights: [-4.8601  0.935  -1.4164  0.1931  0.1325]\n",
      "MSE loss: 85.7024\n",
      "Iteration: 172000\n",
      "Gradient: [  -3.5401   12.0435   -1.1569  146.88   -293.8696]\n",
      "Weights: [-4.8609  0.9334 -1.4149  0.1931  0.1325]\n",
      "MSE loss: 85.7213\n",
      "Iteration: 172100\n",
      "Gradient: [ -0.4493  15.978  -12.0757  37.764   47.3819]\n",
      "Weights: [-4.8444  0.9329 -1.4181  0.1934  0.1324]\n",
      "MSE loss: 85.6563\n",
      "Iteration: 172200\n",
      "Gradient: [  -6.1912  -11.9609   34.6032   23.4836 -123.4094]\n",
      "Weights: [-4.8683  0.9384 -1.4172  0.1934  0.1324]\n",
      "MSE loss: 85.8926\n",
      "Iteration: 172300\n",
      "Gradient: [ 11.4084  17.1082 -13.7207 -47.5226 384.037 ]\n",
      "Weights: [-4.8553  0.945  -1.4221  0.1937  0.1325]\n",
      "MSE loss: 85.6229\n",
      "Iteration: 172400\n",
      "Gradient: [ -0.9375  -2.9273  51.4519  -6.1741 125.3407]\n",
      "Weights: [-4.8505  0.9503 -1.4235  0.1934  0.1326]\n",
      "MSE loss: 85.7822\n",
      "Iteration: 172500\n",
      "Gradient: [   6.967   -22.6511  -26.9101   -4.6747 -312.8136]\n",
      "Weights: [-4.8618  0.9517 -1.4255  0.1944  0.1325]\n",
      "MSE loss: 85.6243\n",
      "Iteration: 172600\n",
      "Gradient: [  -6.2425  -13.7732    2.0584 -106.5512 -295.4084]\n",
      "Weights: [-4.8825  0.9644 -1.4233  0.1935  0.1323]\n",
      "MSE loss: 85.901\n",
      "Iteration: 172700\n",
      "Gradient: [  0.8837  -8.5554  28.952   54.9937 112.9751]\n",
      "Weights: [-4.8532  0.9451 -1.4239  0.1939  0.1324]\n",
      "MSE loss: 85.6761\n",
      "Iteration: 172800\n",
      "Gradient: [ -4.4328  -4.0472  12.6054 -12.582  -35.3295]\n",
      "Weights: [-4.8394  0.9394 -1.4218  0.1943  0.1322]\n",
      "MSE loss: 85.8168\n",
      "Iteration: 172900\n",
      "Gradient: [  6.1529 -18.2565 -13.8308 -62.7729  51.3848]\n",
      "Weights: [-4.8465  0.9364 -1.4205  0.1948  0.1322]\n",
      "MSE loss: 85.6332\n",
      "Iteration: 173000\n",
      "Gradient: [ -4.4336 -12.2062  -6.4677 -95.1073 205.2098]\n",
      "Weights: [-4.8649  0.9402 -1.4229  0.1953  0.1324]\n",
      "MSE loss: 85.8012\n",
      "Iteration: 173100\n",
      "Gradient: [ -8.4293  11.7591   9.7142 -82.48   168.8786]\n",
      "Weights: [-4.841   0.9337 -1.4217  0.1956  0.1322]\n",
      "MSE loss: 85.6891\n",
      "Iteration: 173200\n",
      "Gradient: [  -2.0928   -2.4715    7.4597  101.4795 -327.6547]\n",
      "Weights: [-4.8609  0.9375 -1.4219  0.1952  0.1326]\n",
      "MSE loss: 85.7437\n",
      "Iteration: 173300\n",
      "Gradient: [ -2.245  -13.4203 -11.5357  -8.2103 133.0538]\n",
      "Weights: [-4.8446  0.9266 -1.423   0.1953  0.1327]\n",
      "MSE loss: 85.7155\n",
      "Iteration: 173400\n",
      "Gradient: [  1.5956  -4.0293  -1.1862 -22.3262  16.1456]\n",
      "Weights: [-4.8467  0.9274 -1.4241  0.1967  0.1324]\n",
      "MSE loss: 85.6548\n",
      "Iteration: 173500\n",
      "Gradient: [   4.461     8.0451  -29.2775   49.6916 -128.4395]\n",
      "Weights: [-4.8463  0.9353 -1.4257  0.1968  0.1323]\n",
      "MSE loss: 85.5932\n",
      "Iteration: 173600\n",
      "Gradient: [  -0.9314   -3.5902  -11.5151  -56.7079 -195.7356]\n",
      "Weights: [-4.8668  0.9431 -1.4274  0.1966  0.1324]\n",
      "MSE loss: 85.8895\n",
      "Iteration: 173700\n",
      "Gradient: [  2.7794  26.5045 -28.1078  97.3788 206.8592]\n",
      "Weights: [-4.874   0.9556 -1.4294  0.1972  0.1323]\n",
      "MSE loss: 85.8564\n",
      "Iteration: 173800\n",
      "Gradient: [ -6.4117   4.514  -37.8425  -4.5739  68.6426]\n",
      "Weights: [-4.8636  0.9528 -1.4303  0.197   0.1322]\n",
      "MSE loss: 85.6105\n",
      "Iteration: 173900\n",
      "Gradient: [  4.9385   8.0735 -39.2045 -35.6967 213.4098]\n",
      "Weights: [-4.8484  0.9415 -1.4297  0.1972  0.1324]\n",
      "MSE loss: 85.5821\n",
      "Iteration: 174000\n",
      "Gradient: [ -1.2175  -2.5983  16.6497 -83.1092 142.3202]\n",
      "Weights: [-4.8571  0.9557 -1.4303  0.1962  0.1323]\n",
      "MSE loss: 85.6145\n",
      "Iteration: 174100\n",
      "Gradient: [12.9095 13.915  34.1614  2.3594 21.4653]\n",
      "Weights: [-4.8553  0.9658 -1.4319  0.1965  0.1321]\n",
      "MSE loss: 86.1461\n",
      "Iteration: 174200\n",
      "Gradient: [ -3.236   10.6776 -30.8688  -2.6228  68.7918]\n",
      "Weights: [-4.8516  0.9592 -1.4342  0.1966  0.1324]\n",
      "MSE loss: 85.6884\n",
      "Iteration: 174300\n",
      "Gradient: [  2.9012  -5.9127 -40.2222 -78.8549  85.1512]\n",
      "Weights: [-4.8793  0.9651 -1.433   0.1968  0.1323]\n",
      "MSE loss: 85.8157\n",
      "Iteration: 174400\n",
      "Gradient: [  2.6159 -27.011   -7.5255  43.8008  80.6219]\n",
      "Weights: [-4.8559  0.956  -1.4327  0.1962  0.1325]\n",
      "MSE loss: 85.5824\n",
      "Iteration: 174500\n",
      "Gradient: [ -0.2903   2.1661  -1.643  -53.5606 -22.92  ]\n",
      "Weights: [-4.8647  0.9679 -1.434   0.1953  0.1326]\n",
      "MSE loss: 85.6465\n",
      "Iteration: 174600\n",
      "Gradient: [-3.3147 13.7784 -2.0629 32.883  55.3841]\n",
      "Weights: [-4.8649  0.9623 -1.434   0.1956  0.1327]\n",
      "MSE loss: 85.6086\n",
      "Iteration: 174700\n",
      "Gradient: [ -1.7271  -9.3655 -31.593  -26.8641  90.7553]\n",
      "Weights: [-4.8694  0.9701 -1.433   0.1952  0.1325]\n",
      "MSE loss: 85.6713\n",
      "Iteration: 174800\n",
      "Gradient: [ -2.0372   5.8529   4.0357 -49.8174   5.9727]\n",
      "Weights: [-4.8626  0.9648 -1.4341  0.1956  0.1325]\n",
      "MSE loss: 85.6272\n",
      "Iteration: 174900\n",
      "Gradient: [ -7.3531  15.7606   2.3595   3.7183 -47.9189]\n",
      "Weights: [-4.8485  0.9542 -1.4335  0.1957  0.1327]\n",
      "MSE loss: 85.642\n",
      "Iteration: 175000\n",
      "Gradient: [   0.229     3.4729  -17.4999   41.814  -114.2256]\n",
      "Weights: [-4.8541  0.9544 -1.4345  0.1961  0.1328]\n",
      "MSE loss: 85.5765\n",
      "Iteration: 175100\n",
      "Gradient: [   4.0053   -4.1935   17.5634   86.5046 -162.3919]\n",
      "Weights: [-4.8625  0.9614 -1.4351  0.1974  0.1323]\n",
      "MSE loss: 85.573\n",
      "Iteration: 175200\n",
      "Gradient: [-4.015  -8.4237 27.836  -8.2957 17.873 ]\n",
      "Weights: [-4.866   0.9688 -1.4351  0.1967  0.1324]\n",
      "MSE loss: 85.677\n",
      "Iteration: 175300\n",
      "Gradient: [-7.0224  1.9687 13.0865  8.813  34.3769]\n",
      "Weights: [-4.8581  0.9544 -1.4356  0.1981  0.1324]\n",
      "MSE loss: 85.557\n",
      "Iteration: 175400\n",
      "Gradient: [ -9.6725  15.9097  32.2701 -51.153  157.4802]\n",
      "Weights: [-4.8643  0.9564 -1.4357  0.1984  0.1322]\n",
      "MSE loss: 85.6062\n",
      "Iteration: 175500\n",
      "Gradient: [-10.8464   0.8724   2.26   -41.5649 127.4334]\n",
      "Weights: [-4.8717  0.9698 -1.4394  0.1982  0.1322]\n",
      "MSE loss: 85.6357\n",
      "Iteration: 175600\n",
      "Gradient: [  2.9985  -3.745  -38.2468 -14.0198 304.2979]\n",
      "Weights: [-4.8573  0.9685 -1.4386  0.1984  0.1323]\n",
      "MSE loss: 85.9423\n",
      "Iteration: 175700\n",
      "Gradient: [  9.201   -3.6268 -17.2838  32.4079 -57.6172]\n",
      "Weights: [-4.8395  0.9605 -1.4367  0.1979  0.1322]\n",
      "MSE loss: 86.2622\n",
      "Iteration: 175800\n",
      "Gradient: [ -8.8004  20.0882 -21.9241  17.057  117.2996]\n",
      "Weights: [-4.8607  0.9529 -1.4363  0.1985  0.1325]\n",
      "MSE loss: 85.617\n",
      "Iteration: 175900\n",
      "Gradient: [ -1.024   10.6656  19.061  -22.7904 122.2219]\n",
      "Weights: [-4.8451  0.9483 -1.438   0.1981  0.1325]\n",
      "MSE loss: 85.6495\n",
      "Iteration: 176000\n",
      "Gradient: [  -0.8961   -0.9454   27.3951  -78.376  -191.1125]\n",
      "Weights: [-4.866   0.9662 -1.4393  0.1977  0.1325]\n",
      "MSE loss: 85.5791\n",
      "Iteration: 176100\n",
      "Gradient: [-12.3384  16.5773  -0.2645  44.2241  27.6438]\n",
      "Weights: [-4.8791  0.9683 -1.4389  0.1985  0.1323]\n",
      "MSE loss: 85.8107\n",
      "Iteration: 176200\n",
      "Gradient: [  7.3466   5.6935  16.321  -48.3833 -72.6306]\n",
      "Weights: [-4.8592  0.9594 -1.4373  0.1986  0.1324]\n",
      "MSE loss: 85.6178\n",
      "Iteration: 176300\n",
      "Gradient: [  2.1181   5.2893  23.4989 -16.3891 208.1155]\n",
      "Weights: [-4.8469  0.9507 -1.4389  0.1984  0.1324]\n",
      "MSE loss: 85.6345\n",
      "Iteration: 176400\n",
      "Gradient: [ 3.131000e-01  1.862000e-01  1.054000e+01 -1.178220e+01  1.923927e+02]\n",
      "Weights: [-4.8534  0.9524 -1.4401  0.1998  0.1323]\n",
      "MSE loss: 85.5472\n",
      "Iteration: 176500\n",
      "Gradient: [  -6.8032   -5.8847  -81.2697 -119.4707 -433.6396]\n",
      "Weights: [-4.8499  0.9495 -1.4401  0.1994  0.1322]\n",
      "MSE loss: 85.8061\n",
      "Iteration: 176600\n",
      "Gradient: [  3.414    6.7499  -3.9312 -32.4021 -98.0887]\n",
      "Weights: [-4.8411  0.945  -1.4405  0.1992  0.1327]\n",
      "MSE loss: 85.6543\n",
      "Iteration: 176700\n",
      "Gradient: [-3.25200e-01 -1.64941e+01 -8.85000e-02  7.46576e+01  9.00337e+01]\n",
      "Weights: [-4.8502  0.9546 -1.4412  0.1984  0.1327]\n",
      "MSE loss: 85.5866\n",
      "Iteration: 176800\n",
      "Gradient: [-5.324200e+00 -1.252790e+01  8.600000e-03  1.485260e+01  1.261499e+02]\n",
      "Weights: [-4.8692  0.9603 -1.44    0.1994  0.1327]\n",
      "MSE loss: 85.8336\n",
      "Iteration: 176900\n",
      "Gradient: [ -13.3558    2.267    -4.826   -76.3406 -100.0003]\n",
      "Weights: [-4.8674  0.9617 -1.4404  0.1992  0.1325]\n",
      "MSE loss: 85.6529\n",
      "Iteration: 177000\n",
      "Gradient: [ -4.5082  -4.5074 -19.6648  28.7604 -92.199 ]\n",
      "Weights: [-4.8474  0.9586 -1.4414  0.1995  0.1322]\n",
      "MSE loss: 85.6028\n",
      "Iteration: 177100\n",
      "Gradient: [  12.2981    1.1496   -3.2928 -113.719   -86.1942]\n",
      "Weights: [-4.8472  0.9645 -1.441   0.1995  0.1323]\n",
      "MSE loss: 86.0717\n",
      "Iteration: 177200\n",
      "Gradient: [  -3.0847   -2.8394   -5.1383  -28.6368 -136.693 ]\n",
      "Weights: [-4.8578  0.9646 -1.4422  0.1988  0.1323]\n",
      "MSE loss: 85.5523\n",
      "Iteration: 177300\n",
      "Gradient: [  8.5766  10.6206  -5.8439 183.7576  55.3598]\n",
      "Weights: [-4.8702  0.9692 -1.4423  0.199   0.1326]\n",
      "MSE loss: 85.6929\n",
      "Iteration: 177400\n",
      "Gradient: [  -2.9671   -3.3727  -50.2371  -12.7956 -330.7353]\n",
      "Weights: [-4.8627  0.9578 -1.4392  0.198   0.1324]\n",
      "MSE loss: 85.8391\n",
      "Iteration: 177500\n",
      "Gradient: [-7.2311 20.4744 -1.2883 55.0054 79.9325]\n",
      "Weights: [-4.857   0.9658 -1.44    0.1976  0.1325]\n",
      "MSE loss: 85.5758\n",
      "Iteration: 177600\n",
      "Gradient: [   1.7098    4.0616   -3.5458    4.6911 -232.4771]\n",
      "Weights: [-4.8765  0.9825 -1.4429  0.1971  0.1325]\n",
      "MSE loss: 85.6542\n",
      "Iteration: 177700\n",
      "Gradient: [ -13.7686    3.7949   44.0464 -115.501   135.8227]\n",
      "Weights: [-4.8724  0.9751 -1.4454  0.1979  0.1327]\n",
      "MSE loss: 85.7337\n",
      "Iteration: 177800\n",
      "Gradient: [ 6.4937  1.8578  4.9998 59.6107 51.3406]\n",
      "Weights: [-4.865   0.9759 -1.4432  0.1977  0.1327]\n",
      "MSE loss: 85.6259\n",
      "Iteration: 177900\n",
      "Gradient: [ -2.5218  -0.6406 -26.3014  53.958    5.9989]\n",
      "Weights: [-4.8631  0.9609 -1.4432  0.1984  0.1328]\n",
      "MSE loss: 85.7564\n",
      "Iteration: 178000\n",
      "Gradient: [ -4.0023  -5.8685  -0.307   39.3609 256.3621]\n",
      "Weights: [-4.8435  0.9488 -1.4401  0.1985  0.1329]\n",
      "MSE loss: 85.6148\n",
      "Iteration: 178100\n",
      "Gradient: [   6.6608   12.9655    0.4488  114.2705 -146.8435]\n",
      "Weights: [-4.8517  0.9518 -1.4412  0.1993  0.1327]\n",
      "MSE loss: 85.5814\n",
      "Iteration: 178200\n",
      "Gradient: [  -2.0703   -5.5751  -23.7151   86.934  -343.9065]\n",
      "Weights: [-4.8445  0.9483 -1.4385  0.1988  0.1325]\n",
      "MSE loss: 85.5837\n",
      "Iteration: 178300\n",
      "Gradient: [ -2.6616  -0.8404  13.9477 -57.8997  49.5957]\n",
      "Weights: [-4.8448  0.9482 -1.4348  0.1975  0.1325]\n",
      "MSE loss: 85.6022\n",
      "Iteration: 178400\n",
      "Gradient: [  7.3884   9.821  -25.2293  56.4317 -56.6382]\n",
      "Weights: [-4.8479  0.9429 -1.4333  0.1979  0.1325]\n",
      "MSE loss: 85.5812\n",
      "Iteration: 178500\n",
      "Gradient: [ -1.4529   6.4107 -21.4717  71.6612 214.3331]\n",
      "Weights: [-4.8642  0.9526 -1.4381  0.1991  0.1325]\n",
      "MSE loss: 85.7379\n",
      "Iteration: 178600\n",
      "Gradient: [-10.0973 -27.7156 -26.2656 -83.2678  21.5445]\n",
      "Weights: [-4.8644  0.9499 -1.4355  0.1997  0.1321]\n",
      "MSE loss: 85.691\n",
      "Iteration: 178700\n",
      "Gradient: [  -3.4215    4.0209  -40.3018  -98.0273 -162.1887]\n",
      "Weights: [-4.8659  0.9535 -1.4385  0.1997  0.1322]\n",
      "MSE loss: 85.7688\n",
      "Iteration: 178800\n",
      "Gradient: [   1.2048    5.9046    8.1672  -23.5613 -282.3317]\n",
      "Weights: [-4.872   0.9607 -1.4367  0.1994  0.1321]\n",
      "MSE loss: 85.6968\n",
      "Iteration: 178900\n",
      "Gradient: [-4.79280e+00  5.28000e-02 -6.93000e-02 -6.49039e+01  6.03948e+01]\n",
      "Weights: [-4.8714  0.9614 -1.439   0.2002  0.1317]\n",
      "MSE loss: 85.7597\n",
      "Iteration: 179000\n",
      "Gradient: [ -7.5563   8.0345 -32.5063 -65.8323   0.5666]\n",
      "Weights: [-4.8709  0.9636 -1.4404  0.2003  0.1318]\n",
      "MSE loss: 85.661\n",
      "Iteration: 179100\n",
      "Gradient: [  4.0024  10.5456   0.1188 -59.3266   4.5259]\n",
      "Weights: [-4.8585  0.9589 -1.4388  0.2003  0.1318]\n",
      "MSE loss: 85.5218\n",
      "Iteration: 179200\n",
      "Gradient: [ -3.2042  -5.2419   2.8243  35.9383 -18.6246]\n",
      "Weights: [-4.8651  0.9617 -1.4397  0.2003  0.1318]\n",
      "MSE loss: 85.5525\n",
      "Iteration: 179300\n",
      "Gradient: [   9.7711   -1.1757  -29.0545   46.7713 -199.0593]\n",
      "Weights: [-4.8653  0.9622 -1.4418  0.2015  0.1319]\n",
      "MSE loss: 85.6026\n",
      "Iteration: 179400\n",
      "Gradient: [ -7.4944   3.0075  -5.432  -18.505  127.3626]\n",
      "Weights: [-4.8619  0.9592 -1.4424  0.2021  0.1319]\n",
      "MSE loss: 85.6215\n",
      "Iteration: 179500\n",
      "Gradient: [  2.498   -1.9222  -3.5015  80.7923 235.4363]\n",
      "Weights: [-4.8546  0.9599 -1.4408  0.2015  0.1318]\n",
      "MSE loss: 85.7344\n",
      "Iteration: 179600\n",
      "Gradient: [ -9.9548 -10.2848  -1.9617 -46.4763 -93.3239]\n",
      "Weights: [-4.8756  0.9661 -1.4438  0.2016  0.1316]\n",
      "MSE loss: 85.8823\n",
      "Iteration: 179700\n",
      "Gradient: [ -10.5142   10.5053  -40.1777   -9.9919 -125.0498]\n",
      "Weights: [-4.877   0.9691 -1.4457  0.2027  0.1316]\n",
      "MSE loss: 85.7455\n",
      "Iteration: 179800\n",
      "Gradient: [ -4.441    6.8575  -1.3238 -16.4268  41.0354]\n",
      "Weights: [-4.8497  0.966  -1.4463  0.2029  0.1314]\n",
      "MSE loss: 85.6619\n",
      "Iteration: 179900\n",
      "Gradient: [ -2.1978  -6.6065  16.2205   8.4722 101.8643]\n",
      "Weights: [-4.8466  0.957  -1.4447  0.2037  0.1313]\n",
      "MSE loss: 85.5673\n",
      "Iteration: 180000\n",
      "Gradient: [ -11.565    -6.9155   -1.2759   41.9632 -155.7066]\n",
      "Weights: [-4.8721  0.9628 -1.4436  0.2035  0.1311]\n",
      "MSE loss: 85.7131\n",
      "Iteration: 180100\n",
      "Gradient: [  -3.0619  -14.4203    9.479   -42.8818 -287.3482]\n",
      "Weights: [-4.861   0.9602 -1.4451  0.2034  0.1311]\n",
      "MSE loss: 85.6794\n",
      "Iteration: 180200\n",
      "Gradient: [  -5.0749   -4.1005  -10.2792  -91.0597 -319.9831]\n",
      "Weights: [-4.8493  0.9549 -1.4454  0.203   0.1313]\n",
      "MSE loss: 85.7516\n",
      "Iteration: 180300\n",
      "Gradient: [   3.7173   15.0057    9.1601  -21.0213 -364.6558]\n",
      "Weights: [-4.8452  0.9572 -1.4454  0.204   0.1314]\n",
      "MSE loss: 85.6911\n",
      "Iteration: 180400\n",
      "Gradient: [  3.6255  -7.9959 -26.1489  21.3157 -98.5298]\n",
      "Weights: [-4.8622  0.9536 -1.4456  0.2049  0.1312]\n",
      "MSE loss: 85.6386\n",
      "Iteration: 180500\n",
      "Gradient: [ -2.478    3.175   28.6026 -21.5875 360.5795]\n",
      "Weights: [-4.8517  0.9537 -1.4441  0.2048  0.1311]\n",
      "MSE loss: 85.5038\n",
      "Iteration: 180600\n",
      "Gradient: [  -5.9052   -0.5529    8.5596 -127.1722 -223.5229]\n",
      "Weights: [-4.8427  0.9464 -1.4425  0.2044  0.1309]\n",
      "MSE loss: 85.5557\n",
      "Iteration: 180700\n",
      "Gradient: [   7.678    -7.7689  -28.4279   19.8465 -174.5523]\n",
      "Weights: [-4.8308  0.938  -1.4403  0.2054  0.1309]\n",
      "MSE loss: 85.8656\n",
      "Iteration: 180800\n",
      "Gradient: [  2.8194   6.0584   1.172    7.3839 281.7538]\n",
      "Weights: [-4.8507  0.9389 -1.4401  0.2054  0.1311]\n",
      "MSE loss: 85.6056\n",
      "Iteration: 180900\n",
      "Gradient: [  2.8226  -5.6818  36.2096 -42.8388 125.2755]\n",
      "Weights: [-4.8323  0.9398 -1.4419  0.2058  0.1307]\n",
      "MSE loss: 85.6426\n",
      "Iteration: 181000\n",
      "Gradient: [  -0.8655    1.5097  -22.5935   47.2962 -288.2249]\n",
      "Weights: [-4.8569  0.9485 -1.4436  0.2057  0.1306]\n",
      "MSE loss: 85.6374\n",
      "Iteration: 181100\n",
      "Gradient: [   5.8279  -15.5951  -25.7519 -119.602   100.2891]\n",
      "Weights: [-4.8388  0.9497 -1.4427  0.2049  0.1305]\n",
      "MSE loss: 85.6554\n",
      "Iteration: 181200\n",
      "Gradient: [  7.8651  10.6065 -56.4589 -53.8718 252.6039]\n",
      "Weights: [-4.8302  0.9504 -1.4452  0.2063  0.1306]\n",
      "MSE loss: 86.2163\n",
      "Iteration: 181300\n",
      "Gradient: [-2.3778 -6.1729 22.6357 21.8979 97.5704]\n",
      "Weights: [-4.838   0.9527 -1.4476  0.2068  0.1307]\n",
      "MSE loss: 85.7683\n",
      "Iteration: 181400\n",
      "Gradient: [ -1.9667  -5.4578 -17.0597  34.4202 147.86  ]\n",
      "Weights: [-4.8331  0.9431 -1.4475  0.2075  0.1306]\n",
      "MSE loss: 85.5837\n",
      "Iteration: 181500\n",
      "Gradient: [  2.6391 -11.486    4.4766  54.3941 -22.1798]\n",
      "Weights: [-4.8382  0.9328 -1.4451  0.2081  0.1308]\n",
      "MSE loss: 85.5716\n",
      "Iteration: 181600\n",
      "Gradient: [-5.0042 -3.5129  5.5587 -6.3002 -4.829 ]\n",
      "Weights: [-4.8495  0.9347 -1.441   0.2071  0.1306]\n",
      "MSE loss: 85.5779\n",
      "Iteration: 181700\n",
      "Gradient: [-0.8153 -6.0645 24.8903 37.2365 78.1388]\n",
      "Weights: [-4.8582  0.9331 -1.4403  0.2084  0.1304]\n",
      "MSE loss: 85.8183\n",
      "Iteration: 181800\n",
      "Gradient: [  3.8974  30.2925  -8.6691  26.5027 238.7785]\n",
      "Weights: [-4.8433  0.9371 -1.4421  0.2073  0.1304]\n",
      "MSE loss: 85.4693\n",
      "Iteration: 181900\n",
      "Gradient: [ -0.1929  -6.5694  -5.2465 -89.233   44.2996]\n",
      "Weights: [-4.8449  0.9298 -1.4419  0.2075  0.1306]\n",
      "MSE loss: 85.6955\n",
      "Iteration: 182000\n",
      "Gradient: [  7.722  -21.4447 -18.228    3.0411 -45.934 ]\n",
      "Weights: [-4.8222  0.9229 -1.4404  0.2072  0.1305]\n",
      "MSE loss: 85.7104\n",
      "Iteration: 182100\n",
      "Gradient: [  0.4803  14.002   -3.3117 -19.6429 224.6441]\n",
      "Weights: [-4.8526  0.9328 -1.4386  0.207   0.1304]\n",
      "MSE loss: 85.6482\n",
      "Iteration: 182200\n",
      "Gradient: [   2.4357   11.0496    9.1549  104.0415 -121.6445]\n",
      "Weights: [-4.8358  0.9208 -1.4345  0.2068  0.1304]\n",
      "MSE loss: 85.5463\n",
      "Iteration: 182300\n",
      "Gradient: [ -4.3326 -21.177  -49.1962 -74.2184 242.257 ]\n",
      "Weights: [-4.8309  0.9172 -1.4348  0.2069  0.1304]\n",
      "MSE loss: 85.5776\n",
      "Iteration: 182400\n",
      "Gradient: [ -0.8523  -9.5059 -26.164  -38.8869 -35.0381]\n",
      "Weights: [-4.8333  0.9191 -1.4349  0.2072  0.1302]\n",
      "MSE loss: 85.5457\n",
      "Iteration: 182500\n",
      "Gradient: [-3.456800e+00  1.088000e-01 -9.439400e+00 -2.544280e+01 -3.240653e+02]\n",
      "Weights: [-4.8583  0.923  -1.4347  0.2078  0.13  ]\n",
      "MSE loss: 86.2181\n",
      "Iteration: 182600\n",
      "Gradient: [ -1.4367  -7.3839  -4.7391 -77.9169 131.3877]\n",
      "Weights: [-4.8222  0.9262 -1.4367  0.2078  0.13  ]\n",
      "MSE loss: 86.0699\n",
      "Iteration: 182700\n",
      "Gradient: [  5.4186   0.3331  69.4453 -38.2825 -42.1689]\n",
      "Weights: [-4.8208  0.9228 -1.4383  0.2079  0.1301]\n",
      "MSE loss: 85.756\n",
      "Iteration: 182800\n",
      "Gradient: [ 11.2296  16.7129  -5.055   82.5897 143.3143]\n",
      "Weights: [-4.8278  0.9177 -1.4349  0.2085  0.13  ]\n",
      "MSE loss: 85.7142\n",
      "Iteration: 182900\n",
      "Gradient: [ -2.4503  13.4444 -53.8386  -9.5615  60.9077]\n",
      "Weights: [-4.8364  0.9247 -1.4326  0.2075  0.1297]\n",
      "MSE loss: 85.5482\n",
      "Iteration: 183000\n",
      "Gradient: [  -6.3385  -13.8167   -4.3619 -179.1662 -269.5281]\n",
      "Weights: [-4.8253  0.9084 -1.4334  0.2085  0.1296]\n",
      "MSE loss: 85.8493\n",
      "Iteration: 183100\n",
      "Gradient: [ 15.6371   4.4158  -1.2198  -7.1431 -12.0582]\n",
      "Weights: [-4.8281  0.9171 -1.4327  0.2088  0.1294]\n",
      "MSE loss: 85.5642\n",
      "Iteration: 183200\n",
      "Gradient: [ -4.8766  -7.1957  11.7054  26.0741 -48.4419]\n",
      "Weights: [-4.8403  0.9087 -1.4286  0.2088  0.1293]\n",
      "MSE loss: 85.657\n",
      "Iteration: 183300\n",
      "Gradient: [11.807  -9.9025 30.1251 35.7596 88.2619]\n",
      "Weights: [-4.8312  0.9149 -1.4253  0.2082  0.1291]\n",
      "MSE loss: 85.885\n",
      "Iteration: 183400\n",
      "Gradient: [   4.1227   -4.8891   -3.2014  -40.1306 -167.4694]\n",
      "Weights: [-4.8258  0.9081 -1.4259  0.2081  0.1289]\n",
      "MSE loss: 85.6276\n",
      "Iteration: 183500\n",
      "Gradient: [-3.6612 14.4698 22.1788 35.2821 36.0494]\n",
      "Weights: [-4.8532  0.9273 -1.4267  0.2082  0.1289]\n",
      "MSE loss: 85.6152\n",
      "Iteration: 183600\n",
      "Gradient: [-3.1644 -3.3312 18.0538 20.5225 84.1512]\n",
      "Weights: [-4.8417  0.9219 -1.4238  0.2075  0.1289]\n",
      "MSE loss: 85.7611\n",
      "Iteration: 183700\n",
      "Gradient: [  3.3957   7.4195  74.0483 112.0987 236.1769]\n",
      "Weights: [-4.8477  0.9199 -1.4257  0.2087  0.1287]\n",
      "MSE loss: 85.5119\n",
      "Iteration: 183800\n",
      "Gradient: [  -0.3038   -1.565    23.4788  -24.7352 -162.463 ]\n",
      "Weights: [-4.86    0.92   -1.4278  0.2093  0.1286]\n",
      "MSE loss: 86.0024\n",
      "Iteration: 183900\n",
      "Gradient: [ -1.0941  -1.558   26.7303 -89.0514 296.2432]\n",
      "Weights: [-4.8487  0.9196 -1.4286  0.2099  0.1285]\n",
      "MSE loss: 85.5391\n",
      "Iteration: 184000\n",
      "Gradient: [  -7.0921   -6.9592   14.3356 -115.582  -127.5808]\n",
      "Weights: [-4.8536  0.9261 -1.4299  0.2104  0.1283]\n",
      "MSE loss: 85.4981\n",
      "Iteration: 184100\n",
      "Gradient: [  0.1215  -2.7993  22.1944  52.5717 -65.6921]\n",
      "Weights: [-4.8421  0.9224 -1.4305  0.2107  0.1285]\n",
      "MSE loss: 85.4892\n",
      "Iteration: 184200\n",
      "Gradient: [   3.0999   -0.6474    4.1878   79.9259 -150.1339]\n",
      "Weights: [-4.8391  0.917  -1.4329  0.2118  0.1285]\n",
      "MSE loss: 85.4429\n",
      "Iteration: 184300\n",
      "Gradient: [  8.0733  -4.3361 -30.8574  74.3408  50.9307]\n",
      "Weights: [-4.8321  0.9173 -1.4351  0.2113  0.1287]\n",
      "MSE loss: 85.4732\n",
      "Iteration: 184400\n",
      "Gradient: [ -3.2862  18.1287 -27.751   86.2748 -60.7403]\n",
      "Weights: [-4.8379  0.9255 -1.4366  0.2119  0.1286]\n",
      "MSE loss: 85.4826\n",
      "Iteration: 184500\n",
      "Gradient: [ -6.8331  -7.4646 -19.5925 -35.2175 -73.5536]\n",
      "Weights: [-4.8367  0.921  -1.4363  0.2116  0.1286]\n",
      "MSE loss: 85.4855\n",
      "Iteration: 184600\n",
      "Gradient: [  -2.1905   -3.2503   19.0682   -8.6821 -182.4171]\n",
      "Weights: [-4.8299  0.9155 -1.436   0.2117  0.1287]\n",
      "MSE loss: 85.5099\n",
      "Iteration: 184700\n",
      "Gradient: [  5.2831  -8.1228   7.157  -37.6138  89.4052]\n",
      "Weights: [-4.8377  0.923  -1.4367  0.2112  0.1287]\n",
      "MSE loss: 85.4669\n",
      "Iteration: 184800\n",
      "Gradient: [  2.3598  29.5861 -57.4162  95.4552  -8.9977]\n",
      "Weights: [-4.8317  0.9194 -1.4342  0.211   0.1286]\n",
      "MSE loss: 85.496\n",
      "Iteration: 184900\n",
      "Gradient: [ -2.4089  17.8579  19.3383  20.9392 -98.8333]\n",
      "Weights: [-4.8387  0.917  -1.4324  0.2113  0.1285]\n",
      "MSE loss: 85.4449\n",
      "Iteration: 185000\n",
      "Gradient: [ -13.1555    5.0009    9.335   -59.9606 -223.3219]\n",
      "Weights: [-4.8463  0.9193 -1.4305  0.2115  0.1285]\n",
      "MSE loss: 85.5438\n",
      "Iteration: 185100\n",
      "Gradient: [  0.7215  -4.818  -50.3709 -63.4703 -92.3838]\n",
      "Weights: [-4.8348  0.915  -1.4328  0.211   0.1285]\n",
      "MSE loss: 85.5398\n",
      "Iteration: 185200\n",
      "Gradient: [ -9.5389  17.7988  -5.8719 -15.0226 218.5886]\n",
      "Weights: [-4.8367  0.9222 -1.4339  0.211   0.1286]\n",
      "MSE loss: 85.4535\n",
      "Iteration: 185300\n",
      "Gradient: [  4.6102   0.6123  -8.5489  -7.5513 162.1541]\n",
      "Weights: [-4.8304  0.919  -1.4367  0.2116  0.1288]\n",
      "MSE loss: 85.4873\n",
      "Iteration: 185400\n",
      "Gradient: [   5.4956    9.2242  -36.6186 -131.6049 -114.4363]\n",
      "Weights: [-4.8459  0.9254 -1.4371  0.2115  0.1289]\n",
      "MSE loss: 85.4485\n",
      "Iteration: 185500\n",
      "Gradient: [ -6.6067  10.7908 -21.1081 -42.0258 191.1988]\n",
      "Weights: [-4.845   0.9267 -1.4381  0.2117  0.1288]\n",
      "MSE loss: 85.434\n",
      "Iteration: 185600\n",
      "Gradient: [   9.2903   -0.3185    4.9725   -4.432  -175.4918]\n",
      "Weights: [-4.8304  0.918  -1.4359  0.2121  0.1286]\n",
      "MSE loss: 85.4949\n",
      "Iteration: 185700\n",
      "Gradient: [  6.6047 -17.8872  28.5571  36.2975 -38.3479]\n",
      "Weights: [-4.834   0.9257 -1.4386  0.2123  0.1287]\n",
      "MSE loss: 85.5031\n",
      "Iteration: 185800\n",
      "Gradient: [-4.0447 -4.5113  9.8167 94.5531  6.0545]\n",
      "Weights: [-4.8654  0.9299 -1.4391  0.2124  0.1287]\n",
      "MSE loss: 86.1264\n",
      "Iteration: 185900\n",
      "Gradient: [ -9.6874  13.4789 -10.9158 106.5247 -39.8843]\n",
      "Weights: [-4.8372  0.9256 -1.4426  0.2127  0.1289]\n",
      "MSE loss: 85.4402\n",
      "Iteration: 186000\n",
      "Gradient: [  0.9516 -21.961  -12.7372  11.8546 128.029 ]\n",
      "Weights: [-4.8479  0.9355 -1.4435  0.2126  0.1288]\n",
      "MSE loss: 85.396\n",
      "Iteration: 186100\n",
      "Gradient: [  -7.016   -26.9236   11.7431   69.3575 -395.0247]\n",
      "Weights: [-4.843   0.9312 -1.4457  0.213   0.1288]\n",
      "MSE loss: 85.6501\n",
      "Iteration: 186200\n",
      "Gradient: [16.5137 -8.8479 16.1698 31.8289 25.5375]\n",
      "Weights: [-4.8365  0.9371 -1.4458  0.213   0.1288]\n",
      "MSE loss: 85.4821\n",
      "Iteration: 186300\n",
      "Gradient: [ -4.2461  -7.2361  11.6265  11.0399 192.7475]\n",
      "Weights: [-4.8457  0.9404 -1.4447  0.2122  0.1288]\n",
      "MSE loss: 85.389\n",
      "Iteration: 186400\n",
      "Gradient: [ -0.4412  -8.6987 -18.3837 -64.6165  53.5141]\n",
      "Weights: [-4.8318  0.9398 -1.4474  0.2127  0.1288]\n",
      "MSE loss: 85.5869\n",
      "Iteration: 186500\n",
      "Gradient: [ 10.7854  18.5787  20.3042 111.3828  53.2062]\n",
      "Weights: [-4.8505  0.953  -1.4491  0.2132  0.1288]\n",
      "MSE loss: 85.5255\n",
      "Iteration: 186600\n",
      "Gradient: [ -2.792   -0.5914  -9.5315 107.1741 238.6713]\n",
      "Weights: [-4.8512  0.9492 -1.4474  0.2124  0.1288]\n",
      "MSE loss: 85.3861\n",
      "Iteration: 186700\n",
      "Gradient: [ -5.353  -12.5106  15.296   10.2738 -50.2593]\n",
      "Weights: [-4.8627  0.9481 -1.4456  0.212   0.1289]\n",
      "MSE loss: 85.494\n",
      "Iteration: 186800\n",
      "Gradient: [ 10.9561 -15.073   22.3017  40.8059 -51.2509]\n",
      "Weights: [-4.8345  0.9403 -1.4469  0.212   0.1291]\n",
      "MSE loss: 85.5311\n",
      "Iteration: 186900\n",
      "Gradient: [ -0.7343   6.0556  47.2559 -23.0286   0.6559]\n",
      "Weights: [-4.8564  0.9504 -1.4462  0.2111  0.1291]\n",
      "MSE loss: 85.4185\n",
      "Iteration: 187000\n",
      "Gradient: [  3.2659   5.1466  37.2353 -37.5046 206.3071]\n",
      "Weights: [-4.8494  0.9581 -1.4471  0.2115  0.1289]\n",
      "MSE loss: 85.7929\n",
      "Iteration: 187100\n",
      "Gradient: [  -7.5785   -1.4599  -12.6904  -26.7531 -201.8254]\n",
      "Weights: [-4.8771  0.9743 -1.4494  0.2098  0.1292]\n",
      "MSE loss: 85.5792\n",
      "Iteration: 187200\n",
      "Gradient: [  1.8096  -4.6218  15.0301 -97.4388  58.0796]\n",
      "Weights: [-4.8541  0.9569 -1.4499  0.2106  0.1294]\n",
      "MSE loss: 85.3898\n",
      "Iteration: 187300\n",
      "Gradient: [ -5.7076 -14.304   50.3178 -80.8264  62.2281]\n",
      "Weights: [-4.8538  0.943  -1.445   0.211   0.1294]\n",
      "MSE loss: 85.4308\n",
      "Iteration: 187400\n",
      "Gradient: [ -6.9649  14.1965 -12.3731 -19.3451 -78.3869]\n",
      "Weights: [-4.863   0.952  -1.4465  0.2103  0.1293]\n",
      "MSE loss: 85.5687\n",
      "Iteration: 187500\n",
      "Gradient: [ -2.2067 -13.299  -44.9237 -84.8719 -24.8254]\n",
      "Weights: [-4.8699  0.955  -1.4469  0.2102  0.1294]\n",
      "MSE loss: 85.7401\n",
      "Iteration: 187600\n",
      "Gradient: [ -2.9687   4.2248  13.3537 -87.919  167.2948]\n",
      "Weights: [-4.854   0.9553 -1.4463  0.2105  0.1292]\n",
      "MSE loss: 85.4555\n",
      "Iteration: 187700\n",
      "Gradient: [  1.9906   2.0415 -33.4663  89.2001 -95.6216]\n",
      "Weights: [-4.8408  0.9476 -1.4478  0.2114  0.1293]\n",
      "MSE loss: 85.5373\n",
      "Iteration: 187800\n",
      "Gradient: [  -6.3631   13.115    41.0266   92.791  -160.0945]\n",
      "Weights: [-4.8556  0.9527 -1.4517  0.2126  0.1294]\n",
      "MSE loss: 85.3947\n",
      "Iteration: 187900\n",
      "Gradient: [  9.0697   6.5592  25.7919  46.2014 -60.7204]\n",
      "Weights: [-4.8465  0.9508 -1.454   0.2124  0.1295]\n",
      "MSE loss: 85.3821\n",
      "Iteration: 188000\n",
      "Gradient: [  -3.4599   11.5778   30.9393   10.9354 -186.3321]\n",
      "Weights: [-4.8509  0.9493 -1.453   0.2125  0.1295]\n",
      "MSE loss: 85.3906\n",
      "Iteration: 188100\n",
      "Gradient: [ -8.5722  -6.2028  -5.1933 -68.2695 -83.0523]\n",
      "Weights: [-4.8752  0.965  -1.4561  0.2131  0.1293]\n",
      "MSE loss: 85.6952\n",
      "Iteration: 188200\n",
      "Gradient: [  -7.7317   -4.7637    4.3119   72.5651 -100.9072]\n",
      "Weights: [-4.8631  0.9709 -1.4568  0.213   0.1289]\n",
      "MSE loss: 85.3921\n",
      "Iteration: 188300\n",
      "Gradient: [  7.0578   1.9168  -9.7678 -60.6726 -65.3867]\n",
      "Weights: [-4.8621  0.9673 -1.4546  0.2123  0.1292]\n",
      "MSE loss: 85.3938\n",
      "Iteration: 188400\n",
      "Gradient: [ -4.0003  -3.2432   0.2469  71.8171 -73.2054]\n",
      "Weights: [-4.8745  0.9659 -1.4549  0.2129  0.1291]\n",
      "MSE loss: 85.6389\n",
      "Iteration: 188500\n",
      "Gradient: [ -5.4387 -17.1985   4.3286 -48.6055 -62.6676]\n",
      "Weights: [-4.8559  0.9644 -1.4575  0.2131  0.1293]\n",
      "MSE loss: 85.3472\n",
      "Iteration: 188600\n",
      "Gradient: [ 2.5098 -6.0767  9.2513 -2.3688 14.6634]\n",
      "Weights: [-4.8548  0.9677 -1.4579  0.2133  0.1291]\n",
      "MSE loss: 85.3983\n",
      "Iteration: 188700\n",
      "Gradient: [ 10.4039 -17.4535 -14.8416  59.7489  95.3503]\n",
      "Weights: [-4.861   0.9561 -1.4586  0.214   0.1294]\n",
      "MSE loss: 85.5796\n",
      "Iteration: 188800\n",
      "Gradient: [  -0.2762   -6.501     6.4627  -76.1253 -182.63  ]\n",
      "Weights: [-4.8431  0.9568 -1.4628  0.2148  0.1293]\n",
      "MSE loss: 85.4129\n",
      "Iteration: 188900\n",
      "Gradient: [   5.2008    5.3966  -10.6836  -51.2741 -253.1541]\n",
      "Weights: [-4.848   0.9546 -1.4605  0.2146  0.1294]\n",
      "MSE loss: 85.3541\n",
      "Iteration: 189000\n",
      "Gradient: [  0.6872  -8.9181 -16.6143  72.877   15.5654]\n",
      "Weights: [-4.8488  0.9617 -1.4607  0.2147  0.1293]\n",
      "MSE loss: 85.4109\n",
      "Iteration: 189100\n",
      "Gradient: [  -2.7663   11.7091  -55.8053  -72.8063 -184.3219]\n",
      "Weights: [-4.8615  0.9666 -1.4611  0.2146  0.1293]\n",
      "MSE loss: 85.3598\n",
      "Iteration: 189200\n",
      "Gradient: [  2.1843  12.6005  21.7572  78.5676 279.4483]\n",
      "Weights: [-4.8644  0.9649 -1.4629  0.215   0.1293]\n",
      "MSE loss: 85.5126\n",
      "Iteration: 189300\n",
      "Gradient: [10.5582  3.6601 29.8186 -5.3808 84.914 ]\n",
      "Weights: [-4.8611  0.9605 -1.4597  0.215   0.1292]\n",
      "MSE loss: 85.3964\n",
      "Iteration: 189400\n",
      "Gradient: [  6.2139 -10.9607  10.5729  37.0486  47.4681]\n",
      "Weights: [-4.8479  0.9657 -1.4629  0.2149  0.1292]\n",
      "MSE loss: 85.3872\n",
      "Iteration: 189500\n",
      "Gradient: [  1.4603 -10.2012 -20.3578 -34.7949 -64.3457]\n",
      "Weights: [-4.8609  0.9632 -1.4632  0.2151  0.1293]\n",
      "MSE loss: 85.4338\n",
      "Iteration: 189600\n",
      "Gradient: [  4.7511   2.0478  55.2936  45.1877 137.3482]\n",
      "Weights: [-4.8639  0.966  -1.4657  0.2167  0.1291]\n",
      "MSE loss: 85.4309\n",
      "Iteration: 189700\n",
      "Gradient: [-2.7835 12.9944 25.0818 97.5846 37.985 ]\n",
      "Weights: [-4.8587  0.9694 -1.4666  0.2166  0.129 ]\n",
      "MSE loss: 85.2999\n",
      "Iteration: 189800\n",
      "Gradient: [  2.1003   6.7405   0.8524 -48.4418 216.5492]\n",
      "Weights: [-4.8662  0.9713 -1.4665  0.2168  0.1291]\n",
      "MSE loss: 85.3819\n",
      "Iteration: 189900\n",
      "Gradient: [  -6.1318   -9.7487  -22.8009 -110.6554 -251.1349]\n",
      "Weights: [-4.856   0.9693 -1.4674  0.217   0.129 ]\n",
      "MSE loss: 85.2908\n",
      "Iteration: 190000\n",
      "Gradient: [   6.4245  -12.3276   29.272   -23.4408 -255.9564]\n",
      "Weights: [-4.8517  0.9701 -1.4686  0.2169  0.1287]\n",
      "MSE loss: 85.3855\n",
      "Iteration: 190100\n",
      "Gradient: [ -9.3521   2.7607 -42.2052 -71.9394  43.3599]\n",
      "Weights: [-4.8599  0.9642 -1.4682  0.2179  0.1289]\n",
      "MSE loss: 85.457\n",
      "Iteration: 190200\n",
      "Gradient: [ -2.7184  -8.0524  39.6877  34.4257 -64.6369]\n",
      "Weights: [-4.8496  0.9736 -1.4704  0.2179  0.1286]\n",
      "MSE loss: 85.3416\n",
      "Iteration: 190300\n",
      "Gradient: [  -8.2063   19.9568   14.4423   -2.0913 -142.0181]\n",
      "Weights: [-4.8591  0.9766 -1.4721  0.2182  0.1289]\n",
      "MSE loss: 85.2788\n",
      "Iteration: 190400\n",
      "Gradient: [  7.1063  -8.2086  -4.6698  37.4151 278.3406]\n",
      "Weights: [-4.842   0.9612 -1.4706  0.2182  0.1291]\n",
      "MSE loss: 85.3437\n",
      "Iteration: 190500\n",
      "Gradient: [-17.8518  15.7061   4.8032 -15.1863  13.1324]\n",
      "Weights: [-4.8503  0.9624 -1.4705  0.2179  0.1291]\n",
      "MSE loss: 85.3669\n",
      "Iteration: 190600\n",
      "Gradient: [ -2.4412   1.2707   1.5422  57.3446 154.1682]\n",
      "Weights: [-4.8437  0.9609 -1.4686  0.2177  0.1291]\n",
      "MSE loss: 85.341\n",
      "Iteration: 190700\n",
      "Gradient: [ 6.9312 10.6916 25.6715 13.9012 36.5682]\n",
      "Weights: [-4.8415  0.9568 -1.468   0.2185  0.1288]\n",
      "MSE loss: 85.3354\n",
      "Iteration: 190800\n",
      "Gradient: [  0.9588  -9.1644  -6.2941 -70.8872 105.2903]\n",
      "Weights: [-4.8511  0.9569 -1.4645  0.218   0.1286]\n",
      "MSE loss: 85.3142\n",
      "Iteration: 190900\n",
      "Gradient: [ 13.2191 -17.5771 -21.0236  82.5842  -4.0587]\n",
      "Weights: [-4.8427  0.9463 -1.463   0.2182  0.1286]\n",
      "MSE loss: 85.4009\n",
      "Iteration: 191000\n",
      "Gradient: [-1.4708 -9.7251 13.2137 40.8046 75.5505]\n",
      "Weights: [-4.8605  0.9453 -1.4596  0.218   0.1287]\n",
      "MSE loss: 85.777\n",
      "Iteration: 191100\n",
      "Gradient: [-2.74700e-01 -1.05702e+01 -1.16629e+01 -1.95480e+00  3.15752e+02]\n",
      "Weights: [-4.857   0.9528 -1.4609  0.2184  0.1286]\n",
      "MSE loss: 85.4558\n",
      "Iteration: 191200\n",
      "Gradient: [  -4.2875   10.3579  -32.41    -38.5001 -214.1978]\n",
      "Weights: [-4.8619  0.9658 -1.4643  0.2183  0.1282]\n",
      "MSE loss: 85.3231\n",
      "Iteration: 191300\n",
      "Gradient: [  2.9099  -7.6805  -4.2021 -58.7006 143.7002]\n",
      "Weights: [-4.8652  0.9624 -1.4622  0.2183  0.1281]\n",
      "MSE loss: 85.4606\n",
      "Iteration: 191400\n",
      "Gradient: [-18.2828   4.7813 -41.6389  19.7512 254.1381]\n",
      "Weights: [-4.8624  0.9619 -1.4625  0.2187  0.128 ]\n",
      "MSE loss: 85.3509\n",
      "Iteration: 191500\n",
      "Gradient: [   5.514    16.5112  -30.5701 -118.2081 -191.0889]\n",
      "Weights: [-4.8432  0.9574 -1.4603  0.2188  0.1279]\n",
      "MSE loss: 85.5662\n",
      "Iteration: 191600\n",
      "Gradient: [ 20.7061  -9.4481 -13.3055  28.4793 350.269 ]\n",
      "Weights: [-4.8355  0.9567 -1.4622  0.2192  0.1279]\n",
      "MSE loss: 85.7203\n",
      "Iteration: 191700\n",
      "Gradient: [ -8.4475 -11.8966  11.3147  26.8906 -68.5985]\n",
      "Weights: [-4.868   0.9611 -1.4622  0.2196  0.1277]\n",
      "MSE loss: 85.535\n",
      "Iteration: 191800\n",
      "Gradient: [  7.6084   6.0596  84.8693 122.8763 111.6642]\n",
      "Weights: [-4.8394  0.9529 -1.4605  0.2193  0.1277]\n",
      "MSE loss: 85.4187\n",
      "Iteration: 191900\n",
      "Gradient: [-3.7232 -9.4923  1.8572 48.5188 52.5848]\n",
      "Weights: [-4.8482  0.9597 -1.4624  0.219   0.1277]\n",
      "MSE loss: 85.3218\n",
      "Iteration: 192000\n",
      "Gradient: [  -9.3216   11.0412    6.0429   88.9487 -164.6826]\n",
      "Weights: [-4.8641  0.9642 -1.4614  0.2191  0.1277]\n",
      "MSE loss: 85.3268\n",
      "Iteration: 192100\n",
      "Gradient: [  5.5876   4.363  -23.2476  23.8566 -51.3489]\n",
      "Weights: [-4.8462  0.9599 -1.4632  0.2189  0.1278]\n",
      "MSE loss: 85.3491\n",
      "Iteration: 192200\n",
      "Gradient: [ -1.2476  -3.7889   4.7369 -92.111  115.0202]\n",
      "Weights: [-4.8447  0.9444 -1.4611  0.2201  0.1279]\n",
      "MSE loss: 85.309\n",
      "Iteration: 192300\n",
      "Gradient: [  -1.0532    1.8454  -62.632   -95.7902 -229.9968]\n",
      "Weights: [-4.8537  0.9481 -1.4605  0.2203  0.1278]\n",
      "MSE loss: 85.3378\n",
      "Iteration: 192400\n",
      "Gradient: [  6.609   -6.6765   3.5593 -24.2053 133.0054]\n",
      "Weights: [-4.836   0.9435 -1.4616  0.2199  0.128 ]\n",
      "MSE loss: 85.3415\n",
      "Iteration: 192500\n",
      "Gradient: [ -7.4694 -23.7883 -62.7573  68.6752 111.7845]\n",
      "Weights: [-4.8479  0.9523 -1.4635  0.2201  0.128 ]\n",
      "MSE loss: 85.2713\n",
      "Iteration: 192600\n",
      "Gradient: [-5.16600e-01  9.60000e-03  2.87477e+01  8.66665e+01 -2.74502e+01]\n",
      "Weights: [-4.8407  0.9475 -1.4627  0.2207  0.1278]\n",
      "MSE loss: 85.3101\n",
      "Iteration: 192700\n",
      "Gradient: [ -7.0329  -4.6457  51.096   37.349  156.2552]\n",
      "Weights: [-4.8658  0.9598 -1.4648  0.2205  0.1278]\n",
      "MSE loss: 85.4923\n",
      "Iteration: 192800\n",
      "Gradient: [  -4.0842   -9.7406  -24.7118  -12.6196 -136.1726]\n",
      "Weights: [-4.8562  0.9564 -1.4639  0.2205  0.1277]\n",
      "MSE loss: 85.3194\n",
      "Iteration: 192900\n",
      "Gradient: [  3.7005  21.1126  13.6292 -32.4411 -77.8467]\n",
      "Weights: [-4.8507  0.9637 -1.467   0.2211  0.128 ]\n",
      "MSE loss: 85.6355\n",
      "Iteration: 193000\n",
      "Gradient: [  -2.0121   -6.2796    9.2109 -138.9677  -83.    ]\n",
      "Weights: [-4.8558  0.9682 -1.4686  0.2211  0.1277]\n",
      "MSE loss: 85.2532\n",
      "Iteration: 193100\n",
      "Gradient: [-6.47000e-02 -1.13917e+01  7.91721e+01 -8.05699e+01 -3.09870e+00]\n",
      "Weights: [-4.8541  0.9738 -1.4723  0.222   0.1276]\n",
      "MSE loss: 85.3029\n",
      "Iteration: 193200\n",
      "Gradient: [-10.7928  13.3602  17.2067 -21.6692 -69.5754]\n",
      "Weights: [-4.8632  0.9818 -1.472   0.2208  0.1277]\n",
      "MSE loss: 85.3038\n",
      "Iteration: 193300\n",
      "Gradient: [ -3.4216   7.5433  25.9664 -35.3189  17.6207]\n",
      "Weights: [-4.8679  0.9843 -1.4693  0.2203  0.1277]\n",
      "MSE loss: 85.506\n",
      "Iteration: 193400\n",
      "Gradient: [  5.3864  -3.617    5.76   -91.4799 -32.3102]\n",
      "Weights: [-4.8365  0.9729 -1.4727  0.2206  0.1278]\n",
      "MSE loss: 85.7709\n",
      "Iteration: 193500\n",
      "Gradient: [ -1.5286   6.3823  10.2624  32.05   200.5145]\n",
      "Weights: [-4.8523  0.9739 -1.4715  0.2205  0.1279]\n",
      "MSE loss: 85.2984\n",
      "Iteration: 193600\n",
      "Gradient: [  -6.5893   -2.3821   19.9297  -79.4325 -190.0168]\n",
      "Weights: [-4.8704  0.978  -1.469   0.2203  0.1279]\n",
      "MSE loss: 85.3427\n",
      "Iteration: 193700\n",
      "Gradient: [ -7.4674  -1.4394  -7.8989 -90.7586  94.6842]\n",
      "Weights: [-4.8639  0.9631 -1.4701  0.2214  0.1279]\n",
      "MSE loss: 85.5714\n",
      "Iteration: 193800\n",
      "Gradient: [  2.2151  -2.5907   9.048  -84.9515 -31.9579]\n",
      "Weights: [-4.869   0.96   -1.4678  0.221   0.1279]\n",
      "MSE loss: 85.9149\n",
      "Iteration: 193900\n",
      "Gradient: [  8.9522  -0.1332 -17.4601  10.7703 -92.9765]\n",
      "Weights: [-4.8561  0.9678 -1.4683  0.2209  0.1277]\n",
      "MSE loss: 85.2484\n",
      "Iteration: 194000\n",
      "Gradient: [   1.4501    6.6334  -51.422   -31.0914 -142.5095]\n",
      "Weights: [-4.8478  0.9585 -1.4684  0.2219  0.1275]\n",
      "MSE loss: 85.2573\n",
      "Iteration: 194100\n",
      "Gradient: [ -6.9125   4.6655 -22.1372 -42.0047  92.3654]\n",
      "Weights: [-4.8481  0.9513 -1.4662  0.223   0.1274]\n",
      "MSE loss: 85.2463\n",
      "Iteration: 194200\n",
      "Gradient: [ -2.3646   1.3533  26.264  -42.3057 -23.2904]\n",
      "Weights: [-4.8447  0.9517 -1.4684  0.2232  0.1274]\n",
      "MSE loss: 85.2476\n",
      "Iteration: 194300\n",
      "Gradient: [  9.145    9.6509  14.0911  68.4527 173.2508]\n",
      "Weights: [-4.8394  0.959  -1.4678  0.2226  0.1275]\n",
      "MSE loss: 85.6669\n",
      "Iteration: 194400\n",
      "Gradient: [  3.3895  18.939   19.986  -51.3888 213.7613]\n",
      "Weights: [-4.8423  0.9645 -1.4709  0.2228  0.1276]\n",
      "MSE loss: 85.5668\n",
      "Iteration: 194500\n",
      "Gradient: [ 5.41620e+00 -1.53100e-01 -5.71120e+00  6.43505e+01  1.76190e+02]\n",
      "Weights: [-4.8561  0.9745 -1.4745  0.2229  0.1275]\n",
      "MSE loss: 85.2259\n",
      "Iteration: 194600\n",
      "Gradient: [ -0.4196   6.7399 -35.4203 -11.9794  75.0956]\n",
      "Weights: [-4.8742  0.9798 -1.4739  0.2223  0.1276]\n",
      "MSE loss: 85.405\n",
      "Iteration: 194700\n",
      "Gradient: [ -2.6152  -2.6316  -7.9833 -88.6865 -38.3248]\n",
      "Weights: [-4.8656  0.972  -1.4723  0.2216  0.1276]\n",
      "MSE loss: 85.5445\n",
      "Iteration: 194800\n",
      "Gradient: [ -9.0864  11.7316 -35.0392  -1.6572  -5.6602]\n",
      "Weights: [-4.8628  0.968  -1.4733  0.2218  0.1279]\n",
      "MSE loss: 85.4694\n",
      "Iteration: 194900\n",
      "Gradient: [ -0.4763  10.5979 -40.3044 -51.2591 -20.1412]\n",
      "Weights: [-4.8597  0.9669 -1.4731  0.2221  0.1279]\n",
      "MSE loss: 85.328\n",
      "Iteration: 195000\n",
      "Gradient: [  -6.2979   -2.522    -8.8259 -134.9436 -207.0169]\n",
      "Weights: [-4.8506  0.9558 -1.4687  0.2223  0.1278]\n",
      "MSE loss: 85.2663\n",
      "Iteration: 195100\n",
      "Gradient: [   5.0883   -9.2775    2.6171   60.3981 -221.9339]\n",
      "Weights: [-4.8523  0.964  -1.4719  0.2226  0.1277]\n",
      "MSE loss: 85.221\n",
      "Iteration: 195200\n",
      "Gradient: [   1.4788    9.2307    4.2765  -39.6974 -177.3319]\n",
      "Weights: [-4.8395  0.9598 -1.472   0.2228  0.1275]\n",
      "MSE loss: 85.3181\n",
      "Iteration: 195300\n",
      "Gradient: [ 11.6189   8.9931  -4.9562  55.9576 136.6833]\n",
      "Weights: [-4.837   0.9519 -1.4698  0.2242  0.1273]\n",
      "MSE loss: 85.3725\n",
      "Iteration: 195400\n",
      "Gradient: [   5.8294   -3.0397   -1.7614  -31.3583 -252.0556]\n",
      "Weights: [-4.845   0.9502 -1.4701  0.2245  0.1275]\n",
      "MSE loss: 85.2907\n",
      "Iteration: 195500\n",
      "Gradient: [15.3231 -1.7694 76.0278 90.1615  6.0701]\n",
      "Weights: [-4.8323  0.9519 -1.4733  0.2248  0.1276]\n",
      "MSE loss: 85.484\n",
      "Iteration: 195600\n",
      "Gradient: [   8.7223   -6.0189   10.0549   43.1833 -191.0572]\n",
      "Weights: [-4.8426  0.9596 -1.4758  0.2246  0.1274]\n",
      "MSE loss: 85.2525\n",
      "Iteration: 195700\n",
      "Gradient: [   0.863     5.5898   30.9213   65.4793 -144.7371]\n",
      "Weights: [-4.8449  0.9629 -1.4742  0.2249  0.1273]\n",
      "MSE loss: 85.3707\n",
      "Iteration: 195800\n",
      "Gradient: [ -1.2834   0.9001 -38.8224  50.2843 -83.9379]\n",
      "Weights: [-4.8382  0.9575 -1.4722  0.2242  0.1271]\n",
      "MSE loss: 85.3131\n",
      "Iteration: 195900\n",
      "Gradient: [ -7.0448 -10.0989  38.9525 -42.506  161.4095]\n",
      "Weights: [-4.8381  0.9601 -1.4703  0.2234  0.1272]\n",
      "MSE loss: 85.4264\n",
      "Iteration: 196000\n",
      "Gradient: [   4.5394   -0.4902   30.7024   22.9174 -338.2009]\n",
      "Weights: [-4.8517  0.9603 -1.4693  0.2232  0.1272]\n",
      "MSE loss: 85.2179\n",
      "Iteration: 196100\n",
      "Gradient: [ -4.1945  15.9523  14.1684  77.5462 191.6851]\n",
      "Weights: [-4.8504  0.9611 -1.4719  0.2242  0.1272]\n",
      "MSE loss: 85.2043\n",
      "Iteration: 196200\n",
      "Gradient: [  7.4573  -0.3516   6.6698  -9.5271 -17.919 ]\n",
      "Weights: [-4.8575  0.9711 -1.4708  0.2235  0.1271]\n",
      "MSE loss: 85.2982\n",
      "Iteration: 196300\n",
      "Gradient: [-5.524100e+00  3.074000e-01 -6.377130e+01  2.174110e+01 -3.863466e+02]\n",
      "Weights: [-4.8578  0.9699 -1.4716  0.2229  0.1272]\n",
      "MSE loss: 85.2292\n",
      "Iteration: 196400\n",
      "Gradient: [-5.6704  5.5137 10.9131 12.1544 22.2483]\n",
      "Weights: [-4.8501  0.9601 -1.471   0.2242  0.127 ]\n",
      "MSE loss: 85.2144\n",
      "Iteration: 196500\n",
      "Gradient: [  4.9796  15.0198  40.7966 -83.0077 220.5175]\n",
      "Weights: [-4.8645  0.9682 -1.4715  0.2249  0.127 ]\n",
      "MSE loss: 85.3372\n",
      "Iteration: 196600\n",
      "Gradient: [  -1.9979    1.5687    8.8741 -156.6998   70.1265]\n",
      "Weights: [-4.8566  0.9696 -1.4714  0.2249  0.1268]\n",
      "MSE loss: 85.3506\n",
      "Iteration: 196700\n",
      "Gradient: [ 6.690000e-02 -5.729000e-01 -1.574260e+01  2.442180e+01  2.525819e+02]\n",
      "Weights: [-4.8507  0.971  -1.4743  0.2252  0.1269]\n",
      "MSE loss: 85.3544\n",
      "Iteration: 196800\n",
      "Gradient: [  5.6368   9.6377  19.8671  69.5891 133.7866]\n",
      "Weights: [-4.8584  0.9769 -1.4752  0.2252  0.1266]\n",
      "MSE loss: 85.252\n",
      "Iteration: 196900\n",
      "Gradient: [ -5.0782   3.5697   3.9421 -32.3568 109.1866]\n",
      "Weights: [-4.8467  0.9723 -1.4743  0.2252  0.1266]\n",
      "MSE loss: 85.4517\n",
      "Iteration: 197000\n",
      "Gradient: [  -6.4653  -18.2657  -13.7277 -104.2479  -82.7436]\n",
      "Weights: [-4.849   0.9617 -1.4748  0.2251  0.1269]\n",
      "MSE loss: 85.3065\n",
      "Iteration: 197100\n",
      "Gradient: [  -1.7129    4.5733  -57.9504  -40.5672 -157.4131]\n",
      "Weights: [-4.8282  0.9455 -1.4724  0.2256  0.127 ]\n",
      "MSE loss: 85.4173\n",
      "Iteration: 197200\n",
      "Gradient: [ 10.828    6.4997   8.3613 -21.5679  -9.2253]\n",
      "Weights: [-4.8467  0.9486 -1.4703  0.2256  0.1271]\n",
      "MSE loss: 85.2605\n",
      "Iteration: 197300\n",
      "Gradient: [   3.6725   12.2335   22.4143  -24.5181 -307.1849]\n",
      "Weights: [-4.8387  0.9489 -1.4696  0.2251  0.127 ]\n",
      "MSE loss: 85.2521\n",
      "Iteration: 197400\n",
      "Gradient: [ -12.7646   -2.1732  -18.0198 -101.671    38.193 ]\n",
      "Weights: [-4.8623  0.9375 -1.4673  0.2254  0.1268]\n",
      "MSE loss: 87.3844\n",
      "Iteration: 197500\n",
      "Gradient: [ 10.3636   1.9482 -17.2641 -80.5827  40.3296]\n",
      "Weights: [-4.8303  0.9481 -1.4681  0.2248  0.1269]\n",
      "MSE loss: 85.4731\n",
      "Iteration: 197600\n",
      "Gradient: [-4.8233 -3.2873 47.1776 72.7454 71.8987]\n",
      "Weights: [-4.8453  0.9515 -1.4678  0.2251  0.1268]\n",
      "MSE loss: 85.2377\n",
      "Iteration: 197700\n",
      "Gradient: [-1.075000e-01  1.666400e+00  3.278130e+01 -4.065050e+01 -2.804541e+02]\n",
      "Weights: [-4.8462  0.9521 -1.4696  0.2248  0.1269]\n",
      "MSE loss: 85.2585\n",
      "Iteration: 197800\n",
      "Gradient: [  -2.4539   -2.0615  -17.6665   59.1107 -103.6817]\n",
      "Weights: [-4.8474  0.961  -1.4688  0.225   0.1267]\n",
      "MSE loss: 85.4303\n",
      "Iteration: 197900\n",
      "Gradient: [   6.8285   -8.6027   -9.8563 -101.3731  -45.4052]\n",
      "Weights: [-4.8636  0.9684 -1.4699  0.2243  0.1267]\n",
      "MSE loss: 85.2542\n",
      "Iteration: 198000\n",
      "Gradient: [  -7.15     -1.9272   44.1367 -100.3011 -325.6849]\n",
      "Weights: [-4.8705  0.9732 -1.4718  0.224   0.1268]\n",
      "MSE loss: 85.4271\n",
      "Iteration: 198100\n",
      "Gradient: [  5.6294   5.116  -47.5226 -24.4106 -41.0802]\n",
      "Weights: [-4.8496  0.9567 -1.4676  0.2243  0.1268]\n",
      "MSE loss: 85.2219\n",
      "Iteration: 198200\n",
      "Gradient: [ -0.2564  -7.7119 -30.1782  73.1841 -70.4737]\n",
      "Weights: [-4.8485  0.9586 -1.4693  0.2242  0.127 ]\n",
      "MSE loss: 85.2326\n",
      "Iteration: 198300\n",
      "Gradient: [  6.9124  11.5354  22.57     6.9814 161.5568]\n",
      "Weights: [-4.8628  0.9747 -1.4703  0.2244  0.127 ]\n",
      "MSE loss: 85.8629\n",
      "Iteration: 198400\n",
      "Gradient: [ -1.108    1.5322  30.2664 -19.6139 154.4596]\n",
      "Weights: [-4.8529  0.9665 -1.4721  0.2245  0.127 ]\n",
      "MSE loss: 85.2204\n",
      "Iteration: 198500\n",
      "Gradient: [  4.5365   3.1254   2.0463  51.1257 138.9012]\n",
      "Weights: [-4.8544  0.9661 -1.4728  0.2244  0.1271]\n",
      "MSE loss: 85.2054\n",
      "Iteration: 198600\n",
      "Gradient: [   3.6061  -10.2359   16.6403 -139.6027   90.327 ]\n",
      "Weights: [-4.8521  0.9551 -1.4742  0.2249  0.1271]\n",
      "MSE loss: 85.784\n",
      "Iteration: 198700\n",
      "Gradient: [  -1.0334   -2.4182   16.4822  -75.3128 -263.3431]\n",
      "Weights: [-4.8636  0.9603 -1.4724  0.2246  0.1274]\n",
      "MSE loss: 85.463\n",
      "Iteration: 198800\n",
      "Gradient: [   7.0821   -5.022    39.9121  -71.7018 -242.8098]\n",
      "Weights: [-4.8383  0.9541 -1.4696  0.224   0.1272]\n",
      "MSE loss: 85.3225\n",
      "Iteration: 198900\n",
      "Gradient: [ -1.1757  13.4538  18.2769 -92.6567 158.7281]\n",
      "Weights: [-4.8535  0.9646 -1.4745  0.2248  0.1272]\n",
      "MSE loss: 85.1957\n",
      "Iteration: 199000\n",
      "Gradient: [  9.6096   3.8253  11.3227 -18.3872 -93.0047]\n",
      "Weights: [-4.8411  0.9636 -1.4773  0.2252  0.1275]\n",
      "MSE loss: 85.3636\n",
      "Iteration: 199100\n",
      "Gradient: [  3.579  -13.7972  50.1675  -9.4282   5.1687]\n",
      "Weights: [-4.8301  0.947  -1.4784  0.2265  0.1275]\n",
      "MSE loss: 85.4051\n",
      "Iteration: 199200\n",
      "Gradient: [  7.5641 -14.9669 -12.4149 -51.0057 -11.3333]\n",
      "Weights: [-4.8514  0.9723 -1.4795  0.2258  0.1271]\n",
      "MSE loss: 85.1829\n",
      "Iteration: 199300\n",
      "Gradient: [   2.9642    5.457     5.3952  -27.9305 -357.0871]\n",
      "Weights: [-4.8382  0.9732 -1.4823  0.2255  0.1273]\n",
      "MSE loss: 85.4245\n",
      "Iteration: 199400\n",
      "Gradient: [   3.0327   -9.6272    0.7241 -116.8291  -24.7239]\n",
      "Weights: [-4.8553  0.9766 -1.4807  0.225   0.1275]\n",
      "MSE loss: 85.1803\n",
      "Iteration: 199500\n",
      "Gradient: [-13.2107   4.3663   7.7066 -43.4665  98.6578]\n",
      "Weights: [-4.8564  0.9757 -1.4787  0.225   0.1274]\n",
      "MSE loss: 85.2351\n",
      "Iteration: 199600\n",
      "Gradient: [  9.5346   6.2638  10.1947 -27.8866 136.5821]\n",
      "Weights: [-4.8538  0.9708 -1.4777  0.2252  0.1273]\n",
      "MSE loss: 85.2034\n",
      "Iteration: 199700\n",
      "Gradient: [ 12.0601  10.3198  22.5112  73.1633 -19.3328]\n",
      "Weights: [-4.8498  0.962  -1.474   0.2251  0.1272]\n",
      "MSE loss: 85.2125\n",
      "Iteration: 199800\n",
      "Gradient: [ -13.1846  -16.0133  -22.3496 -101.1564 -589.8003]\n",
      "Weights: [-4.8627  0.9505 -1.4744  0.2261  0.1271]\n",
      "MSE loss: 86.3668\n",
      "Iteration: 199900\n",
      "Gradient: [  6.2675   2.9591  -7.9965  -9.1474 158.8651]\n",
      "Weights: [-4.8324  0.9522 -1.4724  0.2257  0.1271]\n",
      "MSE loss: 85.4846\n"
     ]
    }
   ],
   "source": [
    "# Обучение на случайных батчак, по 10% датасета.\n",
    "# lr - индивидуальный для каждого из параметров модели.\n",
    "weights_3, losses_3, iter_final_3, fit_time_3 = model_fit(X_train, y_train,\n",
    "                                                          learning_rate=[1e-3, 1e-4, 1e-5, 1e-6, 1e-7],\n",
    "                                                          tolerance=(0.2**2 * N_points),\n",
    "                                                          batch_ratio=0.1,\n",
    "                                                          beta=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2309aeb6",
   "metadata": {},
   "source": [
    "Теперь добавим фильтр градиента (momentum) по правилу:\n",
    "\n",
    "${G_n = \\beta G_{n-1} + (1 - \\beta)grad_n}$,\n",
    "\n",
    "${\\beta \\neq 0}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ccc912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Gradient: [ 1714.0968  2277.851   3558.2021  6944.1239 16172.2552]\n",
      "Weights: [-0.0002 -0.     -0.     -0.     -0.    ]\n",
      "MSE loss: 40443.2783\n",
      "Iteration: 100\n",
      "Gradient: [   -7.2258    48.9271   150.0499  -475.7346 -1407.051 ]\n",
      "Weights: [-5.0947 -0.4183  0.1412  0.0772  0.0297]\n",
      "MSE loss: 694.0009\n",
      "Iteration: 200\n",
      "Gradient: [    6.6423    38.3626    65.9427  -214.7784 -1483.1315]\n",
      "Weights: [-4.7759 -0.7562  0.1249  0.095   0.0385]\n",
      "MSE loss: 453.1484\n",
      "Iteration: 300\n",
      "Gradient: [ -10.5842  -23.9014  -33.8848 -106.9348 -327.2692]\n",
      "Weights: [-4.5824 -0.9639  0.1046  0.1034  0.0455]\n",
      "MSE loss: 340.3573\n",
      "Iteration: 400\n",
      "Gradient: [ -6.1023   8.7034  86.6525  68.1228 -32.1214]\n",
      "Weights: [-4.4527 -1.0765  0.0882  0.1115  0.0504]\n",
      "MSE loss: 287.4852\n",
      "Iteration: 500\n",
      "Gradient: [ -11.7672   -7.5045   14.4699 -195.2733 -319.293 ]\n",
      "Weights: [-4.3761 -1.1716  0.0678  0.1156  0.0548]\n",
      "MSE loss: 252.1347\n",
      "Iteration: 600\n",
      "Gradient: [  -0.8032  -17.0094  -54.1181 -209.217    46.6926]\n",
      "Weights: [-4.3043 -1.2422  0.0563  0.1197  0.0574]\n",
      "MSE loss: 236.0708\n",
      "Iteration: 700\n",
      "Gradient: [   3.5148    8.5549   16.571    31.1354 -429.1746]\n",
      "Weights: [-4.2661 -1.2841  0.0483  0.1227  0.0594]\n",
      "MSE loss: 228.0871\n",
      "Iteration: 800\n",
      "Gradient: [ 4.579  14.1731 33.7109 46.9811 38.443 ]\n",
      "Weights: [-4.2316 -1.3048  0.0379  0.1237  0.0614]\n",
      "MSE loss: 222.786\n",
      "Iteration: 900\n",
      "Gradient: [ -23.3561   12.1375  -69.5765   54.8235 -383.9483]\n",
      "Weights: [-4.2152 -1.3269  0.0297  0.1241  0.0627]\n",
      "MSE loss: 218.8841\n",
      "Iteration: 1000\n",
      "Gradient: [  11.124    -0.5859    1.8741   -8.2053 -223.6363]\n",
      "Weights: [-4.2045 -1.3097  0.0141  0.1236  0.0641]\n",
      "MSE loss: 215.0759\n",
      "Iteration: 1100\n",
      "Gradient: [   3.1144    3.4789  -56.9558  -52.813  -168.1536]\n",
      "Weights: [-4.1848 -1.3244  0.0053  0.1235  0.0659]\n",
      "MSE loss: 212.0901\n",
      "Iteration: 1200\n",
      "Gradient: [  0.7468 -14.1402  64.0258 -24.2413 -66.1102]\n",
      "Weights: [-4.1507e+00 -1.3267e+00 -3.5000e-03  1.2340e-01  6.6900e-02]\n",
      "MSE loss: 211.1585\n",
      "Iteration: 1300\n",
      "Gradient: [  20.4926    6.0215  -38.3449 -211.6773 -260.2166]\n",
      "Weights: [-4.1737 -1.3159 -0.016   0.124   0.068 ]\n",
      "MSE loss: 207.7646\n",
      "Iteration: 1400\n",
      "Gradient: [ 2.487900e+00  1.104000e-01 -2.012590e+01 -1.196663e+02 -3.633990e+01]\n",
      "Weights: [-4.173  -1.2978 -0.0271  0.1233  0.0685]\n",
      "MSE loss: 205.9724\n",
      "Iteration: 1500\n",
      "Gradient: [  5.5593  10.6395   0.8511 154.0227 257.0593]\n",
      "Weights: [-4.197  -1.2753 -0.0346  0.1233  0.0695]\n",
      "MSE loss: 203.9488\n",
      "Iteration: 1600\n",
      "Gradient: [ -0.652   -6.8518  15.7933 -42.5111 111.3001]\n",
      "Weights: [-4.2085 -1.2482 -0.0462  0.1217  0.0702]\n",
      "MSE loss: 201.3409\n",
      "Iteration: 1700\n",
      "Gradient: [-1.390000e-02 -3.006180e+01  2.624320e+01 -1.216242e+02 -3.838112e+02]\n",
      "Weights: [-4.2019 -1.2327 -0.0539  0.1194  0.0709]\n",
      "MSE loss: 199.5412\n",
      "Iteration: 1800\n",
      "Gradient: [  -1.0847   -9.1975  -60.2201  -69.6347 -156.243 ]\n",
      "Weights: [-4.2099 -1.2191 -0.0637  0.1192  0.0718]\n",
      "MSE loss: 197.4722\n",
      "Iteration: 1900\n",
      "Gradient: [  8.6116  14.1316 -44.2094  23.0605  -6.6376]\n",
      "Weights: [-4.2037 -1.2124 -0.069   0.1181  0.0729]\n",
      "MSE loss: 195.9292\n",
      "Iteration: 2000\n",
      "Gradient: [ -3.1975 -10.7539 -16.4363  26.3469 307.1432]\n",
      "Weights: [-4.2148 -1.1993 -0.0775  0.117   0.0739]\n",
      "MSE loss: 193.9784\n",
      "Iteration: 2100\n",
      "Gradient: [  4.418   14.5164  65.5321 -32.5602 -23.6712]\n",
      "Weights: [-4.2069 -1.187  -0.0861  0.1165  0.0745]\n",
      "MSE loss: 192.376\n",
      "Iteration: 2200\n",
      "Gradient: [ -1.3111 -10.4239  -5.7649  44.9248  24.3176]\n",
      "Weights: [-4.2239 -1.1741 -0.0951  0.116   0.0751]\n",
      "MSE loss: 191.0865\n",
      "Iteration: 2300\n",
      "Gradient: [  -8.2387   -8.4813   34.2221  -58.1761 -556.2446]\n",
      "Weights: [-4.2104 -1.1556 -0.1037  0.1147  0.076 ]\n",
      "MSE loss: 188.8382\n",
      "Iteration: 2400\n",
      "Gradient: [ -25.1092  -34.5021   24.5231  -46.8307 -345.4078]\n",
      "Weights: [-4.2474 -1.1371 -0.1131  0.1149  0.0768]\n",
      "MSE loss: 187.502\n",
      "Iteration: 2500\n",
      "Gradient: [ 19.6941 -18.7506   9.381   45.484  117.4914]\n",
      "Weights: [-4.2285 -1.1268 -0.1219  0.1137  0.0778]\n",
      "MSE loss: 185.1397\n",
      "Iteration: 2600\n",
      "Gradient: [   2.6145   -7.7624    3.2924  -27.8505 -176.4516]\n",
      "Weights: [-4.2189 -1.1111 -0.1322  0.1127  0.0782]\n",
      "MSE loss: 183.7295\n",
      "Iteration: 2700\n",
      "Gradient: [ -3.538   10.7036  11.7762  79.3133 -37.7927]\n",
      "Weights: [-4.2443 -1.0702 -0.1455  0.1112  0.0789]\n",
      "MSE loss: 181.0133\n",
      "Iteration: 2800\n",
      "Gradient: [   4.6014    5.4079   20.306    11.8094 -361.6194]\n",
      "Weights: [-4.2666 -1.05   -0.1579  0.1109  0.08  ]\n",
      "MSE loss: 178.8235\n",
      "Iteration: 2900\n",
      "Gradient: [ -14.4105  -33.7058  -48.3065   20.1074 -338.6878]\n",
      "Weights: [-4.2684 -1.0301 -0.1695  0.1112  0.0806]\n",
      "MSE loss: 176.8521\n",
      "Iteration: 3000\n",
      "Gradient: [   1.303    -6.8304   26.3786 -133.413  -256.8629]\n",
      "Weights: [-4.2681 -1.0059 -0.1792  0.1099  0.0814]\n",
      "MSE loss: 174.7171\n",
      "Iteration: 3100\n",
      "Gradient: [ -0.2769  -4.3033  28.9038  31.6113 -84.3797]\n",
      "Weights: [-4.2646 -0.9903 -0.1889  0.1097  0.0823]\n",
      "MSE loss: 173.1911\n",
      "Iteration: 3200\n",
      "Gradient: [ 10.9769  21.7249  43.7121  42.59   452.6333]\n",
      "Weights: [-4.2946 -0.9743 -0.196   0.1099  0.0832]\n",
      "MSE loss: 171.8744\n",
      "Iteration: 3300\n",
      "Gradient: [ -1.3482 -17.6686   3.9734  16.2688  72.3365]\n",
      "Weights: [-4.2788 -0.9656 -0.2048  0.1084  0.0838]\n",
      "MSE loss: 169.956\n",
      "Iteration: 3400\n",
      "Gradient: [ -6.1042  -9.3072 -13.8893  37.1575 -78.6571]\n",
      "Weights: [-4.2921 -0.9405 -0.2143  0.1081  0.0844]\n",
      "MSE loss: 168.4426\n",
      "Iteration: 3500\n",
      "Gradient: [  -8.4606    2.9472   16.3607    5.8876 -280.5633]\n",
      "Weights: [-4.3115 -0.9157 -0.2235  0.1067  0.085 ]\n",
      "MSE loss: 166.6233\n",
      "Iteration: 3600\n",
      "Gradient: [  -0.8443   15.0585  -10.8722 -113.1714 -331.231 ]\n",
      "Weights: [-4.3391 -0.8928 -0.2351  0.1062  0.0857]\n",
      "MSE loss: 165.7825\n",
      "Iteration: 3700\n",
      "Gradient: [  1.789    6.2834  31.1984  99.4182 261.8603]\n",
      "Weights: [-4.3141 -0.8704 -0.2434  0.1049  0.0865]\n",
      "MSE loss: 163.6173\n",
      "Iteration: 3800\n",
      "Gradient: [  6.0697 -28.7917  10.1131  88.5987 137.0926]\n",
      "Weights: [-4.3246 -0.8569 -0.2546  0.1049  0.0873]\n",
      "MSE loss: 161.4345\n",
      "Iteration: 3900\n",
      "Gradient: [ -12.1863   -9.2663  -52.7751   72.6171 -318.0482]\n",
      "Weights: [-4.3454 -0.842  -0.2645  0.1037  0.0882]\n",
      "MSE loss: 160.3165\n",
      "Iteration: 4000\n",
      "Gradient: [  -2.485     1.0063   42.7111  -47.5091 -107.853 ]\n",
      "Weights: [-4.3248 -0.8345 -0.2722  0.1041  0.089 ]\n",
      "MSE loss: 158.5701\n",
      "Iteration: 4100\n",
      "Gradient: [   9.2047   12.3844   19.1063   28.4122 -208.1305]\n",
      "Weights: [-4.3059 -0.8348 -0.2792  0.103   0.0896]\n",
      "MSE loss: 157.7273\n",
      "Iteration: 4200\n",
      "Gradient: [  -9.6444   -7.716    17.6116   75.0363 -280.113 ]\n",
      "Weights: [-4.3316 -0.8213 -0.2823  0.1026  0.0903]\n",
      "MSE loss: 156.5799\n",
      "Iteration: 4300\n",
      "Gradient: [-12.2646  -5.9409  32.0454 -45.4343  34.0598]\n",
      "Weights: [-4.3492 -0.8022 -0.2913  0.1018  0.091 ]\n",
      "MSE loss: 155.2902\n",
      "Iteration: 4400\n",
      "Gradient: [  -8.3654   -8.3286    2.0243   21.2935 -340.0623]\n",
      "Weights: [-4.3427 -0.7901 -0.2976  0.1019  0.0911]\n",
      "MSE loss: 154.2684\n",
      "Iteration: 4500\n",
      "Gradient: [ -7.7332   6.9121 -42.5636  37.2771 202.7099]\n",
      "Weights: [-4.362  -0.767  -0.3064  0.1009  0.0919]\n",
      "MSE loss: 152.8788\n",
      "Iteration: 4600\n",
      "Gradient: [  8.7868  -0.4796 -55.1685 179.6346  14.8499]\n",
      "Weights: [-4.3293 -0.76   -0.3143  0.1003  0.0925]\n",
      "MSE loss: 152.2451\n",
      "Iteration: 4700\n",
      "Gradient: [  -8.1692   -9.4818   29.8537  -78.0552 -120.3117]\n",
      "Weights: [-4.3507 -0.7532 -0.3176  0.0995  0.0931]\n",
      "MSE loss: 150.8595\n",
      "Iteration: 4800\n",
      "Gradient: [   2.3075    3.2213   15.7807  -24.7154 -170.2235]\n",
      "Weights: [-4.3573 -0.7329 -0.3272  0.0983  0.0936]\n",
      "MSE loss: 149.5613\n",
      "Iteration: 4900\n",
      "Gradient: [   9.6945  -27.8918  -27.1139  -50.4531 -157.2348]\n",
      "Weights: [-4.3613 -0.7218 -0.3357  0.0976  0.0943]\n",
      "MSE loss: 148.5389\n",
      "Iteration: 5000\n",
      "Gradient: [-11.9446  -1.8146 -10.8369 -38.4161  76.7496]\n",
      "Weights: [-4.3572 -0.7074 -0.343   0.0979  0.0947]\n",
      "MSE loss: 147.2575\n",
      "Iteration: 5100\n",
      "Gradient: [ -12.5231    0.9971  -17.3821   48.5951 -109.    ]\n",
      "Weights: [-4.3953 -0.681  -0.351   0.0975  0.0953]\n",
      "MSE loss: 146.2909\n",
      "Iteration: 5200\n",
      "Gradient: [  13.7501   -0.7376   46.0346  -30.0829 -265.0021]\n",
      "Weights: [-4.3622 -0.6864 -0.3589  0.0973  0.0962]\n",
      "MSE loss: 144.9126\n",
      "Iteration: 5300\n",
      "Gradient: [-5.721800e+00 -1.323000e-01  6.539410e+01  4.527880e+01  1.323116e+02]\n",
      "Weights: [-4.37   -0.6722 -0.3679  0.0977  0.0971]\n",
      "MSE loss: 143.702\n",
      "Iteration: 5400\n",
      "Gradient: [ 9.2937  2.9898 14.3065 45.665  75.5114]\n",
      "Weights: [-4.3839 -0.6454 -0.3755  0.0967  0.0974]\n",
      "MSE loss: 142.5231\n",
      "Iteration: 5500\n",
      "Gradient: [ -4.8471  30.4733   9.0362  57.8425 322.4947]\n",
      "Weights: [-4.3815 -0.6441 -0.3791  0.0965  0.0979]\n",
      "MSE loss: 142.0158\n",
      "Iteration: 5600\n",
      "Gradient: [ 3.5812 13.484  32.914  -4.7434 90.963 ]\n",
      "Weights: [-4.3717 -0.6374 -0.385   0.0959  0.0983]\n",
      "MSE loss: 141.3995\n",
      "Iteration: 5700\n",
      "Gradient: [ -7.6863  -9.2215   6.1372  34.408  -34.1745]\n",
      "Weights: [-4.3946 -0.6147 -0.3909  0.0939  0.0987]\n",
      "MSE loss: 139.9943\n",
      "Iteration: 5800\n",
      "Gradient: [   0.6972    3.0484   13.2813 -119.3854  165.1929]\n",
      "Weights: [-4.3992 -0.6004 -0.4009  0.0946  0.0994]\n",
      "MSE loss: 138.768\n",
      "Iteration: 5900\n",
      "Gradient: [-1.4384 26.2192 41.6803 16.7782 79.9112]\n",
      "Weights: [-4.3945 -0.5836 -0.413   0.0952  0.0998]\n",
      "MSE loss: 137.6036\n",
      "Iteration: 6000\n",
      "Gradient: [ -1.3204 -11.2968 -22.8156 -30.0109 424.2296]\n",
      "Weights: [-4.3966 -0.5756 -0.419   0.0954  0.1002]\n",
      "MSE loss: 136.8892\n",
      "Iteration: 6100\n",
      "Gradient: [ -0.7352   2.9552   6.6306  47.3166 -62.5686]\n",
      "Weights: [-4.4142 -0.5593 -0.4267  0.0954  0.1008]\n",
      "MSE loss: 135.822\n",
      "Iteration: 6200\n",
      "Gradient: [ 10.8215  -2.0362  16.0243  68.5352 -29.2315]\n",
      "Weights: [-4.3871 -0.5582 -0.4293  0.0952  0.1009]\n",
      "MSE loss: 136.2771\n",
      "Iteration: 6300\n",
      "Gradient: [   4.8542   13.5337   40.3158  158.2793 -197.7732]\n",
      "Weights: [-4.4094 -0.5479 -0.4334  0.095   0.1012]\n",
      "MSE loss: 134.9902\n",
      "Iteration: 6400\n",
      "Gradient: [   6.3296   -1.0008   -7.8475   76.42   -139.76  ]\n",
      "Weights: [-4.404  -0.5367 -0.4392  0.0941  0.1017]\n",
      "MSE loss: 134.3774\n",
      "Iteration: 6500\n",
      "Gradient: [ 3.2107 -7.8618  9.151  92.2164 46.035 ]\n",
      "Weights: [-4.4486 -0.5175 -0.4426  0.0936  0.1022]\n",
      "MSE loss: 133.8038\n",
      "Iteration: 6600\n",
      "Gradient: [ 14.9559  25.2029  49.044  136.001  197.022 ]\n",
      "Weights: [-4.4268 -0.505  -0.4494  0.0937  0.1025]\n",
      "MSE loss: 133.2473\n",
      "Iteration: 6700\n",
      "Gradient: [  8.3374  15.9988  45.6228  -0.3835 143.2181]\n",
      "Weights: [-4.4273 -0.496  -0.4563  0.0933  0.1028]\n",
      "MSE loss: 132.0801\n",
      "Iteration: 6800\n",
      "Gradient: [ -5.3034 -16.4834  -3.4535  36.6062 -41.4464]\n",
      "Weights: [-4.4427 -0.4806 -0.4647  0.0928  0.1035]\n",
      "MSE loss: 130.8312\n",
      "Iteration: 6900\n",
      "Gradient: [  2.4998 -11.5598 -28.2957 -55.9069 -93.3063]\n",
      "Weights: [-4.447  -0.4642 -0.4751  0.0924  0.104 ]\n",
      "MSE loss: 129.8283\n",
      "Iteration: 7000\n",
      "Gradient: [ -6.8789   3.1578  15.9464 -94.4426  85.8522]\n",
      "Weights: [-4.4666 -0.4454 -0.4779  0.0927  0.1044]\n",
      "MSE loss: 129.5666\n",
      "Iteration: 7100\n",
      "Gradient: [  15.455    -1.9864    7.894   110.8816 -365.4716]\n",
      "Weights: [-4.4687 -0.4273 -0.4852  0.0915  0.1047]\n",
      "MSE loss: 128.483\n",
      "Iteration: 7200\n",
      "Gradient: [ -5.9     -2.3747 -11.6499 -43.624  -38.3771]\n",
      "Weights: [-4.4665 -0.4297 -0.488   0.0902  0.1052]\n",
      "MSE loss: 127.9887\n",
      "Iteration: 7300\n",
      "Gradient: [  4.5057  -3.5288 -40.7287 -26.693   -9.5975]\n",
      "Weights: [-4.4612 -0.4192 -0.4931  0.0898  0.1057]\n",
      "MSE loss: 127.1897\n",
      "Iteration: 7400\n",
      "Gradient: [ 14.2342   4.1206  -0.7131  96.9959 251.9441]\n",
      "Weights: [-4.454  -0.4132 -0.5001  0.09    0.1064]\n",
      "MSE loss: 126.6235\n",
      "Iteration: 7500\n",
      "Gradient: [ -1.6898   7.9475   5.7225 -77.5442 -79.09  ]\n",
      "Weights: [-4.4673 -0.4015 -0.5064  0.0887  0.1068]\n",
      "MSE loss: 125.719\n",
      "Iteration: 7600\n",
      "Gradient: [  8.0016  -5.5598  20.2246 -59.7136  95.6346]\n",
      "Weights: [-4.4883 -0.3786 -0.5134  0.0881  0.1074]\n",
      "MSE loss: 124.7898\n",
      "Iteration: 7700\n",
      "Gradient: [  13.5052   13.5761   41.0519  -96.345  -219.5118]\n",
      "Weights: [-4.4897 -0.3632 -0.5223  0.0877  0.1081]\n",
      "MSE loss: 123.6724\n",
      "Iteration: 7800\n",
      "Gradient: [   3.0525    6.539    -3.1006  -12.7442 -145.9888]\n",
      "Weights: [-4.4831 -0.3578 -0.5275  0.0872  0.1086]\n",
      "MSE loss: 122.9949\n",
      "Iteration: 7900\n",
      "Gradient: [  -1.7536    7.1125   19.1296   35.9561 -287.7369]\n",
      "Weights: [-4.4768 -0.3581 -0.5315  0.0871  0.1089]\n",
      "MSE loss: 122.6578\n",
      "Iteration: 8000\n",
      "Gradient: [ 4.7803 -4.3471  2.1973 56.4761 18.3005]\n",
      "Weights: [-4.4704 -0.3568 -0.5335  0.0871  0.1094]\n",
      "MSE loss: 122.3727\n",
      "Iteration: 8100\n",
      "Gradient: [-10.6817 -13.4043 -42.6164 -35.83    19.3363]\n",
      "Weights: [-4.4706 -0.3575 -0.5374  0.0864  0.1098]\n",
      "MSE loss: 121.9079\n",
      "Iteration: 8200\n",
      "Gradient: [   0.4534   -5.1573  -20.9977  -58.2834 -336.7242]\n",
      "Weights: [-4.4864 -0.3414 -0.5448  0.0867  0.1104]\n",
      "MSE loss: 121.0579\n",
      "Iteration: 8300\n",
      "Gradient: [ -8.7276  -5.2493 110.4918 -91.9307  49.9639]\n",
      "Weights: [-4.4915 -0.3289 -0.5502  0.0858  0.1108]\n",
      "MSE loss: 120.4404\n",
      "Iteration: 8400\n",
      "Gradient: [   6.1958    5.5602   36.1435  -58.5501 -150.3079]\n",
      "Weights: [-4.49   -0.3213 -0.5549  0.0861  0.111 ]\n",
      "MSE loss: 119.957\n",
      "Iteration: 8500\n",
      "Gradient: [ -3.4593  -6.0035  -3.3828  49.8575 -26.6472]\n",
      "Weights: [-4.4853 -0.3148 -0.5588  0.0854  0.1114]\n",
      "MSE loss: 119.4592\n",
      "Iteration: 8600\n",
      "Gradient: [-11.1764   0.0764  -4.2707 -57.9352  43.3294]\n",
      "Weights: [-4.4934 -0.3027 -0.5638  0.0851  0.1116]\n",
      "MSE loss: 118.9638\n",
      "Iteration: 8700\n",
      "Gradient: [ -2.5433   6.9863   9.8627  80.4838 -93.0795]\n",
      "Weights: [-4.5183 -0.2846 -0.5676  0.0855  0.1118]\n",
      "MSE loss: 118.7432\n",
      "Iteration: 8800\n",
      "Gradient: [ -6.5973  19.4604  12.7645  43.1966 -52.9987]\n",
      "Weights: [-4.4919 -0.2884 -0.5727  0.086   0.1122]\n",
      "MSE loss: 118.294\n",
      "Iteration: 8900\n",
      "Gradient: [  1.425  -17.0828   5.4394  60.8536  25.5996]\n",
      "Weights: [-4.5216 -0.2681 -0.5802  0.085   0.1127]\n",
      "MSE loss: 117.5266\n",
      "Iteration: 9000\n",
      "Gradient: [  -3.2178   -7.176   -46.3967  -89.4344 -518.2634]\n",
      "Weights: [-4.504  -0.2748 -0.5833  0.0841  0.1132]\n",
      "MSE loss: 117.3186\n",
      "Iteration: 9100\n",
      "Gradient: [  -5.5763   16.015    10.3818    6.3244 -337.1421]\n",
      "Weights: [-4.5223 -0.2494 -0.5862  0.0834  0.1134]\n",
      "MSE loss: 116.5341\n",
      "Iteration: 9200\n",
      "Gradient: [  7.7946  10.3821  44.107   59.7785 106.9401]\n",
      "Weights: [-4.5105 -0.2511 -0.5891  0.0829  0.1137]\n",
      "MSE loss: 116.1886\n",
      "Iteration: 9300\n",
      "Gradient: [ -3.2306 -18.5544   3.785    2.2364 -15.2989]\n",
      "Weights: [-4.5398 -0.233  -0.5947  0.0822  0.1144]\n",
      "MSE loss: 115.8339\n",
      "Iteration: 9400\n",
      "Gradient: [ 6.3496 -9.8163 32.1465 11.9583 83.7832]\n",
      "Weights: [-4.5131 -0.2193 -0.6025  0.0811  0.1148]\n",
      "MSE loss: 114.9604\n",
      "Iteration: 9500\n",
      "Gradient: [  -0.772    -6.4379  -48.9099   36.654  -250.2278]\n",
      "Weights: [-4.5244 -0.2191 -0.6064  0.0809  0.1152]\n",
      "MSE loss: 114.3862\n",
      "Iteration: 9600\n",
      "Gradient: [   2.196     7.6336    9.2405  -31.9596 -202.8769]\n",
      "Weights: [-4.5218 -0.2067 -0.6108  0.0808  0.1155]\n",
      "MSE loss: 113.9452\n",
      "Iteration: 9700\n",
      "Gradient: [  -6.0353    6.9633    6.4409  -29.7967 -208.6499]\n",
      "Weights: [-4.5293 -0.2062 -0.6172  0.0812  0.116 ]\n",
      "MSE loss: 113.5574\n",
      "Iteration: 9800\n",
      "Gradient: [-12.7566  19.9102 -17.729   29.1457  91.1377]\n",
      "Weights: [-4.5242 -0.2026 -0.619   0.0811  0.1163]\n",
      "MSE loss: 113.1211\n",
      "Iteration: 9900\n",
      "Gradient: [  3.2588 -39.7634 -21.954  -43.9266 -50.3215]\n",
      "Weights: [-4.5085 -0.2089 -0.6229  0.08    0.1168]\n",
      "MSE loss: 113.2416\n",
      "Iteration: 10000\n",
      "Gradient: [  0.8662  -7.755   12.0717 -40.0722  15.8326]\n",
      "Weights: [-4.5481 -0.1821 -0.6267  0.0803  0.117 ]\n",
      "MSE loss: 112.5223\n",
      "Iteration: 10100\n",
      "Gradient: [  -9.2182    1.7915   -8.9484   36.93   -269.9623]\n",
      "Weights: [-4.5353 -0.1703 -0.635   0.0802  0.1171]\n",
      "MSE loss: 111.8462\n",
      "Iteration: 10200\n",
      "Gradient: [  -6.2147   -9.7819  -32.2341  -36.6868 -287.7748]\n",
      "Weights: [-4.553  -0.1573 -0.6398  0.0802  0.1173]\n",
      "MSE loss: 111.6311\n",
      "Iteration: 10300\n",
      "Gradient: [ 8.250000e-02 -9.133800e+00  2.866600e+00 -2.764200e+01 -3.844186e+02]\n",
      "Weights: [-4.5273 -0.1564 -0.6458  0.0804  0.1176]\n",
      "MSE loss: 111.1934\n",
      "Iteration: 10400\n",
      "Gradient: [ 9.8922  4.2585 52.0396 50.7115 42.2659]\n",
      "Weights: [-4.539  -0.1479 -0.6485  0.0805  0.1181]\n",
      "MSE loss: 110.566\n",
      "Iteration: 10500\n",
      "Gradient: [  9.2586  14.885    8.4599  97.509  116.2755]\n",
      "Weights: [-4.5438 -0.1308 -0.6537  0.0798  0.1185]\n",
      "MSE loss: 110.2227\n",
      "Iteration: 10600\n",
      "Gradient: [  11.1468  -15.3036    0.943   -17.8829 -182.6652]\n",
      "Weights: [-4.5345 -0.1423 -0.6587  0.0801  0.1189]\n",
      "MSE loss: 109.9043\n",
      "Iteration: 10700\n",
      "Gradient: [ -0.0792  -5.9834  52.7769  33.6014 -39.3019]\n",
      "Weights: [-4.5412 -0.1307 -0.6605  0.0793  0.1192]\n",
      "MSE loss: 109.456\n",
      "Iteration: 10800\n",
      "Gradient: [  1.4686  -3.075   32.4196 -14.1267  -6.5601]\n",
      "Weights: [-4.5627 -0.1147 -0.6651  0.079   0.1195]\n",
      "MSE loss: 109.0744\n",
      "Iteration: 10900\n",
      "Gradient: [ -10.3923    6.2813  -33.139    43.7796 -132.7   ]\n",
      "Weights: [-4.5724 -0.0951 -0.672   0.0789  0.1198]\n",
      "MSE loss: 108.4856\n",
      "Iteration: 11000\n",
      "Gradient: [  9.3991  11.8625  10.0444 -26.0166 125.211 ]\n",
      "Weights: [-4.5681 -0.0842 -0.6747  0.0776  0.1202]\n",
      "MSE loss: 108.1407\n",
      "Iteration: 11100\n",
      "Gradient: [ -4.6108   8.6756  -7.5973 -23.8819  82.6893]\n",
      "Weights: [-4.5703 -0.0748 -0.6836  0.0787  0.1204]\n",
      "MSE loss: 107.5364\n",
      "Iteration: 11200\n",
      "Gradient: [ -7.2144  16.6196  30.5995   1.7368 189.0555]\n",
      "Weights: [-4.5735 -0.0614 -0.6899  0.0785  0.1209]\n",
      "MSE loss: 107.0925\n",
      "Iteration: 11300\n",
      "Gradient: [ -0.6832  -8.7609  11.7495  90.1569 -24.0348]\n",
      "Weights: [-4.5657 -0.0561 -0.696   0.0781  0.1212]\n",
      "MSE loss: 106.6551\n",
      "Iteration: 11400\n",
      "Gradient: [ -5.4705  -1.5411 -16.0539  18.7576 110.3107]\n",
      "Weights: [-4.5663 -0.062  -0.697   0.0788  0.1216]\n",
      "MSE loss: 106.4638\n",
      "Iteration: 11500\n",
      "Gradient: [   3.1642   -9.0785   -2.7878   35.3684 -154.8728]\n",
      "Weights: [-4.5725 -0.0559 -0.7004  0.0784  0.1216]\n",
      "MSE loss: 106.2811\n",
      "Iteration: 11600\n",
      "Gradient: [ -4.3392  -4.9516 -39.7126  20.0128  52.1001]\n",
      "Weights: [-4.5676 -0.0526 -0.7048  0.0784  0.1221]\n",
      "MSE loss: 105.8851\n",
      "Iteration: 11700\n",
      "Gradient: [   0.6783    6.2155  -59.4961  -34.2323 -301.644 ]\n",
      "Weights: [-4.5928 -0.0305 -0.7126  0.0788  0.1225]\n",
      "MSE loss: 105.3909\n",
      "Iteration: 11800\n",
      "Gradient: [  2.3561  -4.5535  54.3525 -46.9983  41.5051]\n",
      "Weights: [-4.5737 -0.0212 -0.7185  0.0776  0.1228]\n",
      "MSE loss: 104.9354\n",
      "Iteration: 11900\n",
      "Gradient: [ -28.4222  -33.6128  -47.499  -160.0122 -463.4597]\n",
      "Weights: [-4.6019 -0.0271 -0.7198  0.078   0.1232]\n",
      "MSE loss: 106.1768\n",
      "Iteration: 12000\n",
      "Gradient: [  -1.1085    4.3897   34.5722   69.2207 -221.161 ]\n",
      "Weights: [-4.5824 -0.0177 -0.7226  0.0778  0.1234]\n",
      "MSE loss: 104.4456\n",
      "Iteration: 12100\n",
      "Gradient: [   0.1955  -10.2305   -1.303    56.2076 -186.2589]\n",
      "Weights: [-4.5941 -0.0078 -0.7257  0.0772  0.1236]\n",
      "MSE loss: 104.2257\n",
      "Iteration: 12200\n",
      "Gradient: [  -2.5908   -7.0138  -30.0954   18.614  -183.9718]\n",
      "Weights: [-4.6109  0.0057 -0.7284  0.0766  0.1238]\n",
      "MSE loss: 104.2047\n",
      "Iteration: 12300\n",
      "Gradient: [ -6.013  -19.3812  15.3865   3.0989 110.6289]\n",
      "Weights: [-4.6082  0.013  -0.7322  0.076   0.124 ]\n",
      "MSE loss: 103.8359\n",
      "Iteration: 12400\n",
      "Gradient: [  -5.7755   14.5173  -23.4079   70.0068 -162.801 ]\n",
      "Weights: [-4.604   0.0276 -0.735   0.0746  0.1242]\n",
      "MSE loss: 103.4688\n",
      "Iteration: 12500\n",
      "Gradient: [ -4.7745  14.8742 -24.7464  11.6548 286.8074]\n",
      "Weights: [-4.6045  0.0305 -0.7397  0.0755  0.1246]\n",
      "MSE loss: 103.109\n",
      "Iteration: 12600\n",
      "Gradient: [-1.4532  9.2496 30.446   7.2038 65.688 ]\n",
      "Weights: [-4.6017  0.0409 -0.746   0.0751  0.1249]\n",
      "MSE loss: 102.7418\n",
      "Iteration: 12700\n",
      "Gradient: [-3.00000e-04  4.24510e+00  4.48651e+01 -9.19910e+00 -1.28630e+01]\n",
      "Weights: [-4.6064  0.0434 -0.7481  0.0747  0.1253]\n",
      "MSE loss: 102.4338\n",
      "Iteration: 12800\n",
      "Gradient: [  5.5144  26.5122 -30.5142  98.2861  45.897 ]\n",
      "Weights: [-4.6033  0.0485 -0.7491  0.0741  0.1256]\n",
      "MSE loss: 102.4731\n",
      "Iteration: 12900\n",
      "Gradient: [   3.7854   -7.6949   13.2863   45.2107 -332.3838]\n",
      "Weights: [-4.6179  0.0573 -0.7506  0.0729  0.1259]\n",
      "MSE loss: 102.0937\n",
      "Iteration: 13000\n",
      "Gradient: [-6.9461  5.3963 21.5493 15.2692 95.8437]\n",
      "Weights: [-4.6186  0.0627 -0.7554  0.0723  0.1265]\n",
      "MSE loss: 101.6574\n",
      "Iteration: 13100\n",
      "Gradient: [   0.8881    6.5809    2.6735 -137.281  -135.4951]\n",
      "Weights: [-4.6108  0.0638 -0.7591  0.0715  0.1268]\n",
      "MSE loss: 101.4217\n",
      "Iteration: 13200\n",
      "Gradient: [ -1.9113   0.3384 -34.4588   1.4154  21.8306]\n",
      "Weights: [-4.6284  0.0697 -0.7622  0.0723  0.1271]\n",
      "MSE loss: 101.3682\n",
      "Iteration: 13300\n",
      "Gradient: [  0.5905  22.5035  18.9675 109.2583  31.6897]\n",
      "Weights: [-4.6044  0.0726 -0.7636  0.0715  0.1273]\n",
      "MSE loss: 101.2772\n",
      "Iteration: 13400\n",
      "Gradient: [-13.2221  11.3132 -35.9212  33.4653  99.8222]\n",
      "Weights: [-4.6258  0.0796 -0.7675  0.0716  0.1276]\n",
      "MSE loss: 100.8263\n",
      "Iteration: 13500\n",
      "Gradient: [  1.4429   1.0519 -28.8292  49.3048 196.4681]\n",
      "Weights: [-4.6211  0.0845 -0.7702  0.071   0.128 ]\n",
      "MSE loss: 100.516\n",
      "Iteration: 13600\n",
      "Gradient: [  4.5373 -13.1672   3.3118  23.947  321.5281]\n",
      "Weights: [-4.6222  0.0957 -0.7749  0.0705  0.1282]\n",
      "MSE loss: 100.2205\n",
      "Iteration: 13700\n",
      "Gradient: [   7.8068   -1.1871  -14.1252   51.8283 -276.177 ]\n",
      "Weights: [-4.6212  0.1006 -0.778   0.0702  0.1284]\n",
      "MSE loss: 100.0415\n",
      "Iteration: 13800\n",
      "Gradient: [  5.7729 -11.2136  36.9805  -5.6965 143.4007]\n",
      "Weights: [-4.6232  0.1006 -0.7814  0.071   0.1287]\n",
      "MSE loss: 99.8119\n",
      "Iteration: 13900\n",
      "Gradient: [ -3.7905  13.1209  -9.3404 100.476  324.6565]\n",
      "Weights: [-4.6432  0.1119 -0.7844  0.071   0.129 ]\n",
      "MSE loss: 99.8213\n",
      "Iteration: 14000\n",
      "Gradient: [ 9.0982 -4.1533 23.2407 96.7267 88.3565]\n",
      "Weights: [-4.6077  0.107  -0.7897  0.0713  0.1292]\n",
      "MSE loss: 99.6174\n",
      "Iteration: 14100\n",
      "Gradient: [  3.1282  10.8926  -1.6539  34.7137 -24.8754]\n",
      "Weights: [-4.5979  0.0955 -0.7907  0.0717  0.1295]\n",
      "MSE loss: 99.5735\n",
      "Iteration: 14200\n",
      "Gradient: [  6.1236 -14.619  -11.8015 124.7162  18.5358]\n",
      "Weights: [-4.6128  0.1034 -0.7937  0.0722  0.1297]\n",
      "MSE loss: 99.2163\n",
      "Iteration: 14300\n",
      "Gradient: [  0.173   11.7729 -31.0975  28.2104  99.0264]\n",
      "Weights: [-4.6326  0.1185 -0.7925  0.0706  0.1295]\n",
      "MSE loss: 99.1624\n",
      "Iteration: 14400\n",
      "Gradient: [-12.2209  -3.7506 -44.581  -17.2005  50.3857]\n",
      "Weights: [-4.6283  0.1125 -0.7921  0.0695  0.1298]\n",
      "MSE loss: 99.3889\n",
      "Iteration: 14500\n",
      "Gradient: [  8.778   -1.0137  -1.7217 -34.1186 -47.597 ]\n",
      "Weights: [-4.6222  0.1138 -0.7948  0.0702  0.1302]\n",
      "MSE loss: 98.919\n",
      "Iteration: 14600\n",
      "Gradient: [  -2.5975   11.9077   -2.8865   10.6889 -299.843 ]\n",
      "Weights: [-4.6389  0.1293 -0.7989  0.0696  0.1305]\n",
      "MSE loss: 98.6949\n",
      "Iteration: 14700\n",
      "Gradient: [ -8.1472  -9.1561  -1.4694 -16.9234  56.0288]\n",
      "Weights: [-4.6361  0.145  -0.8056  0.069   0.1308]\n",
      "MSE loss: 98.1999\n",
      "Iteration: 14800\n",
      "Gradient: [ -1.1872 -11.8318 -10.969  -65.0064 129.8236]\n",
      "Weights: [-4.6208  0.1347 -0.8067  0.0693  0.131 ]\n",
      "MSE loss: 98.2242\n",
      "Iteration: 14900\n",
      "Gradient: [  4.5042 -16.4682  12.6745  23.0599 -27.232 ]\n",
      "Weights: [-4.6278  0.1351 -0.8079  0.0696  0.1311]\n",
      "MSE loss: 98.1368\n",
      "Iteration: 15000\n",
      "Gradient: [  5.3249  -7.943   41.5849 -40.5343 129.3282]\n",
      "Weights: [-4.6277  0.1432 -0.8137  0.071   0.1313]\n",
      "MSE loss: 97.995\n",
      "Iteration: 15100\n",
      "Gradient: [   1.0297   -6.329    32.6135  -95.0094 -220.5553]\n",
      "Weights: [-4.6498  0.1589 -0.8185  0.0709  0.1313]\n",
      "MSE loss: 97.7799\n",
      "Iteration: 15200\n",
      "Gradient: [  -2.1088    7.4458   -9.2162 -134.3497  131.9722]\n",
      "Weights: [-4.6199  0.1558 -0.8223  0.0707  0.1315]\n",
      "MSE loss: 97.6378\n",
      "Iteration: 15300\n",
      "Gradient: [  7.4808  -6.1394 -35.9543 -57.192  -36.5817]\n",
      "Weights: [-4.6346  0.1659 -0.8259  0.0705  0.1319]\n",
      "MSE loss: 97.2248\n",
      "Iteration: 15400\n",
      "Gradient: [ -7.7165 -10.9903  31.128  113.2265 317.1394]\n",
      "Weights: [-4.6406  0.1741 -0.8282  0.0697  0.1322]\n",
      "MSE loss: 97.0154\n",
      "Iteration: 15500\n",
      "Gradient: [  3.1071  -9.9825  28.5623 119.1992 -30.8788]\n",
      "Weights: [-4.6431  0.1751 -0.829   0.0703  0.1322]\n",
      "MSE loss: 97.0206\n",
      "Iteration: 15600\n",
      "Gradient: [ -8.8447   0.3214  65.9003  25.9501 115.18  ]\n",
      "Weights: [-4.6619  0.1883 -0.8321  0.0699  0.1324]\n",
      "MSE loss: 96.9669\n",
      "Iteration: 15700\n",
      "Gradient: [ -0.8071   7.5131  -1.2533  61.9136 128.975 ]\n",
      "Weights: [-4.6467  0.1926 -0.8336  0.0689  0.1324]\n",
      "MSE loss: 96.7153\n",
      "Iteration: 15800\n",
      "Gradient: [ 11.2472   8.1169  -9.3277  20.0959 138.4887]\n",
      "Weights: [-4.6496  0.1953 -0.8372  0.0689  0.1326]\n",
      "MSE loss: 96.5282\n",
      "Iteration: 15900\n",
      "Gradient: [  -2.5021   -4.2772  -18.1174  -89.9906 -160.6953]\n",
      "Weights: [-4.6725  0.2    -0.8364  0.0689  0.1328]\n",
      "MSE loss: 96.8928\n",
      "Iteration: 16000\n",
      "Gradient: [  -3.4415   -6.6986   19.8975   48.3842 -222.6318]\n",
      "Weights: [-4.6522  0.2068 -0.8417  0.0683  0.1329]\n",
      "MSE loss: 96.2792\n",
      "Iteration: 16100\n",
      "Gradient: [   1.8901    0.4032   -6.8669 -108.0883  -82.7765]\n",
      "Weights: [-4.6635  0.2044 -0.8416  0.0686  0.1331]\n",
      "MSE loss: 96.4068\n",
      "Iteration: 16200\n",
      "Gradient: [  -2.7081   -6.4604   15.6296 -115.4627  -25.0658]\n",
      "Weights: [-4.6535  0.2083 -0.8444  0.0692  0.133 ]\n",
      "MSE loss: 96.1457\n",
      "Iteration: 16300\n",
      "Gradient: [  -2.0188    5.2851    1.2766  -43.8212 -283.5625]\n",
      "Weights: [-4.6496  0.2087 -0.8484  0.0699  0.1333]\n",
      "MSE loss: 95.9913\n",
      "Iteration: 16400\n",
      "Gradient: [ -6.053   -3.0172  32.8425 -34.83    16.256 ]\n",
      "Weights: [-4.6589  0.2145 -0.8503  0.0691  0.1333]\n",
      "MSE loss: 96.1336\n",
      "Iteration: 16500\n",
      "Gradient: [ 15.0868  13.3166 -22.352   58.9689 313.3251]\n",
      "Weights: [-4.6385  0.2134 -0.8506  0.0694  0.1336]\n",
      "MSE loss: 96.2626\n",
      "Iteration: 16600\n",
      "Gradient: [ 10.5745  14.183   27.3601  -7.5187 355.9552]\n",
      "Weights: [-4.6349  0.2196 -0.855   0.0691  0.1339]\n",
      "MSE loss: 96.2137\n",
      "Iteration: 16700\n",
      "Gradient: [ -2.1313  -2.2074 -22.2557  10.9399 -28.3827]\n",
      "Weights: [-4.6477  0.2191 -0.8573  0.0698  0.1341]\n",
      "MSE loss: 95.585\n",
      "Iteration: 16800\n",
      "Gradient: [   6.3151    8.04    -42.7104  -38.6954 -156.422 ]\n",
      "Weights: [-4.658   0.2263 -0.8585  0.0692  0.1341]\n",
      "MSE loss: 95.4672\n",
      "Iteration: 16900\n",
      "Gradient: [ -16.8023    9.2849  -34.1749   15.8349 -357.9905]\n",
      "Weights: [-4.6741  0.2265 -0.8623  0.0698  0.1343]\n",
      "MSE loss: 96.3194\n",
      "Iteration: 17000\n",
      "Gradient: [  -6.8204    1.5107   47.044   -40.3872 -159.4734]\n",
      "Weights: [-4.6776  0.2393 -0.863   0.0697  0.1344]\n",
      "MSE loss: 95.433\n",
      "Iteration: 17100\n",
      "Gradient: [ -9.9577  -2.7886 -18.5854  23.377  107.9934]\n",
      "Weights: [-4.6682  0.2447 -0.867   0.069   0.1346]\n",
      "MSE loss: 95.0415\n",
      "Iteration: 17200\n",
      "Gradient: [  1.6799  -6.3982  11.6514   9.7296 -35.2888]\n",
      "Weights: [-4.6638  0.255  -0.8701  0.0699  0.1343]\n",
      "MSE loss: 95.0728\n",
      "Iteration: 17300\n",
      "Gradient: [  8.0364 -22.3598  -9.505   14.6439   3.1517]\n",
      "Weights: [-4.6613  0.2543 -0.8737  0.0694  0.1347]\n",
      "MSE loss: 94.8159\n",
      "Iteration: 17400\n",
      "Gradient: [ 5.00000e-02 -1.09219e+01  4.07610e+00 -2.55913e+01 -4.20988e+02]\n",
      "Weights: [-4.6625  0.2604 -0.8747  0.0684  0.1349]\n",
      "MSE loss: 94.7478\n",
      "Iteration: 17500\n",
      "Gradient: [-3.2463 -3.7324 29.1197 55.6155 30.756 ]\n",
      "Weights: [-4.6855  0.2752 -0.8774  0.0685  0.135 ]\n",
      "MSE loss: 94.5649\n",
      "Iteration: 17600\n",
      "Gradient: [  -3.8123   -5.3554   30.7971    5.9737 -183.1188]\n",
      "Weights: [-4.6804  0.2784 -0.8834  0.0681  0.1355]\n",
      "MSE loss: 94.3628\n",
      "Iteration: 17700\n",
      "Gradient: [-6.190000e-02 -1.866690e+01  3.112660e+01  3.775270e+01  1.392945e+02]\n",
      "Weights: [-4.6918  0.2896 -0.8856  0.0675  0.1356]\n",
      "MSE loss: 94.3005\n",
      "Iteration: 17800\n",
      "Gradient: [   1.8955   -7.3883   10.3183   33.4119 -338.1275]\n",
      "Weights: [-4.6863  0.287  -0.8873  0.0673  0.1359]\n",
      "MSE loss: 94.2135\n",
      "Iteration: 17900\n",
      "Gradient: [  2.6056   4.9183  12.0192  31.6534 241.0712]\n",
      "Weights: [-4.6775  0.2894 -0.8906  0.0678  0.1365]\n",
      "MSE loss: 93.9248\n",
      "Iteration: 18000\n",
      "Gradient: [  1.6261  -2.1085  13.1844 -84.4749 -19.3812]\n",
      "Weights: [-4.6751  0.2891 -0.8917  0.0672  0.1365]\n",
      "MSE loss: 93.794\n",
      "Iteration: 18100\n",
      "Gradient: [ -0.4174   9.243  -10.1283  35.0515  59.0635]\n",
      "Weights: [-4.6905  0.304  -0.8948  0.0668  0.1367]\n",
      "MSE loss: 93.6361\n",
      "Iteration: 18200\n",
      "Gradient: [ -3.8054  19.7301 -36.3821 -64.6387 244.3143]\n",
      "Weights: [-4.6922  0.3105 -0.8964  0.0668  0.1366]\n",
      "MSE loss: 93.5916\n",
      "Iteration: 18300\n",
      "Gradient: [   1.3993   18.2323   18.7822  -34.0589 -170.5269]\n",
      "Weights: [-4.6811  0.3054 -0.8988  0.0666  0.1368]\n",
      "MSE loss: 93.5973\n",
      "Iteration: 18400\n",
      "Gradient: [ -7.1264  -3.9905   4.1711  48.3311 -24.561 ]\n",
      "Weights: [-4.6866  0.3144 -0.9024  0.0663  0.1372]\n",
      "MSE loss: 93.2978\n",
      "Iteration: 18500\n",
      "Gradient: [ -4.8983   3.8113  47.779   40.2582 215.2319]\n",
      "Weights: [-4.6824  0.3224 -0.909   0.0665  0.1377]\n",
      "MSE loss: 93.0886\n",
      "Iteration: 18600\n",
      "Gradient: [ -4.9781  -6.2795 -23.6548 -82.0734 -59.568 ]\n",
      "Weights: [-4.6915  0.3236 -0.9116  0.067   0.1378]\n",
      "MSE loss: 92.9594\n",
      "Iteration: 18700\n",
      "Gradient: [  3.6026 -19.8898  16.3043  -6.684  368.605 ]\n",
      "Weights: [-4.696   0.3391 -0.9143  0.0677  0.1377]\n",
      "MSE loss: 93.1811\n",
      "Iteration: 18800\n",
      "Gradient: [  4.5752  12.6258  47.7117  26.4663 -84.107 ]\n",
      "Weights: [-4.7     0.3454 -0.9158  0.0666  0.1378]\n",
      "MSE loss: 92.8517\n",
      "Iteration: 18900\n",
      "Gradient: [-0.8537  3.1217 -4.3754 11.1377 40.5061]\n",
      "Weights: [-4.7066  0.3424 -0.9179  0.0667  0.1382]\n",
      "MSE loss: 92.7336\n",
      "Iteration: 19000\n",
      "Gradient: [  1.8885 -19.0743   8.9111  58.5925 124.3134]\n",
      "Weights: [-4.6831  0.3369 -0.9211  0.0666  0.1385]\n",
      "MSE loss: 92.6167\n",
      "Iteration: 19100\n",
      "Gradient: [   2.6296   -4.5834  -24.6822   -9.511  -336.1899]\n",
      "Weights: [-4.7136  0.354  -0.9254  0.0668  0.1388]\n",
      "MSE loss: 92.5384\n",
      "Iteration: 19200\n",
      "Gradient: [  6.3132   9.5072   6.1601  85.8954 114.2658]\n",
      "Weights: [-4.6957  0.3545 -0.9298  0.0662  0.139 ]\n",
      "MSE loss: 92.2722\n",
      "Iteration: 19300\n",
      "Gradient: [  -2.0385   -8.6423   71.0585 -112.5888   95.3134]\n",
      "Weights: [-4.696   0.3646 -0.9327  0.0658  0.1393]\n",
      "MSE loss: 92.1376\n",
      "Iteration: 19400\n",
      "Gradient: [ -13.2484  -12.4768  -33.7083  -88.1155 -181.7568]\n",
      "Weights: [-4.7001  0.3601 -0.9345  0.0655  0.1396]\n",
      "MSE loss: 92.1788\n",
      "Iteration: 19500\n",
      "Gradient: [  -1.2255   -5.8525  -49.9758   24.212  -186.8877]\n",
      "Weights: [-4.704   0.3641 -0.9353  0.0652  0.1399]\n",
      "MSE loss: 92.0029\n",
      "Iteration: 19600\n",
      "Gradient: [  -1.7723    0.5383  -39.373   -24.3604 -102.7378]\n",
      "Weights: [-4.7025  0.3696 -0.9394  0.0656  0.1401]\n",
      "MSE loss: 91.8065\n",
      "Iteration: 19700\n",
      "Gradient: [  1.7343 -12.3981 -11.2756 -18.7446 186.2889]\n",
      "Weights: [-4.7111  0.3831 -0.9445  0.0652  0.1404]\n",
      "MSE loss: 91.6292\n",
      "Iteration: 19800\n",
      "Gradient: [ -9.0541   3.5232  -2.2186 -65.1381 121.3161]\n",
      "Weights: [-4.7146  0.3878 -0.9454  0.0646  0.1406]\n",
      "MSE loss: 91.5606\n",
      "Iteration: 19900\n",
      "Gradient: [  7.7864  -7.051  -36.3815 -56.0827 -30.1149]\n",
      "Weights: [-4.7128  0.4005 -0.9509  0.0644  0.1406]\n",
      "MSE loss: 91.3869\n",
      "Iteration: 20000\n",
      "Gradient: [   8.2423  -21.6579   -0.4426   30.8305 -247.4973]\n",
      "Weights: [-4.7176  0.395  -0.9517  0.0653  0.1408]\n",
      "MSE loss: 91.4352\n",
      "Iteration: 20100\n",
      "Gradient: [   4.648    -2.6584   -3.683    71.0603 -190.7221]\n",
      "Weights: [-4.7209  0.4047 -0.9533  0.0653  0.1407]\n",
      "MSE loss: 91.3236\n",
      "Iteration: 20200\n",
      "Gradient: [ -2.4499   4.135   33.273  105.2271 -25.4402]\n",
      "Weights: [-4.7055  0.4059 -0.9575  0.0652  0.141 ]\n",
      "MSE loss: 91.2813\n",
      "Iteration: 20300\n",
      "Gradient: [  8.1961   6.7094 -17.9831  81.5588 283.1703]\n",
      "Weights: [-4.7116  0.4077 -0.958   0.065   0.1413]\n",
      "MSE loss: 91.2303\n",
      "Iteration: 20400\n",
      "Gradient: [   1.3697    6.4949  -59.4794    3.9668 -200.8319]\n",
      "Weights: [-4.7251  0.4209 -0.9597  0.0641  0.1413]\n",
      "MSE loss: 91.0732\n",
      "Iteration: 20500\n",
      "Gradient: [   1.4701    3.8187  -33.3947 -100.4543   87.3209]\n",
      "Weights: [-4.7103  0.4185 -0.9632  0.064   0.1414]\n",
      "MSE loss: 91.1279\n",
      "Iteration: 20600\n",
      "Gradient: [ 11.4265  16.3369   4.373  107.072  404.0696]\n",
      "Weights: [-4.7091  0.4327 -0.9649  0.064   0.1415]\n",
      "MSE loss: 91.8479\n",
      "Iteration: 20700\n",
      "Gradient: [ -1.4828  -5.3935   6.0204  51.9486 140.1814]\n",
      "Weights: [-4.7362  0.448  -0.97    0.0646  0.1417]\n",
      "MSE loss: 91.0671\n",
      "Iteration: 20800\n",
      "Gradient: [  -4.014     5.1308   22.262    76.4738 -151.3099]\n",
      "Weights: [-4.7351  0.4444 -0.9723  0.0643  0.1419]\n",
      "MSE loss: 90.706\n",
      "Iteration: 20900\n",
      "Gradient: [ -1.9567   0.1596 -17.7245 -30.7695 -64.2568]\n",
      "Weights: [-4.7265  0.4413 -0.9738  0.0639  0.142 ]\n",
      "MSE loss: 90.6988\n",
      "Iteration: 21000\n",
      "Gradient: [   2.1889   -5.9004  -52.9001 -114.1486   55.1077]\n",
      "Weights: [-4.7217  0.4294 -0.9724  0.0635  0.1425]\n",
      "MSE loss: 90.7865\n",
      "Iteration: 21100\n",
      "Gradient: [   1.377     3.8245    7.2963   -9.6698 -125.328 ]\n",
      "Weights: [-4.7256  0.4371 -0.9732  0.0634  0.1427]\n",
      "MSE loss: 90.597\n",
      "Iteration: 21200\n",
      "Gradient: [-1.70260e+01 -6.50220e+00 -1.26000e-02 -2.08251e+01  1.26189e+02]\n",
      "Weights: [-4.7284  0.4454 -0.9754  0.0626  0.1426]\n",
      "MSE loss: 90.5516\n",
      "Iteration: 21300\n",
      "Gradient: [   0.9767  -21.1977  -32.8119   -4.924  -127.2776]\n",
      "Weights: [-4.7185  0.4397 -0.9777  0.0634  0.143 ]\n",
      "MSE loss: 90.4961\n",
      "Iteration: 21400\n",
      "Gradient: [ -4.5815 -12.2128 -44.6479 -68.7711 113.3133]\n",
      "Weights: [-4.724   0.4399 -0.9812  0.0637  0.1431]\n",
      "MSE loss: 90.5805\n",
      "Iteration: 21500\n",
      "Gradient: [-5.1523  2.4962 31.9853 -9.4278 89.9291]\n",
      "Weights: [-4.7217  0.44   -0.9783  0.0632  0.1431]\n",
      "MSE loss: 90.4425\n",
      "Iteration: 21600\n",
      "Gradient: [ -0.3734  -5.9641 -14.4915 -72.5265  34.0312]\n",
      "Weights: [-4.724   0.4505 -0.9809  0.0636  0.143 ]\n",
      "MSE loss: 90.4841\n",
      "Iteration: 21700\n",
      "Gradient: [ 11.2483  20.7956  -1.0721 -79.8381 -62.2331]\n",
      "Weights: [-4.7207  0.4461 -0.9812  0.063   0.143 ]\n",
      "MSE loss: 90.4839\n",
      "Iteration: 21800\n",
      "Gradient: [ -5.1816  -5.4686  -3.1683 -29.1669  53.9006]\n",
      "Weights: [-4.7355  0.4517 -0.9835  0.0642  0.1431]\n",
      "MSE loss: 90.394\n",
      "Iteration: 21900\n",
      "Gradient: [ -0.1942 -19.5151  -1.902  -29.0641  38.5798]\n",
      "Weights: [-4.7247  0.4508 -0.9852  0.0641  0.143 ]\n",
      "MSE loss: 90.3815\n",
      "Iteration: 22000\n",
      "Gradient: [  -7.417    12.8178   -3.3359    8.594  -300.5648]\n",
      "Weights: [-4.7367  0.4594 -0.9849  0.0636  0.143 ]\n",
      "MSE loss: 90.3202\n",
      "Iteration: 22100\n",
      "Gradient: [  -3.4105  -15.2903  -35.3444  -34.273  -496.992 ]\n",
      "Weights: [-4.7368  0.4579 -0.9864  0.0645  0.1428]\n",
      "MSE loss: 90.4818\n",
      "Iteration: 22200\n",
      "Gradient: [   1.6849   -5.4758   -8.5204  -68.2182 -178.3949]\n",
      "Weights: [-4.7321  0.4641 -0.9878  0.0637  0.143 ]\n",
      "MSE loss: 90.2629\n",
      "Iteration: 22300\n",
      "Gradient: [   5.7758  -13.9738  -67.0081  -46.1535 -207.1929]\n",
      "Weights: [-4.7358  0.4645 -0.9938  0.0645  0.1434]\n",
      "MSE loss: 90.3736\n",
      "Iteration: 22400\n",
      "Gradient: [   2.1066   -1.8754   -8.1633 -126.666   -81.7318]\n",
      "Weights: [-4.7341  0.4684 -0.9943  0.0645  0.1436]\n",
      "MSE loss: 90.0417\n",
      "Iteration: 22500\n",
      "Gradient: [  7.7357  -9.6486  24.1933  42.6672 210.3519]\n",
      "Weights: [-4.7405  0.4693 -0.9964  0.0655  0.1436]\n",
      "MSE loss: 90.0898\n",
      "Iteration: 22600\n",
      "Gradient: [  -7.1893   -7.1518   -7.6859 -116.0387 -622.3062]\n",
      "Weights: [-4.7318  0.4641 -0.9968  0.0656  0.1434]\n",
      "MSE loss: 90.2887\n",
      "Iteration: 22700\n",
      "Gradient: [   7.4773    5.5125   33.6661   25.7024 -137.9209]\n",
      "Weights: [-4.7324  0.4715 -0.9993  0.0661  0.1436]\n",
      "MSE loss: 89.977\n",
      "Iteration: 22800\n",
      "Gradient: [ -7.8606   8.3622 -70.1896  38.9069 -25.078 ]\n",
      "Weights: [-4.7464  0.4788 -1.0007  0.0664  0.1433]\n",
      "MSE loss: 90.0896\n",
      "Iteration: 22900\n",
      "Gradient: [  -2.0694    2.6502   -9.4106 -141.4403  -42.3628]\n",
      "Weights: [-4.7433  0.4829 -1.0025  0.0661  0.1433]\n",
      "MSE loss: 90.0012\n",
      "Iteration: 23000\n",
      "Gradient: [ -5.5477   5.9442   2.2248 -43.6188 -40.5114]\n",
      "Weights: [-4.7386  0.4946 -1.0045  0.0653  0.1435]\n",
      "MSE loss: 89.9397\n",
      "Iteration: 23100\n",
      "Gradient: [  2.0072 -27.5609  16.5395 -80.6533 361.6258]\n",
      "Weights: [-4.7441  0.4955 -1.0057  0.066   0.1437]\n",
      "MSE loss: 89.981\n",
      "Iteration: 23200\n",
      "Gradient: [ 8.8086  5.9278 20.1863 -4.1512 31.7912]\n",
      "Weights: [-4.7223  0.4863 -1.0064  0.0663  0.1437]\n",
      "MSE loss: 90.2389\n",
      "Iteration: 23300\n",
      "Gradient: [  7.0485   0.1537   1.1047 -79.2793  66.4202]\n",
      "Weights: [-4.7361  0.491  -1.0085  0.0666  0.144 ]\n",
      "MSE loss: 89.9442\n",
      "Iteration: 23400\n",
      "Gradient: [ -0.2585 -12.1542 -28.4156  73.4717 258.1883]\n",
      "Weights: [-4.7326  0.4944 -1.0105  0.066   0.1441]\n",
      "MSE loss: 89.8266\n",
      "Iteration: 23500\n",
      "Gradient: [   5.2384  -18.7402   14.6535  -87.0228 -179.5724]\n",
      "Weights: [-4.7523  0.4982 -1.013   0.0659  0.1441]\n",
      "MSE loss: 89.9711\n",
      "Iteration: 23600\n",
      "Gradient: [-6.49340e+00 -9.52000e-02  1.40587e+01 -8.13894e+01 -1.26588e+02]\n",
      "Weights: [-4.7597  0.5089 -1.015   0.066   0.1444]\n",
      "MSE loss: 89.6874\n",
      "Iteration: 23700\n",
      "Gradient: [   7.4861   -1.6407   26.8171   49.1068 -246.5589]\n",
      "Weights: [-4.7327  0.5012 -1.0139  0.0652  0.1444]\n",
      "MSE loss: 89.7124\n",
      "Iteration: 23800\n",
      "Gradient: [ -3.2906 -15.6719 -12.6685 -93.4636 165.7781]\n",
      "Weights: [-4.7482  0.506  -1.0147  0.0648  0.1446]\n",
      "MSE loss: 89.529\n",
      "Iteration: 23900\n",
      "Gradient: [  6.7659  -5.4278 -19.9636 -36.032    1.4271]\n",
      "Weights: [-4.7418  0.506  -1.016   0.0656  0.1445]\n",
      "MSE loss: 89.5547\n",
      "Iteration: 24000\n",
      "Gradient: [ -16.1824  -12.1927  -45.7182  -50.5343 -125.6704]\n",
      "Weights: [-4.742   0.4977 -1.0164  0.0656  0.1447]\n",
      "MSE loss: 89.6626\n",
      "Iteration: 24100\n",
      "Gradient: [ 5.64000e-02 -6.10040e+00  5.52250e+00 -6.17377e+01 -3.70524e+01]\n",
      "Weights: [-4.7454  0.4994 -1.0164  0.0653  0.1448]\n",
      "MSE loss: 89.6179\n",
      "Iteration: 24200\n",
      "Gradient: [-11.4791  -7.1204  -6.9497 -46.5552 207.1743]\n",
      "Weights: [-4.7588  0.5019 -1.0171  0.066   0.1449]\n",
      "MSE loss: 89.9153\n",
      "Iteration: 24300\n",
      "Gradient: [ -13.5299   -1.1125  -47.3847   27.2321 -385.6077]\n",
      "Weights: [-4.7744  0.5258 -1.0225  0.0653  0.1447]\n",
      "MSE loss: 89.9933\n",
      "Iteration: 24400\n",
      "Gradient: [ 8.189800e+00 -3.254300e+00  4.680000e-02  2.838160e+01 -2.348512e+02]\n",
      "Weights: [-4.7567  0.5299 -1.0227  0.0645  0.1448]\n",
      "MSE loss: 89.4081\n",
      "Iteration: 24500\n",
      "Gradient: [ -10.7765   -1.8438  -14.988   128.7878 -150.7851]\n",
      "Weights: [-4.7585  0.5245 -1.0226  0.0644  0.1449]\n",
      "MSE loss: 89.429\n",
      "Iteration: 24600\n",
      "Gradient: [ 1.2965 14.6614 20.6741 20.1487 97.132 ]\n",
      "Weights: [-4.7617  0.5248 -1.0203  0.065   0.145 ]\n",
      "MSE loss: 89.7047\n",
      "Iteration: 24700\n",
      "Gradient: [ -3.0781   3.119    8.2479  38.1974 -53.5305]\n",
      "Weights: [-4.7366  0.5105 -1.0224  0.065   0.1451]\n",
      "MSE loss: 89.4377\n",
      "Iteration: 24800\n",
      "Gradient: [   3.7149    5.1158   -9.6437  -15.234  -291.5591]\n",
      "Weights: [-4.7317  0.5065 -1.0229  0.0654  0.145 ]\n",
      "MSE loss: 89.5488\n",
      "Iteration: 24900\n",
      "Gradient: [  7.0716  -4.0318  42.5805 -89.2599  -8.8932]\n",
      "Weights: [-4.7429  0.5071 -1.0205  0.0655  0.1451]\n",
      "MSE loss: 89.4308\n",
      "Iteration: 25000\n",
      "Gradient: [ -4.3755  -6.6517 -44.3571 -89.0414  98.7457]\n",
      "Weights: [-4.7588  0.5202 -1.024   0.065   0.1453]\n",
      "MSE loss: 89.415\n",
      "Iteration: 25100\n",
      "Gradient: [  -0.6687    5.5858   -3.1      44.295  -211.5996]\n",
      "Weights: [-4.752   0.5285 -1.0252  0.065   0.1452]\n",
      "MSE loss: 89.4389\n",
      "Iteration: 25200\n",
      "Gradient: [ -9.1929  -1.2655 -20.1403 -28.0101 -97.4829]\n",
      "Weights: [-4.7374  0.5223 -1.0267  0.0637  0.1452]\n",
      "MSE loss: 89.5932\n",
      "Iteration: 25300\n",
      "Gradient: [-1.235000e-01  6.106900e+00 -3.381760e+01  1.480580e+01 -3.476242e+02]\n",
      "Weights: [-4.7569  0.5302 -1.0269  0.0647  0.1454]\n",
      "MSE loss: 89.2556\n",
      "Iteration: 25400\n",
      "Gradient: [  -2.4998    3.8314   -8.1665   33.0894 -145.4949]\n",
      "Weights: [-4.7489  0.5252 -1.0261  0.0643  0.1456]\n",
      "MSE loss: 89.3178\n",
      "Iteration: 25500\n",
      "Gradient: [ 11.7201   0.7521  22.5578 -26.5852 218.7239]\n",
      "Weights: [-4.7516  0.5308 -1.0285  0.0635  0.1458]\n",
      "MSE loss: 89.1826\n",
      "Iteration: 25600\n",
      "Gradient: [  8.065   22.2783  25.0635 132.1495 160.2003]\n",
      "Weights: [-4.7348  0.52   -1.0252  0.0638  0.1459]\n",
      "MSE loss: 89.6871\n",
      "Iteration: 25700\n",
      "Gradient: [ -10.1101   -4.0178   18.4651   67.4501 -296.3916]\n",
      "Weights: [-4.7508  0.5257 -1.0276  0.0636  0.1458]\n",
      "MSE loss: 89.2238\n",
      "Iteration: 25800\n",
      "Gradient: [ -5.5756  31.7819  26.3711 134.2888 -92.1801]\n",
      "Weights: [-4.7527  0.5269 -1.0273  0.0635  0.1463]\n",
      "MSE loss: 89.4099\n",
      "Iteration: 25900\n",
      "Gradient: [  6.4426  -5.2958 -27.8929  40.1819  78.125 ]\n",
      "Weights: [-4.7314  0.5213 -1.027   0.0634  0.1461]\n",
      "MSE loss: 89.7466\n",
      "Iteration: 26000\n",
      "Gradient: [ -3.8448 -21.9999   8.8736 -41.7734 158.8977]\n",
      "Weights: [-4.7577  0.5323 -1.0298  0.0636  0.146 ]\n",
      "MSE loss: 89.1674\n",
      "Iteration: 26100\n",
      "Gradient: [   7.1357  -10.0106   22.5199  -25.1481 -396.5471]\n",
      "Weights: [-4.7475  0.5263 -1.0296  0.0633  0.146 ]\n",
      "MSE loss: 89.2417\n",
      "Iteration: 26200\n",
      "Gradient: [ 8.6543  7.849  23.0161  3.4136 29.1329]\n",
      "Weights: [-4.7406  0.528  -1.0323  0.0639  0.1458]\n",
      "MSE loss: 89.3103\n",
      "Iteration: 26300\n",
      "Gradient: [  2.1073  -0.8371 -13.9819 -68.7186 -37.0109]\n",
      "Weights: [-4.7493  0.5273 -1.0332  0.0639  0.1462]\n",
      "MSE loss: 89.2997\n",
      "Iteration: 26400\n",
      "Gradient: [-10.5984   7.3191   4.5338  -0.6577 136.3129]\n",
      "Weights: [-4.7496  0.5181 -1.0323  0.0646  0.1463]\n",
      "MSE loss: 89.5436\n",
      "Iteration: 26500\n",
      "Gradient: [  -1.8571    5.8757    5.9575    8.3755 -126.1737]\n",
      "Weights: [-4.741   0.517  -1.0307  0.0649  0.1462]\n",
      "MSE loss: 89.2455\n",
      "Iteration: 26600\n",
      "Gradient: [  3.6677   6.5163  10.5958 -27.2068  74.8051]\n",
      "Weights: [-4.7414  0.5234 -1.0317  0.065   0.1462]\n",
      "MSE loss: 89.3175\n",
      "Iteration: 26700\n",
      "Gradient: [  0.6298  -4.5948   7.4646 -97.6657 -63.1981]\n",
      "Weights: [-4.7529  0.5218 -1.0324  0.0648  0.1461]\n",
      "MSE loss: 89.4768\n",
      "Iteration: 26800\n",
      "Gradient: [ -2.6871  18.9149   0.8555 -70.8638 -81.335 ]\n",
      "Weights: [-4.7338  0.5292 -1.0381  0.0649  0.1463]\n",
      "MSE loss: 89.2215\n",
      "Iteration: 26900\n",
      "Gradient: [ -4.9819   5.8645  19.9018 -32.4833  21.9686]\n",
      "Weights: [-4.7529  0.5323 -1.0375  0.066   0.1464]\n",
      "MSE loss: 89.2034\n",
      "Iteration: 27000\n",
      "Gradient: [-13.0876   2.2665  19.6106   6.603    5.7462]\n",
      "Weights: [-4.7564  0.5404 -1.0404  0.0655  0.1461]\n",
      "MSE loss: 89.0958\n",
      "Iteration: 27100\n",
      "Gradient: [ -5.7561  -3.1426  -1.488  -40.8861 136.8838]\n",
      "Weights: [-4.7637  0.5427 -1.0407  0.0657  0.1463]\n",
      "MSE loss: 89.094\n",
      "Iteration: 27200\n",
      "Gradient: [  10.2531  -14.0537   25.6946   14.9015 -137.0933]\n",
      "Weights: [-4.7374  0.5425 -1.0414  0.0647  0.1463]\n",
      "MSE loss: 89.2508\n",
      "Iteration: 27300\n",
      "Gradient: [ -2.8766   4.8931 -31.6247 -26.3768  25.1971]\n",
      "Weights: [-4.7643  0.5504 -1.0417  0.0653  0.1462]\n",
      "MSE loss: 88.973\n",
      "Iteration: 27400\n",
      "Gradient: [ 6.9146 11.5784 29.2984 -0.9245 -4.8122]\n",
      "Weights: [-4.7578  0.5561 -1.0439  0.0652  0.1462]\n",
      "MSE loss: 88.967\n",
      "Iteration: 27500\n",
      "Gradient: [  8.6058  -9.3819   8.0491   5.4245 177.1353]\n",
      "Weights: [-4.7387  0.5607 -1.0476  0.0645  0.1463]\n",
      "MSE loss: 89.52\n",
      "Iteration: 27600\n",
      "Gradient: [  -1.0622   -9.2243  -48.6512    1.9118 -385.6828]\n",
      "Weights: [-4.7608  0.5747 -1.0515  0.0641  0.1465]\n",
      "MSE loss: 88.9101\n",
      "Iteration: 27700\n",
      "Gradient: [ -3.0204  -2.1485 -40.6776 -12.198  -52.7062]\n",
      "Weights: [-4.7863  0.5794 -1.0515  0.0645  0.1467]\n",
      "MSE loss: 88.948\n",
      "Iteration: 27800\n",
      "Gradient: [  -4.8786    0.6473  -13.2175   -0.6784 -113.406 ]\n",
      "Weights: [-4.7731  0.5776 -1.0542  0.0653  0.1467]\n",
      "MSE loss: 88.7235\n",
      "Iteration: 27900\n",
      "Gradient: [   8.8104    2.1087  -13.811   -27.6534 -182.505 ]\n",
      "Weights: [-4.7661  0.5774 -1.0554  0.065   0.1468]\n",
      "MSE loss: 88.7036\n",
      "Iteration: 28000\n",
      "Gradient: [  5.2778  -0.6528   2.4877 -68.5459 -55.0265]\n",
      "Weights: [-4.7668  0.5759 -1.0553  0.0655  0.147 ]\n",
      "MSE loss: 88.784\n",
      "Iteration: 28100\n",
      "Gradient: [-3.3646 16.4278 45.0889 13.2904 52.4232]\n",
      "Weights: [-4.7641  0.5794 -1.057   0.0655  0.1469]\n",
      "MSE loss: 88.7701\n",
      "Iteration: 28200\n",
      "Gradient: [  5.4523   6.5914   2.1296 -20.9887 120.7587]\n",
      "Weights: [-4.7621  0.5797 -1.0616  0.0659  0.1472]\n",
      "MSE loss: 88.6159\n",
      "Iteration: 28300\n",
      "Gradient: [  2.1086  12.3536  24.7715 -38.7252 -73.9795]\n",
      "Weights: [-4.7818  0.5833 -1.061   0.0658  0.1473]\n",
      "MSE loss: 88.7607\n",
      "Iteration: 28400\n",
      "Gradient: [  7.9514 -11.9396 -31.1565   5.7495 187.9727]\n",
      "Weights: [-4.7741  0.59   -1.0631  0.0652  0.1474]\n",
      "MSE loss: 88.5667\n",
      "Iteration: 28500\n",
      "Gradient: [  -2.6616  -16.9558  -26.5105  -43.6855 -366.4542]\n",
      "Weights: [-4.7893  0.6057 -1.0702  0.0657  0.1473]\n",
      "MSE loss: 88.6795\n",
      "Iteration: 28600\n",
      "Gradient: [ 1.6794  1.7453 11.9526 65.6681 59.1829]\n",
      "Weights: [-4.7747  0.6024 -1.0701  0.0659  0.1476]\n",
      "MSE loss: 88.5205\n",
      "Iteration: 28700\n",
      "Gradient: [-4.6726 25.7208 -4.3643 59.0688 28.6789]\n",
      "Weights: [-4.7611  0.591  -1.068   0.066   0.1474]\n",
      "MSE loss: 88.5862\n",
      "Iteration: 28800\n",
      "Gradient: [ -10.7793   -3.5982    2.2999   46.1078 -107.1584]\n",
      "Weights: [-4.7867  0.6022 -1.0678  0.0651  0.1477]\n",
      "MSE loss: 88.5449\n",
      "Iteration: 28900\n",
      "Gradient: [  -0.5322   -4.01     21.5539   59.5398 -271.3377]\n",
      "Weights: [-4.7767  0.6    -1.0702  0.0652  0.1478]\n",
      "MSE loss: 88.4428\n",
      "Iteration: 29000\n",
      "Gradient: [  5.0548  -9.8425   0.6523 -18.266   24.6023]\n",
      "Weights: [-4.7779  0.6071 -1.0727  0.0642  0.1481]\n",
      "MSE loss: 88.3865\n",
      "Iteration: 29100\n",
      "Gradient: [  -0.8218   -1.1767   12.6339  -50.5545 -104.258 ]\n",
      "Weights: [-4.761   0.5927 -1.0736  0.0651  0.1481]\n",
      "MSE loss: 88.6145\n",
      "Iteration: 29200\n",
      "Gradient: [  -0.4353   13.0668  -38.4362   26.2116 -101.869 ]\n",
      "Weights: [-4.7685  0.5955 -1.0715  0.0657  0.1481]\n",
      "MSE loss: 88.4845\n",
      "Iteration: 29300\n",
      "Gradient: [ -12.4429    4.6154  -16.792  -134.2466 -224.4455]\n",
      "Weights: [-4.7739  0.5903 -1.0715  0.0652  0.1483]\n",
      "MSE loss: 88.6374\n",
      "Iteration: 29400\n",
      "Gradient: [  4.5066 -20.6117 -15.9541   8.1676 -99.3109]\n",
      "Weights: [-4.759   0.5886 -1.0761  0.0652  0.1484]\n",
      "MSE loss: 88.8239\n",
      "Iteration: 29500\n",
      "Gradient: [   4.9967    1.9661   -6.6712  -57.2643 -209.2128]\n",
      "Weights: [-4.7719  0.5987 -1.0764  0.0651  0.1485]\n",
      "MSE loss: 88.4708\n",
      "Iteration: 29600\n",
      "Gradient: [ 2.4049 -2.3771  0.3479 53.8374 -4.4192]\n",
      "Weights: [-4.7631  0.5948 -1.0754  0.065   0.1486]\n",
      "MSE loss: 88.4182\n",
      "Iteration: 29700\n",
      "Gradient: [ -2.522  -13.5297  -4.2363 -17.3169 -27.9408]\n",
      "Weights: [-4.7721  0.6011 -1.0763  0.0647  0.1485]\n",
      "MSE loss: 88.479\n",
      "Iteration: 29800\n",
      "Gradient: [ -0.5206  -7.4998  24.2908  21.0714 206.9477]\n",
      "Weights: [-4.7757  0.6102 -1.0764  0.0643  0.1488]\n",
      "MSE loss: 88.4316\n",
      "Iteration: 29900\n",
      "Gradient: [   5.4132   16.8343  -23.4706   34.2814 -383.8346]\n",
      "Weights: [-4.7817  0.6114 -1.0751  0.0635  0.1486]\n",
      "MSE loss: 88.3414\n",
      "Iteration: 30000\n",
      "Gradient: [  3.6384  -7.8443 -11.2992   5.2471 111.6256]\n",
      "Weights: [-4.7751  0.6135 -1.0773  0.0633  0.1486]\n",
      "MSE loss: 88.4017\n",
      "Iteration: 30100\n",
      "Gradient: [   7.9575   -1.0029  -11.3116   26.1328 -151.17  ]\n",
      "Weights: [-4.7869  0.61   -1.076   0.0647  0.1486]\n",
      "MSE loss: 88.4501\n",
      "Iteration: 30200\n",
      "Gradient: [  1.7302  10.6154 -40.1002  95.958  -59.5614]\n",
      "Weights: [-4.7825  0.6164 -1.0777  0.0644  0.1486]\n",
      "MSE loss: 88.3424\n",
      "Iteration: 30300\n",
      "Gradient: [ -8.8845 -10.1915 -26.1352 -19.7256  89.4234]\n",
      "Weights: [-4.7903  0.6168 -1.0779  0.0635  0.1486]\n",
      "MSE loss: 88.5709\n",
      "Iteration: 30400\n",
      "Gradient: [  0.6848   5.102  -17.1594  38.0658  16.3004]\n",
      "Weights: [-4.7764  0.6196 -1.0798  0.0632  0.1487]\n",
      "MSE loss: 88.3278\n",
      "Iteration: 30500\n",
      "Gradient: [-2.0067  3.2144 21.066   0.2355 27.4168]\n",
      "Weights: [-4.7882  0.619  -1.0782  0.0633  0.1491]\n",
      "MSE loss: 88.3563\n",
      "Iteration: 30600\n",
      "Gradient: [  -2.4145   -5.4507   -8.5397   -3.0443 -211.0582]\n",
      "Weights: [-4.7812  0.6117 -1.0774  0.0624  0.1491]\n",
      "MSE loss: 88.4199\n",
      "Iteration: 30700\n",
      "Gradient: [  -3.8979   15.2636  -29.5196   -5.159  -162.097 ]\n",
      "Weights: [-4.7731  0.6177 -1.0809  0.063   0.1494]\n",
      "MSE loss: 88.3712\n",
      "Iteration: 30800\n",
      "Gradient: [  9.8706   3.514   25.0304  90.1037 -94.6828]\n",
      "Weights: [-4.7617  0.6114 -1.0804  0.063   0.1496]\n",
      "MSE loss: 88.7619\n",
      "Iteration: 30900\n",
      "Gradient: [  -7.7936  -11.6289  -17.0634 -108.2353 -223.2669]\n",
      "Weights: [-4.7861  0.6178 -1.0811  0.0621  0.1495]\n",
      "MSE loss: 88.4252\n",
      "Iteration: 31000\n",
      "Gradient: [-3.6688 19.5456 -4.9489  0.8349 19.0223]\n",
      "Weights: [-4.7831  0.6215 -1.0824  0.062   0.1494]\n",
      "MSE loss: 88.4535\n",
      "Iteration: 31100\n",
      "Gradient: [ 15.513    9.1244   8.5082  16.9428 -50.2473]\n",
      "Weights: [-4.7833  0.6327 -1.0848  0.0626  0.1494]\n",
      "MSE loss: 88.2506\n",
      "Iteration: 31200\n",
      "Gradient: [  3.365    4.2232 -10.6618  -6.8472  54.8502]\n",
      "Weights: [-4.7776  0.6266 -1.084   0.0628  0.1492]\n",
      "MSE loss: 88.2404\n",
      "Iteration: 31300\n",
      "Gradient: [  0.3354   8.9291 -12.453   57.7952  80.6346]\n",
      "Weights: [-4.7843  0.6308 -1.0856  0.0637  0.1493]\n",
      "MSE loss: 88.2578\n",
      "Iteration: 31400\n",
      "Gradient: [ -5.4585 -14.0699 -13.4947  43.261  124.5196]\n",
      "Weights: [-4.7807  0.6286 -1.0879  0.0643  0.1492]\n",
      "MSE loss: 88.1698\n",
      "Iteration: 31500\n",
      "Gradient: [ -11.7976   -4.7138   13.2743   29.1231 -161.4862]\n",
      "Weights: [-4.7797  0.6229 -1.0871  0.0646  0.1491]\n",
      "MSE loss: 88.2431\n",
      "Iteration: 31600\n",
      "Gradient: [  -8.1212   -8.6456    5.9106  -24.3548 -126.2148]\n",
      "Weights: [-4.7828  0.6249 -1.0888  0.0648  0.1493]\n",
      "MSE loss: 88.2249\n",
      "Iteration: 31700\n",
      "Gradient: [  9.3732  11.7592  -0.3596  61.0499 121.5131]\n",
      "Weights: [-4.785   0.6343 -1.0872  0.064   0.1493]\n",
      "MSE loss: 88.2877\n",
      "Iteration: 31800\n",
      "Gradient: [  -7.3213  -10.0947   -5.0189 -185.6965  169.7471]\n",
      "Weights: [-4.7862  0.6235 -1.0879  0.0644  0.1492]\n",
      "MSE loss: 88.5639\n",
      "Iteration: 31900\n",
      "Gradient: [ -14.7678   -5.2832   12.8075  -60.9842 -120.0208]\n",
      "Weights: [-4.7991  0.6343 -1.0895  0.065   0.1491]\n",
      "MSE loss: 88.3872\n",
      "Iteration: 32000\n",
      "Gradient: [  7.2351   7.1998  -5.3671   6.811  -70.5421]\n",
      "Weights: [-4.7828  0.6276 -1.0894  0.0648  0.1493]\n",
      "MSE loss: 88.1658\n",
      "Iteration: 32100\n",
      "Gradient: [   7.4434   -1.1023   43.1056    6.8618 -249.1004]\n",
      "Weights: [-4.7843  0.6199 -1.0864  0.0644  0.1493]\n",
      "MSE loss: 88.3586\n",
      "Iteration: 32200\n",
      "Gradient: [  7.5768  10.4859   6.4679  21.7048 -67.219 ]\n",
      "Weights: [-4.779   0.6241 -1.0869  0.0651  0.1492]\n",
      "MSE loss: 88.2381\n",
      "Iteration: 32300\n",
      "Gradient: [  -0.2298    3.6353   39.4925 -108.6255  -73.0446]\n",
      "Weights: [-4.7905  0.6306 -1.0885  0.0647  0.1492]\n",
      "MSE loss: 88.2187\n",
      "Iteration: 32400\n",
      "Gradient: [ -2.3166  -7.6272   9.9294 -26.8399 192.0342]\n",
      "Weights: [-4.7904  0.6345 -1.0885  0.0645  0.149 ]\n",
      "MSE loss: 88.1789\n",
      "Iteration: 32500\n",
      "Gradient: [ -3.5671  15.9842  13.6957  50.32   -13.689 ]\n",
      "Weights: [-4.7735  0.6337 -1.0886  0.0641  0.1491]\n",
      "MSE loss: 88.3815\n",
      "Iteration: 32600\n",
      "Gradient: [ 15.0538   3.9306 -29.759  -49.6665 -18.9277]\n",
      "Weights: [-4.7852  0.627  -1.0876  0.0641  0.1492]\n",
      "MSE loss: 88.2551\n",
      "Iteration: 32700\n",
      "Gradient: [ -5.4053 -27.0844  19.4054 -50.3317 209.474 ]\n",
      "Weights: [-4.7976  0.6357 -1.0879  0.0644  0.1491]\n",
      "MSE loss: 88.2739\n",
      "Iteration: 32800\n",
      "Gradient: [ -0.3468   8.1554  -4.6526  19.3882 123.2917]\n",
      "Weights: [-4.7839  0.6403 -1.0944  0.0655  0.1493]\n",
      "MSE loss: 88.1479\n",
      "Iteration: 32900\n",
      "Gradient: [  -6.6854   -3.319    -0.9786   -6.3271 -219.4316]\n",
      "Weights: [-4.7881  0.6374 -1.0954  0.0659  0.1491]\n",
      "MSE loss: 88.1744\n",
      "Iteration: 33000\n",
      "Gradient: [ 17.8893 -13.8054  23.1641 -37.1195 -26.8507]\n",
      "Weights: [-4.775   0.6488 -1.0993  0.0651  0.1493]\n",
      "MSE loss: 88.3142\n",
      "Iteration: 33100\n",
      "Gradient: [ -12.1557  -23.1722  -58.5217  -80.3369 -251.6658]\n",
      "Weights: [-4.7995  0.6439 -1.0999  0.0656  0.1493]\n",
      "MSE loss: 88.9705\n",
      "Iteration: 33200\n",
      "Gradient: [ -2.8256 -17.7035  -0.5278  -0.8219 -86.6427]\n",
      "Weights: [-4.7849  0.6399 -1.097   0.0648  0.1495]\n",
      "MSE loss: 88.1687\n",
      "Iteration: 33300\n",
      "Gradient: [  1.77   -19.1676 -26.1474 118.9882  69.7325]\n",
      "Weights: [-4.7957  0.6541 -1.0996  0.0648  0.1495]\n",
      "MSE loss: 88.0596\n",
      "Iteration: 33400\n",
      "Gradient: [  6.1896  -9.9765   1.7453 -46.9748 -57.9006]\n",
      "Weights: [-4.7877  0.6526 -1.1013  0.065   0.1496]\n",
      "MSE loss: 88.0266\n",
      "Iteration: 33500\n",
      "Gradient: [  -4.2868  -12.1847  -23.9489  -55.5757 -217.4007]\n",
      "Weights: [-4.7974  0.6503 -1.1028  0.0657  0.1496]\n",
      "MSE loss: 88.3106\n",
      "Iteration: 33600\n",
      "Gradient: [ -0.595  -20.1262 -20.1274  61.0175 -27.3878]\n",
      "Weights: [-4.8046  0.6532 -1.1012  0.0653  0.1497]\n",
      "MSE loss: 88.3214\n",
      "Iteration: 33700\n",
      "Gradient: [   0.2448    6.4522   18.3263   63.4136 -124.7906]\n",
      "Weights: [-4.8014  0.6595 -1.1025  0.0654  0.1496]\n",
      "MSE loss: 88.0501\n",
      "Iteration: 33800\n",
      "Gradient: [ -0.2224   3.4967  44.932   21.9916 136.2931]\n",
      "Weights: [-4.7869  0.6636 -1.1026  0.0652  0.1496]\n",
      "MSE loss: 88.5342\n",
      "Iteration: 33900\n",
      "Gradient: [  3.9519 -22.9825  -2.0199  25.4319  -9.0029]\n",
      "Weights: [-4.7942  0.6643 -1.1036  0.0656  0.1494]\n",
      "MSE loss: 88.0934\n",
      "Iteration: 34000\n",
      "Gradient: [ -20.3409   -1.6046   14.797     1.3293 -259.8462]\n",
      "Weights: [-4.803   0.6668 -1.1064  0.0659  0.1495]\n",
      "MSE loss: 88.0288\n",
      "Iteration: 34100\n",
      "Gradient: [  -1.5673    2.8829   -2.9824 -137.3285 -203.5805]\n",
      "Weights: [-4.8029  0.6651 -1.1081  0.0663  0.1497]\n",
      "MSE loss: 88.033\n",
      "Iteration: 34200\n",
      "Gradient: [-2.150000e-02 -6.471400e+00  2.238940e+01  1.069417e+02  2.606652e+02]\n",
      "Weights: [-4.7992  0.6696 -1.1102  0.0674  0.1499]\n",
      "MSE loss: 88.3234\n",
      "Iteration: 34300\n",
      "Gradient: [  -5.3349   -4.1973  -11.8044   85.9118 -107.597 ]\n",
      "Weights: [-4.7911  0.6593 -1.1107  0.0671  0.1498]\n",
      "MSE loss: 87.9796\n",
      "Iteration: 34400\n",
      "Gradient: [  1.6732   9.1946  51.7606  44.7219 324.9143]\n",
      "Weights: [-4.8002  0.6706 -1.1098  0.0666  0.15  ]\n",
      "MSE loss: 88.1602\n",
      "Iteration: 34500\n",
      "Gradient: [  1.1116   0.3875 -50.64   -53.259  334.4521]\n",
      "Weights: [-4.7983  0.6641 -1.1113  0.0669  0.15  ]\n",
      "MSE loss: 87.9445\n",
      "Iteration: 34600\n",
      "Gradient: [  -2.2016    5.1266  -19.742    19.553  -143.6212]\n",
      "Weights: [-4.7933  0.6668 -1.1115  0.067   0.1498]\n",
      "MSE loss: 87.9335\n",
      "Iteration: 34700\n",
      "Gradient: [ -2.1609  17.4564  31.2826  42.2943 384.6735]\n",
      "Weights: [-4.7808  0.6575 -1.1109  0.0671  0.1499]\n",
      "MSE loss: 87.9948\n",
      "Iteration: 34800\n",
      "Gradient: [ -7.67    -5.3011 -51.939   28.0236  53.1113]\n",
      "Weights: [-4.7933  0.6554 -1.1105  0.0671  0.1502]\n",
      "MSE loss: 88.0097\n",
      "Iteration: 34900\n",
      "Gradient: [  2.8249 -35.3133 -32.2447  28.5496  19.4216]\n",
      "Weights: [-4.7919  0.6703 -1.1143  0.0667  0.1499]\n",
      "MSE loss: 87.9059\n",
      "Iteration: 35000\n",
      "Gradient: [-0.6125  0.4434 -3.9719 84.976  88.0349]\n",
      "Weights: [-4.7971  0.6675 -1.1128  0.0671  0.15  ]\n",
      "MSE loss: 87.9031\n",
      "Iteration: 35100\n",
      "Gradient: [   5.6321    6.0798  -15.4824  -81.5293 -216.5147]\n",
      "Weights: [-4.8049  0.6718 -1.1149  0.0672  0.1498]\n",
      "MSE loss: 88.1386\n",
      "Iteration: 35200\n",
      "Gradient: [  7.1583   7.9772 -13.1041 -52.0422 166.6531]\n",
      "Weights: [-4.795   0.6627 -1.112   0.0679  0.1498]\n",
      "MSE loss: 87.9361\n",
      "Iteration: 35300\n",
      "Gradient: [   4.9944   11.5392  -16.5297   32.8232 -113.8759]\n",
      "Weights: [-4.7837  0.6633 -1.1141  0.0675  0.1499]\n",
      "MSE loss: 87.9457\n",
      "Iteration: 35400\n",
      "Gradient: [ -2.4713  17.1461 -25.3622  59.4247 -46.4008]\n",
      "Weights: [-4.7801  0.662  -1.1153  0.0679  0.1501]\n",
      "MSE loss: 88.0173\n",
      "Iteration: 35500\n",
      "Gradient: [  2.5506   7.8304 -15.2535 -13.7772  28.2759]\n",
      "Weights: [-4.7881  0.6568 -1.1147  0.0672  0.1502]\n",
      "MSE loss: 88.1392\n",
      "Iteration: 35600\n",
      "Gradient: [  1.5913  -8.695  -15.5582   0.2124 143.3857]\n",
      "Weights: [-4.7967  0.664  -1.1163  0.0667  0.1506]\n",
      "MSE loss: 88.0513\n",
      "Iteration: 35700\n",
      "Gradient: [  7.5448  -6.6185 -89.2701 -70.2592 418.8405]\n",
      "Weights: [-4.7968  0.6629 -1.1125  0.0663  0.1504]\n",
      "MSE loss: 87.9688\n",
      "Iteration: 35800\n",
      "Gradient: [ 4.3669 -3.2968  2.4299 56.7444 17.4374]\n",
      "Weights: [-4.7814  0.6751 -1.1157  0.0659  0.1502]\n",
      "MSE loss: 88.2237\n",
      "Iteration: 35900\n",
      "Gradient: [  0.4259  -1.8846 -18.206    2.2561 151.8672]\n",
      "Weights: [-4.789   0.676  -1.1165  0.0663  0.15  ]\n",
      "MSE loss: 87.9644\n",
      "Iteration: 36000\n",
      "Gradient: [ -14.0658    2.7379  -16.4397 -106.6364   38.131 ]\n",
      "Weights: [-4.814   0.6892 -1.1156  0.0651  0.1501]\n",
      "MSE loss: 87.9912\n",
      "Iteration: 36100\n",
      "Gradient: [-10.278   -0.8575  14.0593  92.5701  50.0873]\n",
      "Weights: [-4.8009  0.685  -1.1166  0.0649  0.1504]\n",
      "MSE loss: 87.8928\n",
      "Iteration: 36200\n",
      "Gradient: [  1.6928 -14.3668 -16.1386  -4.9173  34.7864]\n",
      "Weights: [-4.8073  0.6878 -1.1179  0.0656  0.1505]\n",
      "MSE loss: 87.9394\n",
      "Iteration: 36300\n",
      "Gradient: [  -3.9729    9.1094   16.3655  -26.922  -179.3171]\n",
      "Weights: [-4.8015  0.6716 -1.1159  0.0663  0.1506]\n",
      "MSE loss: 87.9321\n",
      "Iteration: 36400\n",
      "Gradient: [   4.8892   -9.6068   12.4561  155.2114 -142.2324]\n",
      "Weights: [-4.8004  0.6792 -1.1175  0.0658  0.1507]\n",
      "MSE loss: 87.9139\n",
      "Iteration: 36500\n",
      "Gradient: [ -9.2528   3.2777  34.1759  70.5728 -35.1067]\n",
      "Weights: [-4.804   0.6826 -1.1183  0.0654  0.1507]\n",
      "MSE loss: 87.8514\n",
      "Iteration: 36600\n",
      "Gradient: [ -12.5825  -31.8638    9.9325  120.9374 -284.8286]\n",
      "Weights: [-4.8071  0.6902 -1.1207  0.0653  0.1506]\n",
      "MSE loss: 87.8591\n",
      "Iteration: 36700\n",
      "Gradient: [ -1.5954   8.5781  40.1471 -69.1785 134.3495]\n",
      "Weights: [-4.7978  0.6864 -1.1214  0.0656  0.1507]\n",
      "MSE loss: 87.8384\n",
      "Iteration: 36800\n",
      "Gradient: [  1.8559 -14.6382   3.313   -2.3406  58.8812]\n",
      "Weights: [-4.8125  0.6929 -1.1244  0.0663  0.1507]\n",
      "MSE loss: 87.8651\n",
      "Iteration: 36900\n",
      "Gradient: [  7.5088  14.3103  24.0604 -31.675  339.7268]\n",
      "Weights: [-4.8033  0.6872 -1.1224  0.0663  0.1511]\n",
      "MSE loss: 88.0974\n",
      "Iteration: 37000\n",
      "Gradient: [  -5.5036   -2.2432    7.0333  -33.3417 -242.7725]\n",
      "Weights: [-4.8163  0.6939 -1.1235  0.0649  0.1511]\n",
      "MSE loss: 87.9727\n",
      "Iteration: 37100\n",
      "Gradient: [ -4.5667  18.5933 -18.5071   1.6219  43.4559]\n",
      "Weights: [-4.7891  0.6917 -1.127   0.0658  0.1511]\n",
      "MSE loss: 88.0131\n",
      "Iteration: 37200\n",
      "Gradient: [  0.7465   4.9913  -5.0543 -56.2424 212.1448]\n",
      "Weights: [-4.7865  0.6846 -1.1274  0.0662  0.1512]\n",
      "MSE loss: 87.88\n",
      "Iteration: 37300\n",
      "Gradient: [ 0.2651 14.9557 52.3289 66.9834 78.7761]\n",
      "Weights: [-4.7988  0.6932 -1.1264  0.0663  0.1512]\n",
      "MSE loss: 88.0561\n",
      "Iteration: 37400\n",
      "Gradient: [   0.6349   -4.3987   -7.1397 -124.6749 -294.5536]\n",
      "Weights: [-4.799   0.6925 -1.1287  0.0661  0.151 ]\n",
      "MSE loss: 87.8339\n",
      "Iteration: 37500\n",
      "Gradient: [  4.1565  -0.4577 -11.2603  51.9905  61.7869]\n",
      "Weights: [-4.8031  0.6887 -1.1257  0.0662  0.1511]\n",
      "MSE loss: 87.7949\n",
      "Iteration: 37600\n",
      "Gradient: [   3.7919   11.6909   32.1095   -5.5472 -110.2644]\n",
      "Weights: [-4.8001  0.69   -1.1267  0.0665  0.151 ]\n",
      "MSE loss: 87.7768\n",
      "Iteration: 37700\n",
      "Gradient: [ -15.3415   10.6034   -9.1723 -163.5732 -175.6139]\n",
      "Weights: [-4.8257  0.6961 -1.1261  0.0659  0.1509]\n",
      "MSE loss: 88.576\n",
      "Iteration: 37800\n",
      "Gradient: [  -1.5689  -10.9555   13.8588 -114.9239  -17.3391]\n",
      "Weights: [-4.8046  0.6932 -1.1245  0.0653  0.1509]\n",
      "MSE loss: 87.8139\n",
      "Iteration: 37900\n",
      "Gradient: [  5.0064  -7.0185  -5.2983 -92.8669 -64.6606]\n",
      "Weights: [-4.8049  0.6958 -1.1279  0.0662  0.151 ]\n",
      "MSE loss: 87.7633\n",
      "Iteration: 38000\n",
      "Gradient: [  3.8874  -8.8191 -16.499  -53.2898 -37.848 ]\n",
      "Weights: [-4.8056  0.7001 -1.128   0.0659  0.1511]\n",
      "MSE loss: 87.8087\n",
      "Iteration: 38100\n",
      "Gradient: [  -9.7016   24.5438  -10.8648  -59.6237 -369.5323]\n",
      "Weights: [-4.8172  0.7096 -1.1322  0.0661  0.1508]\n",
      "MSE loss: 88.0127\n",
      "Iteration: 38200\n",
      "Gradient: [ -4.9395   3.5805   4.71    42.3923 204.8654]\n",
      "Weights: [-4.8163  0.7111 -1.134   0.066   0.1511]\n",
      "MSE loss: 87.8614\n",
      "Iteration: 38300\n",
      "Gradient: [  -7.0548   -1.292   -70.4385  -45.6867 -272.9151]\n",
      "Weights: [-4.8138  0.707  -1.1348  0.0664  0.1512]\n",
      "MSE loss: 87.8926\n",
      "Iteration: 38400\n",
      "Gradient: [  0.6916  -9.202    1.0019 -38.8004 209.0959]\n",
      "Weights: [-4.7923  0.7003 -1.1339  0.0665  0.1513]\n",
      "MSE loss: 87.8546\n",
      "Iteration: 38500\n",
      "Gradient: [  6.8106  17.8834  14.3456 140.7481 214.402 ]\n",
      "Weights: [-4.8035  0.71   -1.1357  0.0661  0.1516]\n",
      "MSE loss: 87.8333\n",
      "Iteration: 38600\n",
      "Gradient: [ 2.4974 -4.9233 38.0872 22.4261 29.4675]\n",
      "Weights: [-4.8122  0.7089 -1.1357  0.0665  0.1515]\n",
      "MSE loss: 87.7273\n",
      "Iteration: 38700\n",
      "Gradient: [  -1.6608    5.3216  -10.7766   31.6752 -139.1004]\n",
      "Weights: [-4.789   0.7043 -1.1366  0.0662  0.1516]\n",
      "MSE loss: 87.9914\n",
      "Iteration: 38800\n",
      "Gradient: [  11.2011  -14.1548  -22.4009 -129.7758   -0.6241]\n",
      "Weights: [-4.806   0.7016 -1.1362  0.0665  0.1516]\n",
      "MSE loss: 87.8487\n",
      "Iteration: 38900\n",
      "Gradient: [   1.0275   -6.8339   18.4104 -146.3567   50.3301]\n",
      "Weights: [-4.7954  0.6958 -1.1364  0.0676  0.1515]\n",
      "MSE loss: 87.7443\n",
      "Iteration: 39000\n",
      "Gradient: [  -5.1172  -10.3169  -22.0258 -179.352   -60.7051]\n",
      "Weights: [-4.805   0.703  -1.1415  0.0684  0.1515]\n",
      "MSE loss: 87.8716\n",
      "Iteration: 39100\n",
      "Gradient: [  -0.7408   -2.6401  -28.0198 -107.6908 -325.5261]\n",
      "Weights: [-4.8181  0.7103 -1.1411  0.0681  0.1513]\n",
      "MSE loss: 88.0489\n",
      "Iteration: 39200\n",
      "Gradient: [  12.9656   14.5434  -13.0442   44.5419 -248.2926]\n",
      "Weights: [-4.8089  0.7187 -1.1412  0.0689  0.151 ]\n",
      "MSE loss: 87.7933\n",
      "Iteration: 39300\n",
      "Gradient: [   8.611    -4.4027   30.7734   33.4249 -109.0888]\n",
      "Weights: [-4.8017  0.7203 -1.1439  0.0686  0.1511]\n",
      "MSE loss: 87.8144\n",
      "Iteration: 39400\n",
      "Gradient: [-2.4298 10.2732 37.7029 36.6871 -3.0043]\n",
      "Weights: [-4.8232  0.7208 -1.1434  0.0686  0.1513]\n",
      "MSE loss: 87.7799\n",
      "Iteration: 39500\n",
      "Gradient: [  -1.5485   16.9733   -7.4348   44.0712 -146.3111]\n",
      "Weights: [-4.8133  0.7256 -1.1447  0.0685  0.1511]\n",
      "MSE loss: 87.6745\n",
      "Iteration: 39600\n",
      "Gradient: [  -4.6267   -2.5718   25.4122   80.3573 -296.364 ]\n",
      "Weights: [-4.8089  0.7214 -1.1432  0.068   0.151 ]\n",
      "MSE loss: 87.7493\n",
      "Iteration: 39700\n",
      "Gradient: [  -3.2053    6.2519  -15.2848    4.6502 -126.9749]\n",
      "Weights: [-4.8056  0.716  -1.1415  0.0678  0.151 ]\n",
      "MSE loss: 87.7638\n",
      "Iteration: 39800\n",
      "Gradient: [  2.2995  -2.0244 -51.9329 -44.975  340.6784]\n",
      "Weights: [-4.8005  0.7099 -1.1409  0.0682  0.1511]\n",
      "MSE loss: 87.7159\n",
      "Iteration: 39900\n",
      "Gradient: [  4.806   10.8721   7.3326 -44.7624  45.7115]\n",
      "Weights: [-4.8136  0.7199 -1.1406  0.0686  0.1511]\n",
      "MSE loss: 87.8586\n",
      "Iteration: 40000\n",
      "Gradient: [  5.5299   8.7128  -2.215  -46.4198  72.1972]\n",
      "Weights: [-4.8112  0.7173 -1.1414  0.0685  0.1511]\n",
      "MSE loss: 87.6852\n",
      "Iteration: 40100\n",
      "Gradient: [  2.5357  -1.943    6.6384  21.2844 -65.8366]\n",
      "Weights: [-4.813   0.7241 -1.1454  0.0685  0.1512]\n",
      "MSE loss: 87.6514\n",
      "Iteration: 40200\n",
      "Gradient: [   1.7086   21.7573   -0.4131   14.4971 -109.5936]\n",
      "Weights: [-4.7996  0.7206 -1.1436  0.0683  0.1512]\n",
      "MSE loss: 87.9356\n",
      "Iteration: 40300\n",
      "Gradient: [  -3.2131    3.4574   12.3664   45.4766 -102.6397]\n",
      "Weights: [-4.7956  0.711  -1.1459  0.0697  0.1513]\n",
      "MSE loss: 87.7329\n",
      "Iteration: 40400\n",
      "Gradient: [  2.5361   6.3491   9.0795 -38.0414 147.4022]\n",
      "Weights: [-4.8167  0.7229 -1.1479  0.07    0.1513]\n",
      "MSE loss: 87.6732\n",
      "Iteration: 40500\n",
      "Gradient: [ -1.7654  14.2171  35.4394 -11.2732  39.2688]\n",
      "Weights: [-4.8068  0.7128 -1.1471  0.0702  0.1514]\n",
      "MSE loss: 87.6557\n",
      "Iteration: 40600\n",
      "Gradient: [  9.1601  -2.3512  -9.3942   2.9362 -14.1558]\n",
      "Weights: [-4.803   0.7077 -1.147   0.0709  0.1514]\n",
      "MSE loss: 87.6873\n",
      "Iteration: 40700\n",
      "Gradient: [  -4.9998  -11.8072  -36.5643 -133.1685 -208.225 ]\n",
      "Weights: [-4.8127  0.7088 -1.1458  0.0702  0.1514]\n",
      "MSE loss: 87.8618\n",
      "Iteration: 40800\n",
      "Gradient: [  9.1868  13.1267 -16.9954 102.2861 130.1304]\n",
      "Weights: [-4.8239  0.7244 -1.1467  0.07    0.1512]\n",
      "MSE loss: 87.7458\n",
      "Iteration: 40900\n",
      "Gradient: [  -2.8189  -21.4705   12.5422   14.0623 -367.727 ]\n",
      "Weights: [-4.8213  0.7206 -1.1462  0.0703  0.151 ]\n",
      "MSE loss: 87.738\n",
      "Iteration: 41000\n",
      "Gradient: [  -0.5886   -6.2682  -22.9909  -10.8104 -287.4102]\n",
      "Weights: [-4.8348  0.7314 -1.1482  0.0704  0.1509]\n",
      "MSE loss: 87.9419\n",
      "Iteration: 41100\n",
      "Gradient: [  2.0367   1.8746  -4.7419 103.1633  17.5163]\n",
      "Weights: [-4.8103  0.7276 -1.1484  0.0698  0.151 ]\n",
      "MSE loss: 87.6642\n",
      "Iteration: 41200\n",
      "Gradient: [  8.1228  -2.9913 -45.8351 -77.3746  60.5185]\n",
      "Weights: [-4.7999  0.7215 -1.1503  0.0708  0.1508]\n",
      "MSE loss: 87.7451\n",
      "Iteration: 41300\n",
      "Gradient: [ -17.798   -23.1749   15.2206   54.3511 -226.1951]\n",
      "Weights: [-4.8117  0.7256 -1.1505  0.0701  0.151 ]\n",
      "MSE loss: 87.7475\n",
      "Iteration: 41400\n",
      "Gradient: [ -10.2824    0.4367   38.5923  -62.1088 -261.328 ]\n",
      "Weights: [-4.8272  0.7308 -1.1503  0.0707  0.1511]\n",
      "MSE loss: 87.7334\n",
      "Iteration: 41500\n",
      "Gradient: [  -2.2138    1.8155  -65.303    12.5446 -187.9099]\n",
      "Weights: [-4.8222  0.7258 -1.1499  0.0702  0.1513]\n",
      "MSE loss: 87.7295\n",
      "Iteration: 41600\n",
      "Gradient: [ -3.9319  -8.8333  29.7179  42.7361 251.4056]\n",
      "Weights: [-4.811   0.727  -1.1485  0.07    0.151 ]\n",
      "MSE loss: 87.6437\n",
      "Iteration: 41700\n",
      "Gradient: [  5.3756   4.061   45.5807  42.5133 202.6926]\n",
      "Weights: [-4.8173  0.728  -1.1472  0.0696  0.1511]\n",
      "MSE loss: 87.6586\n",
      "Iteration: 41800\n",
      "Gradient: [  4.6477  -0.7274  16.3549 -27.425  174.507 ]\n",
      "Weights: [-4.8139  0.7194 -1.1483  0.0701  0.1514]\n",
      "MSE loss: 87.6526\n",
      "Iteration: 41900\n",
      "Gradient: [  -1.3349    7.9651  -11.6704  -11.5557 -244.6475]\n",
      "Weights: [-4.8125  0.7291 -1.1521  0.0701  0.1513]\n",
      "MSE loss: 87.6015\n",
      "Iteration: 42000\n",
      "Gradient: [   3.4148   -0.2642   37.8752  -46.6328 -185.2913]\n",
      "Weights: [-4.8113  0.7298 -1.1527  0.0706  0.1513]\n",
      "MSE loss: 87.6201\n",
      "Iteration: 42100\n",
      "Gradient: [ -11.643   -15.7747  -18.5126   32.1435 -313.04  ]\n",
      "Weights: [-4.8215  0.7278 -1.1546  0.0712  0.1513]\n",
      "MSE loss: 87.8357\n",
      "Iteration: 42200\n",
      "Gradient: [-13.3702 -13.762  -20.0813  16.6741  22.0141]\n",
      "Weights: [-4.8181  0.7358 -1.158   0.0718  0.1513]\n",
      "MSE loss: 87.5624\n",
      "Iteration: 42300\n",
      "Gradient: [  0.4589 -16.1535  10.4223 -71.4946  79.4825]\n",
      "Weights: [-4.8312  0.7373 -1.154   0.0712  0.1513]\n",
      "MSE loss: 87.7659\n",
      "Iteration: 42400\n",
      "Gradient: [  1.3257  11.5642  23.1681  93.5773 128.3288]\n",
      "Weights: [-4.8207  0.7531 -1.1598  0.0709  0.1512]\n",
      "MSE loss: 87.7885\n",
      "Iteration: 42500\n",
      "Gradient: [  -5.6112  -16.1274  -28.6978 -126.2716  -22.6216]\n",
      "Weights: [-4.8273  0.7439 -1.1589  0.0709  0.1511]\n",
      "MSE loss: 87.8614\n",
      "Iteration: 42600\n",
      "Gradient: [ 7.260000e-02  1.915670e+01 -3.435040e+01 -2.975350e+01 -1.857938e+02]\n",
      "Weights: [-4.8185  0.7422 -1.1585  0.0712  0.1512]\n",
      "MSE loss: 87.5699\n",
      "Iteration: 42700\n",
      "Gradient: [ -3.071    8.9151  -8.1748 101.1459 189.7167]\n",
      "Weights: [-4.8173  0.7434 -1.1582  0.0712  0.1513]\n",
      "MSE loss: 87.6426\n",
      "Iteration: 42800\n",
      "Gradient: [ -8.8979  -3.8553 -21.499   87.9341  27.5833]\n",
      "Weights: [-4.8221  0.743  -1.1591  0.0708  0.1514]\n",
      "MSE loss: 87.5861\n",
      "Iteration: 42900\n",
      "Gradient: [  -3.5392   11.5186   39.5307   55.3585 -413.6118]\n",
      "Weights: [-4.8285  0.7444 -1.1554  0.0706  0.1513]\n",
      "MSE loss: 87.6698\n",
      "Iteration: 43000\n",
      "Gradient: [ -11.8149  -14.4978   -0.6737   98.5789 -344.5716]\n",
      "Weights: [-4.8446  0.753  -1.155   0.0708  0.1512]\n",
      "MSE loss: 88.0896\n",
      "Iteration: 43100\n",
      "Gradient: [ -0.7776  11.4896   1.6582 -25.8073 -85.5345]\n",
      "Weights: [-4.8282  0.744  -1.1548  0.0699  0.1513]\n",
      "MSE loss: 87.6447\n",
      "Iteration: 43200\n",
      "Gradient: [  1.6138 -12.9378  46.441  -92.4073 -94.3811]\n",
      "Weights: [-4.8244  0.7417 -1.1551  0.0694  0.1513]\n",
      "MSE loss: 87.7466\n",
      "Iteration: 43300\n",
      "Gradient: [  2.9367  -6.2869  -0.8947  16.6067 151.6465]\n",
      "Weights: [-4.8123  0.7428 -1.1536  0.0687  0.1515]\n",
      "MSE loss: 87.8096\n",
      "Iteration: 43400\n",
      "Gradient: [  3.7246   4.4515   2.0932  81.571  -91.5767]\n",
      "Weights: [-4.8144  0.7484 -1.1579  0.0693  0.1516]\n",
      "MSE loss: 87.7924\n",
      "Iteration: 43500\n",
      "Gradient: [   2.5488    3.5801   50.5562   39.6829 -117.6192]\n",
      "Weights: [-4.8167  0.7432 -1.1554  0.0689  0.1517]\n",
      "MSE loss: 87.6615\n",
      "Iteration: 43600\n",
      "Gradient: [   0.5919    1.6708   21.5309  -25.9068 -281.909 ]\n",
      "Weights: [-4.7959  0.7334 -1.1553  0.0691  0.1517]\n",
      "MSE loss: 87.9805\n",
      "Iteration: 43700\n",
      "Gradient: [ -7.014   -1.7622  14.8058 -69.1562  38.728 ]\n",
      "Weights: [-4.8183  0.7326 -1.1532  0.0695  0.1519]\n",
      "MSE loss: 87.6938\n",
      "Iteration: 43800\n",
      "Gradient: [ 13.4915  20.3629   7.0731  35.233  170.7302]\n",
      "Weights: [-4.8064  0.7266 -1.1511  0.0695  0.1518]\n",
      "MSE loss: 87.8633\n",
      "Iteration: 43900\n",
      "Gradient: [  0.2036   8.3824 -21.2835  55.5363 -34.7159]\n",
      "Weights: [-4.7945  0.7231 -1.1518  0.0691  0.1517]\n",
      "MSE loss: 87.827\n",
      "Iteration: 44000\n",
      "Gradient: [ -0.9003 -29.3096 -16.538  -40.3211 195.887 ]\n",
      "Weights: [-4.8082  0.7206 -1.1521  0.0695  0.1517]\n",
      "MSE loss: 87.7253\n",
      "Iteration: 44100\n",
      "Gradient: [  -3.5813   -6.6968   14.0022    1.7127 -236.4111]\n",
      "Weights: [-4.804   0.7269 -1.1531  0.0699  0.1515]\n",
      "MSE loss: 87.6433\n",
      "Iteration: 44200\n",
      "Gradient: [  0.5119  11.8893 -25.3554 100.0187  34.6253]\n",
      "Weights: [-4.812   0.7367 -1.1564  0.0698  0.1516]\n",
      "MSE loss: 87.5985\n",
      "Iteration: 44300\n",
      "Gradient: [   3.3413   -9.5013   -0.3949   14.0151 -125.2015]\n",
      "Weights: [-4.8084  0.7413 -1.159   0.07    0.1518]\n",
      "MSE loss: 87.7126\n",
      "Iteration: 44400\n",
      "Gradient: [ -7.6002  -3.3234   1.9777  73.9929 162.1321]\n",
      "Weights: [-4.808   0.7326 -1.1561  0.0698  0.1518]\n",
      "MSE loss: 87.6186\n",
      "Iteration: 44500\n",
      "Gradient: [ -0.3433  10.6575   9.2174 -94.5313  20.7852]\n",
      "Weights: [-4.8105  0.7381 -1.1586  0.0696  0.1516]\n",
      "MSE loss: 87.7315\n",
      "Iteration: 44600\n",
      "Gradient: [ -7.0459  -6.075  -53.7465  31.4838  81.344 ]\n",
      "Weights: [-4.8291  0.7426 -1.1584  0.0701  0.1519]\n",
      "MSE loss: 87.691\n",
      "Iteration: 44700\n",
      "Gradient: [  11.0172   -4.9838    5.4155   26.6207 -261.6294]\n",
      "Weights: [-4.8175  0.7431 -1.1599  0.0697  0.1519]\n",
      "MSE loss: 87.568\n",
      "Iteration: 44800\n",
      "Gradient: [ -5.157   18.2644  -0.3155 -13.6265 201.3524]\n",
      "Weights: [-4.8024  0.7398 -1.1601  0.0693  0.1519]\n",
      "MSE loss: 87.7448\n",
      "Iteration: 44900\n",
      "Gradient: [  -6.4078   -1.2373    1.2865 -116.765     2.2869]\n",
      "Weights: [-4.8206  0.7421 -1.1582  0.069   0.1521]\n",
      "MSE loss: 87.5835\n",
      "Iteration: 45000\n",
      "Gradient: [ -2.9653  31.0752  40.0215  35.4651 296.0615]\n",
      "Weights: [-4.81    0.7429 -1.1608  0.07    0.152 ]\n",
      "MSE loss: 87.671\n",
      "Iteration: 45100\n",
      "Gradient: [  6.3969  -3.3026  -8.1848 -31.3132 -66.7505]\n",
      "Weights: [-4.8084  0.7416 -1.1608  0.0695  0.152 ]\n",
      "MSE loss: 87.6189\n",
      "Iteration: 45200\n",
      "Gradient: [ -1.0491  10.8896 -37.0406  36.2483 -85.8824]\n",
      "Weights: [-4.8107  0.7401 -1.1607  0.0694  0.1524]\n",
      "MSE loss: 87.6387\n",
      "Iteration: 45300\n",
      "Gradient: [ -3.3615   0.4641 -29.2977 -14.9304 -57.1984]\n",
      "Weights: [-4.8032  0.7332 -1.1613  0.0696  0.1522]\n",
      "MSE loss: 87.6715\n",
      "Iteration: 45400\n",
      "Gradient: [-10.9152  17.1223 -26.4483  37.805  -68.2062]\n",
      "Weights: [-4.8153  0.7384 -1.1614  0.0701  0.1523]\n",
      "MSE loss: 87.584\n",
      "Iteration: 45500\n",
      "Gradient: [  -9.0883    0.9171   50.7928   50.3236 -158.4492]\n",
      "Weights: [-4.836   0.751  -1.1617  0.0697  0.1522]\n",
      "MSE loss: 87.7821\n",
      "Iteration: 45600\n",
      "Gradient: [   2.6934    5.6983   44.2797 -122.3036 -136.9617]\n",
      "Weights: [-4.8272  0.7534 -1.1635  0.0691  0.1523]\n",
      "MSE loss: 87.5818\n",
      "Iteration: 45700\n",
      "Gradient: [ -7.7647   5.7873  16.5407 -79.8285 -59.8812]\n",
      "Weights: [-4.8222  0.7581 -1.1661  0.0691  0.1522]\n",
      "MSE loss: 87.577\n",
      "Iteration: 45800\n",
      "Gradient: [  0.489   14.3006  21.097  -66.1225 104.9168]\n",
      "Weights: [-4.8258  0.7544 -1.1663  0.0703  0.1523]\n",
      "MSE loss: 87.5828\n",
      "Iteration: 45900\n",
      "Gradient: [  -4.5771  -13.363   -49.9031  -40.5882 -167.9318]\n",
      "Weights: [-4.8236  0.7525 -1.1683  0.0707  0.1521]\n",
      "MSE loss: 87.6692\n",
      "Iteration: 46000\n",
      "Gradient: [-3.5567 17.871  30.1697 22.4617 41.2438]\n",
      "Weights: [-4.8229  0.7573 -1.1665  0.0705  0.1522]\n",
      "MSE loss: 87.652\n",
      "Iteration: 46100\n",
      "Gradient: [-10.5258   4.8678   7.6559  40.8354 149.4519]\n",
      "Weights: [-4.8272  0.7575 -1.1664  0.0709  0.152 ]\n",
      "MSE loss: 87.5864\n",
      "Iteration: 46200\n",
      "Gradient: [  -8.3896    1.3637   -2.2263   12.8786 -105.4706]\n",
      "Weights: [-4.8392  0.7618 -1.1671  0.0708  0.1521]\n",
      "MSE loss: 87.7134\n",
      "Iteration: 46300\n",
      "Gradient: [   5.1208   12.5859  -15.7619  -72.7251 -125.6399]\n",
      "Weights: [-4.8036  0.7413 -1.1682  0.0713  0.1525]\n",
      "MSE loss: 87.6513\n",
      "Iteration: 46400\n",
      "Gradient: [  4.1436  -1.689   12.3644 -40.7229 171.7644]\n",
      "Weights: [-4.8318  0.7538 -1.1668  0.0706  0.1522]\n",
      "MSE loss: 87.6872\n",
      "Iteration: 46500\n",
      "Gradient: [ 8.939700e+00  1.569300e+00 -9.366800e+00  2.132000e-01  4.331967e+02]\n",
      "Weights: [-4.8179  0.7546 -1.1694  0.0702  0.1523]\n",
      "MSE loss: 87.5655\n",
      "Iteration: 46600\n",
      "Gradient: [  -3.5904  -12.1925   -0.779   -22.9276 -167.4927]\n",
      "Weights: [-4.8376  0.758  -1.1684  0.0698  0.1524]\n",
      "MSE loss: 87.9689\n",
      "Iteration: 46700\n",
      "Gradient: [  0.8091   6.8725   8.2904 -62.3579 231.129 ]\n",
      "Weights: [-4.8171  0.7614 -1.1688  0.0695  0.1527]\n",
      "MSE loss: 87.8766\n",
      "Iteration: 46800\n",
      "Gradient: [ -1.8721 -11.893  -41.6346  40.7087 -66.7483]\n",
      "Weights: [-4.8399  0.7638 -1.1689  0.0697  0.1526]\n",
      "MSE loss: 87.745\n",
      "Iteration: 46900\n",
      "Gradient: [  5.4754  -3.07     9.3742  73.9716 -63.2291]\n",
      "Weights: [-4.819   0.7606 -1.1687  0.0694  0.1525]\n",
      "MSE loss: 87.6132\n",
      "Iteration: 47000\n",
      "Gradient: [  -4.7183   -4.1701  -48.3206   28.4845 -195.1216]\n",
      "Weights: [-4.8154  0.755  -1.1727  0.0697  0.1529]\n",
      "MSE loss: 87.5861\n",
      "Iteration: 47100\n",
      "Gradient: [  -6.1226    4.2167    1.9937  -63.6358 -182.8705]\n",
      "Weights: [-4.8168  0.7568 -1.1729  0.0692  0.1531]\n",
      "MSE loss: 87.5757\n",
      "Iteration: 47200\n",
      "Gradient: [ -0.6709  -1.0116  -7.983   31.2231 -78.0809]\n",
      "Weights: [-4.8295  0.7698 -1.1766  0.0699  0.153 ]\n",
      "MSE loss: 87.5422\n",
      "Iteration: 47300\n",
      "Gradient: [ -6.8102 -10.1896  -4.4428 -59.1422  29.4167]\n",
      "Weights: [-4.8245  0.7723 -1.1768  0.0689  0.1531]\n",
      "MSE loss: 87.5418\n",
      "Iteration: 47400\n",
      "Gradient: [  1.8701   7.5852   0.5368   1.0451 133.3035]\n",
      "Weights: [-4.8186  0.7701 -1.1772  0.0687  0.1534]\n",
      "MSE loss: 87.5676\n",
      "Iteration: 47500\n",
      "Gradient: [  4.4685   4.3805   0.291    0.1874 119.5724]\n",
      "Weights: [-4.8339  0.7888 -1.1805  0.069   0.1532]\n",
      "MSE loss: 87.6638\n",
      "Iteration: 47600\n",
      "Gradient: [ -1.1076   6.5582  43.2652 134.1072 321.4079]\n",
      "Weights: [-4.8229  0.7812 -1.1776  0.0685  0.1533]\n",
      "MSE loss: 87.8864\n",
      "Iteration: 47700\n",
      "Gradient: [ -6.679   20.276  -24.2519 -11.246  -23.6333]\n",
      "Weights: [-4.824   0.7746 -1.1753  0.068   0.153 ]\n",
      "MSE loss: 87.5929\n",
      "Iteration: 47800\n",
      "Gradient: [   8.4224   20.1178    2.9847   38.5731 -123.2456]\n",
      "Weights: [-4.8214  0.769  -1.1758  0.069   0.1532]\n",
      "MSE loss: 87.5525\n",
      "Iteration: 47900\n",
      "Gradient: [ -15.4814   -6.0593  -24.5986  -76.4452 -142.9054]\n",
      "Weights: [-4.8259  0.7687 -1.1762  0.0687  0.1534]\n",
      "MSE loss: 87.5466\n",
      "Iteration: 48000\n",
      "Gradient: [-1.063650e+01  1.073000e-01 -3.314250e+01  1.105830e+01 -3.461264e+02]\n",
      "Weights: [-4.8255  0.7704 -1.1768  0.0691  0.1531]\n",
      "MSE loss: 87.5426\n",
      "Iteration: 48100\n",
      "Gradient: [   5.9423    3.5313    4.0427   94.7076 -193.2384]\n",
      "Weights: [-4.8274  0.7716 -1.1768  0.0689  0.1533]\n",
      "MSE loss: 87.5357\n",
      "Iteration: 48200\n",
      "Gradient: [  0.1597  -1.4547  13.2903 -72.2698  48.1516]\n",
      "Weights: [-4.8363  0.7814 -1.1783  0.0689  0.1533]\n",
      "MSE loss: 87.577\n",
      "Iteration: 48300\n",
      "Gradient: [  -3.1477   -1.6097  -26.1315  -55.8797 -209.497 ]\n",
      "Weights: [-4.8242  0.7728 -1.1796  0.0695  0.1532]\n",
      "MSE loss: 87.5512\n",
      "Iteration: 48400\n",
      "Gradient: [  4.8255 -11.3255  14.0756  46.9084 166.8843]\n",
      "Weights: [-4.8167  0.7722 -1.1797  0.0687  0.1534]\n",
      "MSE loss: 87.6028\n",
      "Iteration: 48500\n",
      "Gradient: [  6.4324   9.8098  -6.9703  76.0377 187.9848]\n",
      "Weights: [-4.8188  0.7737 -1.177   0.0684  0.1533]\n",
      "MSE loss: 87.6285\n",
      "Iteration: 48600\n",
      "Gradient: [  1.1981   6.1457 -13.1333   7.2572  11.667 ]\n",
      "Weights: [-4.8268  0.7725 -1.1767  0.068   0.1534]\n",
      "MSE loss: 87.5658\n",
      "Iteration: 48700\n",
      "Gradient: [ -13.7815   -6.5436   -1.3137   34.8687 -219.396 ]\n",
      "Weights: [-4.8328  0.7713 -1.177   0.0683  0.1536]\n",
      "MSE loss: 87.6613\n",
      "Iteration: 48800\n",
      "Gradient: [  -4.7706  -10.7271   12.887  -116.9136 -143.3852]\n",
      "Weights: [-4.8214  0.7703 -1.1793  0.0688  0.1535]\n",
      "MSE loss: 87.5808\n",
      "Iteration: 48900\n",
      "Gradient: [-2.9931  9.7702 41.7975 -4.9432 22.8937]\n",
      "Weights: [-4.828   0.7715 -1.1754  0.0687  0.1533]\n",
      "MSE loss: 87.5507\n",
      "Iteration: 49000\n",
      "Gradient: [  5.4173   8.1242 -17.8581  79.4285 -45.2147]\n",
      "Weights: [-4.828   0.7715 -1.177   0.0694  0.1532]\n",
      "MSE loss: 87.5312\n",
      "Iteration: 49100\n",
      "Gradient: [  7.4532  10.5245  24.2804  80.6379 -39.7728]\n",
      "Weights: [-4.8078  0.7669 -1.1797  0.0699  0.1533]\n",
      "MSE loss: 87.7251\n",
      "Iteration: 49200\n",
      "Gradient: [ 10.9346   4.4675  25.939  -16.7919 463.8479]\n",
      "Weights: [-4.8197  0.7686 -1.1809  0.0705  0.1534]\n",
      "MSE loss: 87.5532\n",
      "Iteration: 49300\n",
      "Gradient: [ 1.254800e+01  2.505200e+01 -1.716000e-01 -8.371660e+01 -1.915329e+02]\n",
      "Weights: [-4.8265  0.7749 -1.182   0.0703  0.1532]\n",
      "MSE loss: 87.5171\n",
      "Iteration: 49400\n",
      "Gradient: [  1.5135  12.5654   3.5858 -18.6087 -72.5325]\n",
      "Weights: [-4.8198  0.767  -1.1816  0.0712  0.1532]\n",
      "MSE loss: 87.5277\n",
      "Iteration: 49500\n",
      "Gradient: [ -0.3973  -8.6733 -16.5602 -41.0024  95.9791]\n",
      "Weights: [-4.819   0.7666 -1.1796  0.0711  0.1532]\n",
      "MSE loss: 87.5938\n",
      "Iteration: 49600\n",
      "Gradient: [   0.764   -13.9322   39.3792   83.1422 -144.2178]\n",
      "Weights: [-4.8156  0.7614 -1.1786  0.071   0.1529]\n",
      "MSE loss: 87.5738\n",
      "Iteration: 49700\n",
      "Gradient: [  0.2926  11.7562  33.9623 -15.391   16.8625]\n",
      "Weights: [-4.8324  0.7724 -1.1782  0.0707  0.1532]\n",
      "MSE loss: 87.6667\n",
      "Iteration: 49800\n",
      "Gradient: [ 8.5555 -0.484  32.0608 18.812  22.6264]\n",
      "Weights: [-4.8146  0.76   -1.1744  0.07    0.153 ]\n",
      "MSE loss: 87.5657\n",
      "Iteration: 49900\n",
      "Gradient: [   1.0365  -23.8814  -65.6177  -14.2081 -221.628 ]\n",
      "Weights: [-4.8361  0.7724 -1.1788  0.0695  0.153 ]\n",
      "MSE loss: 88.0511\n",
      "Iteration: 50000\n",
      "Gradient: [  -9.8449  -10.8816  -38.2399   77.8907 -183.4544]\n",
      "Weights: [-4.8272  0.7679 -1.1788  0.0695  0.1534]\n",
      "MSE loss: 87.6391\n",
      "Iteration: 50100\n",
      "Gradient: [  -1.2501    3.8096    4.2604   58.4173 -211.2248]\n",
      "Weights: [-4.8246  0.7823 -1.1825  0.0694  0.1534]\n",
      "MSE loss: 87.5659\n",
      "Iteration: 50200\n",
      "Gradient: [  7.2807   3.7088 -29.8611 -66.2859 -71.0941]\n",
      "Weights: [-4.8264  0.7718 -1.1791  0.0701  0.1534]\n",
      "MSE loss: 87.6249\n",
      "Iteration: 50300\n",
      "Gradient: [  9.7149  -1.2226  10.8718 -16.0079 163.2824]\n",
      "Weights: [-4.8121  0.7687 -1.1775  0.0699  0.1529]\n",
      "MSE loss: 87.688\n",
      "Iteration: 50400\n",
      "Gradient: [ 4.2425  4.9368 44.6421 48.5849 17.4951]\n",
      "Weights: [-4.8169  0.7579 -1.1733  0.0703  0.1531]\n",
      "MSE loss: 87.6718\n",
      "Iteration: 50500\n",
      "Gradient: [  -0.5667   10.8531    1.092    95.4009 -233.3009]\n",
      "Weights: [-4.8091  0.76   -1.1753  0.0702  0.1531]\n",
      "MSE loss: 87.712\n",
      "Iteration: 50600\n",
      "Gradient: [  1.0804  10.3175  19.2934  93.186  195.0319]\n",
      "Weights: [-4.8211  0.7577 -1.1741  0.0705  0.1531]\n",
      "MSE loss: 87.5904\n",
      "Iteration: 50700\n",
      "Gradient: [ -8.2693  -1.8361  -6.6485  33.2337 199.808 ]\n",
      "Weights: [-4.8242  0.747  -1.1714  0.0716  0.1529]\n",
      "MSE loss: 87.8\n",
      "Iteration: 50800\n",
      "Gradient: [ -5.6977 -13.763   -4.2564 102.6527 108.2746]\n",
      "Weights: [-4.8157  0.7463 -1.1723  0.0716  0.1528]\n",
      "MSE loss: 87.6243\n",
      "Iteration: 50900\n",
      "Gradient: [ -8.1491   5.6026  -7.3669  37.3359 -53.1307]\n",
      "Weights: [-4.8257  0.762  -1.1724  0.0711  0.1526]\n",
      "MSE loss: 87.6023\n",
      "Iteration: 51000\n",
      "Gradient: [ 7.7784  2.379  36.1619 79.5109 27.7063]\n",
      "Weights: [-4.8177  0.7589 -1.1734  0.0712  0.1525]\n",
      "MSE loss: 87.5291\n",
      "Iteration: 51100\n",
      "Gradient: [  -4.1549   18.9186    8.2412  -21.3708 -231.8137]\n",
      "Weights: [-4.8045  0.7439 -1.171   0.0716  0.1527]\n",
      "MSE loss: 87.6861\n",
      "Iteration: 51200\n",
      "Gradient: [  6.1577   8.0994  -1.4849 -97.3688 206.0727]\n",
      "Weights: [-4.8185  0.7546 -1.1726  0.0713  0.1523]\n",
      "MSE loss: 87.6313\n",
      "Iteration: 51300\n",
      "Gradient: [  1.6102  12.6334  24.0731 -36.6932 -81.3242]\n",
      "Weights: [-4.8247  0.7659 -1.1731  0.0717  0.1521]\n",
      "MSE loss: 87.5696\n",
      "Iteration: 51400\n",
      "Gradient: [   2.0409    0.9061  -38.1147  -18.0278 -264.1109]\n",
      "Weights: [-4.8245  0.7554 -1.1735  0.0727  0.1523]\n",
      "MSE loss: 87.5605\n",
      "Iteration: 51500\n",
      "Gradient: [   3.6589  -18.6679   25.9565  -27.9263 -160.8126]\n",
      "Weights: [-4.8151  0.7453 -1.1728  0.0731  0.1522]\n",
      "MSE loss: 87.5894\n",
      "Iteration: 51600\n",
      "Gradient: [  -2.693    -1.681    40.5696  -46.9567 -189.8341]\n",
      "Weights: [-4.8234  0.7491 -1.1729  0.0736  0.152 ]\n",
      "MSE loss: 87.6605\n",
      "Iteration: 51700\n",
      "Gradient: [   2.4857   -8.8686   17.982    48.4459 -313.9878]\n",
      "Weights: [-4.8216  0.7482 -1.1723  0.0742  0.1519]\n",
      "MSE loss: 87.5746\n",
      "Iteration: 51800\n",
      "Gradient: [  5.1038   4.0578  17.1289  67.0962 254.9284]\n",
      "Weights: [-4.8221  0.7445 -1.1676  0.0737  0.1519]\n",
      "MSE loss: 87.6959\n",
      "Iteration: 51900\n",
      "Gradient: [   5.8566    0.7714   -5.0412   68.0463 -248.3405]\n",
      "Weights: [-4.804   0.7401 -1.1699  0.0736  0.1516]\n",
      "MSE loss: 87.6311\n",
      "Iteration: 52000\n",
      "Gradient: [  8.9535  -8.2097  21.5989  88.272  -81.8272]\n",
      "Weights: [-4.8067  0.7392 -1.1701  0.0739  0.1518]\n",
      "MSE loss: 87.5456\n",
      "Iteration: 52100\n",
      "Gradient: [-6.5749 12.3944  4.6474 -8.6071 10.7216]\n",
      "Weights: [-4.8116  0.7504 -1.1738  0.074   0.1519]\n",
      "MSE loss: 87.5105\n",
      "Iteration: 52200\n",
      "Gradient: [ -1.1174  12.5476  -4.4098  46.5545 358.8412]\n",
      "Weights: [-4.8015  0.7479 -1.1724  0.0734  0.1518]\n",
      "MSE loss: 87.6754\n",
      "Iteration: 52300\n",
      "Gradient: [  5.0923  12.1304  -5.0753   6.1555 -91.5184]\n",
      "Weights: [-4.8096  0.7565 -1.1746  0.0732  0.1519]\n",
      "MSE loss: 87.5857\n",
      "Iteration: 52400\n",
      "Gradient: [   6.5155   -4.5461   32.5552  -43.3504 -284.2103]\n",
      "Weights: [-4.8104  0.7515 -1.1735  0.0725  0.1522]\n",
      "MSE loss: 87.5339\n",
      "Iteration: 52500\n",
      "Gradient: [  10.9559   -1.5619   33.6887   98.1752 -213.2069]\n",
      "Weights: [-4.8218  0.7593 -1.1737  0.073   0.1521]\n",
      "MSE loss: 87.5652\n",
      "Iteration: 52600\n",
      "Gradient: [  3.7244   7.4719   6.942  -32.1427   1.0851]\n",
      "Weights: [-4.8251  0.7671 -1.1757  0.0735  0.1519]\n",
      "MSE loss: 87.6064\n",
      "Iteration: 52700\n",
      "Gradient: [   3.3158   -6.3242  -58.547    -6.6574 -102.0538]\n",
      "Weights: [-4.8311  0.7649 -1.1765  0.0732  0.152 ]\n",
      "MSE loss: 87.5635\n",
      "Iteration: 52800\n",
      "Gradient: [  0.3718   6.7968  36.7935 -97.9919 -64.7971]\n",
      "Weights: [-4.8073  0.7551 -1.1756  0.0726  0.1521]\n",
      "MSE loss: 87.6114\n",
      "Iteration: 52900\n",
      "Gradient: [-10.6573  10.1825  24.8209 -22.7118 -99.7162]\n",
      "Weights: [-4.8346  0.769  -1.176   0.0725  0.1522]\n",
      "MSE loss: 87.5669\n",
      "Iteration: 53000\n",
      "Gradient: [ -4.9283  -5.8245 -11.5719  -9.8844  52.7966]\n",
      "Weights: [-4.821   0.7626 -1.176   0.0721  0.1523]\n",
      "MSE loss: 87.4942\n",
      "Iteration: 53100\n",
      "Gradient: [ -7.2163   7.4512 -54.1719 -61.035  -69.2215]\n",
      "Weights: [-4.8271  0.7627 -1.1753  0.0729  0.1521]\n",
      "MSE loss: 87.516\n",
      "Iteration: 53200\n",
      "Gradient: [  5.2817 -10.4412  -9.4288 -22.1252  31.9133]\n",
      "Weights: [-4.8181  0.7617 -1.1761  0.0733  0.1517]\n",
      "MSE loss: 87.5125\n",
      "Iteration: 53300\n",
      "Gradient: [  5.6311  -5.2771   8.5471 -24.5849 178.3512]\n",
      "Weights: [-4.81    0.7561 -1.1752  0.0736  0.1519]\n",
      "MSE loss: 87.5718\n",
      "Iteration: 53400\n",
      "Gradient: [  -4.0033  -11.3703  -22.4856  -11.9566 -147.1366]\n",
      "Weights: [-4.8295  0.7626 -1.1759  0.074   0.1519]\n",
      "MSE loss: 87.5353\n",
      "Iteration: 53500\n",
      "Gradient: [-15.9562 -13.51     5.2092  39.8315 -83.1213]\n",
      "Weights: [-4.8275  0.7593 -1.1754  0.0735  0.1519]\n",
      "MSE loss: 87.5674\n",
      "Iteration: 53600\n",
      "Gradient: [  -3.5209   -4.0048  -16.5916  -20.7983 -335.8765]\n",
      "Weights: [-4.8254  0.7544 -1.1736  0.0743  0.1517]\n",
      "MSE loss: 87.5329\n",
      "Iteration: 53700\n",
      "Gradient: [ -1.327   -8.2582  -7.7142  59.7928 279.8958]\n",
      "Weights: [-4.8188  0.7558 -1.1751  0.0747  0.1518]\n",
      "MSE loss: 87.5708\n",
      "Iteration: 53800\n",
      "Gradient: [  3.6204  12.1342  17.2925 -68.1475 -59.4124]\n",
      "Weights: [-4.8179  0.7592 -1.1765  0.0747  0.1516]\n",
      "MSE loss: 87.468\n",
      "Iteration: 53900\n",
      "Gradient: [-2.4762 -1.0612 17.1514 55.1629 35.6562]\n",
      "Weights: [-4.8172  0.7557 -1.177   0.0752  0.1516]\n",
      "MSE loss: 87.4636\n",
      "Iteration: 54000\n",
      "Gradient: [ -11.0182   -2.2173   -0.6934   -6.588  -283.9304]\n",
      "Weights: [-4.8199  0.7533 -1.1739  0.0747  0.1514]\n",
      "MSE loss: 87.4867\n",
      "Iteration: 54100\n",
      "Gradient: [ -0.2639   1.7388  -1.8849 -49.7864 196.4778]\n",
      "Weights: [-4.8193  0.7503 -1.1745  0.0755  0.1515]\n",
      "MSE loss: 87.4942\n",
      "Iteration: 54200\n",
      "Gradient: [  -7.2263    6.0006  -14.0629  -19.8238 -202.0341]\n",
      "Weights: [-4.8196  0.7542 -1.1738  0.0757  0.1511]\n",
      "MSE loss: 87.457\n",
      "Iteration: 54300\n",
      "Gradient: [  1.0699   4.7424 -24.0211  46.8938 277.4868]\n",
      "Weights: [-4.7864  0.7339 -1.1707  0.0754  0.1512]\n",
      "MSE loss: 87.9322\n",
      "Iteration: 54400\n",
      "Gradient: [  -1.1553    4.2879  -40.9954  -60.7423 -196.8633]\n",
      "Weights: [-4.8021  0.7243 -1.1702  0.0761  0.1514]\n",
      "MSE loss: 87.888\n",
      "Iteration: 54500\n",
      "Gradient: [  1.5805   4.8956  19.8784  69.541  263.7022]\n",
      "Weights: [-4.7822  0.7223 -1.166   0.076   0.1512]\n",
      "MSE loss: 88.1344\n",
      "Iteration: 54600\n",
      "Gradient: [  3.9918   0.416  -67.3118 -40.4446  72.329 ]\n",
      "Weights: [-4.7882  0.7256 -1.1672  0.076   0.151 ]\n",
      "MSE loss: 87.7776\n",
      "Iteration: 54700\n",
      "Gradient: [  8.3892  12.6937 -47.6723   6.3138 236.3729]\n",
      "Weights: [-4.7988  0.7229 -1.165   0.0763  0.1508]\n",
      "MSE loss: 87.5832\n",
      "Iteration: 54800\n",
      "Gradient: [  -4.1647   25.3857   21.5801 -151.9987  135.5862]\n",
      "Weights: [-4.7956  0.7285 -1.1655  0.0766  0.1506]\n",
      "MSE loss: 87.6525\n",
      "Iteration: 54900\n",
      "Gradient: [  6.268  -12.9603 -41.9558 -86.6113  92.9862]\n",
      "Weights: [-4.7996  0.7254 -1.166   0.0767  0.1507]\n",
      "MSE loss: 87.5694\n",
      "Iteration: 55000\n",
      "Gradient: [ -0.4983  -8.0832   0.4931  10.3359 -90.1991]\n",
      "Weights: [-4.8159  0.7401 -1.1659  0.0763  0.1506]\n",
      "MSE loss: 87.5116\n",
      "Iteration: 55100\n",
      "Gradient: [  4.3462  13.1318  -5.6946  35.1302 237.2231]\n",
      "Weights: [-4.8232  0.739  -1.1654  0.0759  0.1507]\n",
      "MSE loss: 87.5884\n",
      "Iteration: 55200\n",
      "Gradient: [ -10.025   -14.6368  -18.7719  -57.9824 -147.672 ]\n",
      "Weights: [-4.8257  0.7473 -1.1693  0.0758  0.1507]\n",
      "MSE loss: 87.6692\n",
      "Iteration: 55300\n",
      "Gradient: [ -3.443  -11.7435  23.0087 -81.5686 168.9356]\n",
      "Weights: [-4.8103  0.7439 -1.1701  0.0765  0.1507]\n",
      "MSE loss: 87.4907\n",
      "Iteration: 55400\n",
      "Gradient: [ -0.4112  -5.1388   0.8664 -28.9772  -4.1388]\n",
      "Weights: [-4.8039  0.7376 -1.1706  0.0765  0.1507]\n",
      "MSE loss: 87.5665\n",
      "Iteration: 55500\n",
      "Gradient: [ -5.5021  35.9342  20.2381  29.838  -86.5284]\n",
      "Weights: [-4.8029  0.743  -1.1692  0.0766  0.1508]\n",
      "MSE loss: 87.8633\n",
      "Iteration: 55600\n",
      "Gradient: [ 2.470000e-02  2.190500e+00  6.583100e+00 -4.855070e+01 -1.460418e+02]\n",
      "Weights: [-4.8148  0.7335 -1.169   0.0772  0.1509]\n",
      "MSE loss: 87.5732\n",
      "Iteration: 55700\n",
      "Gradient: [   6.8004  -22.9975    8.8625 -124.9609 -211.391 ]\n",
      "Weights: [-4.7896  0.7174 -1.1679  0.0774  0.1508]\n",
      "MSE loss: 87.7524\n",
      "Iteration: 55800\n",
      "Gradient: [  5.377   12.2125 -17.5684  91.1728 -94.7816]\n",
      "Weights: [-4.7935  0.7307 -1.167   0.0769  0.1505]\n",
      "MSE loss: 87.7126\n",
      "Iteration: 55900\n",
      "Gradient: [ -6.128    2.568   20.9675 -93.6074  11.2919]\n",
      "Weights: [-4.8195  0.7313 -1.1688  0.0781  0.1508]\n",
      "MSE loss: 87.7262\n",
      "Iteration: 56000\n",
      "Gradient: [ -2.6871   1.2735  26.0792 -77.383  286.9697]\n",
      "Weights: [-4.8019  0.7274 -1.1678  0.0774  0.1508]\n",
      "MSE loss: 87.5381\n",
      "Iteration: 56100\n",
      "Gradient: [   1.286     5.9595  -51.7259  -64.4999 -344.6156]\n",
      "Weights: [-4.8091  0.7342 -1.1689  0.0761  0.1508]\n",
      "MSE loss: 87.6643\n",
      "Iteration: 56200\n",
      "Gradient: [  -6.7338    4.4179    2.6155  -45.4283 -139.2686]\n",
      "Weights: [-4.8039  0.7328 -1.1683  0.0761  0.1508]\n",
      "MSE loss: 87.5709\n",
      "Iteration: 56300\n",
      "Gradient: [ -9.4923 -24.7569 -17.5064  58.5743 324.2607]\n",
      "Weights: [-4.8203  0.7419 -1.1677  0.0764  0.1506]\n",
      "MSE loss: 87.5145\n",
      "Iteration: 56400\n",
      "Gradient: [  -7.6846  -20.0947  -23.501   -33.5817 -152.837 ]\n",
      "Weights: [-4.8277  0.7447 -1.1686  0.076   0.1504]\n",
      "MSE loss: 88.0692\n",
      "Iteration: 56500\n",
      "Gradient: [  6.5239  23.0816  43.8446  39.404  277.0505]\n",
      "Weights: [-4.8033  0.7379 -1.1655  0.076   0.1507]\n",
      "MSE loss: 87.8337\n",
      "Iteration: 56600\n",
      "Gradient: [  1.1769   1.7461  -7.9677 124.5208 -24.8021]\n",
      "Weights: [-4.8082  0.7342 -1.1666  0.0764  0.1507]\n",
      "MSE loss: 87.4958\n",
      "Iteration: 56700\n",
      "Gradient: [ 6.1292 -0.9218 31.0755 -2.5022 74.5167]\n",
      "Weights: [-4.8092  0.7332 -1.1678  0.0769  0.1507]\n",
      "MSE loss: 87.5042\n",
      "Iteration: 56800\n",
      "Gradient: [  4.775    5.0681  36.1246 -61.302   86.9424]\n",
      "Weights: [-4.8046  0.7322 -1.1654  0.0765  0.1507]\n",
      "MSE loss: 87.5906\n",
      "Iteration: 56900\n",
      "Gradient: [ 5.330600e+00  6.180000e-02  1.321700e+01 -1.612500e+01 -1.062961e+02]\n",
      "Weights: [-4.8066  0.727  -1.1698  0.0776  0.1509]\n",
      "MSE loss: 87.6183\n",
      "Iteration: 57000\n",
      "Gradient: [  7.8436  -6.659  -41.4423  47.452   73.4671]\n",
      "Weights: [-4.8239  0.7396 -1.1711  0.0771  0.1509]\n",
      "MSE loss: 87.7486\n",
      "Iteration: 57100\n",
      "Gradient: [-8.0776 -6.8364 23.6612 17.2257 -4.3802]\n",
      "Weights: [-4.8142  0.7423 -1.1709  0.0768  0.1506]\n",
      "MSE loss: 87.5336\n",
      "Iteration: 57200\n",
      "Gradient: [   2.6945    5.6618  -14.6106   -2.6569 -259.6149]\n",
      "Weights: [-4.8128  0.752  -1.1739  0.0772  0.1506]\n",
      "MSE loss: 87.4977\n",
      "Iteration: 57300\n",
      "Gradient: [ 10.5172  -4.5455  29.0357  15.4485 -27.4289]\n",
      "Weights: [-4.822   0.7539 -1.1724  0.0769  0.1508]\n",
      "MSE loss: 87.5623\n",
      "Iteration: 57400\n",
      "Gradient: [   0.3063   -1.6194   19.4986  -20.8971 -252.0274]\n",
      "Weights: [-4.8213  0.7589 -1.1741  0.0755  0.1509]\n",
      "MSE loss: 87.4888\n",
      "Iteration: 57500\n",
      "Gradient: [  3.6147 -19.1678  62.3931   5.2329 108.2487]\n",
      "Weights: [-4.8246  0.7597 -1.1768  0.0757  0.1511]\n",
      "MSE loss: 87.5613\n",
      "Iteration: 57600\n",
      "Gradient: [-10.3939 -27.7692  13.7334 -83.585  207.9613]\n",
      "Weights: [-4.8308  0.7646 -1.1761  0.0753  0.1511]\n",
      "MSE loss: 87.5096\n",
      "Iteration: 57700\n",
      "Gradient: [ -8.0812   5.3541  -0.6263 -94.7514  69.7392]\n",
      "Weights: [-4.8265  0.7603 -1.1781  0.0759  0.1512]\n",
      "MSE loss: 87.6012\n",
      "Iteration: 57800\n",
      "Gradient: [ -2.5324   8.1712 -40.5452   4.959   97.3469]\n",
      "Weights: [-4.8225  0.7606 -1.1788  0.0762  0.1513]\n",
      "MSE loss: 87.4386\n",
      "Iteration: 57900\n",
      "Gradient: [  14.8234   17.7523  -64.7913   18.2607 -207.9009]\n",
      "Weights: [-4.8061  0.7421 -1.175   0.0764  0.1513]\n",
      "MSE loss: 87.5223\n",
      "Iteration: 58000\n",
      "Gradient: [ -7.1778  -4.1305  17.3585 102.8293  56.1514]\n",
      "Weights: [-4.8196  0.7499 -1.1778  0.0774  0.1512]\n",
      "MSE loss: 87.5033\n",
      "Iteration: 58100\n",
      "Gradient: [ -12.6584  -12.0214   -8.7425   69.47   -242.3631]\n",
      "Weights: [-4.8276  0.7562 -1.1791  0.0779  0.1513]\n",
      "MSE loss: 87.5877\n",
      "Iteration: 58200\n",
      "Gradient: [  0.6096  -0.7432  -9.2613 157.0187 109.2973]\n",
      "Weights: [-4.8158  0.7513 -1.1791  0.0777  0.151 ]\n",
      "MSE loss: 87.4888\n",
      "Iteration: 58300\n",
      "Gradient: [  2.8545  10.1039  13.4432 -82.8443  -6.4324]\n",
      "Weights: [-4.8091  0.7515 -1.1798  0.0774  0.151 ]\n",
      "MSE loss: 87.5198\n",
      "Iteration: 58400\n",
      "Gradient: [  1.9217   1.4883  37.6171 -11.1632 305.6492]\n",
      "Weights: [-4.8139  0.7506 -1.1783  0.0785  0.1511]\n",
      "MSE loss: 87.5533\n",
      "Iteration: 58500\n",
      "Gradient: [  13.6844   11.9426   23.828   -85.6989 -165.9626]\n",
      "Weights: [-4.807   0.7477 -1.1819  0.0786  0.1512]\n",
      "MSE loss: 87.4788\n",
      "Iteration: 58600\n",
      "Gradient: [ -5.572   10.4964 -23.7471 -46.2348 166.1976]\n",
      "Weights: [-4.8101  0.7574 -1.1823  0.0785  0.1513]\n",
      "MSE loss: 87.6728\n",
      "Iteration: 58700\n",
      "Gradient: [ -7.8228  -3.4879 -27.0473 -47.2969 -55.6972]\n",
      "Weights: [-4.8271  0.7527 -1.1817  0.0784  0.1513]\n",
      "MSE loss: 87.7301\n",
      "Iteration: 58800\n",
      "Gradient: [   5.1748  -13.9953  -20.826  -106.0012   41.1702]\n",
      "Weights: [-4.8043  0.7477 -1.1822  0.0785  0.1512]\n",
      "MSE loss: 87.5066\n",
      "Iteration: 58900\n",
      "Gradient: [ -11.7617    6.8573  -18.3803   17.6664 -314.7945]\n",
      "Weights: [-4.8056  0.7477 -1.1855  0.0787  0.1513]\n",
      "MSE loss: 87.731\n",
      "Iteration: 59000\n",
      "Gradient: [  0.3727  -7.6006  30.7998 -90.4321 -45.1983]\n",
      "Weights: [-4.8066  0.7468 -1.1819  0.0787  0.1512]\n",
      "MSE loss: 87.4959\n",
      "Iteration: 59100\n",
      "Gradient: [  -1.1948   12.6351   -7.4479   54.0996 -100.7957]\n",
      "Weights: [-4.8235  0.7591 -1.1838  0.0796  0.1508]\n",
      "MSE loss: 87.4419\n",
      "Iteration: 59200\n",
      "Gradient: [ -6.0318  -5.9501  -0.095  -56.5738  82.6988]\n",
      "Weights: [-4.8237  0.7595 -1.1825  0.0792  0.1508]\n",
      "MSE loss: 87.4228\n",
      "Iteration: 59300\n",
      "Gradient: [  9.9847   2.6132 -12.6593 -32.9625 383.5668]\n",
      "Weights: [-4.831   0.7696 -1.1832  0.0792  0.1504]\n",
      "MSE loss: 87.4404\n",
      "Iteration: 59400\n",
      "Gradient: [ 7.9157  9.121   4.2062 24.7286 49.0347]\n",
      "Weights: [-4.8123  0.7648 -1.1834  0.0794  0.1504]\n",
      "MSE loss: 87.5364\n",
      "Iteration: 59500\n",
      "Gradient: [  8.2954  -4.5553 -18.1814 -19.8374  99.0124]\n",
      "Weights: [-4.8311  0.7741 -1.1868  0.0799  0.1504]\n",
      "MSE loss: 87.4208\n",
      "Iteration: 59600\n",
      "Gradient: [   4.3032    4.0585  -14.7777 -136.9709  -56.7288]\n",
      "Weights: [-4.8076  0.7604 -1.1879  0.0806  0.1503]\n",
      "MSE loss: 87.6258\n",
      "Iteration: 59700\n",
      "Gradient: [  -3.2179  -10.9463   -4.9129   -4.3778 -190.1548]\n",
      "Weights: [-4.8386  0.7739 -1.1872  0.0815  0.1503]\n",
      "MSE loss: 87.5058\n",
      "Iteration: 59800\n",
      "Gradient: [ 11.3802 -14.9251 -52.1275 146.6935 194.8734]\n",
      "Weights: [-4.8172  0.7681 -1.1884  0.0812  0.1503]\n",
      "MSE loss: 87.3919\n",
      "Iteration: 59900\n",
      "Gradient: [  12.5798   -2.018    17.6211  115.8643 -105.6911]\n",
      "Weights: [-4.8108  0.7695 -1.19    0.0814  0.1505]\n",
      "MSE loss: 87.5963\n",
      "Iteration: 60000\n",
      "Gradient: [  4.5909 -21.4046   3.964   95.0471  34.4261]\n",
      "Weights: [-4.8136  0.7582 -1.1914  0.0819  0.1508]\n",
      "MSE loss: 87.4579\n",
      "Iteration: 60100\n",
      "Gradient: [ -10.8574   10.9162   -5.7332   19.0446 -223.7324]\n",
      "Weights: [-4.8261  0.7738 -1.1923  0.0804  0.1509]\n",
      "MSE loss: 87.3786\n",
      "Iteration: 60200\n",
      "Gradient: [   3.2177   -6.856    -8.2327   27.0931 -185.3031]\n",
      "Weights: [-4.8104  0.7602 -1.1901  0.0802  0.1512]\n",
      "MSE loss: 87.4273\n",
      "Iteration: 60300\n",
      "Gradient: [  -6.1151  -18.8024  -26.6674  -20.3172 -217.5731]\n",
      "Weights: [-4.8255  0.7607 -1.1879  0.0802  0.1511]\n",
      "MSE loss: 87.5298\n",
      "Iteration: 60400\n",
      "Gradient: [   5.1808    5.1889  -25.5178   57.6589 -106.1867]\n",
      "Weights: [-4.8109  0.7553 -1.1893  0.0813  0.1509]\n",
      "MSE loss: 87.4275\n",
      "Iteration: 60500\n",
      "Gradient: [  4.7737 -12.1662  -3.3755  75.2119 -43.1316]\n",
      "Weights: [-4.8218  0.7616 -1.1913  0.0814  0.151 ]\n",
      "MSE loss: 87.4981\n",
      "Iteration: 60600\n",
      "Gradient: [  7.146   -7.8964  -4.7195  85.7126 -12.1743]\n",
      "Weights: [-4.8134  0.7642 -1.1934  0.0816  0.1511]\n",
      "MSE loss: 87.4024\n",
      "Iteration: 60700\n",
      "Gradient: [  3.8914   1.4773  38.2657  41.2352 231.2368]\n",
      "Weights: [-4.816   0.7679 -1.1941  0.0816  0.151 ]\n",
      "MSE loss: 87.3787\n",
      "Iteration: 60800\n",
      "Gradient: [ -0.8138  -0.3191   7.6366  36.7947 248.8779]\n",
      "Weights: [-4.8308  0.775  -1.192   0.0816  0.1507]\n",
      "MSE loss: 87.3985\n",
      "Iteration: 60900\n",
      "Gradient: [ -3.047  -18.2577  18.0169  18.2549 228.5556]\n",
      "Weights: [-4.8235  0.7738 -1.1948  0.0821  0.1506]\n",
      "MSE loss: 87.3494\n",
      "Iteration: 61000\n",
      "Gradient: [-10.5804  15.5897  20.5929 119.2276 -26.3696]\n",
      "Weights: [-4.8319  0.7812 -1.1925  0.082   0.1503]\n",
      "MSE loss: 87.3891\n",
      "Iteration: 61100\n",
      "Gradient: [  -2.418    17.386     8.1802    6.3503 -168.7335]\n",
      "Weights: [-4.8162  0.7683 -1.1948  0.0821  0.1506]\n",
      "MSE loss: 87.4132\n",
      "Iteration: 61200\n",
      "Gradient: [ -4.566   -0.591   43.1241 -76.7704 -14.2407]\n",
      "Weights: [-4.8221  0.7691 -1.1929  0.0824  0.1507]\n",
      "MSE loss: 87.3585\n",
      "Iteration: 61300\n",
      "Gradient: [  -6.2238   17.4456   -3.1951  -44.4184 -219.0523]\n",
      "Weights: [-4.8335  0.7776 -1.195   0.0826  0.1503]\n",
      "MSE loss: 87.484\n",
      "Iteration: 61400\n",
      "Gradient: [  -5.5261    3.5437   20.9443   82.427  -158.3182]\n",
      "Weights: [-4.8225  0.7735 -1.1982  0.0833  0.1504]\n",
      "MSE loss: 87.4516\n",
      "Iteration: 61500\n",
      "Gradient: [  2.166    7.3428 -13.0327  18.7663  22.6352]\n",
      "Weights: [-4.8217  0.778  -1.198   0.0835  0.1504]\n",
      "MSE loss: 87.3129\n",
      "Iteration: 61600\n",
      "Gradient: [  5.6245  -0.775   11.4328 -53.0629  56.6881]\n",
      "Weights: [-4.8282  0.7839 -1.1984  0.0831  0.1503]\n",
      "MSE loss: 87.313\n",
      "Iteration: 61700\n",
      "Gradient: [  3.2863   3.2296  37.7144  26.6001 114.2333]\n",
      "Weights: [-4.8467  0.7893 -1.1981  0.0836  0.1504]\n",
      "MSE loss: 87.549\n",
      "Iteration: 61800\n",
      "Gradient: [ -3.1669  -2.1733 -23.3008   5.7651 -28.5566]\n",
      "Weights: [-4.8288  0.7877 -1.1981  0.0829  0.1504]\n",
      "MSE loss: 87.3774\n",
      "Iteration: 61900\n",
      "Gradient: [  3.2905  -9.0613 -14.7576   1.1188  67.7559]\n",
      "Weights: [-4.8313  0.7835 -1.2015  0.0835  0.1504]\n",
      "MSE loss: 87.5529\n",
      "Iteration: 62000\n",
      "Gradient: [  6.8719  -4.5951  -7.5033 104.5749  94.3362]\n",
      "Weights: [-4.7991  0.7679 -1.197   0.0828  0.1504]\n",
      "MSE loss: 87.7033\n",
      "Iteration: 62100\n",
      "Gradient: [   6.7521    3.8451   -3.0793 -164.7002 -155.1203]\n",
      "Weights: [-4.8197  0.7789 -1.1988  0.0833  0.1505]\n",
      "MSE loss: 87.3286\n",
      "Iteration: 62200\n",
      "Gradient: [   6.3195    3.9117   -7.556   151.2518 -181.1327]\n",
      "Weights: [-4.8175  0.7913 -1.2004  0.0827  0.1507]\n",
      "MSE loss: 87.9921\n",
      "Iteration: 62300\n",
      "Gradient: [ -3.1378  12.8428  30.1628  53.3627 133.3906]\n",
      "Weights: [-4.834   0.8031 -1.2065  0.0832  0.1505]\n",
      "MSE loss: 87.3438\n",
      "Iteration: 62400\n",
      "Gradient: [-12.5814  12.6401 -24.5547  -6.4026  17.6307]\n",
      "Weights: [-4.8388  0.8021 -1.2057  0.084   0.1504]\n",
      "MSE loss: 87.3313\n",
      "Iteration: 62500\n",
      "Gradient: [  1.3544 -11.7753  19.0825  18.7194  -6.7909]\n",
      "Weights: [-4.8414  0.805  -1.2057  0.0832  0.1505]\n",
      "MSE loss: 87.3469\n",
      "Iteration: 62600\n",
      "Gradient: [ -4.2323  -2.4736  13.7495 -43.7525 430.0708]\n",
      "Weights: [-4.8338  0.805  -1.208   0.0834  0.1506]\n",
      "MSE loss: 87.3477\n",
      "Iteration: 62700\n",
      "Gradient: [  -5.2268    5.4929  -19.7796  102.1283 -208.0853]\n",
      "Weights: [-4.8344  0.7969 -1.2063  0.0837  0.1507]\n",
      "MSE loss: 87.2942\n",
      "Iteration: 62800\n",
      "Gradient: [-3.854700e+00  1.391000e-01  1.991520e+01  7.681300e+00 -2.637416e+02]\n",
      "Weights: [-4.8226  0.784  -1.2032  0.084   0.1505]\n",
      "MSE loss: 87.315\n",
      "Iteration: 62900\n",
      "Gradient: [ -7.819    7.5052 -20.967  -79.8347  92.3123]\n",
      "Weights: [-4.8247  0.7882 -1.2031  0.0839  0.1504]\n",
      "MSE loss: 87.3084\n",
      "Iteration: 63000\n",
      "Gradient: [  -4.4763   -1.4836  -27.9518   22.5586 -174.2825]\n",
      "Weights: [-4.8194  0.7739 -1.2003  0.0847  0.1501]\n",
      "MSE loss: 87.4047\n",
      "Iteration: 63100\n",
      "Gradient: [-11.8743  -7.0286  -6.8674 -60.846   13.0266]\n",
      "Weights: [-4.8049  0.7629 -1.2011  0.0856  0.1504]\n",
      "MSE loss: 87.4209\n",
      "Iteration: 63200\n",
      "Gradient: [ -11.7485   10.1944  -26.2763  -64.6786 -198.5985]\n",
      "Weights: [-4.8223  0.7778 -1.2016  0.0848  0.1502]\n",
      "MSE loss: 87.336\n",
      "Iteration: 63300\n",
      "Gradient: [ -0.9463 -13.5375   0.7647  30.585  -12.8523]\n",
      "Weights: [-4.8175  0.7785 -1.2023  0.0843  0.1505]\n",
      "MSE loss: 87.3256\n",
      "Iteration: 63400\n",
      "Gradient: [  5.7476   2.728  -31.898    2.3793 -45.657 ]\n",
      "Weights: [-4.8331  0.7817 -1.1992  0.0833  0.1505]\n",
      "MSE loss: 87.4071\n",
      "Iteration: 63500\n",
      "Gradient: [  -5.3575    3.3889   11.2495   13.7419 -340.2791]\n",
      "Weights: [-4.8216  0.7789 -1.1985  0.0835  0.1504]\n",
      "MSE loss: 87.3141\n",
      "Iteration: 63600\n",
      "Gradient: [  -0.9483  -11.6697    7.907   100.1993 -160.8715]\n",
      "Weights: [-4.8355  0.7844 -1.1983  0.0839  0.1502]\n",
      "MSE loss: 87.3429\n",
      "Iteration: 63700\n",
      "Gradient: [ -0.4874 -10.7596  -9.3354 -19.9267 134.9915]\n",
      "Weights: [-4.8374  0.7879 -1.1992  0.0836  0.15  ]\n",
      "MSE loss: 87.4376\n",
      "Iteration: 63800\n",
      "Gradient: [-2.9702  3.9935 -9.4626 25.2975 10.3016]\n",
      "Weights: [-4.8285  0.7833 -1.1982  0.0836  0.1499]\n",
      "MSE loss: 87.4057\n",
      "Iteration: 63900\n",
      "Gradient: [   8.9109   -6.9452    0.6959 -130.7763 -157.5935]\n",
      "Weights: [-4.8238  0.7745 -1.1956  0.0833  0.1502]\n",
      "MSE loss: 87.3498\n",
      "Iteration: 64000\n",
      "Gradient: [-10.021    9.3176 -15.6981 -31.9247 -57.9699]\n",
      "Weights: [-4.8241  0.7803 -1.1971  0.0837  0.1501]\n",
      "MSE loss: 87.3194\n",
      "Iteration: 64100\n",
      "Gradient: [ -4.4589   2.0744  28.1475 -97.4201 -94.9106]\n",
      "Weights: [-4.8334  0.784  -1.1992  0.0839  0.15  ]\n",
      "MSE loss: 87.4224\n",
      "Iteration: 64200\n",
      "Gradient: [  -3.7661  -18.3864   16.1823 -120.3066 -112.6157]\n",
      "Weights: [-4.8353  0.782  -1.1961  0.0839  0.1502]\n",
      "MSE loss: 87.3946\n",
      "Iteration: 64300\n",
      "Gradient: [  5.7017   8.2478   7.8551 -31.2819 151.853 ]\n",
      "Weights: [-4.8121  0.7742 -1.197   0.0838  0.1501]\n",
      "MSE loss: 87.4298\n",
      "Iteration: 64400\n",
      "Gradient: [ -7.5987  -4.1337 -53.7936 -22.2126 -30.8579]\n",
      "Weights: [-4.8306  0.7747 -1.1989  0.085   0.15  ]\n",
      "MSE loss: 87.5741\n",
      "Iteration: 64500\n",
      "Gradient: [  -1.1044   13.8817   13.5889    3.4537 -296.0215]\n",
      "Weights: [-4.8187  0.7827 -1.2017  0.085   0.1499]\n",
      "MSE loss: 87.3543\n",
      "Iteration: 64600\n",
      "Gradient: [  -1.2377    4.7249   31.1012   20.2119 -248.5909]\n",
      "Weights: [-4.8301  0.7801 -1.2002  0.0858  0.15  ]\n",
      "MSE loss: 87.3135\n",
      "Iteration: 64700\n",
      "Gradient: [   4.6635   -3.9964  -27.1794 -104.2146  108.9754]\n",
      "Weights: [-4.8278  0.7812 -1.2028  0.0856  0.1498]\n",
      "MSE loss: 87.5001\n",
      "Iteration: 64800\n",
      "Gradient: [  -0.8217  -12.5136   -5.0148   -4.3412 -153.3649]\n",
      "Weights: [-4.8235  0.7851 -1.2014  0.0852  0.1499]\n",
      "MSE loss: 87.3166\n",
      "Iteration: 64900\n",
      "Gradient: [  2.8685   0.1927   9.4946 -31.1588  70.3114]\n",
      "Weights: [-4.817   0.7687 -1.1998  0.0862  0.1499]\n",
      "MSE loss: 87.3045\n",
      "Iteration: 65000\n",
      "Gradient: [  2.4189   6.1471 -15.0974  90.2954  37.4975]\n",
      "Weights: [-4.8048  0.7559 -1.1943  0.0862  0.1499]\n",
      "MSE loss: 87.4505\n",
      "Iteration: 65100\n",
      "Gradient: [  -5.679   -13.905    13.8157   66.0527 -518.6676]\n",
      "Weights: [-4.8317  0.7566 -1.192   0.0856  0.1499]\n",
      "MSE loss: 87.8296\n",
      "Iteration: 65200\n",
      "Gradient: [ -9.0882 -10.1789  14.6693  80.9715 -50.2331]\n",
      "Weights: [-4.8323  0.7829 -1.1981  0.0849  0.1498]\n",
      "MSE loss: 87.3081\n",
      "Iteration: 65300\n",
      "Gradient: [  -4.7405    4.6621   -9.7062 -140.8409    5.7591]\n",
      "Weights: [-4.8461  0.7916 -1.1989  0.0849  0.1498]\n",
      "MSE loss: 87.4534\n",
      "Iteration: 65400\n",
      "Gradient: [  -6.7061   15.5577   20.2096  -97.8199 -151.3581]\n",
      "Weights: [-4.836   0.7905 -1.1995  0.0843  0.1498]\n",
      "MSE loss: 87.3501\n",
      "Iteration: 65500\n",
      "Gradient: [ -8.7134   9.5295   4.5273 -43.768  -42.8033]\n",
      "Weights: [-4.8451  0.7945 -1.1996  0.0849  0.1498]\n",
      "MSE loss: 87.4323\n",
      "Iteration: 65600\n",
      "Gradient: [  -8.4543   -3.8243  -58.8024 -100.7287  -62.2389]\n",
      "Weights: [-4.8434  0.7918 -1.1993  0.0851  0.1496]\n",
      "MSE loss: 87.4014\n",
      "Iteration: 65700\n",
      "Gradient: [  -6.7568  -11.2324   -6.9268  -91.5254 -191.4951]\n",
      "Weights: [-4.8394  0.7871 -1.1969  0.0847  0.1496]\n",
      "MSE loss: 87.3686\n",
      "Iteration: 65800\n",
      "Gradient: [  7.4363  -3.7328 -32.7906 -53.1557  13.9858]\n",
      "Weights: [-4.8222  0.7802 -1.1992  0.0854  0.1496]\n",
      "MSE loss: 87.3436\n",
      "Iteration: 65900\n",
      "Gradient: [  3.0096 -11.871  -26.7992 -32.127  278.1026]\n",
      "Weights: [-4.8217  0.7814 -1.2021  0.0865  0.1497]\n",
      "MSE loss: 87.3004\n",
      "Iteration: 66000\n",
      "Gradient: [   1.234   -13.2269  -33.6003  -56.4588 -186.6637]\n",
      "Weights: [-4.8138  0.7789 -1.203   0.0861  0.1498]\n",
      "MSE loss: 87.376\n",
      "Iteration: 66100\n",
      "Gradient: [ 3.05    8.5472 41.278  76.6534 14.3752]\n",
      "Weights: [-4.8201  0.7849 -1.2043  0.0867  0.1499]\n",
      "MSE loss: 87.3945\n",
      "Iteration: 66200\n",
      "Gradient: [ -0.9571  -1.5534  26.8299 -55.484  -30.5234]\n",
      "Weights: [-4.8299  0.7855 -1.2066  0.0873  0.1499]\n",
      "MSE loss: 87.2626\n",
      "Iteration: 66300\n",
      "Gradient: [   3.2775   14.9764    6.0302   -1.6235 -264.3737]\n",
      "Weights: [-4.8211  0.7958 -1.208   0.0866  0.1497]\n",
      "MSE loss: 87.446\n",
      "Iteration: 66400\n",
      "Gradient: [  8.9243   6.88    15.0604 -71.1191 261.5395]\n",
      "Weights: [-4.826   0.7897 -1.2079  0.0866  0.1501]\n",
      "MSE loss: 87.2544\n",
      "Iteration: 66500\n",
      "Gradient: [ 12.4062  -3.869   44.9264  64.4658 -14.0139]\n",
      "Weights: [-4.83    0.7877 -1.208   0.0873  0.1499]\n",
      "MSE loss: 87.256\n",
      "Iteration: 66600\n",
      "Gradient: [ -3.0426   7.3368 -30.5695 -11.8735 186.079 ]\n",
      "Weights: [-4.8129  0.7826 -1.2075  0.0869  0.1499]\n",
      "MSE loss: 87.3922\n",
      "Iteration: 66700\n",
      "Gradient: [  6.0597  14.22    31.8513  38.2822 -96.5941]\n",
      "Weights: [-4.8311  0.7869 -1.2031  0.0871  0.1496]\n",
      "MSE loss: 87.3161\n",
      "Iteration: 66800\n",
      "Gradient: [  -9.1011   -9.967   -11.6514    0.1536 -149.627 ]\n",
      "Weights: [-4.8318  0.7736 -1.203   0.0878  0.1496]\n",
      "MSE loss: 87.584\n",
      "Iteration: 66900\n",
      "Gradient: [-10.7907  -3.428   65.9528 -22.2878 -77.9772]\n",
      "Weights: [-4.8054  0.7681 -1.2016  0.0882  0.1497]\n",
      "MSE loss: 87.6025\n",
      "Iteration: 67000\n",
      "Gradient: [  -3.2148  -25.7511  -19.6086 -100.902  -117.4175]\n",
      "Weights: [-4.8206  0.7688 -1.2055  0.0886  0.1498]\n",
      "MSE loss: 87.4266\n",
      "Iteration: 67100\n",
      "Gradient: [  -3.2007    7.2804   -3.0349  -60.4379 -183.3887]\n",
      "Weights: [-4.8204  0.7681 -1.2033  0.0883  0.1495]\n",
      "MSE loss: 87.5238\n",
      "Iteration: 67200\n",
      "Gradient: [  6.5801  -5.3814 -13.5778 127.1089 164.5906]\n",
      "Weights: [-4.8236  0.7739 -1.2037  0.0882  0.1497]\n",
      "MSE loss: 87.2752\n",
      "Iteration: 67300\n",
      "Gradient: [   3.6448    6.4419  -23.2088  -36.8186 -136.0053]\n",
      "Weights: [-4.8456  0.788  -1.2018  0.087   0.1494]\n",
      "MSE loss: 87.5087\n",
      "Iteration: 67400\n",
      "Gradient: [  1.092   -0.4718  32.4755  12.3751 -78.8908]\n",
      "Weights: [-4.8237  0.7822 -1.2023  0.0871  0.1496]\n",
      "MSE loss: 87.329\n",
      "Iteration: 67500\n",
      "Gradient: [ 6.525  -3.0341 -4.2885 57.4412 10.3775]\n",
      "Weights: [-4.8265  0.7838 -1.2042  0.0862  0.1499]\n",
      "MSE loss: 87.2659\n",
      "Iteration: 67600\n",
      "Gradient: [  -0.3289   -4.9711  -14.276   -82.3202 -258.2757]\n",
      "Weights: [-4.8114  0.7771 -1.203   0.086   0.1499]\n",
      "MSE loss: 87.3897\n",
      "Iteration: 67700\n",
      "Gradient: [  10.3167   -5.7853   19.4096 -115.148  -500.2632]\n",
      "Weights: [-4.8229  0.7722 -1.2013  0.087   0.1499]\n",
      "MSE loss: 87.2957\n",
      "Iteration: 67800\n",
      "Gradient: [   0.8833  -10.9616  -19.0608  -63.2061 -472.9831]\n",
      "Weights: [-4.8233  0.7716 -1.2012  0.087   0.1498]\n",
      "MSE loss: 87.3086\n",
      "Iteration: 67900\n",
      "Gradient: [  8.5598  12.353   75.9252 104.6236 -83.5105]\n",
      "Weights: [-4.8093  0.7681 -1.2007  0.0866  0.1501]\n",
      "MSE loss: 87.4492\n",
      "Iteration: 68000\n",
      "Gradient: [  3.0718  10.1292   8.4024 -28.3575 182.0222]\n",
      "Weights: [-4.8141  0.7651 -1.2023  0.0876  0.1503]\n",
      "MSE loss: 87.5204\n",
      "Iteration: 68100\n",
      "Gradient: [ 5.4121  7.179   1.2965 53.1744 13.2832]\n",
      "Weights: [-4.8192  0.7692 -1.2035  0.0874  0.15  ]\n",
      "MSE loss: 87.3527\n",
      "Iteration: 68200\n",
      "Gradient: [ 4.7704 22.9464 -1.0872  9.584  58.356 ]\n",
      "Weights: [-4.8186  0.7829 -1.205   0.0866  0.15  ]\n",
      "MSE loss: 87.363\n",
      "Iteration: 68300\n",
      "Gradient: [-3.3509 17.9285  1.9488 20.2432 17.7504]\n",
      "Weights: [-4.8165  0.7826 -1.2063  0.086   0.1501]\n",
      "MSE loss: 87.3314\n",
      "Iteration: 68400\n",
      "Gradient: [  1.928   -0.819   10.494   27.4673 285.1135]\n",
      "Weights: [-4.8237  0.7879 -1.207   0.086   0.1502]\n",
      "MSE loss: 87.2661\n",
      "Iteration: 68500\n",
      "Gradient: [   2.0212   12.565   -25.2788   -1.668  -105.7901]\n",
      "Weights: [-4.8336  0.7979 -1.2072  0.0849  0.1502]\n",
      "MSE loss: 87.2871\n",
      "Iteration: 68600\n",
      "Gradient: [  9.3078   3.5485  27.7619 -14.0138  33.1263]\n",
      "Weights: [-4.8184  0.7958 -1.2073  0.0846  0.1503]\n",
      "MSE loss: 87.5212\n",
      "Iteration: 68700\n",
      "Gradient: [ -1.508   -2.6917 -13.2953  19.8549 328.7364]\n",
      "Weights: [-4.8444  0.8032 -1.2067  0.0843  0.1505]\n",
      "MSE loss: 87.3673\n",
      "Iteration: 68800\n",
      "Gradient: [ -0.8395  -9.1317 -10.2437  82.9767 -66.8389]\n",
      "Weights: [-4.8358  0.7932 -1.2069  0.0847  0.1504]\n",
      "MSE loss: 87.4315\n",
      "Iteration: 68900\n",
      "Gradient: [  -5.0171    3.6366    5.6581 -106.3873    9.3262]\n",
      "Weights: [-4.8374  0.7955 -1.2088  0.0845  0.1505]\n",
      "MSE loss: 87.5272\n",
      "Iteration: 69000\n",
      "Gradient: [  5.8093  13.6106  16.8232 -14.9849 380.6571]\n",
      "Weights: [-4.8178  0.7961 -1.2074  0.0844  0.1506]\n",
      "MSE loss: 87.6853\n",
      "Iteration: 69100\n",
      "Gradient: [ 3.754000e-01 -1.227350e+01  1.436200e+01  5.718700e+00  4.076447e+02]\n",
      "Weights: [-4.8194  0.7987 -1.2085  0.0838  0.1507]\n",
      "MSE loss: 87.5405\n",
      "Iteration: 69200\n",
      "Gradient: [  -8.0725  -17.3017  -35.9777   32.6328 -234.723 ]\n",
      "Weights: [-4.8294  0.7883 -1.2075  0.0846  0.1507]\n",
      "MSE loss: 87.3849\n",
      "Iteration: 69300\n",
      "Gradient: [ -2.7872 -21.8879 -39.4118 -36.8227 304.089 ]\n",
      "Weights: [-4.8288  0.7877 -1.2033  0.0837  0.1507]\n",
      "MSE loss: 87.2977\n",
      "Iteration: 69400\n",
      "Gradient: [ -3.3007 -14.8916  10.877   -6.7587 588.5854]\n",
      "Weights: [-4.8361  0.7904 -1.2012  0.0834  0.1504]\n",
      "MSE loss: 87.3318\n",
      "Iteration: 69500\n",
      "Gradient: [ -6.5496   0.4265 -11.091   29.3519  70.1609]\n",
      "Weights: [-4.8466  0.7874 -1.2006  0.0834  0.1506]\n",
      "MSE loss: 87.7282\n",
      "Iteration: 69600\n",
      "Gradient: [  -0.582    11.5694  -15.5124 -146.5347  -45.7331]\n",
      "Weights: [-4.8349  0.7873 -1.2009  0.0838  0.1504]\n",
      "MSE loss: 87.3289\n",
      "Iteration: 69700\n",
      "Gradient: [ 11.486  -18.7491  23.5039  36.0142 -80.2969]\n",
      "Weights: [-4.8412  0.7924 -1.2029  0.0848  0.1503]\n",
      "MSE loss: 87.388\n",
      "Iteration: 69800\n",
      "Gradient: [ -0.7941  -7.9731  17.1718 -10.3073  43.2713]\n",
      "Weights: [-4.8216  0.7879 -1.2045  0.0855  0.15  ]\n",
      "MSE loss: 87.3279\n",
      "Iteration: 69900\n",
      "Gradient: [  11.5289    7.9361   13.519    40.9995 -211.5758]\n",
      "Weights: [-4.8168  0.7919 -1.2074  0.0861  0.15  ]\n",
      "MSE loss: 87.5021\n",
      "Iteration: 70000\n",
      "Gradient: [ 11.9607  -5.6791  48.3765 -23.1963  27.7699]\n",
      "Weights: [-4.8075  0.7806 -1.2074  0.0863  0.1502]\n",
      "MSE loss: 87.4661\n",
      "Iteration: 70100\n",
      "Gradient: [14.332   8.3987  3.9022 39.5451 75.1347]\n",
      "Weights: [-4.8305  0.7881 -1.2055  0.0872  0.15  ]\n",
      "MSE loss: 87.4235\n",
      "Iteration: 70200\n",
      "Gradient: [  2.147   10.7687  -9.7707  68.6668 303.5613]\n",
      "Weights: [-4.8283  0.7898 -1.2074  0.0869  0.15  ]\n",
      "MSE loss: 87.2586\n",
      "Iteration: 70300\n",
      "Gradient: [ 4.8423 -0.9299 15.539  89.5774 -4.4408]\n",
      "Weights: [-4.8355  0.7931 -1.2085  0.0879  0.1497]\n",
      "MSE loss: 87.2647\n",
      "Iteration: 70400\n",
      "Gradient: [  12.1594   -1.4963   19.3818   -4.0069 -129.0534]\n",
      "Weights: [-4.8193  0.7835 -1.2084  0.0887  0.1498]\n",
      "MSE loss: 87.3313\n",
      "Iteration: 70500\n",
      "Gradient: [ -12.7166  -21.5407   15.3138 -102.3186   76.9763]\n",
      "Weights: [-4.8311  0.788  -1.2085  0.0884  0.1497]\n",
      "MSE loss: 87.2395\n",
      "Iteration: 70600\n",
      "Gradient: [ -3.0396  -5.7858   5.4766  75.9963 145.0045]\n",
      "Weights: [-4.8108  0.7796 -1.2093  0.0892  0.1498]\n",
      "MSE loss: 87.5212\n",
      "Iteration: 70700\n",
      "Gradient: [  -4.533    -4.6712   53.1523  182.3856 -144.6347]\n",
      "Weights: [-4.8239  0.7954 -1.2122  0.088   0.1498]\n",
      "MSE loss: 87.2677\n",
      "Iteration: 70800\n",
      "Gradient: [ -3.7711  -2.6992 -51.2972 -38.7013 -42.8774]\n",
      "Weights: [-4.8208  0.7943 -1.2137  0.0876  0.15  ]\n",
      "MSE loss: 87.2943\n",
      "Iteration: 70900\n",
      "Gradient: [   9.5477   20.7067   29.9597  -13.1169 -135.2914]\n",
      "Weights: [-4.8283  0.8026 -1.2119  0.0872  0.15  ]\n",
      "MSE loss: 87.4904\n",
      "Iteration: 71000\n",
      "Gradient: [  11.6317   -4.1874    6.9533   49.7796 -107.7346]\n",
      "Weights: [-4.8242  0.7937 -1.2124  0.0874  0.1501]\n",
      "MSE loss: 87.2455\n",
      "Iteration: 71100\n",
      "Gradient: [  3.1821  12.2907   9.3406 -43.0672  60.8442]\n",
      "Weights: [-4.8281  0.7864 -1.2105  0.0876  0.1501]\n",
      "MSE loss: 87.3099\n",
      "Iteration: 71200\n",
      "Gradient: [ -1.694   -8.2265 -14.0308  84.5522 185.024 ]\n",
      "Weights: [-4.8334  0.7883 -1.2123  0.0872  0.1505]\n",
      "MSE loss: 87.4861\n",
      "Iteration: 71300\n",
      "Gradient: [   1.618     2.2049  -28.4565   51.8243 -216.2108]\n",
      "Weights: [-4.829   0.7964 -1.2138  0.0866  0.1505]\n",
      "MSE loss: 87.2541\n",
      "Iteration: 71400\n",
      "Gradient: [ -5.6094   6.14   -10.8084 -66.0259 -74.0508]\n",
      "Weights: [-4.8304  0.7946 -1.2149  0.0874  0.1504]\n",
      "MSE loss: 87.2941\n",
      "Iteration: 71500\n",
      "Gradient: [  5.0871  -3.141  -11.6492 -13.4145 -40.3128]\n",
      "Weights: [-4.829   0.7902 -1.2134  0.0874  0.1504]\n",
      "MSE loss: 87.345\n",
      "Iteration: 71600\n",
      "Gradient: [ -6.2263 -10.78    16.4749 138.4291 -25.7363]\n",
      "Weights: [-4.8368  0.7939 -1.2128  0.0876  0.1501]\n",
      "MSE loss: 87.4933\n",
      "Iteration: 71700\n",
      "Gradient: [  1.2769   5.8242  18.6268 -71.4032 324.9888]\n",
      "Weights: [-4.8223  0.7901 -1.2138  0.0886  0.1501]\n",
      "MSE loss: 87.2313\n",
      "Iteration: 71800\n",
      "Gradient: [  8.8196  12.1733   0.6285 -51.5827 274.0009]\n",
      "Weights: [-4.8277  0.803  -1.2142  0.0884  0.1499]\n",
      "MSE loss: 87.4119\n",
      "Iteration: 71900\n",
      "Gradient: [  -3.2243   -4.8003   17.1028   30.8753 -298.5668]\n",
      "Weights: [-4.832   0.792  -1.2142  0.0888  0.1499]\n",
      "MSE loss: 87.3316\n",
      "Iteration: 72000\n",
      "Gradient: [ -2.5737   1.3477  28.1888  -0.6654 111.9749]\n",
      "Weights: [-4.8334  0.7929 -1.2157  0.0893  0.1501]\n",
      "MSE loss: 87.3057\n",
      "Iteration: 72100\n",
      "Gradient: [  3.4297 -17.8229  36.8739 -35.3329 196.3128]\n",
      "Weights: [-4.8356  0.8086 -1.2167  0.0883  0.1498]\n",
      "MSE loss: 87.2384\n",
      "Iteration: 72200\n",
      "Gradient: [ -4.5178   2.4223  -9.9497  84.8191 264.2993]\n",
      "Weights: [-4.8391  0.8082 -1.2161  0.0888  0.1497]\n",
      "MSE loss: 87.2355\n",
      "Iteration: 72300\n",
      "Gradient: [   3.158    -4.2375  -27.8496  -96.3309 -154.12  ]\n",
      "Weights: [-4.8445  0.8034 -1.2161  0.0885  0.1496]\n",
      "MSE loss: 87.8111\n",
      "Iteration: 72400\n",
      "Gradient: [   5.5522    2.3536    6.5315 -140.7793 -125.962 ]\n",
      "Weights: [-4.8406  0.7977 -1.2149  0.09    0.1496]\n",
      "MSE loss: 87.3095\n",
      "Iteration: 72500\n",
      "Gradient: [  -7.9501    4.7899  -11.5713  103.4317 -175.8146]\n",
      "Weights: [-4.8315  0.7979 -1.2148  0.0892  0.1496]\n",
      "MSE loss: 87.238\n",
      "Iteration: 72600\n",
      "Gradient: [-5.7007 -1.8388  6.0343 72.5845 79.7447]\n",
      "Weights: [-4.8141  0.7787 -1.2126  0.0906  0.1498]\n",
      "MSE loss: 87.3056\n",
      "Iteration: 72700\n",
      "Gradient: [ -2.9495 -12.7503 -46.3842  -1.9986 142.9903]\n",
      "Weights: [-4.8254  0.7842 -1.215   0.0902  0.1495]\n",
      "MSE loss: 87.6823\n",
      "Iteration: 72800\n",
      "Gradient: [ -13.864   -13.4103   11.6087 -102.5068 -214.8081]\n",
      "Weights: [-4.8153  0.7847 -1.2146  0.0899  0.1496]\n",
      "MSE loss: 87.3085\n",
      "Iteration: 72900\n",
      "Gradient: [-13.724   -5.7529  23.3259 -22.0016 -76.5938]\n",
      "Weights: [-4.829   0.791  -1.2136  0.0899  0.1496]\n",
      "MSE loss: 87.2075\n",
      "Iteration: 73000\n",
      "Gradient: [   2.4251    1.2039   43.4278   14.7203 -113.0537]\n",
      "Weights: [-4.8132  0.7897 -1.2157  0.0901  0.1497]\n",
      "MSE loss: 87.353\n",
      "Iteration: 73100\n",
      "Gradient: [ -0.2747  -5.9235  42.8807 -50.8877 -33.9487]\n",
      "Weights: [-4.8381  0.7975 -1.2152  0.09    0.1498]\n",
      "MSE loss: 87.278\n",
      "Iteration: 73200\n",
      "Gradient: [ -7.8628  -0.358  -28.2453 -81.2211 -87.7675]\n",
      "Weights: [-4.8334  0.7968 -1.2136  0.0894  0.1495]\n",
      "MSE loss: 87.2132\n",
      "Iteration: 73300\n",
      "Gradient: [ -6.6304 -10.6199  59.2567 -46.1856  14.5881]\n",
      "Weights: [-4.8386  0.7987 -1.2138  0.0899  0.1496]\n",
      "MSE loss: 87.2647\n",
      "Iteration: 73400\n",
      "Gradient: [  0.8694  -6.9071  60.9272 217.4214 107.1886]\n",
      "Weights: [-4.8307  0.8022 -1.2173  0.0903  0.1496]\n",
      "MSE loss: 87.2765\n",
      "Iteration: 73500\n",
      "Gradient: [ 4.8919  9.0336 16.5826 53.6693 80.9758]\n",
      "Weights: [-4.8251  0.7939 -1.2171  0.091   0.1494]\n",
      "MSE loss: 87.1848\n",
      "Iteration: 73600\n",
      "Gradient: [  3.5814  15.454   48.8277 -41.3068 431.0814]\n",
      "Weights: [-4.8303  0.8023 -1.2183  0.0909  0.1495]\n",
      "MSE loss: 87.2377\n",
      "Iteration: 73700\n",
      "Gradient: [ -4.8907  25.3576  33.8619 -20.7369  56.8495]\n",
      "Weights: [-4.8351  0.8012 -1.2201  0.0914  0.1493]\n",
      "MSE loss: 87.2487\n",
      "Iteration: 73800\n",
      "Gradient: [  8.6544 -11.9679  18.5144  64.7029 390.5948]\n",
      "Weights: [-4.8179  0.7949 -1.2184  0.0919  0.1496]\n",
      "MSE loss: 87.6497\n",
      "Iteration: 73900\n",
      "Gradient: [ 0.5332  4.2111 20.7381 45.4759 -7.0489]\n",
      "Weights: [-4.8268  0.7908 -1.2153  0.0905  0.1496]\n",
      "MSE loss: 87.1978\n",
      "Iteration: 74000\n",
      "Gradient: [ -2.6488  -8.8994  -1.1119  53.935  101.6791]\n",
      "Weights: [-4.8279  0.7935 -1.2152  0.0905  0.1496]\n",
      "MSE loss: 87.2025\n",
      "Iteration: 74100\n",
      "Gradient: [  -5.5418   -2.4767  -10.7427  -35.9577 -131.4558]\n",
      "Weights: [-4.835   0.7886 -1.2135  0.091   0.1493]\n",
      "MSE loss: 87.3367\n",
      "Iteration: 74200\n",
      "Gradient: [  15.7289   29.6538   12.6649   41.9298 -128.3704]\n",
      "Weights: [-4.8196  0.7951 -1.2132  0.0913  0.1493]\n",
      "MSE loss: 88.0192\n",
      "Iteration: 74300\n",
      "Gradient: [  7.9475  -1.8588  -6.45    52.2127 280.8835]\n",
      "Weights: [-4.8329  0.803  -1.2137  0.0905  0.1492]\n",
      "MSE loss: 87.4057\n",
      "Iteration: 74400\n",
      "Gradient: [  14.3523   -3.908    21.5928 -116.4154   77.0215]\n",
      "Weights: [-4.8193  0.7955 -1.218   0.0916  0.1491]\n",
      "MSE loss: 87.2543\n",
      "Iteration: 74500\n",
      "Gradient: [  7.0722 -21.1377  20.0186 152.5181  71.1338]\n",
      "Weights: [-4.8207  0.7919 -1.2166  0.0928  0.1489]\n",
      "MSE loss: 87.26\n",
      "Iteration: 74600\n",
      "Gradient: [  4.4     24.8263   8.6725  16.4923 -52.339 ]\n",
      "Weights: [-4.8159  0.7908 -1.216   0.0932  0.1488]\n",
      "MSE loss: 87.4841\n",
      "Iteration: 74700\n",
      "Gradient: [ -6.2747  -1.8085  -0.918   14.9131 219.4172]\n",
      "Weights: [-4.8293  0.7882 -1.2148  0.093   0.1488]\n",
      "MSE loss: 87.1718\n",
      "Iteration: 74800\n",
      "Gradient: [  -4.7     -13.7866    5.3006  -19.5524 -179.722 ]\n",
      "Weights: [-4.8333  0.7908 -1.2161  0.0929  0.1487]\n",
      "MSE loss: 87.2825\n",
      "Iteration: 74900\n",
      "Gradient: [  6.441  -14.0506  23.3892  37.5053 -38.7089]\n",
      "Weights: [-4.8265  0.7924 -1.2146  0.0926  0.1486]\n",
      "MSE loss: 87.1741\n",
      "Iteration: 75000\n",
      "Gradient: [  -5.2167    3.521   -21.2168  -51.1993 -351.0075]\n",
      "Weights: [-4.8144  0.7933 -1.2164  0.0927  0.1486]\n",
      "MSE loss: 87.4051\n",
      "Iteration: 75100\n",
      "Gradient: [ -5.2393  -4.2499  -1.6168  -1.923  196.2443]\n",
      "Weights: [-4.8343  0.8044 -1.2192  0.0924  0.1487]\n",
      "MSE loss: 87.2005\n",
      "Iteration: 75200\n",
      "Gradient: [ -11.6595   14.0976   -4.4186   42.4458 -116.6345]\n",
      "Weights: [-4.821   0.7933 -1.2182  0.0927  0.1487]\n",
      "MSE loss: 87.2414\n",
      "Iteration: 75300\n",
      "Gradient: [-5.355600e+00 -1.386420e+01 -2.600140e+01  2.226000e-01 -3.735166e+02]\n",
      "Weights: [-4.8293  0.7814 -1.2176  0.0939  0.149 ]\n",
      "MSE loss: 87.4305\n",
      "Iteration: 75400\n",
      "Gradient: [  -8.0565   -0.189    18.6205   88.7064 -106.5064]\n",
      "Weights: [-4.82    0.7718 -1.2173  0.095   0.149 ]\n",
      "MSE loss: 87.3989\n",
      "Iteration: 75500\n",
      "Gradient: [   2.098     3.5785    0.4453  -25.9073 -278.8418]\n",
      "Weights: [-4.8104  0.7732 -1.217   0.0945  0.1487]\n",
      "MSE loss: 87.3593\n",
      "Iteration: 75600\n",
      "Gradient: [  -1.4292   13.4305  -30.7653 -111.4112  -56.3885]\n",
      "Weights: [-4.8165  0.7739 -1.2183  0.095   0.1488]\n",
      "MSE loss: 87.3717\n",
      "Iteration: 75700\n",
      "Gradient: [  3.7697 -10.419   15.0806 -20.06    86.6177]\n",
      "Weights: [-4.8274  0.7883 -1.2171  0.0951  0.1485]\n",
      "MSE loss: 87.2152\n",
      "Iteration: 75800\n",
      "Gradient: [-11.1284  -2.7537  -7.5557 -45.6103   2.6929]\n",
      "Weights: [-4.8324  0.7896 -1.2193  0.0954  0.1484]\n",
      "MSE loss: 87.2051\n",
      "Iteration: 75900\n",
      "Gradient: [  3.5741  -1.9808 -27.5075 183.1708 -22.9025]\n",
      "Weights: [-4.817   0.7895 -1.2216  0.0955  0.1485]\n",
      "MSE loss: 87.1622\n",
      "Iteration: 76000\n",
      "Gradient: [-3.3439  1.9617 -2.7533 74.9627  3.6115]\n",
      "Weights: [-4.8327  0.7945 -1.2223  0.0954  0.1485]\n",
      "MSE loss: 87.245\n",
      "Iteration: 76100\n",
      "Gradient: [-11.8309 -19.3037 -25.9154 -53.0527  29.7254]\n",
      "Weights: [-4.8375  0.7945 -1.2194  0.0948  0.1484]\n",
      "MSE loss: 87.278\n",
      "Iteration: 76200\n",
      "Gradient: [  4.829  -13.9056 -24.2287  19.4964  59.2481]\n",
      "Weights: [-4.8282  0.7908 -1.2189  0.0952  0.1482]\n",
      "MSE loss: 87.1545\n",
      "Iteration: 76300\n",
      "Gradient: [  4.2653   4.0583  31.2181  64.333  -94.1641]\n",
      "Weights: [-4.8254  0.7882 -1.2175  0.0956  0.1483]\n",
      "MSE loss: 87.1701\n",
      "Iteration: 76400\n",
      "Gradient: [  0.5405  14.2254   8.1176 -23.1094 126.2554]\n",
      "Weights: [-4.8232  0.7896 -1.2175  0.0949  0.1482]\n",
      "MSE loss: 87.1402\n",
      "Iteration: 76500\n",
      "Gradient: [ -1.6338   7.1645 -15.7451  78.652  165.7658]\n",
      "Weights: [-4.8333  0.7979 -1.2186  0.0943  0.1482]\n",
      "MSE loss: 87.1907\n",
      "Iteration: 76600\n",
      "Gradient: [  12.3985    0.7534  -74.1458   40.2838 -208.2283]\n",
      "Weights: [-4.8501  0.8024 -1.2177  0.094   0.1483]\n",
      "MSE loss: 87.4615\n",
      "Iteration: 76700\n",
      "Gradient: [ -1.998   13.3952  12.6319 103.4889  31.0531]\n",
      "Weights: [-4.8465  0.8068 -1.2189  0.0937  0.1484]\n",
      "MSE loss: 87.2544\n",
      "Iteration: 76800\n",
      "Gradient: [ -3.7859  17.2503  11.006  -19.5711 283.043 ]\n",
      "Weights: [-4.8466  0.81   -1.2201  0.0937  0.1485]\n",
      "MSE loss: 87.2379\n",
      "Iteration: 76900\n",
      "Gradient: [  1.2557   7.9394  26.342  -17.4748 -13.2787]\n",
      "Weights: [-4.8279  0.7963 -1.2196  0.0944  0.1484]\n",
      "MSE loss: 87.1368\n",
      "Iteration: 77000\n",
      "Gradient: [ -5.4833  -1.075   -9.4005 -65.0912  74.5416]\n",
      "Weights: [-4.8294  0.7852 -1.2165  0.0951  0.1481]\n",
      "MSE loss: 87.3102\n",
      "Iteration: 77100\n",
      "Gradient: [   5.4023   -7.4476   26.8293 -127.8278  -21.5863]\n",
      "Weights: [-4.8255  0.79   -1.2157  0.0954  0.1479]\n",
      "MSE loss: 87.1614\n",
      "Iteration: 77200\n",
      "Gradient: [ -6.8732  -1.7424  -7.2337 -35.6027 102.6157]\n",
      "Weights: [-4.8301  0.7913 -1.2173  0.0948  0.1481]\n",
      "MSE loss: 87.2223\n",
      "Iteration: 77300\n",
      "Gradient: [  1.1151   3.2089  10.5854 -41.1047 -44.6029]\n",
      "Weights: [-4.8156  0.7897 -1.2196  0.0955  0.1482]\n",
      "MSE loss: 87.2411\n",
      "Iteration: 77400\n",
      "Gradient: [  4.888    8.5825  33.9413 -13.406   59.5641]\n",
      "Weights: [-4.8135  0.7929 -1.2209  0.0956  0.1483]\n",
      "MSE loss: 87.3914\n",
      "Iteration: 77500\n",
      "Gradient: [  -0.4492    1.6548  -29.2333  -37.0883 -168.4528]\n",
      "Weights: [-4.8351  0.7874 -1.2192  0.0962  0.1481]\n",
      "MSE loss: 87.4194\n",
      "Iteration: 77600\n",
      "Gradient: [-6.3528  8.7452 -3.9373 -8.901  21.5343]\n",
      "Weights: [-4.83    0.784  -1.2165  0.0972  0.1478]\n",
      "MSE loss: 87.1595\n",
      "Iteration: 77700\n",
      "Gradient: [ -0.3834 -19.5519  -0.1514 -71.2552 -80.191 ]\n",
      "Weights: [-4.8314  0.7828 -1.2177  0.0975  0.1477]\n",
      "MSE loss: 87.2199\n",
      "Iteration: 77800\n",
      "Gradient: [  10.7458   -6.9097  -35.0634  -50.6448 -183.9493]\n",
      "Weights: [-4.8075  0.7768 -1.221   0.0983  0.1478]\n",
      "MSE loss: 87.208\n",
      "Iteration: 77900\n",
      "Gradient: [ -3.0273   6.3735 -13.857    2.0716 -13.8224]\n",
      "Weights: [-4.8023  0.7732 -1.222   0.0991  0.1478]\n",
      "MSE loss: 87.2582\n",
      "Iteration: 78000\n",
      "Gradient: [  1.2491  10.6817  21.6798  49.1137 -31.3447]\n",
      "Weights: [-4.8211  0.7792 -1.2215  0.0997  0.1476]\n",
      "MSE loss: 87.1224\n",
      "Iteration: 78100\n",
      "Gradient: [ -0.6125   0.9066  17.6474 -64.3295  87.3188]\n",
      "Weights: [-4.8094  0.7804 -1.2223  0.0988  0.1478]\n",
      "MSE loss: 87.1853\n",
      "Iteration: 78200\n",
      "Gradient: [ -3.1291  13.3587  11.5667 -17.7628 120.2723]\n",
      "Weights: [-4.8238  0.7906 -1.2233  0.0989  0.1477]\n",
      "MSE loss: 87.1043\n",
      "Iteration: 78300\n",
      "Gradient: [ -10.755   -16.5175    0.8125  -53.7947 -208.6299]\n",
      "Weights: [-4.8332  0.7898 -1.2253  0.0997  0.1477]\n",
      "MSE loss: 87.195\n",
      "Iteration: 78400\n",
      "Gradient: [-1.9909 -8.1738 55.4409 17.1319 97.3009]\n",
      "Weights: [-4.8309  0.7895 -1.2252  0.1003  0.1475]\n",
      "MSE loss: 87.1237\n",
      "Iteration: 78500\n",
      "Gradient: [ -7.3649  13.9368  39.8305 126.8876  80.2128]\n",
      "Weights: [-4.816   0.7815 -1.2249  0.1009  0.1473]\n",
      "MSE loss: 87.0779\n",
      "Iteration: 78600\n",
      "Gradient: [  -4.374   -11.4851   34.9333  -34.8244 -186.9044]\n",
      "Weights: [-4.8178  0.7859 -1.2265  0.1003  0.1474]\n",
      "MSE loss: 87.1614\n",
      "Iteration: 78700\n",
      "Gradient: [ -4.5117  -8.0054   0.8647  14.3664 152.5401]\n",
      "Weights: [-4.8297  0.7919 -1.2247  0.1003  0.1473]\n",
      "MSE loss: 87.0667\n",
      "Iteration: 78800\n",
      "Gradient: [-2.60250e+00 -9.27000e-02  3.04244e+01  6.44860e+00 -1.02522e+02]\n",
      "Weights: [-4.8156  0.797  -1.2253  0.0996  0.1473]\n",
      "MSE loss: 87.4178\n",
      "Iteration: 78900\n",
      "Gradient: [  -5.9807  -25.9346  -15.3995  -19.3873 -237.3589]\n",
      "Weights: [-4.8306  0.7907 -1.2255  0.1004  0.1472]\n",
      "MSE loss: 87.1612\n",
      "Iteration: 79000\n",
      "Gradient: [ -5.3862 -18.2718  24.481  -25.9927  90.5668]\n",
      "Weights: [-4.8177  0.7909 -1.2259  0.1002  0.1472]\n",
      "MSE loss: 87.1116\n",
      "Iteration: 79100\n",
      "Gradient: [  0.8098   2.4864 -19.5368  24.6619  94.0288]\n",
      "Weights: [-4.8251  0.7952 -1.2265  0.101   0.1472]\n",
      "MSE loss: 87.1346\n",
      "Iteration: 79200\n",
      "Gradient: [ -4.8088  10.0723 -24.7768  98.8837 -98.4425]\n",
      "Weights: [-4.8309  0.8078 -1.231   0.1005  0.1473]\n",
      "MSE loss: 87.0769\n",
      "Iteration: 79300\n",
      "Gradient: [  0.5148  -0.6576 -33.0556   0.0668 -26.4701]\n",
      "Weights: [-4.8402  0.8071 -1.2329  0.1008  0.1475]\n",
      "MSE loss: 87.143\n",
      "Iteration: 79400\n",
      "Gradient: [ -2.1388   3.2753 -40.9504   5.4978  99.5983]\n",
      "Weights: [-4.8316  0.8065 -1.2352  0.101   0.1476]\n",
      "MSE loss: 87.0666\n",
      "Iteration: 79500\n",
      "Gradient: [   1.4217    3.2544   25.6693 -114.0279  -39.9518]\n",
      "Weights: [-4.8221  0.8062 -1.2391  0.1017  0.1477]\n",
      "MSE loss: 87.0787\n",
      "Iteration: 79600\n",
      "Gradient: [  7.1689   0.6256  42.7047  44.7205 -72.3085]\n",
      "Weights: [-4.8211  0.817  -1.2399  0.1017  0.1477]\n",
      "MSE loss: 87.4891\n",
      "Iteration: 79700\n",
      "Gradient: [  3.6974  14.2542  -6.2669  44.9552 175.3957]\n",
      "Weights: [-4.8288  0.8244 -1.244   0.102   0.1477]\n",
      "MSE loss: 87.1244\n",
      "Iteration: 79800\n",
      "Gradient: [ -4.1148  -2.5534 -13.8053 -13.5444  70.9472]\n",
      "Weights: [-4.8433  0.8223 -1.2418  0.1019  0.1478]\n",
      "MSE loss: 87.0501\n",
      "Iteration: 79900\n",
      "Gradient: [ -7.1001  -5.1244 -20.7541  28.5121 113.9406]\n",
      "Weights: [-4.841   0.821  -1.2406  0.1016  0.1478]\n",
      "MSE loss: 87.0484\n",
      "Iteration: 80000\n",
      "Gradient: [-0.9032 -4.014  11.7489 56.3762 64.0466]\n",
      "Weights: [-4.8307  0.8199 -1.2387  0.1007  0.1477]\n",
      "MSE loss: 87.1039\n",
      "Iteration: 80100\n",
      "Gradient: [   8.8231  -14.4323    9.4322  -14.6681 -200.4857]\n",
      "Weights: [-4.8241  0.8193 -1.2381  0.1004  0.1476]\n",
      "MSE loss: 87.2904\n",
      "Iteration: 80200\n",
      "Gradient: [ -6.6776  10.8306 -58.5402 -38.6075  91.3489]\n",
      "Weights: [-4.83    0.8136 -1.2385  0.1009  0.1477]\n",
      "MSE loss: 87.0442\n",
      "Iteration: 80300\n",
      "Gradient: [ -4.4262 -23.2208 -40.2005  -6.6435  87.7277]\n",
      "Weights: [-4.8284  0.8053 -1.2378  0.1015  0.1478]\n",
      "MSE loss: 87.0402\n",
      "Iteration: 80400\n",
      "Gradient: [  4.7633  -7.9237  22.474  -21.8941 -46.8817]\n",
      "Weights: [-4.8291  0.8126 -1.2367  0.1011  0.1476]\n",
      "MSE loss: 87.05\n",
      "Iteration: 80500\n",
      "Gradient: [ 12.8148  -0.3086 -17.9886  34.3459 -43.2821]\n",
      "Weights: [-4.8273  0.8087 -1.2372  0.1014  0.1475]\n",
      "MSE loss: 87.0722\n",
      "Iteration: 80600\n",
      "Gradient: [-3.3345  9.611  -4.1545 58.4352 63.9184]\n",
      "Weights: [-4.8396  0.8037 -1.2345  0.102   0.1475]\n",
      "MSE loss: 87.1922\n",
      "Iteration: 80700\n",
      "Gradient: [  4.1639  11.0473 -32.442   47.9323 136.8415]\n",
      "Weights: [-4.8198  0.8027 -1.2369  0.1023  0.1475]\n",
      "MSE loss: 87.0534\n",
      "Iteration: 80800\n",
      "Gradient: [-1.413880e+01  2.750000e-02  1.528500e+00 -8.927220e+01 -2.994838e+02]\n",
      "Weights: [-4.8267  0.7986 -1.2337  0.1013  0.1473]\n",
      "MSE loss: 87.4122\n",
      "Iteration: 80900\n",
      "Gradient: [ 6.8543 -1.7106 -7.8366 35.9679 -3.8159]\n",
      "Weights: [-4.8308  0.8039 -1.2335  0.1015  0.1474]\n",
      "MSE loss: 87.0178\n",
      "Iteration: 81000\n",
      "Gradient: [  2.4357   6.092  -13.835  -77.8072  90.8518]\n",
      "Weights: [-4.8291  0.8072 -1.2361  0.1011  0.1476]\n",
      "MSE loss: 87.0715\n",
      "Iteration: 81100\n",
      "Gradient: [ -7.509   -0.8342   1.2941  12.9526 -43.8315]\n",
      "Weights: [-4.835   0.8077 -1.2328  0.101   0.1476]\n",
      "MSE loss: 87.0604\n",
      "Iteration: 81200\n",
      "Gradient: [  4.778   -1.0964  43.989  -51.3607 -81.6067]\n",
      "Weights: [-4.8326  0.8097 -1.2336  0.1005  0.1476]\n",
      "MSE loss: 87.0357\n",
      "Iteration: 81300\n",
      "Gradient: [ -3.4419   5.0507  12.38   -63.3885  91.5703]\n",
      "Weights: [-4.8339  0.8066 -1.2328  0.101   0.1476]\n",
      "MSE loss: 87.0884\n",
      "Iteration: 81400\n",
      "Gradient: [   5.9549   -1.0794    1.1717  -29.2714 -169.9843]\n",
      "Weights: [-4.8481  0.812  -1.2346  0.1013  0.1475]\n",
      "MSE loss: 87.2358\n",
      "Iteration: 81500\n",
      "Gradient: [ -4.1607   7.4641 -29.6411  18.5301 236.0216]\n",
      "Weights: [-4.8465  0.8169 -1.2335  0.1003  0.1474]\n",
      "MSE loss: 87.1378\n",
      "Iteration: 81600\n",
      "Gradient: [  1.625   11.5443   3.2601 -40.4904 298.5959]\n",
      "Weights: [-4.8339  0.8113 -1.2353  0.1006  0.1478]\n",
      "MSE loss: 87.0403\n",
      "Iteration: 81700\n",
      "Gradient: [ 10.3662  -4.9319 -23.4555  47.6438 108.9091]\n",
      "Weights: [-4.8218  0.8049 -1.2356  0.1009  0.1478]\n",
      "MSE loss: 87.0657\n",
      "Iteration: 81800\n",
      "Gradient: [ 2.1898 14.2501 49.5195 91.5745 94.7123]\n",
      "Weights: [-4.827   0.8098 -1.2323  0.1001  0.1478]\n",
      "MSE loss: 87.3274\n",
      "Iteration: 81900\n",
      "Gradient: [-13.0536 -19.9228 -11.1679  -5.394  234.2634]\n",
      "Weights: [-4.8301  0.8063 -1.2309  0.0988  0.1478]\n",
      "MSE loss: 87.1015\n",
      "Iteration: 82000\n",
      "Gradient: [ -10.904     0.4599   -8.56   -147.5931  115.277 ]\n",
      "Weights: [-4.8313  0.8058 -1.2303  0.0988  0.1479]\n",
      "MSE loss: 87.0548\n",
      "Iteration: 82100\n",
      "Gradient: [ 8.1589 15.3425 -9.89   65.3385  2.4824]\n",
      "Weights: [-4.827   0.8096 -1.231   0.0986  0.148 ]\n",
      "MSE loss: 87.2025\n",
      "Iteration: 82200\n",
      "Gradient: [ -4.2434   4.6755 -10.6961  63.9514 202.5421]\n",
      "Weights: [-4.8368  0.8078 -1.2299  0.0976  0.1483]\n",
      "MSE loss: 87.0928\n",
      "Iteration: 82300\n",
      "Gradient: [ -2.1316  -1.6607 -45.2244 110.3222 213.5221]\n",
      "Weights: [-4.8341  0.8108 -1.2291  0.0974  0.1483]\n",
      "MSE loss: 87.1954\n",
      "Iteration: 82400\n",
      "Gradient: [  6.0118  -3.8715 -12.627  -96.6227 427.7853]\n",
      "Weights: [-4.8297  0.8148 -1.233   0.0969  0.1484]\n",
      "MSE loss: 87.1308\n",
      "Iteration: 82500\n",
      "Gradient: [   2.5751   -1.1553   -7.4563  -72.9928 -143.9159]\n",
      "Weights: [-4.8119  0.8018 -1.2343  0.098   0.1484]\n",
      "MSE loss: 87.2704\n",
      "Iteration: 82600\n",
      "Gradient: [ -10.8149   -1.8601    9.8126   -5.8881 -174.5946]\n",
      "Weights: [-4.8223  0.8    -1.2326  0.0988  0.1485]\n",
      "MSE loss: 87.0956\n",
      "Iteration: 82700\n",
      "Gradient: [   8.6955   19.28    -32.7574 -130.2273   36.1117]\n",
      "Weights: [-4.83    0.8135 -1.236   0.0981  0.1485]\n",
      "MSE loss: 87.0625\n",
      "Iteration: 82800\n",
      "Gradient: [  2.9194   6.7017 -14.0047  98.0529 105.5216]\n",
      "Weights: [-4.8272  0.8201 -1.2349  0.0983  0.1484]\n",
      "MSE loss: 87.6985\n",
      "Iteration: 82900\n",
      "Gradient: [   2.4721   -2.6202    0.4037    5.432  -182.1929]\n",
      "Weights: [-4.8197  0.8069 -1.2348  0.0981  0.1484]\n",
      "MSE loss: 87.1668\n",
      "Iteration: 83000\n",
      "Gradient: [  1.9539  19.1237  26.0855  24.0422 436.5187]\n",
      "Weights: [-4.8286  0.8127 -1.2336  0.0973  0.1485]\n",
      "MSE loss: 87.0959\n",
      "Iteration: 83100\n",
      "Gradient: [ 1.57000e-02 -9.55480e+00 -5.59680e+00 -5.94815e+01 -2.11689e+01]\n",
      "Weights: [-4.8171  0.8058 -1.2345  0.0979  0.1486]\n",
      "MSE loss: 87.1886\n",
      "Iteration: 83200\n",
      "Gradient: [ -2.8026   6.9995  -4.6203  -7.1756 179.7014]\n",
      "Weights: [-4.8111  0.7941 -1.2327  0.0983  0.1486]\n",
      "MSE loss: 87.2032\n",
      "Iteration: 83300\n",
      "Gradient: [ 10.8896   5.4739  42.7983  32.3408 307.9886]\n",
      "Weights: [-4.821   0.7979 -1.2311  0.0985  0.1485]\n",
      "MSE loss: 87.1187\n",
      "Iteration: 83400\n",
      "Gradient: [ -9.9706  12.3021 -28.1332 -45.4125  39.9568]\n",
      "Weights: [-4.8216  0.7999 -1.2328  0.0987  0.1483]\n",
      "MSE loss: 87.0878\n",
      "Iteration: 83500\n",
      "Gradient: [ -0.5389   9.2137  45.039  -20.1144 -26.3751]\n",
      "Weights: [-4.8188  0.8006 -1.2331  0.0986  0.1485]\n",
      "MSE loss: 87.1205\n",
      "Iteration: 83600\n",
      "Gradient: [ -1.8996  26.9544  -1.8432 148.3972 -14.0218]\n",
      "Weights: [-4.8058  0.7945 -1.2311  0.0988  0.1486]\n",
      "MSE loss: 87.7891\n",
      "Iteration: 83700\n",
      "Gradient: [  -2.3305  -20.3361  -36.3703   -8.9061 -155.8081]\n",
      "Weights: [-4.8201  0.7862 -1.2281  0.0983  0.1486]\n",
      "MSE loss: 87.1813\n",
      "Iteration: 83800\n",
      "Gradient: [  6.5699   1.3659  17.0306 -74.0416 130.4497]\n",
      "Weights: [-4.828   0.7949 -1.2284  0.0976  0.1485]\n",
      "MSE loss: 87.1614\n",
      "Iteration: 83900\n",
      "Gradient: [  1.6839  -0.3249   7.0299 -27.279  111.5702]\n",
      "Weights: [-4.8409  0.8093 -1.2304  0.0974  0.1483]\n",
      "MSE loss: 87.1891\n",
      "Iteration: 84000\n",
      "Gradient: [  3.7349  -9.4697 -41.9186  39.4157 -38.3352]\n",
      "Weights: [-4.8383  0.8167 -1.231   0.0971  0.1482]\n",
      "MSE loss: 87.1011\n",
      "Iteration: 84100\n",
      "Gradient: [  -1.0292    3.9738    6.2245   74.7103 -176.5176]\n",
      "Weights: [-4.8456  0.8104 -1.2307  0.0976  0.1485]\n",
      "MSE loss: 87.2387\n",
      "Iteration: 84200\n",
      "Gradient: [ -0.6913 -12.818  -18.1357 -22.9376 -60.2247]\n",
      "Weights: [-4.8612  0.8172 -1.2298  0.0976  0.1484]\n",
      "MSE loss: 87.6136\n",
      "Iteration: 84300\n",
      "Gradient: [  1.6615  11.8629  28.5639 -84.9985   0.4538]\n",
      "Weights: [-4.8322  0.8125 -1.2329  0.0973  0.1484]\n",
      "MSE loss: 87.0963\n",
      "Iteration: 84400\n",
      "Gradient: [ -6.3987   3.611   18.8418 -76.0547  -4.4309]\n",
      "Weights: [-4.8278  0.8053 -1.2341  0.0981  0.1485]\n",
      "MSE loss: 87.0932\n",
      "Iteration: 84500\n",
      "Gradient: [ -10.2429  -16.8175   22.1285  -50.4344 -210.0467]\n",
      "Weights: [-4.8469  0.8171 -1.2336  0.098   0.1485]\n",
      "MSE loss: 87.1982\n",
      "Iteration: 84600\n",
      "Gradient: [   1.8819  -11.0319   13.1933  -37.258  -112.4493]\n",
      "Weights: [-4.825   0.8042 -1.2301  0.0977  0.1484]\n",
      "MSE loss: 87.1268\n",
      "Iteration: 84700\n",
      "Gradient: [  4.7663   7.7224   8.8466  40.9024 -49.3631]\n",
      "Weights: [-4.8252  0.8049 -1.2306  0.0975  0.1487]\n",
      "MSE loss: 87.2639\n",
      "Iteration: 84800\n",
      "Gradient: [ -6.188   -1.4564 -25.8354 -85.734  -53.799 ]\n",
      "Weights: [-4.8299  0.7997 -1.2324  0.0976  0.1489]\n",
      "MSE loss: 87.1876\n",
      "Iteration: 84900\n",
      "Gradient: [  -4.4688   -8.7599   11.6866   35.9946 -215.937 ]\n",
      "Weights: [-4.8378  0.8115 -1.2329  0.0975  0.1488]\n",
      "MSE loss: 87.135\n",
      "Iteration: 85000\n",
      "Gradient: [  -0.8061   -4.351   -28.5162 -112.3042  127.9452]\n",
      "Weights: [-4.8314  0.8146 -1.2352  0.0974  0.1487]\n",
      "MSE loss: 87.0756\n",
      "Iteration: 85100\n",
      "Gradient: [ -6.0054  -3.9317  -2.1474  46.108  194.0386]\n",
      "Weights: [-4.8362  0.8111 -1.234   0.098   0.1487]\n",
      "MSE loss: 87.098\n",
      "Iteration: 85200\n",
      "Gradient: [   1.2673    1.2604   33.1035  -19.7848 -176.8141]\n",
      "Weights: [-4.8429  0.8129 -1.2329  0.0978  0.1484]\n",
      "MSE loss: 87.1662\n",
      "Iteration: 85300\n",
      "Gradient: [  4.4342 -11.7278 -32.097  127.1285  40.143 ]\n",
      "Weights: [-4.8399  0.8143 -1.2323  0.098   0.1484]\n",
      "MSE loss: 87.1103\n",
      "Iteration: 85400\n",
      "Gradient: [   5.2957   -2.6682  -34.6797   74.1292 -227.8432]\n",
      "Weights: [-4.8353  0.8108 -1.2312  0.0971  0.1485]\n",
      "MSE loss: 87.0815\n",
      "Iteration: 85500\n",
      "Gradient: [ 1.7658  0.9034  9.7791 37.863  30.0433]\n",
      "Weights: [-4.8266  0.809  -1.2325  0.098   0.1485]\n",
      "MSE loss: 87.1866\n",
      "Iteration: 85600\n",
      "Gradient: [ -4.029   22.0633  13.2956 104.9448 262.7436]\n",
      "Weights: [-4.8103  0.8116 -1.2333  0.0979  0.1483]\n",
      "MSE loss: 87.9995\n",
      "Iteration: 85700\n",
      "Gradient: [ -5.9959 -12.3986   3.1975 -24.3065 104.7284]\n",
      "Weights: [-4.8356  0.8084 -1.2326  0.0979  0.1483]\n",
      "MSE loss: 87.1701\n",
      "Iteration: 85800\n",
      "Gradient: [  -3.5454    1.003   -21.6042   49.1058 -182.0558]\n",
      "Weights: [-4.8377  0.819  -1.2305  0.0973  0.1478]\n",
      "MSE loss: 87.1564\n",
      "Iteration: 85900\n",
      "Gradient: [ -2.3645  -1.1779  19.2921  24.6045 -91.9529]\n",
      "Weights: [-4.8364  0.8138 -1.2302  0.0978  0.1479]\n",
      "MSE loss: 87.0999\n",
      "Iteration: 86000\n",
      "Gradient: [  8.5218  13.1586 -21.8973 -13.645   45.3197]\n",
      "Weights: [-4.832   0.8081 -1.231   0.099   0.1479]\n",
      "MSE loss: 87.0761\n",
      "Iteration: 86100\n",
      "Gradient: [  6.0751   2.7598  -2.0585 -32.3317 245.6911]\n",
      "Weights: [-4.8237  0.804  -1.2316  0.0996  0.1481]\n",
      "MSE loss: 87.2482\n",
      "Iteration: 86200\n",
      "Gradient: [  0.8586   6.2173 -36.1285 -48.4396 -59.7143]\n",
      "Weights: [-4.8405  0.8011 -1.2315  0.0999  0.148 ]\n",
      "MSE loss: 87.2757\n",
      "Iteration: 86300\n",
      "Gradient: [   4.1522   -7.9223   -5.6574  -58.298  -173.2811]\n",
      "Weights: [-4.8251  0.8003 -1.2311  0.0996  0.1479]\n",
      "MSE loss: 87.0513\n",
      "Iteration: 86400\n",
      "Gradient: [ 4.3894 17.886  10.7278 -8.0375 47.5766]\n",
      "Weights: [-4.8342  0.8081 -1.2317  0.1001  0.1479]\n",
      "MSE loss: 87.1912\n",
      "Iteration: 86500\n",
      "Gradient: [-5.6077 -2.926  56.2476 55.1951 -5.7279]\n",
      "Weights: [-4.836   0.8095 -1.2332  0.1002  0.1479]\n",
      "MSE loss: 87.1256\n",
      "Iteration: 86600\n",
      "Gradient: [  -0.272     2.3834   31.8549  -32.28   -114.1358]\n",
      "Weights: [-4.8171  0.8009 -1.2317  0.1     0.1478]\n",
      "MSE loss: 87.2029\n",
      "Iteration: 86700\n",
      "Gradient: [ -0.276    4.8213  -1.847   37.2807 169.4931]\n",
      "Weights: [-4.8317  0.8    -1.2316  0.1     0.148 ]\n",
      "MSE loss: 87.0822\n",
      "Iteration: 86800\n",
      "Gradient: [ 12.7816   3.4087  23.9334 144.3707 258.9986]\n",
      "Weights: [-4.8133  0.7936 -1.2298  0.1006  0.1478]\n",
      "MSE loss: 87.3946\n",
      "Iteration: 86900\n",
      "Gradient: [ -1.5963  18.6458  19.6374   6.4352 146.3976]\n",
      "Weights: [-4.8315  0.8025 -1.232   0.1006  0.1479]\n",
      "MSE loss: 87.1106\n",
      "Iteration: 87000\n",
      "Gradient: [ -4.8479   2.9934  17.7498  46.8984 -50.7132]\n",
      "Weights: [-4.8388  0.8028 -1.2321  0.1008  0.1478]\n",
      "MSE loss: 87.1476\n",
      "Iteration: 87100\n",
      "Gradient: [   3.2759   -3.2604  -86.7736   22.1348 -227.1111]\n",
      "Weights: [-4.8278  0.7986 -1.2324  0.1005  0.1477]\n",
      "MSE loss: 87.1041\n",
      "Iteration: 87200\n",
      "Gradient: [  -5.9818   -2.5246  -52.4748 -114.8097 -191.6435]\n",
      "Weights: [-4.8184  0.792  -1.2325  0.1005  0.1478]\n",
      "MSE loss: 87.2053\n",
      "Iteration: 87300\n",
      "Gradient: [  7.5524  18.2824 -25.7417  91.8004 148.942 ]\n",
      "Weights: [-4.8041  0.7852 -1.2313  0.1015  0.1479]\n",
      "MSE loss: 87.3749\n",
      "Iteration: 87400\n",
      "Gradient: [ -13.3489  -10.8926    1.0132   45.1996 -121.7629]\n",
      "Weights: [-4.8306  0.7937 -1.2323  0.1017  0.1478]\n",
      "MSE loss: 87.1624\n",
      "Iteration: 87500\n",
      "Gradient: [  -6.5283   -1.4883    7.4647  -40.7754 -118.5929]\n",
      "Weights: [-4.8093  0.7918 -1.2371  0.1021  0.148 ]\n",
      "MSE loss: 87.1551\n",
      "Iteration: 87600\n",
      "Gradient: [  -3.2821   12.2659  -18.2437   37.2015 -102.3781]\n",
      "Weights: [-4.8229  0.803  -1.2378  0.1016  0.1479]\n",
      "MSE loss: 87.0343\n",
      "Iteration: 87700\n",
      "Gradient: [ -5.6577  19.0354  10.4755 -87.8275  52.765 ]\n",
      "Weights: [-4.8183  0.8013 -1.2354  0.1011  0.1478]\n",
      "MSE loss: 87.0827\n",
      "Iteration: 87800\n",
      "Gradient: [  -5.021     0.8573  -17.54    -77.2788 -193.2956]\n",
      "Weights: [-4.8317  0.7968 -1.2336  0.1019  0.148 ]\n",
      "MSE loss: 87.2243\n",
      "Iteration: 87900\n",
      "Gradient: [ -6.5279 -10.9014 -10.8441 -37.8111  89.2399]\n",
      "Weights: [-4.8229  0.7981 -1.2357  0.1021  0.1479]\n",
      "MSE loss: 87.0926\n",
      "Iteration: 88000\n",
      "Gradient: [ -4.123  -16.0242 -12.2487 -61.9291 153.0654]\n",
      "Weights: [-4.8352  0.7967 -1.2356  0.1023  0.1477]\n",
      "MSE loss: 87.3426\n",
      "Iteration: 88100\n",
      "Gradient: [  -4.1556  -15.9415   19.147   -16.1287 -327.2257]\n",
      "Weights: [-4.8192  0.7978 -1.2371  0.1021  0.1476]\n",
      "MSE loss: 87.1036\n",
      "Iteration: 88200\n",
      "Gradient: [  -4.6208   -7.6855    5.6398  -65.1712 -193.2923]\n",
      "Weights: [-4.8255  0.8011 -1.2356  0.1022  0.1473]\n",
      "MSE loss: 87.0669\n",
      "Iteration: 88300\n",
      "Gradient: [ -1.7354  -2.7045   7.099  -17.7862 251.7434]\n",
      "Weights: [-4.8335  0.8051 -1.2358  0.1022  0.1475]\n",
      "MSE loss: 87.034\n",
      "Iteration: 88400\n",
      "Gradient: [   3.8977    5.7085  -28.6217 -122.4197  -31.2229]\n",
      "Weights: [-4.8213  0.8036 -1.2377  0.1023  0.1473]\n",
      "MSE loss: 87.1078\n",
      "Iteration: 88500\n",
      "Gradient: [ -0.6913  22.6103 -22.4988  34.1749 -73.8856]\n",
      "Weights: [-4.8325  0.8155 -1.2407  0.1025  0.1476]\n",
      "MSE loss: 86.995\n",
      "Iteration: 88600\n",
      "Gradient: [ -12.2895   -8.7642  -17.9447  -15.1076 -190.8018]\n",
      "Weights: [-4.8325  0.812  -1.2396  0.1019  0.1474]\n",
      "MSE loss: 87.1891\n",
      "Iteration: 88700\n",
      "Gradient: [ 0.3023 -5.635  20.0498 38.9933 17.6024]\n",
      "Weights: [-4.8359  0.8088 -1.2379  0.1024  0.1474]\n",
      "MSE loss: 87.0883\n",
      "Iteration: 88800\n",
      "Gradient: [  -0.9833    3.3985   16.0488  -65.2936 -106.5082]\n",
      "Weights: [-4.8264  0.808  -1.239   0.1033  0.1473]\n",
      "MSE loss: 86.9976\n",
      "Iteration: 88900\n",
      "Gradient: [ -6.5007  -6.5883 -17.1931 -47.7291 185.1771]\n",
      "Weights: [-4.8361  0.8141 -1.2373  0.1022  0.1471]\n",
      "MSE loss: 87.0768\n",
      "Iteration: 89000\n",
      "Gradient: [ -3.2467 -11.2648  11.0309 -12.9972 -42.4688]\n",
      "Weights: [-4.8427  0.8235 -1.2382  0.1022  0.1472]\n",
      "MSE loss: 87.1023\n",
      "Iteration: 89100\n",
      "Gradient: [   5.5592    7.719    18.159    52.143  -113.5388]\n",
      "Weights: [-4.8195  0.8102 -1.2409  0.1033  0.1472]\n",
      "MSE loss: 87.0998\n",
      "Iteration: 89200\n",
      "Gradient: [  -0.6756   13.0282  -63.8992  -86.8449 -365.6059]\n",
      "Weights: [-4.8073  0.7854 -1.236   0.1042  0.1472]\n",
      "MSE loss: 87.1423\n",
      "Iteration: 89300\n",
      "Gradient: [  3.1411 -20.1124  21.2341  24.605  -56.1613]\n",
      "Weights: [-4.8108  0.7811 -1.2333  0.1035  0.1473]\n",
      "MSE loss: 87.1528\n",
      "Iteration: 89400\n",
      "Gradient: [ -5.5307  -5.9849 -19.8768  83.336  -49.4738]\n",
      "Weights: [-4.8211  0.798  -1.2376  0.1037  0.1474]\n",
      "MSE loss: 87.023\n",
      "Iteration: 89500\n",
      "Gradient: [  0.3405   1.9807   4.9849 -32.9987 141.7852]\n",
      "Weights: [-4.831   0.7947 -1.2343  0.1031  0.1472]\n",
      "MSE loss: 87.1953\n",
      "Iteration: 89600\n",
      "Gradient: [ 8.4478  5.321  -4.5458 87.7072 18.325 ]\n",
      "Weights: [-4.8234  0.7921 -1.2323  0.1027  0.1473]\n",
      "MSE loss: 87.0341\n",
      "Iteration: 89700\n",
      "Gradient: [  -7.3276    1.5735  -18.6729 -124.7916   51.1142]\n",
      "Weights: [-4.8158  0.7926 -1.2321  0.1024  0.1474]\n",
      "MSE loss: 87.1063\n",
      "Iteration: 89800\n",
      "Gradient: [-10.3025  -4.1862   2.8805 -54.1235  -0.0603]\n",
      "Weights: [-4.8214  0.7957 -1.2353  0.1028  0.1474]\n",
      "MSE loss: 87.0282\n",
      "Iteration: 89900\n",
      "Gradient: [  4.9135   2.368   13.5853  35.616  299.6977]\n",
      "Weights: [-4.8073  0.788  -1.2343  0.103   0.1476]\n",
      "MSE loss: 87.2311\n",
      "Iteration: 90000\n",
      "Gradient: [  3.4339 -25.1156 -24.4726 -55.3859 196.3192]\n",
      "Weights: [-4.83    0.7918 -1.2327  0.1023  0.1474]\n",
      "MSE loss: 87.2697\n",
      "Iteration: 90100\n",
      "Gradient: [  -9.2069  -15.2304   26.0635    5.5645 -145.2506]\n",
      "Weights: [-4.8094  0.7905 -1.2347  0.1029  0.1472]\n",
      "MSE loss: 87.1462\n",
      "Iteration: 90200\n",
      "Gradient: [  8.7146  10.9434 -23.119   19.6324  63.4393]\n",
      "Weights: [-4.8229  0.792  -1.2322  0.1037  0.1472]\n",
      "MSE loss: 87.1189\n",
      "Iteration: 90300\n",
      "Gradient: [ 14.7663  24.9793   9.6729  15.6628 107.8419]\n",
      "Weights: [-4.8034  0.7923 -1.2328  0.1035  0.147 ]\n",
      "MSE loss: 87.6846\n",
      "Iteration: 90400\n",
      "Gradient: [  -9.8273   -4.1251  -21.1804 -101.3209    2.5995]\n",
      "Weights: [-4.8295  0.7993 -1.233   0.1022  0.1471]\n",
      "MSE loss: 87.1039\n",
      "Iteration: 90500\n",
      "Gradient: [  2.2289   9.5594  -0.8202 122.8839 -31.6323]\n",
      "Weights: [-4.8376  0.8084 -1.2345  0.1027  0.1472]\n",
      "MSE loss: 87.0637\n",
      "Iteration: 90600\n",
      "Gradient: [  10.1511   13.0746   -2.6586   -3.5887 -107.7788]\n",
      "Weights: [-4.8313  0.805  -1.2365  0.1025  0.1473]\n",
      "MSE loss: 87.0434\n",
      "Iteration: 90700\n",
      "Gradient: [  0.6713   9.7857  17.7174  65.2233 145.5995]\n",
      "Weights: [-4.8356  0.811  -1.2372  0.103   0.1473]\n",
      "MSE loss: 87.0514\n",
      "Iteration: 90800\n",
      "Gradient: [11.113  15.5648 -7.6663 -4.9159 97.3273]\n",
      "Weights: [-4.8099  0.803  -1.2369  0.1022  0.1472]\n",
      "MSE loss: 87.3266\n",
      "Iteration: 90900\n",
      "Gradient: [  -4.7541   -2.7476   23.9723  -18.1622 -165.3128]\n",
      "Weights: [-4.8255  0.8095 -1.2379  0.1028  0.1472]\n",
      "MSE loss: 87.033\n",
      "Iteration: 91000\n",
      "Gradient: [ -2.82     5.9356  25.2224 113.194  -99.1607]\n",
      "Weights: [-4.8274  0.8066 -1.2371  0.1042  0.147 ]\n",
      "MSE loss: 87.1379\n",
      "Iteration: 91100\n",
      "Gradient: [ -0.2233  -6.5514 -18.6957 100.2786 151.436 ]\n",
      "Weights: [-4.8229  0.794  -1.2346  0.1039  0.1472]\n",
      "MSE loss: 87.0516\n",
      "Iteration: 91200\n",
      "Gradient: [ 10.2961  15.5073  12.4987  92.2296 142.6355]\n",
      "Weights: [-4.8306  0.8089 -1.2344  0.1033  0.147 ]\n",
      "MSE loss: 87.2406\n",
      "Iteration: 91300\n",
      "Gradient: [ 0.1053  6.167  -1.1891 60.939  62.5895]\n",
      "Weights: [-4.8476  0.8076 -1.2335  0.1029  0.1471]\n",
      "MSE loss: 87.2396\n",
      "Iteration: 91400\n",
      "Gradient: [   6.0978    2.2585  -40.6908  -20.2801 -251.4548]\n",
      "Weights: [-4.8245  0.8074 -1.2346  0.1026  0.147 ]\n",
      "MSE loss: 87.1008\n",
      "Iteration: 91500\n",
      "Gradient: [  5.8855   1.888   61.2967  -3.7421 264.3072]\n",
      "Weights: [-4.8225  0.8    -1.2357  0.1032  0.1471]\n",
      "MSE loss: 87.0144\n",
      "Iteration: 91600\n",
      "Gradient: [ -11.062     1.849    -9.2025  -23.6241 -140.618 ]\n",
      "Weights: [-4.8374  0.8084 -1.2373  0.1033  0.1472]\n",
      "MSE loss: 87.0324\n",
      "Iteration: 91700\n",
      "Gradient: [-14.2197 -10.5791  31.89   -87.6923 -75.1171]\n",
      "Weights: [-4.8314  0.8014 -1.2344  0.1027  0.1473]\n",
      "MSE loss: 87.0253\n",
      "Iteration: 91800\n",
      "Gradient: [ -3.6235  -3.7018 -23.3324 -23.7026 -27.3575]\n",
      "Weights: [-4.8484  0.8097 -1.233   0.1024  0.1471]\n",
      "MSE loss: 87.2205\n",
      "Iteration: 91900\n",
      "Gradient: [  2.3849  -0.3817  23.543  -12.0865 116.3727]\n",
      "Weights: [-4.8352  0.8109 -1.2328  0.1024  0.1469]\n",
      "MSE loss: 87.1276\n",
      "Iteration: 92000\n",
      "Gradient: [  6.4459  15.4195 -13.341  131.8131 -14.9423]\n",
      "Weights: [-4.8255  0.8088 -1.2354  0.1032  0.1468]\n",
      "MSE loss: 87.0894\n",
      "Iteration: 92100\n",
      "Gradient: [  12.0649  -11.3382   21.877   -32.6852 -419.9673]\n",
      "Weights: [-4.8115  0.8008 -1.2396  0.1041  0.1469]\n",
      "MSE loss: 87.2045\n",
      "Iteration: 92200\n",
      "Gradient: [ -4.8008   0.6723 -61.1199  57.5259  58.0882]\n",
      "Weights: [-4.8443  0.8145 -1.24    0.1045  0.1468]\n",
      "MSE loss: 87.1034\n",
      "Iteration: 92300\n",
      "Gradient: [  1.3653 -13.3396  37.0993   9.1498 -40.2408]\n",
      "Weights: [-4.8123  0.8005 -1.2395  0.1048  0.1467]\n",
      "MSE loss: 87.1567\n",
      "Iteration: 92400\n",
      "Gradient: [ -1.1396   0.8488 -10.6328 -29.4703  42.955 ]\n",
      "Weights: [-4.8268  0.801  -1.2384  0.105   0.1467]\n",
      "MSE loss: 87.0395\n",
      "Iteration: 92500\n",
      "Gradient: [  5.9175   4.4759 -22.2827  -0.1995  49.7997]\n",
      "Weights: [-4.8347  0.8142 -1.2404  0.1049  0.1467]\n",
      "MSE loss: 86.9744\n",
      "Iteration: 92600\n",
      "Gradient: [  -3.9784    2.3875  -11.8392  -20.3572 -244.6021]\n",
      "Weights: [-4.8473  0.8193 -1.2419  0.1046  0.1465]\n",
      "MSE loss: 87.3862\n",
      "Iteration: 92700\n",
      "Gradient: [  7.7884 -15.1459 -36.3173 -12.5031 -23.4893]\n",
      "Weights: [-4.8317  0.8161 -1.2403  0.1043  0.1465]\n",
      "MSE loss: 87.0616\n",
      "Iteration: 92800\n",
      "Gradient: [ -7.354   -6.8011 -51.8326  23.5803   4.9601]\n",
      "Weights: [-4.8543  0.8258 -1.2442  0.105   0.1467]\n",
      "MSE loss: 87.2614\n",
      "Iteration: 92900\n",
      "Gradient: [  0.31    -9.3091  -6.4578 235.8954 298.3063]\n",
      "Weights: [-4.8394  0.8157 -1.2433  0.1053  0.1469]\n",
      "MSE loss: 87.0025\n",
      "Iteration: 93000\n",
      "Gradient: [-3.388   3.4627 10.8731 25.2224 33.0985]\n",
      "Weights: [-4.8358  0.8152 -1.2415  0.1048  0.147 ]\n",
      "MSE loss: 87.0488\n",
      "Iteration: 93100\n",
      "Gradient: [ -2.0733  -7.7889 -42.0195 -89.8977  27.1404]\n",
      "Weights: [-4.8383  0.8185 -1.2415  0.1038  0.1471]\n",
      "MSE loss: 86.9864\n",
      "Iteration: 93200\n",
      "Gradient: [ -3.4693 -14.6589  24.1624 -32.4477  26.632 ]\n",
      "Weights: [-4.8454  0.8175 -1.2408  0.1044  0.1471]\n",
      "MSE loss: 87.0843\n",
      "Iteration: 93300\n",
      "Gradient: [  -2.9632   11.4614  -42.8385  -32.5339 -201.2877]\n",
      "Weights: [-4.8308  0.8044 -1.2412  0.1041  0.1473]\n",
      "MSE loss: 87.1681\n",
      "Iteration: 93400\n",
      "Gradient: [  -5.4347  -15.5868  -24.2864  -75.0905 -222.4983]\n",
      "Weights: [-4.8314  0.8022 -1.2405  0.1039  0.1475]\n",
      "MSE loss: 87.1421\n",
      "Iteration: 93500\n",
      "Gradient: [ -5.3311  -8.8505 -22.0826 -35.6763  19.0887]\n",
      "Weights: [-4.8388  0.8093 -1.2389  0.1033  0.1471]\n",
      "MSE loss: 87.2474\n",
      "Iteration: 93600\n",
      "Gradient: [  1.0161  -5.6801   9.3207   8.2221 557.2866]\n",
      "Weights: [-4.8179  0.8031 -1.2376  0.1035  0.1472]\n",
      "MSE loss: 87.1173\n",
      "Iteration: 93700\n",
      "Gradient: [  -2.5276   -3.1864  -15.4976   49.2457 -184.9349]\n",
      "Weights: [-4.8083  0.7876 -1.235   0.1039  0.147 ]\n",
      "MSE loss: 87.168\n",
      "Iteration: 93800\n",
      "Gradient: [  -6.3837  -25.1955  -26.2194 -212.9391  -22.1421]\n",
      "Weights: [-4.8381  0.7989 -1.2333  0.1037  0.1469]\n",
      "MSE loss: 87.179\n",
      "Iteration: 93900\n",
      "Gradient: [ -6.9107 -15.4744  -6.3593 -14.0508  24.2035]\n",
      "Weights: [-4.8313  0.7986 -1.2334  0.1037  0.1468]\n",
      "MSE loss: 87.0368\n",
      "Iteration: 94000\n",
      "Gradient: [  3.3205 -22.114   59.1132 -50.2055 -75.9599]\n",
      "Weights: [-4.8248  0.7942 -1.2355  0.1041  0.147 ]\n",
      "MSE loss: 87.0605\n",
      "Iteration: 94100\n",
      "Gradient: [ -1.9372   1.0386 -46.3566  62.7753 103.5676]\n",
      "Weights: [-4.8164  0.7947 -1.2337  0.1029  0.1472]\n",
      "MSE loss: 87.0728\n",
      "Iteration: 94200\n",
      "Gradient: [  1.9658   4.1783  18.1407 -69.518   86.2139]\n",
      "Weights: [-4.8282  0.8015 -1.2313  0.1019  0.1471]\n",
      "MSE loss: 87.0351\n",
      "Iteration: 94300\n",
      "Gradient: [  1.4492  25.6912   7.3937  30.2528 229.0258]\n",
      "Weights: [-4.8341  0.8077 -1.232   0.1021  0.1471]\n",
      "MSE loss: 87.0943\n",
      "Iteration: 94400\n",
      "Gradient: [  -9.7095   -5.8239    9.8258 -115.9087  330.4755]\n",
      "Weights: [-4.836   0.8    -1.2336  0.1021  0.1472]\n",
      "MSE loss: 87.3007\n",
      "Iteration: 94500\n",
      "Gradient: [  -4.3983   11.1085    9.3531  -61.1566 -347.6805]\n",
      "Weights: [-4.8258  0.7923 -1.2337  0.102   0.1475]\n",
      "MSE loss: 87.2666\n",
      "Iteration: 94600\n",
      "Gradient: [  2.0167  -8.2434 -29.4746  -7.075  -46.8012]\n",
      "Weights: [-4.8269  0.7932 -1.231   0.1014  0.1477]\n",
      "MSE loss: 87.0745\n",
      "Iteration: 94700\n",
      "Gradient: [ 11.9698  -0.654    7.5769 110.1872 212.6456]\n",
      "Weights: [-4.8114  0.7973 -1.2306  0.1007  0.1475]\n",
      "MSE loss: 87.3342\n",
      "Iteration: 94800\n",
      "Gradient: [   8.5625  -12.5242  -28.355   -50.7911 -199.2807]\n",
      "Weights: [-4.8217  0.7872 -1.231   0.1011  0.1477]\n",
      "MSE loss: 87.2007\n",
      "Iteration: 94900\n",
      "Gradient: [  2.9385  17.5412  14.8243 -22.5361  79.4349]\n",
      "Weights: [-4.8218  0.7983 -1.2309  0.101   0.1478]\n",
      "MSE loss: 87.3193\n",
      "Iteration: 95000\n",
      "Gradient: [  2.4674   8.5322  12.8479 -52.7817 278.0399]\n",
      "Weights: [-4.8189  0.7949 -1.2341  0.1016  0.1478]\n",
      "MSE loss: 87.0706\n",
      "Iteration: 95100\n",
      "Gradient: [  -2.3122    5.9684  -22.3471   22.2024 -144.9422]\n",
      "Weights: [-4.8187  0.7972 -1.2368  0.102   0.1477]\n",
      "MSE loss: 87.0954\n",
      "Iteration: 95200\n",
      "Gradient: [  0.5069 -16.6997 -42.1078  -6.1385 209.6696]\n",
      "Weights: [-4.8224  0.8051 -1.2368  0.1013  0.1478]\n",
      "MSE loss: 87.0514\n",
      "Iteration: 95300\n",
      "Gradient: [-11.9294   4.5801  16.1528  39.7283 -67.7341]\n",
      "Weights: [-4.8252  0.8098 -1.239   0.1009  0.1479]\n",
      "MSE loss: 87.0435\n",
      "Iteration: 95400\n",
      "Gradient: [  -0.7079   -9.4866  -16.4048   16.0902 -204.8873]\n",
      "Weights: [-4.8313  0.8087 -1.2383  0.1008  0.1479]\n",
      "MSE loss: 87.1087\n",
      "Iteration: 95500\n",
      "Gradient: [ -7.8074 -12.5622  12.7707   2.7742   3.9135]\n",
      "Weights: [-4.8252  0.7957 -1.2366  0.1006  0.1481]\n",
      "MSE loss: 87.4211\n",
      "Iteration: 95600\n",
      "Gradient: [  3.0363   6.3658  33.6062  51.9731 366.8667]\n",
      "Weights: [-4.8104  0.7962 -1.2346  0.1006  0.1482]\n",
      "MSE loss: 87.2891\n",
      "Iteration: 95700\n",
      "Gradient: [ -0.5196   0.5642  24.991  -23.4642 -99.901 ]\n",
      "Weights: [-4.8366  0.8174 -1.2383  0.1003  0.148 ]\n",
      "MSE loss: 87.0287\n",
      "Iteration: 95800\n",
      "Gradient: [  7.8743   8.4338  10.6889  81.6886 289.5211]\n",
      "Weights: [-4.8268  0.8217 -1.241   0.101   0.148 ]\n",
      "MSE loss: 87.3521\n",
      "Iteration: 95900\n",
      "Gradient: [ -1.1433  -9.7396 -41.3359 -17.0193 -65.8298]\n",
      "Weights: [-4.8295  0.8096 -1.2382  0.1011  0.1479]\n",
      "MSE loss: 87.0165\n",
      "Iteration: 96000\n",
      "Gradient: [-10.9709 -15.4578  36.0016 -24.2359  24.7812]\n",
      "Weights: [-4.8393  0.8027 -1.2359  0.1008  0.148 ]\n",
      "MSE loss: 87.4306\n",
      "Iteration: 96100\n",
      "Gradient: [   9.9866    1.5612   50.9953   -7.0864 -186.5697]\n",
      "Weights: [-4.8281  0.8062 -1.2346  0.1005  0.1478]\n",
      "MSE loss: 87.0284\n",
      "Iteration: 96200\n",
      "Gradient: [  -9.3487    9.3933  -45.9643   24.3653 -208.0521]\n",
      "Weights: [-4.8251  0.8013 -1.2313  0.1     0.1476]\n",
      "MSE loss: 87.0643\n",
      "Iteration: 96300\n",
      "Gradient: [ -2.9441  10.845   -3.5404 -13.5209 253.5355]\n",
      "Weights: [-4.8155  0.7962 -1.2324  0.1012  0.1476]\n",
      "MSE loss: 87.1242\n",
      "Iteration: 96400\n",
      "Gradient: [  5.1767   3.2469  13.2046  61.4467 -13.1565]\n",
      "Weights: [-4.8373  0.8043 -1.2312  0.1012  0.1474]\n",
      "MSE loss: 87.0738\n",
      "Iteration: 96500\n",
      "Gradient: [  -4.6236   -9.9506  105.329   -17.6621 -291.2512]\n",
      "Weights: [-4.8285  0.7971 -1.2305  0.1006  0.1474]\n",
      "MSE loss: 87.1568\n",
      "Iteration: 96600\n",
      "Gradient: [  5.9835  20.5722 -11.5714 160.0873 -92.0389]\n",
      "Weights: [-4.845   0.8117 -1.2309  0.1006  0.1474]\n",
      "MSE loss: 87.1358\n",
      "Iteration: 96700\n",
      "Gradient: [  -2.5639    4.2518  -14.1629  -98.7311 -167.46  ]\n",
      "Weights: [-4.8333  0.8034 -1.2352  0.101   0.1475]\n",
      "MSE loss: 87.4307\n",
      "Iteration: 96800\n",
      "Gradient: [ 2.7349  5.1444 22.4857 69.4397 32.1663]\n",
      "Weights: [-4.8441  0.8126 -1.2346  0.1007  0.1475]\n",
      "MSE loss: 87.2002\n",
      "Iteration: 96900\n",
      "Gradient: [  -2.0782    9.6458  -50.9806  -46.6373 -120.8104]\n",
      "Weights: [-4.8359  0.8099 -1.2332  0.1004  0.1474]\n",
      "MSE loss: 87.0731\n",
      "Iteration: 97000\n",
      "Gradient: [  3.3578  16.7112  39.2645 159.0569 -66.3424]\n",
      "Weights: [-4.8169  0.8076 -1.2346  0.1009  0.1475]\n",
      "MSE loss: 87.3014\n",
      "Iteration: 97100\n",
      "Gradient: [  4.7085   4.7993  11.1017 -30.5913 -22.6743]\n",
      "Weights: [-4.8239  0.8016 -1.2354  0.1017  0.1476]\n",
      "MSE loss: 87.0232\n",
      "Iteration: 97200\n",
      "Gradient: [  8.7935   8.9043  -2.6053  12.3613 102.7229]\n",
      "Weights: [-4.8278  0.8112 -1.2366  0.1017  0.1474]\n",
      "MSE loss: 87.0653\n",
      "Iteration: 97300\n",
      "Gradient: [   5.3259   20.7716   16.2009 -105.4899   37.0625]\n",
      "Weights: [-4.8198  0.8133 -1.2367  0.1009  0.1475]\n",
      "MSE loss: 87.2843\n",
      "Iteration: 97400\n",
      "Gradient: [  0.3572 -11.8755 -39.6446  11.2823 -35.1916]\n",
      "Weights: [-4.8398  0.8084 -1.2337  0.1012  0.1475]\n",
      "MSE loss: 87.0911\n",
      "Iteration: 97500\n",
      "Gradient: [  -5.5785   12.5775   15.0309  105.2782 -233.0652]\n",
      "Weights: [-4.8328  0.8065 -1.2326  0.1012  0.1476]\n",
      "MSE loss: 87.117\n",
      "Iteration: 97600\n",
      "Gradient: [  7.8061   0.6317  24.1214  77.1197 219.8145]\n",
      "Weights: [-4.8287  0.8118 -1.2323  0.1008  0.1472]\n",
      "MSE loss: 87.1772\n",
      "Iteration: 97700\n",
      "Gradient: [ 4.7206 13.2291 -5.5593 48.0567 -8.6618]\n",
      "Weights: [-4.8422  0.8141 -1.2352  0.102   0.1472]\n",
      "MSE loss: 87.0583\n",
      "Iteration: 97800\n",
      "Gradient: [ -1.9355   8.4061 -24.8231  87.2126 -23.3911]\n",
      "Weights: [-4.8442  0.8118 -1.2359  0.1027  0.1472]\n",
      "MSE loss: 87.109\n",
      "Iteration: 97900\n",
      "Gradient: [ 3.5253 -7.9712 -1.3921 20.0133 70.3881]\n",
      "Weights: [-4.8184  0.8035 -1.2381  0.1025  0.1474]\n",
      "MSE loss: 87.0655\n",
      "Iteration: 98000\n",
      "Gradient: [ -13.7477   -7.6815   -7.8701 -109.0695  -74.8118]\n",
      "Weights: [-4.8356  0.8061 -1.2392  0.1024  0.1475]\n",
      "MSE loss: 87.4046\n",
      "Iteration: 98100\n",
      "Gradient: [ -5.8603 -15.2419  -2.1832  -6.4498 105.9229]\n",
      "Weights: [-4.8247  0.8049 -1.242   0.103   0.1476]\n",
      "MSE loss: 87.1449\n",
      "Iteration: 98200\n",
      "Gradient: [   3.233    21.1493   10.7508  -23.5146 -181.2423]\n",
      "Weights: [-4.8173  0.8062 -1.2423  0.1037  0.1476]\n",
      "MSE loss: 87.098\n",
      "Iteration: 98300\n",
      "Gradient: [   3.482    13.5574  -19.2377   40.1867 -191.9418]\n",
      "Weights: [-4.8265  0.8124 -1.2443  0.103   0.1477]\n",
      "MSE loss: 87.0595\n",
      "Iteration: 98400\n",
      "Gradient: [-16.7736 -12.6024  17.2462  18.054   74.9788]\n",
      "Weights: [-4.8292  0.8074 -1.2393  0.1028  0.1476]\n",
      "MSE loss: 86.9988\n",
      "Iteration: 98500\n",
      "Gradient: [-16.0634  15.6107 -24.3376 111.2938 141.9806]\n",
      "Weights: [-4.8415  0.817  -1.2376  0.1022  0.1475]\n",
      "MSE loss: 87.0998\n",
      "Iteration: 98600\n",
      "Gradient: [   0.347    13.628   -39.7981  -86.3309 -219.8753]\n",
      "Weights: [-4.8279  0.8081 -1.2372  0.1022  0.1473]\n",
      "MSE loss: 87.0186\n",
      "Iteration: 98700\n",
      "Gradient: [  2.6617 -10.5689   4.3836  70.8261  10.5683]\n",
      "Weights: [-4.8433  0.8138 -1.2395  0.1027  0.1475]\n",
      "MSE loss: 87.1291\n",
      "Iteration: 98800\n",
      "Gradient: [  -0.6619    8.5172  -14.2241  -63.0849 -128.4098]\n",
      "Weights: [-4.8274  0.8098 -1.2415  0.1026  0.1476]\n",
      "MSE loss: 87.0616\n",
      "Iteration: 98900\n",
      "Gradient: [  -3.4221  -14.0867   22.915   -98.8296 -135.6803]\n",
      "Weights: [-4.8336  0.8073 -1.2421  0.1036  0.1475]\n",
      "MSE loss: 87.1456\n",
      "Iteration: 99000\n",
      "Gradient: [ 0.7373 12.0066  6.7502 48.1331 45.778 ]\n",
      "Weights: [-4.8169  0.8092 -1.2434  0.1038  0.1475]\n",
      "MSE loss: 87.1098\n",
      "Iteration: 99100\n",
      "Gradient: [ 16.316  -11.0983 -19.0554  57.4965  45.7254]\n",
      "Weights: [-4.8252  0.8066 -1.2407  0.1036  0.1475]\n",
      "MSE loss: 86.9957\n",
      "Iteration: 99200\n",
      "Gradient: [  5.0042 -23.6938 -35.4855  63.546  107.79  ]\n",
      "Weights: [-4.8323  0.8073 -1.2401  0.1035  0.1473]\n",
      "MSE loss: 87.0419\n",
      "Iteration: 99300\n",
      "Gradient: [  -0.5168  -10.7945  -20.1798   52.4199 -213.4381]\n",
      "Weights: [-4.8263  0.8083 -1.2408  0.1032  0.1472]\n",
      "MSE loss: 87.0998\n",
      "Iteration: 99400\n",
      "Gradient: [-0.1783 -5.6019 11.3685 -7.0236 41.9147]\n",
      "Weights: [-4.8385  0.8139 -1.2394  0.1031  0.1471]\n",
      "MSE loss: 87.0472\n",
      "Iteration: 99500\n",
      "Gradient: [   5.4256   10.2391    9.9746   25.7435 -142.9146]\n",
      "Weights: [-4.825   0.8134 -1.2387  0.104   0.147 ]\n",
      "MSE loss: 87.3557\n",
      "Iteration: 99600\n",
      "Gradient: [  -2.5854   -7.8804  -58.4981 -147.384    27.6133]\n",
      "Weights: [-4.8388  0.8077 -1.2401  0.1043  0.1469]\n",
      "MSE loss: 87.358\n",
      "Iteration: 99700\n",
      "Gradient: [  -6.0588   -7.755    16.0413    2.7471 -133.2489]\n",
      "Weights: [-4.8503  0.8191 -1.243   0.1047  0.147 ]\n",
      "MSE loss: 87.2603\n",
      "Iteration: 99800\n",
      "Gradient: [  3.2637   5.6987  24.5121 -41.3107   7.3571]\n",
      "Weights: [-4.8445  0.8239 -1.2419  0.1048  0.1468]\n",
      "MSE loss: 87.1135\n",
      "Iteration: 99900\n",
      "Gradient: [   4.8501    5.5059   39.0766  -57.4088 -202.4984]\n",
      "Weights: [-4.8259  0.8124 -1.2463  0.1064  0.1469]\n",
      "MSE loss: 86.9594\n",
      "Iteration: 100000\n",
      "Gradient: [  8.1412  -4.7692  -6.5362   8.6291 -58.6961]\n",
      "Weights: [-4.8253  0.8092 -1.2457  0.1064  0.1469]\n",
      "MSE loss: 86.9477\n",
      "Iteration: 100100\n",
      "Gradient: [ -15.1492  -15.9513  -20.0001  -65.7941 -236.9926]\n",
      "Weights: [-4.8285  0.8005 -1.2434  0.1065  0.1468]\n",
      "MSE loss: 87.2284\n",
      "Iteration: 100200\n",
      "Gradient: [   5.9586   12.6463   -6.5892  -43.97   -287.0841]\n",
      "Weights: [-4.8236  0.802  -1.2413  0.1063  0.1469]\n",
      "MSE loss: 86.9983\n",
      "Iteration: 100300\n",
      "Gradient: [ -14.0066  -24.7551   15.0988 -107.856   -90.7775]\n",
      "Weights: [-4.8293  0.7986 -1.2418  0.1067  0.1467]\n",
      "MSE loss: 87.1263\n",
      "Iteration: 100400\n",
      "Gradient: [ -7.0052   7.7052  -3.5554 178.3502 176.2728]\n",
      "Weights: [-4.8185  0.8047 -1.2421  0.107   0.1466]\n",
      "MSE loss: 87.1215\n",
      "Iteration: 100500\n",
      "Gradient: [  0.1659  18.0433  18.3412  21.1732 152.5664]\n",
      "Weights: [-4.8271  0.7973 -1.2408  0.1074  0.1465]\n",
      "MSE loss: 87.0067\n",
      "Iteration: 100600\n",
      "Gradient: [  3.11     0.3411 -13.7361 118.9557  16.5727]\n",
      "Weights: [-4.8128  0.7889 -1.2379  0.1078  0.1464]\n",
      "MSE loss: 87.0932\n",
      "Iteration: 100700\n",
      "Gradient: [ -2.2551  26.1856  11.3772 110.8167 -38.0264]\n",
      "Weights: [-4.8266  0.7982 -1.238   0.1073  0.1465]\n",
      "MSE loss: 87.0924\n",
      "Iteration: 100800\n",
      "Gradient: [ 11.5626   2.6345 -18.7377 -23.2117 -34.4973]\n",
      "Weights: [-4.8313  0.8058 -1.2415  0.1068  0.1465]\n",
      "MSE loss: 86.9512\n",
      "Iteration: 100900\n",
      "Gradient: [  -3.1172   -3.3356  -16.2518   26.7693 -205.2789]\n",
      "Weights: [-4.8191  0.7955 -1.2404  0.1067  0.1465]\n",
      "MSE loss: 87.0238\n",
      "Iteration: 101000\n",
      "Gradient: [  10.9021   -3.2896  -75.3463    3.2665 -305.9613]\n",
      "Weights: [-4.8427  0.8092 -1.2416  0.1057  0.1467]\n",
      "MSE loss: 87.2963\n",
      "Iteration: 101100\n",
      "Gradient: [   9.5896   -9.8909   23.5604  -56.5282 -218.7742]\n",
      "Weights: [-4.8325  0.807  -1.2401  0.1052  0.1468]\n",
      "MSE loss: 86.9925\n",
      "Iteration: 101200\n",
      "Gradient: [  -2.5169    5.3609    8.2828  -14.82   -200.008 ]\n",
      "Weights: [-4.8294  0.804  -1.2411  0.1061  0.1467]\n",
      "MSE loss: 86.9776\n",
      "Iteration: 101300\n",
      "Gradient: [-10.2764   6.2581 -41.3493 -32.6905 132.1516]\n",
      "Weights: [-4.8372  0.8213 -1.2469  0.1064  0.1465]\n",
      "MSE loss: 86.9751\n",
      "Iteration: 101400\n",
      "Gradient: [  2.1193  16.241    2.9058  47.6676 189.4731]\n",
      "Weights: [-4.829   0.8224 -1.2482  0.1078  0.1466]\n",
      "MSE loss: 87.3204\n",
      "Iteration: 101500\n",
      "Gradient: [  3.2334  -8.6472  -3.8393 -50.043   39.6797]\n",
      "Weights: [-4.838   0.8191 -1.2482  0.108   0.1465]\n",
      "MSE loss: 86.9372\n",
      "Iteration: 101600\n",
      "Gradient: [  -8.3608  -12.4871  -22.8022   46.8714 -268.4581]\n",
      "Weights: [-4.8309  0.8211 -1.2512  0.1085  0.1465]\n",
      "MSE loss: 86.9132\n",
      "Iteration: 101700\n",
      "Gradient: [  4.1873 -15.9408  32.5359 -38.9642  -7.3517]\n",
      "Weights: [-4.8275  0.8257 -1.2515  0.1076  0.1464]\n",
      "MSE loss: 87.0234\n",
      "Iteration: 101800\n",
      "Gradient: [ -6.6153 -15.8603  32.6287 -92.0378  18.7485]\n",
      "Weights: [-4.8327  0.8178 -1.2476  0.1082  0.1464]\n",
      "MSE loss: 86.945\n",
      "Iteration: 101900\n",
      "Gradient: [   4.5593   24.735    29.2727  -81.837  -251.9042]\n",
      "Weights: [-4.8359  0.8233 -1.2509  0.1086  0.1464]\n",
      "MSE loss: 86.9223\n",
      "Iteration: 102000\n",
      "Gradient: [  5.3206   5.1457 -12.7462 -10.6533 298.2288]\n",
      "Weights: [-4.818   0.8204 -1.2539  0.1095  0.1462]\n",
      "MSE loss: 87.1108\n",
      "Iteration: 102100\n",
      "Gradient: [  5.0893 -11.2729  -8.1838  45.5877  20.4227]\n",
      "Weights: [-4.8271  0.8188 -1.2554  0.1095  0.1464]\n",
      "MSE loss: 86.9795\n",
      "Iteration: 102200\n",
      "Gradient: [  6.69     2.4514 -26.4474 -97.5617 113.7654]\n",
      "Weights: [-4.8263  0.821  -1.2555  0.1095  0.1463]\n",
      "MSE loss: 86.9724\n",
      "Iteration: 102300\n",
      "Gradient: [   3.4121   -3.9438  -10.2214   40.46   -168.1793]\n",
      "Weights: [-4.8278  0.8185 -1.2578  0.1106  0.1465]\n",
      "MSE loss: 86.9259\n",
      "Iteration: 102400\n",
      "Gradient: [ 16.7941  14.3373  21.9316 -81.8689 -33.4949]\n",
      "Weights: [-4.8286  0.8264 -1.2596  0.1107  0.1464]\n",
      "MSE loss: 86.8779\n",
      "Iteration: 102500\n",
      "Gradient: [ -11.9095  -21.071   -45.3546 -100.1817 -157.9188]\n",
      "Weights: [-4.8465  0.8289 -1.2593  0.1112  0.1461]\n",
      "MSE loss: 87.1764\n",
      "Iteration: 102600\n",
      "Gradient: [15.5806 15.775  39.9967 24.167  50.0927]\n",
      "Weights: [-4.8269  0.8293 -1.2591  0.1115  0.146 ]\n",
      "MSE loss: 86.9905\n",
      "Iteration: 102700\n",
      "Gradient: [  0.4783  12.2746   1.97   -24.2959 225.5727]\n",
      "Weights: [-4.827   0.8291 -1.2571  0.111   0.1459]\n",
      "MSE loss: 87.0214\n",
      "Iteration: 102800\n",
      "Gradient: [ -2.1647   3.6023  23.1241 -60.4403  63.5011]\n",
      "Weights: [-4.8409  0.8283 -1.2538  0.1109  0.1458]\n",
      "MSE loss: 86.9156\n",
      "Iteration: 102900\n",
      "Gradient: [  8.2462  -5.2834 -13.392  -42.7926  66.5237]\n",
      "Weights: [-4.8351  0.8246 -1.2542  0.1106  0.1459]\n",
      "MSE loss: 86.8727\n",
      "Iteration: 103000\n",
      "Gradient: [  4.613   -1.9909  10.4627  11.4496 129.1328]\n",
      "Weights: [-4.8277  0.8177 -1.2531  0.1109  0.1458]\n",
      "MSE loss: 86.8868\n",
      "Iteration: 103100\n",
      "Gradient: [-10.9772  14.5461  15.4432 139.3162 125.4652]\n",
      "Weights: [-4.8319  0.8274 -1.2552  0.1109  0.1459]\n",
      "MSE loss: 86.9441\n",
      "Iteration: 103200\n",
      "Gradient: [ -8.0233   8.3011   8.2593  28.4479 -14.8952]\n",
      "Weights: [-4.828   0.8168 -1.2539  0.1106  0.1461]\n",
      "MSE loss: 86.8794\n",
      "Iteration: 103300\n",
      "Gradient: [ -3.9138   8.5306  10.9243  14.3691 227.2712]\n",
      "Weights: [-4.8223  0.8112 -1.2538  0.1112  0.1461]\n",
      "MSE loss: 86.8924\n",
      "Iteration: 103400\n",
      "Gradient: [  5.3807  -6.7527 -52.0187 -45.3818  39.2923]\n",
      "Weights: [-4.8333  0.8091 -1.2522  0.1112  0.1461]\n",
      "MSE loss: 87.0267\n",
      "Iteration: 103500\n",
      "Gradient: [  -0.9759    4.975    33.7813   47.9877 -268.5479]\n",
      "Weights: [-4.8303  0.8096 -1.2508  0.1106  0.146 ]\n",
      "MSE loss: 86.9801\n",
      "Iteration: 103600\n",
      "Gradient: [  4.6962  22.1019 -16.8667  25.2111  74.7389]\n",
      "Weights: [-4.8217  0.8124 -1.2526  0.1116  0.1461]\n",
      "MSE loss: 87.0911\n",
      "Iteration: 103700\n",
      "Gradient: [  1.2759  12.1115  20.2627 -87.6015  -8.4066]\n",
      "Weights: [-4.8219  0.8054 -1.2544  0.1118  0.1462]\n",
      "MSE loss: 86.9581\n",
      "Iteration: 103800\n",
      "Gradient: [  2.6742  -6.3361  -9.3477 114.2242 -62.4166]\n",
      "Weights: [-4.8305  0.8182 -1.2547  0.1117  0.1461]\n",
      "MSE loss: 86.9253\n",
      "Iteration: 103900\n",
      "Gradient: [  -3.7991   14.5923  -35.8046  -21.73   -164.7389]\n",
      "Weights: [-4.8364  0.8146 -1.2526  0.1106  0.146 ]\n",
      "MSE loss: 87.0298\n",
      "Iteration: 104000\n",
      "Gradient: [   9.305     5.9787  -21.048   -44.2329 -130.6293]\n",
      "Weights: [-4.8149  0.8134 -1.2537  0.1108  0.146 ]\n",
      "MSE loss: 87.0456\n",
      "Iteration: 104100\n",
      "Gradient: [ 12.6864   2.557   -1.661  -59.6775 105.8442]\n",
      "Weights: [-4.8302  0.8262 -1.2577  0.1111  0.1462]\n",
      "MSE loss: 86.9075\n",
      "Iteration: 104200\n",
      "Gradient: [  3.2007   0.4832 -13.7107 -23.7537  28.2091]\n",
      "Weights: [-4.831   0.8251 -1.2574  0.1117  0.1459]\n",
      "MSE loss: 86.8625\n",
      "Iteration: 104300\n",
      "Gradient: [ -0.3976  -7.3247   1.1721  47.9657 -65.7201]\n",
      "Weights: [-4.8374  0.8249 -1.2557  0.1112  0.1456]\n",
      "MSE loss: 87.0331\n",
      "Iteration: 104400\n",
      "Gradient: [ -0.8562 -16.3575 -21.3915 -42.1352 203.648 ]\n",
      "Weights: [-4.8428  0.81   -1.2528  0.1117  0.1459]\n",
      "MSE loss: 87.4797\n",
      "Iteration: 104500\n",
      "Gradient: [  7.2099   8.408   -2.0028 -95.986   99.0367]\n",
      "Weights: [-4.8268  0.8105 -1.254   0.1122  0.1459]\n",
      "MSE loss: 86.8723\n",
      "Iteration: 104600\n",
      "Gradient: [  1.9665 -16.3252   0.2188 -16.4874  29.9606]\n",
      "Weights: [-4.8385  0.8142 -1.253   0.1115  0.1457]\n",
      "MSE loss: 87.1609\n",
      "Iteration: 104700\n",
      "Gradient: [   0.2391   -0.3874   19.1901  -75.6499 -187.8614]\n",
      "Weights: [-4.819   0.8068 -1.2526  0.1121  0.1457]\n",
      "MSE loss: 86.9185\n",
      "Iteration: 104800\n",
      "Gradient: [  -1.2542   -0.9871  -24.4565    1.3852 -171.0867]\n",
      "Weights: [-4.8203  0.8065 -1.2525  0.1123  0.1457]\n",
      "MSE loss: 86.8908\n",
      "Iteration: 104900\n",
      "Gradient: [  4.1328   8.9386  45.2644  16.0868 215.3084]\n",
      "Weights: [-4.8163  0.8068 -1.2501  0.1123  0.1457]\n",
      "MSE loss: 87.2063\n",
      "Iteration: 105000\n",
      "Gradient: [ 6.0116 12.7839 -3.1711 38.6671 70.5281]\n",
      "Weights: [-4.8126  0.8    -1.2509  0.1124  0.1458]\n",
      "MSE loss: 86.9762\n",
      "Iteration: 105100\n",
      "Gradient: [   8.431    -1.7654    5.3503  106.7485 -187.696 ]\n",
      "Weights: [-4.8311  0.8078 -1.2527  0.1129  0.1459]\n",
      "MSE loss: 86.9556\n",
      "Iteration: 105200\n",
      "Gradient: [ -2.8121  18.7406 -23.0688 -96.4638  43.8916]\n",
      "Weights: [-4.8107  0.8029 -1.2569  0.1129  0.1459]\n",
      "MSE loss: 87.1017\n",
      "Iteration: 105300\n",
      "Gradient: [  5.6148   9.4532 -10.4184  53.8193 309.9499]\n",
      "Weights: [-4.8147  0.801  -1.2554  0.1137  0.1459]\n",
      "MSE loss: 86.9473\n",
      "Iteration: 105400\n",
      "Gradient: [ -6.3545  -1.8861 -20.5862 -55.0118  67.0336]\n",
      "Weights: [-4.8355  0.8018 -1.2508  0.1132  0.1458]\n",
      "MSE loss: 87.1466\n",
      "Iteration: 105500\n",
      "Gradient: [  1.8948 -10.052  -16.1939  28.4281 -96.8754]\n",
      "Weights: [-4.8245  0.8015 -1.2543  0.1142  0.1455]\n",
      "MSE loss: 86.9499\n",
      "Iteration: 105600\n",
      "Gradient: [ 14.2412  11.9059 -10.3814 -66.893  343.4108]\n",
      "Weights: [-4.8159  0.8074 -1.2535  0.1139  0.1453]\n",
      "MSE loss: 86.9633\n",
      "Iteration: 105700\n",
      "Gradient: [  -6.4312   -2.3531    2.187  -174.0224 -131.2357]\n",
      "Weights: [-4.8357  0.8087 -1.2512  0.1137  0.1451]\n",
      "MSE loss: 86.9336\n",
      "Iteration: 105800\n",
      "Gradient: [-3.703  16.9719 -0.6718 63.7217 26.9771]\n",
      "Weights: [-4.8151  0.7919 -1.2489  0.1147  0.145 ]\n",
      "MSE loss: 86.8934\n",
      "Iteration: 105900\n",
      "Gradient: [ -2.6209  15.5886  60.657  -99.3115 196.7104]\n",
      "Weights: [-4.8272  0.7958 -1.245   0.1147  0.1448]\n",
      "MSE loss: 86.9919\n",
      "Iteration: 106000\n",
      "Gradient: [  7.9152   5.454   27.8993  23.4103 294.1972]\n",
      "Weights: [-4.8159  0.797  -1.2461  0.1153  0.1446]\n",
      "MSE loss: 87.2758\n",
      "Iteration: 106100\n",
      "Gradient: [  0.2076  11.9613 -38.9363 -39.1975 -59.7851]\n",
      "Weights: [-4.8403  0.806  -1.2455  0.1144  0.1443]\n",
      "MSE loss: 86.9842\n",
      "Iteration: 106200\n",
      "Gradient: [  4.4692 -15.2496 -33.027    2.743  -20.7985]\n",
      "Weights: [-4.8163  0.7878 -1.2438  0.1149  0.1443]\n",
      "MSE loss: 86.926\n",
      "Iteration: 106300\n",
      "Gradient: [  1.3552 -11.7741 -12.1317 -55.6153   3.9943]\n",
      "Weights: [-4.8246  0.7964 -1.2465  0.1154  0.1444]\n",
      "MSE loss: 86.8478\n",
      "Iteration: 106400\n",
      "Gradient: [ -3.7859   0.8239 -25.7494  -0.5138 234.6516]\n",
      "Weights: [-4.813   0.7929 -1.2473  0.115   0.1444]\n",
      "MSE loss: 86.9384\n",
      "Iteration: 106500\n",
      "Gradient: [-0.2005  8.5695 37.7881 89.394  15.5707]\n",
      "Weights: [-4.8339  0.8081 -1.2496  0.1156  0.1443]\n",
      "MSE loss: 86.8487\n",
      "Iteration: 106600\n",
      "Gradient: [ -0.8437  -4.4122  -6.461  -13.0717  77.9604]\n",
      "Weights: [-4.8555  0.8275 -1.2538  0.1159  0.1443]\n",
      "MSE loss: 87.0863\n",
      "Iteration: 106700\n",
      "Gradient: [  -4.0062    0.843   -50.4683 -145.2837   73.6279]\n",
      "Weights: [-4.8395  0.8191 -1.2541  0.1159  0.144 ]\n",
      "MSE loss: 86.9665\n",
      "Iteration: 106800\n",
      "Gradient: [ 9.3807 11.1596 66.5154 15.1313  8.3626]\n",
      "Weights: [-4.8195  0.8187 -1.2559  0.1169  0.1441]\n",
      "MSE loss: 87.2656\n",
      "Iteration: 106900\n",
      "Gradient: [  9.413    5.5623  -9.4258  11.785  164.0813]\n",
      "Weights: [-4.8283  0.8197 -1.2572  0.1164  0.1442]\n",
      "MSE loss: 86.8622\n",
      "Iteration: 107000\n",
      "Gradient: [   2.4419    3.7433   21.4238   76.6715 -118.764 ]\n",
      "Weights: [-4.8137  0.8128 -1.259   0.1177  0.1442]\n",
      "MSE loss: 87.0193\n",
      "Iteration: 107100\n",
      "Gradient: [ -1.6689  19.1327 -25.2993  28.2009 -14.9787]\n",
      "Weights: [-4.831   0.8143 -1.2597  0.1179  0.1443]\n",
      "MSE loss: 86.815\n",
      "Iteration: 107200\n",
      "Gradient: [   5.9324   17.3686   13.1035  -55.121  -413.1263]\n",
      "Weights: [-4.8408  0.8244 -1.2609  0.1179  0.1444]\n",
      "MSE loss: 86.8206\n",
      "Iteration: 107300\n",
      "Gradient: [  7.62     8.2271 -17.0001  59.9148  32.0932]\n",
      "Weights: [-4.823   0.8213 -1.2639  0.1178  0.1444]\n",
      "MSE loss: 86.8505\n",
      "Iteration: 107400\n",
      "Gradient: [ -10.0769   15.6887    3.3769  -47.2511 -185.5938]\n",
      "Weights: [-4.8455  0.8259 -1.2631  0.1183  0.1445]\n",
      "MSE loss: 86.8894\n",
      "Iteration: 107500\n",
      "Gradient: [   6.5166    8.9411   51.2606 -118.5426 -143.7744]\n",
      "Weights: [-4.8112  0.8165 -1.2644  0.1181  0.1446]\n",
      "MSE loss: 87.0584\n",
      "Iteration: 107600\n",
      "Gradient: [  -6.2476   -3.0312   12.8335  -34.96   -167.3065]\n",
      "Weights: [-4.8208  0.8176 -1.2645  0.118   0.1446]\n",
      "MSE loss: 86.8228\n",
      "Iteration: 107700\n",
      "Gradient: [  6.4619 -12.2437 -20.8514 -61.2868 -39.792 ]\n",
      "Weights: [-4.8335  0.8295 -1.2671  0.1179  0.1446]\n",
      "MSE loss: 86.7679\n",
      "Iteration: 107800\n",
      "Gradient: [  8.3118   8.104  -20.7506  -9.0395  39.2154]\n",
      "Weights: [-4.8347  0.8312 -1.268   0.1182  0.1446]\n",
      "MSE loss: 86.7642\n",
      "Iteration: 107900\n",
      "Gradient: [  7.3414   8.6759 -15.5178  52.4462  -6.8758]\n",
      "Weights: [-4.8448  0.8364 -1.2676  0.1185  0.1445]\n",
      "MSE loss: 86.7908\n",
      "Iteration: 108000\n",
      "Gradient: [ -3.0879   1.0979   9.1342  33.6382 -46.0557]\n",
      "Weights: [-4.8397  0.8292 -1.2663  0.119   0.1444]\n",
      "MSE loss: 86.7707\n",
      "Iteration: 108100\n",
      "Gradient: [ 16.6181   3.1953  27.1284  90.4834 196.7816]\n",
      "Weights: [-4.8265  0.8282 -1.2667  0.1192  0.1445]\n",
      "MSE loss: 87.0304\n",
      "Iteration: 108200\n",
      "Gradient: [ -2.3196  -4.8977 -15.8788 -36.2373 109.882 ]\n",
      "Weights: [-4.8062  0.8224 -1.2693  0.1192  0.1442]\n",
      "MSE loss: 87.3533\n",
      "Iteration: 108300\n",
      "Gradient: [  -2.0048    3.2229  -16.362   -70.8093 -289.4624]\n",
      "Weights: [-4.8149  0.8121 -1.2669  0.1198  0.1445]\n",
      "MSE loss: 86.8396\n",
      "Iteration: 108400\n",
      "Gradient: [ 2.89000e-02 -1.69680e+00 -1.53582e+01 -2.19623e+01  4.36536e+01]\n",
      "Weights: [-4.8074  0.8076 -1.2675  0.1202  0.1446]\n",
      "MSE loss: 86.9332\n",
      "Iteration: 108500\n",
      "Gradient: [  0.7254  -0.3865   4.8737   2.0943 145.0869]\n",
      "Weights: [-4.8099  0.8077 -1.2647  0.12    0.1444]\n",
      "MSE loss: 86.9468\n",
      "Iteration: 108600\n",
      "Gradient: [  0.5246  -0.2923 -23.2907 -38.0734  30.3498]\n",
      "Weights: [-4.8277  0.8207 -1.2676  0.1197  0.1445]\n",
      "MSE loss: 86.7438\n",
      "Iteration: 108700\n",
      "Gradient: [  1.7988  13.4054  17.115  -46.4588 -97.4561]\n",
      "Weights: [-4.8283  0.8163 -1.2646  0.1197  0.1445]\n",
      "MSE loss: 86.7955\n",
      "Iteration: 108800\n",
      "Gradient: [-5.92000e-02 -7.04500e+00  4.64010e+00 -3.42252e+01 -7.75226e+01]\n",
      "Weights: [-4.8188  0.8174 -1.2653  0.1189  0.1444]\n",
      "MSE loss: 86.832\n",
      "Iteration: 108900\n",
      "Gradient: [ 7.1683 15.0077 25.8358 -3.3813 21.6586]\n",
      "Weights: [-4.8226  0.8078 -1.2613  0.1195  0.1443]\n",
      "MSE loss: 86.7823\n",
      "Iteration: 109000\n",
      "Gradient: [ -4.5036  10.4069  11.1337 -99.3799  34.0562]\n",
      "Weights: [-4.8239  0.8131 -1.2605  0.1193  0.1441]\n",
      "MSE loss: 86.7923\n",
      "Iteration: 109100\n",
      "Gradient: [   2.4961   16.6871    5.5372   56.4776 -279.0479]\n",
      "Weights: [-4.8287  0.8168 -1.2639  0.12    0.1441]\n",
      "MSE loss: 86.7476\n",
      "Iteration: 109200\n",
      "Gradient: [   2.2619   -0.4983  -17.8138   45.2005 -151.3487]\n",
      "Weights: [-4.8327  0.8209 -1.2641  0.119   0.1443]\n",
      "MSE loss: 86.7611\n",
      "Iteration: 109300\n",
      "Gradient: [  4.7321  15.0788  -9.3212 -38.9167 354.2604]\n",
      "Weights: [-4.8435  0.8364 -1.2654  0.1186  0.1442]\n",
      "MSE loss: 86.8465\n",
      "Iteration: 109400\n",
      "Gradient: [  -0.2539    7.991    15.4047   61.1149 -125.5582]\n",
      "Weights: [-4.8419  0.8293 -1.2665  0.1186  0.1443]\n",
      "MSE loss: 86.8867\n",
      "Iteration: 109500\n",
      "Gradient: [  2.16    -0.4716  22.0489 -30.3338 -13.215 ]\n",
      "Weights: [-4.8329  0.8264 -1.2662  0.1197  0.1442]\n",
      "MSE loss: 86.7806\n",
      "Iteration: 109600\n",
      "Gradient: [-12.3135 -10.4119  -3.9473  17.1644   2.4444]\n",
      "Weights: [-4.8504  0.8282 -1.2671  0.1201  0.1442]\n",
      "MSE loss: 87.0369\n",
      "Iteration: 109700\n",
      "Gradient: [ 9.417800e+00 -2.567000e-01 -1.574800e+00 -5.661700e+01  3.105599e+02]\n",
      "Weights: [-4.8304  0.8268 -1.2669  0.1199  0.144 ]\n",
      "MSE loss: 86.7594\n",
      "Iteration: 109800\n",
      "Gradient: [ 13.5267  12.283   16.7808 127.8554  77.4817]\n",
      "Weights: [-4.8203  0.8271 -1.2684  0.1212  0.144 ]\n",
      "MSE loss: 87.285\n",
      "Iteration: 109900\n",
      "Gradient: [   5.3709    2.3483   33.3943   23.6904 -138.9215]\n",
      "Weights: [-4.8243  0.8246 -1.2688  0.1206  0.144 ]\n",
      "MSE loss: 86.7772\n",
      "Iteration: 110000\n",
      "Gradient: [  7.594   -1.9331   0.4478 -31.0835 -17.8509]\n",
      "Weights: [-4.8472  0.834  -1.2679  0.1199  0.1441]\n",
      "MSE loss: 86.8226\n",
      "Iteration: 110100\n",
      "Gradient: [ -2.7572  10.7965  12.768  -28.4148  -5.7333]\n",
      "Weights: [-4.8288  0.8262 -1.2684  0.1201  0.144 ]\n",
      "MSE loss: 86.7811\n",
      "Iteration: 110200\n",
      "Gradient: [   0.9076  -22.2207  -34.6007  -25.5408 -165.0928]\n",
      "Weights: [-4.8215  0.8107 -1.2655  0.1199  0.1441]\n",
      "MSE loss: 87.1126\n",
      "Iteration: 110300\n",
      "Gradient: [  4.9697  -9.0308  12.0344  70.588  243.7845]\n",
      "Weights: [-4.8028  0.7976 -1.2614  0.1203  0.1442]\n",
      "MSE loss: 87.0043\n",
      "Iteration: 110400\n",
      "Gradient: [  -7.4427   14.0643  -34.9467   60.1859 -259.6896]\n",
      "Weights: [-4.8079  0.7981 -1.2618  0.121   0.1439]\n",
      "MSE loss: 86.8774\n",
      "Iteration: 110500\n",
      "Gradient: [  8.4816 -11.6884  -1.3322 -10.9113  74.7473]\n",
      "Weights: [-4.8237  0.8062 -1.263   0.1218  0.1439]\n",
      "MSE loss: 86.7936\n",
      "Iteration: 110600\n",
      "Gradient: [ -3.0579   9.8406 -19.6731  -8.8348 -70.2222]\n",
      "Weights: [-4.8134  0.8112 -1.2634  0.1206  0.1439]\n",
      "MSE loss: 86.9399\n",
      "Iteration: 110700\n",
      "Gradient: [ -1.3301  -7.9782 -18.8982 -71.9586 205.9804]\n",
      "Weights: [-4.832   0.8141 -1.2629  0.1204  0.1441]\n",
      "MSE loss: 86.7838\n",
      "Iteration: 110800\n",
      "Gradient: [ -3.7104 -12.3846  12.9347  57.4498 -78.012 ]\n",
      "Weights: [-4.823   0.8061 -1.2621  0.1199  0.1441]\n",
      "MSE loss: 86.8367\n",
      "Iteration: 110900\n",
      "Gradient: [   4.0793  -11.1627   -6.4196 -127.5366 -178.6404]\n",
      "Weights: [-4.8258  0.8107 -1.2661  0.1204  0.1442]\n",
      "MSE loss: 86.9271\n",
      "Iteration: 111000\n",
      "Gradient: [  7.2147  -3.3916  26.8872  18.71   -33.1996]\n",
      "Weights: [-4.8327  0.8256 -1.2691  0.1201  0.1443]\n",
      "MSE loss: 86.7469\n",
      "Iteration: 111100\n",
      "Gradient: [ -0.2207  -7.9486 -40.41   -68.9777  -5.6608]\n",
      "Weights: [-4.8411  0.8277 -1.2684  0.1206  0.1442]\n",
      "MSE loss: 86.7841\n",
      "Iteration: 111200\n",
      "Gradient: [  -1.3596    3.5711  -29.3211  -18.9533 -135.5582]\n",
      "Weights: [-4.8272  0.8301 -1.2704  0.1209  0.1442]\n",
      "MSE loss: 86.9097\n",
      "Iteration: 111300\n",
      "Gradient: [ -2.58    -9.5664  -7.0904  49.0984 205.5831]\n",
      "Weights: [-4.8399  0.8279 -1.2696  0.1212  0.1441]\n",
      "MSE loss: 86.758\n",
      "Iteration: 111400\n",
      "Gradient: [ -2.1152 -11.5889  -4.4645 -47.9718   3.1219]\n",
      "Weights: [-4.8529  0.8355 -1.2725  0.1217  0.1441]\n",
      "MSE loss: 86.98\n",
      "Iteration: 111500\n",
      "Gradient: [  11.2705   -2.0498   48.6285  -47.6282 -181.7736]\n",
      "Weights: [-4.8425  0.8412 -1.2711  0.1203  0.1442]\n",
      "MSE loss: 86.8471\n",
      "Iteration: 111600\n",
      "Gradient: [  6.708    5.9747 -32.2708 -49.8912 308.3222]\n",
      "Weights: [-4.8408  0.8419 -1.2738  0.1201  0.1443]\n",
      "MSE loss: 86.7332\n",
      "Iteration: 111700\n",
      "Gradient: [-11.5778  10.4857  21.9632  -4.6477  73.1157]\n",
      "Weights: [-4.8424  0.8473 -1.2761  0.1199  0.1444]\n",
      "MSE loss: 86.749\n",
      "Iteration: 111800\n",
      "Gradient: [-5.0028 16.5711 37.6135 35.952  47.4396]\n",
      "Weights: [-4.8429  0.8477 -1.2775  0.1202  0.1445]\n",
      "MSE loss: 86.7246\n",
      "Iteration: 111900\n",
      "Gradient: [   6.7657   -0.4495   -5.4254  -17.5097 -196.0449]\n",
      "Weights: [-4.8313  0.8384 -1.2765  0.1203  0.1446]\n",
      "MSE loss: 86.7321\n",
      "Iteration: 112000\n",
      "Gradient: [ 2.7674 -8.297   9.0362 19.6755 96.6604]\n",
      "Weights: [-4.8358  0.8328 -1.2754  0.1211  0.1444]\n",
      "MSE loss: 86.786\n",
      "Iteration: 112100\n",
      "Gradient: [ -7.4843   0.4453 -10.4309 -33.7582 111.8845]\n",
      "Weights: [-4.8365  0.832  -1.2756  0.1218  0.1442]\n",
      "MSE loss: 86.7839\n",
      "Iteration: 112200\n",
      "Gradient: [   8.705   -16.9615   -6.2655   -2.1126 -217.1145]\n",
      "Weights: [-4.8334  0.83   -1.2743  0.122   0.1441]\n",
      "MSE loss: 86.7114\n",
      "Iteration: 112300\n",
      "Gradient: [ 12.4065  -2.5082   3.0391  53.3322 143.528 ]\n",
      "Weights: [-4.8352  0.834  -1.2741  0.1222  0.1443]\n",
      "MSE loss: 86.8862\n",
      "Iteration: 112400\n",
      "Gradient: [ -1.6654   0.7201  -2.7223 -91.8378 -23.9346]\n",
      "Weights: [-4.844   0.8483 -1.2786  0.1223  0.1439]\n",
      "MSE loss: 86.7148\n",
      "Iteration: 112500\n",
      "Gradient: [ -0.8501   6.3736  10.998  -98.5875 138.218 ]\n",
      "Weights: [-4.8399  0.8392 -1.2789  0.1228  0.1437]\n",
      "MSE loss: 86.9955\n",
      "Iteration: 112600\n",
      "Gradient: [  0.6063   3.3153   3.6281 -87.7602 -39.8882]\n",
      "Weights: [-4.8328  0.8328 -1.2741  0.1225  0.1439]\n",
      "MSE loss: 86.7217\n",
      "Iteration: 112700\n",
      "Gradient: [  -8.4074   -0.6107    8.5199  -14.9722 -170.2624]\n",
      "Weights: [-4.8194  0.8215 -1.2744  0.1226  0.1439]\n",
      "MSE loss: 86.8418\n",
      "Iteration: 112800\n",
      "Gradient: [   0.1661    6.3106   -5.9529   97.7253 -117.9078]\n",
      "Weights: [-4.8234  0.8245 -1.2722  0.1229  0.1439]\n",
      "MSE loss: 86.8229\n",
      "Iteration: 112900\n",
      "Gradient: [ -9.9197  -8.3922 -30.7124 -65.773    0.1935]\n",
      "Weights: [-4.8395  0.829  -1.2749  0.1224  0.144 ]\n",
      "MSE loss: 86.9571\n",
      "Iteration: 113000\n",
      "Gradient: [  -9.4128   -6.5052   -4.4642  -27.0955 -107.4096]\n",
      "Weights: [-4.8271  0.8313 -1.2778  0.1229  0.144 ]\n",
      "MSE loss: 86.7445\n",
      "Iteration: 113100\n",
      "Gradient: [  -2.4033  -12.2667  -14.7228 -141.9554 -334.7394]\n",
      "Weights: [-4.8299  0.8365 -1.2775  0.1223  0.144 ]\n",
      "MSE loss: 86.7207\n",
      "Iteration: 113200\n",
      "Gradient: [   7.1484   -1.5582   15.8692   12.7824 -201.6489]\n",
      "Weights: [-4.8454  0.8432 -1.2789  0.1218  0.1443]\n",
      "MSE loss: 86.7879\n",
      "Iteration: 113300\n",
      "Gradient: [   2.4219   12.0456   23.9039  -30.2237 -141.0169]\n",
      "Weights: [-4.8416  0.8563 -1.2818  0.121   0.1443]\n",
      "MSE loss: 86.7652\n",
      "Iteration: 113400\n",
      "Gradient: [ 1.000000e-04  3.263300e+00  2.350400e+00 -6.375300e+00  1.299344e+02]\n",
      "Weights: [-4.8392  0.8534 -1.281   0.1204  0.1445]\n",
      "MSE loss: 86.7631\n",
      "Iteration: 113500\n",
      "Gradient: [  8.8421  -4.4395 -33.0293   2.655   80.9412]\n",
      "Weights: [-4.8371  0.8516 -1.2823  0.1216  0.1443]\n",
      "MSE loss: 86.7379\n",
      "Iteration: 113600\n",
      "Gradient: [ -4.12    12.0839   5.9982 -31.8791 188.3341]\n",
      "Weights: [-4.8333  0.8491 -1.2828  0.1225  0.1444]\n",
      "MSE loss: 86.8609\n",
      "Iteration: 113700\n",
      "Gradient: [ -3.3259   5.1283 -16.8011 -91.0738 135.8306]\n",
      "Weights: [-4.847   0.8506 -1.2802  0.122   0.1442]\n",
      "MSE loss: 86.7151\n",
      "Iteration: 113800\n",
      "Gradient: [ -0.517   11.1977   0.7798  85.7554 148.9022]\n",
      "Weights: [-4.8491  0.8537 -1.2783  0.1213  0.1442]\n",
      "MSE loss: 86.7807\n",
      "Iteration: 113900\n",
      "Gradient: [ -3.688  -13.8383  18.8494  12.5314 445.4435]\n",
      "Weights: [-4.8608  0.8616 -1.2799  0.1216  0.1442]\n",
      "MSE loss: 86.9658\n",
      "Iteration: 114000\n",
      "Gradient: [  5.3674  22.3096  48.7579  51.038  135.0281]\n",
      "Weights: [-4.8399  0.8603 -1.2833  0.1223  0.1443]\n",
      "MSE loss: 87.1587\n",
      "Iteration: 114100\n",
      "Gradient: [  -7.6628   -8.063   -12.5204  -17.2867 -118.9432]\n",
      "Weights: [-4.8571  0.8521 -1.2841  0.1228  0.1444]\n",
      "MSE loss: 86.9988\n",
      "Iteration: 114200\n",
      "Gradient: [  -4.5265   24.1747    9.6357 -138.0062   82.3571]\n",
      "Weights: [-4.847   0.8551 -1.2851  0.122   0.1445]\n",
      "MSE loss: 86.7289\n",
      "Iteration: 114300\n",
      "Gradient: [  4.3471  -1.2008 -18.3791  31.6401 114.82  ]\n",
      "Weights: [-4.8346  0.853  -1.2848  0.1223  0.1444]\n",
      "MSE loss: 86.7623\n",
      "Iteration: 114400\n",
      "Gradient: [ -0.8122  -6.2773   6.6537 -73.5767  11.254 ]\n",
      "Weights: [-4.838   0.8515 -1.2829  0.1217  0.1444]\n",
      "MSE loss: 86.7191\n",
      "Iteration: 114500\n",
      "Gradient: [  -2.9401   17.1294  -22.8643  -48.0525 -171.5202]\n",
      "Weights: [-4.8356  0.8467 -1.2844  0.1223  0.1445]\n",
      "MSE loss: 86.7119\n",
      "Iteration: 114600\n",
      "Gradient: [ -13.9072   -9.968   -84.1477  -97.7118 -114.9119]\n",
      "Weights: [-4.831   0.8374 -1.2855  0.1224  0.1447]\n",
      "MSE loss: 87.0632\n",
      "Iteration: 114700\n",
      "Gradient: [   9.4625   -8.311   -16.5615   86.3899 -282.4085]\n",
      "Weights: [-4.8288  0.8446 -1.2867  0.1222  0.1449]\n",
      "MSE loss: 86.7273\n",
      "Iteration: 114800\n",
      "Gradient: [  -6.3134   -4.0727  -34.2219    9.9016 -343.4943]\n",
      "Weights: [-4.8396  0.8363 -1.2861  0.1229  0.1451]\n",
      "MSE loss: 87.0349\n",
      "Iteration: 114900\n",
      "Gradient: [ -5.0074   4.4366  19.7458  24.4897 127.7735]\n",
      "Weights: [-4.8355  0.8356 -1.2882  0.1232  0.1451]\n",
      "MSE loss: 87.138\n",
      "Iteration: 115000\n",
      "Gradient: [  13.2759    4.9678   13.027   -52.2593 -129.5228]\n",
      "Weights: [-4.821   0.8336 -1.2867  0.1229  0.1451]\n",
      "MSE loss: 86.799\n",
      "Iteration: 115100\n",
      "Gradient: [ 11.2587  -1.6868 -33.0713 -18.3321 -69.8855]\n",
      "Weights: [-4.8154  0.8317 -1.2862  0.1237  0.1452]\n",
      "MSE loss: 87.1399\n",
      "Iteration: 115200\n",
      "Gradient: [  1.8081  -6.0334  22.2858 -45.6837 141.6398]\n",
      "Weights: [-4.8183  0.8399 -1.2896  0.1234  0.1452]\n",
      "MSE loss: 86.9083\n",
      "Iteration: 115300\n",
      "Gradient: [ -2.0222 -10.3633  -1.6453 -70.3133   4.9034]\n",
      "Weights: [-4.8323  0.8474 -1.2894  0.123   0.1451]\n",
      "MSE loss: 86.7224\n",
      "Iteration: 115400\n",
      "Gradient: [  -6.7891  -23.4072   35.3793  -56.7359 -277.1835]\n",
      "Weights: [-4.8394  0.8444 -1.2899  0.123   0.1451]\n",
      "MSE loss: 86.9546\n",
      "Iteration: 115500\n",
      "Gradient: [  4.8733  -0.7202   7.1495 -25.3501 -81.6379]\n",
      "Weights: [-4.834   0.8513 -1.2923  0.1232  0.1449]\n",
      "MSE loss: 86.7965\n",
      "Iteration: 115600\n",
      "Gradient: [-3.0681  2.2165 32.0389 -0.6874 32.9028]\n",
      "Weights: [-4.8389  0.859  -1.293   0.1232  0.1451]\n",
      "MSE loss: 86.7227\n",
      "Iteration: 115700\n",
      "Gradient: [   8.2699  -11.9612  -38.1701 -124.7191 -328.1975]\n",
      "Weights: [-4.8304  0.8469 -1.2916  0.1228  0.1454]\n",
      "MSE loss: 86.7381\n",
      "Iteration: 115800\n",
      "Gradient: [   0.9042   -9.5058   11.7459   51.4463 -203.9384]\n",
      "Weights: [-4.8274  0.8543 -1.2936  0.1229  0.1451]\n",
      "MSE loss: 86.7693\n",
      "Iteration: 115900\n",
      "Gradient: [  -7.8965    8.3299  -20.1864   15.1013 -168.7281]\n",
      "Weights: [-4.8296  0.8513 -1.2928  0.1234  0.145 ]\n",
      "MSE loss: 86.7144\n",
      "Iteration: 116000\n",
      "Gradient: [-1.063320e+01 -2.417000e-01  1.352490e+01  2.961970e+01 -3.157088e+02]\n",
      "Weights: [-4.8415  0.8614 -1.2956  0.1235  0.145 ]\n",
      "MSE loss: 86.6754\n",
      "Iteration: 116100\n",
      "Gradient: [ -3.1768   0.0404 -35.2618  31.4301  33.9849]\n",
      "Weights: [-4.8344  0.8607 -1.2949  0.1232  0.1449]\n",
      "MSE loss: 86.7041\n",
      "Iteration: 116200\n",
      "Gradient: [   6.2981   -4.5896   29.7772  192.3817 -132.1206]\n",
      "Weights: [-4.8378  0.8649 -1.2938  0.1226  0.1449]\n",
      "MSE loss: 86.7211\n",
      "Iteration: 116300\n",
      "Gradient: [  7.0284   5.1266 -18.6373  16.8141 239.1615]\n",
      "Weights: [-4.8406  0.8656 -1.2956  0.1232  0.1449]\n",
      "MSE loss: 86.6756\n",
      "Iteration: 116400\n",
      "Gradient: [  -3.0129   -2.9842  -15.5932 -133.5442  -44.2676]\n",
      "Weights: [-4.8419  0.8574 -1.2954  0.1244  0.1449]\n",
      "MSE loss: 86.6858\n",
      "Iteration: 116500\n",
      "Gradient: [  -0.8427    4.1486  -28.3934 -162.7577  184.0174]\n",
      "Weights: [-4.8503  0.8703 -1.2959  0.1243  0.1446]\n",
      "MSE loss: 86.6648\n",
      "Iteration: 116600\n",
      "Gradient: [  6.1752   3.2349 -26.9456 -75.1081   5.3106]\n",
      "Weights: [-4.8414  0.8641 -1.2929  0.1238  0.1443]\n",
      "MSE loss: 86.6875\n",
      "Iteration: 116700\n",
      "Gradient: [ -2.7173  -8.47   -12.7676 -22.8535 300.5859]\n",
      "Weights: [-4.8451  0.8562 -1.2919  0.1251  0.1445]\n",
      "MSE loss: 86.6914\n",
      "Iteration: 116800\n",
      "Gradient: [ -4.4249 -23.1332 -29.6266 -26.7533  89.0903]\n",
      "Weights: [-4.8351  0.8454 -1.2914  0.1255  0.1443]\n",
      "MSE loss: 86.7559\n",
      "Iteration: 116900\n",
      "Gradient: [-4.5219 20.2096  7.9372 64.6589 39.9604]\n",
      "Weights: [-4.834   0.8522 -1.2912  0.1259  0.144 ]\n",
      "MSE loss: 86.6624\n",
      "Iteration: 117000\n",
      "Gradient: [  0.6661 -15.7934 -13.6754  32.869  -65.0493]\n",
      "Weights: [-4.8337  0.852  -1.2891  0.1265  0.1436]\n",
      "MSE loss: 86.7543\n",
      "Iteration: 117100\n",
      "Gradient: [ -6.0797  -2.7159 -12.9815  95.3609  -3.8901]\n",
      "Weights: [-4.8332  0.853  -1.2898  0.1265  0.1436]\n",
      "MSE loss: 86.7253\n",
      "Iteration: 117200\n",
      "Gradient: [  -2.6836    5.1924  -59.9316  -19.558  -145.5981]\n",
      "Weights: [-4.8508  0.859  -1.2908  0.1259  0.1437]\n",
      "MSE loss: 86.6895\n",
      "Iteration: 117300\n",
      "Gradient: [ -8.6137   7.8939  29.9036  50.0881 249.8723]\n",
      "Weights: [-4.8512  0.8623 -1.2907  0.126   0.1437]\n",
      "MSE loss: 86.6721\n",
      "Iteration: 117400\n",
      "Gradient: [  -9.2026  -10.7894  -41.4221 -103.1584 -273.2218]\n",
      "Weights: [-4.8501  0.848  -1.2911  0.1264  0.1437]\n",
      "MSE loss: 87.385\n",
      "Iteration: 117500\n",
      "Gradient: [   2.1119    6.0328    5.7609 -111.8071 -104.419 ]\n",
      "Weights: [-4.8405  0.8538 -1.2927  0.1269  0.1438]\n",
      "MSE loss: 86.6069\n",
      "Iteration: 117600\n",
      "Gradient: [  3.9991   9.5467  40.9642 -59.8211  38.0371]\n",
      "Weights: [-4.832   0.8529 -1.2931  0.1267  0.1438]\n",
      "MSE loss: 86.6567\n",
      "Iteration: 117700\n",
      "Gradient: [   5.3598    1.5144   19.6227   48.723  -158.7614]\n",
      "Weights: [-4.841   0.8495 -1.2909  0.1266  0.1439]\n",
      "MSE loss: 86.6358\n",
      "Iteration: 117800\n",
      "Gradient: [  -0.6228    6.9207  -18.9735 -113.1965 -229.5226]\n",
      "Weights: [-4.8422  0.8508 -1.2901  0.126   0.1439]\n",
      "MSE loss: 86.6435\n",
      "Iteration: 117900\n",
      "Gradient: [  1.8592  -9.6615 -15.0964 -10.8755 149.4605]\n",
      "Weights: [-4.8356  0.8557 -1.2927  0.1259  0.1441]\n",
      "MSE loss: 86.6651\n",
      "Iteration: 118000\n",
      "Gradient: [ -10.5604   -3.7906   13.8272   48.6106 -129.3824]\n",
      "Weights: [-4.8343  0.8518 -1.2939  0.126   0.1443]\n",
      "MSE loss: 86.6327\n",
      "Iteration: 118100\n",
      "Gradient: [-1.8497 15.3387 11.8587 -9.6935 45.7928]\n",
      "Weights: [-4.8372  0.8513 -1.2908  0.1266  0.1442]\n",
      "MSE loss: 86.9356\n",
      "Iteration: 118200\n",
      "Gradient: [-1.04981e+01 -1.14920e+00 -1.32000e-02 -5.00699e+01  6.06254e+01]\n",
      "Weights: [-4.8392  0.84   -1.2924  0.1267  0.1445]\n",
      "MSE loss: 86.8952\n",
      "Iteration: 118300\n",
      "Gradient: [ 6.0684 -5.9695 22.8046 96.3237 23.9956]\n",
      "Weights: [-4.8357  0.8491 -1.292   0.1257  0.1446]\n",
      "MSE loss: 86.7397\n",
      "Iteration: 118400\n",
      "Gradient: [-10.1424  25.9854 -25.4798 -42.2272 100.7275]\n",
      "Weights: [-4.8421  0.8416 -1.2902  0.1255  0.1446]\n",
      "MSE loss: 86.9203\n",
      "Iteration: 118500\n",
      "Gradient: [  -8.0565  -16.3887  -33.4004    7.213  -217.6398]\n",
      "Weights: [-4.8353  0.8437 -1.2895  0.1251  0.1442]\n",
      "MSE loss: 86.8092\n",
      "Iteration: 118600\n",
      "Gradient: [  -2.211    -8.7468  -19.4881   43.2995 -325.4605]\n",
      "Weights: [-4.826   0.8432 -1.2878  0.1248  0.1441]\n",
      "MSE loss: 86.72\n",
      "Iteration: 118700\n",
      "Gradient: [ -4.0314 -14.883   -7.1529 -64.5108 -36.2863]\n",
      "Weights: [-4.8527  0.8554 -1.2909  0.1247  0.144 ]\n",
      "MSE loss: 87.2525\n",
      "Iteration: 118800\n",
      "Gradient: [ -4.1762  -1.8366 -33.5754 -12.8877  44.9662]\n",
      "Weights: [-4.8435  0.8488 -1.2876  0.1249  0.1441]\n",
      "MSE loss: 86.6751\n",
      "Iteration: 118900\n",
      "Gradient: [  6.6835  10.1235  -8.4929  60.0376 113.6399]\n",
      "Weights: [-4.8295  0.8466 -1.2879  0.1259  0.1437]\n",
      "MSE loss: 86.7018\n",
      "Iteration: 119000\n",
      "Gradient: [ -0.3753   6.9868  49.0631  45.7903 -78.7041]\n",
      "Weights: [-4.827   0.8372 -1.2859  0.1265  0.1437]\n",
      "MSE loss: 86.6598\n",
      "Iteration: 119100\n",
      "Gradient: [  4.517    3.5843  -0.3858 -66.5739 106.0581]\n",
      "Weights: [-4.833   0.8404 -1.2858  0.1266  0.1436]\n",
      "MSE loss: 86.6293\n",
      "Iteration: 119200\n",
      "Gradient: [ 4.3186  4.0843 -3.1968 -2.732  29.4282]\n",
      "Weights: [-4.8269  0.8369 -1.2855  0.1269  0.1433]\n",
      "MSE loss: 86.6757\n",
      "Iteration: 119300\n",
      "Gradient: [   2.4431   -2.3031  -19.9329   77.4211 -236.9631]\n",
      "Weights: [-4.8397  0.8423 -1.2827  0.1265  0.1433]\n",
      "MSE loss: 86.6734\n",
      "Iteration: 119400\n",
      "Gradient: [  -0.3785   -3.4765   -9.0919 -116.6452   23.6291]\n",
      "Weights: [-4.8313  0.8415 -1.2827  0.1258  0.1434]\n",
      "MSE loss: 86.7005\n",
      "Iteration: 119500\n",
      "Gradient: [  9.7357   3.1947 -19.3651 -97.659  131.1794]\n",
      "Weights: [-4.824   0.8322 -1.2827  0.1266  0.1435]\n",
      "MSE loss: 86.6946\n",
      "Iteration: 119600\n",
      "Gradient: [ 3.1696  4.7236 32.9467 21.6166 57.5815]\n",
      "Weights: [-4.8209  0.8238 -1.2807  0.1276  0.1435]\n",
      "MSE loss: 86.988\n",
      "Iteration: 119700\n",
      "Gradient: [   3.2723   -7.3451   13.2976  -18.5505 -388.4541]\n",
      "Weights: [-4.8188  0.8281 -1.284   0.1274  0.1434]\n",
      "MSE loss: 86.7052\n",
      "Iteration: 119800\n",
      "Gradient: [   1.3894   -0.5711    0.4859  149.4065 -107.1343]\n",
      "Weights: [-4.8162  0.8284 -1.2849  0.1273  0.1435]\n",
      "MSE loss: 86.7664\n",
      "Iteration: 119900\n",
      "Gradient: [ -9.7317 -12.6801  28.6327  13.382  248.8452]\n",
      "Weights: [-4.835   0.8326 -1.2868  0.1288  0.1434]\n",
      "MSE loss: 86.698\n",
      "Iteration: 120000\n",
      "Gradient: [  -2.693    -2.8359    4.1546   52.6783 -196.1229]\n",
      "Weights: [-4.8315  0.8286 -1.2879  0.1289  0.1433]\n",
      "MSE loss: 86.8625\n",
      "Iteration: 120100\n",
      "Gradient: [  5.6088  11.4707  -1.48    69.4938 188.9331]\n",
      "Weights: [-4.8154  0.8223 -1.2853  0.129   0.1432]\n",
      "MSE loss: 86.7016\n",
      "Iteration: 120200\n",
      "Gradient: [   4.2753   -9.5508  -19.0768  -63.4047 -258.6057]\n",
      "Weights: [-4.8227  0.8255 -1.2862  0.1288  0.1432]\n",
      "MSE loss: 86.695\n",
      "Iteration: 120300\n",
      "Gradient: [ -3.3882   6.6645 -48.7734  -0.097  -41.0492]\n",
      "Weights: [-4.8303  0.8298 -1.2884  0.1288  0.1434]\n",
      "MSE loss: 86.7471\n",
      "Iteration: 120400\n",
      "Gradient: [   0.2982   -9.3757   -7.0384   15.6321 -191.4957]\n",
      "Weights: [-4.825   0.8295 -1.2884  0.1287  0.1434]\n",
      "MSE loss: 86.6854\n",
      "Iteration: 120500\n",
      "Gradient: [  -0.5183  -19.5684   -7.0417   -8.0774 -110.2275]\n",
      "Weights: [-4.8343  0.8377 -1.2909  0.1299  0.1434]\n",
      "MSE loss: 86.6914\n",
      "Iteration: 120600\n",
      "Gradient: [-1.9542 -3.3346  1.5862 41.2611 87.7049]\n",
      "Weights: [-4.8354  0.8342 -1.2864  0.1292  0.1432]\n",
      "MSE loss: 86.6836\n",
      "Iteration: 120700\n",
      "Gradient: [  0.4572 -11.7623 -18.3409 104.3028 237.5626]\n",
      "Weights: [-4.8193  0.8285 -1.2849  0.1286  0.1431]\n",
      "MSE loss: 86.6846\n",
      "Iteration: 120800\n",
      "Gradient: [ -3.7116  -5.2812  46.3635 -53.6831 129.2707]\n",
      "Weights: [-4.8193  0.8196 -1.2819  0.1284  0.1432]\n",
      "MSE loss: 86.6734\n",
      "Iteration: 120900\n",
      "Gradient: [ 6.450500e+00  3.566500e+00 -8.740000e-02 -9.332530e+01  3.778004e+02]\n",
      "Weights: [-4.8282  0.8253 -1.2848  0.1288  0.1432]\n",
      "MSE loss: 86.6791\n",
      "Iteration: 121000\n",
      "Gradient: [  7.5053   0.7709  -2.7933 -12.8794 120.9515]\n",
      "Weights: [-4.8217  0.8286 -1.2838  0.1278  0.1434]\n",
      "MSE loss: 86.724\n",
      "Iteration: 121100\n",
      "Gradient: [ -2.1331 -17.1558  -6.2465 -75.3237 110.6684]\n",
      "Weights: [-4.8432  0.8396 -1.2855  0.1281  0.1431]\n",
      "MSE loss: 86.6861\n",
      "Iteration: 121200\n",
      "Gradient: [ -7.7701  -5.4925 -38.6677  53.006   22.6604]\n",
      "Weights: [-4.8421  0.85   -1.2876  0.1274  0.143 ]\n",
      "MSE loss: 86.6312\n",
      "Iteration: 121300\n",
      "Gradient: [ 8.7252 -4.3013 34.503   0.4039  0.1705]\n",
      "Weights: [-4.8376  0.8512 -1.2882  0.1272  0.1431]\n",
      "MSE loss: 86.6419\n",
      "Iteration: 121400\n",
      "Gradient: [  -8.9166    5.7885   -9.057   -22.7184 -124.0889]\n",
      "Weights: [-4.8442  0.8534 -1.2895  0.1275  0.143 ]\n",
      "MSE loss: 86.6728\n",
      "Iteration: 121500\n",
      "Gradient: [ 9.1365 -3.0429 17.5484 47.5503 39.7088]\n",
      "Weights: [-4.8211  0.8494 -1.2918  0.1279  0.1433]\n",
      "MSE loss: 86.9816\n",
      "Iteration: 121600\n",
      "Gradient: [  3.1258   5.7392   8.0127 -65.822   74.345 ]\n",
      "Weights: [-4.8202  0.8376 -1.291   0.1286  0.1435]\n",
      "MSE loss: 86.7084\n",
      "Iteration: 121700\n",
      "Gradient: [   7.463     5.2508  -10.0516   34.5595 -187.3872]\n",
      "Weights: [-4.8072  0.8236 -1.2908  0.1296  0.1437]\n",
      "MSE loss: 86.8991\n",
      "Iteration: 121800\n",
      "Gradient: [  15.1066   16.8583   25.963    94.2521 -357.0887]\n",
      "Weights: [-4.8172  0.8348 -1.29    0.1291  0.1434]\n",
      "MSE loss: 86.8639\n",
      "Iteration: 121900\n",
      "Gradient: [   1.8451    7.2103  -42.4808   21.5759 -297.4396]\n",
      "Weights: [-4.8219  0.8341 -1.2877  0.1276  0.1433]\n",
      "MSE loss: 86.7501\n",
      "Iteration: 122000\n",
      "Gradient: [ -1.3134  -2.4967 -27.2359  11.9829  46.4353]\n",
      "Weights: [-4.83    0.8335 -1.2847  0.1274  0.1433]\n",
      "MSE loss: 86.6274\n",
      "Iteration: 122100\n",
      "Gradient: [ -4.7675   2.961  -19.7334  87.4935  40.5526]\n",
      "Weights: [-4.8411  0.8383 -1.2844  0.1268  0.1433]\n",
      "MSE loss: 86.7639\n",
      "Iteration: 122200\n",
      "Gradient: [ 9.330000e-02 -1.409940e+01 -1.617840e+01  1.890160e+01  1.657001e+02]\n",
      "Weights: [-4.825   0.8288 -1.283   0.1274  0.1433]\n",
      "MSE loss: 86.6453\n",
      "Iteration: 122300\n",
      "Gradient: [ 1.2292 -2.935  51.6867 -6.9688 67.2129]\n",
      "Weights: [-4.8307  0.8323 -1.285   0.1273  0.1436]\n",
      "MSE loss: 86.6439\n",
      "Iteration: 122400\n",
      "Gradient: [ -2.9748 -31.3144 -34.0854  -4.7906  27.1447]\n",
      "Weights: [-4.8331  0.8301 -1.2869  0.1276  0.1436]\n",
      "MSE loss: 86.9425\n",
      "Iteration: 122500\n",
      "Gradient: [   3.786     6.3564    8.1074  -12.4322 -347.3259]\n",
      "Weights: [-4.8256  0.8354 -1.2841  0.1277  0.1434]\n",
      "MSE loss: 86.9433\n",
      "Iteration: 122600\n",
      "Gradient: [ -11.4902    5.8277   33.3624   14.3356 -111.6398]\n",
      "Weights: [-4.8273  0.8337 -1.2857  0.1275  0.1432]\n",
      "MSE loss: 86.6977\n",
      "Iteration: 122700\n",
      "Gradient: [ -4.1472  -1.722  -36.4923 -14.0546 165.813 ]\n",
      "Weights: [-4.8145  0.8338 -1.2857  0.1266  0.1435]\n",
      "MSE loss: 86.8812\n",
      "Iteration: 122800\n",
      "Gradient: [  5.9243  18.7744 -12.522    7.2626 306.4475]\n",
      "Weights: [-4.8265  0.8368 -1.2851  0.1265  0.1436]\n",
      "MSE loss: 86.6748\n",
      "Iteration: 122900\n",
      "Gradient: [ -3.0629  -3.4623 -13.7244  42.9455 325.3868]\n",
      "Weights: [-4.8215  0.8334 -1.2827  0.1261  0.1435]\n",
      "MSE loss: 86.743\n",
      "Iteration: 123000\n",
      "Gradient: [ -1.8157  10.0403  25.5372 110.7974 -84.1947]\n",
      "Weights: [-4.8248  0.8441 -1.2836  0.1264  0.1433]\n",
      "MSE loss: 87.1186\n",
      "Iteration: 123100\n",
      "Gradient: [  7.8731   7.96   -10.1763 -26.6119 -83.0495]\n",
      "Weights: [-4.8291  0.8423 -1.2828  0.1262  0.1433]\n",
      "MSE loss: 86.8151\n",
      "Iteration: 123200\n",
      "Gradient: [  -6.5806  -14.0575   -5.4144   -3.6438 -136.4786]\n",
      "Weights: [-4.8454  0.8456 -1.2844  0.1269  0.143 ]\n",
      "MSE loss: 86.744\n",
      "Iteration: 123300\n",
      "Gradient: [ -7.8761  11.6694 -12.2936 -55.3273 119.4091]\n",
      "Weights: [-4.857   0.8534 -1.2865  0.1273  0.1433]\n",
      "MSE loss: 86.8358\n",
      "Iteration: 123400\n",
      "Gradient: [  4.0882  -5.719  -38.1202  60.3056 186.1131]\n",
      "Weights: [-4.8313  0.8481 -1.2874  0.1272  0.143 ]\n",
      "MSE loss: 86.7094\n",
      "Iteration: 123500\n",
      "Gradient: [  -1.2337  -21.9409  -28.5294  -22.0577 -105.068 ]\n",
      "Weights: [-4.8329  0.8404 -1.2889  0.1278  0.1433]\n",
      "MSE loss: 86.6712\n",
      "Iteration: 123600\n",
      "Gradient: [-11.4385   0.9341  12.2566  59.1272  70.5   ]\n",
      "Weights: [-4.8331  0.8394 -1.287   0.1277  0.1433]\n",
      "MSE loss: 86.6111\n",
      "Iteration: 123700\n",
      "Gradient: [  1.6698  -9.0963 -18.592  -25.3113 -12.9241]\n",
      "Weights: [-4.8471  0.8369 -1.2866  0.1275  0.1434]\n",
      "MSE loss: 87.1858\n",
      "Iteration: 123800\n",
      "Gradient: [  -4.787     9.137    -5.1278   42.6029 -267.2287]\n",
      "Weights: [-4.8453  0.8465 -1.2857  0.1267  0.1433]\n",
      "MSE loss: 86.6748\n",
      "Iteration: 123900\n",
      "Gradient: [  -4.6958    4.0607  -17.2078  -32.1107 -295.121 ]\n",
      "Weights: [-4.845   0.8472 -1.2853  0.1262  0.1434]\n",
      "MSE loss: 86.6762\n",
      "Iteration: 124000\n",
      "Gradient: [   7.105   -12.8752   -0.4088  -70.9113 -306.531 ]\n",
      "Weights: [-4.8597  0.859  -1.2879  0.1267  0.1434]\n",
      "MSE loss: 86.8159\n",
      "Iteration: 124100\n",
      "Gradient: [  8.6873   7.117   -3.1222 -55.7833 200.5233]\n",
      "Weights: [-4.8362  0.8606 -1.2917  0.1268  0.1432]\n",
      "MSE loss: 86.7713\n",
      "Iteration: 124200\n",
      "Gradient: [ 9.0348 14.0884 12.3744 89.5982 82.3687]\n",
      "Weights: [-4.844   0.8664 -1.2934  0.1271  0.1431]\n",
      "MSE loss: 86.7143\n",
      "Iteration: 124300\n",
      "Gradient: [  4.0376   7.7379  -1.311   72.1102 -86.7262]\n",
      "Weights: [-4.8399  0.8618 -1.2928  0.1272  0.1432]\n",
      "MSE loss: 86.6929\n",
      "Iteration: 124400\n",
      "Gradient: [ 5.4746  4.9366  3.8876 -4.5696 30.9404]\n",
      "Weights: [-4.8331  0.8519 -1.2947  0.1288  0.1432]\n",
      "MSE loss: 86.6197\n",
      "Iteration: 124500\n",
      "Gradient: [  -6.6983   26.9124    1.6897   70.5946 -114.679 ]\n",
      "Weights: [-4.8342  0.8459 -1.2938  0.1303  0.1434]\n",
      "MSE loss: 86.8179\n",
      "Iteration: 124600\n",
      "Gradient: [  7.0774   1.0375  36.0498  27.9216 254.5711]\n",
      "Weights: [-4.8382  0.8485 -1.2949  0.1307  0.1431]\n",
      "MSE loss: 86.6179\n",
      "Iteration: 124700\n",
      "Gradient: [ -0.8567 -26.8182 -46.3914   1.1136  54.0552]\n",
      "Weights: [-4.8364  0.8425 -1.2962  0.1308  0.143 ]\n",
      "MSE loss: 86.92\n",
      "Iteration: 124800\n",
      "Gradient: [  5.9898  -8.7773 -23.3942 106.009   11.7422]\n",
      "Weights: [-4.8488  0.849  -1.2909  0.1304  0.1429]\n",
      "MSE loss: 86.6939\n",
      "Iteration: 124900\n",
      "Gradient: [ -0.2033  -1.3078  27.1269 -61.4032 -23.6579]\n",
      "Weights: [-4.8361  0.8523 -1.295   0.1309  0.1424]\n",
      "MSE loss: 86.6245\n",
      "Iteration: 125000\n",
      "Gradient: [ -2.0999   4.8029 -28.9407  50.5968  70.629 ]\n",
      "Weights: [-4.8276  0.8478 -1.2947  0.1319  0.1426]\n",
      "MSE loss: 86.7744\n",
      "Iteration: 125100\n",
      "Gradient: [  -0.5685  -11.8992    6.7666   -6.8082 -156.4015]\n",
      "Weights: [-4.8345  0.8407 -1.296   0.1327  0.1426]\n",
      "MSE loss: 86.5957\n",
      "Iteration: 125200\n",
      "Gradient: [  9.6706   5.7397  13.0856  -7.6913 -80.3956]\n",
      "Weights: [-4.8307  0.8416 -1.2941  0.1331  0.1424]\n",
      "MSE loss: 86.6218\n",
      "Iteration: 125300\n",
      "Gradient: [ 1.066510e+01  1.051900e+00  1.312000e-01 -1.995620e+01  2.718795e+02]\n",
      "Weights: [-4.8177  0.8399 -1.294   0.133   0.1424]\n",
      "MSE loss: 87.0049\n",
      "Iteration: 125400\n",
      "Gradient: [ -1.5947  -9.9224 -18.1645 -82.021   89.9088]\n",
      "Weights: [-4.8339  0.8423 -1.2959  0.1325  0.1423]\n",
      "MSE loss: 86.7421\n",
      "Iteration: 125500\n",
      "Gradient: [ -0.7208  -4.3421  11.6501 -18.5306 -47.8723]\n",
      "Weights: [-4.8454  0.8491 -1.2951  0.133   0.1422]\n",
      "MSE loss: 86.585\n",
      "Iteration: 125600\n",
      "Gradient: [   5.2245   -8.3864   -7.4772    3.2594 -353.5712]\n",
      "Weights: [-4.8368  0.85   -1.2972  0.1335  0.1421]\n",
      "MSE loss: 86.5185\n",
      "Iteration: 125700\n",
      "Gradient: [ 3.194000e-01 -6.649500e+00  5.297700e+00 -5.325270e+01 -3.752554e+02]\n",
      "Weights: [-4.83    0.8405 -1.2992  0.1345  0.1421]\n",
      "MSE loss: 86.7035\n",
      "Iteration: 125800\n",
      "Gradient: [ -1.2173  15.8361 -32.629   26.6856 222.8711]\n",
      "Weights: [-4.8478  0.8509 -1.2978  0.135   0.1417]\n",
      "MSE loss: 86.6141\n",
      "Iteration: 125900\n",
      "Gradient: [ 5.717  -8.3769 -9.2066 34.0768 36.4952]\n",
      "Weights: [-4.8415  0.8488 -1.2951  0.1353  0.1415]\n",
      "MSE loss: 86.6066\n",
      "Iteration: 126000\n",
      "Gradient: [ 2.181   3.9296  7.9279 21.3325 20.6513]\n",
      "Weights: [-4.8244  0.8345 -1.2932  0.1345  0.1416]\n",
      "MSE loss: 86.5618\n",
      "Iteration: 126100\n",
      "Gradient: [ -4.0641  10.2635  -6.3801 -21.9707 417.4149]\n",
      "Weights: [-4.818   0.8281 -1.2918  0.1348  0.1417]\n",
      "MSE loss: 86.6093\n",
      "Iteration: 126200\n",
      "Gradient: [ -0.846   20.8943   0.1462 122.6056   7.9513]\n",
      "Weights: [-4.8334  0.8287 -1.2913  0.1355  0.1415]\n",
      "MSE loss: 86.6003\n",
      "Iteration: 126300\n",
      "Gradient: [  8.9109   6.2971  -3.2241  26.7044 -10.8536]\n",
      "Weights: [-4.8081  0.8291 -1.2961  0.1364  0.1415]\n",
      "MSE loss: 86.8262\n",
      "Iteration: 126400\n",
      "Gradient: [-4.7373  7.4287 14.7767 50.9726 74.9432]\n",
      "Weights: [-4.8263  0.8278 -1.294   0.1364  0.1415]\n",
      "MSE loss: 86.5346\n",
      "Iteration: 126500\n",
      "Gradient: [  3.0088   8.7984 -26.9018 -55.563  138.509 ]\n",
      "Weights: [-4.8297  0.84   -1.2956  0.1367  0.1414]\n",
      "MSE loss: 86.602\n",
      "Iteration: 126600\n",
      "Gradient: [ -3.4313  -6.8839 -33.5696  48.89   -40.525 ]\n",
      "Weights: [-4.8317  0.8375 -1.2981  0.1379  0.1409]\n",
      "MSE loss: 86.5662\n",
      "Iteration: 126700\n",
      "Gradient: [  7.8513  11.5929  27.5974 108.3641  64.9892]\n",
      "Weights: [-4.8316  0.8464 -1.2969  0.1383  0.1406]\n",
      "MSE loss: 86.7036\n",
      "Iteration: 126800\n",
      "Gradient: [-6.3332 17.3346 -3.547  20.2031 -8.6131]\n",
      "Weights: [-4.8405  0.8465 -1.2986  0.138   0.1408]\n",
      "MSE loss: 86.4982\n",
      "Iteration: 126900\n",
      "Gradient: [  -1.3076   -5.9664   14.2209   41.4572 -145.1829]\n",
      "Weights: [-4.8303  0.8468 -1.3004  0.1382  0.1409]\n",
      "MSE loss: 86.5188\n",
      "Iteration: 127000\n",
      "Gradient: [  6.4262 -10.7588  10.2421 -43.0558 184.9374]\n",
      "Weights: [-4.8347  0.8426 -1.299   0.1387  0.141 ]\n",
      "MSE loss: 86.5075\n",
      "Iteration: 127100\n",
      "Gradient: [  0.8883   3.2013  -0.6565 114.5999 100.3227]\n",
      "Weights: [-4.8461  0.8527 -1.3012  0.1387  0.141 ]\n",
      "MSE loss: 86.5516\n",
      "Iteration: 127200\n",
      "Gradient: [   0.3898    4.0919  -20.5697  -22.9558 -111.3759]\n",
      "Weights: [-4.8326  0.8534 -1.3039  0.1388  0.1408]\n",
      "MSE loss: 86.5129\n",
      "Iteration: 127300\n",
      "Gradient: [  -4.16     -3.2222  -32.07     43.9465 -123.8664]\n",
      "Weights: [-4.8453  0.8507 -1.3062  0.1389  0.1412]\n",
      "MSE loss: 86.7489\n",
      "Iteration: 127400\n",
      "Gradient: [ -12.186   -16.5054  -44.9497 -105.2441 -190.9902]\n",
      "Weights: [-4.842   0.8416 -1.3041  0.1387  0.1414]\n",
      "MSE loss: 86.8977\n",
      "Iteration: 127500\n",
      "Gradient: [ -0.4065  -1.5425   5.0642 133.6807 -43.4153]\n",
      "Weights: [-4.844   0.8537 -1.3037  0.1384  0.1411]\n",
      "MSE loss: 86.4888\n",
      "Iteration: 127600\n",
      "Gradient: [ 4.3001  3.6473 18.005  22.9145 68.427 ]\n",
      "Weights: [-4.8441  0.855  -1.3022  0.1379  0.1411]\n",
      "MSE loss: 86.4864\n",
      "Iteration: 127700\n",
      "Gradient: [  3.6308   1.888    6.6215  78.5975 123.3676]\n",
      "Weights: [-4.8468  0.8707 -1.3053  0.1374  0.1411]\n",
      "MSE loss: 86.6831\n",
      "Iteration: 127800\n",
      "Gradient: [ 13.5495 -17.2069  17.0287 -32.1739 210.6482]\n",
      "Weights: [-4.858   0.8626 -1.3042  0.1375  0.1412]\n",
      "MSE loss: 86.7238\n",
      "Iteration: 127900\n",
      "Gradient: [  -0.452    14.0633   -8.405     5.6388 -425.0746]\n",
      "Weights: [-4.8484  0.8609 -1.3035  0.1369  0.1413]\n",
      "MSE loss: 86.5139\n",
      "Iteration: 128000\n",
      "Gradient: [ -8.4386  18.6754  -4.6371 -59.2713 340.0972]\n",
      "Weights: [-4.839   0.866  -1.3084  0.1373  0.1414]\n",
      "MSE loss: 86.5101\n",
      "Iteration: 128100\n",
      "Gradient: [ 3.4438 -7.4609 -0.2406 47.0765  2.053 ]\n",
      "Weights: [-4.8406  0.8645 -1.309   0.1372  0.1417]\n",
      "MSE loss: 86.4611\n",
      "Iteration: 128200\n",
      "Gradient: [ -7.3508 -11.3427 -12.2464 -49.3401  66.9014]\n",
      "Weights: [-4.8438  0.8666 -1.3111  0.1386  0.1414]\n",
      "MSE loss: 86.4409\n",
      "Iteration: 128300\n",
      "Gradient: [   2.4302    8.3649  -50.094   -50.9909 -147.1077]\n",
      "Weights: [-4.8417  0.8643 -1.3125  0.1385  0.1413]\n",
      "MSE loss: 86.6595\n",
      "Iteration: 128400\n",
      "Gradient: [ -6.4337   9.1342  -4.2081 102.8705 178.4219]\n",
      "Weights: [-4.8409  0.8714 -1.3137  0.1394  0.1414]\n",
      "MSE loss: 86.5795\n",
      "Iteration: 128500\n",
      "Gradient: [ -0.7021   9.6232 -65.0913  67.8145  61.8498]\n",
      "Weights: [-4.8373  0.8613 -1.3147  0.1394  0.1414]\n",
      "MSE loss: 86.6795\n",
      "Iteration: 128600\n",
      "Gradient: [ 0.2899 15.4743 19.0706 14.7636 49.871 ]\n",
      "Weights: [-4.8187  0.8589 -1.3142  0.1399  0.1412]\n",
      "MSE loss: 86.7578\n",
      "Iteration: 128700\n",
      "Gradient: [   1.5555   -2.8701   42.1153  -33.3153 -163.6122]\n",
      "Weights: [-4.83    0.8615 -1.3149  0.1404  0.1413]\n",
      "MSE loss: 86.4746\n",
      "Iteration: 128800\n",
      "Gradient: [  5.0254  -1.2523  14.9327 116.944  -59.2229]\n",
      "Weights: [-4.8299  0.8559 -1.313   0.1403  0.1414]\n",
      "MSE loss: 86.4397\n",
      "Iteration: 128900\n",
      "Gradient: [ -1.7558  13.024   38.6986  37.7209 -27.1657]\n",
      "Weights: [-4.8485  0.8683 -1.3127  0.1408  0.1413]\n",
      "MSE loss: 86.8151\n",
      "Iteration: 129000\n",
      "Gradient: [   1.3047  -19.0708  -13.2545  -98.5396 -141.9225]\n",
      "Weights: [-4.8295  0.8553 -1.312   0.1405  0.1411]\n",
      "MSE loss: 86.4437\n",
      "Iteration: 129100\n",
      "Gradient: [ -4.3667   4.7855  21.5856  -4.9856 -43.5444]\n",
      "Weights: [-4.8343  0.858  -1.3151  0.1414  0.1412]\n",
      "MSE loss: 86.3996\n",
      "Iteration: 129200\n",
      "Gradient: [  -6.5368   -0.7121  -32.752   -49.961  -315.1205]\n",
      "Weights: [-4.8335  0.8602 -1.3173  0.1412  0.1411]\n",
      "MSE loss: 86.6012\n",
      "Iteration: 129300\n",
      "Gradient: [ -5.5354   0.2787  29.9978 -51.3111 -13.6284]\n",
      "Weights: [-4.8339  0.8599 -1.3155  0.1408  0.1412]\n",
      "MSE loss: 86.4276\n",
      "Iteration: 129400\n",
      "Gradient: [   3.6668   12.9423  -24.9242   84.2651 -105.4642]\n",
      "Weights: [-4.8328  0.8597 -1.3167  0.1418  0.1411]\n",
      "MSE loss: 86.4023\n",
      "Iteration: 129500\n",
      "Gradient: [  -4.8664   -8.9232   25.6473    9.0717 -125.8144]\n",
      "Weights: [-4.834   0.8568 -1.3177  0.1418  0.1412]\n",
      "MSE loss: 86.5256\n",
      "Iteration: 129600\n",
      "Gradient: [ -4.5383   1.8719 -11.0662 -76.2782 -62.0069]\n",
      "Weights: [-4.8424  0.8579 -1.3168  0.1421  0.1411]\n",
      "MSE loss: 86.5717\n",
      "Iteration: 129700\n",
      "Gradient: [  2.6571 -18.5675  21.0589  72.6751  87.3858]\n",
      "Weights: [-4.8311  0.857  -1.3144  0.1419  0.1409]\n",
      "MSE loss: 86.4147\n",
      "Iteration: 129800\n",
      "Gradient: [   2.0798    1.4443   -0.6401   23.628  -176.7624]\n",
      "Weights: [-4.859   0.8742 -1.3162  0.1424  0.1404]\n",
      "MSE loss: 86.5987\n",
      "Iteration: 129900\n",
      "Gradient: [  -4.2817  -19.7151   19.2303  -57.4641 -361.5979]\n",
      "Weights: [-4.8466  0.8722 -1.3181  0.142   0.1406]\n",
      "MSE loss: 86.5634\n",
      "Iteration: 130000\n",
      "Gradient: [  2.6015   2.5984 -24.7175  59.2972 325.8881]\n",
      "Weights: [-4.8309  0.8763 -1.3212  0.1429  0.1407]\n",
      "MSE loss: 86.8571\n",
      "Iteration: 130100\n",
      "Gradient: [  3.4211   5.3161 -13.3734 -49.7383 312.2014]\n",
      "Weights: [-4.8482  0.879  -1.3208  0.1425  0.1409]\n",
      "MSE loss: 86.4284\n",
      "Iteration: 130200\n",
      "Gradient: [  4.0832 -10.2648 -23.0563 -20.1779 109.3295]\n",
      "Weights: [-4.8456  0.8756 -1.3213  0.1431  0.1409]\n",
      "MSE loss: 86.4057\n",
      "Iteration: 130300\n",
      "Gradient: [ -0.0961  -2.0709   8.4596  78.6083 -32.335 ]\n",
      "Weights: [-4.8589  0.8882 -1.3226  0.1424  0.1409]\n",
      "MSE loss: 86.4899\n",
      "Iteration: 130400\n",
      "Gradient: [  1.5073  10.5401  -5.3263 -89.9455 117.0254]\n",
      "Weights: [-4.8484  0.8839 -1.3249  0.1419  0.1411]\n",
      "MSE loss: 86.4382\n",
      "Iteration: 130500\n",
      "Gradient: [  -5.9137   -3.1903   13.8447  -26.1384 -236.082 ]\n",
      "Weights: [-4.8464  0.8782 -1.3244  0.1425  0.1412]\n",
      "MSE loss: 86.3754\n",
      "Iteration: 130600\n",
      "Gradient: [ 10.892   11.2262  36.4512  -6.5237 153.533 ]\n",
      "Weights: [-4.8378  0.8825 -1.3265  0.1419  0.1416]\n",
      "MSE loss: 86.5509\n",
      "Iteration: 130700\n",
      "Gradient: [ 9.8347 -1.651   0.9305 39.1968 79.4038]\n",
      "Weights: [-4.8381  0.8791 -1.3288  0.1424  0.1417]\n",
      "MSE loss: 86.3848\n",
      "Iteration: 130800\n",
      "Gradient: [  3.035    9.6953 -47.9028  -2.1     26.0762]\n",
      "Weights: [-4.844   0.8751 -1.3286  0.143   0.1417]\n",
      "MSE loss: 86.4329\n",
      "Iteration: 130900\n",
      "Gradient: [ -0.5294   1.9913  -0.7288 -22.3121 -27.6054]\n",
      "Weights: [-4.8447  0.8778 -1.3257  0.1424  0.1416]\n",
      "MSE loss: 86.3833\n",
      "Iteration: 131000\n",
      "Gradient: [   6.2057  -14.282     6.6049  -22.1961 -270.2666]\n",
      "Weights: [-4.844   0.8689 -1.3242  0.1426  0.1416]\n",
      "MSE loss: 86.4771\n",
      "Iteration: 131100\n",
      "Gradient: [  9.9305  10.7166   4.5934 -17.3153 340.5592]\n",
      "Weights: [-4.8341  0.8706 -1.3229  0.1423  0.1416]\n",
      "MSE loss: 86.552\n",
      "Iteration: 131200\n",
      "Gradient: [ -9.1776   5.9665 -31.7253  42.5347 125.0951]\n",
      "Weights: [-4.8382  0.875  -1.3267  0.1427  0.1413]\n",
      "MSE loss: 86.4349\n",
      "Iteration: 131300\n",
      "Gradient: [  2.852  -10.0895  -8.5956 -42.9715 -23.3782]\n",
      "Weights: [-4.8424  0.8796 -1.3249  0.1416  0.1415]\n",
      "MSE loss: 86.3835\n",
      "Iteration: 131400\n",
      "Gradient: [ -9.2132   9.7995 -13.9427  26.9667 -12.1146]\n",
      "Weights: [-4.8457  0.8786 -1.3243  0.1419  0.1412]\n",
      "MSE loss: 86.4754\n",
      "Iteration: 131500\n",
      "Gradient: [   0.7741   -3.8211  -16.3552 -104.327     9.7333]\n",
      "Weights: [-4.8609  0.8802 -1.3209  0.1415  0.1411]\n",
      "MSE loss: 86.6691\n",
      "Iteration: 131600\n",
      "Gradient: [  -2.5819   11.6582  -18.9646  -12.2577 -216.1869]\n",
      "Weights: [-4.8302  0.8737 -1.3212  0.1418  0.141 ]\n",
      "MSE loss: 86.5917\n",
      "Iteration: 131700\n",
      "Gradient: [  2.7013 -10.4618 -10.5701 -29.6863  58.2607]\n",
      "Weights: [-4.8569  0.8855 -1.3206  0.1416  0.1411]\n",
      "MSE loss: 86.5034\n",
      "Iteration: 131800\n",
      "Gradient: [  -1.2876    4.2534   -5.3203 -112.8513  -98.1465]\n",
      "Weights: [-4.8518  0.8963 -1.3263  0.1406  0.1411]\n",
      "MSE loss: 86.5774\n",
      "Iteration: 131900\n",
      "Gradient: [   8.35      3.8489  -40.1285    9.1189 -421.4699]\n",
      "Weights: [-4.8492  0.8865 -1.3263  0.1409  0.1415]\n",
      "MSE loss: 86.4955\n",
      "Iteration: 132000\n",
      "Gradient: [  1.6893   5.3693  12.4467  40.4904 -76.0873]\n",
      "Weights: [-4.8638  0.8981 -1.3263  0.1411  0.1417]\n",
      "MSE loss: 86.6708\n",
      "Iteration: 132100\n",
      "Gradient: [ -9.1457  -2.688  -48.8261   8.7762 -22.4388]\n",
      "Weights: [-4.8647  0.8956 -1.3281  0.1408  0.1417]\n",
      "MSE loss: 86.5652\n",
      "Iteration: 132200\n",
      "Gradient: [  -2.6985   -3.0258   -3.4301 -159.6137 -288.9085]\n",
      "Weights: [-4.8524  0.8936 -1.3268  0.1407  0.1417]\n",
      "MSE loss: 86.448\n",
      "Iteration: 132300\n",
      "Gradient: [  0.4359   6.9398 -28.6937 -23.6597 -14.1981]\n",
      "Weights: [-4.8576  0.8915 -1.326   0.1408  0.1416]\n",
      "MSE loss: 86.4395\n",
      "Iteration: 132400\n",
      "Gradient: [  -5.4939   -1.5818   17.3353  -37.0607 -292.2664]\n",
      "Weights: [-4.8513  0.8892 -1.3263  0.1411  0.1415]\n",
      "MSE loss: 86.4024\n",
      "Iteration: 132500\n",
      "Gradient: [ -6.0488  -1.0866 -25.3819 121.0073 -61.7632]\n",
      "Weights: [-4.8543  0.8884 -1.3258  0.1409  0.1417]\n",
      "MSE loss: 86.4181\n",
      "Iteration: 132600\n",
      "Gradient: [ -5.7443  10.1001 -34.9306 114.9692   0.9066]\n",
      "Weights: [-4.8436  0.8674 -1.3236  0.1419  0.142 ]\n",
      "MSE loss: 86.529\n",
      "Iteration: 132700\n",
      "Gradient: [ -3.1626   3.6609 -35.6533 -27.2129 -51.7932]\n",
      "Weights: [-4.8307  0.8664 -1.3224  0.1409  0.1417]\n",
      "MSE loss: 86.4708\n",
      "Iteration: 132800\n",
      "Gradient: [  1.6833  -4.5988   8.8194 -31.5311 326.0076]\n",
      "Weights: [-4.8408  0.8752 -1.3208  0.141   0.1416]\n",
      "MSE loss: 86.492\n",
      "Iteration: 132900\n",
      "Gradient: [   4.9186   -1.4004   11.7024 -164.1374   53.3376]\n",
      "Weights: [-4.842   0.8706 -1.3201  0.1406  0.1416]\n",
      "MSE loss: 86.4072\n",
      "Iteration: 133000\n",
      "Gradient: [  5.1149   2.3282 -25.1758 -86.3939 274.2286]\n",
      "Weights: [-4.8532  0.8755 -1.32    0.1408  0.1414]\n",
      "MSE loss: 86.5003\n",
      "Iteration: 133100\n",
      "Gradient: [ -6.5656 -15.8896  -7.3575  73.2011  65.4457]\n",
      "Weights: [-4.8539  0.8811 -1.3223  0.1417  0.1415]\n",
      "MSE loss: 86.4716\n",
      "Iteration: 133200\n",
      "Gradient: [ -11.9632  -15.7237    9.2885  -51.2717 -196.9015]\n",
      "Weights: [-4.8537  0.8829 -1.3222  0.1416  0.1413]\n",
      "MSE loss: 86.4238\n",
      "Iteration: 133300\n",
      "Gradient: [ 3.8468 11.3372 57.7787 29.5791 64.6695]\n",
      "Weights: [-4.8363  0.8791 -1.3236  0.1417  0.1413]\n",
      "MSE loss: 86.4976\n",
      "Iteration: 133400\n",
      "Gradient: [ -6.0204 -10.4636 -19.9109 -64.4325 196.5446]\n",
      "Weights: [-4.8409  0.8699 -1.3248  0.1425  0.1414]\n",
      "MSE loss: 86.4842\n",
      "Iteration: 133500\n",
      "Gradient: [  -1.1496   -9.3253  -23.5298  -31.9153 -163.5027]\n",
      "Weights: [-4.8445  0.8721 -1.3257  0.1431  0.1413]\n",
      "MSE loss: 86.5358\n",
      "Iteration: 133600\n",
      "Gradient: [ -2.2711   0.8046   0.4896  43.1498 110.7756]\n",
      "Weights: [-4.8538  0.8878 -1.3242  0.1426  0.1411]\n",
      "MSE loss: 86.5228\n",
      "Iteration: 133700\n",
      "Gradient: [  0.6811   2.357  -20.6591  -6.3047 322.0263]\n",
      "Weights: [-4.8495  0.8789 -1.3216  0.1427  0.141 ]\n",
      "MSE loss: 86.4094\n",
      "Iteration: 133800\n",
      "Gradient: [  1.4536  12.6526  33.5931  62.6114 -39.0509]\n",
      "Weights: [-4.8366  0.8748 -1.3237  0.1425  0.1411]\n",
      "MSE loss: 86.4051\n",
      "Iteration: 133900\n",
      "Gradient: [-10.5958  18.7846   2.5634  28.7346   8.2063]\n",
      "Weights: [-4.854   0.8885 -1.3258  0.1423  0.1411]\n",
      "MSE loss: 86.4009\n",
      "Iteration: 134000\n",
      "Gradient: [  1.4859   6.6977  24.1072   7.9281 -71.9629]\n",
      "Weights: [-4.8456  0.8762 -1.3234  0.1434  0.141 ]\n",
      "MSE loss: 86.3748\n",
      "Iteration: 134100\n",
      "Gradient: [  -2.5029   -9.8427   -5.8489 -103.3493 -101.9552]\n",
      "Weights: [-4.8648  0.8854 -1.3274  0.1445  0.1407]\n",
      "MSE loss: 86.7269\n",
      "Iteration: 134200\n",
      "Gradient: [ -5.9748 -10.6244  -6.5105  32.8077  65.546 ]\n",
      "Weights: [-4.8667  0.8873 -1.3276  0.1454  0.1409]\n",
      "MSE loss: 86.725\n",
      "Iteration: 134300\n",
      "Gradient: [  0.2611   4.2612 -18.1359 -82.9728 -68.6791]\n",
      "Weights: [-4.8506  0.8826 -1.3294  0.1451  0.1407]\n",
      "MSE loss: 86.3804\n",
      "Iteration: 134400\n",
      "Gradient: [ -13.7183    9.0395  -43.1547  -41.5797 -144.5865]\n",
      "Weights: [-4.8433  0.8775 -1.3284  0.1451  0.1404]\n",
      "MSE loss: 86.5884\n",
      "Iteration: 134500\n",
      "Gradient: [ -1.1184   2.799   28.8615 -29.7265 147.9274]\n",
      "Weights: [-4.8249  0.8669 -1.3272  0.1458  0.1406]\n",
      "MSE loss: 86.4615\n",
      "Iteration: 134600\n",
      "Gradient: [-9.7956 -1.2026  3.3292 26.7194 39.981 ]\n",
      "Weights: [-4.8417  0.8662 -1.3232  0.1451  0.1407]\n",
      "MSE loss: 86.3632\n",
      "Iteration: 134700\n",
      "Gradient: [15.9022 15.2417 27.4362 -0.3573 50.3513]\n",
      "Weights: [-4.8422  0.8781 -1.3248  0.1448  0.1409]\n",
      "MSE loss: 86.7964\n",
      "Iteration: 134800\n",
      "Gradient: [ 5.055  -6.7361 38.4433 -3.7284 -7.0567]\n",
      "Weights: [-4.8476  0.8779 -1.3258  0.1442  0.1409]\n",
      "MSE loss: 86.3584\n",
      "Iteration: 134900\n",
      "Gradient: [-19.4046 -20.4697  10.1095  13.085   82.9982]\n",
      "Weights: [-4.8559  0.8858 -1.3271  0.1444  0.1407]\n",
      "MSE loss: 86.3957\n",
      "Iteration: 135000\n",
      "Gradient: [  2.427  -19.2021 -18.8868  21.6431  11.2672]\n",
      "Weights: [-4.8499  0.8831 -1.3276  0.1449  0.1407]\n",
      "MSE loss: 86.3522\n",
      "Iteration: 135100\n",
      "Gradient: [ -6.9175  16.0425  33.9799  42.5311 209.2739]\n",
      "Weights: [-4.8618  0.8871 -1.3252  0.1446  0.1406]\n",
      "MSE loss: 86.5272\n",
      "Iteration: 135200\n",
      "Gradient: [ 11.2854   7.0221   1.3927 -82.5105  10.3021]\n",
      "Weights: [-4.8448  0.8798 -1.3246  0.1446  0.1405]\n",
      "MSE loss: 86.4107\n",
      "Iteration: 135300\n",
      "Gradient: [  1.7711   5.2876  27.4665  77.4079 -59.6737]\n",
      "Weights: [-4.8324  0.8631 -1.3233  0.1449  0.1407]\n",
      "MSE loss: 86.3568\n",
      "Iteration: 135400\n",
      "Gradient: [  4.5968  -4.0216  26.1602 -85.4465 114.0576]\n",
      "Weights: [-4.8518  0.8683 -1.3225  0.1447  0.1408]\n",
      "MSE loss: 86.5248\n",
      "Iteration: 135500\n",
      "Gradient: [ -2.3153  -2.6142  21.2371  54.4914 202.4765]\n",
      "Weights: [-4.8586  0.8771 -1.3242  0.1449  0.1407]\n",
      "MSE loss: 86.5332\n",
      "Iteration: 135600\n",
      "Gradient: [  2.4857  -1.0749   2.0168 -90.5297 230.7094]\n",
      "Weights: [-4.8288  0.8705 -1.326   0.1455  0.1405]\n",
      "MSE loss: 86.4714\n",
      "Iteration: 135700\n",
      "Gradient: [  8.9449  -3.8682  21.5726  -9.6356 -41.4571]\n",
      "Weights: [-4.8261  0.8758 -1.3304  0.1456  0.1406]\n",
      "MSE loss: 86.5547\n",
      "Iteration: 135800\n",
      "Gradient: [   6.7752    5.137    -3.132    39.1617 -106.2336]\n",
      "Weights: [-4.8339  0.8834 -1.3329  0.1462  0.1406]\n",
      "MSE loss: 86.4676\n",
      "Iteration: 135900\n",
      "Gradient: [   0.4494  -15.5687   -8.9605  -76.6154 -173.8555]\n",
      "Weights: [-4.8366  0.8869 -1.3362  0.1461  0.1408]\n",
      "MSE loss: 86.3913\n",
      "Iteration: 136000\n",
      "Gradient: [  7.076   -4.7537 -13.3537  22.7861  83.6974]\n",
      "Weights: [-4.8439  0.8812 -1.3356  0.147   0.141 ]\n",
      "MSE loss: 86.3338\n",
      "Iteration: 136100\n",
      "Gradient: [  8.75    10.2451 -22.4169   8.249  294.701 ]\n",
      "Weights: [-4.8436  0.886  -1.3351  0.1469  0.1408]\n",
      "MSE loss: 86.3186\n",
      "Iteration: 136200\n",
      "Gradient: [-4.913  -5.3014  6.8207 29.1837 -2.3444]\n",
      "Weights: [-4.8424  0.881  -1.3355  0.1475  0.1407]\n",
      "MSE loss: 86.3012\n",
      "Iteration: 136300\n",
      "Gradient: [  -1.303     3.2725  -18.2667   73.2633 -207.9148]\n",
      "Weights: [-4.8437  0.8833 -1.3367  0.1478  0.1407]\n",
      "MSE loss: 86.2991\n",
      "Iteration: 136400\n",
      "Gradient: [   1.5675   -5.1748    5.8415   15.9003 -116.6004]\n",
      "Weights: [-4.8424  0.8857 -1.3344  0.1474  0.1405]\n",
      "MSE loss: 86.3376\n",
      "Iteration: 136500\n",
      "Gradient: [  -3.3927   10.4255  -22.165    32.5367 -107.6843]\n",
      "Weights: [-4.8421  0.8862 -1.3322  0.1462  0.1405]\n",
      "MSE loss: 86.3591\n",
      "Iteration: 136600\n",
      "Gradient: [ 1.2991 -2.3495 45.3327 44.868  99.7672]\n",
      "Weights: [-4.8402  0.8843 -1.3332  0.1462  0.1407]\n",
      "MSE loss: 86.3385\n",
      "Iteration: 136700\n",
      "Gradient: [ -4.6385  -7.8543  -8.9915 -10.3411  16.3128]\n",
      "Weights: [-4.8548  0.8941 -1.3333  0.1458  0.1406]\n",
      "MSE loss: 86.3477\n",
      "Iteration: 136800\n",
      "Gradient: [  -2.6843   -3.927   -19.0512  -86.8846 -134.8884]\n",
      "Weights: [-4.84    0.8822 -1.333   0.1459  0.1408]\n",
      "MSE loss: 86.3337\n",
      "Iteration: 136900\n",
      "Gradient: [  9.1442  -7.6496   7.1937  57.0871 -42.9581]\n",
      "Weights: [-4.8347  0.8832 -1.3352  0.146   0.1409]\n",
      "MSE loss: 86.3845\n",
      "Iteration: 137000\n",
      "Gradient: [ -10.2604   -2.4927  -20.1702  -62.5097 -112.3462]\n",
      "Weights: [-4.8415  0.8878 -1.3373  0.1459  0.1412]\n",
      "MSE loss: 86.3232\n",
      "Iteration: 137100\n",
      "Gradient: [-10.6776 -19.2515 -24.8541   2.5805  62.7174]\n",
      "Weights: [-4.8443  0.8852 -1.3403  0.1465  0.1412]\n",
      "MSE loss: 86.5859\n",
      "Iteration: 137200\n",
      "Gradient: [ -4.0948  -0.7264 -12.6469  31.0998 101.6243]\n",
      "Weights: [-4.8334  0.8911 -1.3401  0.1459  0.1413]\n",
      "MSE loss: 86.4611\n",
      "Iteration: 137300\n",
      "Gradient: [  -4.9153  -27.2024    7.1431 -150.8595 -185.2152]\n",
      "Weights: [-4.8423  0.8852 -1.3399  0.1457  0.1414]\n",
      "MSE loss: 86.6278\n",
      "Iteration: 137400\n",
      "Gradient: [  -4.5211  -11.2799   15.6465  -56.2159 -225.5145]\n",
      "Weights: [-4.8298  0.8814 -1.3392  0.1461  0.1415]\n",
      "MSE loss: 86.4231\n",
      "Iteration: 137500\n",
      "Gradient: [  6.9841  -2.847    2.2082 -47.2294 -79.9882]\n",
      "Weights: [-4.8291  0.8844 -1.3397  0.1464  0.1414]\n",
      "MSE loss: 86.4656\n",
      "Iteration: 137600\n",
      "Gradient: [ -10.0674    1.3818  -30.0376 -141.6184   21.5835]\n",
      "Weights: [-4.8307  0.8784 -1.3396  0.1462  0.1413]\n",
      "MSE loss: 86.6605\n",
      "Iteration: 137700\n",
      "Gradient: [-10.2522 -30.371  -26.9457 -22.1382 219.9258]\n",
      "Weights: [-4.8644  0.8975 -1.3392  0.1456  0.1413]\n",
      "MSE loss: 86.6855\n",
      "Iteration: 137800\n",
      "Gradient: [ -9.7967  10.0978 -23.3062 -12.1317 -39.0692]\n",
      "Weights: [-4.8534  0.8992 -1.34    0.1454  0.1413]\n",
      "MSE loss: 86.3549\n",
      "Iteration: 137900\n",
      "Gradient: [ 5.268   5.4849 15.2334  2.1581 62.4623]\n",
      "Weights: [-4.8481  0.9003 -1.3377  0.1447  0.141 ]\n",
      "MSE loss: 86.3974\n",
      "Iteration: 138000\n",
      "Gradient: [ -4.9691   3.6546  61.0114 -64.0207 -25.4281]\n",
      "Weights: [-4.8496  0.9013 -1.3395  0.1449  0.1412]\n",
      "MSE loss: 86.3569\n",
      "Iteration: 138100\n",
      "Gradient: [  4.7173 -10.9337  14.757  -32.372  177.4962]\n",
      "Weights: [-4.8473  0.9026 -1.3395  0.1448  0.1414]\n",
      "MSE loss: 86.4281\n",
      "Iteration: 138200\n",
      "Gradient: [   5.0143    6.1815   12.9876 -131.0478 -167.1539]\n",
      "Weights: [-4.8601  0.9152 -1.3426  0.1448  0.1414]\n",
      "MSE loss: 86.4341\n",
      "Iteration: 138300\n",
      "Gradient: [  1.5447   6.5827 -21.1733  56.9102 -61.5816]\n",
      "Weights: [-4.8595  0.9082 -1.3431  0.1454  0.1414]\n",
      "MSE loss: 86.3586\n",
      "Iteration: 138400\n",
      "Gradient: [  2.2246  -8.6144 -51.4384  -9.4784 132.5852]\n",
      "Weights: [-4.8561  0.9085 -1.3455  0.1465  0.1413]\n",
      "MSE loss: 86.3192\n",
      "Iteration: 138500\n",
      "Gradient: [  2.7037  -0.444  -17.4538 -79.7814 278.0226]\n",
      "Weights: [-4.8489  0.9001 -1.3462  0.1471  0.1417]\n",
      "MSE loss: 86.3558\n",
      "Iteration: 138600\n",
      "Gradient: [ -1.2445   3.6115  -8.5081 -50.9216 278.6211]\n",
      "Weights: [-4.8489  0.908  -1.3504  0.1476  0.1415]\n",
      "MSE loss: 86.3027\n",
      "Iteration: 138700\n",
      "Gradient: [  -2.0068    5.4538   20.1377   53.6357 -132.2076]\n",
      "Weights: [-4.8663  0.9145 -1.3497  0.1478  0.1414]\n",
      "MSE loss: 86.4238\n",
      "Iteration: 138800\n",
      "Gradient: [  3.7547 -13.2611 -26.7642  73.7973  82.9568]\n",
      "Weights: [-4.8621  0.9044 -1.3476  0.1482  0.1412]\n",
      "MSE loss: 86.4963\n",
      "Iteration: 138900\n",
      "Gradient: [ -2.1436 -17.8836  15.9554  89.6554 -74.095 ]\n",
      "Weights: [-4.845   0.9053 -1.3492  0.1482  0.1411]\n",
      "MSE loss: 86.3161\n",
      "Iteration: 139000\n",
      "Gradient: [ -17.6835  -16.6205  -20.7532  -51.3548 -200.967 ]\n",
      "Weights: [-4.8571  0.8974 -1.346   0.1482  0.1409]\n",
      "MSE loss: 86.767\n",
      "Iteration: 139100\n",
      "Gradient: [  5.9186  -7.2543  -4.1626   6.3248 -88.992 ]\n",
      "Weights: [-4.8466  0.8979 -1.3448  0.1479  0.1411]\n",
      "MSE loss: 86.2878\n",
      "Iteration: 139200\n",
      "Gradient: [  5.7852  10.1704  38.7688 -87.3796   2.7248]\n",
      "Weights: [-4.8437  0.8962 -1.342   0.1476  0.1411]\n",
      "MSE loss: 86.4362\n",
      "Iteration: 139300\n",
      "Gradient: [  2.0423  10.3775   7.7909  12.6    -93.3309]\n",
      "Weights: [-4.8409  0.8909 -1.3431  0.1479  0.1411]\n",
      "MSE loss: 86.3025\n",
      "Iteration: 139400\n",
      "Gradient: [  9.8407  -2.4185 -29.2914 115.0945   5.0136]\n",
      "Weights: [-4.8269  0.8885 -1.3432  0.1485  0.1411]\n",
      "MSE loss: 86.777\n",
      "Iteration: 139500\n",
      "Gradient: [  -4.2553    9.9244   16.9501 -121.9715  160.0831]\n",
      "Weights: [-4.8386  0.8869 -1.344   0.1488  0.1411]\n",
      "MSE loss: 86.3186\n",
      "Iteration: 139600\n",
      "Gradient: [  -1.8128  -24.5848  -11.7199   15.3728 -164.4375]\n",
      "Weights: [-4.8475  0.8925 -1.3441  0.1488  0.1408]\n",
      "MSE loss: 86.3267\n",
      "Iteration: 139700\n",
      "Gradient: [ -5.3986 -21.4862 -54.0024 -28.5909  20.8823]\n",
      "Weights: [-4.8333  0.8807 -1.3464  0.1493  0.1411]\n",
      "MSE loss: 86.5585\n",
      "Iteration: 139800\n",
      "Gradient: [-2.675  -7.9489  3.7362 39.0457 37.6006]\n",
      "Weights: [-4.8273  0.8735 -1.3436  0.1498  0.1412]\n",
      "MSE loss: 86.4471\n",
      "Iteration: 139900\n",
      "Gradient: [-10.3878  -9.1978   0.1605 -73.2431 116.6909]\n",
      "Weights: [-4.8441  0.88   -1.3433  0.1491  0.1413]\n",
      "MSE loss: 86.5155\n",
      "Iteration: 140000\n",
      "Gradient: [ -11.31     -4.387     4.2978 -141.3613   -5.1943]\n",
      "Weights: [-4.8328  0.877  -1.3444  0.1498  0.1411]\n",
      "MSE loss: 86.4219\n",
      "Iteration: 140100\n",
      "Gradient: [  12.3617   -3.0213   25.7357  -66.1112 -451.0825]\n",
      "Weights: [-4.8385  0.8749 -1.3423  0.1497  0.141 ]\n",
      "MSE loss: 86.4961\n",
      "Iteration: 140200\n",
      "Gradient: [ -4.0433   1.1545  23.6549  39.7773 100.0804]\n",
      "Weights: [-4.8504  0.8924 -1.343   0.1488  0.1408]\n",
      "MSE loss: 86.3249\n",
      "Iteration: 140300\n",
      "Gradient: [  4.1941   2.1195  17.8891  83.9442 -12.0829]\n",
      "Weights: [-4.8395  0.8835 -1.3408  0.149   0.1409]\n",
      "MSE loss: 86.3224\n",
      "Iteration: 140400\n",
      "Gradient: [   6.8323   -6.7652   -6.991  -100.4373   43.7436]\n",
      "Weights: [-4.8388  0.8863 -1.3418  0.1492  0.1405]\n",
      "MSE loss: 86.2951\n",
      "Iteration: 140500\n",
      "Gradient: [ -3.1779  -9.168  -25.2031  14.4822  61.1878]\n",
      "Weights: [-4.84    0.8919 -1.3406  0.1494  0.1405]\n",
      "MSE loss: 86.5349\n",
      "Iteration: 140600\n",
      "Gradient: [ -4.7963   3.4838  65.1096  95.664  -18.6184]\n",
      "Weights: [-4.8498  0.8905 -1.34    0.1489  0.1406]\n",
      "MSE loss: 86.3014\n",
      "Iteration: 140700\n",
      "Gradient: [ -6.3484   0.7251   1.5735   8.9513 123.9669]\n",
      "Weights: [-4.8426  0.8792 -1.3399  0.15    0.1405]\n",
      "MSE loss: 86.3235\n",
      "Iteration: 140800\n",
      "Gradient: [ -0.2216   5.5401 -42.4675 -83.8038 -84.2834]\n",
      "Weights: [-4.8401  0.8884 -1.3404  0.1493  0.1402]\n",
      "MSE loss: 86.2892\n",
      "Iteration: 140900\n",
      "Gradient: [ -1.5646  17.4194 -29.9542 -92.6861 104.6313]\n",
      "Weights: [-4.8559  0.8909 -1.3384  0.149   0.1402]\n",
      "MSE loss: 86.3783\n",
      "Iteration: 141000\n",
      "Gradient: [  3.8577  -9.1344   8.8429  90.5232 309.6257]\n",
      "Weights: [-4.862   0.897  -1.3406  0.1497  0.1405]\n",
      "MSE loss: 86.5226\n",
      "Iteration: 141100\n",
      "Gradient: [ -1.567  -20.77   -43.088  144.2891 124.4667]\n",
      "Weights: [-4.8677  0.9013 -1.3445  0.15    0.1403]\n",
      "MSE loss: 86.6015\n",
      "Iteration: 141200\n",
      "Gradient: [ -3.7603  -8.5564  10.6625  96.572  -23.4735]\n",
      "Weights: [-4.8349  0.8854 -1.342   0.1501  0.1405]\n",
      "MSE loss: 86.3501\n",
      "Iteration: 141300\n",
      "Gradient: [  7.3363 -10.636   28.0875  73.5028 -79.9105]\n",
      "Weights: [-4.8481  0.8984 -1.3406  0.15    0.1401]\n",
      "MSE loss: 86.5982\n",
      "Iteration: 141400\n",
      "Gradient: [ 9.807  -7.0104 14.342   4.4372 25.7856]\n",
      "Weights: [-4.8518  0.8944 -1.3416  0.1497  0.1403]\n",
      "MSE loss: 86.2847\n",
      "Iteration: 141500\n",
      "Gradient: [  11.5897  -10.4401   11.6087  -76.1513 -308.5089]\n",
      "Weights: [-4.84    0.8873 -1.3439  0.15    0.1404]\n",
      "MSE loss: 86.355\n",
      "Iteration: 141600\n",
      "Gradient: [ 10.6178  -3.4401   0.4389  69.4964 237.6143]\n",
      "Weights: [-4.8589  0.9031 -1.3461  0.1504  0.1402]\n",
      "MSE loss: 86.3\n",
      "Iteration: 141700\n",
      "Gradient: [ -1.3314   0.8086   4.7549  96.5214 126.7718]\n",
      "Weights: [-4.843   0.9034 -1.3457  0.1505  0.1402]\n",
      "MSE loss: 86.5687\n",
      "Iteration: 141800\n",
      "Gradient: [  -0.983    15.5561   10.4547   65.7995 -247.2847]\n",
      "Weights: [-4.8544  0.9114 -1.3472  0.1499  0.14  ]\n",
      "MSE loss: 86.3333\n",
      "Iteration: 141900\n",
      "Gradient: [ -6.7362   1.0529 -35.5669  11.1396 -72.4591]\n",
      "Weights: [-4.8652  0.9129 -1.3493  0.1506  0.1401]\n",
      "MSE loss: 86.3724\n",
      "Iteration: 142000\n",
      "Gradient: [ -5.2852  -9.8174   2.9378 -11.9713   9.3442]\n",
      "Weights: [-4.8579  0.9117 -1.3501  0.1504  0.1401]\n",
      "MSE loss: 86.4004\n",
      "Iteration: 142100\n",
      "Gradient: [  9.3868  13.9848  32.6981 109.1102 -74.8796]\n",
      "Weights: [-4.8503  0.9139 -1.3501  0.1495  0.1405]\n",
      "MSE loss: 86.3589\n",
      "Iteration: 142200\n",
      "Gradient: [  11.7451   14.2309  -51.2358   -9.897  -215.8596]\n",
      "Weights: [-4.8432  0.8982 -1.3478  0.1495  0.1407]\n",
      "MSE loss: 86.2987\n",
      "Iteration: 142300\n",
      "Gradient: [  -9.4799    2.5554  -62.2299  -54.0709 -130.19  ]\n",
      "Weights: [-4.8457  0.899  -1.3481  0.1498  0.1406]\n",
      "MSE loss: 86.2774\n",
      "Iteration: 142400\n",
      "Gradient: [  7.8879  -1.3121  31.6764  40.7571 229.8858]\n",
      "Weights: [-4.8309  0.9008 -1.3488  0.1506  0.1407]\n",
      "MSE loss: 87.1032\n",
      "Iteration: 142500\n",
      "Gradient: [ 12.5942  -3.5655  54.8568  75.7144 134.9745]\n",
      "Weights: [-4.8353  0.8955 -1.3497  0.1498  0.1409]\n",
      "MSE loss: 86.3391\n",
      "Iteration: 142600\n",
      "Gradient: [ -7.0767  -6.1162 -16.7729  35.4428  29.6185]\n",
      "Weights: [-4.8538  0.9042 -1.349   0.1491  0.1411]\n",
      "MSE loss: 86.2929\n",
      "Iteration: 142700\n",
      "Gradient: [   1.727    -2.3825  -63.8181   -3.2458 -119.4682]\n",
      "Weights: [-4.8563  0.9054 -1.3491  0.1489  0.1411]\n",
      "MSE loss: 86.3221\n",
      "Iteration: 142800\n",
      "Gradient: [ -6.8457   0.5537   2.4925 -59.598  -69.969 ]\n",
      "Weights: [-4.855   0.9105 -1.3493  0.1489  0.1409]\n",
      "MSE loss: 86.2867\n",
      "Iteration: 142900\n",
      "Gradient: [ -1.3201   2.4496 -21.1905  64.7252 151.5227]\n",
      "Weights: [-4.8613  0.9171 -1.3494  0.1484  0.141 ]\n",
      "MSE loss: 86.3643\n",
      "Iteration: 143000\n",
      "Gradient: [  0.3782  10.69    46.7136 -10.1551 198.647 ]\n",
      "Weights: [-4.8507  0.9117 -1.3512  0.1487  0.141 ]\n",
      "MSE loss: 86.2949\n",
      "Iteration: 143100\n",
      "Gradient: [  -2.0236    3.9089  -37.3862  -72.7779 -273.2918]\n",
      "Weights: [-4.8496  0.904  -1.3531  0.149   0.1412]\n",
      "MSE loss: 86.5583\n",
      "Iteration: 143200\n",
      "Gradient: [  9.0104  -4.5524  35.7159  50.245  187.4684]\n",
      "Weights: [-4.829   0.907  -1.3541  0.1488  0.1412]\n",
      "MSE loss: 86.6957\n",
      "Iteration: 143300\n",
      "Gradient: [ -6.5547 -14.5593  -4.7847 -60.0664 226.9672]\n",
      "Weights: [-4.8562  0.9139 -1.3581  0.1499  0.1413]\n",
      "MSE loss: 86.4346\n",
      "Iteration: 143400\n",
      "Gradient: [  -5.5844  -19.5547   -0.3941  -70.1535 -111.9951]\n",
      "Weights: [-4.8692  0.9313 -1.36    0.1492  0.1413]\n",
      "MSE loss: 86.3552\n",
      "Iteration: 143500\n",
      "Gradient: [  3.1262  -3.7965  -1.2337 185.0167  95.9225]\n",
      "Weights: [-4.8545  0.9269 -1.3598  0.1488  0.1414]\n",
      "MSE loss: 86.3263\n",
      "Iteration: 143600\n",
      "Gradient: [ -5.0661  -1.6237 -29.4724  26.1424 270.0429]\n",
      "Weights: [-4.8595  0.9356 -1.3644  0.149   0.1415]\n",
      "MSE loss: 86.3397\n",
      "Iteration: 143700\n",
      "Gradient: [ -1.9494  -8.0494 -53.2784  66.0785  84.9707]\n",
      "Weights: [-4.8686  0.9383 -1.3652  0.1486  0.1416]\n",
      "MSE loss: 86.576\n",
      "Iteration: 143800\n",
      "Gradient: [ -2.3481 -21.2415 -79.338  -38.4562  57.8834]\n",
      "Weights: [-4.8572  0.9294 -1.3649  0.1491  0.1417]\n",
      "MSE loss: 86.4158\n",
      "Iteration: 143900\n",
      "Gradient: [ -6.8647  -7.8046  -1.4005 -31.982  121.1222]\n",
      "Weights: [-4.863   0.9296 -1.3623  0.1492  0.1417]\n",
      "MSE loss: 86.3164\n",
      "Iteration: 144000\n",
      "Gradient: [   1.296     0.5061   23.5415   39.8124 -451.8591]\n",
      "Weights: [-4.8472  0.924  -1.3619  0.1489  0.1417]\n",
      "MSE loss: 86.3666\n",
      "Iteration: 144100\n",
      "Gradient: [  -3.3968   -7.1455  -14.4352 -109.8608 -368.1388]\n",
      "Weights: [-4.8722  0.9365 -1.3628  0.1493  0.1415]\n",
      "MSE loss: 86.3804\n",
      "Iteration: 144200\n",
      "Gradient: [  2.7991   7.9728  16.0588  88.1012 157.5109]\n",
      "Weights: [-4.8547  0.9328 -1.3623  0.1497  0.1414]\n",
      "MSE loss: 86.4994\n",
      "Iteration: 144300\n",
      "Gradient: [ -10.5468  -16.5476   15.2721  -70.2404 -183.9092]\n",
      "Weights: [-4.8711  0.9329 -1.3602  0.1495  0.1412]\n",
      "MSE loss: 86.3772\n",
      "Iteration: 144400\n",
      "Gradient: [  3.5204 -17.0253 -24.4089  39.6456  30.7056]\n",
      "Weights: [-4.8638  0.9333 -1.3596  0.1493  0.1412]\n",
      "MSE loss: 86.3703\n",
      "Iteration: 144500\n",
      "Gradient: [  8.3611   0.9843  23.0888 -42.2321 418.9643]\n",
      "Weights: [-4.8474  0.9208 -1.3571  0.1485  0.1415]\n",
      "MSE loss: 86.403\n",
      "Iteration: 144600\n",
      "Gradient: [   5.6024    6.3153   10.0208   51.1998 -180.6932]\n",
      "Weights: [-4.8624  0.9254 -1.3548  0.1493  0.141 ]\n",
      "MSE loss: 86.3656\n",
      "Iteration: 144700\n",
      "Gradient: [  8.6712  -1.2115  28.0598  85.4246 -52.0022]\n",
      "Weights: [-4.829   0.9093 -1.3551  0.1499  0.141 ]\n",
      "MSE loss: 86.8513\n",
      "Iteration: 144800\n",
      "Gradient: [-11.579  -20.7475   5.4704 -31.027  -30.0113]\n",
      "Weights: [-4.8581  0.909  -1.3546  0.151   0.1409]\n",
      "MSE loss: 86.3268\n",
      "Iteration: 144900\n",
      "Gradient: [  10.5163    7.805     1.8806   21.6288 -117.6796]\n",
      "Weights: [-4.845   0.9044 -1.3562  0.1514  0.141 ]\n",
      "MSE loss: 86.2659\n",
      "Iteration: 145000\n",
      "Gradient: [   4.5045   -5.779   -27.3318   39.5412 -122.9692]\n",
      "Weights: [-4.8476  0.9096 -1.3576  0.1511  0.1409]\n",
      "MSE loss: 86.3209\n",
      "Iteration: 145100\n",
      "Gradient: [ 13.8981   4.4818 -11.4597  12.6006 -48.2048]\n",
      "Weights: [-4.845   0.9099 -1.3584  0.152   0.141 ]\n",
      "MSE loss: 86.3586\n",
      "Iteration: 145200\n",
      "Gradient: [  0.926    1.0839  12.1474 172.793  121.8809]\n",
      "Weights: [-4.8493  0.9143 -1.3603  0.1521  0.141 ]\n",
      "MSE loss: 86.2846\n",
      "Iteration: 145300\n",
      "Gradient: [  4.2441  -3.6994 -12.1177 -90.0297 -25.6025]\n",
      "Weights: [-4.8608  0.9236 -1.3629  0.1522  0.1409]\n",
      "MSE loss: 86.2647\n",
      "Iteration: 145400\n",
      "Gradient: [ -7.7722   6.2542   8.8776  65.7748 449.4636]\n",
      "Weights: [-4.8614  0.9316 -1.3626  0.1512  0.1409]\n",
      "MSE loss: 86.2847\n",
      "Iteration: 145500\n",
      "Gradient: [  -1.3765  -17.6048   15.5597  -22.8589 -380.1238]\n",
      "Weights: [-4.8738  0.9297 -1.3617  0.1513  0.1408]\n",
      "MSE loss: 86.506\n",
      "Iteration: 145600\n",
      "Gradient: [ -6.9897   9.9812  -0.6344 -49.4537 -45.6785]\n",
      "Weights: [-4.8524  0.9264 -1.3611  0.1514  0.1409]\n",
      "MSE loss: 86.4043\n",
      "Iteration: 145700\n",
      "Gradient: [  4.9644   1.8634  41.9507  77.5103 186.7029]\n",
      "Weights: [-4.8502  0.9269 -1.3614  0.1515  0.1407]\n",
      "MSE loss: 86.4062\n",
      "Iteration: 145800\n",
      "Gradient: [ -0.3928  10.1128 -50.7183 -78.3883 -32.6828]\n",
      "Weights: [-4.858   0.9289 -1.3612  0.1519  0.1405]\n",
      "MSE loss: 86.3031\n",
      "Iteration: 145900\n",
      "Gradient: [2.930000e-02 4.703500e+00 1.943050e+01 5.389720e+01 1.702646e+02]\n",
      "Weights: [-4.8419  0.9214 -1.3612  0.1523  0.1405]\n",
      "MSE loss: 86.5326\n",
      "Iteration: 146000\n",
      "Gradient: [-15.6337  -4.6212  10.9669 -11.2844  36.955 ]\n",
      "Weights: [-4.8607  0.917  -1.3597  0.1522  0.1408]\n",
      "MSE loss: 86.3106\n",
      "Iteration: 146100\n",
      "Gradient: [ -6.6844  16.6017   7.8562  40.5988 207.9298]\n",
      "Weights: [-4.8576  0.9239 -1.3624  0.1525  0.1408]\n",
      "MSE loss: 86.2605\n",
      "Iteration: 146200\n",
      "Gradient: [ -7.2415  -2.042   16.0681  75.1138 -11.8505]\n",
      "Weights: [-4.8412  0.9125 -1.362   0.1528  0.1406]\n",
      "MSE loss: 86.3197\n",
      "Iteration: 146300\n",
      "Gradient: [  0.8853   7.6503  -5.6969 -78.361  404.0373]\n",
      "Weights: [-4.8418  0.9079 -1.3617  0.1536  0.1407]\n",
      "MSE loss: 86.2567\n",
      "Iteration: 146400\n",
      "Gradient: [ 2.5101 19.493  -7.9806  0.9585 -6.1817]\n",
      "Weights: [-4.8435  0.9008 -1.3618  0.1552  0.1406]\n",
      "MSE loss: 86.2995\n",
      "Iteration: 146500\n",
      "Gradient: [   6.0897    6.0178   45.1933   63.5255 -139.0431]\n",
      "Weights: [-4.853   0.91   -1.3604  0.1551  0.1404]\n",
      "MSE loss: 86.4094\n",
      "Iteration: 146600\n",
      "Gradient: [  3.021   14.0851  61.8699  70.4487 113.5824]\n",
      "Weights: [-4.8346  0.9031 -1.3601  0.1554  0.1401]\n",
      "MSE loss: 86.361\n",
      "Iteration: 146700\n",
      "Gradient: [   4.56      8.4321   23.3109   13.2618 -104.2451]\n",
      "Weights: [-4.8419  0.8918 -1.359   0.1553  0.1405]\n",
      "MSE loss: 86.3878\n",
      "Iteration: 146800\n",
      "Gradient: [  0.0734   3.3855  24.699  -11.1863  22.2197]\n",
      "Weights: [-4.8211  0.8884 -1.3601  0.1552  0.1405]\n",
      "MSE loss: 86.4761\n",
      "Iteration: 146900\n",
      "Gradient: [  4.7836  -2.6294  -6.6386  81.2091 -46.0832]\n",
      "Weights: [-4.8327  0.8909 -1.3589  0.1564  0.1402]\n",
      "MSE loss: 86.3364\n",
      "Iteration: 147000\n",
      "Gradient: [ -14.4051   -0.7844   -2.4585    4.176  -269.1445]\n",
      "Weights: [-4.8392  0.8917 -1.3599  0.1566  0.14  ]\n",
      "MSE loss: 86.3417\n",
      "Iteration: 147100\n",
      "Gradient: [  4.4256   7.2266 -20.1432 -59.1424   6.3072]\n",
      "Weights: [-4.8462  0.904  -1.3625  0.1565  0.1399]\n",
      "MSE loss: 86.2325\n",
      "Iteration: 147200\n",
      "Gradient: [  7.5793  27.253   31.3408 117.492   42.2607]\n",
      "Weights: [-4.8443  0.9089 -1.3619  0.1566  0.1399]\n",
      "MSE loss: 86.3242\n",
      "Iteration: 147300\n",
      "Gradient: [   4.4236   -8.8537   26.6294   -8.8823 -144.2225]\n",
      "Weights: [-4.8443  0.9036 -1.3608  0.1571  0.1395]\n",
      "MSE loss: 86.1583\n",
      "Iteration: 147400\n",
      "Gradient: [ -9.7768   0.3772   7.3138 -32.9258  -2.0493]\n",
      "Weights: [-4.8487  0.8998 -1.3629  0.1578  0.1398]\n",
      "MSE loss: 86.2938\n",
      "Iteration: 147500\n",
      "Gradient: [ -2.0996   1.1345  13.5551 114.5974 111.7139]\n",
      "Weights: [-4.841   0.9042 -1.3647  0.1581  0.1394]\n",
      "MSE loss: 86.2243\n",
      "Iteration: 147600\n",
      "Gradient: [   5.7284   -3.7086   16.1521   87.0629 -114.0557]\n",
      "Weights: [-4.8509  0.9089 -1.3641  0.1587  0.1393]\n",
      "MSE loss: 86.1378\n",
      "Iteration: 147700\n",
      "Gradient: [-2.1843 21.639  27.871  42.2508 98.2722]\n",
      "Weights: [-4.8531  0.921  -1.368   0.1592  0.1391]\n",
      "MSE loss: 86.1485\n",
      "Iteration: 147800\n",
      "Gradient: [   0.3416   12.0968  -38.2815  -53.0158 -122.6549]\n",
      "Weights: [-4.8615  0.9111 -1.366   0.1593  0.1391]\n",
      "MSE loss: 86.4424\n",
      "Iteration: 147900\n",
      "Gradient: [ -8.9475  11.6342  -3.8084 -56.9075  38.7676]\n",
      "Weights: [-4.8469  0.9065 -1.3655  0.1594  0.1393]\n",
      "MSE loss: 86.1316\n",
      "Iteration: 148000\n",
      "Gradient: [  8.8558   1.255   40.4209 123.133  -23.8361]\n",
      "Weights: [-4.8414  0.9021 -1.3631  0.1593  0.1391]\n",
      "MSE loss: 86.1455\n",
      "Iteration: 148100\n",
      "Gradient: [ -2.7966   6.5904  56.9384 -50.1896  27.9406]\n",
      "Weights: [-4.848   0.902  -1.3614  0.1589  0.1392]\n",
      "MSE loss: 86.1683\n",
      "Iteration: 148200\n",
      "Gradient: [ -5.5257 -14.876   15.6682 -54.8211 -88.227 ]\n",
      "Weights: [-4.8588  0.9173 -1.3646  0.159   0.1388]\n",
      "MSE loss: 86.1653\n",
      "Iteration: 148300\n",
      "Gradient: [ -0.2692   0.3955  13.9261 -64.4715  50.6351]\n",
      "Weights: [-4.8422  0.9088 -1.3652  0.16    0.139 ]\n",
      "MSE loss: 86.259\n",
      "Iteration: 148400\n",
      "Gradient: [  10.1994   -2.3153    9.4446  -25.7197 -178.1723]\n",
      "Weights: [-4.8501  0.9176 -1.368   0.1595  0.139 ]\n",
      "MSE loss: 86.1197\n",
      "Iteration: 148500\n",
      "Gradient: [ 7.9532  8.2375 11.8324 49.412  66.2886]\n",
      "Weights: [-4.8568  0.9288 -1.3715  0.1594  0.1392]\n",
      "MSE loss: 86.1684\n",
      "Iteration: 148600\n",
      "Gradient: [  7.4157  18.7175  51.063  -29.8359  26.9224]\n",
      "Weights: [-4.8436  0.9283 -1.374   0.1593  0.1395]\n",
      "MSE loss: 86.4669\n",
      "Iteration: 148700\n",
      "Gradient: [   6.6117    3.76     -2.3124  -34.9157 -128.5136]\n",
      "Weights: [-4.8455  0.9157 -1.3742  0.1599  0.1395]\n",
      "MSE loss: 86.2371\n",
      "Iteration: 148800\n",
      "Gradient: [ 1.243000e-01 -1.065100e+01  2.206770e+01  2.350200e+01  2.462432e+02]\n",
      "Weights: [-4.8394  0.9185 -1.3758  0.1599  0.1396]\n",
      "MSE loss: 86.2162\n",
      "Iteration: 148900\n",
      "Gradient: [ -3.4613  15.9146   6.9691 -14.8628 169.9177]\n",
      "Weights: [-4.8386  0.9147 -1.375   0.1605  0.1397]\n",
      "MSE loss: 86.2148\n",
      "Iteration: 149000\n",
      "Gradient: [  -1.6069   -9.478    -9.8647  -94.7037 -130.5552]\n",
      "Weights: [-4.8351  0.9084 -1.373   0.1608  0.1392]\n",
      "MSE loss: 86.2542\n",
      "Iteration: 149100\n",
      "Gradient: [  -8.9782   10.67      4.4654   75.9129 -346.7659]\n",
      "Weights: [-4.8419  0.9137 -1.3741  0.162   0.1391]\n",
      "MSE loss: 86.1267\n",
      "Iteration: 149200\n",
      "Gradient: [  2.6357  16.7104  53.2558 129.9262 166.3604]\n",
      "Weights: [-4.8472  0.9208 -1.3754  0.1624  0.1392]\n",
      "MSE loss: 86.3756\n",
      "Iteration: 149300\n",
      "Gradient: [  8.8758  17.6335 -27.935  -12.8181  -7.4494]\n",
      "Weights: [-4.8329  0.9171 -1.3775  0.163   0.1389]\n",
      "MSE loss: 86.365\n",
      "Iteration: 149400\n",
      "Gradient: [ 6.8243 12.4317  1.4618 -4.7891 23.4042]\n",
      "Weights: [-4.8437  0.9212 -1.3759  0.1627  0.1387]\n",
      "MSE loss: 86.1537\n",
      "Iteration: 149500\n",
      "Gradient: [  8.0543   7.0208 -17.0448 -65.6634 152.1731]\n",
      "Weights: [-4.8392  0.9117 -1.375   0.1633  0.1387]\n",
      "MSE loss: 86.1178\n",
      "Iteration: 149600\n",
      "Gradient: [ -1.4902  -8.6671 -22.0143 -18.1499 -37.8165]\n",
      "Weights: [-4.8557  0.9097 -1.3698  0.1627  0.1385]\n",
      "MSE loss: 86.1826\n",
      "Iteration: 149700\n",
      "Gradient: [ -2.164   -3.9575 -22.324   87.5578 -71.7882]\n",
      "Weights: [-4.8533  0.9092 -1.3678  0.163   0.138 ]\n",
      "MSE loss: 86.1199\n",
      "Iteration: 149800\n",
      "Gradient: [   2.6832   15.2013    0.3369  -76.8675 -109.5191]\n",
      "Weights: [-4.8466  0.9018 -1.3664  0.163   0.1382]\n",
      "MSE loss: 86.0897\n",
      "Iteration: 149900\n",
      "Gradient: [  -4.9172    3.8746   -5.6756 -155.3892 -124.67  ]\n",
      "Weights: [-4.8355  0.8936 -1.368   0.1637  0.1382]\n",
      "MSE loss: 86.2105\n",
      "Iteration: 150000\n",
      "Gradient: [ -3.5696  -3.7677 -28.2088  14.5098 176.4971]\n",
      "Weights: [-4.8505  0.9047 -1.3674  0.1635  0.1382]\n",
      "MSE loss: 86.0857\n",
      "Iteration: 150100\n",
      "Gradient: [  -2.0053    8.5148    9.5266   66.3023 -274.5057]\n",
      "Weights: [-4.848   0.9064 -1.3689  0.1638  0.1382]\n",
      "MSE loss: 86.0818\n",
      "Iteration: 150200\n",
      "Gradient: [ -15.0768   -7.5192  -23.3608    8.6757 -451.0306]\n",
      "Weights: [-4.8548  0.9122 -1.3689  0.1633  0.1381]\n",
      "MSE loss: 86.0756\n",
      "Iteration: 150300\n",
      "Gradient: [ -6.8578   7.6883  -0.0345 -20.9689 -10.9277]\n",
      "Weights: [-4.8433  0.9063 -1.3677  0.1633  0.138 ]\n",
      "MSE loss: 86.0688\n",
      "Iteration: 150400\n",
      "Gradient: [-11.4957   3.3384   7.5891 -99.6759 203.6351]\n",
      "Weights: [-4.8521  0.9043 -1.3693  0.164   0.1381]\n",
      "MSE loss: 86.2116\n",
      "Iteration: 150500\n",
      "Gradient: [ -9.4846  -0.5381  23.3072 -45.2416 219.3022]\n",
      "Weights: [-4.8411  0.8889 -1.3669  0.1651  0.138 ]\n",
      "MSE loss: 86.2313\n",
      "Iteration: 150600\n",
      "Gradient: [  9.6608  -7.99    -1.3055  31.1597 172.007 ]\n",
      "Weights: [-4.8471  0.9097 -1.3708  0.1641  0.138 ]\n",
      "MSE loss: 86.0536\n",
      "Iteration: 150700\n",
      "Gradient: [  1.2194   3.451   -8.5095  36.3307 -53.6116]\n",
      "Weights: [-4.8473  0.917  -1.3734  0.1651  0.1378]\n",
      "MSE loss: 86.0707\n",
      "Iteration: 150800\n",
      "Gradient: [ -0.5107  -9.3946 -23.57     2.4673  74.97  ]\n",
      "Weights: [-4.844   0.91   -1.3753  0.1658  0.1379]\n",
      "MSE loss: 86.0427\n",
      "Iteration: 150900\n",
      "Gradient: [ -9.7738  -4.5973 -41.5183  54.9681 405.2245]\n",
      "Weights: [-4.8344  0.8986 -1.3726  0.167   0.1376]\n",
      "MSE loss: 86.0642\n",
      "Iteration: 151000\n",
      "Gradient: [ -12.9161  -17.7306  -17.3917  -60.6012 -122.7205]\n",
      "Weights: [-4.8567  0.8997 -1.3693  0.1671  0.1375]\n",
      "MSE loss: 86.2578\n",
      "Iteration: 151100\n",
      "Gradient: [  1.7966  10.1504  -5.095  -32.0447 123.2112]\n",
      "Weights: [-4.836   0.8986 -1.371   0.1669  0.1374]\n",
      "MSE loss: 86.0444\n",
      "Iteration: 151200\n",
      "Gradient: [ -5.039    5.559   12.5779 -54.4078 -88.6843]\n",
      "Weights: [-4.838   0.9023 -1.374   0.1673  0.1374]\n",
      "MSE loss: 86.0471\n",
      "Iteration: 151300\n",
      "Gradient: [  -8.1614   -4.0619    9.557     8.4547 -444.5655]\n",
      "Weights: [-4.8414  0.8991 -1.3744  0.1678  0.1375]\n",
      "MSE loss: 86.1063\n",
      "Iteration: 151400\n",
      "Gradient: [ -8.2624  38.08   -26.8113 -42.3317 -38.1306]\n",
      "Weights: [-4.8352  0.8972 -1.3738  0.1679  0.1377]\n",
      "MSE loss: 86.1101\n",
      "Iteration: 151500\n",
      "Gradient: [  0.2907  18.2289  -4.9967 -58.4493 124.9699]\n",
      "Weights: [-4.8355  0.9009 -1.3776  0.1681  0.1378]\n",
      "MSE loss: 86.0733\n",
      "Iteration: 151600\n",
      "Gradient: [-15.0716  -6.2547  16.9159 -48.7515  52.2872]\n",
      "Weights: [-4.8316  0.9037 -1.3774  0.1673  0.1376]\n",
      "MSE loss: 86.1527\n",
      "Iteration: 151700\n",
      "Gradient: [ 10.5996   0.4747 -53.3272 -29.5831 121.7034]\n",
      "Weights: [-4.8492  0.9187 -1.3774  0.1681  0.1374]\n",
      "MSE loss: 86.17\n",
      "Iteration: 151800\n",
      "Gradient: [  -8.8842    3.0607    7.9438   -4.6961 -125.9635]\n",
      "Weights: [-4.8594  0.9215 -1.3828  0.1686  0.1374]\n",
      "MSE loss: 86.2031\n",
      "Iteration: 151900\n",
      "Gradient: [ 5.425100e+00 -2.900000e-03  7.149000e-01 -4.538300e+00 -1.995053e+02]\n",
      "Weights: [-4.8506  0.9327 -1.3873  0.1691  0.1373]\n",
      "MSE loss: 86.0134\n",
      "Iteration: 152000\n",
      "Gradient: [  -9.6219    2.3268    8.4555   10.5927 -253.5932]\n",
      "Weights: [-4.8576  0.9292 -1.3853  0.1685  0.1375]\n",
      "MSE loss: 86.0151\n",
      "Iteration: 152100\n",
      "Gradient: [  -6.3185   11.4546   31.6812  -38.2481 -120.2641]\n",
      "Weights: [-4.8487  0.9291 -1.3864  0.1686  0.1376]\n",
      "MSE loss: 86.0047\n",
      "Iteration: 152200\n",
      "Gradient: [   4.5204   12.5914   -9.0819   35.2999 -403.417 ]\n",
      "Weights: [-4.8726  0.9302 -1.384   0.1693  0.1374]\n",
      "MSE loss: 86.2573\n",
      "Iteration: 152300\n",
      "Gradient: [  -7.3273    0.9801    0.1768   20.9709 -122.1864]\n",
      "Weights: [-4.8531  0.9197 -1.3856  0.1691  0.1374]\n",
      "MSE loss: 86.3908\n",
      "Iteration: 152400\n",
      "Gradient: [ 3.7     5.3374 -8.5605 15.8911 28.9753]\n",
      "Weights: [-4.8433  0.9154 -1.3813  0.1688  0.1373]\n",
      "MSE loss: 85.9966\n",
      "Iteration: 152500\n",
      "Gradient: [ -6.3074  -9.5461  22.5609  74.9991 103.9156]\n",
      "Weights: [-4.8562  0.9203 -1.3786  0.1687  0.1371]\n",
      "MSE loss: 85.9887\n",
      "Iteration: 152600\n",
      "Gradient: [ -7.6729   9.3134  -2.9241   9.1275 158.6655]\n",
      "Weights: [-4.8609  0.9286 -1.3808  0.1681  0.1371]\n",
      "MSE loss: 86.0397\n",
      "Iteration: 152700\n",
      "Gradient: [  3.1998   6.6812  36.4061  78.201  326.8608]\n",
      "Weights: [-4.8685  0.9369 -1.3801  0.1677  0.137 ]\n",
      "MSE loss: 86.0907\n",
      "Iteration: 152800\n",
      "Gradient: [  -7.3872   -9.0083   18.6506  -33.1164 -280.0709]\n",
      "Weights: [-4.8691  0.9428 -1.3829  0.1668  0.137 ]\n",
      "MSE loss: 86.3928\n",
      "Iteration: 152900\n",
      "Gradient: [  0.4305 -21.2346  12.7003   8.8605 179.4618]\n",
      "Weights: [-4.8588  0.9368 -1.3826  0.1679  0.1372]\n",
      "MSE loss: 86.0894\n",
      "Iteration: 153000\n",
      "Gradient: [  3.5699   4.8559 -25.3507  27.8747 172.3302]\n",
      "Weights: [-4.8702  0.9462 -1.3851  0.1676  0.1373]\n",
      "MSE loss: 86.0966\n",
      "Iteration: 153100\n",
      "Gradient: [ -8.2295   3.9062  -3.9056  24.6329 144.3291]\n",
      "Weights: [-4.8602  0.939  -1.3831  0.1675  0.1373]\n",
      "MSE loss: 86.0925\n",
      "Iteration: 153200\n",
      "Gradient: [  -3.6782  -10.7811   -5.7299   66.2031 -194.3876]\n",
      "Weights: [-4.8625  0.9314 -1.381   0.1671  0.1374]\n",
      "MSE loss: 86.0732\n",
      "Iteration: 153300\n",
      "Gradient: [  8.5938  12.1958 -10.9931 -65.0117 -53.2487]\n",
      "Weights: [-4.8416  0.9102 -1.3792  0.1684  0.1375]\n",
      "MSE loss: 86.0038\n",
      "Iteration: 153400\n",
      "Gradient: [ 8.5017 12.7596 45.1814 78.2637 18.2271]\n",
      "Weights: [-4.8363  0.9067 -1.38    0.1689  0.1376]\n",
      "MSE loss: 86.0401\n",
      "Iteration: 153500\n",
      "Gradient: [  2.835    1.331   15.7192  15.84   208.0901]\n",
      "Weights: [-4.8326  0.9091 -1.3807  0.1684  0.1376]\n",
      "MSE loss: 86.1074\n",
      "Iteration: 153600\n",
      "Gradient: [  -3.4173  -25.2681    3.1588   -7.4717 -125.2891]\n",
      "Weights: [-4.8569  0.9264 -1.3841  0.1688  0.1375]\n",
      "MSE loss: 85.9872\n",
      "Iteration: 153700\n",
      "Gradient: [  0.2326  -2.9982  49.3904  79.3262 178.2513]\n",
      "Weights: [-4.8562  0.9389 -1.3867  0.1689  0.1374]\n",
      "MSE loss: 86.1512\n",
      "Iteration: 153800\n",
      "Gradient: [  6.2788  -6.8816 -12.7011  57.1019 -79.7996]\n",
      "Weights: [-4.8483  0.9346 -1.3887  0.1693  0.1377]\n",
      "MSE loss: 86.2201\n",
      "Iteration: 153900\n",
      "Gradient: [  -3.5071  -13.9431  -18.942  -104.1638 -190.9679]\n",
      "Weights: [-4.8721  0.9437 -1.3879  0.1674  0.1377]\n",
      "MSE loss: 86.1814\n",
      "Iteration: 154000\n",
      "Gradient: [   4.9274   -4.6633   10.8984  -54.6511 -214.6129]\n",
      "Weights: [-4.8657  0.9474 -1.3882  0.1672  0.1378]\n",
      "MSE loss: 86.0686\n",
      "Iteration: 154100\n",
      "Gradient: [ -2.9503  -4.3725  -3.9366 -40.1016  10.435 ]\n",
      "Weights: [-4.8642  0.9376 -1.388   0.1676  0.1379]\n",
      "MSE loss: 86.0886\n",
      "Iteration: 154200\n",
      "Gradient: [  -7.5204   -4.7033  -12.8163   61.8651 -355.1692]\n",
      "Weights: [-4.8813  0.9446 -1.3889  0.1679  0.138 ]\n",
      "MSE loss: 86.3887\n",
      "Iteration: 154300\n",
      "Gradient: [  2.9795  11.4725  -2.4316 -43.5572 -56.4432]\n",
      "Weights: [-4.8506  0.9415 -1.389   0.1674  0.138 ]\n",
      "MSE loss: 86.2744\n",
      "Iteration: 154400\n",
      "Gradient: [  7.6838   3.1151 -11.3331  39.3412  99.3056]\n",
      "Weights: [-4.8576  0.9472 -1.3922  0.1679  0.138 ]\n",
      "MSE loss: 86.1055\n",
      "Iteration: 154500\n",
      "Gradient: [ 7.5778  9.9547 -9.7621 87.6511 35.035 ]\n",
      "Weights: [-4.8522  0.9451 -1.3925  0.1675  0.1382]\n",
      "MSE loss: 86.1368\n",
      "Iteration: 154600\n",
      "Gradient: [ -11.1757  -10.5515   19.6182  -58.7264 -149.7023]\n",
      "Weights: [-4.8673  0.9492 -1.3937  0.1678  0.138 ]\n",
      "MSE loss: 86.0772\n",
      "Iteration: 154700\n",
      "Gradient: [  -0.8091  -11.6412   75.7924  -26.1638 -301.8101]\n",
      "Weights: [-4.8519  0.9362 -1.3903  0.1684  0.1379]\n",
      "MSE loss: 86.0037\n",
      "Iteration: 154800\n",
      "Gradient: [  -1.4136    9.4313  -19.7815 -173.9056  251.9138]\n",
      "Weights: [-4.8591  0.9378 -1.3922  0.1685  0.1378]\n",
      "MSE loss: 86.2553\n",
      "Iteration: 154900\n",
      "Gradient: [ -2.0188  12.4625  24.8967 -61.6051  32.1262]\n",
      "Weights: [-4.8599  0.9337 -1.3862  0.1686  0.1377]\n",
      "MSE loss: 86.0466\n",
      "Iteration: 155000\n",
      "Gradient: [ 10.4513  -3.8962   2.8808  35.967  -32.6021]\n",
      "Weights: [-4.8542  0.9283 -1.3866  0.1695  0.1378]\n",
      "MSE loss: 86.1392\n",
      "Iteration: 155100\n",
      "Gradient: [ -7.5056  -0.3983 -11.7303 -14.0478 -74.0897]\n",
      "Weights: [-4.8678  0.9464 -1.3912  0.169   0.1376]\n",
      "MSE loss: 86.0284\n",
      "Iteration: 155200\n",
      "Gradient: [12.2908  0.2879 23.5742 55.2642 -6.5632]\n",
      "Weights: [-4.8483  0.9423 -1.3928  0.169   0.1378]\n",
      "MSE loss: 86.1734\n",
      "Iteration: 155300\n",
      "Gradient: [  -3.9635   -7.7703  -35.3808   54.2628 -218.4117]\n",
      "Weights: [-4.8633  0.9468 -1.3945  0.1693  0.1376]\n",
      "MSE loss: 86.0311\n",
      "Iteration: 155400\n",
      "Gradient: [  1.2922   0.0537  -5.7599 -14.7387 -11.2318]\n",
      "Weights: [-4.8581  0.9478 -1.395   0.1698  0.1375]\n",
      "MSE loss: 86.01\n",
      "Iteration: 155500\n",
      "Gradient: [ 11.4043  -5.7427  23.6525  81.3027 166.0047]\n",
      "Weights: [-4.8533  0.9468 -1.393   0.17    0.1374]\n",
      "MSE loss: 86.3083\n",
      "Iteration: 155600\n",
      "Gradient: [  8.4275 -11.6191  26.1788 -33.2083  -4.4948]\n",
      "Weights: [-4.8681  0.9509 -1.397   0.1701  0.1374]\n",
      "MSE loss: 86.1646\n",
      "Iteration: 155700\n",
      "Gradient: [  6.7525  -8.836  -13.5699 178.246  116.0628]\n",
      "Weights: [-4.8613  0.9559 -1.3976  0.1713  0.1374]\n",
      "MSE loss: 86.3517\n",
      "Iteration: 155800\n",
      "Gradient: [   1.2485   -1.3383  -26.0839   83.8074 -180.1224]\n",
      "Weights: [-4.8672  0.9504 -1.3998  0.1718  0.1374]\n",
      "MSE loss: 85.9918\n",
      "Iteration: 155900\n",
      "Gradient: [ -6.425    0.344  -31.4967  68.25   -20.7618]\n",
      "Weights: [-4.8655  0.947  -1.399   0.1723  0.1374]\n",
      "MSE loss: 85.9674\n",
      "Iteration: 156000\n",
      "Gradient: [ -7.8472  -9.4685  -1.9316 -36.2184 -35.0744]\n",
      "Weights: [-4.8564  0.9447 -1.398   0.1717  0.1374]\n",
      "MSE loss: 85.9538\n",
      "Iteration: 156100\n",
      "Gradient: [   9.9648   -0.6712   -0.5016  -98.2926 -142.9351]\n",
      "Weights: [-4.854   0.9436 -1.3993  0.1722  0.1374]\n",
      "MSE loss: 85.9443\n",
      "Iteration: 156200\n",
      "Gradient: [-4.39000e-02 -8.58210e+00  4.21673e+01 -4.22420e+00  8.52975e+01]\n",
      "Weights: [-4.8465  0.9447 -1.4003  0.1723  0.1373]\n",
      "MSE loss: 86.0864\n",
      "Iteration: 156300\n",
      "Gradient: [ -2.0714  10.8284  -9.3244  57.218  145.0439]\n",
      "Weights: [-4.8589  0.9465 -1.4001  0.1728  0.1373]\n",
      "MSE loss: 85.9537\n",
      "Iteration: 156400\n",
      "Gradient: [  2.2751   7.8285  24.4495  52.1408 -97.8651]\n",
      "Weights: [-4.8538  0.9415 -1.4001  0.1733  0.1371]\n",
      "MSE loss: 85.9236\n",
      "Iteration: 156500\n",
      "Gradient: [  2.7194 -27.3547  32.7076  52.3827 102.266 ]\n",
      "Weights: [-4.8491  0.9298 -1.3975  0.1731  0.1373]\n",
      "MSE loss: 85.9759\n",
      "Iteration: 156600\n",
      "Gradient: [ -14.0603  -25.2541  -23.5513   93.3166 -303.7806]\n",
      "Weights: [-4.8482  0.9287 -1.4007  0.1737  0.1374]\n",
      "MSE loss: 86.1295\n",
      "Iteration: 156700\n",
      "Gradient: [ -4.3142 -13.5841 -25.2785   2.3129 323.2125]\n",
      "Weights: [-4.842   0.9286 -1.4002  0.1744  0.1374]\n",
      "MSE loss: 86.0171\n",
      "Iteration: 156800\n",
      "Gradient: [ -7.3529   6.0873 -20.9319 -54.9216   6.848 ]\n",
      "Weights: [-4.8538  0.9225 -1.3974  0.1749  0.1372]\n",
      "MSE loss: 86.1354\n",
      "Iteration: 156900\n",
      "Gradient: [ -6.3305  -9.9191  30.5163  63.9465 197.2765]\n",
      "Weights: [-4.828   0.9102 -1.395   0.1752  0.1369]\n",
      "MSE loss: 86.0879\n",
      "Iteration: 157000\n",
      "Gradient: [  4.4109  11.5724  36.2923 -27.5264  -3.6208]\n",
      "Weights: [-4.8301  0.9148 -1.395   0.1748  0.137 ]\n",
      "MSE loss: 86.0856\n",
      "Iteration: 157100\n",
      "Gradient: [  0.1027  -0.3223 -33.2058  -2.8102 -76.6616]\n",
      "Weights: [-4.8445  0.9124 -1.3962  0.1756  0.1373]\n",
      "MSE loss: 86.1717\n",
      "Iteration: 157200\n",
      "Gradient: [ -4.967  -24.8876 -11.892  -72.1211  20.7911]\n",
      "Weights: [-4.84    0.9155 -1.3968  0.1752  0.1369]\n",
      "MSE loss: 86.1192\n",
      "Iteration: 157300\n",
      "Gradient: [  2.0463  19.6245  18.1383  64.5891 201.7032]\n",
      "Weights: [-4.8326  0.9322 -1.3979  0.1738  0.1371]\n",
      "MSE loss: 86.458\n",
      "Iteration: 157400\n",
      "Gradient: [ 5.410000e-02  9.119400e+00 -3.055300e+01 -6.418390e+01  1.609117e+02]\n",
      "Weights: [-4.8597  0.9365 -1.4001  0.1736  0.1372]\n",
      "MSE loss: 86.1705\n",
      "Iteration: 157500\n",
      "Gradient: [  9.0552   6.8321  10.2332 -38.9227 165.5288]\n",
      "Weights: [-4.8297  0.9338 -1.4011  0.1744  0.1372]\n",
      "MSE loss: 86.5418\n",
      "Iteration: 157600\n",
      "Gradient: [  0.7258   3.8067   3.3495  87.62   104.6448]\n",
      "Weights: [-4.8263  0.9195 -1.4003  0.175   0.1372]\n",
      "MSE loss: 86.1914\n",
      "Iteration: 157700\n",
      "Gradient: [ -2.0089  -6.7838  37.75   -49.3365  41.2036]\n",
      "Weights: [-4.8476  0.9197 -1.3973  0.1753  0.137 ]\n",
      "MSE loss: 86.0512\n",
      "Iteration: 157800\n",
      "Gradient: [   5.7378    3.7109   29.1544 -109.1852  -36.9611]\n",
      "Weights: [-4.8546  0.9254 -1.395   0.1753  0.1366]\n",
      "MSE loss: 85.9545\n",
      "Iteration: 157900\n",
      "Gradient: [  1.1087  13.437   24.1617  -3.6692 103.0833]\n",
      "Weights: [-4.8385  0.9257 -1.3961  0.1756  0.1364]\n",
      "MSE loss: 85.9976\n",
      "Iteration: 158000\n",
      "Gradient: [ -6.467   13.8457  15.2568 122.9983 304.9177]\n",
      "Weights: [-4.8505  0.9279 -1.3962  0.1755  0.1365]\n",
      "MSE loss: 85.8912\n",
      "Iteration: 158100\n",
      "Gradient: [  3.1637 -23.0233 -36.4438  55.8637  66.3047]\n",
      "Weights: [-4.8444  0.9255 -1.3953  0.1759  0.1363]\n",
      "MSE loss: 85.8982\n",
      "Iteration: 158200\n",
      "Gradient: [ 5.0648 -6.9635 -7.591  56.783  25.9571]\n",
      "Weights: [-4.8337  0.9234 -1.3934  0.1753  0.136 ]\n",
      "MSE loss: 86.1289\n",
      "Iteration: 158300\n",
      "Gradient: [   2.7846    3.2267   16.4059    9.7876 -100.3419]\n",
      "Weights: [-4.8617  0.9276 -1.3923  0.1755  0.1362]\n",
      "MSE loss: 85.9735\n",
      "Iteration: 158400\n",
      "Gradient: [   0.9811  -18.2206  -18.618   -36.9218 -146.5299]\n",
      "Weights: [-4.8517  0.9264 -1.3933  0.1753  0.1361]\n",
      "MSE loss: 85.9308\n",
      "Iteration: 158500\n",
      "Gradient: [   5.7514    0.4947   -3.6924   -2.2157 -108.8206]\n",
      "Weights: [-4.8441  0.93   -1.3943  0.1743  0.1363]\n",
      "MSE loss: 85.9766\n",
      "Iteration: 158600\n",
      "Gradient: [  10.7736  -12.3913   22.6009   89.252  -326.8446]\n",
      "Weights: [-4.8512  0.9288 -1.3961  0.1755  0.1364]\n",
      "MSE loss: 85.8809\n",
      "Iteration: 158700\n",
      "Gradient: [   0.4094  -20.4261  -18.336   -39.0711 -172.1782]\n",
      "Weights: [-4.8448  0.927  -1.3941  0.1749  0.1362]\n",
      "MSE loss: 85.944\n",
      "Iteration: 158800\n",
      "Gradient: [  -7.0705   10.7769    6.4645   32.1081 -295.0654]\n",
      "Weights: [-4.867   0.9388 -1.3925  0.1739  0.1362]\n",
      "MSE loss: 85.9968\n",
      "Iteration: 158900\n",
      "Gradient: [-9.640000e-02 -3.095120e+01  2.603770e+01 -6.484200e+00 -3.572536e+02]\n",
      "Weights: [-4.854   0.9291 -1.393   0.1743  0.1362]\n",
      "MSE loss: 85.9866\n",
      "Iteration: 159000\n",
      "Gradient: [-12.95     3.8363 -69.8638  53.8662  -5.6506]\n",
      "Weights: [-4.8672  0.9404 -1.3954  0.1738  0.1363]\n",
      "MSE loss: 86.1888\n",
      "Iteration: 159100\n",
      "Gradient: [  0.211    4.8204 -27.1245   6.7263  45.19  ]\n",
      "Weights: [-4.8693  0.9454 -1.3941  0.1742  0.1364]\n",
      "MSE loss: 86.1\n",
      "Iteration: 159200\n",
      "Gradient: [-6.0306 -4.5988 30.9964 43.3833  9.4531]\n",
      "Weights: [-4.8495  0.9337 -1.3937  0.1737  0.1364]\n",
      "MSE loss: 85.9487\n",
      "Iteration: 159300\n",
      "Gradient: [-1.5312  0.0989  1.1972 31.6185 18.9599]\n",
      "Weights: [-4.8535  0.934  -1.3917  0.1735  0.1364]\n",
      "MSE loss: 85.9378\n",
      "Iteration: 159400\n",
      "Gradient: [  2.7307   9.6467  -0.3353 -65.9986  59.6263]\n",
      "Weights: [-4.8515  0.9283 -1.3924  0.174   0.1363]\n",
      "MSE loss: 85.9262\n",
      "Iteration: 159500\n",
      "Gradient: [-12.8525  -2.0559 -13.7675  38.0041 -55.9053]\n",
      "Weights: [-4.8785  0.9457 -1.3935  0.1738  0.1364]\n",
      "MSE loss: 86.1519\n",
      "Iteration: 159600\n",
      "Gradient: [ -0.4164  -7.4962  50.2977 -42.0534 -99.4093]\n",
      "Weights: [-4.8473  0.9381 -1.3954  0.1729  0.1365]\n",
      "MSE loss: 86.0654\n",
      "Iteration: 159700\n",
      "Gradient: [ -6.4795 -11.0882  -2.8596 -26.0761 166.8531]\n",
      "Weights: [-4.8692  0.9419 -1.3931  0.1721  0.1368]\n",
      "MSE loss: 86.0289\n",
      "Iteration: 159800\n",
      "Gradient: [ -4.6194  -5.9267 -41.9559  49.0389 -75.9553]\n",
      "Weights: [-4.8633  0.9388 -1.3933  0.1723  0.1369]\n",
      "MSE loss: 85.9547\n",
      "Iteration: 159900\n",
      "Gradient: [-14.4677  -7.2158  -7.27   -56.2145 137.7066]\n",
      "Weights: [-4.8776  0.9436 -1.3938  0.1721  0.1369]\n",
      "MSE loss: 86.2937\n",
      "Iteration: 160000\n",
      "Gradient: [   0.1854    3.004    18.7087   34.9969 -174.0626]\n",
      "Weights: [-4.8513  0.9411 -1.3939  0.1718  0.1369]\n",
      "MSE loss: 86.0534\n",
      "Iteration: 160100\n",
      "Gradient: [ -4.1448  24.5066   7.9344 -46.9212  21.6437]\n",
      "Weights: [-4.8574  0.9373 -1.3933  0.1716  0.1369]\n",
      "MSE loss: 86.0051\n",
      "Iteration: 160200\n",
      "Gradient: [ -6.0796  -1.2998 -33.0366 -40.6007  23.6868]\n",
      "Weights: [-4.8654  0.9446 -1.3932  0.1711  0.1371]\n",
      "MSE loss: 85.9728\n",
      "Iteration: 160300\n",
      "Gradient: [ 10.4853  21.057   17.3352  48.962  -95.4992]\n",
      "Weights: [-4.8499  0.933  -1.3898  0.1716  0.1371]\n",
      "MSE loss: 86.1497\n",
      "Iteration: 160400\n",
      "Gradient: [   1.2414   -3.9552    0.7175   92.4948 -245.7281]\n",
      "Weights: [-4.8533  0.9244 -1.3906  0.1719  0.1373]\n",
      "MSE loss: 85.9811\n",
      "Iteration: 160500\n",
      "Gradient: [  0.6557  -4.4347   9.2794 135.7231 121.1826]\n",
      "Weights: [-4.8528  0.9175 -1.3884  0.1723  0.1372]\n",
      "MSE loss: 86.042\n",
      "Iteration: 160600\n",
      "Gradient: [   7.5595  -14.5027   -1.1617  -33.7571 -126.0319]\n",
      "Weights: [-4.8509  0.9274 -1.3916  0.172   0.1372]\n",
      "MSE loss: 85.9355\n",
      "Iteration: 160700\n",
      "Gradient: [  3.355   13.9545  19.8874 -18.1727 -79.972 ]\n",
      "Weights: [-4.8574  0.9346 -1.3925  0.1715  0.1372]\n",
      "MSE loss: 85.9411\n",
      "Iteration: 160800\n",
      "Gradient: [-5.065800e+00 -1.290570e+01 -4.740000e-02 -1.873670e+01 -1.371538e+02]\n",
      "Weights: [-4.8545  0.9317 -1.3917  0.1712  0.137 ]\n",
      "MSE loss: 86.0755\n",
      "Iteration: 160900\n",
      "Gradient: [ -0.6011  12.3345  10.0674 -19.6614 208.6004]\n",
      "Weights: [-4.8408  0.9175 -1.3904  0.1717  0.1373]\n",
      "MSE loss: 86.0083\n",
      "Iteration: 161000\n",
      "Gradient: [  9.1612  12.0078   9.9253 104.9471  49.0619]\n",
      "Weights: [-4.8501  0.9178 -1.3889  0.172   0.1375]\n",
      "MSE loss: 86.0528\n",
      "Iteration: 161100\n",
      "Gradient: [-2.1389  7.6759 47.2947 52.962  -9.3814]\n",
      "Weights: [-4.8583  0.9293 -1.3879  0.1714  0.1374]\n",
      "MSE loss: 86.3146\n",
      "Iteration: 161200\n",
      "Gradient: [  5.1696   8.8523  21.5923 -48.1448  51.5843]\n",
      "Weights: [-4.8534  0.9327 -1.3921  0.1714  0.1374]\n",
      "MSE loss: 85.9775\n",
      "Iteration: 161300\n",
      "Gradient: [   4.9116  -10.9553   24.0034  113.7931 -157.1818]\n",
      "Weights: [-4.8623  0.9387 -1.3937  0.171   0.1372]\n",
      "MSE loss: 86.0654\n",
      "Iteration: 161400\n",
      "Gradient: [   2.6052    2.077   -38.4134   53.6781 -227.725 ]\n",
      "Weights: [-4.8632  0.9489 -1.3954  0.1708  0.1372]\n",
      "MSE loss: 85.9882\n",
      "Iteration: 161500\n",
      "Gradient: [  4.8098  -1.9514 -23.4249 -76.3829 -16.2679]\n",
      "Weights: [-4.8597  0.9486 -1.3945  0.1699  0.1373]\n",
      "MSE loss: 86.0148\n",
      "Iteration: 161600\n",
      "Gradient: [  -3.8369    2.9252  -31.7507  -69.8688 -127.1291]\n",
      "Weights: [-4.8673  0.9457 -1.3935  0.1701  0.1374]\n",
      "MSE loss: 86.0087\n",
      "Iteration: 161700\n",
      "Gradient: [ -2.808    3.203   27.7781 127.507  355.4421]\n",
      "Weights: [-4.8588  0.9478 -1.3941  0.1698  0.1377]\n",
      "MSE loss: 86.1692\n",
      "Iteration: 161800\n",
      "Gradient: [-0.8114 -6.8665 13.6935 77.0368 14.6996]\n",
      "Weights: [-4.8619  0.9485 -1.3965  0.1703  0.1375]\n",
      "MSE loss: 85.9842\n",
      "Iteration: 161900\n",
      "Gradient: [ 0.1253  4.9712 35.0714 46.4699 50.4784]\n",
      "Weights: [-4.8662  0.9505 -1.3967  0.1708  0.1373]\n",
      "MSE loss: 85.9805\n",
      "Iteration: 162000\n",
      "Gradient: [  8.4879   6.9006 -21.9131 -58.8869 143.9533]\n",
      "Weights: [-4.8835  0.9665 -1.3986  0.1704  0.1373]\n",
      "MSE loss: 86.1576\n",
      "Iteration: 162100\n",
      "Gradient: [  -1.0667   -4.9647  -18.2226   41.0179 -197.2269]\n",
      "Weights: [-4.8792  0.9666 -1.3985  0.1698  0.1374]\n",
      "MSE loss: 86.1141\n",
      "Iteration: 162200\n",
      "Gradient: [-4.914200e+00  2.124000e-01  3.020820e+01 -5.256120e+01  3.172675e+02]\n",
      "Weights: [-4.8689  0.9619 -1.3976  0.1699  0.1374]\n",
      "MSE loss: 86.105\n",
      "Iteration: 162300\n",
      "Gradient: [ -3.3164  15.2768 -17.7855   2.3614 202.4627]\n",
      "Weights: [-4.8747  0.9664 -1.3997  0.1704  0.1375]\n",
      "MSE loss: 86.1347\n",
      "Iteration: 162400\n",
      "Gradient: [  2.1988  -3.578  -46.6378  15.8741  88.3085]\n",
      "Weights: [-4.8704  0.96   -1.4003  0.1708  0.1374]\n",
      "MSE loss: 86.0062\n",
      "Iteration: 162500\n",
      "Gradient: [  1.5167  18.6275  10.5171 -54.2393  48.2071]\n",
      "Weights: [-4.8564  0.9505 -1.3979  0.1716  0.1373]\n",
      "MSE loss: 86.1085\n",
      "Iteration: 162600\n",
      "Gradient: [ -4.7362 -21.4365   8.9262   5.562   -1.9173]\n",
      "Weights: [-4.8632  0.9536 -1.3988  0.1721  0.1372]\n",
      "MSE loss: 86.0635\n",
      "Iteration: 162700\n",
      "Gradient: [   1.8843  -17.4122  -50.1256   32.9166 -470.77  ]\n",
      "Weights: [-4.8403  0.9395 -1.402   0.1727  0.1374]\n",
      "MSE loss: 86.0977\n",
      "Iteration: 162800\n",
      "Gradient: [   1.0579   -0.5992   31.4844    5.3379 -118.9408]\n",
      "Weights: [-4.8543  0.9435 -1.4011  0.173   0.1374]\n",
      "MSE loss: 85.9274\n",
      "Iteration: 162900\n",
      "Gradient: [  9.1186  15.3973  33.7438  32.8827 -31.285 ]\n",
      "Weights: [-4.8463  0.9484 -1.4008  0.1724  0.1371]\n",
      "MSE loss: 86.1799\n",
      "Iteration: 163000\n",
      "Gradient: [-11.0575   3.3422 -36.2362 -81.2177 310.5433]\n",
      "Weights: [-4.8594  0.9505 -1.4048  0.1739  0.1371]\n",
      "MSE loss: 85.9525\n",
      "Iteration: 163100\n",
      "Gradient: [ -5.3331  -9.6523  47.9618  40.2525 279.3612]\n",
      "Weights: [-4.8575  0.9492 -1.4055  0.1746  0.137 ]\n",
      "MSE loss: 85.9083\n",
      "Iteration: 163200\n",
      "Gradient: [ -13.0091   -5.0431    8.0858 -110.5322 -214.8429]\n",
      "Weights: [-4.8681  0.95   -1.4069  0.1741  0.1372]\n",
      "MSE loss: 86.4142\n",
      "Iteration: 163300\n",
      "Gradient: [   5.4109   -7.9094   24.3318  -55.8742 -130.438 ]\n",
      "Weights: [-4.8431  0.9412 -1.405   0.174   0.1375]\n",
      "MSE loss: 86.0269\n",
      "Iteration: 163400\n",
      "Gradient: [  -9.3963  -18.2137    6.6615  -29.6944 -155.8166]\n",
      "Weights: [-4.8568  0.9423 -1.4079  0.175   0.1374]\n",
      "MSE loss: 86.1097\n",
      "Iteration: 163500\n",
      "Gradient: [ -9.2811  -4.7876 -74.9159  12.798   -2.2895]\n",
      "Weights: [-4.8503  0.9429 -1.4106  0.1756  0.1375]\n",
      "MSE loss: 86.0478\n",
      "Iteration: 163600\n",
      "Gradient: [  -1.3145    8.5903  -32.425   -12.8809 -133.0675]\n",
      "Weights: [-4.8533  0.9435 -1.4069  0.175   0.1373]\n",
      "MSE loss: 85.9375\n",
      "Iteration: 163700\n",
      "Gradient: [  -4.1589  -19.1421   22.1681  -54.5293 -119.483 ]\n",
      "Weights: [-4.851   0.936  -1.4065  0.1755  0.1374]\n",
      "MSE loss: 86.0473\n",
      "Iteration: 163800\n",
      "Gradient: [  -3.6777   15.5417  -14.7803   34.5519 -151.8509]\n",
      "Weights: [-4.8399  0.9305 -1.4046  0.1755  0.1371]\n",
      "MSE loss: 86.0437\n",
      "Iteration: 163900\n",
      "Gradient: [ -6.7066   6.4612  38.5643  60.9006 -10.6574]\n",
      "Weights: [-4.8568  0.9427 -1.4041  0.175   0.1372]\n",
      "MSE loss: 85.9161\n",
      "Iteration: 164000\n",
      "Gradient: [ -6.9952 -13.389  -20.3752 -42.6769 177.4154]\n",
      "Weights: [-4.8517  0.942  -1.4066  0.1755  0.1371]\n",
      "MSE loss: 85.9182\n",
      "Iteration: 164100\n",
      "Gradient: [  -4.1936   11.4189  -10.1001  -38.4617 -360.3296]\n",
      "Weights: [-4.8476  0.9335 -1.4032  0.1758  0.1369]\n",
      "MSE loss: 85.9226\n",
      "Iteration: 164200\n",
      "Gradient: [  0.5484  -2.7393  -5.1662 212.5574 -88.1879]\n",
      "Weights: [-4.8554  0.9393 -1.4039  0.1757  0.137 ]\n",
      "MSE loss: 85.9096\n",
      "Iteration: 164300\n",
      "Gradient: [  4.6107   3.6734 -34.809   20.975   26.0687]\n",
      "Weights: [-4.8477  0.9373 -1.4043  0.1752  0.1372]\n",
      "MSE loss: 85.9285\n",
      "Iteration: 164400\n",
      "Gradient: [  0.8345  22.5818 -37.5602  25.4261 -42.6729]\n",
      "Weights: [-4.8531  0.9421 -1.4053  0.176   0.1373]\n",
      "MSE loss: 86.1321\n",
      "Iteration: 164500\n",
      "Gradient: [  -2.8042   -4.9711   24.7429   88.5839 -223.7677]\n",
      "Weights: [-4.8523  0.9412 -1.4066  0.1761  0.1373]\n",
      "MSE loss: 85.9727\n",
      "Iteration: 164600\n",
      "Gradient: [  -1.4002   -0.8401   -7.2729 -104.5172   52.5827]\n",
      "Weights: [-4.8632  0.943  -1.4075  0.176   0.1373]\n",
      "MSE loss: 86.0966\n",
      "Iteration: 164700\n",
      "Gradient: [  -4.5826  -19.5535   12.0834 -125.7865 -350.4292]\n",
      "Weights: [-4.8614  0.9393 -1.4067  0.1762  0.137 ]\n",
      "MSE loss: 86.2094\n",
      "Iteration: 164800\n",
      "Gradient: [-4.0165 -0.5398 10.2522 90.3769 80.3729]\n",
      "Weights: [-4.8373  0.94   -1.406   0.1759  0.137 ]\n",
      "MSE loss: 86.2416\n",
      "Iteration: 164900\n",
      "Gradient: [-8.3912  0.2885  2.9776 37.1546 82.122 ]\n",
      "Weights: [-4.8624  0.9485 -1.4045  0.1753  0.1369]\n",
      "MSE loss: 85.9035\n",
      "Iteration: 165000\n",
      "Gradient: [  -8.6959   15.5084   -9.3954 -122.3241 -179.7087]\n",
      "Weights: [-4.8554  0.9464 -1.4031  0.1753  0.1366]\n",
      "MSE loss: 85.8975\n",
      "Iteration: 165100\n",
      "Gradient: [  0.6784   4.7606 -43.8656 -34.5741 -48.8614]\n",
      "Weights: [-4.8618  0.956  -1.4053  0.1755  0.1367]\n",
      "MSE loss: 85.978\n",
      "Iteration: 165200\n",
      "Gradient: [ -0.7314   0.885    4.7535  -0.4703 -11.9145]\n",
      "Weights: [-4.8669  0.9533 -1.4059  0.1765  0.1364]\n",
      "MSE loss: 85.9037\n",
      "Iteration: 165300\n",
      "Gradient: [-1.849000e-01  8.646100e+00 -5.623810e+01 -9.441400e+00  2.293179e+02]\n",
      "Weights: [-4.8563  0.9486 -1.4036  0.1761  0.1365]\n",
      "MSE loss: 86.0014\n",
      "Iteration: 165400\n",
      "Gradient: [ -1.4067  21.1453   4.5396  65.3712 217.1363]\n",
      "Weights: [-4.8732  0.9601 -1.4045  0.1758  0.1365]\n",
      "MSE loss: 86.059\n",
      "Iteration: 165500\n",
      "Gradient: [  -8.3494    9.1179  -10.6697   52.9613 -184.0911]\n",
      "Weights: [-4.8534  0.9492 -1.4073  0.1768  0.1364]\n",
      "MSE loss: 85.9016\n",
      "Iteration: 165600\n",
      "Gradient: [  6.6812 -12.8263 -28.4823   8.6564 -32.2346]\n",
      "Weights: [-4.8599  0.9487 -1.4051  0.1771  0.1363]\n",
      "MSE loss: 85.8625\n",
      "Iteration: 165700\n",
      "Gradient: [ -0.2335   5.9254  19.895  -17.9462  65.8372]\n",
      "Weights: [-4.8747  0.9596 -1.406   0.1763  0.1364]\n",
      "MSE loss: 85.9668\n",
      "Iteration: 165800\n",
      "Gradient: [ -7.8006   4.6624 -11.3176 -24.1387 295.6643]\n",
      "Weights: [-4.8578  0.9563 -1.4067  0.1758  0.1362]\n",
      "MSE loss: 86.0173\n",
      "Iteration: 165900\n",
      "Gradient: [  1.6409   7.5978  48.809   34.4368 256.0212]\n",
      "Weights: [-4.8539  0.9543 -1.4052  0.1766  0.1363]\n",
      "MSE loss: 86.2496\n",
      "Iteration: 166000\n",
      "Gradient: [-6.4467 -7.6234  3.9897  9.9998 48.513 ]\n",
      "Weights: [-4.8671  0.9558 -1.4072  0.1772  0.1363]\n",
      "MSE loss: 85.8834\n",
      "Iteration: 166100\n",
      "Gradient: [ -0.6588  24.7515  29.6469 130.0511  72.9621]\n",
      "Weights: [-4.8583  0.9536 -1.4073  0.1776  0.1361]\n",
      "MSE loss: 85.9229\n",
      "Iteration: 166200\n",
      "Gradient: [   1.5822    4.2187  -13.7346 -112.5518  133.4993]\n",
      "Weights: [-4.8823  0.9677 -1.4097  0.1775  0.1361]\n",
      "MSE loss: 86.0586\n",
      "Iteration: 166300\n",
      "Gradient: [  8.3204  23.5221  20.9062 120.9122  86.0811]\n",
      "Weights: [-4.8477  0.955  -1.4097  0.1786  0.136 ]\n",
      "MSE loss: 86.3563\n",
      "Iteration: 166400\n",
      "Gradient: [   1.7608    8.7063  -12.461   -91.5219 -300.2134]\n",
      "Weights: [-4.8623  0.9626 -1.4114  0.1781  0.136 ]\n",
      "MSE loss: 85.9235\n",
      "Iteration: 166500\n",
      "Gradient: [  -7.0494    2.3255   -0.4539 -119.3882  -71.2562]\n",
      "Weights: [-4.8721  0.9558 -1.4109  0.1788  0.1361]\n",
      "MSE loss: 85.9759\n",
      "Iteration: 166600\n",
      "Gradient: [ -1.1423  14.313    3.3653 -45.5105  -6.9822]\n",
      "Weights: [-4.8578  0.9487 -1.4097  0.1787  0.1361]\n",
      "MSE loss: 85.8543\n",
      "Iteration: 166700\n",
      "Gradient: [  -5.5074    2.6745  -52.4034  -32.5887 -328.4317]\n",
      "Weights: [-4.8544  0.9427 -1.4104  0.1791  0.1362]\n",
      "MSE loss: 85.9276\n",
      "Iteration: 166800\n",
      "Gradient: [  8.2775  -1.4759   8.5649  56.2316 -55.1936]\n",
      "Weights: [-4.8546  0.9428 -1.4098  0.1789  0.1363]\n",
      "MSE loss: 85.8775\n",
      "Iteration: 166900\n",
      "Gradient: [ 0.1404 16.515  33.5362  6.4726 28.5978]\n",
      "Weights: [-4.8371  0.9359 -1.4075  0.1785  0.1364]\n",
      "MSE loss: 86.0493\n",
      "Iteration: 167000\n",
      "Gradient: [  1.498   -7.7598   8.9142  29.3582 100.4771]\n",
      "Weights: [-4.8487  0.9366 -1.4075  0.1791  0.1364]\n",
      "MSE loss: 85.9021\n",
      "Iteration: 167100\n",
      "Gradient: [ 8.225100e+00  3.161000e-01  1.715000e-01  1.736592e+02 -4.060440e+01]\n",
      "Weights: [-4.8551  0.9453 -1.4087  0.1794  0.1361]\n",
      "MSE loss: 85.8491\n",
      "Iteration: 167200\n",
      "Gradient: [  -3.8847   -6.7742   -9.7751  -12.887  -113.8809]\n",
      "Weights: [-4.8786  0.9548 -1.4079  0.1787  0.136 ]\n",
      "MSE loss: 86.0967\n",
      "Iteration: 167300\n",
      "Gradient: [-11.5106   5.5414 -12.3952  64.6872  24.1948]\n",
      "Weights: [-4.8598  0.9574 -1.4101  0.1785  0.1361]\n",
      "MSE loss: 85.9822\n",
      "Iteration: 167400\n",
      "Gradient: [   1.6713   -3.9274   31.2719   -3.3245 -146.7064]\n",
      "Weights: [-4.8666  0.9548 -1.4102  0.1784  0.1362]\n",
      "MSE loss: 85.8798\n",
      "Iteration: 167500\n",
      "Gradient: [  4.3468   1.2171  29.6149  44.3258 290.8075]\n",
      "Weights: [-4.8518  0.9547 -1.4104  0.178   0.1363]\n",
      "MSE loss: 86.0142\n",
      "Iteration: 167600\n",
      "Gradient: [ -6.8191  10.2251  -4.5621  80.0234 229.042 ]\n",
      "Weights: [-4.8565  0.9548 -1.4125  0.1781  0.1365]\n",
      "MSE loss: 85.8601\n",
      "Iteration: 167700\n",
      "Gradient: [  2.9443 -14.2425 -19.6698  49.583  -32.6697]\n",
      "Weights: [-4.8705  0.9692 -1.4166  0.1784  0.1362]\n",
      "MSE loss: 85.9664\n",
      "Iteration: 167800\n",
      "Gradient: [ 0.9035  3.6212 -5.6716 -2.4997 60.5   ]\n",
      "Weights: [-4.8452  0.955  -1.415   0.1783  0.1365]\n",
      "MSE loss: 86.0403\n",
      "Iteration: 167900\n",
      "Gradient: [  5.81    11.5684  36.4634 -18.5257  33.7106]\n",
      "Weights: [-4.8758  0.97   -1.4148  0.1775  0.1365]\n",
      "MSE loss: 85.9384\n",
      "Iteration: 168000\n",
      "Gradient: [-0.2859 15.3211 42.9664 64.8282 98.8868]\n",
      "Weights: [-4.8591  0.965  -1.4139  0.1778  0.1365]\n",
      "MSE loss: 86.0099\n",
      "Iteration: 168100\n",
      "Gradient: [ 14.1123   6.1136  18.4194 -99.4477 -73.2441]\n",
      "Weights: [-4.8484  0.9579 -1.414   0.179   0.1362]\n",
      "MSE loss: 86.1155\n",
      "Iteration: 168200\n",
      "Gradient: [  2.2286  -8.9853   4.4738 -27.5311 -17.4498]\n",
      "Weights: [-4.8846  0.9702 -1.4158  0.1787  0.1363]\n",
      "MSE loss: 86.1555\n",
      "Iteration: 168300\n",
      "Gradient: [  9.3657  -7.1494 -40.8654 -93.4856 176.3773]\n",
      "Weights: [-4.8583  0.9743 -1.4202  0.1783  0.1365]\n",
      "MSE loss: 86.0452\n",
      "Iteration: 168400\n",
      "Gradient: [   0.532   -28.7484  -23.7004   24.9769 -347.328 ]\n",
      "Weights: [-4.8791  0.9753 -1.4215  0.1791  0.1367]\n",
      "MSE loss: 85.9891\n",
      "Iteration: 168500\n",
      "Gradient: [ -5.4852  -1.4146   1.9417 -13.6016  57.0128]\n",
      "Weights: [-4.8676  0.9727 -1.4213  0.1786  0.1367]\n",
      "MSE loss: 85.8861\n",
      "Iteration: 168600\n",
      "Gradient: [-1.2889 -2.5647  2.0608 52.3261 -0.5954]\n",
      "Weights: [-4.8616  0.9697 -1.4222  0.1796  0.1366]\n",
      "MSE loss: 85.8565\n",
      "Iteration: 168700\n",
      "Gradient: [  -5.7172   -4.8906   -1.5895   11.4439 -192.8785]\n",
      "Weights: [-4.8713  0.9678 -1.42    0.1798  0.1366]\n",
      "MSE loss: 85.8847\n",
      "Iteration: 168800\n",
      "Gradient: [-2.8799  0.6793 29.7828 45.3515 33.1749]\n",
      "Weights: [-4.8759  0.9755 -1.4186  0.1799  0.1362]\n",
      "MSE loss: 85.9997\n",
      "Iteration: 168900\n",
      "Gradient: [ 1.5443  3.7352 23.7655 20.8875 66.5831]\n",
      "Weights: [-4.861   0.9727 -1.4214  0.1802  0.1361]\n",
      "MSE loss: 85.9371\n",
      "Iteration: 169000\n",
      "Gradient: [-2.332  -8.892  19.0256 34.4926 53.9161]\n",
      "Weights: [-4.8663  0.9729 -1.4234  0.18    0.1364]\n",
      "MSE loss: 85.8641\n",
      "Iteration: 169100\n",
      "Gradient: [ -14.2153   -6.3258  -20.9998 -108.9476  106.3095]\n",
      "Weights: [-4.876   0.9756 -1.424   0.1807  0.1363]\n",
      "MSE loss: 85.9423\n",
      "Iteration: 169200\n",
      "Gradient: [ -10.4078   -1.6326   15.4664  -65.6673 -192.6921]\n",
      "Weights: [-4.8659  0.9753 -1.4259  0.1812  0.136 ]\n",
      "MSE loss: 85.9598\n",
      "Iteration: 169300\n",
      "Gradient: [   6.5759    1.7908   21.4447   58.7091 -134.5392]\n",
      "Weights: [-4.8614  0.9722 -1.4227  0.1808  0.1362]\n",
      "MSE loss: 85.919\n",
      "Iteration: 169400\n",
      "Gradient: [  10.5366  -12.2273   24.8402   72.018  -135.8421]\n",
      "Weights: [-4.8568  0.9671 -1.4243  0.1813  0.1363]\n",
      "MSE loss: 85.8514\n",
      "Iteration: 169500\n",
      "Gradient: [  8.2152   7.5395 -15.8099  53.6925 133.7424]\n",
      "Weights: [-4.8603  0.9673 -1.4236  0.1821  0.1363]\n",
      "MSE loss: 85.9405\n",
      "Iteration: 169600\n",
      "Gradient: [  0.6582 -15.3648  -6.2283 -62.4832  74.6547]\n",
      "Weights: [-4.863   0.9575 -1.42    0.1823  0.136 ]\n",
      "MSE loss: 85.8177\n",
      "Iteration: 169700\n",
      "Gradient: [   7.1345   -3.3448   21.4906    7.8342 -154.2144]\n",
      "Weights: [-4.8641  0.9642 -1.4198  0.1825  0.1358]\n",
      "MSE loss: 85.9322\n",
      "Iteration: 169800\n",
      "Gradient: [ 1.106480e+01  4.500000e-02 -2.453530e+01  1.037528e+02 -4.428739e+02]\n",
      "Weights: [-4.8656  0.9693 -1.4212  0.1822  0.1356]\n",
      "MSE loss: 85.8191\n",
      "Iteration: 169900\n",
      "Gradient: [  -4.0539    8.0649    4.8784  -35.4573 -309.431 ]\n",
      "Weights: [-4.8713  0.9696 -1.4225  0.1814  0.1359]\n",
      "MSE loss: 85.9652\n",
      "Iteration: 170000\n",
      "Gradient: [-0.2567 -2.3668 23.3521  9.417  -1.2136]\n",
      "Weights: [-4.8576  0.9614 -1.4188  0.1814  0.1359]\n",
      "MSE loss: 85.8656\n",
      "Iteration: 170100\n",
      "Gradient: [  1.4625   1.3877   3.8097 -29.4938  88.1615]\n",
      "Weights: [-4.8535  0.9631 -1.4187  0.1822  0.1356]\n",
      "MSE loss: 86.1076\n",
      "Iteration: 170200\n",
      "Gradient: [ -9.6777   5.2881  37.0329   8.2022 -51.3429]\n",
      "Weights: [-4.8688  0.9717 -1.4189  0.1816  0.1357]\n",
      "MSE loss: 85.9764\n",
      "Iteration: 170300\n",
      "Gradient: [ -6.2181  -4.0227 -17.6273 -61.1928  12.6208]\n",
      "Weights: [-4.8856  0.9777 -1.4219  0.1808  0.1359]\n",
      "MSE loss: 86.1286\n",
      "Iteration: 170400\n",
      "Gradient: [ 4.1123 -4.4891 25.0818 46.9817 50.2987]\n",
      "Weights: [-4.8675  0.9698 -1.4209  0.1802  0.1362]\n",
      "MSE loss: 85.8521\n",
      "Iteration: 170500\n",
      "Gradient: [  4.4705   3.24    -1.427  188.134  -37.1664]\n",
      "Weights: [-4.8676  0.9708 -1.4195  0.1804  0.1362]\n",
      "MSE loss: 85.9165\n",
      "Iteration: 170600\n",
      "Gradient: [  -5.2465   -3.4896   -0.6175 -162.4532  136.3437]\n",
      "Weights: [-4.8807  0.9754 -1.4218  0.1808  0.1362]\n",
      "MSE loss: 85.9616\n",
      "Iteration: 170700\n",
      "Gradient: [-4.6409 -3.1113 -4.3497 -9.1982 75.1402]\n",
      "Weights: [-4.8666  0.9735 -1.4263  0.1811  0.1363]\n",
      "MSE loss: 85.8945\n",
      "Iteration: 170800\n",
      "Gradient: [  1.2918 -10.926    4.6147 -44.4713  20.9621]\n",
      "Weights: [-4.8695  0.98   -1.428   0.1816  0.1362]\n",
      "MSE loss: 85.8363\n",
      "Iteration: 170900\n",
      "Gradient: [  3.3978  -9.9514   4.163   84.1222 -21.7656]\n",
      "Weights: [-4.8532  0.9678 -1.4283  0.1826  0.1363]\n",
      "MSE loss: 85.8628\n",
      "Iteration: 171000\n",
      "Gradient: [ -3.1558 -15.2632 -21.9491 -14.3493 167.805 ]\n",
      "Weights: [-4.8672  0.9698 -1.4287  0.1825  0.1363]\n",
      "MSE loss: 85.9208\n",
      "Iteration: 171100\n",
      "Gradient: [  -7.6238    6.0677   -8.3849  -68.0336 -168.133 ]\n",
      "Weights: [-4.8803  0.9734 -1.4274  0.1824  0.1364]\n",
      "MSE loss: 86.0977\n",
      "Iteration: 171200\n",
      "Gradient: [  -5.4993  -12.4053   -1.0107    3.0024 -131.1806]\n",
      "Weights: [-4.8596  0.9717 -1.4279  0.1827  0.1361]\n",
      "MSE loss: 85.8251\n",
      "Iteration: 171300\n",
      "Gradient: [  -0.173    -1.7797  -11.7281   95.1765 -105.8526]\n",
      "Weights: [-4.853   0.9626 -1.4271  0.1829  0.1363]\n",
      "MSE loss: 85.8379\n",
      "Iteration: 171400\n",
      "Gradient: [ -12.9075    7.6607  -50.5762  -21.9207 -120.6086]\n",
      "Weights: [-4.8623  0.9679 -1.4297  0.1828  0.1364]\n",
      "MSE loss: 85.9039\n",
      "Iteration: 171500\n",
      "Gradient: [-20.2872   1.069  -16.9821 -33.1419 -17.8721]\n",
      "Weights: [-4.8604  0.9598 -1.4286  0.183   0.1365]\n",
      "MSE loss: 86.0341\n",
      "Iteration: 171600\n",
      "Gradient: [  3.7454 -12.8169   2.0628 -63.2288 -97.7239]\n",
      "Weights: [-4.8534  0.9556 -1.4274  0.1841  0.1363]\n",
      "MSE loss: 85.8549\n",
      "Iteration: 171700\n",
      "Gradient: [ -0.9062  -6.6387  11.4834  51.5366 260.0759]\n",
      "Weights: [-4.8597  0.9637 -1.4296  0.1843  0.1363]\n",
      "MSE loss: 85.8458\n",
      "Iteration: 171800\n",
      "Gradient: [  3.4637  13.1933  21.995  104.4345 176.551 ]\n",
      "Weights: [-4.8498  0.9649 -1.4284  0.184   0.1362]\n",
      "MSE loss: 86.0786\n",
      "Iteration: 171900\n",
      "Gradient: [  1.6438  -1.9083 -70.2072  45.4162  43.103 ]\n",
      "Weights: [-4.8536  0.9643 -1.4293  0.1836  0.1362]\n",
      "MSE loss: 85.8213\n",
      "Iteration: 172000\n",
      "Gradient: [   0.5309   -2.1236   -8.4645   13.6887 -323.2421]\n",
      "Weights: [-4.8565  0.9636 -1.4278  0.184   0.1363]\n",
      "MSE loss: 85.9329\n",
      "Iteration: 172100\n",
      "Gradient: [-11.8734 -15.5937  16.5926  46.022  215.9611]\n",
      "Weights: [-4.8617  0.9624 -1.428   0.184   0.1361]\n",
      "MSE loss: 85.8417\n",
      "Iteration: 172200\n",
      "Gradient: [  0.9554   2.422    6.5354  34.9147 143.4536]\n",
      "Weights: [-4.8394  0.962  -1.4297  0.184   0.1361]\n",
      "MSE loss: 86.123\n",
      "Iteration: 172300\n",
      "Gradient: [  3.9207   4.3994  28.02   119.1084 371.6333]\n",
      "Weights: [-4.8738  0.9855 -1.4306  0.1837  0.1362]\n",
      "MSE loss: 86.2504\n",
      "Iteration: 172400\n",
      "Gradient: [-10.8899  13.2914  34.7264 -47.8557  20.6735]\n",
      "Weights: [-4.8647  0.9765 -1.4299  0.1837  0.1359]\n",
      "MSE loss: 85.8103\n",
      "Iteration: 172500\n",
      "Gradient: [ -6.9703  18.9816  48.8438  68.4008 -52.1044]\n",
      "Weights: [-4.8692  0.9821 -1.4316  0.1846  0.1359]\n",
      "MSE loss: 86.0272\n",
      "Iteration: 172600\n",
      "Gradient: [ -14.7064  -21.8504    6.73    -38.4095 -367.1333]\n",
      "Weights: [-4.8717  0.9761 -1.4323  0.184   0.1358]\n",
      "MSE loss: 86.1522\n",
      "Iteration: 172700\n",
      "Gradient: [  3.7165 -11.5819   1.8417  20.8059  82.0436]\n",
      "Weights: [-4.8631  0.9841 -1.4333  0.1839  0.1361]\n",
      "MSE loss: 86.0469\n",
      "Iteration: 172800\n",
      "Gradient: [  5.4212 -11.4947 -15.9569  77.5713 503.0902]\n",
      "Weights: [-4.8643  0.9793 -1.4325  0.1839  0.136 ]\n",
      "MSE loss: 85.8011\n",
      "Iteration: 172900\n",
      "Gradient: [  4.7176 -32.3413  13.9308  41.4454 237.4242]\n",
      "Weights: [-4.8794  0.9944 -1.434   0.184   0.1357]\n",
      "MSE loss: 85.8704\n",
      "Iteration: 173000\n",
      "Gradient: [ -0.4847   8.8592  21.9838  22.512  -49.0528]\n",
      "Weights: [-4.8827  0.9944 -1.4355  0.1836  0.136 ]\n",
      "MSE loss: 85.8847\n",
      "Iteration: 173100\n",
      "Gradient: [  3.3861   7.9036 -18.5287  -1.9603  18.3218]\n",
      "Weights: [-4.8878  1.006  -1.4403  0.184   0.136 ]\n",
      "MSE loss: 85.9399\n",
      "Iteration: 173200\n",
      "Gradient: [ -13.6828   -9.4667   -2.1795 -110.8341 -158.1797]\n",
      "Weights: [-4.8787  0.997  -1.4425  0.1845  0.1361]\n",
      "MSE loss: 86.0871\n",
      "Iteration: 173300\n",
      "Gradient: [ -6.8228  21.4533  50.163   94.0871 527.9573]\n",
      "Weights: [-4.8645  0.9867 -1.4412  0.1849  0.1368]\n",
      "MSE loss: 86.0176\n",
      "Iteration: 173400\n",
      "Gradient: [ 10.597   20.5634  56.1483 -28.693   25.8941]\n",
      "Weights: [-4.8625  0.9874 -1.4408  0.185   0.1366]\n",
      "MSE loss: 85.9734\n",
      "Iteration: 173500\n",
      "Gradient: [  6.9264   6.1236  -9.3618   3.8982 228.0806]\n",
      "Weights: [-4.874   0.9887 -1.4421  0.1848  0.1364]\n",
      "MSE loss: 86.0241\n",
      "Iteration: 173600\n",
      "Gradient: [   1.5595   -3.1441   62.5278   33.4255 -174.5184]\n",
      "Weights: [-4.8742  0.9937 -1.4403  0.1849  0.1361]\n",
      "MSE loss: 85.8097\n",
      "Iteration: 173700\n",
      "Gradient: [ -2.418    2.0083 -26.6356 -86.9699 -86.0003]\n",
      "Weights: [-4.8865  0.9912 -1.4398  0.186   0.136 ]\n",
      "MSE loss: 86.0424\n",
      "Iteration: 173800\n",
      "Gradient: [ -2.3181 -17.554  -72.2491   4.4402  26.828 ]\n",
      "Weights: [-4.8879  0.9926 -1.4398  0.186   0.1358]\n",
      "MSE loss: 86.0819\n",
      "Iteration: 173900\n",
      "Gradient: [ -1.0504   3.4094 -50.4962 -44.6875 145.6988]\n",
      "Weights: [-4.8696  0.99   -1.439   0.1858  0.1356]\n",
      "MSE loss: 85.8005\n",
      "Iteration: 174000\n",
      "Gradient: [  3.7767  11.0044  11.6697   5.7578 244.3587]\n",
      "Weights: [-4.8759  0.9986 -1.4412  0.1863  0.1357]\n",
      "MSE loss: 85.8764\n",
      "Iteration: 174100\n",
      "Gradient: [  -8.4128  -17.0532  -40.8554   41.3401 -122.0291]\n",
      "Weights: [-4.8745  0.9987 -1.4454  0.1865  0.1358]\n",
      "MSE loss: 85.8166\n",
      "Iteration: 174200\n",
      "Gradient: [ -1.4681  -2.6536 -28.1769  22.5556  37.5986]\n",
      "Weights: [-4.8706  1.0001 -1.4449  0.1851  0.1361]\n",
      "MSE loss: 85.8589\n",
      "Iteration: 174300\n",
      "Gradient: [ -8.3234   9.6844 -11.989  -64.2694 210.0025]\n",
      "Weights: [-4.8928  1.0105 -1.4468  0.1853  0.1363]\n",
      "MSE loss: 86.0204\n",
      "Iteration: 174400\n",
      "Gradient: [ -6.2114  -6.6539 -49.592  -94.9932 -63.5096]\n",
      "Weights: [-4.8788  1.0043 -1.4485  0.1856  0.1361]\n",
      "MSE loss: 86.1101\n",
      "Iteration: 174500\n",
      "Gradient: [  -6.4199    8.0968    8.6601 -126.8716  -22.9558]\n",
      "Weights: [-4.8802  0.9976 -1.4456  0.1857  0.1363]\n",
      "MSE loss: 85.9322\n",
      "Iteration: 174600\n",
      "Gradient: [ -5.7275  13.0922  48.6116 -34.3064 107.8927]\n",
      "Weights: [-4.8702  0.9947 -1.4478  0.1868  0.1364]\n",
      "MSE loss: 85.7986\n",
      "Iteration: 174700\n",
      "Gradient: [  -2.545     0.7829   16.1023  -97.4895 -119.6362]\n",
      "Weights: [-4.8654  0.9953 -1.4494  0.1871  0.1364]\n",
      "MSE loss: 85.8108\n",
      "Iteration: 174800\n",
      "Gradient: [  6.529   23.0548  -1.7371 -14.446  341.96  ]\n",
      "Weights: [-4.8628  0.9944 -1.4494  0.1878  0.1363]\n",
      "MSE loss: 85.8959\n",
      "Iteration: 174900\n",
      "Gradient: [ -0.6987  13.2974  41.0201  13.2569 295.3194]\n",
      "Weights: [-4.8734  1.0007 -1.4477  0.1878  0.1361]\n",
      "MSE loss: 86.0281\n",
      "Iteration: 175000\n",
      "Gradient: [  3.9379 -22.8189 -49.8303  84.1362 -88.135 ]\n",
      "Weights: [-4.8644  0.9849 -1.4475  0.1884  0.136 ]\n",
      "MSE loss: 85.8177\n",
      "Iteration: 175100\n",
      "Gradient: [ -2.2939 -12.9603 -10.4781  27.0481 318.822 ]\n",
      "Weights: [-4.8616  0.9853 -1.4463  0.1879  0.1359]\n",
      "MSE loss: 85.8155\n",
      "Iteration: 175200\n",
      "Gradient: [ -2.4532  -0.9707 -95.9545 -10.8059 165.0474]\n",
      "Weights: [-4.8672  0.9862 -1.4452  0.1881  0.1358]\n",
      "MSE loss: 85.7824\n",
      "Iteration: 175300\n",
      "Gradient: [  7.9668   5.8608  18.3633  94.9966 300.4628]\n",
      "Weights: [-4.8605  0.987  -1.4433  0.1879  0.1356]\n",
      "MSE loss: 85.8218\n",
      "Iteration: 175400\n",
      "Gradient: [ -2.2108  -2.8703 -39.0011   2.3952 354.8103]\n",
      "Weights: [-4.8692  0.9865 -1.4439  0.1877  0.1357]\n",
      "MSE loss: 85.7905\n",
      "Iteration: 175500\n",
      "Gradient: [-11.567  -11.263    1.3434 113.7444 -34.6243]\n",
      "Weights: [-4.8758  0.9945 -1.4444  0.1875  0.1357]\n",
      "MSE loss: 85.7764\n",
      "Iteration: 175600\n",
      "Gradient: [  7.6711  -3.2282   6.4463  83.0219 -90.195 ]\n",
      "Weights: [-4.8683  0.9972 -1.4462  0.1874  0.1358]\n",
      "MSE loss: 85.815\n",
      "Iteration: 175700\n",
      "Gradient: [-3.6773 -7.469   0.7154 42.4678 16.844 ]\n",
      "Weights: [-4.8576  0.9928 -1.4481  0.1881  0.1359]\n",
      "MSE loss: 85.9087\n",
      "Iteration: 175800\n",
      "Gradient: [-6.223500e+00  1.188000e-01 -2.557500e+01  7.927990e+01  2.399953e+02]\n",
      "Weights: [-4.882   1.0037 -1.45    0.1882  0.136 ]\n",
      "MSE loss: 85.8385\n",
      "Iteration: 175900\n",
      "Gradient: [ -4.9161   6.9987  -3.0149 -39.2249  32.8753]\n",
      "Weights: [-4.8766  1.011  -1.452   0.1875  0.1359]\n",
      "MSE loss: 85.8269\n",
      "Iteration: 176000\n",
      "Gradient: [   0.9684  -13.4232   -6.612  -108.2649  -94.8149]\n",
      "Weights: [-4.9021  1.0159 -1.4512  0.1879  0.1358]\n",
      "MSE loss: 86.2435\n",
      "Iteration: 176100\n",
      "Gradient: [   4.3686   -9.8346  -14.9747    0.9168 -254.9584]\n",
      "Weights: [-4.892   1.0196 -1.4503  0.1864  0.1357]\n",
      "MSE loss: 85.9935\n",
      "Iteration: 176200\n",
      "Gradient: [-8.7698  4.535  28.5541 21.8881 89.1751]\n",
      "Weights: [-4.8754  1.0143 -1.4483  0.186   0.1357]\n",
      "MSE loss: 85.9998\n",
      "Iteration: 176300\n",
      "Gradient: [  0.1505 -27.1451  29.3323   3.821   30.3108]\n",
      "Weights: [-4.8949  1.0144 -1.4467  0.1858  0.1358]\n",
      "MSE loss: 86.101\n",
      "Iteration: 176400\n",
      "Gradient: [  -6.9505   -3.4809  -23.1899   41.9879 -140.7088]\n",
      "Weights: [-4.8786  1.0027 -1.4476  0.187   0.1358]\n",
      "MSE loss: 85.8291\n",
      "Iteration: 176500\n",
      "Gradient: [  -2.4031  -14.147   -16.0546  -54.9902 -187.868 ]\n",
      "Weights: [-4.8817  0.9994 -1.4468  0.1878  0.1357]\n",
      "MSE loss: 85.8641\n",
      "Iteration: 176600\n",
      "Gradient: [  6.1938 -18.4281   6.6073 -86.0048 -53.9394]\n",
      "Weights: [-4.887   1.0081 -1.4487  0.1874  0.1357]\n",
      "MSE loss: 85.9121\n",
      "Iteration: 176700\n",
      "Gradient: [  0.7484  10.7093 -18.7705   5.8042 185.4971]\n",
      "Weights: [-4.8732  0.9984 -1.4492  0.1887  0.1357]\n",
      "MSE loss: 85.7535\n",
      "Iteration: 176800\n",
      "Gradient: [  0.6404  -7.6305  31.1159 -34.98    75.3195]\n",
      "Weights: [-4.8766  0.9953 -1.449   0.1896  0.1356]\n",
      "MSE loss: 85.7852\n",
      "Iteration: 176900\n",
      "Gradient: [-1.40000e-02 -4.38460e+00 -2.61853e+01  7.23307e+01  2.94241e+01]\n",
      "Weights: [-4.8656  0.9877 -1.4495  0.1895  0.1356]\n",
      "MSE loss: 85.8733\n",
      "Iteration: 177000\n",
      "Gradient: [ 15.607    0.7371  34.108  -39.2792  89.1382]\n",
      "Weights: [-4.8649  0.9938 -1.4493  0.1898  0.1357]\n",
      "MSE loss: 85.8947\n",
      "Iteration: 177100\n",
      "Gradient: [-3.6048 19.4953 19.7343 38.1011 22.3813]\n",
      "Weights: [-4.8687  0.9904 -1.4488  0.1896  0.1357]\n",
      "MSE loss: 85.7545\n",
      "Iteration: 177200\n",
      "Gradient: [  1.4723  -8.3449  48.4914   8.0437 135.9719]\n",
      "Weights: [-4.8608  0.9927 -1.4481  0.1895  0.1357]\n",
      "MSE loss: 86.0787\n",
      "Iteration: 177300\n",
      "Gradient: [  3.5994  -6.0579  -5.0882  70.386  440.2753]\n",
      "Weights: [-4.8639  0.9883 -1.4461  0.1893  0.1353]\n",
      "MSE loss: 85.733\n",
      "Iteration: 177400\n",
      "Gradient: [  6.4191 -14.0949  -4.3623  51.6344 -82.5524]\n",
      "Weights: [-4.857   0.984  -1.4448  0.1891  0.1353]\n",
      "MSE loss: 85.7966\n",
      "Iteration: 177500\n",
      "Gradient: [ 1.680800e+00 -4.270000e-02  3.412070e+01 -8.900230e+01  2.889406e+02]\n",
      "Weights: [-4.8823  0.9953 -1.444   0.1897  0.1349]\n",
      "MSE loss: 85.7934\n",
      "Iteration: 177600\n",
      "Gradient: [ -8.8209  -6.4968  -9.6964 -64.9655  11.8305]\n",
      "Weights: [-4.8785  0.9952 -1.4442  0.1888  0.135 ]\n",
      "MSE loss: 85.8425\n",
      "Iteration: 177700\n",
      "Gradient: [  3.584  -12.7259   9.292   56.4507  25.007 ]\n",
      "Weights: [-4.8783  0.9976 -1.442   0.1886  0.135 ]\n",
      "MSE loss: 85.808\n",
      "Iteration: 177800\n",
      "Gradient: [ 0.907   7.0968 14.6747 10.3054 47.4005]\n",
      "Weights: [-4.8756  0.995  -1.4397  0.1886  0.1348]\n",
      "MSE loss: 85.8827\n",
      "Iteration: 177900\n",
      "Gradient: [ 5.0218 27.6433  2.0225  0.8657 41.2075]\n",
      "Weights: [-4.8692  0.9925 -1.444   0.1889  0.1353]\n",
      "MSE loss: 85.7946\n",
      "Iteration: 178000\n",
      "Gradient: [  -2.3397    0.5037    7.0697  -17.2896 -217.0702]\n",
      "Weights: [-4.8669  0.9786 -1.4398  0.1883  0.1354]\n",
      "MSE loss: 85.7485\n",
      "Iteration: 178100\n",
      "Gradient: [ -0.5875   2.2199  21.814  -11.7729  47.1737]\n",
      "Weights: [-4.8803  0.9813 -1.4389  0.1882  0.1353]\n",
      "MSE loss: 86.0362\n",
      "Iteration: 178200\n",
      "Gradient: [  7.8103  -8.9942  -5.6714 -19.1553 156.7595]\n",
      "Weights: [-4.8831  0.9885 -1.44    0.1881  0.1353]\n",
      "MSE loss: 85.8958\n",
      "Iteration: 178300\n",
      "Gradient: [  2.0636   4.5299  -9.2092 -94.6796 -82.1208]\n",
      "Weights: [-4.8672  0.99   -1.4414  0.188   0.1352]\n",
      "MSE loss: 85.7717\n",
      "Iteration: 178400\n",
      "Gradient: [  -1.6109    7.9105   14.2178   92.0997 -128.0309]\n",
      "Weights: [-4.8819  1.0038 -1.4455  0.1881  0.1352]\n",
      "MSE loss: 85.8065\n",
      "Iteration: 178500\n",
      "Gradient: [ -7.5822  -5.1947   2.4163  69.4525 229.6092]\n",
      "Weights: [-4.8828  1.0017 -1.4453  0.1886  0.1354]\n",
      "MSE loss: 85.8668\n",
      "Iteration: 178600\n",
      "Gradient: [ 1.75    0.9849 -7.1272 49.2114 90.9615]\n",
      "Weights: [-4.8674  0.9815 -1.4425  0.1895  0.1353]\n",
      "MSE loss: 85.7193\n",
      "Iteration: 178700\n",
      "Gradient: [  1.8302   8.0347 -11.0031 -47.3208 -28.5229]\n",
      "Weights: [-4.8676  0.9851 -1.4421  0.1888  0.1352]\n",
      "MSE loss: 85.7187\n",
      "Iteration: 178800\n",
      "Gradient: [  3.7577   4.1495  -5.5693 -17.5169 136.8698]\n",
      "Weights: [-4.8697  0.9832 -1.4426  0.1898  0.135 ]\n",
      "MSE loss: 85.7247\n",
      "Iteration: 178900\n",
      "Gradient: [  4.7432  25.2833 -18.257   12.1109  54.3979]\n",
      "Weights: [-4.8643  0.9818 -1.4416  0.1898  0.135 ]\n",
      "MSE loss: 85.7241\n",
      "Iteration: 179000\n",
      "Gradient: [  2.6217  -4.0711  49.5355 -45.5282 -38.9679]\n",
      "Weights: [-4.8697  0.976  -1.4401  0.1895  0.1351]\n",
      "MSE loss: 85.8021\n",
      "Iteration: 179100\n",
      "Gradient: [  5.465    9.3561 -10.3427 -68.3779 201.3008]\n",
      "Weights: [-4.8646  0.9743 -1.4372  0.1898  0.1349]\n",
      "MSE loss: 85.7626\n",
      "Iteration: 179200\n",
      "Gradient: [  5.8301  -3.6033   7.469   23.129  -33.3791]\n",
      "Weights: [-4.8619  0.9608 -1.436   0.1903  0.1348]\n",
      "MSE loss: 85.9024\n",
      "Iteration: 179300\n",
      "Gradient: [ -10.4962   -5.8905   27.3062   35.8899 -162.1287]\n",
      "Weights: [-4.8635  0.9586 -1.4323  0.1891  0.1349]\n",
      "MSE loss: 85.9368\n",
      "Iteration: 179400\n",
      "Gradient: [ -6.648    3.807    2.9326 129.4346 253.8849]\n",
      "Weights: [-4.8526  0.9549 -1.4315  0.1896  0.1349]\n",
      "MSE loss: 85.7291\n",
      "Iteration: 179500\n",
      "Gradient: [  4.7349  15.9787  23.7351 103.5184 243.6848]\n",
      "Weights: [-4.8549  0.967  -1.4311  0.1885  0.1348]\n",
      "MSE loss: 85.8688\n",
      "Iteration: 179600\n",
      "Gradient: [   8.8136    9.2785   21.0279 -142.7684 -295.2353]\n",
      "Weights: [-4.8442  0.9608 -1.4317  0.1882  0.135 ]\n",
      "MSE loss: 85.95\n",
      "Iteration: 179700\n",
      "Gradient: [  0.5988  -1.8509  -7.3633  22.2754 195.3097]\n",
      "Weights: [-4.8577  0.9649 -1.4326  0.1882  0.1351]\n",
      "MSE loss: 85.7119\n",
      "Iteration: 179800\n",
      "Gradient: [  -1.8498  -11.2427   -9.1812   92.6952 -155.311 ]\n",
      "Weights: [-4.853   0.9623 -1.4324  0.1885  0.135 ]\n",
      "MSE loss: 85.7372\n",
      "Iteration: 179900\n",
      "Gradient: [  -1.9709   -6.913    -2.7668  -12.9755 -391.7825]\n",
      "Weights: [-4.852   0.9547 -1.4306  0.1887  0.1349]\n",
      "MSE loss: 85.7332\n",
      "Iteration: 180000\n",
      "Gradient: [  0.9479 -14.5358  48.0725 -30.0222 -51.125 ]\n",
      "Weights: [-4.8508  0.9586 -1.4298  0.1884  0.1348]\n",
      "MSE loss: 85.7343\n",
      "Iteration: 180100\n",
      "Gradient: [-10.6427   2.9321 -31.0574 -39.0791  51.258 ]\n",
      "Weights: [-4.8606  0.9533 -1.4294  0.1889  0.1349]\n",
      "MSE loss: 85.8555\n",
      "Iteration: 180200\n",
      "Gradient: [-4.1017 -0.2702 18.8126 32.357  38.4712]\n",
      "Weights: [-4.8544  0.9541 -1.4303  0.1891  0.1349]\n",
      "MSE loss: 85.7349\n",
      "Iteration: 180300\n",
      "Gradient: [  -5.8306  -13.9912   -9.8312  -41.8851 -141.2641]\n",
      "Weights: [-4.8686  0.9632 -1.433   0.1885  0.135 ]\n",
      "MSE loss: 86.0804\n",
      "Iteration: 180400\n",
      "Gradient: [   6.8449   10.2983  -14.4603 -172.8263 -115.6772]\n",
      "Weights: [-4.846   0.9617 -1.4367  0.1888  0.135 ]\n",
      "MSE loss: 85.9092\n",
      "Iteration: 180500\n",
      "Gradient: [  1.8384  10.334   23.0997  -9.52   291.4888]\n",
      "Weights: [-4.8444  0.9587 -1.4372  0.1898  0.1353]\n",
      "MSE loss: 85.8427\n",
      "Iteration: 180600\n",
      "Gradient: [ 5.8322 15.7628  2.3192 22.9463 -9.9901]\n",
      "Weights: [-4.868   0.9719 -1.4391  0.1901  0.1353]\n",
      "MSE loss: 85.8565\n",
      "Iteration: 180700\n",
      "Gradient: [ 1.199000e-01 -2.539750e+01 -6.945000e-01  5.241440e+01 -1.927314e+02]\n",
      "Weights: [-4.8684  0.9845 -1.4395  0.1897  0.1347]\n",
      "MSE loss: 85.7349\n",
      "Iteration: 180800\n",
      "Gradient: [   1.9808  -19.7606   45.8495  -16.8259 -120.8153]\n",
      "Weights: [-4.8754  0.9825 -1.4372  0.1901  0.1345]\n",
      "MSE loss: 85.7377\n",
      "Iteration: 180900\n",
      "Gradient: [ -6.4677   2.7498   8.9301   9.2205 136.4234]\n",
      "Weights: [-4.8858  0.9781 -1.4353  0.1902  0.1343]\n",
      "MSE loss: 86.1094\n",
      "Iteration: 181000\n",
      "Gradient: [  -9.0111    4.5291  -35.2353 -107.5361 -137.2086]\n",
      "Weights: [-4.8659  0.9658 -1.4359  0.1909  0.1342]\n",
      "MSE loss: 86.0021\n",
      "Iteration: 181100\n",
      "Gradient: [ -13.8904    0.327   -37.4445  -69.9948 -200.0705]\n",
      "Weights: [-4.8765  0.9638 -1.4345  0.1916  0.1341]\n",
      "MSE loss: 86.2778\n",
      "Iteration: 181200\n",
      "Gradient: [  7.021   20.7183  18.482   16.1224 108.7699]\n",
      "Weights: [-4.8476  0.9557 -1.4324  0.1917  0.1341]\n",
      "MSE loss: 85.738\n",
      "Iteration: 181300\n",
      "Gradient: [ -0.6141  -0.3347 -42.6154 -74.3818 -23.1529]\n",
      "Weights: [-4.8421  0.9493 -1.4323  0.1918  0.1341]\n",
      "MSE loss: 85.7459\n",
      "Iteration: 181400\n",
      "Gradient: [   9.9684   -3.6122  -13.2821   68.9906 -112.4069]\n",
      "Weights: [-4.845   0.9493 -1.4325  0.1921  0.1339]\n",
      "MSE loss: 85.8001\n",
      "Iteration: 181500\n",
      "Gradient: [  4.3184  -4.0829 -24.0649 -42.7205  99.9047]\n",
      "Weights: [-4.8622  0.9514 -1.4299  0.1932  0.1338]\n",
      "MSE loss: 85.7631\n",
      "Iteration: 181600\n",
      "Gradient: [-1.416800e+00 -1.167000e-01  2.373830e+01 -1.478530e+01  1.858202e+02]\n",
      "Weights: [-4.844   0.9563 -1.4328  0.1925  0.1336]\n",
      "MSE loss: 85.772\n",
      "Iteration: 181700\n",
      "Gradient: [  9.3026  11.0084  -4.0617 105.8429  73.7519]\n",
      "Weights: [-4.8608  0.9554 -1.432   0.1935  0.1338]\n",
      "MSE loss: 85.71\n",
      "Iteration: 181800\n",
      "Gradient: [ -2.6823  -8.0674  31.0576 -80.9415  23.8341]\n",
      "Weights: [-4.8537  0.9519 -1.432   0.1937  0.1337]\n",
      "MSE loss: 85.6582\n",
      "Iteration: 181900\n",
      "Gradient: [   3.3589   13.7941  -49.7634   37.2125 -315.3513]\n",
      "Weights: [-4.8522  0.9486 -1.4334  0.1945  0.1335]\n",
      "MSE loss: 85.6625\n",
      "Iteration: 182000\n",
      "Gradient: [ -0.4362   3.2843  27.6175 -63.1797  21.6666]\n",
      "Weights: [-4.8544  0.9488 -1.4306  0.1933  0.1335]\n",
      "MSE loss: 85.6905\n",
      "Iteration: 182100\n",
      "Gradient: [ -4.2207  25.1032  19.478   44.0515 312.1767]\n",
      "Weights: [-4.8467  0.9468 -1.4321  0.1949  0.1336]\n",
      "MSE loss: 85.8891\n",
      "Iteration: 182200\n",
      "Gradient: [  -0.5759  -11.3444   21.027    17.8526 -152.4868]\n",
      "Weights: [-4.8404  0.9418 -1.4307  0.1947  0.1333]\n",
      "MSE loss: 85.6878\n",
      "Iteration: 182300\n",
      "Gradient: [ -2.7567   8.8549 -36.7999 108.697   41.9044]\n",
      "Weights: [-4.8479  0.9406 -1.4281  0.1944  0.1332]\n",
      "MSE loss: 85.6304\n",
      "Iteration: 182400\n",
      "Gradient: [ 13.6965 -16.3857 -19.4018 -46.9717  97.5348]\n",
      "Weights: [-4.8379  0.9328 -1.4263  0.1943  0.1332]\n",
      "MSE loss: 85.6891\n",
      "Iteration: 182500\n",
      "Gradient: [ -9.4904  -7.7967  -6.2421 -80.048   87.0621]\n",
      "Weights: [-4.8374  0.9362 -1.426   0.1938  0.1332]\n",
      "MSE loss: 85.7134\n",
      "Iteration: 182600\n",
      "Gradient: [  3.4055  -3.0335 -19.6771  43.4257 176.7428]\n",
      "Weights: [-4.8431  0.9385 -1.4239  0.1937  0.1332]\n",
      "MSE loss: 85.8268\n",
      "Iteration: 182700\n",
      "Gradient: [-8.3619  5.9667 -1.9493 21.2632 -3.552 ]\n",
      "Weights: [-4.8537  0.9459 -1.4236  0.1935  0.133 ]\n",
      "MSE loss: 85.7335\n",
      "Iteration: 182800\n",
      "Gradient: [ 11.522   -5.4263  24.5745  10.1387 466.4309]\n",
      "Weights: [-4.8391  0.9383 -1.4251  0.1946  0.1328]\n",
      "MSE loss: 85.7355\n",
      "Iteration: 182900\n",
      "Gradient: [  5.009   -1.2521 -12.1721 -36.4729  63.8631]\n",
      "Weights: [-4.8332  0.9298 -1.4233  0.1944  0.1328]\n",
      "MSE loss: 85.7432\n",
      "Iteration: 183000\n",
      "Gradient: [ -8.8925   1.8455  19.9054 -26.0728 -72.2228]\n",
      "Weights: [-4.8406  0.9276 -1.4231  0.1946  0.1329]\n",
      "MSE loss: 85.6871\n",
      "Iteration: 183100\n",
      "Gradient: [ -9.3188 -11.133  -50.5768 -53.8285 152.2942]\n",
      "Weights: [-4.8441  0.9227 -1.4232  0.1948  0.133 ]\n",
      "MSE loss: 85.8896\n",
      "Iteration: 183200\n",
      "Gradient: [ 11.9511   3.532    1.2962 -10.8207 186.0589]\n",
      "Weights: [-4.843   0.9379 -1.4237  0.1947  0.133 ]\n",
      "MSE loss: 85.9545\n",
      "Iteration: 183300\n",
      "Gradient: [ 3.1303 -5.9453 -6.5458 48.9921 52.2055]\n",
      "Weights: [-4.8648  0.9457 -1.426   0.194   0.1331]\n",
      "MSE loss: 85.7804\n",
      "Iteration: 183400\n",
      "Gradient: [ -11.6846   -1.0502    9.2387  -53.8899 -181.6778]\n",
      "Weights: [-4.8598  0.9525 -1.4272  0.1939  0.1328]\n",
      "MSE loss: 85.6237\n",
      "Iteration: 183500\n",
      "Gradient: [  -8.3205   13.5081   -9.1541   25.5091 -231.6415]\n",
      "Weights: [-4.8603  0.9428 -1.4248  0.1937  0.1329]\n",
      "MSE loss: 85.7829\n",
      "Iteration: 183600\n",
      "Gradient: [ -12.3623   -7.6526  -17.0646   -8.416  -170.5487]\n",
      "Weights: [-4.8478  0.9392 -1.4275  0.1953  0.133 ]\n",
      "MSE loss: 85.6342\n",
      "Iteration: 183700\n",
      "Gradient: [  5.4138  -6.1133  18.626   47.4297 129.0563]\n",
      "Weights: [-4.8392  0.927  -1.4245  0.1958  0.1328]\n",
      "MSE loss: 85.6591\n",
      "Iteration: 183800\n",
      "Gradient: [ 9.5343 -7.9924 27.0102 32.8853  8.4222]\n",
      "Weights: [-4.8612  0.9437 -1.4267  0.1964  0.1326]\n",
      "MSE loss: 85.7121\n",
      "Iteration: 183900\n",
      "Gradient: [ 11.3944  -3.1305   7.6544  16.526  -90.1155]\n",
      "Weights: [-4.8421  0.9526 -1.4301  0.1963  0.1324]\n",
      "MSE loss: 86.1845\n",
      "Iteration: 184000\n",
      "Gradient: [ -4.5575  15.4621   8.2952  14.2965 203.0036]\n",
      "Weights: [-4.868   0.9542 -1.4305  0.1965  0.1324]\n",
      "MSE loss: 85.6815\n",
      "Iteration: 184100\n",
      "Gradient: [ 6.0133  1.4652 74.9474 42.8132  2.2843]\n",
      "Weights: [-4.867   0.956  -1.4308  0.1968  0.1323]\n",
      "MSE loss: 85.6403\n",
      "Iteration: 184200\n",
      "Gradient: [  4.4048   1.2028  39.3997  41.1897 -45.4471]\n",
      "Weights: [-4.8438  0.9452 -1.4303  0.1965  0.1324]\n",
      "MSE loss: 85.6391\n",
      "Iteration: 184300\n",
      "Gradient: [   3.0396   -8.2396  -13.8564   48.2241 -194.8106]\n",
      "Weights: [-4.841   0.9361 -1.4283  0.1968  0.1323]\n",
      "MSE loss: 85.6247\n",
      "Iteration: 184400\n",
      "Gradient: [  -0.2872    0.4616  -19.3383   58.8194 -251.2783]\n",
      "Weights: [-4.8404  0.9369 -1.4292  0.1971  0.1323]\n",
      "MSE loss: 85.62\n",
      "Iteration: 184500\n",
      "Gradient: [ -5.2355  -1.9399 -32.0715  67.557   41.6125]\n",
      "Weights: [-4.8649  0.9495 -1.4279  0.197   0.1321]\n",
      "MSE loss: 85.6473\n",
      "Iteration: 184600\n",
      "Gradient: [  -0.5726    7.3585   -7.33     29.7658 -248.0971]\n",
      "Weights: [-4.8477  0.9522 -1.432   0.1972  0.132 ]\n",
      "MSE loss: 85.659\n",
      "Iteration: 184700\n",
      "Gradient: [  0.6925  15.1326   2.0297   3.2428 184.2878]\n",
      "Weights: [-4.856   0.9522 -1.4306  0.1973  0.1322]\n",
      "MSE loss: 85.6781\n",
      "Iteration: 184800\n",
      "Gradient: [ -2.5058  -2.3338  12.499   -7.0677 -95.6721]\n",
      "Weights: [-4.8556  0.9494 -1.4316  0.198   0.1319]\n",
      "MSE loss: 85.5899\n",
      "Iteration: 184900\n",
      "Gradient: [-10.2713  -5.8031  20.4787  49.6844 144.1569]\n",
      "Weights: [-4.8642  0.9581 -1.4357  0.1984  0.1322]\n",
      "MSE loss: 85.583\n",
      "Iteration: 185000\n",
      "Gradient: [  4.5709  16.2806 -25.7346  81.6836  86.9516]\n",
      "Weights: [-4.8719  0.9613 -1.4339  0.1986  0.1319]\n",
      "MSE loss: 85.676\n",
      "Iteration: 185100\n",
      "Gradient: [  3.3394  -6.5627 -22.2864  65.3404  83.4154]\n",
      "Weights: [-4.8651  0.9561 -1.4338  0.1991  0.1316]\n",
      "MSE loss: 85.617\n",
      "Iteration: 185200\n",
      "Gradient: [   6.4527   14.5718    3.1033   24.612  -150.9006]\n",
      "Weights: [-4.8675  0.9544 -1.4338  0.2004  0.1314]\n",
      "MSE loss: 85.6144\n",
      "Iteration: 185300\n",
      "Gradient: [ -7.5537  -6.6541  27.2522 -46.6708 -12.5737]\n",
      "Weights: [-4.8607  0.9483 -1.4366  0.2012  0.1314]\n",
      "MSE loss: 85.7838\n",
      "Iteration: 185400\n",
      "Gradient: [  3.581    2.0215 -38.1512  53.0991 322.9768]\n",
      "Weights: [-4.8484  0.9525 -1.4372  0.2007  0.1315]\n",
      "MSE loss: 85.5655\n",
      "Iteration: 185500\n",
      "Gradient: [ 7.2658 -3.8226 13.9659 44.1111  6.0297]\n",
      "Weights: [-4.8572  0.9634 -1.4416  0.2013  0.1316]\n",
      "MSE loss: 85.5805\n",
      "Iteration: 185600\n",
      "Gradient: [  3.1103  11.309  -22.8889  92.4899  65.2704]\n",
      "Weights: [-4.8568  0.9588 -1.4417  0.2008  0.1317]\n",
      "MSE loss: 85.5758\n",
      "Iteration: 185700\n",
      "Gradient: [  8.3811  10.859  -36.8149  79.0368 121.2997]\n",
      "Weights: [-4.8563  0.9602 -1.4389  0.2007  0.1317]\n",
      "MSE loss: 85.6373\n",
      "Iteration: 185800\n",
      "Gradient: [  4.159  -11.7607 -30.9198  17.535  -29.1702]\n",
      "Weights: [-4.8577  0.9492 -1.4391  0.201   0.1319]\n",
      "MSE loss: 85.6486\n",
      "Iteration: 185900\n",
      "Gradient: [ -2.8713  -8.5154 -28.2712   7.5578 104.89  ]\n",
      "Weights: [-4.8439  0.9482 -1.4384  0.2004  0.1317]\n",
      "MSE loss: 85.6217\n",
      "Iteration: 186000\n",
      "Gradient: [ -5.0394 -19.8868 -17.0943 -37.5415  18.5345]\n",
      "Weights: [-4.8597  0.9539 -1.4395  0.2011  0.1316]\n",
      "MSE loss: 85.5878\n",
      "Iteration: 186100\n",
      "Gradient: [  1.0123  -1.1779 -18.2394 103.4522  69.2792]\n",
      "Weights: [-4.845   0.9606 -1.4418  0.2008  0.1316]\n",
      "MSE loss: 85.704\n",
      "Iteration: 186200\n",
      "Gradient: [ -2.2537  16.0053   2.6273 160.1887 -35.7697]\n",
      "Weights: [-4.8548  0.9511 -1.4412  0.2022  0.1318]\n",
      "MSE loss: 85.5379\n",
      "Iteration: 186300\n",
      "Gradient: [ -1.1883  -2.2659   6.6801  97.898  155.4097]\n",
      "Weights: [-4.8429  0.9534 -1.4406  0.2019  0.1317]\n",
      "MSE loss: 85.861\n",
      "Iteration: 186400\n",
      "Gradient: [  10.6465  -11.7528  -40.3816    7.5812 -152.3167]\n",
      "Weights: [-4.8554  0.9511 -1.4409  0.2017  0.1318]\n",
      "MSE loss: 85.5419\n",
      "Iteration: 186500\n",
      "Gradient: [   8.0965   -4.3061   18.9355   65.7144 -438.7671]\n",
      "Weights: [-4.8517  0.9486 -1.4413  0.2019  0.1319]\n",
      "MSE loss: 85.5334\n",
      "Iteration: 186600\n",
      "Gradient: [-5.9812 -0.0937 50.7653 66.5593  1.0663]\n",
      "Weights: [-4.8539  0.9564 -1.4404  0.2012  0.132 ]\n",
      "MSE loss: 85.6531\n",
      "Iteration: 186700\n",
      "Gradient: [ -6.1018   2.2719 -19.6034 106.5312 176.1323]\n",
      "Weights: [-4.8584  0.9628 -1.442   0.2007  0.1317]\n",
      "MSE loss: 85.5378\n",
      "Iteration: 186800\n",
      "Gradient: [-4.5132  6.6189 24.2602 52.8944 48.2844]\n",
      "Weights: [-4.8497  0.959  -1.4407  0.2001  0.1318]\n",
      "MSE loss: 85.584\n",
      "Iteration: 186900\n",
      "Gradient: [  1.6378  -0.6109  25.8585  97.2696 -42.1194]\n",
      "Weights: [-4.8577  0.9648 -1.4432  0.2005  0.132 ]\n",
      "MSE loss: 85.5386\n",
      "Iteration: 187000\n",
      "Gradient: [  11.0312  -11.5259  -30.9153  116.3528 -167.3345]\n",
      "Weights: [-4.8571  0.9663 -1.4421  0.2001  0.1321]\n",
      "MSE loss: 85.6252\n",
      "Iteration: 187100\n",
      "Gradient: [-15.2573  -6.4159 -13.8289  42.8722  55.4576]\n",
      "Weights: [-4.868   0.9655 -1.4433  0.1994  0.1322]\n",
      "MSE loss: 85.8766\n",
      "Iteration: 187200\n",
      "Gradient: [  3.2675   2.5651  23.2923 -61.969   64.6401]\n",
      "Weights: [-4.8628  0.9644 -1.44    0.1993  0.1321]\n",
      "MSE loss: 85.5403\n",
      "Iteration: 187300\n",
      "Gradient: [-11.3064   1.5445 -39.5861 -40.4234 -21.7293]\n",
      "Weights: [-4.8803  0.965  -1.4376  0.1987  0.1322]\n",
      "MSE loss: 85.9231\n",
      "Iteration: 187400\n",
      "Gradient: [ -0.7002  13.5422   2.4396  13.1685 -57.3781]\n",
      "Weights: [-4.8783  0.965  -1.4386  0.1992  0.1321]\n",
      "MSE loss: 85.8503\n",
      "Iteration: 187500\n",
      "Gradient: [ -1.4653 -15.5996 -11.8402  25.1278 181.5035]\n",
      "Weights: [-4.8776  0.975  -1.4393  0.1988  0.1318]\n",
      "MSE loss: 85.6781\n",
      "Iteration: 187600\n",
      "Gradient: [-6.349   2.5755  0.442  43.6748 95.2495]\n",
      "Weights: [-4.8744  0.971  -1.4403  0.2     0.1318]\n",
      "MSE loss: 85.6417\n",
      "Iteration: 187700\n",
      "Gradient: [  4.836   -8.8348   9.2299 -88.8819  58.3359]\n",
      "Weights: [-4.8701  0.975  -1.4438  0.2005  0.1318]\n",
      "MSE loss: 85.5823\n",
      "Iteration: 187800\n",
      "Gradient: [ 0.2579 15.3475 40.3238 27.5483 -5.7565]\n",
      "Weights: [-4.8631  0.9726 -1.4433  0.2006  0.1318]\n",
      "MSE loss: 85.6304\n",
      "Iteration: 187900\n",
      "Gradient: [ -0.6172  -6.7856  -1.4792 -56.5164 -24.0142]\n",
      "Weights: [-4.8695  0.9714 -1.4453  0.2008  0.132 ]\n",
      "MSE loss: 85.5596\n",
      "Iteration: 188000\n",
      "Gradient: [ -1.522    2.4588   2.6913  -5.1442 124.7748]\n",
      "Weights: [-4.8645  0.969  -1.4465  0.2008  0.132 ]\n",
      "MSE loss: 85.601\n",
      "Iteration: 188100\n",
      "Gradient: [  2.3242  -7.1103  12.352  -51.4844 -16.019 ]\n",
      "Weights: [-4.8628  0.9739 -1.4477  0.2013  0.132 ]\n",
      "MSE loss: 85.5317\n",
      "Iteration: 188200\n",
      "Gradient: [ -7.8427  -6.5004  10.9227  16.3289 137.7742]\n",
      "Weights: [-4.8741  0.9756 -1.4476  0.2022  0.1318]\n",
      "MSE loss: 85.609\n",
      "Iteration: 188300\n",
      "Gradient: [ -3.6665  -1.6008 -55.2389 -14.1745 221.6295]\n",
      "Weights: [-4.8718  0.9763 -1.4482  0.2023  0.1316]\n",
      "MSE loss: 85.5525\n",
      "Iteration: 188400\n",
      "Gradient: [ -7.1482 -16.1517   8.2761  16.2415  88.2351]\n",
      "Weights: [-4.8634  0.9687 -1.4467  0.2025  0.1315]\n",
      "MSE loss: 85.4982\n",
      "Iteration: 188500\n",
      "Gradient: [ -1.1876   8.3546  -3.4653 -52.843  123.0567]\n",
      "Weights: [-4.8578  0.9665 -1.4477  0.2026  0.1315]\n",
      "MSE loss: 85.5435\n",
      "Iteration: 188600\n",
      "Gradient: [  8.3099   5.8197   6.4235  76.5351 -82.3965]\n",
      "Weights: [-4.8416  0.9625 -1.4494  0.2027  0.1317]\n",
      "MSE loss: 85.6668\n",
      "Iteration: 188700\n",
      "Gradient: [ -5.0536   5.0215 -19.7215 -11.0756  32.4109]\n",
      "Weights: [-4.8622  0.97   -1.4501  0.2033  0.1319]\n",
      "MSE loss: 85.5798\n",
      "Iteration: 188800\n",
      "Gradient: [   2.8629  -17.9676   -8.9772 -100.4115  131.8161]\n",
      "Weights: [-4.8708  0.9723 -1.4493  0.2027  0.132 ]\n",
      "MSE loss: 85.6091\n",
      "Iteration: 188900\n",
      "Gradient: [  -5.7741    6.1039    5.0945   91.8479 -415.4112]\n",
      "Weights: [-4.855   0.9711 -1.4495  0.2022  0.1319]\n",
      "MSE loss: 85.5584\n",
      "Iteration: 189000\n",
      "Gradient: [ -3.2496 -23.2672 -11.0121 -62.3896 113.1631]\n",
      "Weights: [-4.877   0.9821 -1.4504  0.2023  0.1318]\n",
      "MSE loss: 85.6022\n",
      "Iteration: 189100\n",
      "Gradient: [-6.662100e+00  8.253400e+00 -2.054000e-01  3.236600e+01 -2.467278e+02]\n",
      "Weights: [-4.8695  0.9677 -1.4512  0.2038  0.1319]\n",
      "MSE loss: 85.6579\n",
      "Iteration: 189200\n",
      "Gradient: [  1.3419  -4.3172 -18.5601  36.1605  70.5615]\n",
      "Weights: [-4.8492  0.9634 -1.4542  0.2038  0.1321]\n",
      "MSE loss: 85.5178\n",
      "Iteration: 189300\n",
      "Gradient: [  -7.1611  -16.7489   -0.5551  -81.301  -167.508 ]\n",
      "Weights: [-4.8622  0.9625 -1.454   0.204   0.132 ]\n",
      "MSE loss: 85.9106\n",
      "Iteration: 189400\n",
      "Gradient: [ -4.1431  10.9985 -19.2083 -53.1378 -72.2875]\n",
      "Weights: [-4.862   0.9746 -1.4571  0.2048  0.1318]\n",
      "MSE loss: 85.4891\n",
      "Iteration: 189500\n",
      "Gradient: [ -12.771    11.209     2.6905 -128.7427   69.4727]\n",
      "Weights: [-4.8612  0.9652 -1.4568  0.2055  0.1318]\n",
      "MSE loss: 85.7442\n",
      "Iteration: 189600\n",
      "Gradient: [ 21.073  -15.3633  13.9534  42.3174 266.3109]\n",
      "Weights: [-4.8618  0.9795 -1.458   0.2058  0.1314]\n",
      "MSE loss: 85.4445\n",
      "Iteration: 189700\n",
      "Gradient: [-14.8463   9.1371 -12.5479  13.6929 180.2372]\n",
      "Weights: [-4.8835  0.9955 -1.4574  0.2054  0.1311]\n",
      "MSE loss: 85.6251\n",
      "Iteration: 189800\n",
      "Gradient: [   1.832    -8.9132  -12.3476  -43.5607 -109.0574]\n",
      "Weights: [-4.8515  0.9768 -1.4596  0.2056  0.1311]\n",
      "MSE loss: 85.784\n",
      "Iteration: 189900\n",
      "Gradient: [  1.7142   3.837   17.7683  24.6862 -27.52  ]\n",
      "Weights: [-4.8489  0.9669 -1.4584  0.207   0.1314]\n",
      "MSE loss: 85.4699\n",
      "Iteration: 190000\n",
      "Gradient: [   1.1723    1.4927   12.5221   74.7373 -175.8861]\n",
      "Weights: [-4.8651  0.9727 -1.4566  0.207   0.1313]\n",
      "MSE loss: 85.5113\n",
      "Iteration: 190100\n",
      "Gradient: [ -0.5849 -13.6051   9.0094  19.4948 -48.7766]\n",
      "Weights: [-4.8589  0.979  -1.4582  0.2062  0.1313]\n",
      "MSE loss: 85.4636\n",
      "Iteration: 190200\n",
      "Gradient: [ -4.0241  -2.8996  13.7941  45.1429 276.7811]\n",
      "Weights: [-4.8677  0.9818 -1.4576  0.2065  0.131 ]\n",
      "MSE loss: 85.4478\n",
      "Iteration: 190300\n",
      "Gradient: [-10.7244   6.3924   4.5213  84.7041 -30.7953]\n",
      "Weights: [-4.854   0.9779 -1.46    0.2065  0.1312]\n",
      "MSE loss: 85.4852\n",
      "Iteration: 190400\n",
      "Gradient: [  11.6587  -25.721   -51.4961   94.3112 -119.7621]\n",
      "Weights: [-4.8573  0.9733 -1.4577  0.2059  0.1313]\n",
      "MSE loss: 85.489\n",
      "Iteration: 190500\n",
      "Gradient: [  4.5583   6.4714 -21.3562 100.5487  81.8041]\n",
      "Weights: [-4.8578  0.9776 -1.4607  0.2064  0.1315]\n",
      "MSE loss: 85.439\n",
      "Iteration: 190600\n",
      "Gradient: [ -10.9983    9.4052   19.1795  -26.1339 -118.5051]\n",
      "Weights: [-4.8693  0.9757 -1.4591  0.2068  0.1314]\n",
      "MSE loss: 85.5783\n",
      "Iteration: 190700\n",
      "Gradient: [  2.4704   0.9605 -23.5678  10.0462 234.1409]\n",
      "Weights: [-4.8467  0.9747 -1.463   0.2073  0.1314]\n",
      "MSE loss: 85.5306\n",
      "Iteration: 190800\n",
      "Gradient: [  1.8742 -10.3131 -38.1761 -69.9052 -71.3407]\n",
      "Weights: [-4.8556  0.9635 -1.4609  0.2085  0.1313]\n",
      "MSE loss: 85.6173\n",
      "Iteration: 190900\n",
      "Gradient: [  5.5763 -14.9206  15.2377  26.6414  70.5304]\n",
      "Weights: [-4.8408  0.9695 -1.4638  0.2082  0.1313]\n",
      "MSE loss: 85.5775\n",
      "Iteration: 191000\n",
      "Gradient: [ -7.2904  -7.2002  21.379    0.5831 -91.7242]\n",
      "Weights: [-4.8514  0.9701 -1.4646  0.2084  0.1315]\n",
      "MSE loss: 85.4891\n",
      "Iteration: 191100\n",
      "Gradient: [  -8.0464  -11.0233  -13.1242   -0.4183 -183.766 ]\n",
      "Weights: [-4.8656  0.9833 -1.4629  0.2071  0.1314]\n",
      "MSE loss: 85.4345\n",
      "Iteration: 191200\n",
      "Gradient: [  0.3575   6.5708 -13.5878  59.1117 203.63  ]\n",
      "Weights: [-4.86    0.9939 -1.4645  0.2064  0.1315]\n",
      "MSE loss: 85.7377\n",
      "Iteration: 191300\n",
      "Gradient: [ -1.0569  -0.609    0.3002 -38.5785   6.6772]\n",
      "Weights: [-4.8656  0.9893 -1.4645  0.2066  0.1315]\n",
      "MSE loss: 85.4398\n",
      "Iteration: 191400\n",
      "Gradient: [  1.6993  -1.9456 -10.1614  -3.8807  36.7346]\n",
      "Weights: [-4.8625  0.9829 -1.4669  0.2072  0.1316]\n",
      "MSE loss: 85.6147\n",
      "Iteration: 191500\n",
      "Gradient: [  -2.3368   33.5038   18.588   -89.8782 -130.4493]\n",
      "Weights: [-4.8583  0.9863 -1.4665  0.2077  0.1315]\n",
      "MSE loss: 85.4765\n",
      "Iteration: 191600\n",
      "Gradient: [ -6.7627  -2.0828 -15.071  -68.1988  -6.8813]\n",
      "Weights: [-4.8641  0.9889 -1.4674  0.2078  0.1313]\n",
      "MSE loss: 85.4608\n",
      "Iteration: 191700\n",
      "Gradient: [  2.7926   4.3318  -4.1937  14.5686 156.778 ]\n",
      "Weights: [-4.8604  0.9844 -1.467   0.2088  0.1313]\n",
      "MSE loss: 85.4237\n",
      "Iteration: 191800\n",
      "Gradient: [  3.8334  -7.8514 -21.9391  -9.9205 -95.1626]\n",
      "Weights: [-4.8555  0.9873 -1.4685  0.2086  0.1312]\n",
      "MSE loss: 85.4633\n",
      "Iteration: 191900\n",
      "Gradient: [  6.0536  10.9847   4.2338  62.3032 -70.0181]\n",
      "Weights: [-4.8458  0.9848 -1.4645  0.2076  0.1311]\n",
      "MSE loss: 85.8362\n",
      "Iteration: 192000\n",
      "Gradient: [  1.2187   3.0031 -19.3802  50.0296 -29.4978]\n",
      "Weights: [-4.86    0.9794 -1.4671  0.2082  0.1314]\n",
      "MSE loss: 85.582\n",
      "Iteration: 192100\n",
      "Gradient: [ -1.7005 -12.2305 -21.1159  25.2541 253.6711]\n",
      "Weights: [-4.8616  0.9777 -1.4668  0.2086  0.1314]\n",
      "MSE loss: 85.5964\n",
      "Iteration: 192200\n",
      "Gradient: [ 4.9474  6.8543 61.3888  2.9149 89.7079]\n",
      "Weights: [-4.8494  0.9797 -1.4665  0.2086  0.1315]\n",
      "MSE loss: 85.5906\n",
      "Iteration: 192300\n",
      "Gradient: [ -6.7774  12.0033 -23.9029 -83.2718  46.3852]\n",
      "Weights: [-4.8635  0.9795 -1.4662  0.2094  0.1314]\n",
      "MSE loss: 85.489\n",
      "Iteration: 192400\n",
      "Gradient: [  5.2717 -16.0274  12.319   85.9781  17.0411]\n",
      "Weights: [-4.8487  0.9699 -1.4632  0.2092  0.1312]\n",
      "MSE loss: 85.4648\n",
      "Iteration: 192500\n",
      "Gradient: [-11.3132 -22.413  -68.586   16.5121 134.6217]\n",
      "Weights: [-4.8711  0.976  -1.4625  0.2084  0.1312]\n",
      "MSE loss: 85.6759\n",
      "Iteration: 192600\n",
      "Gradient: [ 15.5966   8.3449  35.0757 147.9987  48.6085]\n",
      "Weights: [-4.8584  0.9855 -1.4628  0.2081  0.1311]\n",
      "MSE loss: 85.6484\n",
      "Iteration: 192700\n",
      "Gradient: [ -1.7531  10.3573 -35.0727  21.2699 195.1388]\n",
      "Weights: [-4.8547  0.9766 -1.4637  0.2085  0.1312]\n",
      "MSE loss: 85.4185\n",
      "Iteration: 192800\n",
      "Gradient: [ 15.9866  20.6423  19.437    8.3398 242.3482]\n",
      "Weights: [-4.8446  0.9708 -1.4611  0.2089  0.1311]\n",
      "MSE loss: 85.7543\n",
      "Iteration: 192900\n",
      "Gradient: [ -6.9099  -5.3428  34.9698  18.5378 -86.363 ]\n",
      "Weights: [-4.8501  0.9714 -1.4625  0.209   0.131 ]\n",
      "MSE loss: 85.4325\n",
      "Iteration: 193000\n",
      "Gradient: [ -5.9961 -26.6111  -6.6799   0.0875 -17.1976]\n",
      "Weights: [-4.8677  0.9761 -1.4593  0.2077  0.131 ]\n",
      "MSE loss: 85.5018\n",
      "Iteration: 193100\n",
      "Gradient: [  9.3627   1.2599  11.2136 -31.8581  78.5441]\n",
      "Weights: [-4.8568  0.9694 -1.4561  0.2078  0.1309]\n",
      "MSE loss: 85.4223\n",
      "Iteration: 193200\n",
      "Gradient: [-0.8603 -7.5211  5.5103 87.2667 73.9295]\n",
      "Weights: [-4.848   0.963  -1.4528  0.2071  0.1309]\n",
      "MSE loss: 85.5363\n",
      "Iteration: 193300\n",
      "Gradient: [   3.3646   -0.3444   -3.2177  104.5533 -204.5037]\n",
      "Weights: [-4.8583  0.965  -1.4529  0.207   0.1308]\n",
      "MSE loss: 85.4413\n",
      "Iteration: 193400\n",
      "Gradient: [ -2.1752   3.8534   9.2163 -60.724  -39.6723]\n",
      "Weights: [-4.8563  0.9622 -1.4548  0.2079  0.1307]\n",
      "MSE loss: 85.4999\n",
      "Iteration: 193500\n",
      "Gradient: [  9.7273  -0.3621 -12.8474   7.3273 -91.7228]\n",
      "Weights: [-4.8426  0.9525 -1.4525  0.2075  0.1307]\n",
      "MSE loss: 85.562\n",
      "Iteration: 193600\n",
      "Gradient: [   3.7126   -1.8328   47.5091  -31.4016 -193.0173]\n",
      "Weights: [-4.8438  0.9623 -1.4514  0.2069  0.1307]\n",
      "MSE loss: 85.6273\n",
      "Iteration: 193700\n",
      "Gradient: [-10.4708  -3.7817 -17.3813  75.896  205.1706]\n",
      "Weights: [-4.8473  0.9494 -1.4501  0.2082  0.1308]\n",
      "MSE loss: 85.4615\n",
      "Iteration: 193800\n",
      "Gradient: [-10.3618   6.6312  21.1596  -6.3612  98.9541]\n",
      "Weights: [-4.8571  0.9616 -1.4515  0.2079  0.1306]\n",
      "MSE loss: 85.4219\n",
      "Iteration: 193900\n",
      "Gradient: [ -2.1451 -24.0341  46.3669  29.8734 217.2627]\n",
      "Weights: [-4.857   0.952  -1.4489  0.2082  0.1304]\n",
      "MSE loss: 85.5103\n",
      "Iteration: 194000\n",
      "Gradient: [  3.9586   9.5239 -25.2828 -12.5718  41.3642]\n",
      "Weights: [-4.8473  0.9507 -1.4507  0.2082  0.1305]\n",
      "MSE loss: 85.4691\n",
      "Iteration: 194100\n",
      "Gradient: [  -0.2805  -11.5628  -38.2234  -63.6011 -221.4544]\n",
      "Weights: [-4.8453  0.9502 -1.4491  0.2079  0.1303]\n",
      "MSE loss: 85.4906\n",
      "Iteration: 194200\n",
      "Gradient: [ 13.9219  14.571   30.4497 -28.3234 190.606 ]\n",
      "Weights: [-4.8511  0.9657 -1.4516  0.208   0.1305]\n",
      "MSE loss: 85.7769\n",
      "Iteration: 194300\n",
      "Gradient: [ -0.4593  -2.2292 -15.997   12.3307 -12.0529]\n",
      "Weights: [-4.8589  0.9564 -1.4484  0.208   0.1306]\n",
      "MSE loss: 85.5087\n",
      "Iteration: 194400\n",
      "Gradient: [  7.7884  15.2311   5.9169 -19.2283 275.3448]\n",
      "Weights: [-4.8492  0.958  -1.4508  0.2082  0.1304]\n",
      "MSE loss: 85.4367\n",
      "Iteration: 194500\n",
      "Gradient: [  -9.7145   -3.724   -40.7978  -33.5544 -138.8751]\n",
      "Weights: [-4.8649  0.9626 -1.4498  0.2087  0.13  ]\n",
      "MSE loss: 85.4787\n",
      "Iteration: 194600\n",
      "Gradient: [-1.45200e-01  1.30530e+01 -7.99800e-01 -1.64797e+01 -2.17461e+02]\n",
      "Weights: [-4.8603  0.9677 -1.4488  0.2081  0.1298]\n",
      "MSE loss: 85.479\n",
      "Iteration: 194700\n",
      "Gradient: [ -1.4696  -6.4202 -26.95    -6.9827 109.8744]\n",
      "Weights: [-4.8582  0.9693 -1.45    0.2082  0.1298]\n",
      "MSE loss: 85.5039\n",
      "Iteration: 194800\n",
      "Gradient: [  -8.9679    0.9369  -16.2163   15.8819 -312.4038]\n",
      "Weights: [-4.8618  0.9703 -1.4518  0.2086  0.1298]\n",
      "MSE loss: 85.4573\n",
      "Iteration: 194900\n",
      "Gradient: [ 12.7126  11.1622  -2.154   32.3034 -35.3917]\n",
      "Weights: [-4.8496  0.9669 -1.4509  0.2079  0.13  ]\n",
      "MSE loss: 85.5913\n",
      "Iteration: 195000\n",
      "Gradient: [   2.4403   22.5495  -31.5826   31.9901 -186.983 ]\n",
      "Weights: [-4.8649  0.9716 -1.4533  0.2086  0.1302]\n",
      "MSE loss: 85.4269\n",
      "Iteration: 195100\n",
      "Gradient: [   1.4215    8.4007   13.6111 -117.1746  155.9728]\n",
      "Weights: [-4.8544  0.9658 -1.4529  0.2087  0.1301]\n",
      "MSE loss: 85.4283\n",
      "Iteration: 195200\n",
      "Gradient: [ 2.574  -1.9354 29.7864 13.831  17.1787]\n",
      "Weights: [-4.8591  0.97   -1.4555  0.2099  0.1301]\n",
      "MSE loss: 85.4675\n",
      "Iteration: 195300\n",
      "Gradient: [  1.1722   7.908  -32.0152  55.3712  48.5319]\n",
      "Weights: [-4.8518  0.9683 -1.4564  0.2098  0.1301]\n",
      "MSE loss: 85.4519\n",
      "Iteration: 195400\n",
      "Gradient: [-11.9776  -5.8765   2.8265 -36.1318 285.8603]\n",
      "Weights: [-4.8846  0.9774 -1.4557  0.2098  0.1301]\n",
      "MSE loss: 85.7941\n",
      "Iteration: 195500\n",
      "Gradient: [   1.0993   -2.7039    9.257   -15.2698 -204.3433]\n",
      "Weights: [-4.8591  0.964  -1.4544  0.2092  0.1301]\n",
      "MSE loss: 85.4711\n",
      "Iteration: 195600\n",
      "Gradient: [ -2.3446  14.697  -20.2455  87.9619 128.6219]\n",
      "Weights: [-4.8635  0.9541 -1.4528  0.2098  0.1305]\n",
      "MSE loss: 85.6613\n",
      "Iteration: 195700\n",
      "Gradient: [  -7.537    18.9943    0.4548  -53.184  -105.3176]\n",
      "Weights: [-4.8499  0.9525 -1.4529  0.2094  0.1302]\n",
      "MSE loss: 85.5855\n",
      "Iteration: 195800\n",
      "Gradient: [ 5.32000e-02  3.27052e+01  7.60960e+00 -2.38272e+01 -6.83549e+01]\n",
      "Weights: [-4.8486  0.958  -1.4525  0.2089  0.1303]\n",
      "MSE loss: 85.4195\n",
      "Iteration: 195900\n",
      "Gradient: [ -6.0113  -5.5271 -25.2939  52.0215 163.0802]\n",
      "Weights: [-4.8554  0.9641 -1.452   0.209   0.1303]\n",
      "MSE loss: 85.5384\n",
      "Iteration: 196000\n",
      "Gradient: [  3.6974   3.3603 -24.3938 -10.8172  30.8146]\n",
      "Weights: [-4.8479  0.9546 -1.452   0.2091  0.1304]\n",
      "MSE loss: 85.4209\n",
      "Iteration: 196100\n",
      "Gradient: [   1.5368   11.2033   -6.0673 -162.1636 -147.0326]\n",
      "Weights: [-4.8518  0.9607 -1.4532  0.2094  0.1303]\n",
      "MSE loss: 85.4345\n",
      "Iteration: 196200\n",
      "Gradient: [  -3.6475  -10.682    -0.4504  -76.7342 -112.5787]\n",
      "Weights: [-4.8588  0.9604 -1.4523  0.2096  0.1302]\n",
      "MSE loss: 85.4115\n",
      "Iteration: 196300\n",
      "Gradient: [ -6.0388  -3.7044   8.5686 -42.0397 -67.1662]\n",
      "Weights: [-4.8594  0.9621 -1.4557  0.2104  0.1302]\n",
      "MSE loss: 85.4172\n",
      "Iteration: 196400\n",
      "Gradient: [ -8.5915 -11.173  -16.6729 -72.9438  64.635 ]\n",
      "Weights: [-4.8674  0.977  -1.459   0.2101  0.1302]\n",
      "MSE loss: 85.4161\n",
      "Iteration: 196500\n",
      "Gradient: [   0.9973   -9.0326   20.9917 -140.7274  -22.9883]\n",
      "Weights: [-4.8726  0.9753 -1.4574  0.2095  0.1301]\n",
      "MSE loss: 85.5855\n",
      "Iteration: 196600\n",
      "Gradient: [  5.2844   5.7947  27.3478  56.2128 -35.2879]\n",
      "Weights: [-4.857   0.9743 -1.4592  0.2104  0.1301]\n",
      "MSE loss: 85.4236\n",
      "Iteration: 196700\n",
      "Gradient: [ 12.3147  -0.2234   0.9206  -0.7493 170.2946]\n",
      "Weights: [-4.8527  0.9778 -1.4632  0.2105  0.13  ]\n",
      "MSE loss: 85.4982\n",
      "Iteration: 196800\n",
      "Gradient: [  -4.2166   -9.5973   19.3254  -20.8565 -118.8422]\n",
      "Weights: [-4.8654  0.9735 -1.4609  0.2112  0.1301]\n",
      "MSE loss: 85.4123\n",
      "Iteration: 196900\n",
      "Gradient: [ -8.1293  -7.1227 -37.1071 -65.476  190.0308]\n",
      "Weights: [-4.8594  0.9706 -1.462   0.2108  0.1303]\n",
      "MSE loss: 85.4092\n",
      "Iteration: 197000\n",
      "Gradient: [ -1.1986  -5.8913  12.294    7.336  -59.143 ]\n",
      "Weights: [-4.8455  0.9697 -1.4619  0.2109  0.1305]\n",
      "MSE loss: 85.6319\n",
      "Iteration: 197100\n",
      "Gradient: [-1.45000e-02  6.25130e+00 -1.33276e+01  8.31789e+01  1.52849e+01]\n",
      "Weights: [-4.8425  0.9607 -1.4625  0.2108  0.1307]\n",
      "MSE loss: 85.4587\n",
      "Iteration: 197200\n",
      "Gradient: [  -7.48      1.3361  -34.342    52.8905 -525.5177]\n",
      "Weights: [-4.8569  0.9583 -1.4603  0.2099  0.1308]\n",
      "MSE loss: 85.8573\n",
      "Iteration: 197300\n",
      "Gradient: [   4.8396   12.3268   14.6817  -28.3898 -114.33  ]\n",
      "Weights: [-4.8585  0.9769 -1.4597  0.2091  0.1309]\n",
      "MSE loss: 85.8237\n",
      "Iteration: 197400\n",
      "Gradient: [  -5.698     0.8524  -24.593  -101.0461 -130.3014]\n",
      "Weights: [-4.8749  0.9795 -1.4642  0.209   0.131 ]\n",
      "MSE loss: 85.7959\n",
      "Iteration: 197500\n",
      "Gradient: [   6.4095   10.5205  -12.1977  -77.077  -197.3377]\n",
      "Weights: [-4.867   0.99   -1.4683  0.2088  0.1312]\n",
      "MSE loss: 85.4075\n",
      "Iteration: 197600\n",
      "Gradient: [  -5.8168   14.5259   38.9553 -105.0669  -27.9978]\n",
      "Weights: [-4.8716  0.9987 -1.4681  0.2078  0.1311]\n",
      "MSE loss: 85.4488\n",
      "Iteration: 197700\n",
      "Gradient: [ 1.1882 13.6559 11.3329 96.4515 97.6972]\n",
      "Weights: [-4.8782  1.0055 -1.4675  0.2075  0.1311]\n",
      "MSE loss: 85.5256\n",
      "Iteration: 197800\n",
      "Gradient: [ -6.4317   7.1186  18.6042  13.2192 129.0666]\n",
      "Weights: [-4.871   0.9977 -1.4659  0.2079  0.131 ]\n",
      "MSE loss: 85.4881\n",
      "Iteration: 197900\n",
      "Gradient: [ -6.0637 -21.7459   6.2965 -38.5386  -2.0551]\n",
      "Weights: [-4.8745  0.9999 -1.469   0.2085  0.131 ]\n",
      "MSE loss: 85.4476\n",
      "Iteration: 198000\n",
      "Gradient: [  -0.9791    9.9665    6.9134 -112.609   298.385 ]\n",
      "Weights: [-4.8757  1.0021 -1.4679  0.2078  0.131 ]\n",
      "MSE loss: 85.4726\n",
      "Iteration: 198100\n",
      "Gradient: [ -8.8678  -1.2904  -0.4519 -56.0727  41.2355]\n",
      "Weights: [-4.8748  0.9936 -1.469   0.2088  0.1312]\n",
      "MSE loss: 85.4766\n",
      "Iteration: 198200\n",
      "Gradient: [ -9.4299  -8.5746   9.8103 -36.6881 311.5883]\n",
      "Weights: [-4.8645  0.9882 -1.4689  0.2099  0.1311]\n",
      "MSE loss: 85.4301\n",
      "Iteration: 198300\n",
      "Gradient: [ -0.1697 -12.1675 -17.1976 -48.854  -16.347 ]\n",
      "Weights: [-4.8541  0.9763 -1.4681  0.2107  0.1311]\n",
      "MSE loss: 85.421\n",
      "Iteration: 198400\n",
      "Gradient: [   7.4135   14.3976  -24.9428   81.411  -226.8733]\n",
      "Weights: [-4.8472  0.9693 -1.4679  0.2104  0.131 ]\n",
      "MSE loss: 85.5405\n",
      "Iteration: 198500\n",
      "Gradient: [ -1.5727   1.2468   3.2722 -94.8504 403.9902]\n",
      "Weights: [-4.8675  0.9789 -1.4646  0.2104  0.1308]\n",
      "MSE loss: 85.447\n",
      "Iteration: 198600\n",
      "Gradient: [ -2.9046  -2.8785  -3.3459   8.8481 194.2134]\n",
      "Weights: [-4.858   0.967  -1.4632  0.2107  0.1309]\n",
      "MSE loss: 85.4777\n",
      "Iteration: 198700\n",
      "Gradient: [  9.3546  -9.7916  27.3513  48.274  107.0843]\n",
      "Weights: [-4.8463  0.9637 -1.461   0.2102  0.131 ]\n",
      "MSE loss: 85.5977\n",
      "Iteration: 198800\n",
      "Gradient: [   1.0066    9.4154  -49.0906 -156.6005 -170.094 ]\n",
      "Weights: [-4.8439  0.963  -1.4621  0.2094  0.1309]\n",
      "MSE loss: 85.508\n",
      "Iteration: 198900\n",
      "Gradient: [  1.0369  -4.3807 -52.0236 -88.2726  34.4203]\n",
      "Weights: [-4.869   0.9693 -1.4604  0.2096  0.1306]\n",
      "MSE loss: 85.7978\n",
      "Iteration: 199000\n",
      "Gradient: [-7.6157 -6.2935 16.3545 47.8184 34.7917]\n",
      "Weights: [-4.8729  0.9806 -1.4588  0.2087  0.1304]\n",
      "MSE loss: 85.5071\n",
      "Iteration: 199100\n",
      "Gradient: [  4.5149   4.316   -1.2836  25.5083 387.0341]\n",
      "Weights: [-4.8572  0.9853 -1.4611  0.2088  0.1304]\n",
      "MSE loss: 85.6242\n",
      "Iteration: 199200\n",
      "Gradient: [ -1.0414   8.2008  10.6968 -40.7    -42.1684]\n",
      "Weights: [-4.8656  0.9894 -1.4617  0.2092  0.1303]\n",
      "MSE loss: 85.544\n",
      "Iteration: 199300\n",
      "Gradient: [  0.4917  27.2553 -41.6309 -30.6808 -52.7234]\n",
      "Weights: [-4.8612  0.9831 -1.4615  0.2083  0.1305]\n",
      "MSE loss: 85.4902\n",
      "Iteration: 199400\n",
      "Gradient: [  8.0317  -9.6516  22.4263  -2.4396 -49.7409]\n",
      "Weights: [-4.8537  0.9768 -1.4605  0.2081  0.1309]\n",
      "MSE loss: 85.4746\n",
      "Iteration: 199500\n",
      "Gradient: [  0.701  -11.127   41.2996 -45.5324 150.9487]\n",
      "Weights: [-4.8621  0.9787 -1.4626  0.2082  0.1312]\n",
      "MSE loss: 85.4119\n",
      "Iteration: 199600\n",
      "Gradient: [-9.5169  7.1748  8.3508 50.2216  9.5388]\n",
      "Weights: [-4.8714  0.984  -1.4634  0.2087  0.1309]\n",
      "MSE loss: 85.463\n",
      "Iteration: 199700\n",
      "Gradient: [   1.6358   -6.4983   38.1355   66.7129 -301.2398]\n",
      "Weights: [-4.8628  0.9861 -1.4643  0.209   0.1308]\n",
      "MSE loss: 85.4405\n",
      "Iteration: 199800\n",
      "Gradient: [ -6.436   -5.3927  47.2998 -45.5453 210.4739]\n",
      "Weights: [-4.8778  0.9991 -1.4658  0.2084  0.1308]\n",
      "MSE loss: 85.4945\n",
      "Iteration: 199900\n",
      "Gradient: [   2.4961   -2.4188   -3.3274  -25.1211 -167.3487]\n",
      "Weights: [-4.87    1.0061 -1.4658  0.2074  0.1306]\n",
      "MSE loss: 85.7211\n"
     ]
    }
   ],
   "source": [
    "# Обучение на случайных батчак, по 10% датасета.\n",
    "# lr - индивидуальный для каждого из параметров модели.\n",
    "# Фильтрация градиента (beta=0.8).\n",
    "weights_4, losses_4, iter_final_4, fit_time_4 = model_fit(X_train, y_train,\n",
    "                                                          learning_rate=[1e-3, 1e-4, 1e-5, 1e-6, 1e-7],\n",
    "                                                          tolerance=(0.2**2 * N_points),\n",
    "                                                          batch_ratio=0.1,\n",
    "                                                          beta=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09af430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Gradient: [ 1663.9761  2233.8379  4251.7216  8800.175  16958.556 ]\n",
      "Weights: [-0.0006 -0.0001 -0.     -0.     -0.    ]\n",
      "MSE loss: 40435.4191\n",
      "Iteration: 100\n",
      "Gradient: [   -7.053     19.3734    62.9229   271.7812 -1904.7665]\n",
      "Weights: [-5.0785 -0.3719  0.1135  0.0759  0.0316]\n",
      "MSE loss: 685.352\n",
      "Iteration: 200\n",
      "Gradient: [    4.2012    46.7002    11.3447    40.217  -1543.2498]\n",
      "Weights: [-4.7718 -0.7012  0.0979  0.0941  0.0407]\n",
      "MSE loss: 447.3195\n",
      "Iteration: 300\n",
      "Gradient: [ 10.1334   5.4908  65.8489  62.4328 418.5031]\n",
      "Weights: [-4.614  -0.9031  0.071   0.1062  0.0479]\n",
      "MSE loss: 329.7207\n",
      "Iteration: 400\n",
      "Gradient: [  13.4966   49.4242   37.0195  220.8005 -320.4087]\n",
      "Weights: [-4.4296 -1.0499  0.0541  0.1127  0.0524]\n",
      "MSE loss: 275.3564\n",
      "Iteration: 500\n",
      "Gradient: [-15.8935  20.5579 -12.2773 154.665  319.5088]\n",
      "Weights: [-4.3733 -1.1449  0.0459  0.1171  0.0558]\n",
      "MSE loss: 247.1443\n",
      "Iteration: 600\n",
      "Gradient: [ -17.9932  -13.091   -49.5344  -85.9726 -194.5252]\n",
      "Weights: [-4.3289 -1.2006  0.0348  0.1187  0.059 ]\n",
      "MSE loss: 231.7904\n",
      "Iteration: 700\n",
      "Gradient: [ -6.1463   4.9716 -34.3086 129.3632  16.3302]\n",
      "Weights: [-4.2673 -1.2425  0.0218  0.1206  0.0613]\n",
      "MSE loss: 222.2773\n",
      "Iteration: 800\n",
      "Gradient: [  21.7771    0.3929   74.8293   21.8694 -123.2611]\n",
      "Weights: [-4.2371 -1.257   0.0105  0.1226  0.0629]\n",
      "MSE loss: 217.3858\n",
      "Iteration: 900\n",
      "Gradient: [  -3.2232   16.6694   39.3968  107.5506 -425.3554]\n",
      "Weights: [-4.2617e+00 -1.2480e+00 -1.2000e-03  1.2330e-01  6.4200e-02]\n",
      "MSE loss: 214.0231\n",
      "Iteration: 1000\n",
      "Gradient: [  -5.0133   -7.1828   35.916  -155.8991 -140.2369]\n",
      "Weights: [-4.2153 -1.2528 -0.0166  0.1233  0.0656]\n",
      "MSE loss: 209.7639\n",
      "Iteration: 1100\n",
      "Gradient: [   0.8167    5.7193   30.6939  177.5158 -553.6639]\n",
      "Weights: [-4.2081 -1.2557 -0.0255  0.1242  0.0669]\n",
      "MSE loss: 207.2634\n",
      "Iteration: 1200\n",
      "Gradient: [  -3.2311   -5.3696   -8.2837  -77.662  -597.1229]\n",
      "Weights: [-4.2232 -1.2481 -0.0392  0.1236  0.0681]\n",
      "MSE loss: 204.8968\n",
      "Iteration: 1300\n",
      "Gradient: [-1.44465e+01 -1.42343e+01  4.44000e-02 -6.24220e+01 -9.05973e+01]\n",
      "Weights: [-4.2288 -1.2387 -0.0464  0.1244  0.0692]\n",
      "MSE loss: 202.5914\n",
      "Iteration: 1400\n",
      "Gradient: [ -19.6177  -14.7989  -16.6409   58.7376 -159.426 ]\n",
      "Weights: [-4.2175 -1.2271 -0.0582  0.1239  0.0696]\n",
      "MSE loss: 200.6736\n",
      "Iteration: 1500\n",
      "Gradient: [  -6.9967   -6.41     20.8905   25.7514 -230.4129]\n",
      "Weights: [-4.2324 -1.1945 -0.0666  0.1227  0.0704]\n",
      "MSE loss: 198.3909\n",
      "Iteration: 1600\n",
      "Gradient: [  -7.1189  -12.8662  -64.0686    2.5041 -654.1213]\n",
      "Weights: [-4.2206 -1.2004 -0.0742  0.1221  0.0713]\n",
      "MSE loss: 196.8087\n",
      "Iteration: 1700\n",
      "Gradient: [-9.230000e-02  1.416790e+01  3.573230e+01  1.222260e+02  3.570158e+02]\n",
      "Weights: [-4.2238 -1.1723 -0.0846  0.1211  0.0721]\n",
      "MSE loss: 194.4969\n",
      "Iteration: 1800\n",
      "Gradient: [  8.0049  -4.7442  -3.9084 -82.0389 -29.1568]\n",
      "Weights: [-4.2274 -1.1612 -0.0916  0.119   0.0732]\n",
      "MSE loss: 192.5121\n",
      "Iteration: 1900\n",
      "Gradient: [  12.4652   12.293     8.3191  178.063  -168.5742]\n",
      "Weights: [-4.2424 -1.1397 -0.1052  0.1194  0.0743]\n",
      "MSE loss: 189.9579\n",
      "Iteration: 2000\n",
      "Gradient: [ -10.815    14.4482   -0.3399 -166.0391  179.8262]\n",
      "Weights: [-4.2276 -1.1236 -0.1194  0.1182  0.0752]\n",
      "MSE loss: 187.62\n",
      "Iteration: 2100\n",
      "Gradient: [  13.793    -1.5122   11.6162 -110.3808   53.2943]\n",
      "Weights: [-4.2188 -1.1107 -0.1297  0.1177  0.0761]\n",
      "MSE loss: 185.9344\n",
      "Iteration: 2200\n",
      "Gradient: [  -5.8485   -2.1312   -2.183  -119.1649 -439.7521]\n",
      "Weights: [-4.257  -1.0935 -0.1379  0.1188  0.077 ]\n",
      "MSE loss: 184.059\n",
      "Iteration: 2300\n",
      "Gradient: [  8.8678   6.6851   9.516   40.8476 -98.1656]\n",
      "Weights: [-4.218  -1.0891 -0.1483  0.1184  0.0779]\n",
      "MSE loss: 182.9833\n",
      "Iteration: 2400\n",
      "Gradient: [   8.3715  -12.4281   10.4803    1.8359 -220.7987]\n",
      "Weights: [-4.2404 -1.0729 -0.1597  0.1181  0.0787]\n",
      "MSE loss: 180.2146\n",
      "Iteration: 2500\n",
      "Gradient: [   8.5412    2.8159   -2.8988  -75.2944 -265.3252]\n",
      "Weights: [-4.2519 -1.0511 -0.168   0.117   0.0792]\n",
      "MSE loss: 178.6845\n",
      "Iteration: 2600\n",
      "Gradient: [  9.1162 -14.7674 -14.4491  58.9637  73.9777]\n",
      "Weights: [-4.2395 -1.0343 -0.1777  0.1163  0.0801]\n",
      "MSE loss: 177.1321\n",
      "Iteration: 2700\n",
      "Gradient: [  -2.4137    9.5534    9.1806   37.3479 -241.7316]\n",
      "Weights: [-4.2654 -1.0175 -0.184   0.1161  0.0808]\n",
      "MSE loss: 175.49\n",
      "Iteration: 2800\n",
      "Gradient: [  -0.487    -4.8577   -6.8685  -14.2626 -417.088 ]\n",
      "Weights: [-4.2668 -0.9964 -0.1963  0.1143  0.0815]\n",
      "MSE loss: 173.3214\n",
      "Iteration: 2900\n",
      "Gradient: [ -1.1491   7.5213   2.781  101.9458 144.7835]\n",
      "Weights: [-4.2674 -0.9759 -0.2025  0.1136  0.0821]\n",
      "MSE loss: 172.237\n",
      "Iteration: 3000\n",
      "Gradient: [ -5.6876  -1.5767 -45.3498  -4.5149 -40.2055]\n",
      "Weights: [-4.2771 -0.9575 -0.2114  0.1123  0.0825]\n",
      "MSE loss: 170.5886\n",
      "Iteration: 3100\n",
      "Gradient: [   7.9795  -15.8805   36.4365   78.8245 -501.3026]\n",
      "Weights: [-4.2915 -0.9421 -0.2164  0.1113  0.0832]\n",
      "MSE loss: 169.1576\n",
      "Iteration: 3200\n",
      "Gradient: [  2.7808  27.7525   3.0741  62.7084 184.0951]\n",
      "Weights: [-4.2941 -0.9304 -0.2232  0.1102  0.0841]\n",
      "MSE loss: 167.6536\n",
      "Iteration: 3300\n",
      "Gradient: [14.0868 41.8939 55.0908 70.5207 -1.0607]\n",
      "Weights: [-4.2794 -0.9299 -0.2299  0.1104  0.0847]\n",
      "MSE loss: 166.9076\n",
      "Iteration: 3400\n",
      "Gradient: [ -3.923  -16.9061  -1.7648  49.8219 -71.7611]\n",
      "Weights: [-4.3052 -0.902  -0.2369  0.1101  0.0848]\n",
      "MSE loss: 165.6039\n",
      "Iteration: 3500\n",
      "Gradient: [   9.6827    7.8139  -16.9544 -106.8973 -597.9307]\n",
      "Weights: [-4.3071 -0.8939 -0.2444  0.1093  0.0855]\n",
      "MSE loss: 164.2934\n",
      "Iteration: 3600\n",
      "Gradient: [  -9.019   -25.4258  -13.13     26.1927 -607.627 ]\n",
      "Weights: [-4.3239 -0.8846 -0.2529  0.1092  0.0865]\n",
      "MSE loss: 163.2795\n",
      "Iteration: 3700\n",
      "Gradient: [   8.7525  -22.8512   -1.947    63.0101 -148.4658]\n",
      "Weights: [-4.3157 -0.8537 -0.2631  0.107   0.0873]\n",
      "MSE loss: 160.8167\n",
      "Iteration: 3800\n",
      "Gradient: [   2.6735  -10.0986   36.6787  -14.9922 -151.6918]\n",
      "Weights: [-4.3099 -0.8508 -0.2684  0.1063  0.088 ]\n",
      "MSE loss: 159.8757\n",
      "Iteration: 3900\n",
      "Gradient: [   5.6908  -12.3683    3.0282   57.5039 -152.8205]\n",
      "Weights: [-4.3311 -0.8384 -0.2737  0.1063  0.0884]\n",
      "MSE loss: 158.9679\n",
      "Iteration: 4000\n",
      "Gradient: [   1.4419  -20.1528   44.4277  -29.5026 -210.0853]\n",
      "Weights: [-4.3302 -0.8351 -0.2781  0.1061  0.0889]\n",
      "MSE loss: 158.2045\n",
      "Iteration: 4100\n",
      "Gradient: [-14.1576 -21.7379 -43.5736 148.3564 147.4111]\n",
      "Weights: [-4.3303 -0.8243 -0.2861  0.1054  0.0895]\n",
      "MSE loss: 157.0905\n",
      "Iteration: 4200\n",
      "Gradient: [-11.0564  -1.2306 -11.8434  49.6903 125.6147]\n",
      "Weights: [-4.3238 -0.8137 -0.2891  0.1047  0.0898]\n",
      "MSE loss: 156.3596\n",
      "Iteration: 4300\n",
      "Gradient: [   7.1295   -4.3939   16.5271   35.5264 -159.8935]\n",
      "Weights: [-4.326  -0.7982 -0.2961  0.1052  0.0904]\n",
      "MSE loss: 155.9449\n",
      "Iteration: 4400\n",
      "Gradient: [ -9.4514 -25.1308 -41.9051 -43.5304  13.9863]\n",
      "Weights: [-4.3127 -0.7977 -0.3048  0.1041  0.0909]\n",
      "MSE loss: 154.4871\n",
      "Iteration: 4500\n",
      "Gradient: [-9.869   3.9318 47.0376 51.9021 98.2826]\n",
      "Weights: [-4.3382 -0.7832 -0.3085  0.1037  0.0914]\n",
      "MSE loss: 153.3326\n",
      "Iteration: 4600\n",
      "Gradient: [ -1.2818 -12.1216  22.9517  22.762  -72.3962]\n",
      "Weights: [-4.3256 -0.7756 -0.3175  0.1042  0.0919]\n",
      "MSE loss: 152.3591\n",
      "Iteration: 4700\n",
      "Gradient: [-3.48000e-02  6.00000e-02  5.65527e+01 -5.15550e+00  7.77613e+01]\n",
      "Weights: [-4.3293 -0.7643 -0.32    0.1035  0.0924]\n",
      "MSE loss: 151.9924\n",
      "Iteration: 4800\n",
      "Gradient: [   0.9682  -16.0431  -44.9099  -18.9102 -205.025 ]\n",
      "Weights: [-4.364  -0.7365 -0.3277  0.1026  0.0925]\n",
      "MSE loss: 150.3718\n",
      "Iteration: 4900\n",
      "Gradient: [  9.7967  -6.4465 -37.723  -87.7644 -87.478 ]\n",
      "Weights: [-4.3787 -0.715  -0.334   0.1012  0.0931]\n",
      "MSE loss: 149.2048\n",
      "Iteration: 5000\n",
      "Gradient: [ -3.4256 -13.5312  -3.0285 -67.7626   8.1882]\n",
      "Weights: [-4.3745 -0.7051 -0.3381  0.0999  0.0939]\n",
      "MSE loss: 148.2719\n",
      "Iteration: 5100\n",
      "Gradient: [   3.1049    1.7692  -14.2462  -42.1901 -282.706 ]\n",
      "Weights: [-4.3883 -0.6997 -0.343   0.0991  0.0943]\n",
      "MSE loss: 147.7989\n",
      "Iteration: 5200\n",
      "Gradient: [ -3.0611 -23.6623  14.8352  -9.2337  23.025 ]\n",
      "Weights: [-4.3646 -0.7023 -0.3497  0.099   0.0949]\n",
      "MSE loss: 146.7186\n",
      "Iteration: 5300\n",
      "Gradient: [  11.9184  -14.696    15.3591   -8.5607 -283.8855]\n",
      "Weights: [-4.3633 -0.6946 -0.353   0.0986  0.0957]\n",
      "MSE loss: 145.8855\n",
      "Iteration: 5400\n",
      "Gradient: [ -0.4168   6.1109  38.8781 115.8371  15.1785]\n",
      "Weights: [-4.3664 -0.6726 -0.359   0.0979  0.0959]\n",
      "MSE loss: 145.448\n",
      "Iteration: 5500\n",
      "Gradient: [   2.3951  -22.5681   11.0288   74.8846 -333.9419]\n",
      "Weights: [-4.3771 -0.6586 -0.3662  0.0963  0.0964]\n",
      "MSE loss: 143.8101\n",
      "Iteration: 5600\n",
      "Gradient: [  -1.4654   12.1784   46.239  -192.6699 -218.8545]\n",
      "Weights: [-4.4036 -0.6371 -0.3767  0.0958  0.0974]\n",
      "MSE loss: 142.3358\n",
      "Iteration: 5700\n",
      "Gradient: [  1.8985  11.862  -18.1234  50.1455 -68.7306]\n",
      "Weights: [-4.3834 -0.6245 -0.385   0.0955  0.0979]\n",
      "MSE loss: 141.2626\n",
      "Iteration: 5800\n",
      "Gradient: [ -1.2317  -5.8457  45.1996 -62.6409  95.518 ]\n",
      "Weights: [-4.4003 -0.6062 -0.3919  0.0948  0.0983]\n",
      "MSE loss: 140.0939\n",
      "Iteration: 5900\n",
      "Gradient: [ 12.3815  -3.5455  -1.3502  12.7514 104.5854]\n",
      "Weights: [-4.4116 -0.5866 -0.3985  0.0936  0.0989]\n",
      "MSE loss: 139.0222\n",
      "Iteration: 6000\n",
      "Gradient: [ -9.3201   5.4376 -46.7782 -33.1148 -80.9305]\n",
      "Weights: [-4.4328 -0.5746 -0.4082  0.0935  0.0996]\n",
      "MSE loss: 138.6393\n",
      "Iteration: 6100\n",
      "Gradient: [  4.9971  -0.6762  -4.4148  53.0666 278.4156]\n",
      "Weights: [-4.3971 -0.5701 -0.4148  0.0939  0.1001]\n",
      "MSE loss: 137.363\n",
      "Iteration: 6200\n",
      "Gradient: [  -5.8075    5.6343   12.7543 -117.2419  120.6363]\n",
      "Weights: [-4.4042 -0.5778 -0.4186  0.0936  0.1008]\n",
      "MSE loss: 136.674\n",
      "Iteration: 6300\n",
      "Gradient: [ -7.3753   6.8679 -30.5172 -81.7368  -5.1982]\n",
      "Weights: [-4.4185 -0.5604 -0.4232  0.0929  0.1014]\n",
      "MSE loss: 135.6664\n",
      "Iteration: 6400\n",
      "Gradient: [  6.57     1.2309  -5.4927  -3.0661 -11.0414]\n",
      "Weights: [-4.4148 -0.544  -0.4332  0.0925  0.1021]\n",
      "MSE loss: 134.3914\n",
      "Iteration: 6500\n",
      "Gradient: [ -7.9117  -0.7284 -16.1966 -41.4186  26.912 ]\n",
      "Weights: [-4.4235 -0.5291 -0.438   0.092   0.1024]\n",
      "MSE loss: 133.7119\n",
      "Iteration: 6600\n",
      "Gradient: [  -1.7186   -4.0945   14.6092   31.4834 -138.9664]\n",
      "Weights: [-4.4233 -0.5157 -0.4474  0.0923  0.1027]\n",
      "MSE loss: 132.7389\n",
      "Iteration: 6700\n",
      "Gradient: [  -7.7681    6.1634   27.4026  -97.1844 -170.7764]\n",
      "Weights: [-4.4272 -0.5132 -0.4506  0.0919  0.1033]\n",
      "MSE loss: 132.2032\n",
      "Iteration: 6800\n",
      "Gradient: [   0.7761    3.2597   18.8814 -134.9525 -178.1983]\n",
      "Weights: [-4.444  -0.4977 -0.4592  0.0922  0.1039]\n",
      "MSE loss: 131.3993\n",
      "Iteration: 6900\n",
      "Gradient: [  24.2656    8.3401   27.8461   27.7673 -193.7602]\n",
      "Weights: [-4.428  -0.4832 -0.4663  0.0909  0.1043]\n",
      "MSE loss: 130.344\n",
      "Iteration: 7000\n",
      "Gradient: [  17.2005   -7.6663   -5.2917    1.8705 -102.5335]\n",
      "Weights: [-4.4456 -0.4539 -0.4729  0.09    0.1044]\n",
      "MSE loss: 129.6017\n",
      "Iteration: 7100\n",
      "Gradient: [ -0.9816  12.6812  13.7037 -37.9164  89.1734]\n",
      "Weights: [-4.46   -0.4425 -0.4773  0.0897  0.1051]\n",
      "MSE loss: 129.0064\n",
      "Iteration: 7200\n",
      "Gradient: [   8.2799    4.1492   12.956  -106.7274   77.497 ]\n",
      "Weights: [-4.4512 -0.4387 -0.4834  0.0881  0.1056]\n",
      "MSE loss: 127.9635\n",
      "Iteration: 7300\n",
      "Gradient: [  -7.0475  -20.1693  -24.389  -169.9162 -258.019 ]\n",
      "Weights: [-4.4471 -0.4394 -0.4919  0.0881  0.1065]\n",
      "MSE loss: 127.285\n",
      "Iteration: 7400\n",
      "Gradient: [ 14.4392  13.1611  47.2957  12.6935 161.9979]\n",
      "Weights: [-4.4302 -0.4289 -0.4982  0.0878  0.1073]\n",
      "MSE loss: 126.6767\n",
      "Iteration: 7500\n",
      "Gradient: [  13.2565   15.5969  -43.3845   46.1147 -134.2227]\n",
      "Weights: [-4.4599 -0.4048 -0.5081  0.0885  0.1075]\n",
      "MSE loss: 125.1926\n",
      "Iteration: 7600\n",
      "Gradient: [  -6.9777   -9.2211   51.2019   -9.3383 -120.5086]\n",
      "Weights: [-4.4711 -0.3897 -0.5141  0.0883  0.1079]\n",
      "MSE loss: 124.4849\n",
      "Iteration: 7700\n",
      "Gradient: [   7.6859  -16.7775    5.9363 -111.0695  -78.9854]\n",
      "Weights: [-4.4633 -0.3858 -0.5216  0.0881  0.1084]\n",
      "MSE loss: 123.7613\n",
      "Iteration: 7800\n",
      "Gradient: [  8.5433   1.5156   0.3205  45.6407 118.676 ]\n",
      "Weights: [-4.4726 -0.3634 -0.53    0.0878  0.1089]\n",
      "MSE loss: 122.7806\n",
      "Iteration: 7900\n",
      "Gradient: [  0.2281  -2.3047  11.025  112.9189 -76.7311]\n",
      "Weights: [-4.4978 -0.3388 -0.5369  0.0866  0.1093]\n",
      "MSE loss: 122.0241\n",
      "Iteration: 8000\n",
      "Gradient: [  4.2832   2.6059  -9.3231 -77.9437  46.1701]\n",
      "Weights: [-4.477  -0.3362 -0.5426  0.0861  0.1098]\n",
      "MSE loss: 121.3927\n",
      "Iteration: 8100\n",
      "Gradient: [ -4.9037   5.9211 -24.1489  38.7588  36.026 ]\n",
      "Weights: [-4.4954 -0.332  -0.544   0.0849  0.1103]\n",
      "MSE loss: 121.2261\n",
      "Iteration: 8200\n",
      "Gradient: [  0.472    8.0512  10.8594  65.9857 -94.3444]\n",
      "Weights: [-4.4944 -0.3165 -0.5492  0.0837  0.1109]\n",
      "MSE loss: 120.1749\n",
      "Iteration: 8300\n",
      "Gradient: [-8.7443 12.1455 33.6501 13.352  28.4001]\n",
      "Weights: [-4.5007 -0.3012 -0.5528  0.0834  0.1112]\n",
      "MSE loss: 119.8861\n",
      "Iteration: 8400\n",
      "Gradient: [ 7.70000e-02 -1.50472e+01  3.46373e+01  6.99633e+01  9.21422e+01]\n",
      "Weights: [-4.4902 -0.3132 -0.5512  0.0826  0.1117]\n",
      "MSE loss: 119.7622\n",
      "Iteration: 8500\n",
      "Gradient: [ 14.9542  -6.6787 -13.926   -3.3429 -19.2822]\n",
      "Weights: [-4.5101 -0.2965 -0.555   0.082   0.1116]\n",
      "MSE loss: 119.3536\n",
      "Iteration: 8600\n",
      "Gradient: [  -3.6215   17.3497   -3.8694 -142.8729  105.6989]\n",
      "Weights: [-4.5051 -0.2923 -0.5598  0.081   0.1124]\n",
      "MSE loss: 118.6145\n",
      "Iteration: 8700\n",
      "Gradient: [-11.2356 -11.8448  20.9874 -28.0007 412.8434]\n",
      "Weights: [-4.5044 -0.2808 -0.5674  0.0808  0.1129]\n",
      "MSE loss: 117.858\n",
      "Iteration: 8800\n",
      "Gradient: [  -2.0732   -5.9797  -41.5712  146.3022 -103.1892]\n",
      "Weights: [-4.5161 -0.2719 -0.5719  0.081   0.1131]\n",
      "MSE loss: 117.6109\n",
      "Iteration: 8900\n",
      "Gradient: [ -11.0619  -13.752    38.7216  -39.6214 -148.3817]\n",
      "Weights: [-4.5179 -0.2725 -0.5734  0.0806  0.1135]\n",
      "MSE loss: 117.3995\n",
      "Iteration: 9000\n",
      "Gradient: [  3.3638   8.8076   5.8606  94.7013 102.1479]\n",
      "Weights: [-4.4948 -0.2724 -0.5776  0.0796  0.1141]\n",
      "MSE loss: 116.7408\n",
      "Iteration: 9100\n",
      "Gradient: [ 15.4833  -3.6855 -21.1783  -1.808    6.1917]\n",
      "Weights: [-4.4997 -0.259  -0.5816  0.0796  0.1144]\n",
      "MSE loss: 116.5205\n",
      "Iteration: 9200\n",
      "Gradient: [ -5.9528  15.8866   7.2505 -26.3074   4.2542]\n",
      "Weights: [-4.4942 -0.2645 -0.5853  0.0794  0.1149]\n",
      "MSE loss: 115.9961\n",
      "Iteration: 9300\n",
      "Gradient: [  -3.747   -12.1204   -4.1649  -28.4752 -299.7936]\n",
      "Weights: [-4.5064 -0.2571 -0.5907  0.0801  0.1154]\n",
      "MSE loss: 115.491\n",
      "Iteration: 9400\n",
      "Gradient: [  0.5661   5.623   13.9388 -27.7539 178.7066]\n",
      "Weights: [-4.5196 -0.2407 -0.5964  0.0794  0.1157]\n",
      "MSE loss: 114.9187\n",
      "Iteration: 9500\n",
      "Gradient: [-10.9184  -2.2802   7.5813  36.6741 -17.061 ]\n",
      "Weights: [-4.5077 -0.2327 -0.6043  0.0789  0.1161]\n",
      "MSE loss: 114.2626\n",
      "Iteration: 9600\n",
      "Gradient: [  10.7351   -3.8207    5.0892   21.4589 -143.9332]\n",
      "Weights: [-4.4995 -0.236  -0.6042  0.0778  0.1165]\n",
      "MSE loss: 114.2469\n",
      "Iteration: 9700\n",
      "Gradient: [-1.8322  2.3675 23.0531  1.5977 -6.971 ]\n",
      "Weights: [-4.5017 -0.2229 -0.6087  0.0764  0.117 ]\n",
      "MSE loss: 113.6605\n",
      "Iteration: 9800\n",
      "Gradient: [   7.3051   -3.1547   59.9789   62.5264 -176.8119]\n",
      "Weights: [-4.5132 -0.2051 -0.6138  0.0767  0.1171]\n",
      "MSE loss: 113.0704\n",
      "Iteration: 9900\n",
      "Gradient: [   2.4065   -6.0142   10.1919  -11.8083 -265.0815]\n",
      "Weights: [-4.5247 -0.2028 -0.6162  0.0767  0.1176]\n",
      "MSE loss: 112.6802\n",
      "Iteration: 10000\n",
      "Gradient: [  3.566    1.7619  19.641  -91.893    9.7463]\n",
      "Weights: [-4.5151 -0.195  -0.6209  0.076   0.1179]\n",
      "MSE loss: 112.338\n",
      "Iteration: 10100\n",
      "Gradient: [  -0.8094  -13.0036  -22.3054 -159.701  -212.4148]\n",
      "Weights: [-4.5227 -0.187  -0.6302  0.0775  0.1183]\n",
      "MSE loss: 111.6581\n",
      "Iteration: 10200\n",
      "Gradient: [   1.2706  -17.6481    4.1482   11.5715 -225.5046]\n",
      "Weights: [-4.5322 -0.1734 -0.634   0.0769  0.1187]\n",
      "MSE loss: 111.1895\n",
      "Iteration: 10300\n",
      "Gradient: [ 1.7314 -2.2702  8.4154  3.8225 -3.9195]\n",
      "Weights: [-4.5411 -0.1557 -0.6399  0.0759  0.1188]\n",
      "MSE loss: 110.7245\n",
      "Iteration: 10400\n",
      "Gradient: [   3.8942  -17.4455   10.3535   -9.6328 -365.0221]\n",
      "Weights: [-4.55   -0.1478 -0.6464  0.0762  0.1193]\n",
      "MSE loss: 110.2949\n",
      "Iteration: 10500\n",
      "Gradient: [  4.4674  -7.9983   0.2257 -13.6948 -18.9454]\n",
      "Weights: [-4.5301 -0.1353 -0.6525  0.0755  0.1196]\n",
      "MSE loss: 109.8442\n",
      "Iteration: 10600\n",
      "Gradient: [   0.5005   -5.7656  -44.6335 -104.4801 -151.2311]\n",
      "Weights: [-4.5429 -0.1341 -0.6543  0.0756  0.1198]\n",
      "MSE loss: 109.4742\n",
      "Iteration: 10700\n",
      "Gradient: [   1.9025   -9.3844    7.5612  104.6631 -230.9255]\n",
      "Weights: [-4.5502 -0.1244 -0.658   0.075   0.1203]\n",
      "MSE loss: 109.0284\n",
      "Iteration: 10800\n",
      "Gradient: [   0.6531  -18.4977   49.7791 -151.7063  -67.8023]\n",
      "Weights: [-4.5504 -0.1167 -0.6623  0.0747  0.1206]\n",
      "MSE loss: 108.6105\n",
      "Iteration: 10900\n",
      "Gradient: [   5.6867  -13.7418  -39.4945  -23.5287 -420.3568]\n",
      "Weights: [-4.5545 -0.1119 -0.665   0.0744  0.1209]\n",
      "MSE loss: 108.3691\n",
      "Iteration: 11000\n",
      "Gradient: [ -7.8793 -15.5771 -20.2597 -49.0096  -2.02  ]\n",
      "Weights: [-4.5805 -0.098  -0.668   0.0738  0.1213]\n",
      "MSE loss: 108.4175\n",
      "Iteration: 11100\n",
      "Gradient: [  -7.174    -6.952   -35.9776   88.7619 -244.9946]\n",
      "Weights: [-4.5628 -0.0956 -0.6745  0.0741  0.1215]\n",
      "MSE loss: 107.7705\n",
      "Iteration: 11200\n",
      "Gradient: [ -17.199    -0.3683   43.7658 -137.5096   74.3051]\n",
      "Weights: [-4.5646 -0.0831 -0.676   0.0737  0.1218]\n",
      "MSE loss: 107.4328\n",
      "Iteration: 11300\n",
      "Gradient: [   6.539     5.1854   38.6383   20.7668 -113.8569]\n",
      "Weights: [-4.5703 -0.0809 -0.6788  0.0743  0.1219]\n",
      "MSE loss: 107.2712\n",
      "Iteration: 11400\n",
      "Gradient: [  3.0721  -0.874  -38.918   30.434   63.4835]\n",
      "Weights: [-4.5545 -0.0854 -0.6838  0.0746  0.1222]\n",
      "MSE loss: 106.9128\n",
      "Iteration: 11500\n",
      "Gradient: [  3.2261  -3.4809  36.639   39.0978 158.1375]\n",
      "Weights: [-4.5412 -0.0781 -0.6898  0.0747  0.1225]\n",
      "MSE loss: 106.9339\n",
      "Iteration: 11600\n",
      "Gradient: [-1.5268  0.5989 40.8368 38.2252 -9.6879]\n",
      "Weights: [-4.5616 -0.0649 -0.695   0.0745  0.123 ]\n",
      "MSE loss: 106.0702\n",
      "Iteration: 11700\n",
      "Gradient: [  -4.8081    0.3685   15.4333  -42.6641 -200.7076]\n",
      "Weights: [-4.5813 -0.0538 -0.7005  0.0741  0.1234]\n",
      "MSE loss: 105.851\n",
      "Iteration: 11800\n",
      "Gradient: [  1.1802  12.9962 -16.9047  37.8966 -47.7551]\n",
      "Weights: [-4.5602 -0.0494 -0.7029  0.0738  0.1235]\n",
      "MSE loss: 105.5261\n",
      "Iteration: 11900\n",
      "Gradient: [  5.3739  -5.1561 -28.2736 -38.6938  -7.0341]\n",
      "Weights: [-4.5709 -0.039  -0.7078  0.0732  0.1237]\n",
      "MSE loss: 105.0411\n",
      "Iteration: 12000\n",
      "Gradient: [-6.3106  8.452  10.3015 17.8836  8.1231]\n",
      "Weights: [-4.5746 -0.0361 -0.7111  0.0737  0.1241]\n",
      "MSE loss: 104.7642\n",
      "Iteration: 12100\n",
      "Gradient: [  -2.8944  -10.6789  -10.6914  -39.4966 -180.1083]\n",
      "Weights: [-4.5713 -0.0347 -0.7141  0.0746  0.1242]\n",
      "MSE loss: 104.6603\n",
      "Iteration: 12200\n",
      "Gradient: [  -4.2752   -3.0357    4.6251   40.9268 -235.6723]\n",
      "Weights: [-4.5689 -0.0189 -0.7234  0.075   0.1244]\n",
      "MSE loss: 104.1854\n",
      "Iteration: 12300\n",
      "Gradient: [ 1.8857  9.2739 25.1729 76.313  77.3064]\n",
      "Weights: [-4.5628e+00 -3.6000e-03 -7.2850e-01  7.3400e-02  1.2520e-01]\n",
      "MSE loss: 104.5646\n",
      "Iteration: 12400\n",
      "Gradient: [ -3.735    9.5588 -25.4589   9.5298   4.9083]\n",
      "Weights: [-4.5954  0.0146 -0.7329  0.0728  0.1254]\n",
      "MSE loss: 103.1489\n",
      "Iteration: 12500\n",
      "Gradient: [ -11.8549    2.4022  -18.314   -77.478  -150.3054]\n",
      "Weights: [-4.6155  0.0252 -0.7351  0.0714  0.1255]\n",
      "MSE loss: 103.348\n",
      "Iteration: 12600\n",
      "Gradient: [12.7418 -4.822  11.4292 72.934  36.6906]\n",
      "Weights: [-4.6107  0.0436 -0.7406  0.0716  0.1257]\n",
      "MSE loss: 102.6703\n",
      "Iteration: 12700\n",
      "Gradient: [  3.887   20.8769  35.2258 122.2655 265.2529]\n",
      "Weights: [-4.6054  0.0433 -0.7439  0.0723  0.1259]\n",
      "MSE loss: 102.5394\n",
      "Iteration: 12800\n",
      "Gradient: [   2.12      7.6626   34.4954  -10.1662 -257.3388]\n",
      "Weights: [-4.6086  0.0491 -0.7467  0.0715  0.1262]\n",
      "MSE loss: 102.1831\n",
      "Iteration: 12900\n",
      "Gradient: [  0.5393   6.918   37.4232  75.1597 -40.7961]\n",
      "Weights: [-4.6024  0.049  -0.7522  0.0714  0.1266]\n",
      "MSE loss: 101.7914\n",
      "Iteration: 13000\n",
      "Gradient: [ -6.5755 -10.1161  13.0799  59.112  274.7073]\n",
      "Weights: [-4.6124  0.0472 -0.752   0.0718  0.1268]\n",
      "MSE loss: 101.817\n",
      "Iteration: 13100\n",
      "Gradient: [ -8.1225  -5.0075 -36.8898  65.3088 -42.6777]\n",
      "Weights: [-4.6107  0.048  -0.7545  0.0713  0.1271]\n",
      "MSE loss: 101.7518\n",
      "Iteration: 13200\n",
      "Gradient: [  6.2429  10.9441  11.7348  26.2635 104.4042]\n",
      "Weights: [-4.6092  0.0592 -0.7552  0.0696  0.1274]\n",
      "MSE loss: 101.3125\n",
      "Iteration: 13300\n",
      "Gradient: [  -7.7965   -7.1621  -40.9585  -43.3913 -105.6645]\n",
      "Weights: [-4.6181  0.0637 -0.7578  0.0693  0.1279]\n",
      "MSE loss: 101.1102\n",
      "Iteration: 13400\n",
      "Gradient: [ -7.3529  12.6631   6.194  -90.8837 -48.9138]\n",
      "Weights: [-4.6216  0.0776 -0.7628  0.0684  0.128 ]\n",
      "MSE loss: 100.8495\n",
      "Iteration: 13500\n",
      "Gradient: [ 12.1703   2.767   35.3635  28.8918 -54.9801]\n",
      "Weights: [-4.6072  0.0834 -0.7649  0.0682  0.1281]\n",
      "MSE loss: 100.9111\n",
      "Iteration: 13600\n",
      "Gradient: [ 18.8646  15.1374  12.7967  11.0965 -70.4591]\n",
      "Weights: [-4.6026  0.0831 -0.767   0.0686  0.1282]\n",
      "MSE loss: 100.9129\n",
      "Iteration: 13700\n",
      "Gradient: [ -0.9693  -6.2224 -11.2236  60.2137 -81.2743]\n",
      "Weights: [-4.6096  0.0741 -0.7697  0.0692  0.1286]\n",
      "MSE loss: 100.4413\n",
      "Iteration: 13800\n",
      "Gradient: [  -3.324     7.671   -35.4268 -109.3951  -57.0105]\n",
      "Weights: [-4.6079  0.0697 -0.7716  0.0695  0.129 ]\n",
      "MSE loss: 100.4198\n",
      "Iteration: 13900\n",
      "Gradient: [ 0.5731 11.6846 29.4127  8.6428 22.1942]\n",
      "Weights: [-4.5969  0.0719 -0.771   0.0698  0.1289]\n",
      "MSE loss: 100.5097\n",
      "Iteration: 14000\n",
      "Gradient: [ -4.8334  17.8041  60.7912   8.5887 -34.8368]\n",
      "Weights: [-4.5954  0.0779 -0.7733  0.0695  0.129 ]\n",
      "MSE loss: 100.5019\n",
      "Iteration: 14100\n",
      "Gradient: [  -0.574    -2.6153   -9.0066   13.2712 -321.9285]\n",
      "Weights: [-4.6016  0.0886 -0.7811  0.0704  0.1293]\n",
      "MSE loss: 99.9878\n",
      "Iteration: 14200\n",
      "Gradient: [ -4.2568  -9.1024 -13.4176 -68.8025  26.4168]\n",
      "Weights: [-4.616   0.0937 -0.7857  0.0705  0.1296]\n",
      "MSE loss: 99.5466\n",
      "Iteration: 14300\n",
      "Gradient: [ -2.6361   3.4135  -6.7474 -82.5427  41.84  ]\n",
      "Weights: [-4.6212  0.1146 -0.793   0.0704  0.1297]\n",
      "MSE loss: 99.0758\n",
      "Iteration: 14400\n",
      "Gradient: [  -2.0632   16.4226  -11.464     2.8036 -156.912 ]\n",
      "Weights: [-4.6209  0.1251 -0.7989  0.0707  0.1298]\n",
      "MSE loss: 98.8421\n",
      "Iteration: 14500\n",
      "Gradient: [-18.3887  -5.702   12.0599  62.0817 149.1395]\n",
      "Weights: [-4.6236  0.1304 -0.8006  0.0699  0.1301]\n",
      "MSE loss: 98.6385\n",
      "Iteration: 14600\n",
      "Gradient: [   1.1612   13.5471  -19.8031  -48.1718 -193.7511]\n",
      "Weights: [-4.6284  0.1372 -0.8041  0.0697  0.1302]\n",
      "MSE loss: 98.606\n",
      "Iteration: 14700\n",
      "Gradient: [ 5.439100e+00  2.823000e-01  7.926500e+00 -2.016430e+01  2.968402e+02]\n",
      "Weights: [-4.6307  0.1395 -0.8059  0.0707  0.1305]\n",
      "MSE loss: 98.3617\n",
      "Iteration: 14800\n",
      "Gradient: [ -1.4637   7.24    17.8196  36.3972 120.2667]\n",
      "Weights: [-4.6325  0.1416 -0.8092  0.0703  0.1307]\n",
      "MSE loss: 98.1636\n",
      "Iteration: 14900\n",
      "Gradient: [  7.2885  -4.0601 -18.6873  13.1612  -6.1745]\n",
      "Weights: [-4.627   0.1502 -0.814   0.0705  0.1311]\n",
      "MSE loss: 97.9659\n",
      "Iteration: 15000\n",
      "Gradient: [ 12.5871  -8.4102 -10.7574  18.5827  12.2426]\n",
      "Weights: [-4.6409  0.1695 -0.8182  0.0701  0.131 ]\n",
      "MSE loss: 97.7447\n",
      "Iteration: 15100\n",
      "Gradient: [  -8.3526  -15.5742   36.1235  -20.0971 -301.4773]\n",
      "Weights: [-4.6408  0.1722 -0.82    0.0697  0.1313]\n",
      "MSE loss: 97.5984\n",
      "Iteration: 15200\n",
      "Gradient: [  2.8606 -13.4309 -10.4568  26.0069   7.0469]\n",
      "Weights: [-4.6402  0.175  -0.8231  0.0699  0.1315]\n",
      "MSE loss: 97.4178\n",
      "Iteration: 15300\n",
      "Gradient: [  -1.1803   11.929   -50.7303  -55.5834 -482.0524]\n",
      "Weights: [-4.6312  0.1705 -0.8252  0.0694  0.1317]\n",
      "MSE loss: 97.3494\n",
      "Iteration: 15400\n",
      "Gradient: [  8.9692  12.5904  23.6145 111.4102 274.0921]\n",
      "Weights: [-4.6538  0.1918 -0.8283  0.0689  0.1321]\n",
      "MSE loss: 97.0787\n",
      "Iteration: 15500\n",
      "Gradient: [   3.8802   -4.8967    1.1083   41.3004 -113.7188]\n",
      "Weights: [-4.6567  0.1872 -0.8289  0.0689  0.1321]\n",
      "MSE loss: 96.9744\n",
      "Iteration: 15600\n",
      "Gradient: [ 10.6585   0.944   15.6755  45.9046 200.9185]\n",
      "Weights: [-4.6539  0.1917 -0.8318  0.0691  0.1324]\n",
      "MSE loss: 96.7973\n",
      "Iteration: 15700\n",
      "Gradient: [ -0.5784  -2.7924 -22.4524  -2.7135 -93.3676]\n",
      "Weights: [-4.6441  0.1914 -0.8364  0.0694  0.1325]\n",
      "MSE loss: 96.6173\n",
      "Iteration: 15800\n",
      "Gradient: [  -7.7946   -6.6339  -26.5495   -6.6851 -124.1808]\n",
      "Weights: [-4.6502  0.1968 -0.8394  0.0696  0.1326]\n",
      "MSE loss: 96.4552\n",
      "Iteration: 15900\n",
      "Gradient: [   4.053    13.6835   -5.3065  128.3155 -174.5459]\n",
      "Weights: [-4.6565  0.1954 -0.8408  0.0701  0.1329]\n",
      "MSE loss: 96.4905\n",
      "Iteration: 16000\n",
      "Gradient: [ 6.6887 10.4023 28.533  12.6164 64.0631]\n",
      "Weights: [-4.655   0.2053 -0.8417  0.0693  0.1329]\n",
      "MSE loss: 96.2797\n",
      "Iteration: 16100\n",
      "Gradient: [  8.2093   4.3392  35.9957 -28.9091 223.4209]\n",
      "Weights: [-4.6482  0.2091 -0.8468  0.0694  0.1332]\n",
      "MSE loss: 96.0695\n",
      "Iteration: 16200\n",
      "Gradient: [  -2.6415    3.4218  -13.4939   46.7144 -106.0997]\n",
      "Weights: [-4.6639  0.2133 -0.8467  0.069   0.1336]\n",
      "MSE loss: 96.0642\n",
      "Iteration: 16300\n",
      "Gradient: [-3.1906 20.8011  2.7938 57.9942 -8.3457]\n",
      "Weights: [-4.6526  0.2191 -0.8503  0.0686  0.1336]\n",
      "MSE loss: 95.8423\n",
      "Iteration: 16400\n",
      "Gradient: [ -4.0053   7.6496  10.7819 -26.9472 -48.6975]\n",
      "Weights: [-4.6604  0.2224 -0.8509  0.0684  0.1338]\n",
      "MSE loss: 95.741\n",
      "Iteration: 16500\n",
      "Gradient: [  7.4674  22.1043 -21.4797   7.6949 -74.7918]\n",
      "Weights: [-4.6515  0.2163 -0.8545  0.0695  0.1338]\n",
      "MSE loss: 95.6879\n",
      "Iteration: 16600\n",
      "Gradient: [ -3.3669  16.0706  32.8682 -13.5806 -33.1165]\n",
      "Weights: [-4.6662  0.236  -0.8554  0.0683  0.134 ]\n",
      "MSE loss: 95.6587\n",
      "Iteration: 16700\n",
      "Gradient: [  7.6869  -1.805   57.372  145.1265 -66.8136]\n",
      "Weights: [-4.6588  0.2312 -0.8601  0.069   0.1343]\n",
      "MSE loss: 95.3312\n",
      "Iteration: 16800\n",
      "Gradient: [  2.0493   8.1079  25.4838  25.8    248.1368]\n",
      "Weights: [-4.6487  0.2461 -0.866   0.0685  0.1345]\n",
      "MSE loss: 95.5359\n",
      "Iteration: 16900\n",
      "Gradient: [ 11.9056 -14.8241  14.3238 -67.7899  -4.7191]\n",
      "Weights: [-4.659   0.2477 -0.8681  0.069   0.1345]\n",
      "MSE loss: 95.033\n",
      "Iteration: 17000\n",
      "Gradient: [ -0.0664   1.2484  12.9773  50.1218 -38.5011]\n",
      "Weights: [-4.6804  0.2631 -0.8704  0.0685  0.1346]\n",
      "MSE loss: 94.8763\n",
      "Iteration: 17100\n",
      "Gradient: [  -2.552     4.6384    3.4727   32.4023 -185.1576]\n",
      "Weights: [-4.6708  0.2586 -0.8713  0.0687  0.1346]\n",
      "MSE loss: 94.8243\n",
      "Iteration: 17200\n",
      "Gradient: [  -9.3218   -7.6499  -10.3548   80.9262 -104.8443]\n",
      "Weights: [-4.6841  0.2651 -0.8708  0.0686  0.1348]\n",
      "MSE loss: 94.9631\n",
      "Iteration: 17300\n",
      "Gradient: [  -4.6527   -6.6401    9.4682  -44.3928 -186.3262]\n",
      "Weights: [-4.6656  0.2576 -0.8761  0.069   0.1352]\n",
      "MSE loss: 94.5901\n",
      "Iteration: 17400\n",
      "Gradient: [   4.5737   10.1196    2.6626  -59.7022 -115.458 ]\n",
      "Weights: [-4.6799  0.2689 -0.8792  0.0693  0.1354]\n",
      "MSE loss: 94.5283\n",
      "Iteration: 17500\n",
      "Gradient: [  -5.0906    5.7744   20.1802 -148.4053  -93.541 ]\n",
      "Weights: [-4.6744  0.2714 -0.8837  0.069   0.1356]\n",
      "MSE loss: 94.2825\n",
      "Iteration: 17600\n",
      "Gradient: [   5.8486    7.5795   14.7013   45.9105 -166.6875]\n",
      "Weights: [-4.6648  0.2774 -0.8823  0.0682  0.1358]\n",
      "MSE loss: 94.8472\n",
      "Iteration: 17700\n",
      "Gradient: [   7.5935   -6.655    -2.1073  -58.8537 -177.4895]\n",
      "Weights: [-4.6852  0.2856 -0.889   0.0683  0.1359]\n",
      "MSE loss: 94.1602\n",
      "Iteration: 17800\n",
      "Gradient: [  -2.7658  -14.5279    1.0165  -57.9682 -196.44  ]\n",
      "Weights: [-4.6764  0.2789 -0.8896  0.0695  0.136 ]\n",
      "MSE loss: 94.0211\n",
      "Iteration: 17900\n",
      "Gradient: [  3.6149  10.318    1.5413  15.2262 165.4917]\n",
      "Weights: [-4.6795  0.292  -0.8912  0.0689  0.1361]\n",
      "MSE loss: 94.0256\n",
      "Iteration: 18000\n",
      "Gradient: [-7.580500e+00 -1.978470e+01 -2.782000e-01 -2.865100e+01 -3.023435e+02]\n",
      "Weights: [-4.6774  0.2876 -0.894   0.0687  0.1363]\n",
      "MSE loss: 93.8571\n",
      "Iteration: 18100\n",
      "Gradient: [   4.9305   -1.4604   -2.1697 -235.5942  144.6121]\n",
      "Weights: [-4.6713  0.2831 -0.8959  0.0693  0.1363]\n",
      "MSE loss: 93.9107\n",
      "Iteration: 18200\n",
      "Gradient: [ -3.9489  -4.6568 -19.0519 -35.4879 225.4105]\n",
      "Weights: [-4.6683  0.2812 -0.8961  0.0693  0.1365]\n",
      "MSE loss: 93.8226\n",
      "Iteration: 18300\n",
      "Gradient: [   6.9543   -3.2031  -12.9964   14.8907 -167.4845]\n",
      "Weights: [-4.6812  0.2967 -0.9011  0.0694  0.1368]\n",
      "MSE loss: 93.5336\n",
      "Iteration: 18400\n",
      "Gradient: [ -2.5269 -22.1667  -8.4666  -4.7776  79.4341]\n",
      "Weights: [-4.6997  0.3197 -0.9037  0.0679  0.1368]\n",
      "MSE loss: 93.3951\n",
      "Iteration: 18500\n",
      "Gradient: [   9.4355    0.324    18.0743 -103.7434   97.4907]\n",
      "Weights: [-4.6815  0.322  -0.9066  0.0683  0.1368]\n",
      "MSE loss: 93.517\n",
      "Iteration: 18600\n",
      "Gradient: [   3.0028  -11.2507   -9.3785  -21.5257 -156.1776]\n",
      "Weights: [-4.6904  0.3335 -0.9099  0.068   0.1368]\n",
      "MSE loss: 93.3005\n",
      "Iteration: 18700\n",
      "Gradient: [ -8.9599  -4.9973  13.0497 -27.314  369.2204]\n",
      "Weights: [-4.7106  0.333  -0.9111  0.0682  0.137 ]\n",
      "MSE loss: 93.37\n",
      "Iteration: 18800\n",
      "Gradient: [ -4.1462  12.5465   2.6514   4.0653 101.8873]\n",
      "Weights: [-4.7069  0.3355 -0.9104  0.0677  0.1373]\n",
      "MSE loss: 93.137\n",
      "Iteration: 18900\n",
      "Gradient: [  -4.173    -1.1524   49.1353  -58.1132 -148.0853]\n",
      "Weights: [-4.7129  0.3345 -0.9128  0.0677  0.1375]\n",
      "MSE loss: 93.2946\n",
      "Iteration: 19000\n",
      "Gradient: [ -2.3921   2.3266  25.1758  14.2429 183.4991]\n",
      "Weights: [-4.7081  0.3453 -0.9146  0.0674  0.1376]\n",
      "MSE loss: 92.9539\n",
      "Iteration: 19100\n",
      "Gradient: [  7.0497  17.9162 -12.4483 -19.7104 178.9407]\n",
      "Weights: [-4.7014  0.3375 -0.9187  0.0677  0.1379]\n",
      "MSE loss: 92.8265\n",
      "Iteration: 19200\n",
      "Gradient: [-9.5673 -5.1023 35.6916 -5.9664 -4.5228]\n",
      "Weights: [-4.6921  0.3406 -0.92    0.0677  0.1381]\n",
      "MSE loss: 92.6859\n",
      "Iteration: 19300\n",
      "Gradient: [ -1.9017  -1.6937   6.3146 -14.1293 159.724 ]\n",
      "Weights: [-4.7023  0.3556 -0.9243  0.068   0.1381]\n",
      "MSE loss: 92.6373\n",
      "Iteration: 19400\n",
      "Gradient: [  5.4359   7.356   10.8946 -28.0018 -61.8745]\n",
      "Weights: [-4.7092  0.359  -0.9267  0.068   0.1381]\n",
      "MSE loss: 92.4548\n",
      "Iteration: 19500\n",
      "Gradient: [   9.7472    1.9004   23.3853   -2.3674 -154.3116]\n",
      "Weights: [-4.731   0.3752 -0.9292  0.0679  0.1382]\n",
      "MSE loss: 92.6745\n",
      "Iteration: 19600\n",
      "Gradient: [  9.5819   8.6062 -13.1075  36.6392 286.9046]\n",
      "Weights: [-4.7006  0.3666 -0.9315  0.0677  0.1385]\n",
      "MSE loss: 92.3568\n",
      "Iteration: 19700\n",
      "Gradient: [  3.4351  12.7655  30.1295  38.8438 -38.9749]\n",
      "Weights: [-4.6918  0.3645 -0.9351  0.068   0.1386]\n",
      "MSE loss: 92.2654\n",
      "Iteration: 19800\n",
      "Gradient: [   7.6376  -12.7675    0.9299  104.4057 -153.4349]\n",
      "Weights: [-4.6899  0.3634 -0.9369  0.0682  0.1389]\n",
      "MSE loss: 92.1786\n",
      "Iteration: 19900\n",
      "Gradient: [ -1.8386   6.5607 -12.39    23.6865 178.2547]\n",
      "Weights: [-4.7056  0.3685 -0.9393  0.0688  0.1391]\n",
      "MSE loss: 91.9771\n",
      "Iteration: 20000\n",
      "Gradient: [ -0.7959   6.9056   5.1094 -25.0064  46.5686]\n",
      "Weights: [-4.7036  0.3647 -0.9402  0.069   0.1393]\n",
      "MSE loss: 91.9791\n",
      "Iteration: 20100\n",
      "Gradient: [  0.828  -24.8053 -30.2281 -13.1935 -22.9911]\n",
      "Weights: [-4.7059  0.3746 -0.9437  0.0683  0.1393]\n",
      "MSE loss: 91.8969\n",
      "Iteration: 20200\n",
      "Gradient: [ -3.2002  -9.1854 -17.0241 -49.0172   1.5553]\n",
      "Weights: [-4.7051  0.3721 -0.9437  0.0687  0.1394]\n",
      "MSE loss: 91.8735\n",
      "Iteration: 20300\n",
      "Gradient: [ -3.6957  20.4655  50.3117  -2.2576 -32.3069]\n",
      "Weights: [-4.6925  0.38   -0.9452  0.0684  0.1392]\n",
      "MSE loss: 92.036\n",
      "Iteration: 20400\n",
      "Gradient: [  7.5088   1.365   -4.0398 -92.5433 -58.9441]\n",
      "Weights: [-4.7118  0.391  -0.9504  0.0689  0.1394]\n",
      "MSE loss: 91.6265\n",
      "Iteration: 20500\n",
      "Gradient: [ 1.6386 -7.6478 44.4675 27.1453  6.2249]\n",
      "Weights: [-4.7101  0.3944 -0.9524  0.0689  0.1395]\n",
      "MSE loss: 91.549\n",
      "Iteration: 20600\n",
      "Gradient: [  5.2485   6.722   22.0877 -62.8864   1.2873]\n",
      "Weights: [-4.7171  0.4003 -0.9544  0.0691  0.1395]\n",
      "MSE loss: 91.4917\n",
      "Iteration: 20700\n",
      "Gradient: [ -5.18   -12.7862  -4.6224 -81.0118 112.7179]\n",
      "Weights: [-4.7069  0.395  -0.9562  0.0692  0.1397]\n",
      "MSE loss: 91.4725\n",
      "Iteration: 20800\n",
      "Gradient: [-0.0789 -2.1401 -5.3586  1.8913 22.1509]\n",
      "Weights: [-4.6983  0.3949 -0.9572  0.0699  0.1398]\n",
      "MSE loss: 91.6155\n",
      "Iteration: 20900\n",
      "Gradient: [  -0.7126   -2.2466   52.2359  -44.3002 -173.3381]\n",
      "Weights: [-4.7066  0.3934 -0.9598  0.0704  0.14  ]\n",
      "MSE loss: 91.3504\n",
      "Iteration: 21000\n",
      "Gradient: [  -2.17      7.7286   -3.2822  -36.3411 -142.2441]\n",
      "Weights: [-4.7018  0.3942 -0.9646  0.07    0.1404]\n",
      "MSE loss: 91.4158\n",
      "Iteration: 21100\n",
      "Gradient: [  6.8008  -3.7698  49.888  129.0373  20.4749]\n",
      "Weights: [-4.6999  0.4034 -0.9675  0.0705  0.1404]\n",
      "MSE loss: 91.1948\n",
      "Iteration: 21200\n",
      "Gradient: [  -2.3909    2.4588   30.6609   60.1139 -284.4531]\n",
      "Weights: [-4.7112  0.4065 -0.9703  0.0706  0.1407]\n",
      "MSE loss: 91.0721\n",
      "Iteration: 21300\n",
      "Gradient: [ -1.5886 -22.6794  23.676   26.0738 440.7863]\n",
      "Weights: [-4.731   0.4134 -0.9703  0.0706  0.1408]\n",
      "MSE loss: 91.3923\n",
      "Iteration: 21400\n",
      "Gradient: [ -2.2167  -9.1512  16.3503  -0.577  138.8621]\n",
      "Weights: [-4.7237  0.4217 -0.9747  0.0709  0.1405]\n",
      "MSE loss: 91.0279\n",
      "Iteration: 21500\n",
      "Gradient: [  5.2389  -8.1858  26.3485 -40.9805 -85.2362]\n",
      "Weights: [-4.7219  0.4168 -0.9742  0.0712  0.1406]\n",
      "MSE loss: 91.066\n",
      "Iteration: 21600\n",
      "Gradient: [  -1.162   -20.1545  -36.5446  -48.83   -182.7217]\n",
      "Weights: [-4.7239  0.4098 -0.9728  0.071   0.1405]\n",
      "MSE loss: 91.8507\n",
      "Iteration: 21700\n",
      "Gradient: [ -0.6754   7.8074  -9.6317 -13.8488 -29.3073]\n",
      "Weights: [-4.7161  0.4184 -0.9725  0.0701  0.1407]\n",
      "MSE loss: 90.932\n",
      "Iteration: 21800\n",
      "Gradient: [ -3.47    -1.3326 -17.8468 136.2806 187.0316]\n",
      "Weights: [-4.731   0.4252 -0.9749  0.0707  0.1409]\n",
      "MSE loss: 91.0032\n",
      "Iteration: 21900\n",
      "Gradient: [ 15.9958   7.5196 -25.6121  75.2762 233.1443]\n",
      "Weights: [-4.6959  0.4173 -0.9774  0.0706  0.1411]\n",
      "MSE loss: 91.1209\n",
      "Iteration: 22000\n",
      "Gradient: [14.7934  6.7334 12.2595 92.012  71.1128]\n",
      "Weights: [-4.7087  0.424  -0.9765  0.0702  0.1411]\n",
      "MSE loss: 90.9392\n",
      "Iteration: 22100\n",
      "Gradient: [   4.68     -1.6113   -9.5241  -18.3483 -126.6332]\n",
      "Weights: [-4.7089  0.4325 -0.9811  0.0696  0.1411]\n",
      "MSE loss: 90.8078\n",
      "Iteration: 22200\n",
      "Gradient: [ -7.3665 -11.1707  -4.0277 -81.7864  37.6002]\n",
      "Weights: [-4.7316  0.4352 -0.9832  0.0706  0.1413]\n",
      "MSE loss: 90.8396\n",
      "Iteration: 22300\n",
      "Gradient: [  14.2345    7.8847   41.7937   18.8759 -246.5021]\n",
      "Weights: [-4.6917  0.4312 -0.9858  0.0712  0.1412]\n",
      "MSE loss: 91.3085\n",
      "Iteration: 22400\n",
      "Gradient: [  -3.5601   27.8458    4.7343  -66.8749 -273.4787]\n",
      "Weights: [-4.719   0.4405 -0.9884  0.0713  0.1414]\n",
      "MSE loss: 90.4872\n",
      "Iteration: 22500\n",
      "Gradient: [  -4.3113   -0.2024   22.0213   28.6037 -166.6157]\n",
      "Weights: [-4.7219  0.4484 -0.9902  0.0714  0.1415]\n",
      "MSE loss: 90.4914\n",
      "Iteration: 22600\n",
      "Gradient: [   5.8326   20.7272   49.8255    4.9036 -195.9136]\n",
      "Weights: [-4.7214  0.4402 -0.9909  0.0712  0.1415]\n",
      "MSE loss: 90.6897\n",
      "Iteration: 22700\n",
      "Gradient: [  -8.7032    1.6172   22.8092   -9.6991 -140.5989]\n",
      "Weights: [-4.7159  0.4406 -0.9913  0.0716  0.1415]\n",
      "MSE loss: 90.476\n",
      "Iteration: 22800\n",
      "Gradient: [  10.909    -7.7033   -2.7527  -68.427  -133.1548]\n",
      "Weights: [-4.7126  0.4471 -0.9921  0.071   0.1418]\n",
      "MSE loss: 90.537\n",
      "Iteration: 22900\n",
      "Gradient: [   0.4652    0.7884   -2.1295   65.3174 -243.4324]\n",
      "Weights: [-4.7238  0.4524 -0.9952  0.0711  0.1419]\n",
      "MSE loss: 90.2801\n",
      "Iteration: 23000\n",
      "Gradient: [  10.6896  -12.1945   11.9725   46.5335 -263.5601]\n",
      "Weights: [-4.7327  0.4633 -1.0005  0.0719  0.142 ]\n",
      "MSE loss: 90.1545\n",
      "Iteration: 23100\n",
      "Gradient: [ -6.0624   3.5213  30.4893  10.4026 -50.7238]\n",
      "Weights: [-4.7238  0.4688 -1.0025  0.0718  0.1419]\n",
      "MSE loss: 90.1852\n",
      "Iteration: 23200\n",
      "Gradient: [  6.4464  -9.1367  19.6651 -80.7959 233.8515]\n",
      "Weights: [-4.7361  0.4788 -1.0049  0.0716  0.142 ]\n",
      "MSE loss: 90.0377\n",
      "Iteration: 23300\n",
      "Gradient: [ 3.4112  4.3844 17.9943 52.8838 31.4678]\n",
      "Weights: [-4.7266  0.4726 -1.0045  0.072   0.142 ]\n",
      "MSE loss: 90.1205\n",
      "Iteration: 23400\n",
      "Gradient: [ 2.070000e-02 -3.841800e+00 -1.127790e+01 -3.616160e+01  3.397425e+02]\n",
      "Weights: [-4.7376  0.4702 -1.0005  0.0711  0.1419]\n",
      "MSE loss: 90.1471\n",
      "Iteration: 23500\n",
      "Gradient: [   6.0733    6.5097   20.1724  -22.9365 -117.1157]\n",
      "Weights: [-4.7231  0.4735 -1.0028  0.0711  0.1419]\n",
      "MSE loss: 90.258\n",
      "Iteration: 23600\n",
      "Gradient: [-10.2454  14.679   19.9301  31.4127   3.3862]\n",
      "Weights: [-4.7275  0.4739 -1.002   0.0704  0.142 ]\n",
      "MSE loss: 90.155\n",
      "Iteration: 23700\n",
      "Gradient: [  2.477   15.2952 -30.9303 -32.7574 -56.8746]\n",
      "Weights: [-4.734   0.472  -1.0042  0.0709  0.1424]\n",
      "MSE loss: 90.0169\n",
      "Iteration: 23800\n",
      "Gradient: [  10.7417   11.338   -10.7587  -26.5961 -317.7783]\n",
      "Weights: [-4.719   0.4705 -1.0058  0.071   0.1425]\n",
      "MSE loss: 90.1382\n",
      "Iteration: 23900\n",
      "Gradient: [  -2.502   -21.219    -1.1298  -16.664  -137.9534]\n",
      "Weights: [-4.7266  0.4676 -1.0052  0.0704  0.1427]\n",
      "MSE loss: 90.022\n",
      "Iteration: 24000\n",
      "Gradient: [  -6.8233  -11.8319    1.5788  -45.6704 -218.2654]\n",
      "Weights: [-4.7288  0.4675 -1.007   0.0709  0.1428]\n",
      "MSE loss: 90.0124\n",
      "Iteration: 24100\n",
      "Gradient: [  2.8804   0.6652   2.6651 -93.326  -37.8741]\n",
      "Weights: [-4.7372  0.4852 -1.009   0.0702  0.1427]\n",
      "MSE loss: 89.8588\n",
      "Iteration: 24200\n",
      "Gradient: [  3.6879  -2.4777 -15.5356 -52.6866 -96.7593]\n",
      "Weights: [-4.7383  0.4842 -1.0084  0.0696  0.1427]\n",
      "MSE loss: 89.925\n",
      "Iteration: 24300\n",
      "Gradient: [  -0.2017    5.629     9.9171 -100.3689   35.8358]\n",
      "Weights: [-4.739   0.4787 -1.0093  0.0701  0.1429]\n",
      "MSE loss: 90.0004\n",
      "Iteration: 24400\n",
      "Gradient: [ -7.149    3.6322 -12.618   33.5344  48.8575]\n",
      "Weights: [-4.7209  0.4724 -1.0128  0.0705  0.1433]\n",
      "MSE loss: 89.9365\n",
      "Iteration: 24500\n",
      "Gradient: [ -12.2172  -20.4844  -22.1074   -9.9667 -160.0383]\n",
      "Weights: [-4.7449  0.4803 -1.0114  0.0706  0.1431]\n",
      "MSE loss: 90.1632\n",
      "Iteration: 24600\n",
      "Gradient: [ 8.1367  2.7468 38.1698 63.5085 44.1185]\n",
      "Weights: [-4.7188  0.4835 -1.0107  0.0702  0.143 ]\n",
      "MSE loss: 90.3379\n",
      "Iteration: 24700\n",
      "Gradient: [ -7.1271  -1.6559  14.0289  36.4639 219.7058]\n",
      "Weights: [-4.7464  0.4909 -1.0106  0.0698  0.1432]\n",
      "MSE loss: 89.8847\n",
      "Iteration: 24800\n",
      "Gradient: [-5.5849  6.2805  2.2876 39.6804 72.2011]\n",
      "Weights: [-4.7304  0.4915 -1.0142  0.0695  0.1432]\n",
      "MSE loss: 89.7788\n",
      "Iteration: 24900\n",
      "Gradient: [ -4.6644   6.5579  30.1581 -15.2084 202.0759]\n",
      "Weights: [-4.752   0.5073 -1.0169  0.0688  0.1434]\n",
      "MSE loss: 89.63\n",
      "Iteration: 25000\n",
      "Gradient: [ -2.7024  10.2044  13.2032 -20.9262 188.2785]\n",
      "Weights: [-4.747   0.5117 -1.0194  0.0694  0.1436]\n",
      "MSE loss: 89.7319\n",
      "Iteration: 25100\n",
      "Gradient: [  1.3153 -15.6977 -18.4347  13.9748  66.067 ]\n",
      "Weights: [-4.7519  0.5159 -1.0219  0.0691  0.1435]\n",
      "MSE loss: 89.522\n",
      "Iteration: 25200\n",
      "Gradient: [ 4.81600e-01  1.09000e-02  6.24440e+00  9.44530e+00 -3.67893e+01]\n",
      "Weights: [-4.7525  0.5189 -1.0196  0.0679  0.1435]\n",
      "MSE loss: 89.5916\n",
      "Iteration: 25300\n",
      "Gradient: [   3.3115    9.9429    6.3298   43.4287 -194.4962]\n",
      "Weights: [-4.7653  0.5225 -1.0196  0.0684  0.1435]\n",
      "MSE loss: 89.6609\n",
      "Iteration: 25400\n",
      "Gradient: [ -1.178   14.3858  -5.9482 -81.8644 152.1188]\n",
      "Weights: [-4.7592  0.5113 -1.0222  0.0688  0.1438]\n",
      "MSE loss: 89.8796\n",
      "Iteration: 25500\n",
      "Gradient: [  -0.6379   12.3065   17.939    37.6444 -170.6797]\n",
      "Weights: [-4.7373  0.5082 -1.0217  0.0689  0.1437]\n",
      "MSE loss: 89.5815\n",
      "Iteration: 25600\n",
      "Gradient: [  -7.6223   -6.2861   -8.224   -17.3604 -271.9066]\n",
      "Weights: [-4.7517  0.5183 -1.0251  0.069   0.1439]\n",
      "MSE loss: 89.428\n",
      "Iteration: 25700\n",
      "Gradient: [ 11.0966   1.3659  28.2521 136.5368  54.8574]\n",
      "Weights: [-4.754   0.5159 -1.0251  0.0697  0.144 ]\n",
      "MSE loss: 89.4567\n",
      "Iteration: 25800\n",
      "Gradient: [ -1.8608 -10.3663   8.8133 -30.3271 102.6106]\n",
      "Weights: [-4.7602  0.5228 -1.0287  0.0694  0.1439]\n",
      "MSE loss: 89.5724\n",
      "Iteration: 25900\n",
      "Gradient: [ 11.3029   8.1288  -8.0555 -50.3532 -26.5683]\n",
      "Weights: [-4.742   0.515  -1.0276  0.0698  0.144 ]\n",
      "MSE loss: 89.3974\n",
      "Iteration: 26000\n",
      "Gradient: [  4.6259  19.1749  66.2091  36.7569 -50.2109]\n",
      "Weights: [-4.7532  0.5236 -1.0295  0.0696  0.1441]\n",
      "MSE loss: 89.3268\n",
      "Iteration: 26100\n",
      "Gradient: [ 12.3355  21.1629  31.8384  96.9445 145.9173]\n",
      "Weights: [-4.7322  0.5232 -1.0333  0.0701  0.1444]\n",
      "MSE loss: 89.8221\n",
      "Iteration: 26200\n",
      "Gradient: [  8.1513 -15.8557 -21.2273  37.391  240.2083]\n",
      "Weights: [-4.7466  0.5312 -1.0364  0.0703  0.1445]\n",
      "MSE loss: 89.3\n",
      "Iteration: 26300\n",
      "Gradient: [   0.3316   -6.0002   37.3396  -63.153  -247.3334]\n",
      "Weights: [-4.7646  0.5395 -1.0378  0.0698  0.1445]\n",
      "MSE loss: 89.2013\n",
      "Iteration: 26400\n",
      "Gradient: [   1.5325   -8.8587    0.9964   77.9696 -164.5639]\n",
      "Weights: [-4.7666  0.5363 -1.0384  0.0699  0.1448]\n",
      "MSE loss: 89.2986\n",
      "Iteration: 26500\n",
      "Gradient: [  1.7567   4.787    8.4214  35.3975 171.5733]\n",
      "Weights: [-4.7597  0.5478 -1.0436  0.0694  0.1448]\n",
      "MSE loss: 89.0404\n",
      "Iteration: 26600\n",
      "Gradient: [  -0.3059   -2.0996    1.9353   -4.0194 -130.7467]\n",
      "Weights: [-4.7581  0.5441 -1.0452  0.0704  0.145 ]\n",
      "MSE loss: 89.0065\n",
      "Iteration: 26700\n",
      "Gradient: [  -3.4957   -0.3328  -51.1269  -84.4887 -328.5975]\n",
      "Weights: [-4.7607  0.5424 -1.0428  0.0701  0.1451]\n",
      "MSE loss: 89.0905\n",
      "Iteration: 26800\n",
      "Gradient: [19.6738 -4.0225 37.0599 83.3852 29.5731]\n",
      "Weights: [-4.7568  0.5523 -1.043   0.069   0.1449]\n",
      "MSE loss: 89.1677\n",
      "Iteration: 26900\n",
      "Gradient: [ 13.4392  22.023   -6.0637 -71.4179 141.6348]\n",
      "Weights: [-4.7611  0.564  -1.0451  0.0685  0.1448]\n",
      "MSE loss: 89.2271\n",
      "Iteration: 27000\n",
      "Gradient: [-3.8913  8.6319 29.8499 11.4166 41.9108]\n",
      "Weights: [-4.7751  0.5644 -1.0432  0.0679  0.1451]\n",
      "MSE loss: 89.1467\n",
      "Iteration: 27100\n",
      "Gradient: [   7.2835    4.5554    7.7192   28.5994 -159.6716]\n",
      "Weights: [-4.7624  0.5587 -1.0458  0.0685  0.1452]\n",
      "MSE loss: 89.0195\n",
      "Iteration: 27200\n",
      "Gradient: [   2.9261   12.2052   -3.3846   19.8453 -154.7375]\n",
      "Weights: [-4.7702  0.5652 -1.049   0.0687  0.1451]\n",
      "MSE loss: 88.9574\n",
      "Iteration: 27300\n",
      "Gradient: [-11.3593   2.3204  -0.7573 -91.338  -26.14  ]\n",
      "Weights: [-4.761   0.5599 -1.051   0.0695  0.1452]\n",
      "MSE loss: 88.9055\n",
      "Iteration: 27400\n",
      "Gradient: [  2.5043  -7.7658  15.0716  81.5213 -90.4951]\n",
      "Weights: [-4.7464  0.5442 -1.0503  0.0705  0.1454]\n",
      "MSE loss: 88.9456\n",
      "Iteration: 27500\n",
      "Gradient: [ 6.98000e-01 -2.11100e-01 -9.47040e+00 -1.08000e-02  2.89352e+01]\n",
      "Weights: [-4.7458  0.5508 -1.0524  0.0701  0.1456]\n",
      "MSE loss: 88.9548\n",
      "Iteration: 27600\n",
      "Gradient: [  0.4084  -9.8343 -17.9927  67.4469  78.4253]\n",
      "Weights: [-4.7478  0.5455 -1.052   0.0698  0.1459]\n",
      "MSE loss: 88.9025\n",
      "Iteration: 27700\n",
      "Gradient: [ -5.4398   3.5636  -2.4702 -47.5301 -32.5932]\n",
      "Weights: [-4.7434  0.555  -1.0542  0.0695  0.1458]\n",
      "MSE loss: 88.9949\n",
      "Iteration: 27800\n",
      "Gradient: [ -0.8411  -5.8995  36.5173 -28.9266  41.9369]\n",
      "Weights: [-4.751   0.5556 -1.0528  0.0699  0.1455]\n",
      "MSE loss: 88.8906\n",
      "Iteration: 27900\n",
      "Gradient: [  2.8194  -5.3958 -18.2389  32.7543   8.7794]\n",
      "Weights: [-4.7496  0.555  -1.0538  0.0703  0.1456]\n",
      "MSE loss: 88.8879\n",
      "Iteration: 28000\n",
      "Gradient: [  5.0151 -15.8796 -20.6101 -22.99   -94.8488]\n",
      "Weights: [-4.7704  0.5555 -1.0524  0.0702  0.1456]\n",
      "MSE loss: 89.0605\n",
      "Iteration: 28100\n",
      "Gradient: [ -5.0521  -0.6882  19.6985 -64.0968 -25.6598]\n",
      "Weights: [-4.7544  0.5534 -1.0541  0.0706  0.1455]\n",
      "MSE loss: 88.8536\n",
      "Iteration: 28200\n",
      "Gradient: [  4.3948  16.3304  -3.367   51.5489 -10.3413]\n",
      "Weights: [-4.772   0.5594 -1.0542  0.0706  0.1456]\n",
      "MSE loss: 89.0\n",
      "Iteration: 28300\n",
      "Gradient: [  7.5984 -21.5016  31.5145 -97.1348  15.1443]\n",
      "Weights: [-4.763   0.5587 -1.0551  0.0709  0.1456]\n",
      "MSE loss: 88.8368\n",
      "Iteration: 28400\n",
      "Gradient: [  -7.229   -17.411   -28.7376 -124.2974 -276.6012]\n",
      "Weights: [-4.758   0.5636 -1.0591  0.0706  0.1455]\n",
      "MSE loss: 88.8684\n",
      "Iteration: 28500\n",
      "Gradient: [ -16.8296  -16.1995   -7.9424    9.046  -302.197 ]\n",
      "Weights: [-4.769   0.5674 -1.0562  0.0698  0.1455]\n",
      "MSE loss: 88.8554\n",
      "Iteration: 28600\n",
      "Gradient: [  1.5558  -9.5629  37.4001 143.0205 161.614 ]\n",
      "Weights: [-4.7649  0.5719 -1.0555  0.0694  0.1456]\n",
      "MSE loss: 88.8342\n",
      "Iteration: 28700\n",
      "Gradient: [   0.8023    2.5945  -17.4546 -124.7083  255.0592]\n",
      "Weights: [-4.7529  0.5653 -1.0562  0.0701  0.1456]\n",
      "MSE loss: 88.9599\n",
      "Iteration: 28800\n",
      "Gradient: [   5.6444   -5.0833    3.4807  -40.4783 -269.2462]\n",
      "Weights: [-4.7561  0.5666 -1.0581  0.0708  0.1455]\n",
      "MSE loss: 88.8165\n",
      "Iteration: 28900\n",
      "Gradient: [-3.8107  3.5553 37.7071 -0.4523 43.0994]\n",
      "Weights: [-4.7548  0.5681 -1.0606  0.0708  0.1458]\n",
      "MSE loss: 88.7713\n",
      "Iteration: 29000\n",
      "Gradient: [  4.913   -3.0699 -25.9694  82.1006 142.3899]\n",
      "Weights: [-4.7711  0.5798 -1.0619  0.0703  0.1458]\n",
      "MSE loss: 88.6916\n",
      "Iteration: 29100\n",
      "Gradient: [ -4.9786   4.7744 -42.7695  29.6407 -48.6055]\n",
      "Weights: [-4.7517  0.5669 -1.0655  0.071   0.1459]\n",
      "MSE loss: 88.9004\n",
      "Iteration: 29200\n",
      "Gradient: [  1.0666   9.6874  -6.0363  30.7424 141.9731]\n",
      "Weights: [-4.7598  0.5674 -1.0644  0.0711  0.1461]\n",
      "MSE loss: 88.6885\n",
      "Iteration: 29300\n",
      "Gradient: [ -10.2074   -7.3882  -37.6845 -126.6073 -250.9187]\n",
      "Weights: [-4.7787  0.5823 -1.0657  0.0706  0.1461]\n",
      "MSE loss: 88.7329\n",
      "Iteration: 29400\n",
      "Gradient: [ -8.1267 -11.6272  26.8181 -36.2805  -6.325 ]\n",
      "Weights: [-4.7891  0.5877 -1.0671  0.0703  0.1462]\n",
      "MSE loss: 88.9937\n",
      "Iteration: 29500\n",
      "Gradient: [ -0.9009 -10.4361   9.0301 -64.8174   9.0092]\n",
      "Weights: [-4.762   0.5931 -1.0701  0.0698  0.1461]\n",
      "MSE loss: 88.7174\n",
      "Iteration: 29600\n",
      "Gradient: [  -1.9307   12.4373   -6.1212   25.7881 -327.1087]\n",
      "Weights: [-4.7779  0.5924 -1.0727  0.0709  0.1464]\n",
      "MSE loss: 88.549\n",
      "Iteration: 29700\n",
      "Gradient: [  -3.6257    8.6316    9.0174   26.3277 -179.1218]\n",
      "Weights: [-4.7677  0.5957 -1.0731  0.0702  0.1463]\n",
      "MSE loss: 88.5166\n",
      "Iteration: 29800\n",
      "Gradient: [  -0.5366    0.8483   -1.9519  -71.8968 -197.2897]\n",
      "Weights: [-4.7702  0.5917 -1.0744  0.0706  0.1464]\n",
      "MSE loss: 88.6714\n",
      "Iteration: 29900\n",
      "Gradient: [ -5.4541   5.1552   7.1208  86.1131 202.4319]\n",
      "Weights: [-4.7724  0.6045 -1.0752  0.0704  0.1465]\n",
      "MSE loss: 88.612\n",
      "Iteration: 30000\n",
      "Gradient: [  -4.2073    8.8938   28.3052   98.982  -116.4159]\n",
      "Weights: [-4.7852  0.6095 -1.0738  0.0697  0.1465]\n",
      "MSE loss: 88.5344\n",
      "Iteration: 30100\n",
      "Gradient: [  9.8921  13.1116  19.9618 122.4004 338.2436]\n",
      "Weights: [-4.7734  0.6092 -1.0738  0.0694  0.1465]\n",
      "MSE loss: 88.7997\n",
      "Iteration: 30200\n",
      "Gradient: [ 3.7511  1.6338  4.7354 32.6754 28.3435]\n",
      "Weights: [-4.7865  0.6099 -1.0733  0.0704  0.1462]\n",
      "MSE loss: 88.6619\n",
      "Iteration: 30300\n",
      "Gradient: [  -5.0873   15.3272  -18.5207   -3.2092 -138.7799]\n",
      "Weights: [-4.8038  0.611  -1.0742  0.0702  0.1464]\n",
      "MSE loss: 88.9357\n",
      "Iteration: 30400\n",
      "Gradient: [ -1.4547   6.5906 -34.5462 -61.5152 164.3758]\n",
      "Weights: [-4.7751  0.6057 -1.0774  0.07    0.1467]\n",
      "MSE loss: 88.4091\n",
      "Iteration: 30500\n",
      "Gradient: [ 7.1947 -2.66   11.423  28.9826 62.8641]\n",
      "Weights: [-4.7739  0.6116 -1.081   0.0695  0.1471]\n",
      "MSE loss: 88.3662\n",
      "Iteration: 30600\n",
      "Gradient: [ -6.2462 -10.9119 -15.8649  57.1532 -18.348 ]\n",
      "Weights: [-4.7724  0.609  -1.0812  0.0698  0.147 ]\n",
      "MSE loss: 88.361\n",
      "Iteration: 30700\n",
      "Gradient: [  -0.7428  -14.3682    7.4882  -48.1698 -148.1641]\n",
      "Weights: [-4.7745  0.6086 -1.0803  0.0701  0.1471]\n",
      "MSE loss: 88.4102\n",
      "Iteration: 30800\n",
      "Gradient: [ -3.1961  -3.132   -0.3483 -28.158  153.4987]\n",
      "Weights: [-4.7699  0.608  -1.0831  0.0692  0.1475]\n",
      "MSE loss: 88.3311\n",
      "Iteration: 30900\n",
      "Gradient: [  5.8382   1.2824 -40.059  -39.6604  27.6334]\n",
      "Weights: [-4.7959  0.6234 -1.0833  0.0686  0.1475]\n",
      "MSE loss: 88.4114\n",
      "Iteration: 31000\n",
      "Gradient: [  8.3614   0.78    17.0443  22.7297 -60.1208]\n",
      "Weights: [-4.7846  0.6208 -1.0841  0.0689  0.1475]\n",
      "MSE loss: 88.2817\n",
      "Iteration: 31100\n",
      "Gradient: [  6.2151   1.2036 -16.1134 -57.0966 132.0692]\n",
      "Weights: [-4.773   0.6151 -1.0804  0.0678  0.1475]\n",
      "MSE loss: 88.4171\n",
      "Iteration: 31200\n",
      "Gradient: [  11.8356   10.5964   33.2248  -44.4321 -124.4232]\n",
      "Weights: [-4.7723  0.6146 -1.0832  0.0681  0.1478]\n",
      "MSE loss: 88.338\n",
      "Iteration: 31300\n",
      "Gradient: [  -8.0612   -2.6675   -6.315    53.4405 -219.3041]\n",
      "Weights: [-4.7939  0.6281 -1.0848  0.0677  0.1479]\n",
      "MSE loss: 88.3133\n",
      "Iteration: 31400\n",
      "Gradient: [   0.2915   21.9665  -22.4984   21.6331 -122.2111]\n",
      "Weights: [-4.7812  0.6214 -1.087   0.0679  0.148 ]\n",
      "MSE loss: 88.249\n",
      "Iteration: 31500\n",
      "Gradient: [-0.3326 12.7947 -4.4337 21.7726 51.7829]\n",
      "Weights: [-4.7716  0.6169 -1.0862  0.0686  0.148 ]\n",
      "MSE loss: 88.3091\n",
      "Iteration: 31600\n",
      "Gradient: [ 4.10000e-02  5.88730e+00  1.70575e+01  3.11569e+01 -4.98097e+01]\n",
      "Weights: [-4.774   0.6162 -1.0851  0.0684  0.148 ]\n",
      "MSE loss: 88.3318\n",
      "Iteration: 31700\n",
      "Gradient: [ -13.248     8.88     -0.8523   89.0939 -104.6282]\n",
      "Weights: [-4.7715  0.6131 -1.0877  0.0687  0.1482]\n",
      "MSE loss: 88.2419\n",
      "Iteration: 31800\n",
      "Gradient: [  5.9389  23.0413  28.914   34.451  113.1154]\n",
      "Weights: [-4.7621  0.6189 -1.0895  0.0686  0.1483]\n",
      "MSE loss: 88.5998\n",
      "Iteration: 31900\n",
      "Gradient: [ -2.1722  -2.4554   8.1068   7.7211 240.5418]\n",
      "Weights: [-4.7657  0.6136 -1.0887  0.0682  0.1485]\n",
      "MSE loss: 88.2892\n",
      "Iteration: 32000\n",
      "Gradient: [ -3.7387  14.5162  16.9605  47.6325 -47.4971]\n",
      "Weights: [-4.771   0.6189 -1.088   0.0685  0.1482]\n",
      "MSE loss: 88.3452\n",
      "Iteration: 32100\n",
      "Gradient: [  2.3999 -11.1162  52.6157  23.1384  58.2579]\n",
      "Weights: [-4.7923  0.629  -1.0877  0.0681  0.1481]\n",
      "MSE loss: 88.2656\n",
      "Iteration: 32200\n",
      "Gradient: [ -2.2826   4.977   23.5421  65.4266 -16.7407]\n",
      "Weights: [-4.7924  0.6332 -1.0903  0.0678  0.1481]\n",
      "MSE loss: 88.2255\n",
      "Iteration: 32300\n",
      "Gradient: [ 10.0865  -9.5743  24.9907  43.9064 160.279 ]\n",
      "Weights: [-4.7821  0.631  -1.0903  0.0673  0.1483]\n",
      "MSE loss: 88.1774\n",
      "Iteration: 32400\n",
      "Gradient: [  5.6634  -3.7134  -9.5485 -26.444  -52.5215]\n",
      "Weights: [-4.7957  0.6352 -1.0895  0.067   0.1481]\n",
      "MSE loss: 88.3271\n",
      "Iteration: 32500\n",
      "Gradient: [   5.9059   -0.6383  -25.7153  -20.0805 -110.6915]\n",
      "Weights: [-4.7953  0.6452 -1.0922  0.0666  0.1483]\n",
      "MSE loss: 88.1934\n",
      "Iteration: 32600\n",
      "Gradient: [  1.821   -3.4771 -38.0925  -3.8304 188.5676]\n",
      "Weights: [-4.7867  0.6407 -1.0934  0.067   0.1483]\n",
      "MSE loss: 88.1872\n",
      "Iteration: 32700\n",
      "Gradient: [   6.3614   42.879    26.443    16.4424 -134.6386]\n",
      "Weights: [-4.8047  0.645  -1.0939  0.0681  0.1484]\n",
      "MSE loss: 88.3214\n",
      "Iteration: 32800\n",
      "Gradient: [  8.8218 -12.785   19.3544 -26.3336 248.9156]\n",
      "Weights: [-4.7942  0.6509 -1.0978  0.0672  0.1487]\n",
      "MSE loss: 88.129\n",
      "Iteration: 32900\n",
      "Gradient: [ 6.7762 12.912  20.0175 13.1535 80.3699]\n",
      "Weights: [-4.7957  0.6562 -1.0981  0.0674  0.1483]\n",
      "MSE loss: 88.1729\n",
      "Iteration: 33000\n",
      "Gradient: [  4.2023  -7.6604 -13.2345  -8.0313 148.481 ]\n",
      "Weights: [-4.8049  0.6619 -1.1002  0.0676  0.1483]\n",
      "MSE loss: 88.1723\n",
      "Iteration: 33100\n",
      "Gradient: [-5.7764  5.9622 36.3974 64.6621 16.152 ]\n",
      "Weights: [-4.7974  0.664  -1.1034  0.0687  0.1482]\n",
      "MSE loss: 88.1549\n",
      "Iteration: 33200\n",
      "Gradient: [ -0.6279  13.8526   3.6107 -75.6676  -0.3703]\n",
      "Weights: [-4.7949  0.6615 -1.1057  0.0694  0.1482]\n",
      "MSE loss: 88.0879\n",
      "Iteration: 33300\n",
      "Gradient: [  -5.5137    0.8933  -11.4627  -17.9663 -121.5091]\n",
      "Weights: [-4.796   0.6515 -1.1052  0.0702  0.1486]\n",
      "MSE loss: 88.0287\n",
      "Iteration: 33400\n",
      "Gradient: [ -6.8354   8.0306 -10.3207  60.285  155.1733]\n",
      "Weights: [-4.7951  0.6507 -1.1042  0.0698  0.1486]\n",
      "MSE loss: 88.029\n",
      "Iteration: 33500\n",
      "Gradient: [ -2.5213  -5.0105 -14.9161 -56.8188  48.0668]\n",
      "Weights: [-4.7851  0.6459 -1.1056  0.0697  0.1487]\n",
      "MSE loss: 88.0413\n",
      "Iteration: 33600\n",
      "Gradient: [ -3.6637 -13.8717  11.3292 -58.3431 268.7469]\n",
      "Weights: [-4.7963  0.6462 -1.1022  0.0698  0.1484]\n",
      "MSE loss: 88.1173\n",
      "Iteration: 33700\n",
      "Gradient: [  6.0255   9.1383  22.114   26.6261 -74.7388]\n",
      "Weights: [-4.8112  0.6621 -1.1039  0.0696  0.1483]\n",
      "MSE loss: 88.2029\n",
      "Iteration: 33800\n",
      "Gradient: [  4.4926 -14.6155  34.0972 -13.2536  45.8105]\n",
      "Weights: [-4.7781  0.6492 -1.1032  0.0698  0.1481]\n",
      "MSE loss: 88.2381\n",
      "Iteration: 33900\n",
      "Gradient: [   0.9734   -4.1257   -8.9449 -117.8938   45.2165]\n",
      "Weights: [-4.8054  0.6584 -1.104   0.0709  0.148 ]\n",
      "MSE loss: 88.1335\n",
      "Iteration: 34000\n",
      "Gradient: [-7.255   0.897  49.3652 23.9095 30.4377]\n",
      "Weights: [-4.7953  0.6538 -1.1064  0.0722  0.1478]\n",
      "MSE loss: 88.0206\n",
      "Iteration: 34100\n",
      "Gradient: [   1.6688  -20.459    15.2201   27.863  -173.8574]\n",
      "Weights: [-4.7931  0.6524 -1.1077  0.0722  0.148 ]\n",
      "MSE loss: 87.9889\n",
      "Iteration: 34200\n",
      "Gradient: [ -0.9239   5.3299 -16.4956  20.1771 -77.4743]\n",
      "Weights: [-4.7947  0.656  -1.1069  0.0714  0.1482]\n",
      "MSE loss: 88.0397\n",
      "Iteration: 34300\n",
      "Gradient: [  9.7829  -8.5554  -5.9456  21.9644 -60.6399]\n",
      "Weights: [-4.7885  0.6633 -1.1105  0.0714  0.1482]\n",
      "MSE loss: 88.1524\n",
      "Iteration: 34400\n",
      "Gradient: [  6.8696   2.588   -7.9395 -55.1649 -33.667 ]\n",
      "Weights: [-4.7907  0.665  -1.112   0.0715  0.1483]\n",
      "MSE loss: 88.0602\n",
      "Iteration: 34500\n",
      "Gradient: [   1.7336    8.7042  -18.0828  -27.6975 -177.2391]\n",
      "Weights: [-4.7899  0.6699 -1.1164  0.0713  0.1483]\n",
      "MSE loss: 88.0406\n",
      "Iteration: 34600\n",
      "Gradient: [   5.2996   -9.6774    6.3981 -105.3157 -154.7237]\n",
      "Weights: [-4.7835  0.6677 -1.1183  0.072   0.1486]\n",
      "MSE loss: 88.0089\n",
      "Iteration: 34700\n",
      "Gradient: [-1.8917  4.5707 59.317  -4.1397 13.9787]\n",
      "Weights: [-4.7981  0.663  -1.1202  0.0732  0.1487]\n",
      "MSE loss: 88.0831\n",
      "Iteration: 34800\n",
      "Gradient: [  -1.7541  -11.3703  -19.1358    1.2693 -356.4476]\n",
      "Weights: [-4.807   0.6709 -1.1223  0.0728  0.1488]\n",
      "MSE loss: 88.1946\n",
      "Iteration: 34900\n",
      "Gradient: [ 11.2482  -0.6686 -22.1497   4.1439 186.5224]\n",
      "Weights: [-4.7916  0.6754 -1.1203  0.072   0.1489]\n",
      "MSE loss: 88.0187\n",
      "Iteration: 35000\n",
      "Gradient: [  1.2676 -17.8216  20.503   21.25    46.4614]\n",
      "Weights: [-4.7919  0.6733 -1.1203  0.0712  0.1489]\n",
      "MSE loss: 87.8753\n",
      "Iteration: 35100\n",
      "Gradient: [ 20.8928  -9.834  -17.5991  39.508  -84.2458]\n",
      "Weights: [-4.7976  0.6846 -1.1214  0.0711  0.1491]\n",
      "MSE loss: 88.1268\n",
      "Iteration: 35200\n",
      "Gradient: [ 8.1203 20.7085 43.9611 90.023  68.467 ]\n",
      "Weights: [-4.7737  0.6833 -1.1253  0.0705  0.1494]\n",
      "MSE loss: 88.8555\n",
      "Iteration: 35300\n",
      "Gradient: [ -0.6589   1.6462  32.1104 -95.2268  38.7986]\n",
      "Weights: [-4.7814  0.6807 -1.1291  0.0705  0.1498]\n",
      "MSE loss: 87.9542\n",
      "Iteration: 35400\n",
      "Gradient: [12.6697  3.2399 -3.5555 28.1769 95.1466]\n",
      "Weights: [-4.7932  0.6742 -1.1254  0.0714  0.1499]\n",
      "MSE loss: 87.8964\n",
      "Iteration: 35500\n",
      "Gradient: [  1.9077  21.614    9.9835 -26.2791  25.6726]\n",
      "Weights: [-4.7883  0.6698 -1.1262  0.0718  0.1498]\n",
      "MSE loss: 87.8277\n",
      "Iteration: 35600\n",
      "Gradient: [ -0.976   -1.9651 -25.2555 -33.6278 -96.178 ]\n",
      "Weights: [-4.8033  0.6806 -1.1265  0.071   0.1498]\n",
      "MSE loss: 87.8333\n",
      "Iteration: 35700\n",
      "Gradient: [ -0.739    1.5347  -7.8188  75.4146 196.4239]\n",
      "Weights: [-4.7947  0.6847 -1.1286  0.0703  0.1499]\n",
      "MSE loss: 87.7721\n",
      "Iteration: 35800\n",
      "Gradient: [  3.9223  -4.5864  24.1317  -6.1007 177.7833]\n",
      "Weights: [-4.8022  0.6937 -1.1279  0.0696  0.1499]\n",
      "MSE loss: 87.7991\n",
      "Iteration: 35900\n",
      "Gradient: [  5.2355   1.102  -15.6676 100.1219 -49.9252]\n",
      "Weights: [-4.8046  0.6861 -1.1259  0.0691  0.1502]\n",
      "MSE loss: 87.7995\n",
      "Iteration: 36000\n",
      "Gradient: [   3.2427    8.5849   13.9575  -30.8659 -157.7512]\n",
      "Weights: [-4.803   0.6871 -1.1251  0.0683  0.1503]\n",
      "MSE loss: 87.7872\n",
      "Iteration: 36100\n",
      "Gradient: [ 11.2284  16.6877  11.9716 128.0316 542.3456]\n",
      "Weights: [-4.7841  0.6831 -1.1259  0.0697  0.1503]\n",
      "MSE loss: 88.5769\n",
      "Iteration: 36200\n",
      "Gradient: [ -0.7251   9.7871 -42.525   35.3696 335.53  ]\n",
      "Weights: [-4.786   0.6724 -1.1246  0.0692  0.1503]\n",
      "MSE loss: 87.8455\n",
      "Iteration: 36300\n",
      "Gradient: [ -2.7526  13.425   35.2908  35.0304 -62.3586]\n",
      "Weights: [-4.7723  0.6663 -1.1259  0.0691  0.1506]\n",
      "MSE loss: 88.0342\n",
      "Iteration: 36400\n",
      "Gradient: [ 10.3171  -8.3011   5.0028 -50.0968 -37.5429]\n",
      "Weights: [-4.7986  0.6829 -1.1272  0.0689  0.1504]\n",
      "MSE loss: 87.7917\n",
      "Iteration: 36500\n",
      "Gradient: [   2.7615  -19.3401   -3.0993    9.0784 -177.5715]\n",
      "Weights: [-4.803   0.6873 -1.1288  0.0691  0.1504]\n",
      "MSE loss: 87.8037\n",
      "Iteration: 36600\n",
      "Gradient: [   7.5935   11.0724  -41.7278   -3.2646 -125.6759]\n",
      "Weights: [-4.7908  0.6823 -1.1274  0.0692  0.1504]\n",
      "MSE loss: 87.8171\n",
      "Iteration: 36700\n",
      "Gradient: [-14.2258  -1.2688 -14.3493 -24.4006  31.4306]\n",
      "Weights: [-4.8092  0.6883 -1.1282  0.0695  0.1503]\n",
      "MSE loss: 87.8572\n",
      "Iteration: 36800\n",
      "Gradient: [ -3.547   -2.9612 -22.2161  88.664  -54.5869]\n",
      "Weights: [-4.8107  0.6892 -1.1269  0.0686  0.1503]\n",
      "MSE loss: 87.9082\n",
      "Iteration: 36900\n",
      "Gradient: [ -5.6927  -6.7765 -28.9969 -72.1601 -52.6416]\n",
      "Weights: [-4.82    0.696  -1.1294  0.069   0.1504]\n",
      "MSE loss: 88.0328\n",
      "Iteration: 37000\n",
      "Gradient: [-3.114000e-01 -6.458800e+00 -6.960000e-02 -2.806240e+01  2.157717e+02]\n",
      "Weights: [-4.8044  0.6994 -1.1294  0.0685  0.1503]\n",
      "MSE loss: 87.8041\n",
      "Iteration: 37100\n",
      "Gradient: [ -2.7584  -8.6199   7.9853  -1.0132 -98.0069]\n",
      "Weights: [-4.8075  0.6983 -1.1316  0.0695  0.1502]\n",
      "MSE loss: 87.7413\n",
      "Iteration: 37200\n",
      "Gradient: [   3.1409  -27.6433  -37.4314  -27.1147 -203.893 ]\n",
      "Weights: [-4.8005  0.7043 -1.1374  0.0697  0.1504]\n",
      "MSE loss: 87.7217\n",
      "Iteration: 37300\n",
      "Gradient: [ -0.9324  18.981   15.7216  13.9012 222.7287]\n",
      "Weights: [-4.7983  0.7049 -1.1377  0.0699  0.1507]\n",
      "MSE loss: 87.9085\n",
      "Iteration: 37400\n",
      "Gradient: [ -5.5533   8.9071  15.6847 -61.6855 105.8789]\n",
      "Weights: [-4.7938  0.7005 -1.1384  0.0694  0.1507]\n",
      "MSE loss: 87.7608\n",
      "Iteration: 37500\n",
      "Gradient: [  -8.9026   -6.2103  -37.0486 -111.334   -54.353 ]\n",
      "Weights: [-4.7916  0.6974 -1.141   0.0697  0.1509]\n",
      "MSE loss: 87.8341\n",
      "Iteration: 37600\n",
      "Gradient: [-1.023180e+01  8.635100e+00 -1.524000e-01 -4.917100e+01  2.146832e+02]\n",
      "Weights: [-4.8026  0.7073 -1.1418  0.0702  0.1509]\n",
      "MSE loss: 87.6652\n",
      "Iteration: 37700\n",
      "Gradient: [  1.6232  -9.5858 -17.5978 -38.0522  99.8259]\n",
      "Weights: [-4.8016  0.713  -1.1436  0.0701  0.1509]\n",
      "MSE loss: 87.7111\n",
      "Iteration: 37800\n",
      "Gradient: [ -7.4862  20.4045  24.8029  67.048  148.1123]\n",
      "Weights: [-4.8098  0.7231 -1.1475  0.0714  0.1508]\n",
      "MSE loss: 87.8459\n",
      "Iteration: 37900\n",
      "Gradient: [ -4.3109 -11.6539  -6.748  -90.2181  33.0699]\n",
      "Weights: [-4.8238  0.7359 -1.1498  0.0706  0.1507]\n",
      "MSE loss: 87.6622\n",
      "Iteration: 38000\n",
      "Gradient: [  4.3086 -11.4699  18.0801   0.737  143.3139]\n",
      "Weights: [-4.8201  0.7293 -1.1476  0.0708  0.1507]\n",
      "MSE loss: 87.6876\n",
      "Iteration: 38100\n",
      "Gradient: [ -4.0302  19.6126  14.0119 -44.0001 -31.3224]\n",
      "Weights: [-4.8089  0.7177 -1.1439  0.0705  0.1507]\n",
      "MSE loss: 87.7067\n",
      "Iteration: 38200\n",
      "Gradient: [  -6.531     2.1413   -2.8056 -118.6618   28.1871]\n",
      "Weights: [-4.8187  0.7114 -1.141   0.07    0.1506]\n",
      "MSE loss: 87.8772\n",
      "Iteration: 38300\n",
      "Gradient: [-12.9268   4.1841 -20.1981  -4.6307  89.5666]\n",
      "Weights: [-4.8171  0.7139 -1.1383  0.0694  0.1506]\n",
      "MSE loss: 87.7231\n",
      "Iteration: 38400\n",
      "Gradient: [  2.2154 -12.3279  -0.189  -31.2026 -29.1679]\n",
      "Weights: [-4.8224  0.7219 -1.1382  0.0687  0.1506]\n",
      "MSE loss: 87.7714\n",
      "Iteration: 38500\n",
      "Gradient: [ 0.8973  4.356  12.0663 29.7576 39.0411]\n",
      "Weights: [-4.8287  0.7338 -1.1412  0.0683  0.1505]\n",
      "MSE loss: 87.8252\n",
      "Iteration: 38600\n",
      "Gradient: [  12.0296    9.5084    3.7779   50.0553 -229.4615]\n",
      "Weights: [-4.8216  0.7264 -1.1398  0.0686  0.1505]\n",
      "MSE loss: 87.7731\n",
      "Iteration: 38700\n",
      "Gradient: [  -8.3898    0.2975  -10.4353  -83.3316 -136.8562]\n",
      "Weights: [-4.8182  0.7219 -1.1384  0.0682  0.1507]\n",
      "MSE loss: 87.759\n",
      "Iteration: 38800\n",
      "Gradient: [ -2.2974   3.2635  26.8788 -20.1465 291.1201]\n",
      "Weights: [-4.8113  0.7073 -1.1376  0.0688  0.151 ]\n",
      "MSE loss: 87.7088\n",
      "Iteration: 38900\n",
      "Gradient: [  3.463   -4.69   -11.3835 -84.6618 151.1377]\n",
      "Weights: [-4.8004  0.7078 -1.1403  0.0684  0.151 ]\n",
      "MSE loss: 87.7659\n",
      "Iteration: 39000\n",
      "Gradient: [   4.8669    6.4945   17.1236   73.4519 -178.2613]\n",
      "Weights: [-4.8204  0.7262 -1.1404  0.0683  0.151 ]\n",
      "MSE loss: 87.8961\n",
      "Iteration: 39100\n",
      "Gradient: [  -0.366    20.4647   41.9466  -34.5007 -101.2317]\n",
      "Weights: [-4.817   0.7261 -1.1398  0.0677  0.151 ]\n",
      "MSE loss: 87.8596\n",
      "Iteration: 39200\n",
      "Gradient: [  4.225    3.4293  -8.0689 -46.3083  51.1524]\n",
      "Weights: [-4.8183  0.7154 -1.1381  0.068   0.1509]\n",
      "MSE loss: 87.7365\n",
      "Iteration: 39300\n",
      "Gradient: [ -6.4889  10.708   15.0375 -44.1171 127.3588]\n",
      "Weights: [-4.8147  0.7198 -1.1381  0.0679  0.1509]\n",
      "MSE loss: 87.8117\n",
      "Iteration: 39400\n",
      "Gradient: [  1.7739 -20.696  -17.9722  11.714  -60.8061]\n",
      "Weights: [-4.8061  0.7092 -1.1385  0.0678  0.1509]\n",
      "MSE loss: 87.7909\n",
      "Iteration: 39500\n",
      "Gradient: [  4.1953  -1.1479 -23.8268  28.6144  39.405 ]\n",
      "Weights: [-4.8075  0.7186 -1.1389  0.0673  0.151 ]\n",
      "MSE loss: 87.7742\n",
      "Iteration: 39600\n",
      "Gradient: [  2.1027   5.9242  19.6094  50.5873 276.6199]\n",
      "Weights: [-4.8205  0.7292 -1.1418  0.068   0.1511]\n",
      "MSE loss: 87.866\n",
      "Iteration: 39700\n",
      "Gradient: [10.1423 15.6271  9.374  18.4149 21.014 ]\n",
      "Weights: [-4.8011  0.7207 -1.1447  0.0685  0.1511]\n",
      "MSE loss: 87.7885\n",
      "Iteration: 39800\n",
      "Gradient: [  6.4906   6.5371 -23.4614 -19.5567  56.4419]\n",
      "Weights: [-4.7833  0.705  -1.1418  0.0688  0.1513]\n",
      "MSE loss: 88.132\n",
      "Iteration: 39900\n",
      "Gradient: [  2.4125   6.4366   9.5985   1.1933 158.0454]\n",
      "Weights: [-4.8141  0.7148 -1.142   0.0681  0.1514]\n",
      "MSE loss: 87.7021\n",
      "Iteration: 40000\n",
      "Gradient: [  9.0508   7.7808  11.5056  73.6015 194.0476]\n",
      "Weights: [-4.7964  0.7129 -1.1399  0.0681  0.1513]\n",
      "MSE loss: 88.0727\n",
      "Iteration: 40100\n",
      "Gradient: [  -8.8217   -6.053   -21.6137  -69.7064 -154.0256]\n",
      "Weights: [-4.802   0.7081 -1.1407  0.0679  0.1511]\n",
      "MSE loss: 87.8384\n",
      "Iteration: 40200\n",
      "Gradient: [  2.4618  -2.476  -34.7714  69.5823  25.0961]\n",
      "Weights: [-4.8046  0.7124 -1.1427  0.0689  0.1512]\n",
      "MSE loss: 87.6592\n",
      "Iteration: 40300\n",
      "Gradient: [  1.4844   4.1913 -20.0883 -70.1342  95.3176]\n",
      "Weights: [-4.8012  0.7124 -1.1424  0.0691  0.1512]\n",
      "MSE loss: 87.7667\n",
      "Iteration: 40400\n",
      "Gradient: [-4.4108  9.8961 36.856  24.6377 42.004 ]\n",
      "Weights: [-4.8066  0.709  -1.1419  0.0698  0.151 ]\n",
      "MSE loss: 87.6639\n",
      "Iteration: 40500\n",
      "Gradient: [  -2.072    -0.6527  -17.0769  -44.5144 -198.2621]\n",
      "Weights: [-4.815   0.7149 -1.1461  0.07    0.1514]\n",
      "MSE loss: 87.7226\n",
      "Iteration: 40600\n",
      "Gradient: [  3.741   -5.2116  17.1353 -57.3998 -70.9273]\n",
      "Weights: [-4.8025  0.7194 -1.1503  0.0706  0.1513]\n",
      "MSE loss: 87.6453\n",
      "Iteration: 40700\n",
      "Gradient: [ -2.6041   7.6585  10.402  -56.299  -75.2153]\n",
      "Weights: [-4.8102  0.7179 -1.1516  0.0718  0.1513]\n",
      "MSE loss: 87.6407\n",
      "Iteration: 40800\n",
      "Gradient: [   7.7944    1.5002   -3.1828   17.6158 -134.5374]\n",
      "Weights: [-4.7871  0.7179 -1.1548  0.0716  0.1511]\n",
      "MSE loss: 87.9255\n",
      "Iteration: 40900\n",
      "Gradient: [ -4.974    0.5461   6.1945 117.4677 -69.1031]\n",
      "Weights: [-4.8046  0.7244 -1.1578  0.0722  0.1512]\n",
      "MSE loss: 87.6946\n",
      "Iteration: 41000\n",
      "Gradient: [ -1.9961   5.9916 -43.618  -74.445   22.7103]\n",
      "Weights: [-4.8316  0.7369 -1.1562  0.0718  0.1512]\n",
      "MSE loss: 87.8019\n",
      "Iteration: 41100\n",
      "Gradient: [   5.2142   17.1733   -0.7425  -22.7636 -147.1679]\n",
      "Weights: [-4.799   0.7183 -1.1542  0.0723  0.1513]\n",
      "MSE loss: 87.6733\n",
      "Iteration: 41200\n",
      "Gradient: [ -2.6642   7.8738  16.0274 -12.1321   2.4174]\n",
      "Weights: [-4.802   0.7257 -1.1538  0.0714  0.1513]\n",
      "MSE loss: 87.7337\n",
      "Iteration: 41300\n",
      "Gradient: [  8.4891  16.9231   4.6153 -40.812  174.5911]\n",
      "Weights: [-4.8086  0.7249 -1.1528  0.0718  0.1511]\n",
      "MSE loss: 87.6233\n",
      "Iteration: 41400\n",
      "Gradient: [  0.2959  -1.4649  27.5749  22.7303 211.8568]\n",
      "Weights: [-4.8017  0.7162 -1.153   0.0721  0.151 ]\n",
      "MSE loss: 87.6449\n",
      "Iteration: 41500\n",
      "Gradient: [  9.4841   4.9086  48.6749  22.6302 -51.0954]\n",
      "Weights: [-4.7849  0.7171 -1.151   0.0712  0.1509]\n",
      "MSE loss: 88.1156\n",
      "Iteration: 41600\n",
      "Gradient: [  -3.9348  -13.0979  -17.32    -31.2942 -308.0572]\n",
      "Weights: [-4.8167  0.7224 -1.1508  0.0717  0.151 ]\n",
      "MSE loss: 87.6469\n",
      "Iteration: 41700\n",
      "Gradient: [   0.7067   -8.8811   -3.1624   27.5649 -359.5536]\n",
      "Weights: [-4.8119  0.7295 -1.155   0.0715  0.151 ]\n",
      "MSE loss: 87.5948\n",
      "Iteration: 41800\n",
      "Gradient: [ -8.9343  15.8412 110.5637  34.5806 -31.3084]\n",
      "Weights: [-4.8118  0.7261 -1.1522  0.0708  0.1514]\n",
      "MSE loss: 87.6072\n",
      "Iteration: 41900\n",
      "Gradient: [   3.5107  -22.0648  -13.1267  -20.9103 -148.8091]\n",
      "Weights: [-4.8017  0.7367 -1.1546  0.0699  0.1514]\n",
      "MSE loss: 88.0618\n",
      "Iteration: 42000\n",
      "Gradient: [  -5.1388  -10.9261   -0.3741 -108.0139   66.405 ]\n",
      "Weights: [-4.8294  0.7507 -1.1568  0.0695  0.1515]\n",
      "MSE loss: 87.6554\n",
      "Iteration: 42100\n",
      "Gradient: [-15.637    6.0206   4.9231  26.9586  55.9036]\n",
      "Weights: [-4.8302  0.7436 -1.1585  0.0702  0.1516]\n",
      "MSE loss: 87.767\n",
      "Iteration: 42200\n",
      "Gradient: [   1.019     1.7235   25.074    32.7086 -279.3393]\n",
      "Weights: [-4.8247  0.749  -1.1594  0.0703  0.1516]\n",
      "MSE loss: 87.601\n",
      "Iteration: 42300\n",
      "Gradient: [  6.7665   3.0447  41.5984 -54.0517 -94.127 ]\n",
      "Weights: [-4.8022  0.7392 -1.1614  0.0703  0.1518]\n",
      "MSE loss: 87.7096\n",
      "Iteration: 42400\n",
      "Gradient: [ -1.2657   4.5959  -4.3913 -38.7569 -33.998 ]\n",
      "Weights: [-4.7946  0.722  -1.1593  0.0711  0.1519]\n",
      "MSE loss: 87.7075\n",
      "Iteration: 42500\n",
      "Gradient: [  0.8785 -19.0923   2.3181  -7.2615 249.3907]\n",
      "Weights: [-4.8145  0.7345 -1.1571  0.0715  0.1516]\n",
      "MSE loss: 87.6983\n",
      "Iteration: 42600\n",
      "Gradient: [   1.7121    7.6894   21.4366  163.3259 -196.3118]\n",
      "Weights: [-4.8041  0.7359 -1.1605  0.0709  0.1517]\n",
      "MSE loss: 87.6327\n",
      "Iteration: 42700\n",
      "Gradient: [ -4.4299 -14.473   51.5054 -52.1822 -76.583 ]\n",
      "Weights: [-4.8342  0.7519 -1.1612  0.0708  0.1519]\n",
      "MSE loss: 87.7735\n",
      "Iteration: 42800\n",
      "Gradient: [   3.7607    5.5826  -24.0074  116.4583 -291.5575]\n",
      "Weights: [-4.8243  0.7501 -1.1649  0.0715  0.1518]\n",
      "MSE loss: 87.5635\n",
      "Iteration: 42900\n",
      "Gradient: [  -2.7135  -13.7866  -62.4786  -94.6118 -227.4508]\n",
      "Weights: [-4.8265  0.7481 -1.1665  0.0717  0.1519]\n",
      "MSE loss: 87.66\n",
      "Iteration: 43000\n",
      "Gradient: [ -4.8476  12.9063  22.1145  45.7748 175.6575]\n",
      "Weights: [-4.8193  0.7508 -1.1672  0.0713  0.1519]\n",
      "MSE loss: 87.5256\n",
      "Iteration: 43100\n",
      "Gradient: [   7.7811   -7.3859  -16.4982  -74.0293 -133.6603]\n",
      "Weights: [-4.8214  0.7502 -1.1678  0.0711  0.152 ]\n",
      "MSE loss: 87.5954\n",
      "Iteration: 43200\n",
      "Gradient: [ -4.9931 -17.1432 -31.3184   7.3722 -12.7309]\n",
      "Weights: [-4.8228  0.7476 -1.1706  0.0724  0.152 ]\n",
      "MSE loss: 87.7367\n",
      "Iteration: 43300\n",
      "Gradient: [   6.5561   11.9304    6.2115  -24.2121 -153.3628]\n",
      "Weights: [-4.8113  0.75   -1.1712  0.0724  0.1521]\n",
      "MSE loss: 87.5349\n",
      "Iteration: 43400\n",
      "Gradient: [ 1.779000e-01 -1.594160e+01  2.431780e+01  2.630360e+01  2.195092e+02]\n",
      "Weights: [-4.8086  0.7547 -1.1739  0.073   0.152 ]\n",
      "MSE loss: 87.6221\n",
      "Iteration: 43500\n",
      "Gradient: [ -1.0034 -17.7075  25.9765   3.9497  52.2546]\n",
      "Weights: [-4.8286  0.7621 -1.1728  0.0732  0.1519]\n",
      "MSE loss: 87.6062\n",
      "Iteration: 43600\n",
      "Gradient: [  1.5875 -10.3027 -34.1724  56.1731 232.404 ]\n",
      "Weights: [-4.8355  0.758  -1.1702  0.0731  0.1518]\n",
      "MSE loss: 87.6746\n",
      "Iteration: 43700\n",
      "Gradient: [  -1.7958    9.394    18.3396    1.4519 -141.7048]\n",
      "Weights: [-4.8234  0.7569 -1.1715  0.0737  0.1514]\n",
      "MSE loss: 87.4858\n",
      "Iteration: 43800\n",
      "Gradient: [  3.2493  13.9197 -34.7441  72.7026 201.9643]\n",
      "Weights: [-4.8189  0.7541 -1.169   0.0736  0.1514]\n",
      "MSE loss: 87.6076\n",
      "Iteration: 43900\n",
      "Gradient: [ -0.484   14.6317  22.6759 -28.0477 225.2124]\n",
      "Weights: [-4.8094  0.7485 -1.1701  0.0741  0.1515]\n",
      "MSE loss: 87.6269\n",
      "Iteration: 44000\n",
      "Gradient: [ -2.7081  13.8376  -0.4571  -6.5448 -84.9827]\n",
      "Weights: [-4.8206  0.752  -1.1696  0.0735  0.1517]\n",
      "MSE loss: 87.565\n",
      "Iteration: 44100\n",
      "Gradient: [  -2.1238    7.3692   -6.5746  -57.2412 -116.9332]\n",
      "Weights: [-4.8218  0.7583 -1.1721  0.0737  0.1517]\n",
      "MSE loss: 87.6032\n",
      "Iteration: 44200\n",
      "Gradient: [ -3.2619   8.4829   4.5346 -82.8529 198.9401]\n",
      "Weights: [-4.8353  0.7649 -1.1716  0.0734  0.1513]\n",
      "MSE loss: 87.586\n",
      "Iteration: 44300\n",
      "Gradient: [  -7.7021    5.5391  -33.5822  -37.3337 -110.7645]\n",
      "Weights: [-4.8242  0.7564 -1.1704  0.0738  0.1511]\n",
      "MSE loss: 87.5443\n",
      "Iteration: 44400\n",
      "Gradient: [  1.3287  11.3192  14.5394  27.4733 -61.0326]\n",
      "Weights: [-4.8412  0.765  -1.1704  0.0738  0.1512]\n",
      "MSE loss: 87.6711\n",
      "Iteration: 44500\n",
      "Gradient: [   1.7735   -2.1725    2.4105   45.4404 -146.7474]\n",
      "Weights: [-4.8096  0.7579 -1.1744  0.0751  0.1512]\n",
      "MSE loss: 87.711\n",
      "Iteration: 44600\n",
      "Gradient: [  5.9758  20.1619  35.7984 -13.9153 -27.8183]\n",
      "Weights: [-4.7998  0.7501 -1.1732  0.0754  0.1513]\n",
      "MSE loss: 87.9648\n",
      "Iteration: 44700\n",
      "Gradient: [ -9.0214  -0.5087   8.5224  41.9581 235.751 ]\n",
      "Weights: [-4.8311  0.7582 -1.1714  0.0742  0.1514]\n",
      "MSE loss: 87.5537\n",
      "Iteration: 44800\n",
      "Gradient: [   7.5237    4.5144   -7.1442  -28.7192 -444.3824]\n",
      "Weights: [-4.8116  0.7474 -1.171   0.0739  0.1515]\n",
      "MSE loss: 87.511\n",
      "Iteration: 44900\n",
      "Gradient: [   4.666    -0.4347  -23.9574   22.0744 -170.1153]\n",
      "Weights: [-4.825   0.7566 -1.1707  0.0746  0.1512]\n",
      "MSE loss: 87.537\n",
      "Iteration: 45000\n",
      "Gradient: [   4.1036  -29.1989   -6.6262   86.3136 -284.6635]\n",
      "Weights: [-4.8277  0.7611 -1.1733  0.0742  0.1511]\n",
      "MSE loss: 87.6486\n",
      "Iteration: 45100\n",
      "Gradient: [  -4.4349  -21.3395  -16.9301   24.8706 -208.5897]\n",
      "Weights: [-4.8208  0.7565 -1.1735  0.0742  0.1513]\n",
      "MSE loss: 87.5083\n",
      "Iteration: 45200\n",
      "Gradient: [ 4.497600e+00  3.600000e-03 -2.653970e+01 -9.092830e+01 -1.565209e+02]\n",
      "Weights: [-4.8226  0.7577 -1.1752  0.0747  0.1517]\n",
      "MSE loss: 87.5089\n",
      "Iteration: 45300\n",
      "Gradient: [   6.0133   14.1831  -16.5563 -117.9953  185.3193]\n",
      "Weights: [-4.812   0.7629 -1.1779  0.0748  0.1516]\n",
      "MSE loss: 87.6699\n",
      "Iteration: 45400\n",
      "Gradient: [  3.0408   1.4943 -26.0974 -53.7378 216.3502]\n",
      "Weights: [-4.8269  0.7664 -1.1767  0.0748  0.1514]\n",
      "MSE loss: 87.4715\n",
      "Iteration: 45500\n",
      "Gradient: [ -5.3      3.7034  -0.4623 -49.6186 -19.883 ]\n",
      "Weights: [-4.8181  0.7762 -1.1802  0.0744  0.1513]\n",
      "MSE loss: 87.7361\n",
      "Iteration: 45600\n",
      "Gradient: [ -1.2144  -0.3052   3.8304 -40.6178 -32.3001]\n",
      "Weights: [-4.83    0.7766 -1.1804  0.0743  0.1515]\n",
      "MSE loss: 87.4889\n",
      "Iteration: 45700\n",
      "Gradient: [  -3.6159  -21.3543  -10.1328 -118.1257 -222.9385]\n",
      "Weights: [-4.8533  0.7816 -1.1801  0.0742  0.1516]\n",
      "MSE loss: 87.9068\n",
      "Iteration: 45800\n",
      "Gradient: [   4.927     9.6134  -43.981   -29.9855 -190.5211]\n",
      "Weights: [-4.8254  0.7648 -1.1774  0.074   0.1516]\n",
      "MSE loss: 87.5021\n",
      "Iteration: 45900\n",
      "Gradient: [ -13.7702  -23.889   -41.8918  -11.014  -105.3195]\n",
      "Weights: [-4.8308  0.7613 -1.1777  0.0738  0.1518]\n",
      "MSE loss: 87.8802\n",
      "Iteration: 46000\n",
      "Gradient: [  -0.178   -15.4264   41.4453  -61.8981 -129.44  ]\n",
      "Weights: [-4.8244  0.7633 -1.1757  0.0736  0.1517]\n",
      "MSE loss: 87.4733\n",
      "Iteration: 46100\n",
      "Gradient: [  4.9231  15.8256 -33.1593  32.6247 -37.6657]\n",
      "Weights: [-4.8246  0.7676 -1.1734  0.0737  0.1516]\n",
      "MSE loss: 87.8885\n",
      "Iteration: 46200\n",
      "Gradient: [ -2.3262   7.6711 -15.8532 -19.3081 251.7457]\n",
      "Weights: [-4.8294  0.7537 -1.1719  0.0726  0.1518]\n",
      "MSE loss: 87.9473\n",
      "Iteration: 46300\n",
      "Gradient: [  -4.8125   -7.0425  -33.5653  -16.8078 -329.9009]\n",
      "Weights: [-4.8201  0.7514 -1.1685  0.0722  0.1518]\n",
      "MSE loss: 87.5095\n",
      "Iteration: 46400\n",
      "Gradient: [ -8.1369   4.603   41.0456 -24.7031  99.3994]\n",
      "Weights: [-4.8161  0.7499 -1.1682  0.0717  0.1519]\n",
      "MSE loss: 87.5207\n",
      "Iteration: 46500\n",
      "Gradient: [ -3.1753   9.2744 -22.1116 -91.5365 -95.7523]\n",
      "Weights: [-4.8226  0.7501 -1.1671  0.0717  0.1518]\n",
      "MSE loss: 87.5519\n",
      "Iteration: 46600\n",
      "Gradient: [ 13.6445  11.6544  16.2311 -58.6291 397.9172]\n",
      "Weights: [-4.8243  0.7617 -1.1672  0.071   0.152 ]\n",
      "MSE loss: 87.7707\n",
      "Iteration: 46700\n",
      "Gradient: [  7.6923 -24.8204  -2.69   -25.2057 101.5992]\n",
      "Weights: [-4.8123  0.7467 -1.1665  0.0721  0.1518]\n",
      "MSE loss: 87.5542\n",
      "Iteration: 46800\n",
      "Gradient: [ -4.3222  11.9161  22.331  -53.4693  70.5651]\n",
      "Weights: [-4.8169  0.7514 -1.1665  0.072   0.1518]\n",
      "MSE loss: 87.6237\n",
      "Iteration: 46900\n",
      "Gradient: [ -5.4487 -15.1637 -62.4148  60.0517 177.648 ]\n",
      "Weights: [-4.8081  0.7391 -1.1643  0.0721  0.1517]\n",
      "MSE loss: 87.555\n",
      "Iteration: 47000\n",
      "Gradient: [-10.3538  -4.2102  23.6421 -84.8588 171.4721]\n",
      "Weights: [-4.8297  0.7463 -1.1646  0.0718  0.1516]\n",
      "MSE loss: 87.8295\n",
      "Iteration: 47100\n",
      "Gradient: [  3.5932  16.7774  19.9679  77.9984 118.9481]\n",
      "Weights: [-4.8123  0.7511 -1.1649  0.0721  0.1516]\n",
      "MSE loss: 87.8506\n",
      "Iteration: 47200\n",
      "Gradient: [   4.5558    9.4523   32.7316  -30.7856 -203.1225]\n",
      "Weights: [-4.8181  0.7527 -1.1662  0.0719  0.1517]\n",
      "MSE loss: 87.6392\n",
      "Iteration: 47300\n",
      "Gradient: [ -5.6076  -0.2283 -20.9009 -51.3833  77.2083]\n",
      "Weights: [-4.8241  0.7447 -1.167   0.0719  0.1519]\n",
      "MSE loss: 87.785\n",
      "Iteration: 47400\n",
      "Gradient: [ -2.2573  -2.7846 -17.2179 -65.2099  42.5684]\n",
      "Weights: [-4.8221  0.7422 -1.1644  0.0726  0.1518]\n",
      "MSE loss: 87.6069\n",
      "Iteration: 47500\n",
      "Gradient: [  0.5515   7.5092 -18.6907  85.9377 -61.2598]\n",
      "Weights: [-4.8008  0.733  -1.1655  0.0725  0.1519]\n",
      "MSE loss: 87.6088\n",
      "Iteration: 47600\n",
      "Gradient: [   4.5404    1.5392   16.924   -44.0985 -272.0469]\n",
      "Weights: [-4.8077  0.7338 -1.1636  0.0723  0.1517]\n",
      "MSE loss: 87.5778\n",
      "Iteration: 47700\n",
      "Gradient: [  6.8523  -6.965    8.2337 -16.5765 134.9391]\n",
      "Weights: [-4.8051  0.7255 -1.1612  0.0725  0.1517]\n",
      "MSE loss: 87.6112\n",
      "Iteration: 47800\n",
      "Gradient: [ -5.8055   2.3862 -33.4004  58.2982 -35.3144]\n",
      "Weights: [-4.815   0.7358 -1.1616  0.0722  0.1515]\n",
      "MSE loss: 87.5588\n",
      "Iteration: 47900\n",
      "Gradient: [ -2.7093  11.9016   4.2162 -81.9484 337.0463]\n",
      "Weights: [-4.8173  0.7231 -1.1578  0.0724  0.1518]\n",
      "MSE loss: 87.8173\n",
      "Iteration: 48000\n",
      "Gradient: [   2.158     2.8298   32.8646    5.8667 -304.352 ]\n",
      "Weights: [-4.8147  0.7287 -1.1578  0.0713  0.1517]\n",
      "MSE loss: 87.6331\n",
      "Iteration: 48100\n",
      "Gradient: [-1.7001  3.9946  7.6892 -5.1469  7.5368]\n",
      "Weights: [-4.8061  0.7255 -1.1583  0.0715  0.1517]\n",
      "MSE loss: 87.5993\n",
      "Iteration: 48200\n",
      "Gradient: [ -16.2635    8.6649  -15.5903  -21.6496 -298.5538]\n",
      "Weights: [-4.8124  0.7256 -1.1557  0.0707  0.1514]\n",
      "MSE loss: 87.8304\n",
      "Iteration: 48300\n",
      "Gradient: [  2.6299 -10.024  -12.9285  48.534   63.9907]\n",
      "Weights: [-4.8222  0.7271 -1.1528  0.0705  0.1515]\n",
      "MSE loss: 87.7706\n",
      "Iteration: 48400\n",
      "Gradient: [ 19.1571  -2.5047  24.7225 -90.9711 -70.857 ]\n",
      "Weights: [-4.7971  0.7325 -1.1568  0.0704  0.1516]\n",
      "MSE loss: 87.9196\n",
      "Iteration: 48500\n",
      "Gradient: [ -1.1952  -0.9165 -23.2454  -2.613   67.3305]\n",
      "Weights: [-4.8164  0.74   -1.1564  0.0697  0.1516]\n",
      "MSE loss: 87.5909\n",
      "Iteration: 48600\n",
      "Gradient: [  -3.6002   -9.4189   22.7108  -19.7816 -139.2362]\n",
      "Weights: [-4.8045  0.7386 -1.1596  0.0701  0.1517]\n",
      "MSE loss: 87.6825\n",
      "Iteration: 48700\n",
      "Gradient: [ 10.2014   5.2747   3.3101 102.3669 180.9508]\n",
      "Weights: [-4.8088  0.7391 -1.1593  0.0707  0.1518]\n",
      "MSE loss: 87.7543\n",
      "Iteration: 48800\n",
      "Gradient: [   1.6028   -4.7752   47.972    65.6985 -157.7781]\n",
      "Weights: [-4.8138  0.7363 -1.1574  0.0703  0.1517]\n",
      "MSE loss: 87.5817\n",
      "Iteration: 48900\n",
      "Gradient: [ -10.8028   -4.0971  -37.1841 -204.1787   41.1526]\n",
      "Weights: [-4.8239  0.7326 -1.1585  0.0699  0.1519]\n",
      "MSE loss: 88.1244\n",
      "Iteration: 49000\n",
      "Gradient: [ 3.3674  2.4653 24.2999 47.2208 53.5458]\n",
      "Weights: [-4.8015  0.7385 -1.1609  0.0704  0.1519]\n",
      "MSE loss: 87.7765\n",
      "Iteration: 49100\n",
      "Gradient: [   1.7531   -8.0343   -8.7772   12.8183 -116.409 ]\n",
      "Weights: [-4.8165  0.7485 -1.1657  0.0705  0.1519]\n",
      "MSE loss: 87.6094\n",
      "Iteration: 49200\n",
      "Gradient: [  0.6681  -7.4895  -3.818  -49.5727 138.4781]\n",
      "Weights: [-4.814   0.7406 -1.1642  0.0713  0.1521]\n",
      "MSE loss: 87.5583\n",
      "Iteration: 49300\n",
      "Gradient: [  4.133  -14.3568 -13.3304  45.5181 114.6548]\n",
      "Weights: [-4.8108  0.7347 -1.1627  0.0712  0.1522]\n",
      "MSE loss: 87.594\n",
      "Iteration: 49400\n",
      "Gradient: [  1.3843   9.2998  31.8076  56.1536 120.6334]\n",
      "Weights: [-4.8019  0.7296 -1.1597  0.0712  0.1519]\n",
      "MSE loss: 87.6608\n",
      "Iteration: 49500\n",
      "Gradient: [   5.1937   -9.3849   23.7913   64.106  -126.3362]\n",
      "Weights: [-4.8004  0.7254 -1.1599  0.0711  0.152 ]\n",
      "MSE loss: 87.6344\n",
      "Iteration: 49600\n",
      "Gradient: [  -0.9059   -2.4713  -15.415   -80.039  -108.8155]\n",
      "Weights: [-4.7978  0.7248 -1.1601  0.071   0.1521]\n",
      "MSE loss: 87.6645\n",
      "Iteration: 49700\n",
      "Gradient: [ -5.2742  -7.4331  10.1847  14.5128 -79.4145]\n",
      "Weights: [-4.8095  0.7316 -1.1603  0.0709  0.1521]\n",
      "MSE loss: 87.6035\n",
      "Iteration: 49800\n",
      "Gradient: [  -3.2675  -15.2836  -40.3652  -30.4962 -156.4939]\n",
      "Weights: [-4.816   0.7391 -1.1607  0.0702  0.1519]\n",
      "MSE loss: 87.608\n",
      "Iteration: 49900\n",
      "Gradient: [  0.8262  23.7175  11.3907 173.5351 -68.1404]\n",
      "Weights: [-4.8142  0.7478 -1.163   0.0696  0.152 ]\n",
      "MSE loss: 87.5885\n",
      "Iteration: 50000\n",
      "Gradient: [-1.085030e+01 -1.102680e+01  3.181340e+01 -4.140000e-02 -1.511465e+02]\n",
      "Weights: [-4.8311  0.7478 -1.1605  0.07    0.152 ]\n",
      "MSE loss: 87.6792\n",
      "Iteration: 50100\n",
      "Gradient: [ 15.4109 -13.9065   1.3408 -20.6098 -74.6636]\n",
      "Weights: [-4.8188  0.7467 -1.1601  0.0698  0.1518]\n",
      "MSE loss: 87.572\n",
      "Iteration: 50200\n",
      "Gradient: [  -0.5913   -4.4876   13.2228 -100.1914   69.9138]\n",
      "Weights: [-4.8245  0.7526 -1.1623  0.0696  0.1516]\n",
      "MSE loss: 87.7148\n",
      "Iteration: 50300\n",
      "Gradient: [  -5.2085  -14.1115  -50.9495   36.2421 -192.1799]\n",
      "Weights: [-4.8292  0.7453 -1.1581  0.0696  0.1517]\n",
      "MSE loss: 87.6778\n",
      "Iteration: 50400\n",
      "Gradient: [ 1.015000e-01  1.379820e+01  2.391640e+01 -1.578350e+02 -2.948912e+02]\n",
      "Weights: [-4.8289  0.7473 -1.1581  0.0692  0.1518]\n",
      "MSE loss: 87.6385\n",
      "Iteration: 50500\n",
      "Gradient: [-11.0817  23.3714 -53.8302  45.6724  14.2645]\n",
      "Weights: [-4.8196  0.7359 -1.1599  0.07    0.1522]\n",
      "MSE loss: 87.6849\n",
      "Iteration: 50600\n",
      "Gradient: [  6.1862  -3.5839 -44.823   57.2343 -41.3453]\n",
      "Weights: [-4.8115  0.73   -1.1586  0.0697  0.1521]\n",
      "MSE loss: 87.7046\n",
      "Iteration: 50700\n",
      "Gradient: [ -0.4064   2.8388  16.4837  48.2938 151.1584]\n",
      "Weights: [-4.8004  0.7272 -1.1574  0.0697  0.1521]\n",
      "MSE loss: 87.6505\n",
      "Iteration: 50800\n",
      "Gradient: [10.5006 -7.9624  4.957   4.1355 -9.6702]\n",
      "Weights: [-4.8053  0.7237 -1.155   0.0701  0.152 ]\n",
      "MSE loss: 87.6203\n",
      "Iteration: 50900\n",
      "Gradient: [ -2.7307  16.3146 -27.5585  46.8396  19.9484]\n",
      "Weights: [-4.8024  0.7175 -1.1505  0.069   0.1519]\n",
      "MSE loss: 87.6506\n",
      "Iteration: 51000\n",
      "Gradient: [-10.3063   2.463   10.8244 127.5331  50.1195]\n",
      "Weights: [-4.7993  0.7204 -1.1521  0.0694  0.1519]\n",
      "MSE loss: 87.6842\n",
      "Iteration: 51100\n",
      "Gradient: [  -3.5468   -0.616    -7.4312    2.0425 -175.6975]\n",
      "Weights: [-4.8155  0.7254 -1.1515  0.0696  0.1518]\n",
      "MSE loss: 87.6692\n",
      "Iteration: 51200\n",
      "Gradient: [   0.8413   -6.0499   -4.8655    7.1697 -432.3442]\n",
      "Weights: [-4.8133  0.729  -1.1544  0.0699  0.1517]\n",
      "MSE loss: 87.6024\n",
      "Iteration: 51300\n",
      "Gradient: [ -2.7205   7.9559 -43.2708  10.3167 -13.1583]\n",
      "Weights: [-4.8114  0.7272 -1.1543  0.0701  0.1517]\n",
      "MSE loss: 87.5956\n",
      "Iteration: 51400\n",
      "Gradient: [ -0.0814   2.6497  -5.2858  51.8207 -71.6119]\n",
      "Weights: [-4.8027  0.7193 -1.1567  0.0705  0.1518]\n",
      "MSE loss: 87.8451\n",
      "Iteration: 51500\n",
      "Gradient: [  -3.0119  -12.0776  -18.1144  -37.6964 -147.6442]\n",
      "Weights: [-4.7975  0.7245 -1.1554  0.0701  0.1519]\n",
      "MSE loss: 87.7739\n",
      "Iteration: 51600\n",
      "Gradient: [ -3.2238   2.3864 -13.1004  61.1641 453.3211]\n",
      "Weights: [-4.8016  0.7233 -1.1528  0.0702  0.1517]\n",
      "MSE loss: 87.6911\n",
      "Iteration: 51700\n",
      "Gradient: [  8.0162  13.174    2.4661  19.7375 -93.676 ]\n",
      "Weights: [-4.8042  0.7276 -1.153   0.0701  0.1518]\n",
      "MSE loss: 87.8842\n",
      "Iteration: 51800\n",
      "Gradient: [ -1.1975  18.581   23.2888 126.6564 -75.6365]\n",
      "Weights: [-4.813   0.7229 -1.1501  0.0697  0.1516]\n",
      "MSE loss: 87.6321\n",
      "Iteration: 51900\n",
      "Gradient: [ -0.4176  11.3317 -17.3627 -10.4991 -11.2283]\n",
      "Weights: [-4.8112  0.7195 -1.148   0.0697  0.1512]\n",
      "MSE loss: 87.6738\n",
      "Iteration: 52000\n",
      "Gradient: [ -5.5189 -11.3837  42.6592 -38.0594  11.712 ]\n",
      "Weights: [-4.8032  0.7222 -1.15    0.0697  0.1512]\n",
      "MSE loss: 87.697\n",
      "Iteration: 52100\n",
      "Gradient: [  6.8537  -6.5713 -55.5978 -31.4727  70.6239]\n",
      "Weights: [-4.8158  0.7311 -1.1514  0.0707  0.1513]\n",
      "MSE loss: 87.7465\n",
      "Iteration: 52200\n",
      "Gradient: [ -5.6884   8.4879 -26.9654  40.7716  11.3072]\n",
      "Weights: [-4.8115  0.7243 -1.1514  0.0706  0.1512]\n",
      "MSE loss: 87.5993\n",
      "Iteration: 52300\n",
      "Gradient: [ -3.0596  -1.7106   4.9958  62.8896 302.697 ]\n",
      "Weights: [-4.8017  0.7199 -1.1483  0.0706  0.1512]\n",
      "MSE loss: 87.8681\n",
      "Iteration: 52400\n",
      "Gradient: [-11.899    1.1807  31.0028  12.3219  93.1851]\n",
      "Weights: [-4.809   0.7175 -1.148   0.0704  0.1514]\n",
      "MSE loss: 87.6666\n",
      "Iteration: 52500\n",
      "Gradient: [ 13.4458   5.5332  31.1135  21.5877 129.2702]\n",
      "Weights: [-4.7976  0.7184 -1.1485  0.0704  0.1512]\n",
      "MSE loss: 87.8459\n",
      "Iteration: 52600\n",
      "Gradient: [-10.5033   5.3556  -5.2729  56.0954 -79.6121]\n",
      "Weights: [-4.8096  0.7148 -1.149   0.0702  0.1516]\n",
      "MSE loss: 87.6815\n",
      "Iteration: 52700\n",
      "Gradient: [ -9.7263  -0.8085 -56.1258 105.6145 117.1281]\n",
      "Weights: [-4.815   0.7162 -1.1485  0.07    0.1515]\n",
      "MSE loss: 87.7657\n",
      "Iteration: 52800\n",
      "Gradient: [   7.011    13.6641  -31.0105  -55.3973 -362.2811]\n",
      "Weights: [-4.7819  0.7137 -1.151   0.0707  0.1515]\n",
      "MSE loss: 88.2771\n",
      "Iteration: 52900\n",
      "Gradient: [  3.252   28.5601  50.3207 172.0172  88.6057]\n",
      "Weights: [-4.7933  0.7157 -1.1491  0.0707  0.1514]\n",
      "MSE loss: 87.9839\n",
      "Iteration: 53000\n",
      "Gradient: [-7.6244 10.0604 -2.1826 32.641  48.3121]\n",
      "Weights: [-4.798   0.7071 -1.1468  0.0698  0.1513]\n",
      "MSE loss: 87.7699\n",
      "Iteration: 53100\n",
      "Gradient: [  -7.2868   -7.2576  -13.8832 -113.5899   61.4405]\n",
      "Weights: [-4.8166  0.7092 -1.1442  0.0693  0.1515]\n",
      "MSE loss: 87.9729\n",
      "Iteration: 53200\n",
      "Gradient: [ -0.3323 -17.4099  -4.7411  61.7281  18.4613]\n",
      "Weights: [-4.7981  0.6976 -1.1414  0.0696  0.1515]\n",
      "MSE loss: 87.7446\n",
      "Iteration: 53300\n",
      "Gradient: [  -9.4303    6.241    -8.4677   -4.0502 -137.6523]\n",
      "Weights: [-4.7947  0.6935 -1.1421  0.0696  0.1514]\n",
      "MSE loss: 87.9248\n",
      "Iteration: 53400\n",
      "Gradient: [ -7.3943   3.4621 -20.9084  12.8612  49.4843]\n",
      "Weights: [-4.7895  0.695  -1.1409  0.0697  0.1515]\n",
      "MSE loss: 87.8333\n",
      "Iteration: 53500\n",
      "Gradient: [ -4.267   -9.1619   4.4362 -11.1262  41.6764]\n",
      "Weights: [-4.7903  0.6928 -1.1407  0.0702  0.1513]\n",
      "MSE loss: 87.7679\n",
      "Iteration: 53600\n",
      "Gradient: [  8.6482   7.9458   8.4217  10.172  180.7068]\n",
      "Weights: [-4.7885  0.7076 -1.1437  0.0705  0.151 ]\n",
      "MSE loss: 88.0856\n",
      "Iteration: 53700\n",
      "Gradient: [ -0.2895  12.7023 -23.5303 -77.9599 283.1572]\n",
      "Weights: [-4.8059  0.7093 -1.1437  0.0698  0.1512]\n",
      "MSE loss: 87.6602\n",
      "Iteration: 53800\n",
      "Gradient: [  2.5873  -7.3625 -45.7215 -60.6325  96.363 ]\n",
      "Weights: [-4.8129  0.7191 -1.1457  0.0692  0.1514]\n",
      "MSE loss: 87.6456\n",
      "Iteration: 53900\n",
      "Gradient: [  2.5157  -6.5985  25.5714 116.5287 253.4741]\n",
      "Weights: [-4.8044  0.7303 -1.148   0.0693  0.1512]\n",
      "MSE loss: 88.084\n",
      "Iteration: 54000\n",
      "Gradient: [ -1.4312  -5.6803  17.4111  48.5037 110.4631]\n",
      "Weights: [-4.8256  0.7291 -1.1435  0.069   0.1511]\n",
      "MSE loss: 87.8674\n",
      "Iteration: 54100\n",
      "Gradient: [  8.0742  10.5267  59.2707  82.652  177.3564]\n",
      "Weights: [-4.8001  0.7194 -1.1469  0.0698  0.1512]\n",
      "MSE loss: 87.7915\n",
      "Iteration: 54200\n",
      "Gradient: [   1.4831    2.9077  -11.8915 -148.1011   35.2109]\n",
      "Weights: [-4.8171  0.72   -1.1466  0.0696  0.1511]\n",
      "MSE loss: 87.8027\n",
      "Iteration: 54300\n",
      "Gradient: [  6.6763   2.5176  45.1027  32.8745 212.965 ]\n",
      "Weights: [-4.8137  0.7229 -1.1467  0.0694  0.1514]\n",
      "MSE loss: 87.7103\n",
      "Iteration: 54400\n",
      "Gradient: [ -1.6878  10.8215  -2.3184 -71.1232  95.1434]\n",
      "Weights: [-4.8166  0.7259 -1.1492  0.0694  0.1515]\n",
      "MSE loss: 87.632\n",
      "Iteration: 54500\n",
      "Gradient: [  -5.3668   -1.7991   31.95     22.0784 -154.4154]\n",
      "Weights: [-4.816   0.7394 -1.1553  0.07    0.1516]\n",
      "MSE loss: 87.6949\n",
      "Iteration: 54600\n",
      "Gradient: [-10.7267  -9.9781  -4.604   23.8174 -97.7439]\n",
      "Weights: [-4.8239  0.7363 -1.1559  0.0694  0.1515]\n",
      "MSE loss: 88.0141\n",
      "Iteration: 54700\n",
      "Gradient: [  6.3377  17.2137 -13.8668 142.7426 -47.5356]\n",
      "Weights: [-4.809   0.7395 -1.1585  0.0699  0.1518]\n",
      "MSE loss: 87.6406\n",
      "Iteration: 54800\n",
      "Gradient: [-16.537  -17.5688  42.642  -37.5968  53.739 ]\n",
      "Weights: [-4.8213  0.7371 -1.1536  0.0697  0.1517]\n",
      "MSE loss: 87.6819\n",
      "Iteration: 54900\n",
      "Gradient: [   4.1271   11.807    32.0723 -170.9021  198.5753]\n",
      "Weights: [-4.8091  0.7409 -1.1577  0.07    0.1516]\n",
      "MSE loss: 87.7309\n",
      "Iteration: 55000\n",
      "Gradient: [   2.5858   18.8523   20.1856   14.3546 -106.5188]\n",
      "Weights: [-4.8022  0.7376 -1.1591  0.0712  0.1517]\n",
      "MSE loss: 88.0431\n",
      "Iteration: 55100\n",
      "Gradient: [  -8.0969   -7.8983   27.3312  -15.3792 -224.7811]\n",
      "Weights: [-4.8118  0.7391 -1.1605  0.0706  0.1517]\n",
      "MSE loss: 87.5771\n",
      "Iteration: 55200\n",
      "Gradient: [   8.4411   13.5571    5.8098    8.5374 -192.6749]\n",
      "Weights: [-4.8016  0.7381 -1.1637  0.0717  0.1517]\n",
      "MSE loss: 87.6674\n",
      "Iteration: 55300\n",
      "Gradient: [ -4.245  -20.3133 -48.3524  15.3226 346.136 ]\n",
      "Weights: [-4.8248  0.7465 -1.1622  0.0716  0.1515]\n",
      "MSE loss: 87.5652\n",
      "Iteration: 55400\n",
      "Gradient: [ -7.0965 -14.9801  -9.5587  -5.0908  48.85  ]\n",
      "Weights: [-4.8285  0.7502 -1.1656  0.0718  0.1514]\n",
      "MSE loss: 87.8173\n",
      "Iteration: 55500\n",
      "Gradient: [   1.3355  -14.1267  -14.4258   16.5803 -253.4127]\n",
      "Weights: [-4.8195  0.7478 -1.1687  0.0722  0.1517]\n",
      "MSE loss: 87.7065\n",
      "Iteration: 55600\n",
      "Gradient: [  -4.8053  -19.4009  -39.8776  -54.6746 -136.9398]\n",
      "Weights: [-4.834   0.7587 -1.1697  0.072   0.1518]\n",
      "MSE loss: 87.6517\n",
      "Iteration: 55700\n",
      "Gradient: [  9.9145   3.8015  -3.5999 -19.267   30.9398]\n",
      "Weights: [-4.8233  0.7547 -1.1693  0.072   0.1519]\n",
      "MSE loss: 87.5139\n",
      "Iteration: 55800\n",
      "Gradient: [  5.8682  12.4878  -4.1332   1.8421 137.235 ]\n",
      "Weights: [-4.8264  0.7608 -1.1701  0.0716  0.152 ]\n",
      "MSE loss: 87.5572\n",
      "Iteration: 55900\n",
      "Gradient: [  4.7071 -13.0582 -50.3975  10.2913  82.5492]\n",
      "Weights: [-4.8133  0.758  -1.1758  0.0728  0.152 ]\n",
      "MSE loss: 87.5526\n",
      "Iteration: 56000\n",
      "Gradient: [  -1.3978   17.5242   -5.9506    0.7305 -100.8266]\n",
      "Weights: [-4.8207  0.7607 -1.1736  0.0724  0.152 ]\n",
      "MSE loss: 87.4983\n",
      "Iteration: 56100\n",
      "Gradient: [  0.5073  -1.5786  27.1614  66.3893 -28.2395]\n",
      "Weights: [-4.8302  0.7602 -1.1721  0.0728  0.152 ]\n",
      "MSE loss: 87.5694\n",
      "Iteration: 56200\n",
      "Gradient: [-2.0534  0.1573 58.8734 49.8223  3.5404]\n",
      "Weights: [-4.829   0.7617 -1.1683  0.0717  0.1517]\n",
      "MSE loss: 87.5511\n",
      "Iteration: 56300\n",
      "Gradient: [  2.6628   7.3754 -24.0703 -93.3464 203.3725]\n",
      "Weights: [-4.8299  0.7624 -1.169   0.0712  0.1518]\n",
      "MSE loss: 87.5449\n",
      "Iteration: 56400\n",
      "Gradient: [ -18.2509  -37.3528  -20.2025 -163.9502   41.575 ]\n",
      "Weights: [-4.8446  0.7716 -1.1727  0.0715  0.1518]\n",
      "MSE loss: 87.8349\n",
      "Iteration: 56500\n",
      "Gradient: [ -0.2736  -7.0935 -14.3871 -15.0359  46.017 ]\n",
      "Weights: [-4.8414  0.7656 -1.1718  0.0723  0.1517]\n",
      "MSE loss: 87.7786\n",
      "Iteration: 56600\n",
      "Gradient: [ -1.3896  -9.2738   8.0722 120.1899  49.6722]\n",
      "Weights: [-4.8251  0.7641 -1.1706  0.0717  0.1517]\n",
      "MSE loss: 87.5391\n",
      "Iteration: 56700\n",
      "Gradient: [  0.591  -21.7357  -8.9859 -46.9711   6.5991]\n",
      "Weights: [-4.8158  0.7613 -1.1714  0.072   0.1518]\n",
      "MSE loss: 87.6441\n",
      "Iteration: 56800\n",
      "Gradient: [   6.4989   -4.7449   27.2087  -82.6647 -192.374 ]\n",
      "Weights: [-4.8325  0.7665 -1.1711  0.0711  0.1519]\n",
      "MSE loss: 87.5823\n",
      "Iteration: 56900\n",
      "Gradient: [ -6.9757  -0.897   22.8786 -35.1881  45.2127]\n",
      "Weights: [-4.8379  0.7648 -1.1696  0.0712  0.152 ]\n",
      "MSE loss: 87.6407\n",
      "Iteration: 57000\n",
      "Gradient: [ 0.3308  7.3033 30.3782 41.6822 40.1898]\n",
      "Weights: [-4.8244  0.7698 -1.1722  0.0716  0.152 ]\n",
      "MSE loss: 87.7867\n",
      "Iteration: 57100\n",
      "Gradient: [ -6.5968 -18.4478 -27.9947  23.9981 103.0489]\n",
      "Weights: [-4.8294  0.7682 -1.1735  0.071   0.152 ]\n",
      "MSE loss: 87.6085\n",
      "Iteration: 57200\n",
      "Gradient: [ 3.9871 -6.4198 14.5012 30.0557 80.2611]\n",
      "Weights: [-4.8103  0.7478 -1.1719  0.0717  0.1523]\n",
      "MSE loss: 87.5966\n",
      "Iteration: 57300\n",
      "Gradient: [-0.8847 13.7524 14.8467 -4.0645 91.246 ]\n",
      "Weights: [-4.8155  0.7545 -1.1704  0.0717  0.1522]\n",
      "MSE loss: 87.5955\n",
      "Iteration: 57400\n",
      "Gradient: [ -3.8811  14.3232  11.8615  33.2793 164.2046]\n",
      "Weights: [-4.8066  0.7609 -1.1765  0.0723  0.1525]\n",
      "MSE loss: 87.9267\n",
      "Iteration: 57500\n",
      "Gradient: [  9.5923  10.8192  -4.8146  -9.7101 181.5417]\n",
      "Weights: [-4.8288  0.7668 -1.1743  0.0717  0.1525]\n",
      "MSE loss: 87.6831\n",
      "Iteration: 57600\n",
      "Gradient: [ -7.345  -17.0817  12.5637  33.0047 335.6737]\n",
      "Weights: [-4.8273  0.7526 -1.1708  0.072   0.1523]\n",
      "MSE loss: 87.6392\n",
      "Iteration: 57700\n",
      "Gradient: [  0.4833   7.7107  19.1422 -32.8224  48.5423]\n",
      "Weights: [-4.809   0.7475 -1.1675  0.0717  0.1521]\n",
      "MSE loss: 87.6894\n",
      "Iteration: 57800\n",
      "Gradient: [  4.7612  10.6585  18.2337  97.3788 -42.7308]\n",
      "Weights: [-4.8068  0.7417 -1.1657  0.072   0.1521]\n",
      "MSE loss: 87.7851\n",
      "Iteration: 57900\n",
      "Gradient: [ 10.3871   8.075   20.6668  92.8682 168.0728]\n",
      "Weights: [-4.8139  0.7396 -1.1654  0.0718  0.1523]\n",
      "MSE loss: 87.6298\n",
      "Iteration: 58000\n",
      "Gradient: [  0.7458   0.493    0.0977 -95.2635  -8.5764]\n",
      "Weights: [-4.8185  0.7411 -1.1658  0.0711  0.1522]\n",
      "MSE loss: 87.6764\n",
      "Iteration: 58100\n",
      "Gradient: [ -5.0686 -21.5305   4.8734 -49.3892  15.5924]\n",
      "Weights: [-4.8143  0.7421 -1.1686  0.0715  0.1523]\n",
      "MSE loss: 87.6587\n",
      "Iteration: 58200\n",
      "Gradient: [ -7.3873  -3.3785  60.0467 -30.939  133.4603]\n",
      "Weights: [-4.8306  0.7602 -1.1691  0.0716  0.152 ]\n",
      "MSE loss: 87.5616\n",
      "Iteration: 58300\n",
      "Gradient: [   2.8241   -5.4779  -52.1202    3.9156 -138.9687]\n",
      "Weights: [-4.8255  0.7658 -1.174   0.0713  0.1522]\n",
      "MSE loss: 87.5043\n",
      "Iteration: 58400\n",
      "Gradient: [  0.6412 -11.7716 -18.3566 -73.9243 120.2897]\n",
      "Weights: [-4.8197  0.7596 -1.1752  0.0719  0.1524]\n",
      "MSE loss: 87.5032\n",
      "Iteration: 58500\n",
      "Gradient: [  5.8536 -28.3545   0.5243  53.7184 194.194 ]\n",
      "Weights: [-4.8238  0.7535 -1.1716  0.0712  0.1524]\n",
      "MSE loss: 87.6226\n",
      "Iteration: 58600\n",
      "Gradient: [ -5.5269 -20.9804  19.3179  42.603  448.4193]\n",
      "Weights: [-4.8125  0.75   -1.171   0.0708  0.1526]\n",
      "MSE loss: 87.5481\n",
      "Iteration: 58700\n",
      "Gradient: [  -1.3001   18.606     2.6654   21.9141 -509.4883]\n",
      "Weights: [-4.8313  0.7581 -1.1697  0.071   0.1522]\n",
      "MSE loss: 87.6447\n",
      "Iteration: 58800\n",
      "Gradient: [-5.23000e-02 -1.46754e+01  2.95050e+01 -5.79590e+01  8.35872e+01]\n",
      "Weights: [-4.8216  0.7564 -1.1689  0.0707  0.1521]\n",
      "MSE loss: 87.5305\n",
      "Iteration: 58900\n",
      "Gradient: [  -5.6427  -10.3607  -27.2868  -33.5572 -218.756 ]\n",
      "Weights: [-4.8249  0.7504 -1.168   0.0712  0.1521]\n",
      "MSE loss: 87.6147\n",
      "Iteration: 59000\n",
      "Gradient: [  7.4037 -27.1314 -24.0279  65.965   91.2378]\n",
      "Weights: [-4.8072  0.7472 -1.1684  0.0707  0.1523]\n",
      "MSE loss: 87.596\n",
      "Iteration: 59100\n",
      "Gradient: [  0.1546   8.4157   7.9901 -13.3199  74.7184]\n",
      "Weights: [-4.822   0.7573 -1.1689  0.0703  0.1523]\n",
      "MSE loss: 87.5271\n",
      "Iteration: 59200\n",
      "Gradient: [-1.8667 -5.8382  2.4495 28.3546 10.4415]\n",
      "Weights: [-4.8309  0.7555 -1.1679  0.0694  0.1524]\n",
      "MSE loss: 87.8904\n",
      "Iteration: 59300\n",
      "Gradient: [  -6.0881   -2.0478   21.0059  -29.4523 -103.8643]\n",
      "Weights: [-4.8262  0.7634 -1.1677  0.069   0.1524]\n",
      "MSE loss: 87.5616\n",
      "Iteration: 59400\n",
      "Gradient: [  2.1112   5.4873  25.5291  11.5736 196.7867]\n",
      "Weights: [-4.8127  0.7532 -1.1646  0.0691  0.1522]\n",
      "MSE loss: 87.6532\n",
      "Iteration: 59500\n",
      "Gradient: [  5.9209  -1.1153  48.0352  24.0895 -92.9366]\n",
      "Weights: [-4.8208  0.7512 -1.1628  0.0691  0.1523]\n",
      "MSE loss: 87.5696\n",
      "Iteration: 59600\n",
      "Gradient: [  8.9211   6.9751  51.8055 -66.9502  68.4215]\n",
      "Weights: [-4.8326  0.7587 -1.161   0.0687  0.1522]\n",
      "MSE loss: 87.7292\n",
      "Iteration: 59700\n",
      "Gradient: [-6.5168  7.3761 -7.0865 12.9783 53.6398]\n",
      "Weights: [-4.8181  0.7432 -1.1591  0.0683  0.1522]\n",
      "MSE loss: 87.5948\n",
      "Iteration: 59800\n",
      "Gradient: [  -2.8175   -6.6401  -22.671    53.8775 -220.6253]\n",
      "Weights: [-4.8268  0.7456 -1.1586  0.0688  0.1521]\n",
      "MSE loss: 87.6429\n",
      "Iteration: 59900\n",
      "Gradient: [  2.8115  -5.3041  -5.1932 -17.4276 141.678 ]\n",
      "Weights: [-4.8145  0.742  -1.1562  0.0684  0.152 ]\n",
      "MSE loss: 87.6476\n",
      "Iteration: 60000\n",
      "Gradient: [ 13.6835 -12.3379  -5.308   24.7604  62.1235]\n",
      "Weights: [-4.8166  0.7463 -1.1538  0.0678  0.152 ]\n",
      "MSE loss: 87.9421\n",
      "Iteration: 60100\n",
      "Gradient: [ -10.1486    1.4847   14.0304 -126.4907  194.0209]\n",
      "Weights: [-4.8213  0.7485 -1.158   0.0678  0.1519]\n",
      "MSE loss: 87.6776\n",
      "Iteration: 60200\n",
      "Gradient: [ -4.2956  -8.6861  19.8161 103.8639 -28.8223]\n",
      "Weights: [-4.8079  0.7408 -1.1595  0.0687  0.1522]\n",
      "MSE loss: 87.6438\n",
      "Iteration: 60300\n",
      "Gradient: [   5.5536   -2.2717   74.0616  110.1058 -105.9649]\n",
      "Weights: [-4.8065  0.7487 -1.1619  0.0691  0.1522]\n",
      "MSE loss: 87.9037\n",
      "Iteration: 60400\n",
      "Gradient: [ -1.5548  -6.167   11.4911 -73.2258 358.5808]\n",
      "Weights: [-4.8157  0.746  -1.1629  0.069   0.1525]\n",
      "MSE loss: 87.5758\n",
      "Iteration: 60500\n",
      "Gradient: [ -2.6158 -18.0551   5.3227  79.2039 113.5201]\n",
      "Weights: [-4.829   0.7504 -1.162   0.0691  0.1523]\n",
      "MSE loss: 87.6304\n",
      "Iteration: 60600\n",
      "Gradient: [  8.7548   7.0288 -39.5955 -11.8901 285.8884]\n",
      "Weights: [-4.8058  0.7483 -1.1632  0.0683  0.1523]\n",
      "MSE loss: 87.7628\n",
      "Iteration: 60700\n",
      "Gradient: [ -5.4596   8.2458  14.4352 128.6444 196.8769]\n",
      "Weights: [-4.8188  0.7638 -1.1635  0.0684  0.1522]\n",
      "MSE loss: 88.021\n",
      "Iteration: 60800\n",
      "Gradient: [-17.8761   6.1366  54.8094 -70.818  329.3514]\n",
      "Weights: [-4.8407  0.7609 -1.1637  0.0688  0.1523]\n",
      "MSE loss: 87.7623\n",
      "Iteration: 60900\n",
      "Gradient: [  -5.366   -13.6834  -23.1847  -82.5433 -147.4539]\n",
      "Weights: [-4.8103  0.7424 -1.1644  0.0694  0.1524]\n",
      "MSE loss: 87.627\n",
      "Iteration: 61000\n",
      "Gradient: [  -5.0152  -10.9045  -12.6903  -54.0414 -135.4428]\n",
      "Weights: [-4.8367  0.7629 -1.1643  0.0695  0.1522]\n",
      "MSE loss: 87.7075\n",
      "Iteration: 61100\n",
      "Gradient: [ -1.1939  -6.75    33.9837 -39.4345  -6.5879]\n",
      "Weights: [-4.8301  0.7707 -1.1679  0.0687  0.1522]\n",
      "MSE loss: 87.6234\n",
      "Iteration: 61200\n",
      "Gradient: [  9.5159   5.8107  19.2855 -27.115   91.8664]\n",
      "Weights: [-4.8236  0.7623 -1.165   0.0691  0.1523]\n",
      "MSE loss: 87.8416\n",
      "Iteration: 61300\n",
      "Gradient: [  5.0075  -0.0821 -36.5103 -56.9963  -5.908 ]\n",
      "Weights: [-4.8076  0.754  -1.1677  0.069   0.1526]\n",
      "MSE loss: 87.7324\n",
      "Iteration: 61400\n",
      "Gradient: [  4.5335  18.8126 -13.8758  -3.6614 -59.3964]\n",
      "Weights: [-4.8246  0.7587 -1.1654  0.068   0.1528]\n",
      "MSE loss: 87.5826\n",
      "Iteration: 61500\n",
      "Gradient: [ -5.6551  17.5975  17.3258  88.3162 -33.1258]\n",
      "Weights: [-4.8133  0.7532 -1.1649  0.0679  0.1527]\n",
      "MSE loss: 87.6344\n",
      "Iteration: 61600\n",
      "Gradient: [ -3.5105  -3.5066  16.9501 -18.2607  94.1717]\n",
      "Weights: [-4.8361  0.7579 -1.168   0.0687  0.1529]\n",
      "MSE loss: 87.8209\n",
      "Iteration: 61700\n",
      "Gradient: [  8.5745  -4.2903 -12.7897  73.11   273.5671]\n",
      "Weights: [-4.8086  0.7567 -1.1677  0.0684  0.1527]\n",
      "MSE loss: 87.7469\n",
      "Iteration: 61800\n",
      "Gradient: [ -6.6346 -19.3901 -31.6579 -63.7235 238.5705]\n",
      "Weights: [-4.8323  0.7509 -1.1664  0.0689  0.1528]\n",
      "MSE loss: 87.9372\n",
      "Iteration: 61900\n",
      "Gradient: [ -5.4041   9.671  -11.5374 105.7471  62.4468]\n",
      "Weights: [-4.8236  0.7492 -1.1668  0.0684  0.1529]\n",
      "MSE loss: 87.8004\n",
      "Iteration: 62000\n",
      "Gradient: [ -8.1806  -7.7969   8.0374 -75.9211 -33.2123]\n",
      "Weights: [-4.8227  0.7486 -1.1676  0.0685  0.1528]\n",
      "MSE loss: 88.0942\n",
      "Iteration: 62100\n",
      "Gradient: [ 15.6533   5.7458  20.675  -67.0967 -36.7787]\n",
      "Weights: [-4.8223  0.7486 -1.1647  0.0683  0.153 ]\n",
      "MSE loss: 87.6203\n",
      "Iteration: 62200\n",
      "Gradient: [  5.2844 -10.4615   7.4983  14.9284   5.0254]\n",
      "Weights: [-4.8266  0.7481 -1.1641  0.0686  0.1529]\n",
      "MSE loss: 87.6886\n",
      "Iteration: 62300\n",
      "Gradient: [  6.0521  12.1683  36.4696 -52.8607 111.8738]\n",
      "Weights: [-4.8195  0.7615 -1.1653  0.0676  0.1528]\n",
      "MSE loss: 87.8146\n",
      "Iteration: 62400\n",
      "Gradient: [ -3.767    6.7736 -25.4076  67.9265 -62.9191]\n",
      "Weights: [-4.8059  0.7521 -1.1675  0.0681  0.1529]\n",
      "MSE loss: 87.707\n",
      "Iteration: 62500\n",
      "Gradient: [ -2.0009  -7.5315 -13.7933 -19.2577 133.7205]\n",
      "Weights: [-4.8268  0.7572 -1.1676  0.0687  0.1529]\n",
      "MSE loss: 87.5919\n",
      "Iteration: 62600\n",
      "Gradient: [  0.2881   4.7961 -54.2455  20.0017 246.1046]\n",
      "Weights: [-4.8162  0.7535 -1.1668  0.0683  0.1527]\n",
      "MSE loss: 87.5832\n",
      "Iteration: 62700\n",
      "Gradient: [  5.4372  -1.2873  35.485  -28.6339 470.972 ]\n",
      "Weights: [-4.8217  0.7582 -1.167   0.0686  0.1531]\n",
      "MSE loss: 87.8549\n",
      "Iteration: 62800\n",
      "Gradient: [ 11.0388  -8.8516 -10.0148  46.6121 318.057 ]\n",
      "Weights: [-4.8358  0.7616 -1.1661  0.0681  0.1529]\n",
      "MSE loss: 87.6842\n",
      "Iteration: 62900\n",
      "Gradient: [ -5.9501  -9.4768  -2.9194 -63.8101  41.1036]\n",
      "Weights: [-4.8246  0.7637 -1.166   0.0676  0.1526]\n",
      "MSE loss: 87.6026\n",
      "Iteration: 63000\n",
      "Gradient: [ -1.9862   1.8439  17.4041  37.3111 -25.0917]\n",
      "Weights: [-4.8185  0.7655 -1.167   0.0677  0.1527]\n",
      "MSE loss: 87.8141\n",
      "Iteration: 63100\n",
      "Gradient: [ 4.5088 -5.1855 16.2723 27.0358 -4.7505]\n",
      "Weights: [-4.8377  0.7642 -1.1653  0.0676  0.1529]\n",
      "MSE loss: 87.6974\n",
      "Iteration: 63200\n",
      "Gradient: [  2.8766  -7.0798 -19.1743 -12.942  258.2592]\n",
      "Weights: [-4.8302  0.7552 -1.1631  0.0678  0.1528]\n",
      "MSE loss: 87.6377\n",
      "Iteration: 63300\n",
      "Gradient: [ -5.7269  -6.557   50.3459  50.5844 171.5782]\n",
      "Weights: [-4.8265  0.7688 -1.1656  0.0674  0.1526]\n",
      "MSE loss: 87.782\n",
      "Iteration: 63400\n",
      "Gradient: [ -3.6946   7.5293  23.4463  34.8458 153.0479]\n",
      "Weights: [-4.83    0.7634 -1.1633  0.0681  0.1524]\n",
      "MSE loss: 87.6663\n",
      "Iteration: 63500\n",
      "Gradient: [ -8.146   11.9962 -32.1904 -18.4467  32.9119]\n",
      "Weights: [-4.8213  0.7526 -1.1633  0.0685  0.1524]\n",
      "MSE loss: 87.5678\n",
      "Iteration: 63600\n",
      "Gradient: [ -5.0633  -2.1226 -29.1084  81.4618  41.593 ]\n",
      "Weights: [-4.819   0.7516 -1.1641  0.069   0.1524]\n",
      "MSE loss: 87.5641\n",
      "Iteration: 63700\n",
      "Gradient: [  8.4225   3.7726 -25.7658  57.2947 227.268 ]\n",
      "Weights: [-4.8132  0.7589 -1.1678  0.0695  0.1525]\n",
      "MSE loss: 87.8688\n",
      "Iteration: 63800\n",
      "Gradient: [   1.164    -8.5433  -33.2622 -107.3202  153.1791]\n",
      "Weights: [-4.8289  0.7601 -1.1672  0.0692  0.1525]\n",
      "MSE loss: 87.5681\n",
      "Iteration: 63900\n",
      "Gradient: [  -5.1339   -2.1796   49.2942 -113.4114 -354.7089]\n",
      "Weights: [-4.8346  0.7697 -1.1719  0.0695  0.1526]\n",
      "MSE loss: 87.5819\n",
      "Iteration: 64000\n",
      "Gradient: [ 1.9321 -0.5061 54.9309 32.8894 46.4694]\n",
      "Weights: [-4.8174  0.7658 -1.1771  0.0707  0.1526]\n",
      "MSE loss: 87.582\n",
      "Iteration: 64100\n",
      "Gradient: [   5.7535    3.0869  -17.579   -65.6319 -151.1051]\n",
      "Weights: [-4.8305  0.7681 -1.1746  0.0713  0.1525]\n",
      "MSE loss: 87.5572\n",
      "Iteration: 64200\n",
      "Gradient: [   2.5839  -10.195    30.1613  -26.5776 -149.3566]\n",
      "Weights: [-4.8174  0.7578 -1.1729  0.071   0.1522]\n",
      "MSE loss: 87.5912\n",
      "Iteration: 64300\n",
      "Gradient: [  -9.3774   -4.8329   -3.8283 -101.0377 -223.2669]\n",
      "Weights: [-4.8235  0.7658 -1.1758  0.0718  0.1522]\n",
      "MSE loss: 87.4924\n",
      "Iteration: 64400\n",
      "Gradient: [ -3.0217  17.4268  41.6148  90.3479 110.7603]\n",
      "Weights: [-4.8186  0.7634 -1.1741  0.0717  0.1523]\n",
      "MSE loss: 87.5861\n",
      "Iteration: 64500\n",
      "Gradient: [ -7.5306  -1.9669  -7.5269  14.9963 -75.3016]\n",
      "Weights: [-4.8148  0.7615 -1.1757  0.0719  0.1522]\n",
      "MSE loss: 87.5583\n",
      "Iteration: 64600\n",
      "Gradient: [ 1.0442 20.4818 20.5881 70.2314  5.8858]\n",
      "Weights: [-4.8274  0.7724 -1.1762  0.0723  0.152 ]\n",
      "MSE loss: 87.5548\n",
      "Iteration: 64700\n",
      "Gradient: [-13.3653  18.3546  -2.7857 -25.106   74.9431]\n",
      "Weights: [-4.8353  0.7644 -1.174   0.0722  0.1522]\n",
      "MSE loss: 87.6261\n",
      "Iteration: 64800\n",
      "Gradient: [  -5.3946   -1.4531   38.0491 -127.2672  273.8413]\n",
      "Weights: [-4.81    0.7581 -1.1725  0.0714  0.1523]\n",
      "MSE loss: 87.6761\n",
      "Iteration: 64900\n",
      "Gradient: [-13.3055   5.4539  -0.7281   6.9504  61.4842]\n",
      "Weights: [-4.8467  0.7672 -1.1725  0.0716  0.1522]\n",
      "MSE loss: 87.944\n",
      "Iteration: 65000\n",
      "Gradient: [  4.558    1.2488  19.4873 116.6104 113.7808]\n",
      "Weights: [-4.8105  0.7544 -1.172   0.0713  0.1521]\n",
      "MSE loss: 87.6085\n",
      "Iteration: 65100\n",
      "Gradient: [  -2.9299    2.9206  -29.2375 -131.9618  -17.2166]\n",
      "Weights: [-4.8443  0.7746 -1.1737  0.0719  0.152 ]\n",
      "MSE loss: 87.6817\n",
      "Iteration: 65200\n",
      "Gradient: [ -5.6814  -5.6008  -5.6762  45.3161 179.8496]\n",
      "Weights: [-4.8494  0.7806 -1.1742  0.0717  0.1518]\n",
      "MSE loss: 87.7404\n",
      "Iteration: 65300\n",
      "Gradient: [ -3.4367 -13.6361  48.7038 -53.4552  64.999 ]\n",
      "Weights: [-4.8271  0.7662 -1.176   0.0721  0.1519]\n",
      "MSE loss: 87.6468\n",
      "Iteration: 65400\n",
      "Gradient: [   4.204    19.0063  -30.9414   47.9946 -161.5209]\n",
      "Weights: [-4.8087  0.7498 -1.1746  0.0734  0.1522]\n",
      "MSE loss: 87.561\n",
      "Iteration: 65500\n",
      "Gradient: [-11.0164  -8.7764 -39.6011 -32.8153 137.0328]\n",
      "Weights: [-4.8109  0.7396 -1.174   0.0739  0.1522]\n",
      "MSE loss: 87.7461\n",
      "Iteration: 65600\n",
      "Gradient: [ -1.5727  -7.1546 -11.0382 -41.7165  80.7489]\n",
      "Weights: [-4.8165  0.7494 -1.1726  0.0736  0.152 ]\n",
      "MSE loss: 87.5087\n",
      "Iteration: 65700\n",
      "Gradient: [  -7.9827  -18.6721  -43.8183 -193.3342  -74.7007]\n",
      "Weights: [-4.8287  0.7482 -1.1731  0.0739  0.152 ]\n",
      "MSE loss: 87.9273\n",
      "Iteration: 65800\n",
      "Gradient: [ -1.0355   8.1366 -23.9951 -16.3808 316.0593]\n",
      "Weights: [-4.8084  0.7527 -1.1734  0.0738  0.1516]\n",
      "MSE loss: 87.5716\n",
      "Iteration: 65900\n",
      "Gradient: [  -6.4007   -6.6269    0.4816   41.2061 -191.1421]\n",
      "Weights: [-4.8208  0.7504 -1.1737  0.0745  0.1518]\n",
      "MSE loss: 87.5265\n",
      "Iteration: 66000\n",
      "Gradient: [  6.338   -6.839   51.7685 158.2275  34.7699]\n",
      "Weights: [-4.8102  0.755  -1.1745  0.0738  0.1519]\n",
      "MSE loss: 87.6086\n",
      "Iteration: 66100\n",
      "Gradient: [  6.0485  -7.8489   7.1948  49.1128 -93.9392]\n",
      "Weights: [-4.82    0.7576 -1.1743  0.0741  0.1518]\n",
      "MSE loss: 87.5636\n",
      "Iteration: 66200\n",
      "Gradient: [   7.0583    2.5483   12.4322  -72.9681 -330.8672]\n",
      "Weights: [-4.812   0.7469 -1.1733  0.0738  0.1519]\n",
      "MSE loss: 87.5399\n",
      "Iteration: 66300\n",
      "Gradient: [ -2.583   -2.9362 -27.8619  33.6527 134.1329]\n",
      "Weights: [-4.8082  0.7565 -1.1771  0.0743  0.1518]\n",
      "MSE loss: 87.5929\n",
      "Iteration: 66400\n",
      "Gradient: [  -4.1577    2.2476  -31.8148  151.1946 -204.4344]\n",
      "Weights: [-4.8275  0.757  -1.1739  0.0742  0.1518]\n",
      "MSE loss: 87.5463\n",
      "Iteration: 66500\n",
      "Gradient: [ -2.7862   4.6533  -3.5261  16.2791 211.8131]\n",
      "Weights: [-4.8276  0.7584 -1.1736  0.0744  0.1515]\n",
      "MSE loss: 87.5088\n",
      "Iteration: 66600\n",
      "Gradient: [  -7.7961   -0.5145  -35.5019  -92.6655 -121.2123]\n",
      "Weights: [-4.832   0.7693 -1.176   0.0736  0.1515]\n",
      "MSE loss: 87.5194\n",
      "Iteration: 66700\n",
      "Gradient: [  4.9824  10.5106 -38.0744  20.5974 150.5584]\n",
      "Weights: [-4.8219  0.7664 -1.1761  0.0746  0.1516]\n",
      "MSE loss: 87.7282\n",
      "Iteration: 66800\n",
      "Gradient: [   1.1947  -13.1061   18.7318 -102.8427 -235.3227]\n",
      "Weights: [-4.8332  0.767  -1.1779  0.0746  0.1514]\n",
      "MSE loss: 87.6401\n",
      "Iteration: 66900\n",
      "Gradient: [-2.223   4.831  34.4514 77.9021 90.9227]\n",
      "Weights: [-4.8155  0.7708 -1.1815  0.0744  0.1519]\n",
      "MSE loss: 87.6528\n",
      "Iteration: 67000\n",
      "Gradient: [  -6.2673   11.5933    1.3529   37.957  -159.8796]\n",
      "Weights: [-4.8267  0.7709 -1.183   0.0749  0.152 ]\n",
      "MSE loss: 87.4474\n",
      "Iteration: 67100\n",
      "Gradient: [  -5.1064    3.9259  -59.5557 -102.8119  -80.329 ]\n",
      "Weights: [-4.8266  0.7757 -1.1833  0.0745  0.1517]\n",
      "MSE loss: 87.5122\n",
      "Iteration: 67200\n",
      "Gradient: [  0.1499   0.4205   6.0837  33.0028 -90.0656]\n",
      "Weights: [-4.8331  0.7723 -1.1839  0.0758  0.1519]\n",
      "MSE loss: 87.5222\n",
      "Iteration: 67300\n",
      "Gradient: [ -4.5039 -17.7119 -17.478  -73.6147 -71.7488]\n",
      "Weights: [-4.8373  0.7754 -1.1856  0.0755  0.1517]\n",
      "MSE loss: 87.8184\n",
      "Iteration: 67400\n",
      "Gradient: [  6.0538   1.8976   5.3543  22.3941 -74.6036]\n",
      "Weights: [-4.8279  0.7868 -1.1864  0.0745  0.1517]\n",
      "MSE loss: 87.5384\n",
      "Iteration: 67500\n",
      "Gradient: [   8.6799    8.0639    8.6868  -15.915  -189.4467]\n",
      "Weights: [-4.829   0.7844 -1.1828  0.075   0.1514]\n",
      "MSE loss: 87.6913\n",
      "Iteration: 67600\n",
      "Gradient: [ 13.092  -24.7485  33.6486  -5.0897 -37.2727]\n",
      "Weights: [-4.8233  0.7693 -1.181   0.0761  0.1513]\n",
      "MSE loss: 87.4492\n",
      "Iteration: 67700\n",
      "Gradient: [ -2.4887   9.6946 -31.1153  25.1868 -73.3875]\n",
      "Weights: [-4.8233  0.7667 -1.1816  0.0769  0.1513]\n",
      "MSE loss: 87.4656\n",
      "Iteration: 67800\n",
      "Gradient: [  -5.7903  -15.5825    0.9698    4.8275 -128.0509]\n",
      "Weights: [-4.8252  0.7632 -1.1838  0.077   0.1514]\n",
      "MSE loss: 87.6213\n",
      "Iteration: 67900\n",
      "Gradient: [  17.3604    3.1336   -4.8608  -72.8884 -126.7497]\n",
      "Weights: [-4.8357  0.7836 -1.1854  0.0768  0.1512]\n",
      "MSE loss: 87.4792\n",
      "Iteration: 68000\n",
      "Gradient: [  6.756   27.8744  14.6763  11.2067 167.1618]\n",
      "Weights: [-4.8197  0.7749 -1.1862  0.0766  0.1514]\n",
      "MSE loss: 87.4772\n",
      "Iteration: 68100\n",
      "Gradient: [  -5.0102   -4.6807   -1.6148   -9.7447 -334.6155]\n",
      "Weights: [-4.8256  0.7719 -1.1859  0.0765  0.1517]\n",
      "MSE loss: 87.4284\n",
      "Iteration: 68200\n",
      "Gradient: [  4.2341 -13.4458 -21.3531 -93.9645  53.9411]\n",
      "Weights: [-4.8245  0.7687 -1.1868  0.0762  0.1517]\n",
      "MSE loss: 87.6605\n",
      "Iteration: 68300\n",
      "Gradient: [   1.3999   10.7234   -0.6664  -33.1535 -117.6273]\n",
      "Weights: [-4.827   0.7702 -1.1838  0.0758  0.1517]\n",
      "MSE loss: 87.4456\n",
      "Iteration: 68400\n",
      "Gradient: [  4.6801  -5.0628 -16.8603   6.3662 -72.3928]\n",
      "Weights: [-4.8126  0.7575 -1.1805  0.0766  0.1518]\n",
      "MSE loss: 87.6453\n",
      "Iteration: 68500\n",
      "Gradient: [ -6.871   11.8659 -29.7097 -64.718  118.0387]\n",
      "Weights: [-4.8201  0.7649 -1.1825  0.0766  0.1517]\n",
      "MSE loss: 87.5464\n",
      "Iteration: 68600\n",
      "Gradient: [  -1.1299   -3.9995  -35.7893    2.3704 -129.8181]\n",
      "Weights: [-4.8001  0.7469 -1.179   0.0764  0.1516]\n",
      "MSE loss: 87.6017\n",
      "Iteration: 68700\n",
      "Gradient: [ 8.2412 19.5123  6.2582 -9.0271 30.3674]\n",
      "Weights: [-4.7966  0.7515 -1.1826  0.0769  0.1517]\n",
      "MSE loss: 87.7251\n",
      "Iteration: 68800\n",
      "Gradient: [  -1.7201    6.469    -6.9799   34.1641 -234.0794]\n",
      "Weights: [-4.8104  0.7508 -1.1808  0.0763  0.152 ]\n",
      "MSE loss: 87.5293\n",
      "Iteration: 68900\n",
      "Gradient: [  2.184   -3.8464  19.8077 -47.1302 116.2734]\n",
      "Weights: [-4.788   0.745  -1.1831  0.0766  0.152 ]\n",
      "MSE loss: 87.88\n",
      "Iteration: 69000\n",
      "Gradient: [-0.4399 -8.1923  5.2571 62.8552 23.8955]\n",
      "Weights: [-4.8238  0.758  -1.1805  0.0761  0.1519]\n",
      "MSE loss: 87.5453\n",
      "Iteration: 69100\n",
      "Gradient: [ -4.9509  -9.6829  -6.341  -17.5069 -17.5295]\n",
      "Weights: [-4.8267  0.7646 -1.1838  0.0763  0.1518]\n",
      "MSE loss: 87.515\n",
      "Iteration: 69200\n",
      "Gradient: [ -7.8386  -5.1447   8.7729 -48.5164 -23.1959]\n",
      "Weights: [-4.8187  0.7636 -1.1823  0.0763  0.1515]\n",
      "MSE loss: 87.4319\n",
      "Iteration: 69300\n",
      "Gradient: [ -2.4323  -0.7071  21.8193  15.0631 -15.9381]\n",
      "Weights: [-4.8195  0.7627 -1.1808  0.0757  0.1516]\n",
      "MSE loss: 87.4399\n",
      "Iteration: 69400\n",
      "Gradient: [  5.9677 -15.9035  -7.9482 -37.325  216.7859]\n",
      "Weights: [-4.8097  0.7521 -1.179   0.0762  0.1516]\n",
      "MSE loss: 87.4942\n",
      "Iteration: 69500\n",
      "Gradient: [  2.8046   4.2973 -14.7418 -15.3999  -8.958 ]\n",
      "Weights: [-4.838   0.7695 -1.1801  0.0759  0.1516]\n",
      "MSE loss: 87.5996\n",
      "Iteration: 69600\n",
      "Gradient: [ -3.2329  -8.6392  -4.7322 -48.9853 -57.9277]\n",
      "Weights: [-4.8197  0.7732 -1.1834  0.0751  0.1515]\n",
      "MSE loss: 87.522\n",
      "Iteration: 69700\n",
      "Gradient: [  0.7302   9.8649  26.8415 -32.579  370.2052]\n",
      "Weights: [-4.8152  0.7712 -1.1823  0.0756  0.1516]\n",
      "MSE loss: 87.7326\n",
      "Iteration: 69800\n",
      "Gradient: [  9.4689  -7.0119 -28.5549  87.1454 204.2966]\n",
      "Weights: [-4.8277  0.7723 -1.1831  0.0769  0.1513]\n",
      "MSE loss: 87.4882\n",
      "Iteration: 69900\n",
      "Gradient: [  -5.2631   -2.5376   -8.6512 -143.1062 -146.3743]\n",
      "Weights: [-4.8424  0.7759 -1.1818  0.0759  0.1512]\n",
      "MSE loss: 87.7074\n",
      "Iteration: 70000\n",
      "Gradient: [  -3.7824  -28.3317  -36.1353   45.1639 -206.8058]\n",
      "Weights: [-4.8228  0.7653 -1.179   0.0758  0.1513]\n",
      "MSE loss: 87.4437\n",
      "Iteration: 70100\n",
      "Gradient: [  2.1425  -1.8423   2.1808  21.0133 252.0161]\n",
      "Weights: [-4.8236  0.7656 -1.1782  0.0758  0.1512]\n",
      "MSE loss: 87.4567\n",
      "Iteration: 70200\n",
      "Gradient: [ -3.4095  -4.7871 -36.7975  67.4401 215.1343]\n",
      "Weights: [-4.8447  0.7783 -1.1783  0.075   0.1511]\n",
      "MSE loss: 87.6384\n",
      "Iteration: 70300\n",
      "Gradient: [-3.0388  2.769  -3.517  51.578  49.1885]\n",
      "Weights: [-4.8459  0.782  -1.1813  0.0754  0.1511]\n",
      "MSE loss: 87.7321\n",
      "Iteration: 70400\n",
      "Gradient: [  2.5975  -1.6037  17.3781  12.8106 222.8543]\n",
      "Weights: [-4.8243  0.7761 -1.1822  0.0758  0.1512]\n",
      "MSE loss: 87.5239\n",
      "Iteration: 70500\n",
      "Gradient: [  9.9997  -0.617    5.9514 -34.029   65.2064]\n",
      "Weights: [-4.8281  0.7778 -1.1835  0.0757  0.1513]\n",
      "MSE loss: 87.4737\n",
      "Iteration: 70600\n",
      "Gradient: [  5.3887  -4.5166  21.7765 -57.3625 130.8367]\n",
      "Weights: [-4.8239  0.7738 -1.1828  0.076   0.1511]\n",
      "MSE loss: 87.4871\n",
      "Iteration: 70700\n",
      "Gradient: [  1.765  -18.1503  10.8043  76.9594 -59.8846]\n",
      "Weights: [-4.84    0.7759 -1.1833  0.0761  0.1513]\n",
      "MSE loss: 87.6327\n",
      "Iteration: 70800\n",
      "Gradient: [ -3.9704  -7.159   -6.3452 -85.32   141.3592]\n",
      "Weights: [-4.8338  0.7736 -1.183   0.076   0.1515]\n",
      "MSE loss: 87.4886\n",
      "Iteration: 70900\n",
      "Gradient: [15.1083  1.2075 12.5738 -9.2475 84.6406]\n",
      "Weights: [-4.8179  0.7763 -1.1827  0.0752  0.1515]\n",
      "MSE loss: 87.7023\n",
      "Iteration: 71000\n",
      "Gradient: [  4.5591   7.3605 -35.653  -58.249   37.1833]\n",
      "Weights: [-4.8248  0.77   -1.1793  0.0749  0.1514]\n",
      "MSE loss: 87.4672\n",
      "Iteration: 71100\n",
      "Gradient: [-11.286   -4.1212  -9.5675  36.3    -58.6425]\n",
      "Weights: [-4.8229  0.7679 -1.1775  0.0748  0.1514]\n",
      "MSE loss: 87.5083\n",
      "Iteration: 71200\n",
      "Gradient: [  -0.3313    1.0491  -31.4443  -62.4368 -157.9264]\n",
      "Weights: [-4.8237  0.7625 -1.1793  0.075   0.1515]\n",
      "MSE loss: 87.5866\n",
      "Iteration: 71300\n",
      "Gradient: [ 1.44310e+00 -1.66770e+00  1.84920e+01 -5.10000e-03 -9.55119e+01]\n",
      "Weights: [-4.8175  0.7621 -1.1796  0.0752  0.1515]\n",
      "MSE loss: 87.4748\n",
      "Iteration: 71400\n",
      "Gradient: [  -6.1478  -15.0993    6.2448  -96.6738 -188.9181]\n",
      "Weights: [-4.8265  0.7705 -1.1832  0.0752  0.1514]\n",
      "MSE loss: 87.7535\n",
      "Iteration: 71500\n",
      "Gradient: [-10.8073 -18.7092 -66.6901 -21.1914 -45.6137]\n",
      "Weights: [-4.8277  0.7722 -1.1838  0.0751  0.1515]\n",
      "MSE loss: 87.7337\n",
      "Iteration: 71600\n",
      "Gradient: [  -7.074   -11.871   -82.7851  -47.1625 -144.771 ]\n",
      "Weights: [-4.8308  0.7666 -1.182   0.0754  0.1517]\n",
      "MSE loss: 87.5791\n",
      "Iteration: 71700\n",
      "Gradient: [  5.4454  -5.4036   1.1305 -32.1731 234.7964]\n",
      "Weights: [-4.8195  0.7684 -1.1815  0.0748  0.1515]\n",
      "MSE loss: 87.5468\n",
      "Iteration: 71800\n",
      "Gradient: [  4.1266  13.0204  28.8233 -93.8631 -35.9746]\n",
      "Weights: [-4.8076  0.7693 -1.1833  0.0757  0.1514]\n",
      "MSE loss: 87.7628\n",
      "Iteration: 71900\n",
      "Gradient: [  -2.5182   -0.6047  -11.1306 -141.3397 -206.4746]\n",
      "Weights: [-4.8278  0.7825 -1.1873  0.0756  0.1517]\n",
      "MSE loss: 87.4492\n",
      "Iteration: 72000\n",
      "Gradient: [  3.4343   7.5076  -8.1745 -13.3786 126.3283]\n",
      "Weights: [-4.8233  0.7705 -1.1845  0.0761  0.1516]\n",
      "MSE loss: 87.4242\n",
      "Iteration: 72100\n",
      "Gradient: [  -6.8494  -13.4661    6.0773  107.5696 -213.4686]\n",
      "Weights: [-4.8363  0.7765 -1.1857  0.0762  0.1516]\n",
      "MSE loss: 87.5516\n",
      "Iteration: 72200\n",
      "Gradient: [ -3.6002  -5.7046 -17.8729 -64.4883  53.2036]\n",
      "Weights: [-4.8342  0.7779 -1.1875  0.0756  0.1518]\n",
      "MSE loss: 87.6229\n",
      "Iteration: 72300\n",
      "Gradient: [-14.0801   0.3753  -8.7241   2.2441 -41.2374]\n",
      "Weights: [-4.8419  0.7894 -1.1877  0.0751  0.1519]\n",
      "MSE loss: 87.5037\n",
      "Iteration: 72400\n",
      "Gradient: [  5.2871  11.2844 -12.5238 103.7495 218.2689]\n",
      "Weights: [-4.8312  0.7803 -1.1879  0.0755  0.1521]\n",
      "MSE loss: 87.4426\n",
      "Iteration: 72500\n",
      "Gradient: [ -3.8853   6.7377 -29.551   13.3259 272.1529]\n",
      "Weights: [-4.8257  0.776  -1.1884  0.0755  0.1522]\n",
      "MSE loss: 87.4334\n",
      "Iteration: 72600\n",
      "Gradient: [   1.63      3.0823   29.4621   15.2052 -247.3987]\n",
      "Weights: [-4.8258  0.7768 -1.1882  0.0759  0.1521]\n",
      "MSE loss: 87.4761\n",
      "Iteration: 72700\n",
      "Gradient: [  3.5241   6.7478 -21.0174 -20.704   99.9187]\n",
      "Weights: [-4.8071  0.7632 -1.1875  0.0764  0.152 ]\n",
      "MSE loss: 87.5359\n",
      "Iteration: 72800\n",
      "Gradient: [ -1.0983  -9.4269  -9.342  -18.2914 -48.637 ]\n",
      "Weights: [-4.8205  0.7659 -1.1852  0.0754  0.152 ]\n",
      "MSE loss: 87.5167\n",
      "Iteration: 72900\n",
      "Gradient: [  -2.8612    5.4402   -3.8076  -20.0799 -301.5492]\n",
      "Weights: [-4.8166  0.7606 -1.1814  0.0748  0.1518]\n",
      "MSE loss: 87.6035\n",
      "Iteration: 73000\n",
      "Gradient: [ -15.0937    5.1449  -15.2673   48.6155 -134.0087]\n",
      "Weights: [-4.8262  0.7682 -1.1818  0.0749  0.1518]\n",
      "MSE loss: 87.4746\n",
      "Iteration: 73100\n",
      "Gradient: [ -8.7857 -14.3457  28.7573 -82.569   34.7185]\n",
      "Weights: [-4.8443  0.7845 -1.1832  0.0741  0.1519]\n",
      "MSE loss: 87.5809\n",
      "Iteration: 73200\n",
      "Gradient: [  1.1385   3.0818  32.1862  61.8645 -31.3924]\n",
      "Weights: [-4.8412  0.7906 -1.187   0.0734  0.152 ]\n",
      "MSE loss: 87.653\n",
      "Iteration: 73300\n",
      "Gradient: [   0.3953   -5.5901  -10.587   -24.5535 -341.1154]\n",
      "Weights: [-4.8341  0.7939 -1.1904  0.0738  0.1523]\n",
      "MSE loss: 87.4769\n",
      "Iteration: 73400\n",
      "Gradient: [  3.7037 -16.2997   9.0285  62.2659 -85.4818]\n",
      "Weights: [-4.8216  0.7819 -1.1897  0.0745  0.1524]\n",
      "MSE loss: 87.4854\n",
      "Iteration: 73500\n",
      "Gradient: [ 7.20000e-03  2.69678e+01  1.00035e+01  4.18497e+01 -3.75003e+01]\n",
      "Weights: [-4.8236  0.7849 -1.1913  0.0738  0.1526]\n",
      "MSE loss: 87.4817\n",
      "Iteration: 73600\n",
      "Gradient: [ -8.0569   6.8228 -16.6465 -58.9318 -20.6635]\n",
      "Weights: [-4.8414  0.7917 -1.1905  0.0737  0.1528]\n",
      "MSE loss: 87.5471\n",
      "Iteration: 73700\n",
      "Gradient: [  -2.8705  -16.8347  -19.5978  -14.2018 -158.387 ]\n",
      "Weights: [-4.8206  0.7813 -1.1914  0.0744  0.1526]\n",
      "MSE loss: 87.4673\n",
      "Iteration: 73800\n",
      "Gradient: [ 5.19000e-02 -1.03376e+01 -3.70104e+01 -4.89702e+01 -1.76996e+02]\n",
      "Weights: [-4.83    0.7779 -1.1906  0.0748  0.1525]\n",
      "MSE loss: 87.5705\n",
      "Iteration: 73900\n",
      "Gradient: [   4.6701   -4.2321   -6.016    72.1999 -120.3808]\n",
      "Weights: [-4.8156  0.7835 -1.1914  0.0745  0.1526]\n",
      "MSE loss: 87.6674\n",
      "Iteration: 74000\n",
      "Gradient: [ -11.0143  -19.7286  -42.1308   44.2889 -100.7307]\n",
      "Weights: [-4.8279  0.7882 -1.1914  0.0738  0.1524]\n",
      "MSE loss: 87.5428\n",
      "Iteration: 74100\n",
      "Gradient: [  7.5301 -29.0133 -40.9322 -34.9377  11.8719]\n",
      "Weights: [-4.8524  0.7974 -1.1903  0.0738  0.1524]\n",
      "MSE loss: 87.6898\n",
      "Iteration: 74200\n",
      "Gradient: [  -5.8765   14.2461  -18.9788  -58.8189 -255.5925]\n",
      "Weights: [-4.8387  0.7878 -1.1904  0.0737  0.1524]\n",
      "MSE loss: 87.7056\n",
      "Iteration: 74300\n",
      "Gradient: [  -2.6255    4.5767   17.0064   39.0538 -281.902 ]\n",
      "Weights: [-4.836   0.7862 -1.1909  0.0741  0.1526]\n",
      "MSE loss: 87.5148\n",
      "Iteration: 74400\n",
      "Gradient: [ -5.1189  -3.3515  19.1899 203.8326 247.3902]\n",
      "Weights: [-4.8326  0.7919 -1.1919  0.0744  0.1526]\n",
      "MSE loss: 87.5365\n",
      "Iteration: 74500\n",
      "Gradient: [ -5.2592   0.4272 -27.6659 -27.7301 140.9707]\n",
      "Weights: [-4.834   0.7831 -1.1885  0.0734  0.1527]\n",
      "MSE loss: 87.5132\n",
      "Iteration: 74600\n",
      "Gradient: [   7.4488   13.0285   29.5946   23.2611 -188.654 ]\n",
      "Weights: [-4.8246  0.7883 -1.1859  0.072   0.1525]\n",
      "MSE loss: 87.6181\n",
      "Iteration: 74700\n",
      "Gradient: [-1.547000e-01 -1.113760e+01 -1.926280e+01 -2.000330e+01 -3.021975e+02]\n",
      "Weights: [-4.8368  0.7931 -1.1877  0.0729  0.1523]\n",
      "MSE loss: 87.5075\n",
      "Iteration: 74800\n",
      "Gradient: [ -6.6971  -4.4438 -34.9354  40.2568  37.8127]\n",
      "Weights: [-4.8465  0.7954 -1.1894  0.0732  0.1526]\n",
      "MSE loss: 87.5694\n",
      "Iteration: 74900\n",
      "Gradient: [  6.0219 -10.8091  30.6061  26.7665 -86.4765]\n",
      "Weights: [-4.8365  0.7964 -1.189   0.0724  0.1527]\n",
      "MSE loss: 87.5311\n",
      "Iteration: 75000\n",
      "Gradient: [  -0.5483    6.9741    2.6041 -122.5836  166.2822]\n",
      "Weights: [-4.8581  0.8099 -1.1879  0.0706  0.1528]\n",
      "MSE loss: 87.7335\n",
      "Iteration: 75100\n",
      "Gradient: [  1.3632   1.7927  10.1236 -26.2723   9.7074]\n",
      "Weights: [-4.8391  0.8067 -1.1876  0.0706  0.1528]\n",
      "MSE loss: 87.8567\n",
      "Iteration: 75200\n",
      "Gradient: [-13.9275 -21.8318 -15.5932 -35.227  -47.0261]\n",
      "Weights: [-4.8468  0.8098 -1.1877  0.0702  0.1526]\n",
      "MSE loss: 87.6933\n",
      "Iteration: 75300\n",
      "Gradient: [-14.4476   1.9509 -10.881   53.8839 192.6494]\n",
      "Weights: [-4.8413  0.807  -1.1913  0.0717  0.1528]\n",
      "MSE loss: 87.6439\n",
      "Iteration: 75400\n",
      "Gradient: [ -3.7749   8.7328  13.1765 -64.0986 331.3207]\n",
      "Weights: [-4.8445  0.8146 -1.1951  0.0716  0.1529]\n",
      "MSE loss: 87.606\n",
      "Iteration: 75500\n",
      "Gradient: [   1.3518   -9.9308  -38.8239   -4.3326 -352.6314]\n",
      "Weights: [-4.859   0.8098 -1.1956  0.0722  0.1531]\n",
      "MSE loss: 87.8995\n",
      "Iteration: 75600\n",
      "Gradient: [ 10.3057   9.8592  41.2389  28.1235 100.8301]\n",
      "Weights: [-4.8374  0.819  -1.2007  0.0728  0.1529]\n",
      "MSE loss: 87.6887\n",
      "Iteration: 75700\n",
      "Gradient: [   3.723    -5.8935   -7.8343   49.8489 -239.48  ]\n",
      "Weights: [-4.8366  0.8112 -1.2     0.0732  0.1531]\n",
      "MSE loss: 87.5421\n",
      "Iteration: 75800\n",
      "Gradient: [ 5.2361 22.7156 28.25   73.7002 54.6719]\n",
      "Weights: [-4.8345  0.8171 -1.2002  0.0731  0.1531]\n",
      "MSE loss: 87.9582\n",
      "Iteration: 75900\n",
      "Gradient: [ -5.7693  -2.4452   3.9042  21.7387 190.0928]\n",
      "Weights: [-4.852   0.8172 -1.2013  0.0728  0.1531]\n",
      "MSE loss: 87.6435\n",
      "Iteration: 76000\n",
      "Gradient: [ -3.799   -7.1056  18.8867 -43.5853 -18.3383]\n",
      "Weights: [-4.8441  0.81   -1.2     0.0728  0.1533]\n",
      "MSE loss: 87.4957\n",
      "Iteration: 76100\n",
      "Gradient: [ -17.3046   -4.2952   -6.414    72.1431 -145.8816]\n",
      "Weights: [-4.8427  0.8086 -1.2008  0.0726  0.1535]\n",
      "MSE loss: 87.5419\n",
      "Iteration: 76200\n",
      "Gradient: [-9.91180e+00 -4.80000e-03  9.45750e+00  2.42228e+01  7.72183e+01]\n",
      "Weights: [-4.8453  0.8187 -1.2027  0.0727  0.1534]\n",
      "MSE loss: 87.5161\n",
      "Iteration: 76300\n",
      "Gradient: [ -1.865  -13.7053  13.8782   7.6266 182.0127]\n",
      "Weights: [-4.8524  0.8202 -1.2026  0.0724  0.1534]\n",
      "MSE loss: 87.5645\n",
      "Iteration: 76400\n",
      "Gradient: [  2.3624   3.3758  16.1875  23.2991 114.637 ]\n",
      "Weights: [-4.852   0.8282 -1.2032  0.0728  0.1532]\n",
      "MSE loss: 87.6673\n",
      "Iteration: 76500\n",
      "Gradient: [ -5.7854 -14.9853   7.4761 -39.9892 329.7997]\n",
      "Weights: [-4.8429  0.8305 -1.2083  0.0727  0.1533]\n",
      "MSE loss: 87.6204\n",
      "Iteration: 76600\n",
      "Gradient: [  1.255  -12.9929  22.3324 -95.2773 -63.1738]\n",
      "Weights: [-4.8382  0.8192 -1.2097  0.0737  0.1535]\n",
      "MSE loss: 87.5717\n",
      "Iteration: 76700\n",
      "Gradient: [  -1.6797   -6.7551   -8.0388 -121.6034   73.8173]\n",
      "Weights: [-4.8419  0.8245 -1.2103  0.0745  0.1536]\n",
      "MSE loss: 87.5329\n",
      "Iteration: 76800\n",
      "Gradient: [  3.873    2.4146 -17.9832  68.0933 148.5879]\n",
      "Weights: [-4.8503  0.8248 -1.2123  0.0749  0.1535]\n",
      "MSE loss: 87.5467\n",
      "Iteration: 76900\n",
      "Gradient: [ -1.6137 -10.5764  -5.9671 -95.4487  29.829 ]\n",
      "Weights: [-4.8466  0.8259 -1.2126  0.0753  0.1532]\n",
      "MSE loss: 87.4921\n",
      "Iteration: 77000\n",
      "Gradient: [  8.4241 -12.2167  16.2408 -20.8211 259.5376]\n",
      "Weights: [-4.8462  0.82   -1.2111  0.0752  0.1534]\n",
      "MSE loss: 87.4991\n",
      "Iteration: 77100\n",
      "Gradient: [  2.906   15.9964  16.8394 -22.5383 280.6366]\n",
      "Weights: [-4.8181  0.8077 -1.2109  0.0756  0.1535]\n",
      "MSE loss: 87.644\n",
      "Iteration: 77200\n",
      "Gradient: [   3.1787   17.3664  -19.3647 -113.5743  -13.0644]\n",
      "Weights: [-4.8381  0.8133 -1.2104  0.0764  0.1536]\n",
      "MSE loss: 87.6396\n",
      "Iteration: 77300\n",
      "Gradient: [  5.9487 -10.2295  31.6099  -5.0412 -99.9288]\n",
      "Weights: [-4.8371  0.8109 -1.2096  0.0761  0.1533]\n",
      "MSE loss: 87.4315\n",
      "Iteration: 77400\n",
      "Gradient: [-6.2689  7.0355  1.444  14.7353 26.0099]\n",
      "Weights: [-4.8411  0.8155 -1.2088  0.0756  0.1532]\n",
      "MSE loss: 87.431\n",
      "Iteration: 77500\n",
      "Gradient: [   0.2641   -8.3455   26.8115 -105.9657  190.5471]\n",
      "Weights: [-4.8363  0.8096 -1.206   0.0756  0.153 ]\n",
      "MSE loss: 87.4281\n",
      "Iteration: 77600\n",
      "Gradient: [  3.5827  10.4229  -7.8586 109.2647 245.1279]\n",
      "Weights: [-4.8247  0.8061 -1.2084  0.0769  0.153 ]\n",
      "MSE loss: 87.5389\n",
      "Iteration: 77700\n",
      "Gradient: [  -9.4518  -14.8135  -31.5819   67.1485 -193.9926]\n",
      "Weights: [-4.8327  0.7973 -1.2056  0.0767  0.153 ]\n",
      "MSE loss: 87.5331\n",
      "Iteration: 77800\n",
      "Gradient: [ 3.1175 13.3586 23.4263 10.5401 85.6418]\n",
      "Weights: [-4.8334  0.7989 -1.203   0.0764  0.1529]\n",
      "MSE loss: 87.4334\n",
      "Iteration: 77900\n",
      "Gradient: [ -3.8351  -4.1163 -35.8376 -59.919   49.8357]\n",
      "Weights: [-4.8472  0.8058 -1.2044  0.0771  0.1526]\n",
      "MSE loss: 87.5484\n",
      "Iteration: 78000\n",
      "Gradient: [  5.7182  -3.3604 -26.5904  43.8775  18.6511]\n",
      "Weights: [-4.8464  0.8082 -1.2031  0.0769  0.1526]\n",
      "MSE loss: 87.5011\n",
      "Iteration: 78100\n",
      "Gradient: [  3.8585  16.9451  29.3699  20.7553 235.1826]\n",
      "Weights: [-4.8515  0.8151 -1.2017  0.0762  0.1524]\n",
      "MSE loss: 87.5475\n",
      "Iteration: 78200\n",
      "Gradient: [  -7.4555   -2.7956   43.9831  -36.1857 -420.8691]\n",
      "Weights: [-4.8399  0.7991 -1.1984  0.0759  0.1524]\n",
      "MSE loss: 87.4511\n",
      "Iteration: 78300\n",
      "Gradient: [   4.0569    2.698    39.0436  130.4225 -229.4433]\n",
      "Weights: [-4.8286  0.792  -1.1987  0.0761  0.1523]\n",
      "MSE loss: 87.5283\n",
      "Iteration: 78400\n",
      "Gradient: [   6.4547   -1.8056   -3.9988   -7.9394 -176.073 ]\n",
      "Weights: [-4.8047  0.7788 -1.1966  0.0766  0.1525]\n",
      "MSE loss: 87.7793\n",
      "Iteration: 78500\n",
      "Gradient: [  -9.0267  -16.5144   -5.0199 -115.5369   55.7417]\n",
      "Weights: [-4.8387  0.7872 -1.1981  0.0768  0.1525]\n",
      "MSE loss: 87.7115\n",
      "Iteration: 78600\n",
      "Gradient: [  0.5211  -4.3668 -17.3856  56.605   21.0024]\n",
      "Weights: [-4.8429  0.8052 -1.2013  0.077   0.1522]\n",
      "MSE loss: 87.4302\n",
      "Iteration: 78700\n",
      "Gradient: [ -3.1163  -7.425   13.1034 -15.413   28.7658]\n",
      "Weights: [-4.8419  0.805  -1.2031  0.077   0.1524]\n",
      "MSE loss: 87.4285\n",
      "Iteration: 78800\n",
      "Gradient: [ -6.5725 -13.3611  -8.8277  77.2025  24.2518]\n",
      "Weights: [-4.8545  0.8131 -1.2034  0.0769  0.1523]\n",
      "MSE loss: 87.5964\n",
      "Iteration: 78900\n",
      "Gradient: [ -2.2208 -17.3029  -5.8115   1.7455 244.7791]\n",
      "Weights: [-4.8529  0.8216 -1.2071  0.077   0.1523]\n",
      "MSE loss: 87.4927\n",
      "Iteration: 79000\n",
      "Gradient: [   0.7597  -11.8341  -15.513    37.072  -104.3154]\n",
      "Weights: [-4.8444  0.8108 -1.2073  0.0772  0.1524]\n",
      "MSE loss: 87.5862\n",
      "Iteration: 79100\n",
      "Gradient: [  -1.8986   11.2713  -29.1417   20.9612 -319.1901]\n",
      "Weights: [-4.8394  0.8073 -1.2067  0.0774  0.1524]\n",
      "MSE loss: 87.4961\n",
      "Iteration: 79200\n",
      "Gradient: [  3.5427  -6.6161  -0.4083 -16.2376 -97.5228]\n",
      "Weights: [-4.8319  0.8075 -1.2068  0.0767  0.1526]\n",
      "MSE loss: 87.4473\n",
      "Iteration: 79300\n",
      "Gradient: [  0.1201   4.8746   1.4607  35.5226 -92.2802]\n",
      "Weights: [-4.8414  0.8211 -1.2095  0.0772  0.1524]\n",
      "MSE loss: 87.4469\n",
      "Iteration: 79400\n",
      "Gradient: [  9.1314   6.3598 -35.781    8.1321 245.2955]\n",
      "Weights: [-4.8364  0.8147 -1.2075  0.0771  0.1523]\n",
      "MSE loss: 87.471\n",
      "Iteration: 79500\n",
      "Gradient: [ -2.0061   4.7936 -50.3323  22.7863   3.6194]\n",
      "Weights: [-4.8398  0.8031 -1.2099  0.0789  0.1525]\n",
      "MSE loss: 87.6735\n",
      "Iteration: 79600\n",
      "Gradient: [  -2.3179  -29.0675  -28.2407   -7.5381 -137.6965]\n",
      "Weights: [-4.837   0.8    -1.2101  0.0796  0.1525]\n",
      "MSE loss: 87.5548\n",
      "Iteration: 79700\n",
      "Gradient: [-0.1956  7.8837 -0.52   13.7585 13.5931]\n",
      "Weights: [-4.8348  0.8072 -1.2095  0.08    0.1522]\n",
      "MSE loss: 87.3893\n",
      "Iteration: 79800\n",
      "Gradient: [ 6.10000e-03 -8.76020e+00 -3.84164e+01  2.55369e+01  5.31830e+01]\n",
      "Weights: [-4.8334  0.8125 -1.2089  0.0791  0.1517]\n",
      "MSE loss: 87.4578\n",
      "Iteration: 79900\n",
      "Gradient: [  12.1361    2.041    32.8641   39.8879 -283.8757]\n",
      "Weights: [-4.8399  0.8135 -1.206   0.0798  0.1517]\n",
      "MSE loss: 87.7041\n",
      "Iteration: 80000\n",
      "Gradient: [ -2.7918  14.0816  27.0365  35.942  181.7786]\n",
      "Weights: [-4.8525  0.8146 -1.2086  0.08    0.1518]\n",
      "MSE loss: 87.49\n",
      "Iteration: 80100\n",
      "Gradient: [   7.5523    4.846    13.6527  -28.7065 -301.0447]\n",
      "Weights: [-4.8418  0.8141 -1.2085  0.0801  0.1517]\n",
      "MSE loss: 87.3975\n",
      "Iteration: 80200\n",
      "Gradient: [  8.1239 -18.237  -26.6495 -95.6772  29.2374]\n",
      "Weights: [-4.8314  0.8012 -1.2079  0.0798  0.1519]\n",
      "MSE loss: 87.3867\n",
      "Iteration: 80300\n",
      "Gradient: [  -6.6239   11.2713  -49.8218  -60.92   -128.3353]\n",
      "Weights: [-4.835   0.7998 -1.2079  0.0803  0.1518]\n",
      "MSE loss: 87.472\n",
      "Iteration: 80400\n",
      "Gradient: [ -6.8372 -17.343  -25.254  -72.4938  53.1277]\n",
      "Weights: [-4.8298  0.7964 -1.2097  0.0814  0.1518]\n",
      "MSE loss: 87.4025\n",
      "Iteration: 80500\n",
      "Gradient: [  -7.1248   -0.2641  -10.4372   37.8372 -216.1693]\n",
      "Weights: [-4.8396  0.7935 -1.2071  0.0808  0.1518]\n",
      "MSE loss: 87.8557\n",
      "Iteration: 80600\n",
      "Gradient: [ -6.675  -12.3456  12.4177  22.9665 -12.9596]\n",
      "Weights: [-4.855   0.8074 -1.2095  0.0813  0.1518]\n",
      "MSE loss: 87.7732\n",
      "Iteration: 80700\n",
      "Gradient: [ -8.582    9.6729 -28.1548 -18.5177 -51.127 ]\n",
      "Weights: [-4.8476  0.805  -1.2063  0.0805  0.1516]\n",
      "MSE loss: 87.4973\n",
      "Iteration: 80800\n",
      "Gradient: [   1.4252    1.1809    5.9529 -199.0532 -211.7391]\n",
      "Weights: [-4.8312  0.8049 -1.2099  0.0811  0.1515]\n",
      "MSE loss: 87.3645\n",
      "Iteration: 80900\n",
      "Gradient: [  2.1049  15.9795 -33.8639  16.3204 105.1969]\n",
      "Weights: [-4.8501  0.8165 -1.2101  0.0809  0.1515]\n",
      "MSE loss: 87.4261\n",
      "Iteration: 81000\n",
      "Gradient: [ -2.0075  -4.8368   8.3573  43.2537 -12.005 ]\n",
      "Weights: [-4.8474  0.8192 -1.2132  0.0813  0.1514]\n",
      "MSE loss: 87.4319\n",
      "Iteration: 81100\n",
      "Gradient: [   1.7019   -6.135   -13.1478   19.942  -101.6885]\n",
      "Weights: [-4.8404  0.8157 -1.2167  0.0829  0.1515]\n",
      "MSE loss: 87.3217\n",
      "Iteration: 81200\n",
      "Gradient: [  8.9965 -17.2478  12.8588   2.8505 106.6466]\n",
      "Weights: [-4.8353  0.8116 -1.2186  0.084   0.1517]\n",
      "MSE loss: 87.3124\n",
      "Iteration: 81300\n",
      "Gradient: [  5.0187  -2.2159 -23.2913  66.5652 -19.8898]\n",
      "Weights: [-4.8326  0.8097 -1.2171  0.084   0.1514]\n",
      "MSE loss: 87.2843\n",
      "Iteration: 81400\n",
      "Gradient: [   1.7207   14.0606  -29.3118 -105.1278  204.1869]\n",
      "Weights: [-4.8385  0.8017 -1.2158  0.085   0.1516]\n",
      "MSE loss: 87.4287\n",
      "Iteration: 81500\n",
      "Gradient: [ 10.0755  -2.6189  15.0265 -21.7687 -11.2217]\n",
      "Weights: [-4.8292  0.8046 -1.219   0.0857  0.1514]\n",
      "MSE loss: 87.3174\n",
      "Iteration: 81600\n",
      "Gradient: [  -0.7134    3.4432   -9.4936  -31.2543 -174.0387]\n",
      "Weights: [-4.82    0.7971 -1.2212  0.0861  0.1514]\n",
      "MSE loss: 87.4094\n",
      "Iteration: 81700\n",
      "Gradient: [   9.3498  -14.8579   37.3415  -65.8996 -112.4884]\n",
      "Weights: [-4.8205  0.8069 -1.2185  0.0851  0.1512]\n",
      "MSE loss: 87.4411\n",
      "Iteration: 81800\n",
      "Gradient: [ 10.4308  -3.0109  20.6424 -39.2342 292.4835]\n",
      "Weights: [-4.824   0.7952 -1.2157  0.0859  0.1511]\n",
      "MSE loss: 87.2861\n",
      "Iteration: 81900\n",
      "Gradient: [  3.6632  10.1591   4.835  -13.4932  29.1223]\n",
      "Weights: [-4.8249  0.7984 -1.215   0.0864  0.1509]\n",
      "MSE loss: 87.3663\n",
      "Iteration: 82000\n",
      "Gradient: [ -0.169   -2.3326  51.0147  70.5937 156.8551]\n",
      "Weights: [-4.8274  0.7978 -1.2144  0.0864  0.1508]\n",
      "MSE loss: 87.307\n",
      "Iteration: 82100\n",
      "Gradient: [ -7.4945 -11.4127 -53.8657 -40.8795 337.8506]\n",
      "Weights: [-4.8312  0.8068 -1.2204  0.0865  0.1508]\n",
      "MSE loss: 87.3528\n",
      "Iteration: 82200\n",
      "Gradient: [   4.5344  -11.6046    2.1233   20.3651 -222.8901]\n",
      "Weights: [-4.8423  0.8155 -1.2176  0.0857  0.1508]\n",
      "MSE loss: 87.2777\n",
      "Iteration: 82300\n",
      "Gradient: [   5.3624   21.7574   12.4601 -160.2743  246.1384]\n",
      "Weights: [-4.8352  0.8058 -1.2184  0.0867  0.1507]\n",
      "MSE loss: 87.2701\n",
      "Iteration: 82400\n",
      "Gradient: [ -0.6993   5.3464  24.9086 -67.1746 322.351 ]\n",
      "Weights: [-4.8365  0.8011 -1.2177  0.087   0.1509]\n",
      "MSE loss: 87.3323\n",
      "Iteration: 82500\n",
      "Gradient: [   5.0161    6.0941    7.4712   -6.7099 -240.4791]\n",
      "Weights: [-4.8416  0.8005 -1.2152  0.0867  0.1506]\n",
      "MSE loss: 87.4363\n",
      "Iteration: 82600\n",
      "Gradient: [   2.2432   22.5654    6.0145  144.7326 -219.764 ]\n",
      "Weights: [-4.8044  0.7888 -1.2184  0.0873  0.1509]\n",
      "MSE loss: 87.5335\n",
      "Iteration: 82700\n",
      "Gradient: [ -4.3477 -16.3328 -35.155  -12.6351 329.0494]\n",
      "Weights: [-4.8243  0.7995 -1.2198  0.0873  0.151 ]\n",
      "MSE loss: 87.2868\n",
      "Iteration: 82800\n",
      "Gradient: [ 16.8407  16.6394  -0.6919 -15.796  -74.8009]\n",
      "Weights: [-4.8189  0.8055 -1.2205  0.0875  0.1509]\n",
      "MSE loss: 87.644\n",
      "Iteration: 82900\n",
      "Gradient: [  -3.258    17.2144   -8.9584   56.5718 -114.5493]\n",
      "Weights: [-4.842   0.8151 -1.2226  0.0879  0.1506]\n",
      "MSE loss: 87.2512\n",
      "Iteration: 83000\n",
      "Gradient: [  -2.5641   -1.3611    9.051  -102.051    71.6779]\n",
      "Weights: [-4.8275  0.809  -1.2209  0.0877  0.1506]\n",
      "MSE loss: 87.2752\n",
      "Iteration: 83100\n",
      "Gradient: [   2.975   -16.0074  -13.4901   -4.8468 -289.8681]\n",
      "Weights: [-4.8227  0.7994 -1.2228  0.0894  0.1505]\n",
      "MSE loss: 87.2387\n",
      "Iteration: 83200\n",
      "Gradient: [  7.4561  -2.9929 -30.8649  27.2707 112.1808]\n",
      "Weights: [-4.8234  0.7933 -1.2193  0.0902  0.1502]\n",
      "MSE loss: 87.2599\n",
      "Iteration: 83300\n",
      "Gradient: [  -8.9219  -21.9454   13.2792 -110.0544  213.6708]\n",
      "Weights: [-4.8365  0.8003 -1.2208  0.09    0.1502]\n",
      "MSE loss: 87.3138\n",
      "Iteration: 83400\n",
      "Gradient: [  1.9783   3.4982  37.5927 105.2325 -74.7468]\n",
      "Weights: [-4.8361  0.8167 -1.225   0.0896  0.1503]\n",
      "MSE loss: 87.2498\n",
      "Iteration: 83500\n",
      "Gradient: [   1.6037   -3.8198  -23.2948 -109.9921 -121.0085]\n",
      "Weights: [-4.8376  0.8202 -1.229   0.0892  0.1506]\n",
      "MSE loss: 87.2164\n",
      "Iteration: 83600\n",
      "Gradient: [  -2.6083  -12.6634   22.1747  -60.6237 -374.2716]\n",
      "Weights: [-4.8462  0.8213 -1.2288  0.0891  0.1505]\n",
      "MSE loss: 87.4261\n",
      "Iteration: 83700\n",
      "Gradient: [  7.0959   2.5135  20.562   27.5368 398.3016]\n",
      "Weights: [-4.8356  0.8176 -1.2249  0.0894  0.1505]\n",
      "MSE loss: 87.4091\n",
      "Iteration: 83800\n",
      "Gradient: [  -9.0304   -6.4339  -41.428    10.7767 -448.0375]\n",
      "Weights: [-4.8507  0.8195 -1.2273  0.0896  0.1504]\n",
      "MSE loss: 87.4759\n",
      "Iteration: 83900\n",
      "Gradient: [  5.5612  -4.1312  -2.6168   1.1071 289.3119]\n",
      "Weights: [-4.841   0.8255 -1.2292  0.0893  0.1504]\n",
      "MSE loss: 87.2028\n",
      "Iteration: 84000\n",
      "Gradient: [ -3.9036   2.7916  -4.5412  82.5648 260.257 ]\n",
      "Weights: [-4.8428  0.823  -1.2268  0.0891  0.1503]\n",
      "MSE loss: 87.2118\n",
      "Iteration: 84100\n",
      "Gradient: [  9.4867  -3.7291  18.7243  -5.5321 498.7496]\n",
      "Weights: [-4.8075  0.8072 -1.2267  0.0892  0.1505]\n",
      "MSE loss: 87.738\n",
      "Iteration: 84200\n",
      "Gradient: [ -6.3254  22.803  -50.9618  74.715  152.5156]\n",
      "Weights: [-4.8279  0.8137 -1.2282  0.089   0.1509]\n",
      "MSE loss: 87.2737\n",
      "Iteration: 84300\n",
      "Gradient: [-5.8065 -1.9429 -6.7707 89.7169 23.861 ]\n",
      "Weights: [-4.8294  0.8114 -1.2263  0.0888  0.1507]\n",
      "MSE loss: 87.2237\n",
      "Iteration: 84400\n",
      "Gradient: [ -2.0643  -9.8076  -2.4434  52.7147 141.0106]\n",
      "Weights: [-4.8409  0.819  -1.2292  0.0896  0.1505]\n",
      "MSE loss: 87.2603\n",
      "Iteration: 84500\n",
      "Gradient: [ -9.7905   5.1356 -32.9542 -40.5793  29.6513]\n",
      "Weights: [-4.8483  0.8234 -1.2299  0.0897  0.1506]\n",
      "MSE loss: 87.3061\n",
      "Iteration: 84600\n",
      "Gradient: [ -6.832   -6.4514 -33.0059  -6.898  -74.4222]\n",
      "Weights: [-4.8362  0.8157 -1.2319  0.0905  0.1505]\n",
      "MSE loss: 87.358\n",
      "Iteration: 84700\n",
      "Gradient: [ -0.5542  14.6338   3.2394  54.3744 247.825 ]\n",
      "Weights: [-4.8224  0.8225 -1.2336  0.0909  0.1505]\n",
      "MSE loss: 87.5503\n",
      "Iteration: 84800\n",
      "Gradient: [   5.525    17.6792    0.9891  -43.0638 -221.8815]\n",
      "Weights: [-4.8424  0.8265 -1.2357  0.0907  0.1508]\n",
      "MSE loss: 87.2267\n",
      "Iteration: 84900\n",
      "Gradient: [  7.3579 -22.3124   4.0246  13.0458 -65.015 ]\n",
      "Weights: [-4.825   0.8216 -1.2379  0.0915  0.1508]\n",
      "MSE loss: 87.2742\n",
      "Iteration: 85000\n",
      "Gradient: [ -0.3285  -6.5412  27.5111  34.0862 190.6947]\n",
      "Weights: [-4.8342  0.8157 -1.2365  0.0923  0.1509]\n",
      "MSE loss: 87.304\n",
      "Iteration: 85100\n",
      "Gradient: [   4.4469    8.3525  -13.5159   57.7862 -121.8415]\n",
      "Weights: [-4.8308  0.8227 -1.2362  0.0915  0.1505]\n",
      "MSE loss: 87.1863\n",
      "Iteration: 85200\n",
      "Gradient: [ 1.847000e-01  1.436900e+00 -1.805280e+01  3.088980e+01 -2.215962e+02]\n",
      "Weights: [-4.8221  0.8251 -1.2374  0.0911  0.1504]\n",
      "MSE loss: 87.3718\n",
      "Iteration: 85300\n",
      "Gradient: [  -4.6363  -26.376   -19.454  -107.4684   -6.3086]\n",
      "Weights: [-4.8475  0.8222 -1.2347  0.0916  0.1503]\n",
      "MSE loss: 87.5816\n",
      "Iteration: 85400\n",
      "Gradient: [ -2.0379  -3.7159 -20.3715 -38.7327 188.9873]\n",
      "Weights: [-4.848   0.8262 -1.2374  0.093   0.1502]\n",
      "MSE loss: 87.3235\n",
      "Iteration: 85500\n",
      "Gradient: [   0.7674  -13.8917   14.5473   14.9558 -207.4938]\n",
      "Weights: [-4.8345  0.8278 -1.2385  0.0926  0.1502]\n",
      "MSE loss: 87.1562\n",
      "Iteration: 85600\n",
      "Gradient: [   3.2503   -6.6024  -12.4327  -19.6674 -327.0756]\n",
      "Weights: [-4.8519  0.8341 -1.2417  0.0936  0.1502]\n",
      "MSE loss: 87.3437\n",
      "Iteration: 85700\n",
      "Gradient: [-3.0774 -3.6805 47.6984 69.0096 57.9646]\n",
      "Weights: [-4.8372  0.8357 -1.2433  0.0941  0.1501]\n",
      "MSE loss: 87.1576\n",
      "Iteration: 85800\n",
      "Gradient: [  -6.3022   -7.0675    7.9397  -40.1245 -207.3542]\n",
      "Weights: [-4.8553  0.8469 -1.2477  0.0947  0.1499]\n",
      "MSE loss: 87.3452\n",
      "Iteration: 85900\n",
      "Gradient: [   1.0606   -6.6881    4.7329  -32.4319 -164.8235]\n",
      "Weights: [-4.8485  0.8529 -1.2482  0.0948  0.1498]\n",
      "MSE loss: 87.1938\n",
      "Iteration: 86000\n",
      "Gradient: [ -4.6962   3.738   -6.0909 -41.826  251.4316]\n",
      "Weights: [-4.8388  0.8418 -1.2492  0.0955  0.1498]\n",
      "MSE loss: 87.2063\n",
      "Iteration: 86100\n",
      "Gradient: [ -10.6312   -8.7598   -4.5492  -62.1134 -281.1523]\n",
      "Weights: [-4.8428  0.8312 -1.2461  0.0963  0.1497]\n",
      "MSE loss: 87.237\n",
      "Iteration: 86200\n",
      "Gradient: [-14.2774 -19.737  -15.9128 -77.1072 245.3874]\n",
      "Weights: [-4.8511  0.8433 -1.2481  0.0963  0.1495]\n",
      "MSE loss: 87.1956\n",
      "Iteration: 86300\n",
      "Gradient: [  1.4105  -8.1318  37.6773 -33.4104  94.733 ]\n",
      "Weights: [-4.833   0.8401 -1.2489  0.0971  0.1494]\n",
      "MSE loss: 87.1911\n",
      "Iteration: 86400\n",
      "Gradient: [  2.8976 -17.2685 -29.3408  20.1386 120.3345]\n",
      "Weights: [-4.8389  0.841  -1.2492  0.0974  0.1494]\n",
      "MSE loss: 87.0924\n",
      "Iteration: 86500\n",
      "Gradient: [ -0.5401   8.3233  32.1018  -0.8259 -60.4845]\n",
      "Weights: [-4.8391  0.8434 -1.2498  0.0967  0.1496]\n",
      "MSE loss: 87.1147\n",
      "Iteration: 86600\n",
      "Gradient: [  -3.3766   -3.0791   13.7146  -63.9031 -451.7231]\n",
      "Weights: [-4.8505  0.8421 -1.2484  0.0959  0.1498]\n",
      "MSE loss: 87.2305\n",
      "Iteration: 86700\n",
      "Gradient: [  2.2187  10.7638 -22.7239 -36.6986 136.0563]\n",
      "Weights: [-4.8561  0.8391 -1.2496  0.0961  0.15  ]\n",
      "MSE loss: 87.6235\n",
      "Iteration: 86800\n",
      "Gradient: [  1.5454  12.6068   5.2056  86.7507 -32.2001]\n",
      "Weights: [-4.8454  0.8407 -1.2479  0.0961  0.1499]\n",
      "MSE loss: 87.1094\n",
      "Iteration: 86900\n",
      "Gradient: [-11.4859  15.7902   7.3433 -27.758  114.1536]\n",
      "Weights: [-4.8561  0.8448 -1.2479  0.096   0.1498]\n",
      "MSE loss: 87.2438\n",
      "Iteration: 87000\n",
      "Gradient: [-6.452000e-01  1.566000e-01  1.407520e+01 -8.855770e+01 -4.783362e+02]\n",
      "Weights: [-4.85    0.8413 -1.2506  0.0971  0.1499]\n",
      "MSE loss: 87.1743\n",
      "Iteration: 87100\n",
      "Gradient: [-10.0949  -3.4012   4.0729  21.8851  80.9812]\n",
      "Weights: [-4.846   0.8369 -1.2492  0.0962  0.15  ]\n",
      "MSE loss: 87.2729\n",
      "Iteration: 87200\n",
      "Gradient: [  -5.2602   -6.1072   25.032  -111.9853 -169.5868]\n",
      "Weights: [-4.8313  0.8332 -1.2491  0.096   0.1499]\n",
      "MSE loss: 87.1889\n",
      "Iteration: 87300\n",
      "Gradient: [ 1.7702  0.3919 28.1376 45.1945 22.3229]\n",
      "Weights: [-4.865   0.8559 -1.2473  0.0956  0.1497]\n",
      "MSE loss: 87.3684\n",
      "Iteration: 87400\n",
      "Gradient: [  2.1708  -2.2873  -9.3659 -43.7324 -55.557 ]\n",
      "Weights: [-4.8699  0.8595 -1.2476  0.0951  0.1498]\n",
      "MSE loss: 87.4222\n",
      "Iteration: 87500\n",
      "Gradient: [ 6.5771  6.5922 39.4759 84.4662 76.774 ]\n",
      "Weights: [-4.8393  0.8437 -1.2479  0.0952  0.1499]\n",
      "MSE loss: 87.1493\n",
      "Iteration: 87600\n",
      "Gradient: [ -5.531  -11.7083 -10.9029 -86.2749  -3.3372]\n",
      "Weights: [-4.8354  0.8324 -1.2476  0.0959  0.1499]\n",
      "MSE loss: 87.1589\n",
      "Iteration: 87700\n",
      "Gradient: [  6.6244 -10.4913 -33.2649  75.746  -48.11  ]\n",
      "Weights: [-4.8407  0.8399 -1.2486  0.0962  0.1498]\n",
      "MSE loss: 87.0932\n",
      "Iteration: 87800\n",
      "Gradient: [   3.9714   -5.1916   -8.3473   21.4146 -249.8555]\n",
      "Weights: [-4.8533  0.856  -1.2522  0.0968  0.1497]\n",
      "MSE loss: 87.2147\n",
      "Iteration: 87900\n",
      "Gradient: [ -2.3984   1.9031  -5.9823  90.834  -40.8671]\n",
      "Weights: [-4.8377  0.8527 -1.2533  0.0961  0.1498]\n",
      "MSE loss: 87.241\n",
      "Iteration: 88000\n",
      "Gradient: [-13.0093 -10.522   12.2767   2.1337 -10.2802]\n",
      "Weights: [-4.8599  0.8528 -1.2534  0.0968  0.1496]\n",
      "MSE loss: 87.4827\n",
      "Iteration: 88100\n",
      "Gradient: [ 3.849  14.5868 -0.5067 38.1678 92.3784]\n",
      "Weights: [-4.8452  0.8434 -1.2526  0.0976  0.1498]\n",
      "MSE loss: 87.0968\n",
      "Iteration: 88200\n",
      "Gradient: [-1.76000e-02 -4.82280e+00  4.18890e+01 -6.05500e+00 -9.33281e+01]\n",
      "Weights: [-4.8437  0.8422 -1.2512  0.0979  0.1497]\n",
      "MSE loss: 87.145\n",
      "Iteration: 88300\n",
      "Gradient: [  9.0996  11.3322 -22.7587  12.6041 -99.2422]\n",
      "Weights: [-4.8369  0.8508 -1.2527  0.0975  0.1495]\n",
      "MSE loss: 87.3477\n",
      "Iteration: 88400\n",
      "Gradient: [   1.9976  -14.2827   23.9552   13.7596 -124.2522]\n",
      "Weights: [-4.8536  0.8482 -1.2507  0.0976  0.1494]\n",
      "MSE loss: 87.1265\n",
      "Iteration: 88500\n",
      "Gradient: [  0.617  -31.2887   7.8672  70.1636 136.3813]\n",
      "Weights: [-4.8421  0.8424 -1.2504  0.0983  0.1492]\n",
      "MSE loss: 87.0677\n",
      "Iteration: 88600\n",
      "Gradient: [  3.5728  -2.3223  -6.8566  25.9897 377.4273]\n",
      "Weights: [-4.8499  0.8507 -1.2503  0.0977  0.1491]\n",
      "MSE loss: 87.1105\n",
      "Iteration: 88700\n",
      "Gradient: [ -9.2814 -23.7498  31.3634 -65.6883 -65.3718]\n",
      "Weights: [-4.8648  0.8506 -1.2526  0.0976  0.1493]\n",
      "MSE loss: 87.78\n",
      "Iteration: 88800\n",
      "Gradient: [  -5.8027   -6.3965   -4.7726  -21.3695 -236.2795]\n",
      "Weights: [-4.8442  0.8359 -1.2517  0.0985  0.1492]\n",
      "MSE loss: 87.5425\n",
      "Iteration: 88900\n",
      "Gradient: [  -1.0158   -6.8275  -19.1651  -16.9817 -327.7618]\n",
      "Weights: [-4.8441  0.8333 -1.2506  0.0993  0.1491]\n",
      "MSE loss: 87.2353\n",
      "Iteration: 89000\n",
      "Gradient: [ -1.9269 -10.4617 -23.3673  27.6395  57.1208]\n",
      "Weights: [-4.8222  0.8224 -1.248   0.0988  0.1491]\n",
      "MSE loss: 87.1986\n",
      "Iteration: 89100\n",
      "Gradient: [  -1.4247   -2.4777  -55.2748   26.4423 -255.6339]\n",
      "Weights: [-4.8287  0.8217 -1.2476  0.0992  0.149 ]\n",
      "MSE loss: 87.2108\n",
      "Iteration: 89200\n",
      "Gradient: [  -4.0873   -7.9642  -10.2778  -72.5726 -260.7883]\n",
      "Weights: [-4.845   0.8368 -1.2493  0.0994  0.149 ]\n",
      "MSE loss: 87.0576\n",
      "Iteration: 89300\n",
      "Gradient: [  -3.6556   -8.7996   -8.7871  -47.592  -155.5012]\n",
      "Weights: [-4.8382  0.8331 -1.2512  0.0995  0.1491]\n",
      "MSE loss: 87.1003\n",
      "Iteration: 89400\n",
      "Gradient: [ 4.4814 -6.43   16.6649 10.005  27.2517]\n",
      "Weights: [-4.8349  0.8327 -1.2492  0.0992  0.149 ]\n",
      "MSE loss: 87.047\n",
      "Iteration: 89500\n",
      "Gradient: [   5.9685   15.1755   17.0762  -16.6102 -146.6715]\n",
      "Weights: [-4.8433  0.836  -1.2476  0.099   0.1488]\n",
      "MSE loss: 87.0516\n",
      "Iteration: 89600\n",
      "Gradient: [ -0.7364   1.0389   7.3151  38.8757 112.5094]\n",
      "Weights: [-4.8393  0.8302 -1.2464  0.0996  0.1489]\n",
      "MSE loss: 87.0713\n",
      "Iteration: 89700\n",
      "Gradient: [-12.0366  -5.3784  35.1393 -64.1143  97.5526]\n",
      "Weights: [-4.8284  0.8214 -1.2451  0.0995  0.1489]\n",
      "MSE loss: 87.0798\n",
      "Iteration: 89800\n",
      "Gradient: [ -11.112     0.2607   -2.8279  -78.2175 -237.077 ]\n",
      "Weights: [-4.8466  0.8298 -1.2442  0.0994  0.1486]\n",
      "MSE loss: 87.0957\n",
      "Iteration: 89900\n",
      "Gradient: [  0.9029   1.41    40.1844  21.3401 364.038 ]\n",
      "Weights: [-4.828   0.8328 -1.2463  0.0996  0.1487]\n",
      "MSE loss: 87.4352\n",
      "Iteration: 90000\n",
      "Gradient: [   0.953    -5.8479  -19.3793  -25.0673 -108.7722]\n",
      "Weights: [-4.8531  0.8425 -1.2509  0.1004  0.1485]\n",
      "MSE loss: 87.1432\n",
      "Iteration: 90100\n",
      "Gradient: [  -4.1377   -5.6619   20.439   -59.3629 -317.3256]\n",
      "Weights: [-4.8405  0.8391 -1.2505  0.1002  0.1488]\n",
      "MSE loss: 87.086\n",
      "Iteration: 90200\n",
      "Gradient: [  8.2924   8.2546  20.3678  17.9986 121.6596]\n",
      "Weights: [-4.8303  0.8351 -1.2524  0.1009  0.1488]\n",
      "MSE loss: 87.1494\n",
      "Iteration: 90300\n",
      "Gradient: [ -7.8359   2.9504 -26.1102  45.0358 -50.6591]\n",
      "Weights: [-4.8381  0.8287 -1.2477  0.0996  0.149 ]\n",
      "MSE loss: 87.0502\n",
      "Iteration: 90400\n",
      "Gradient: [  10.832    -9.1595  -12.4752    5.6746 -278.3404]\n",
      "Weights: [-4.8418  0.8366 -1.2485  0.0995  0.1488]\n",
      "MSE loss: 87.036\n",
      "Iteration: 90500\n",
      "Gradient: [  3.3886  14.6745  40.9987 -36.9402 -92.8739]\n",
      "Weights: [-4.8518  0.8452 -1.2486  0.0991  0.1486]\n",
      "MSE loss: 87.0926\n",
      "Iteration: 90600\n",
      "Gradient: [  0.4429  -7.8151  -6.2035  -9.8945 -32.2864]\n",
      "Weights: [-4.841   0.8403 -1.2483  0.0995  0.1486]\n",
      "MSE loss: 87.0799\n",
      "Iteration: 90700\n",
      "Gradient: [  -4.7482   -1.616     4.1414   35.5854 -143.1039]\n",
      "Weights: [-4.8504  0.8507 -1.2496  0.0989  0.1487]\n",
      "MSE loss: 87.1853\n",
      "Iteration: 90800\n",
      "Gradient: [  8.6453   1.5633  41.5055  33.2207 133.7292]\n",
      "Weights: [-4.8535  0.851  -1.2502  0.099   0.1487]\n",
      "MSE loss: 87.1281\n",
      "Iteration: 90900\n",
      "Gradient: [  -4.3378  -11.6292  -18.8051  -35.6577 -326.7666]\n",
      "Weights: [-4.8599  0.8564 -1.254   0.0998  0.1487]\n",
      "MSE loss: 87.1423\n",
      "Iteration: 91000\n",
      "Gradient: [ -5.6933 -18.4217  11.6697 -21.769   38.3843]\n",
      "Weights: [-4.8577  0.858  -1.2551  0.0998  0.1489]\n",
      "MSE loss: 87.1374\n",
      "Iteration: 91100\n",
      "Gradient: [  4.7971  -1.1808 -41.1295  59.7683 160.1374]\n",
      "Weights: [-4.8586  0.8573 -1.2531  0.0991  0.1486]\n",
      "MSE loss: 87.1703\n",
      "Iteration: 91200\n",
      "Gradient: [  2.5576   8.1869   2.9676 -77.4141 147.3699]\n",
      "Weights: [-4.8524  0.8563 -1.255   0.0998  0.1486]\n",
      "MSE loss: 87.0972\n",
      "Iteration: 91300\n",
      "Gradient: [  8.1636  -2.0986  60.2885 152.7233  69.638 ]\n",
      "Weights: [-4.8285  0.8495 -1.2531  0.0997  0.1488]\n",
      "MSE loss: 87.9333\n",
      "Iteration: 91400\n",
      "Gradient: [  5.1819   1.8507  29.062   31.299  296.1107]\n",
      "Weights: [-4.8397  0.851  -1.2536  0.1002  0.1488]\n",
      "MSE loss: 87.4617\n",
      "Iteration: 91500\n",
      "Gradient: [   1.7204    0.1523  -14.6869   31.2414 -130.9914]\n",
      "Weights: [-4.8344  0.8352 -1.2529  0.1004  0.1489]\n",
      "MSE loss: 87.0469\n",
      "Iteration: 91600\n",
      "Gradient: [  0.9395   8.3236 -22.9498  21.2746  -9.1716]\n",
      "Weights: [-4.8446  0.8438 -1.2515  0.1001  0.1488]\n",
      "MSE loss: 87.0716\n",
      "Iteration: 91700\n",
      "Gradient: [ -1.9657  -0.9714 -28.6422  22.9983  57.6222]\n",
      "Weights: [-4.8496  0.8468 -1.2519  0.0997  0.1487]\n",
      "MSE loss: 87.0594\n",
      "Iteration: 91800\n",
      "Gradient: [ -3.5314  -5.4346  16.0217 -25.4423 -72.2804]\n",
      "Weights: [-4.8435  0.8407 -1.2514  0.0988  0.149 ]\n",
      "MSE loss: 87.1284\n",
      "Iteration: 91900\n",
      "Gradient: [ 3.600000e-02 -1.362700e+00  1.076080e+01  7.528360e+01 -3.071909e+02]\n",
      "Weights: [-4.8387  0.8453 -1.253   0.0996  0.1491]\n",
      "MSE loss: 87.2581\n",
      "Iteration: 92000\n",
      "Gradient: [  2.7913   7.1531  35.1797  34.2338 217.857 ]\n",
      "Weights: [-4.8445  0.8334 -1.2487  0.1     0.1492]\n",
      "MSE loss: 87.2336\n",
      "Iteration: 92100\n",
      "Gradient: [ -10.0402   16.1791   63.9778 -100.6101   -2.6658]\n",
      "Weights: [-4.8356  0.8377 -1.2502  0.0996  0.1489]\n",
      "MSE loss: 87.1176\n",
      "Iteration: 92200\n",
      "Gradient: [ -4.8193  17.0558  20.045   24.6523 118.2437]\n",
      "Weights: [-4.8453  0.8397 -1.2509  0.0995  0.1489]\n",
      "MSE loss: 87.0534\n",
      "Iteration: 92300\n",
      "Gradient: [ -3.9666   4.4768   4.6039  64.0256 132.4776]\n",
      "Weights: [-4.8276  0.8393 -1.2521  0.1004  0.1489]\n",
      "MSE loss: 87.509\n",
      "Iteration: 92400\n",
      "Gradient: [  -2.0971   -2.608    -4.3878  -91.5869 -214.0006]\n",
      "Weights: [-4.853   0.8448 -1.2549  0.1001  0.1489]\n",
      "MSE loss: 87.3188\n",
      "Iteration: 92500\n",
      "Gradient: [ -0.2621  -8.1183 -65.0969 -79.2709 -50.33  ]\n",
      "Weights: [-4.8485  0.8378 -1.254   0.1004  0.1489]\n",
      "MSE loss: 87.4196\n",
      "Iteration: 92600\n",
      "Gradient: [ 7.160300e+00  1.091000e-01  5.533400e+00  2.506890e+01 -2.098112e+02]\n",
      "Weights: [-4.841   0.8441 -1.2561  0.101   0.1488]\n",
      "MSE loss: 87.018\n",
      "Iteration: 92700\n",
      "Gradient: [  3.2431  12.6588 -18.2155  91.6356 258.9049]\n",
      "Weights: [-4.8474  0.8398 -1.2527  0.1006  0.1489]\n",
      "MSE loss: 87.0649\n",
      "Iteration: 92800\n",
      "Gradient: [   7.6015    7.5749   -9.4796    8.6277 -117.7223]\n",
      "Weights: [-4.8377  0.8427 -1.2548  0.1014  0.1488]\n",
      "MSE loss: 87.1638\n",
      "Iteration: 92900\n",
      "Gradient: [ -7.3859  24.799   -9.3282 148.751   59.2762]\n",
      "Weights: [-4.8487  0.8449 -1.2536  0.1009  0.1489]\n",
      "MSE loss: 87.1433\n",
      "Iteration: 93000\n",
      "Gradient: [ -7.159    2.8119 -43.6036  89.6916 130.3351]\n",
      "Weights: [-4.8408  0.8403 -1.2556  0.1011  0.1489]\n",
      "MSE loss: 87.0352\n",
      "Iteration: 93100\n",
      "Gradient: [   5.1837    9.5992  -46.99    -24.7376 -111.7669]\n",
      "Weights: [-4.8248  0.8428 -1.2569  0.1005  0.1489]\n",
      "MSE loss: 87.3027\n",
      "Iteration: 93200\n",
      "Gradient: [ -0.6369   3.2648 -17.5225 -60.2976  19.6283]\n",
      "Weights: [-4.8369  0.8447 -1.2567  0.1004  0.149 ]\n",
      "MSE loss: 87.0508\n",
      "Iteration: 93300\n",
      "Gradient: [ 1.5608 35.2362  6.7314 34.3249 60.6419]\n",
      "Weights: [-4.8203  0.8418 -1.2566  0.1009  0.149 ]\n",
      "MSE loss: 87.649\n",
      "Iteration: 93400\n",
      "Gradient: [-14.0706  -3.5893 -66.9398 -60.8668 -49.125 ]\n",
      "Weights: [-4.8469  0.8454 -1.2585  0.1006  0.1491]\n",
      "MSE loss: 87.2276\n",
      "Iteration: 93500\n",
      "Gradient: [   8.4885    5.3651   -4.0428   17.5888 -148.3692]\n",
      "Weights: [-4.8419  0.8372 -1.2546  0.1009  0.1492]\n",
      "MSE loss: 87.0695\n",
      "Iteration: 93600\n",
      "Gradient: [-16.6844   8.5314 -23.2047 -51.3653 138.2606]\n",
      "Weights: [-4.8398  0.836  -1.2564  0.101   0.1491]\n",
      "MSE loss: 87.133\n",
      "Iteration: 93700\n",
      "Gradient: [-1.06312e+01  1.66800e-01  1.57708e+01 -3.12150e+01  3.86495e+02]\n",
      "Weights: [-4.8559  0.8452 -1.2563  0.0996  0.1495]\n",
      "MSE loss: 87.355\n",
      "Iteration: 93800\n",
      "Gradient: [  -3.9498    2.8808    7.9218  -59.4245 -194.1317]\n",
      "Weights: [-4.8405  0.8457 -1.256   0.0997  0.1495]\n",
      "MSE loss: 87.1303\n",
      "Iteration: 93900\n",
      "Gradient: [-18.1099 -14.321  -32.0439 -36.8081 -96.1449]\n",
      "Weights: [-4.8482  0.8426 -1.2561  0.0998  0.1493]\n",
      "MSE loss: 87.2073\n",
      "Iteration: 94000\n",
      "Gradient: [  -1.4197   -4.9105  -34.5425  -28.6208 -250.7819]\n",
      "Weights: [-4.8384  0.8389 -1.2547  0.0991  0.1494]\n",
      "MSE loss: 87.088\n",
      "Iteration: 94100\n",
      "Gradient: [16.7533 15.7951 22.8229  4.4441 -7.9998]\n",
      "Weights: [-4.8391  0.8461 -1.2553  0.0995  0.1495]\n",
      "MSE loss: 87.1911\n",
      "Iteration: 94200\n",
      "Gradient: [  2.5152  -5.4269 -21.1759 -96.3127 233.6101]\n",
      "Weights: [-4.8328  0.8377 -1.2569  0.0995  0.1494]\n",
      "MSE loss: 87.1948\n",
      "Iteration: 94300\n",
      "Gradient: [ 10.6034   9.1103  32.0876  58.1246 182.7845]\n",
      "Weights: [-4.8247  0.8387 -1.2578  0.1004  0.1496]\n",
      "MSE loss: 87.4193\n",
      "Iteration: 94400\n",
      "Gradient: [  8.11    25.1711   4.969  105.0683   7.592 ]\n",
      "Weights: [-4.83    0.8396 -1.2556  0.1002  0.1494]\n",
      "MSE loss: 87.324\n",
      "Iteration: 94500\n",
      "Gradient: [ -7.1871  13.3689  35.0203  35.348  181.3241]\n",
      "Weights: [-4.844   0.8409 -1.2538  0.0998  0.1495]\n",
      "MSE loss: 87.1299\n",
      "Iteration: 94600\n",
      "Gradient: [  2.8504  -5.157  -27.671  -94.368  -93.7623]\n",
      "Weights: [-4.849   0.8506 -1.2548  0.0992  0.1491]\n",
      "MSE loss: 87.0597\n",
      "Iteration: 94700\n",
      "Gradient: [ -3.6707  -7.5286  -7.0719  16.062  232.3633]\n",
      "Weights: [-4.839   0.8445 -1.255   0.0997  0.1492]\n",
      "MSE loss: 87.0521\n",
      "Iteration: 94800\n",
      "Gradient: [ 13.4933  -7.3343  12.519   85.3985 215.4756]\n",
      "Weights: [-4.8384  0.8488 -1.2563  0.1     0.1491]\n",
      "MSE loss: 87.1272\n",
      "Iteration: 94900\n",
      "Gradient: [  -5.3098  -15.4575   -7.9042 -178.5235  -89.0587]\n",
      "Weights: [-4.8609  0.8518 -1.2556  0.0995  0.149 ]\n",
      "MSE loss: 87.4755\n",
      "Iteration: 95000\n",
      "Gradient: [ -12.551   -11.0594    7.6089  -70.7072 -165.9573]\n",
      "Weights: [-4.8552  0.8416 -1.254   0.1004  0.1491]\n",
      "MSE loss: 87.3151\n",
      "Iteration: 95100\n",
      "Gradient: [   1.6593    9.2347   -5.6255   48.5974 -169.8624]\n",
      "Weights: [-4.8325  0.8286 -1.2546  0.1011  0.1491]\n",
      "MSE loss: 87.0949\n",
      "Iteration: 95200\n",
      "Gradient: [ -1.8587  -1.624  -27.7479 -33.6765  11.5972]\n",
      "Weights: [-4.844   0.8371 -1.2563  0.1009  0.1492]\n",
      "MSE loss: 87.1701\n",
      "Iteration: 95300\n",
      "Gradient: [  2.8115  -1.2418 -33.1075  -6.8183 110.8641]\n",
      "Weights: [-4.8407  0.8353 -1.254   0.1014  0.1492]\n",
      "MSE loss: 87.1293\n",
      "Iteration: 95400\n",
      "Gradient: [  -0.6002    3.5373   -6.5744  -24.5016 -209.4514]\n",
      "Weights: [-4.8384  0.8369 -1.2538  0.1009  0.1488]\n",
      "MSE loss: 87.0224\n",
      "Iteration: 95500\n",
      "Gradient: [ 10.206   -2.4889  28.3509 -13.2067  80.6544]\n",
      "Weights: [-4.8349  0.8307 -1.2503  0.1002  0.1488]\n",
      "MSE loss: 87.05\n",
      "Iteration: 95600\n",
      "Gradient: [  -2.051   -18.2254   -2.1484 -116.6384  -61.6703]\n",
      "Weights: [-4.8304  0.8273 -1.2528  0.1011  0.149 ]\n",
      "MSE loss: 87.0528\n",
      "Iteration: 95700\n",
      "Gradient: [   0.7658  -16.8667    9.3505  -38.8265 -232.6637]\n",
      "Weights: [-4.8295  0.8362 -1.2565  0.1009  0.1489]\n",
      "MSE loss: 87.1818\n",
      "Iteration: 95800\n",
      "Gradient: [  -3.4962   -2.6678  -22.5921  -36.5531 -161.8441]\n",
      "Weights: [-4.8569  0.8542 -1.2595  0.1019  0.1486]\n",
      "MSE loss: 87.1535\n",
      "Iteration: 95900\n",
      "Gradient: [  1.386   -8.9146  63.8512 114.3756 156.2796]\n",
      "Weights: [-4.8487  0.8582 -1.2591  0.1019  0.1486]\n",
      "MSE loss: 87.1868\n",
      "Iteration: 96000\n",
      "Gradient: [   9.336    -1.817     9.8334   14.0876 -221.5276]\n",
      "Weights: [-4.8398  0.8453 -1.2598  0.1018  0.1489]\n",
      "MSE loss: 87.0138\n",
      "Iteration: 96100\n",
      "Gradient: [ -1.9144   4.5671 -38.1424 -64.3065 364.658 ]\n",
      "Weights: [-4.8241  0.8326 -1.2565  0.1018  0.149 ]\n",
      "MSE loss: 87.1381\n",
      "Iteration: 96200\n",
      "Gradient: [ -9.6733  -8.1912  -3.8428 -36.6138 -68.2343]\n",
      "Weights: [-4.8212  0.8214 -1.257   0.1024  0.1491]\n",
      "MSE loss: 87.2073\n",
      "Iteration: 96300\n",
      "Gradient: [ -5.6811 -19.3245  17.8231   4.5835 301.7922]\n",
      "Weights: [-4.8477  0.8308 -1.2537  0.1029  0.1488]\n",
      "MSE loss: 87.2422\n",
      "Iteration: 96400\n",
      "Gradient: [   4.7063    5.0329   21.4409  -96.5541 -101.1589]\n",
      "Weights: [-4.8314  0.8275 -1.2534  0.1032  0.1485]\n",
      "MSE loss: 87.0268\n",
      "Iteration: 96500\n",
      "Gradient: [  -3.1268    4.5041   -5.1749  -80.5898 -239.6034]\n",
      "Weights: [-4.8345  0.8242 -1.2515  0.1026  0.1484]\n",
      "MSE loss: 87.0635\n",
      "Iteration: 96600\n",
      "Gradient: [  -4.9304   13.3703   -7.0959  -47.18   -116.1016]\n",
      "Weights: [-4.8455  0.8245 -1.2485  0.1022  0.1484]\n",
      "MSE loss: 87.2014\n",
      "Iteration: 96700\n",
      "Gradient: [  -7.3241   -3.7464   -5.4804 -141.859    39.8545]\n",
      "Weights: [-4.8441  0.8234 -1.2469  0.1012  0.1485]\n",
      "MSE loss: 87.2342\n",
      "Iteration: 96800\n",
      "Gradient: [ -9.1386  -1.5497 -58.9895 -46.2026 -15.1542]\n",
      "Weights: [-4.8435  0.8291 -1.2501  0.1019  0.1484]\n",
      "MSE loss: 87.0963\n",
      "Iteration: 96900\n",
      "Gradient: [  4.4604  20.9639 -33.6105 -22.5755  67.465 ]\n",
      "Weights: [-4.8342  0.8346 -1.2536  0.1027  0.1484]\n",
      "MSE loss: 87.0324\n",
      "Iteration: 97000\n",
      "Gradient: [ -4.219  -18.9991 -40.1671 -78.8499  49.0571]\n",
      "Weights: [-4.8371  0.8307 -1.2524  0.1028  0.1481]\n",
      "MSE loss: 87.0482\n",
      "Iteration: 97100\n",
      "Gradient: [  8.2368  -8.2355 -19.9135  33.5345 243.3175]\n",
      "Weights: [-4.8393  0.8308 -1.2526  0.1027  0.1481]\n",
      "MSE loss: 87.0943\n",
      "Iteration: 97200\n",
      "Gradient: [   1.2573   -3.33    -15.5037  -24.1323 -136.1867]\n",
      "Weights: [-4.8444  0.8353 -1.2541  0.1027  0.1482]\n",
      "MSE loss: 87.1281\n",
      "Iteration: 97300\n",
      "Gradient: [ -2.4574   0.4156 -36.9362  96.7733 144.5552]\n",
      "Weights: [-4.836   0.8371 -1.2538  0.1031  0.1482]\n",
      "MSE loss: 87.0447\n",
      "Iteration: 97400\n",
      "Gradient: [  6.7078   6.5126  62.4951 106.3281 182.9085]\n",
      "Weights: [-4.824   0.8297 -1.2517  0.1027  0.1483]\n",
      "MSE loss: 87.2469\n",
      "Iteration: 97500\n",
      "Gradient: [  7.5089  14.2096  21.3564 -30.6338 -81.2801]\n",
      "Weights: [-4.84    0.8338 -1.2519  0.1032  0.1482]\n",
      "MSE loss: 87.0463\n",
      "Iteration: 97600\n",
      "Gradient: [  -7.874    -5.4882  -12.7437   22.2109 -131.8035]\n",
      "Weights: [-4.8489  0.8332 -1.2516  0.1034  0.1482]\n",
      "MSE loss: 87.0993\n",
      "Iteration: 97700\n",
      "Gradient: [  8.627   12.2222  -3.2316 -97.1796  55.2318]\n",
      "Weights: [-4.8286  0.8264 -1.2533  0.1033  0.1482]\n",
      "MSE loss: 87.0322\n",
      "Iteration: 97800\n",
      "Gradient: [  5.54     3.0549  39.6469 -47.0967  60.3352]\n",
      "Weights: [-4.8321  0.8382 -1.2546  0.1024  0.1483]\n",
      "MSE loss: 87.0633\n",
      "Iteration: 97900\n",
      "Gradient: [  -4.5106    9.2522   -5.0653 -110.3926   17.7541]\n",
      "Weights: [-4.8443  0.8386 -1.2542  0.1027  0.1481]\n",
      "MSE loss: 87.0496\n",
      "Iteration: 98000\n",
      "Gradient: [  3.7118   5.9067  11.2721 -60.376   20.767 ]\n",
      "Weights: [-4.8495  0.8525 -1.2553  0.1026  0.148 ]\n",
      "MSE loss: 87.0996\n",
      "Iteration: 98100\n",
      "Gradient: [ 14.6333 -10.1899  56.6625 105.1731   1.7293]\n",
      "Weights: [-4.8511  0.8556 -1.2553  0.1026  0.1483]\n",
      "MSE loss: 87.5772\n",
      "Iteration: 98200\n",
      "Gradient: [  9.6778 -20.1854  -8.161  -39.7653  91.7579]\n",
      "Weights: [-4.8372  0.8421 -1.2567  0.1027  0.1483]\n",
      "MSE loss: 87.0063\n",
      "Iteration: 98300\n",
      "Gradient: [  -5.7695   -3.1669  -30.3244  -10.8755 -156.7708]\n",
      "Weights: [-4.8514  0.838  -1.2529  0.1028  0.1481]\n",
      "MSE loss: 87.1287\n",
      "Iteration: 98400\n",
      "Gradient: [  4.1744  -4.9527 -29.9143   6.5462   4.3111]\n",
      "Weights: [-4.8502  0.8406 -1.2514  0.103   0.1479]\n",
      "MSE loss: 87.0513\n",
      "Iteration: 98500\n",
      "Gradient: [-1.587000e-01 -6.948000e-01  2.310640e+01  6.018950e+01  1.637338e+02]\n",
      "Weights: [-4.8498  0.8426 -1.2516  0.1029  0.1479]\n",
      "MSE loss: 87.0495\n",
      "Iteration: 98600\n",
      "Gradient: [  -2.8477   -0.4975   20.3002 -113.6452  -82.5993]\n",
      "Weights: [-4.8251  0.8365 -1.2543  0.1032  0.1479]\n",
      "MSE loss: 87.229\n",
      "Iteration: 98700\n",
      "Gradient: [  -7.7524  -14.2221    5.861   -86.7199 -125.9338]\n",
      "Weights: [-4.838   0.827  -1.254   0.1047  0.148 ]\n",
      "MSE loss: 87.0105\n",
      "Iteration: 98800\n",
      "Gradient: [  3.8457   5.9511 -13.6134 116.1336 193.4136]\n",
      "Weights: [-4.8164  0.8229 -1.2532  0.1051  0.1478]\n",
      "MSE loss: 87.343\n",
      "Iteration: 98900\n",
      "Gradient: [  0.944    5.8282  -8.7206  64.0092 151.9025]\n",
      "Weights: [-4.8243  0.8194 -1.2544  0.1055  0.1479]\n",
      "MSE loss: 87.0054\n",
      "Iteration: 99000\n",
      "Gradient: [ -5.5894  -2.3261  -1.6971  31.8193 473.0504]\n",
      "Weights: [-4.8361  0.8188 -1.2531  0.1061  0.1477]\n",
      "MSE loss: 87.0785\n",
      "Iteration: 99100\n",
      "Gradient: [ -0.4137 -10.4667  10.3953 -48.0686 -60.4584]\n",
      "Weights: [-4.8254  0.82   -1.2537  0.106   0.1475]\n",
      "MSE loss: 86.9765\n",
      "Iteration: 99200\n",
      "Gradient: [  11.3214    2.5273  -24.9483  -71.4285 -299.3378]\n",
      "Weights: [-4.8171  0.8205 -1.2553  0.1071  0.1475]\n",
      "MSE loss: 87.2026\n",
      "Iteration: 99300\n",
      "Gradient: [  1.5015   3.8548  20.475   59.9966 291.3329]\n",
      "Weights: [-4.8346  0.8299 -1.2562  0.1073  0.1475]\n",
      "MSE loss: 87.0548\n",
      "Iteration: 99400\n",
      "Gradient: [-9.3455  5.2666  8.5963  8.5396 92.6051]\n",
      "Weights: [-4.8292  0.8292 -1.2612  0.1066  0.1475]\n",
      "MSE loss: 87.4199\n",
      "Iteration: 99500\n",
      "Gradient: [  8.9821  19.4539 -38.4831 -60.4835 290.6186]\n",
      "Weights: [-4.8225  0.8255 -1.2594  0.1072  0.1475]\n",
      "MSE loss: 87.0004\n",
      "Iteration: 99600\n",
      "Gradient: [   1.3589   -5.3688   36.3197  -16.5797 -121.9831]\n",
      "Weights: [-4.8405  0.8311 -1.2575  0.1075  0.1475]\n",
      "MSE loss: 86.9719\n",
      "Iteration: 99700\n",
      "Gradient: [ 16.4863   2.1936 -14.6019  40.7604  29.8208]\n",
      "Weights: [-4.8237  0.8339 -1.2593  0.108   0.1473]\n",
      "MSE loss: 87.3445\n",
      "Iteration: 99800\n",
      "Gradient: [  -4.8076    0.3682   17.4507 -107.5007 -322.1705]\n",
      "Weights: [-4.8251  0.8254 -1.2609  0.1092  0.147 ]\n",
      "MSE loss: 86.9523\n",
      "Iteration: 99900\n",
      "Gradient: [ -7.9018  -1.4136  -6.6244  27.2847 224.5915]\n",
      "Weights: [-4.8435  0.8336 -1.2613  0.1096  0.1472]\n",
      "MSE loss: 86.9811\n",
      "Iteration: 100000\n",
      "Gradient: [  19.3571   27.3169    7.4201   80.0706 -116.5403]\n",
      "Weights: [-4.8483  0.8315 -1.2574  0.1089  0.1471]\n",
      "MSE loss: 87.0603\n",
      "Iteration: 100100\n",
      "Gradient: [  -9.9376    7.6614  -54.808  -100.5199    7.0318]\n",
      "Weights: [-4.8533  0.8382 -1.2581  0.1084  0.147 ]\n",
      "MSE loss: 87.0603\n",
      "Iteration: 100200\n",
      "Gradient: [  3.7976 -16.3201 -11.5199 -61.2251 -43.0435]\n",
      "Weights: [-4.8547  0.8391 -1.2596  0.1086  0.1468]\n",
      "MSE loss: 87.2493\n",
      "Iteration: 100300\n",
      "Gradient: [  7.6163  12.3508   8.5025  13.708  -17.0467]\n",
      "Weights: [-4.8294  0.8371 -1.2629  0.1096  0.1469]\n",
      "MSE loss: 87.0212\n",
      "Iteration: 100400\n",
      "Gradient: [ -2.2303 -21.5478  24.8449 -22.5618  62.0281]\n",
      "Weights: [-4.832   0.8336 -1.2638  0.1103  0.1469]\n",
      "MSE loss: 86.8751\n",
      "Iteration: 100500\n",
      "Gradient: [ -1.8398  -7.9619   8.7313 114.7662 -88.1967]\n",
      "Weights: [-4.8552  0.8469 -1.2617  0.1098  0.1463]\n",
      "MSE loss: 87.0302\n",
      "Iteration: 100600\n",
      "Gradient: [-13.1687 -14.7194   2.2595 -43.5119   1.6399]\n",
      "Weights: [-4.8631  0.8513 -1.2647  0.1103  0.1465]\n",
      "MSE loss: 87.1927\n",
      "Iteration: 100700\n",
      "Gradient: [  -9.9761  -21.0164  -28.9287  -45.676  -295.739 ]\n",
      "Weights: [-4.8323  0.8458 -1.2681  0.1105  0.1466]\n",
      "MSE loss: 86.9563\n",
      "Iteration: 100800\n",
      "Gradient: [ -6.6704  -3.6704 -29.3186  55.1874  27.3223]\n",
      "Weights: [-4.8375  0.8467 -1.2668  0.1101  0.1466]\n",
      "MSE loss: 86.9278\n",
      "Iteration: 100900\n",
      "Gradient: [  -3.6169  -10.4215  -18.39   -124.8067  -41.0379]\n",
      "Weights: [-4.8593  0.8493 -1.2637  0.1096  0.1468]\n",
      "MSE loss: 87.0717\n",
      "Iteration: 101000\n",
      "Gradient: [-8.36000e-01 -1.73600e-01  1.19195e+01  5.05430e+01 -1.75263e+02]\n",
      "Weights: [-4.8402  0.847  -1.2626  0.1097  0.1465]\n",
      "MSE loss: 87.0054\n",
      "Iteration: 101100\n",
      "Gradient: [   8.46     13.7937   -1.368    81.0489 -405.1247]\n",
      "Weights: [-4.847   0.8503 -1.2641  0.1093  0.1468]\n",
      "MSE loss: 86.9453\n",
      "Iteration: 101200\n",
      "Gradient: [ -0.9158   4.3052 -13.6084  80.0636  81.8177]\n",
      "Weights: [-4.8605  0.8655 -1.2668  0.1088  0.1468]\n",
      "MSE loss: 87.044\n",
      "Iteration: 101300\n",
      "Gradient: [-13.2525   4.2326  38.3225 -19.73    80.203 ]\n",
      "Weights: [-4.8569  0.8615 -1.2669  0.1092  0.1468]\n",
      "MSE loss: 86.9876\n",
      "Iteration: 101400\n",
      "Gradient: [  1.4121  -6.4699  15.9365 -31.1247  50.4305]\n",
      "Weights: [-4.8727  0.8635 -1.2678  0.1093  0.1468]\n",
      "MSE loss: 87.3906\n",
      "Iteration: 101500\n",
      "Gradient: [ 1.022000e-01 -6.496300e+00  8.167200e+00  5.584090e+01 -2.368207e+02]\n",
      "Weights: [-4.8689  0.8653 -1.2709  0.11    0.1469]\n",
      "MSE loss: 87.2187\n",
      "Iteration: 101600\n",
      "Gradient: [ -8.1978   1.8141  16.2092 -86.1148 153.028 ]\n",
      "Weights: [-4.8537  0.8646 -1.2712  0.1099  0.1468]\n",
      "MSE loss: 86.9357\n",
      "Iteration: 101700\n",
      "Gradient: [   2.0892    0.674    -3.1046 -110.1576   53.2989]\n",
      "Weights: [-4.8536  0.864  -1.2719  0.1099  0.147 ]\n",
      "MSE loss: 86.9193\n",
      "Iteration: 101800\n",
      "Gradient: [ -0.6167  10.1417 -11.3277  80.0434 271.66  ]\n",
      "Weights: [-4.8488  0.8619 -1.2695  0.109   0.1469]\n",
      "MSE loss: 86.9557\n",
      "Iteration: 101900\n",
      "Gradient: [ -6.7895  -0.4879 -17.6569  18.6977 -46.4728]\n",
      "Weights: [-4.8591  0.8537 -1.269   0.1104  0.1468]\n",
      "MSE loss: 87.1669\n",
      "Iteration: 102000\n",
      "Gradient: [  -1.5069   -4.0857  -24.7899 -128.5141 -218.5121]\n",
      "Weights: [-4.8534  0.8526 -1.2711  0.1108  0.147 ]\n",
      "MSE loss: 86.9832\n",
      "Iteration: 102100\n",
      "Gradient: [ -12.3505  -23.5671  -19.2521 -101.2862 -138.1253]\n",
      "Weights: [-4.8359  0.8382 -1.2717  0.1108  0.1471]\n",
      "MSE loss: 87.3492\n",
      "Iteration: 102200\n",
      "Gradient: [ 1.24964e+01 -3.63000e-02  3.60004e+01 -8.47819e+01 -1.60037e+01]\n",
      "Weights: [-4.8229  0.8426 -1.2738  0.1126  0.1471]\n",
      "MSE loss: 87.2841\n",
      "Iteration: 102300\n",
      "Gradient: [  -1.9667    4.0075  -38.456    52.6685 -112.9855]\n",
      "Weights: [-4.8334  0.8447 -1.275   0.112   0.1471]\n",
      "MSE loss: 86.8909\n",
      "Iteration: 102400\n",
      "Gradient: [  7.6723   0.766    6.9707  -8.4334 -59.3745]\n",
      "Weights: [-4.8345  0.8563 -1.2767  0.1118  0.1469]\n",
      "MSE loss: 86.9307\n",
      "Iteration: 102500\n",
      "Gradient: [  3.2314  -0.7489  11.2013 -22.749   -4.2277]\n",
      "Weights: [-4.8501  0.8661 -1.2792  0.1123  0.147 ]\n",
      "MSE loss: 86.8565\n",
      "Iteration: 102600\n",
      "Gradient: [ -9.0075   1.7509  35.9333 110.2487 104.1276]\n",
      "Weights: [-4.8559  0.866  -1.2775  0.1123  0.1469]\n",
      "MSE loss: 86.9184\n",
      "Iteration: 102700\n",
      "Gradient: [ -1.1276  -9.7983  30.0389 -42.9189 167.6792]\n",
      "Weights: [-4.8456  0.8606 -1.2771  0.1125  0.1469]\n",
      "MSE loss: 86.9166\n",
      "Iteration: 102800\n",
      "Gradient: [  1.2504  -1.6064 -25.0771 -92.5181 152.1447]\n",
      "Weights: [-4.8491  0.8652 -1.2784  0.112   0.1469]\n",
      "MSE loss: 86.8537\n",
      "Iteration: 102900\n",
      "Gradient: [ 10.3045   1.8228  14.3637 -56.0949 244.1322]\n",
      "Weights: [-4.8584  0.8664 -1.2783  0.113   0.1468]\n",
      "MSE loss: 86.9547\n",
      "Iteration: 103000\n",
      "Gradient: [ -3.4729  -5.977  -44.4785 -15.3547 -20.2964]\n",
      "Weights: [-4.8563  0.8653 -1.2808  0.1133  0.1466]\n",
      "MSE loss: 87.0385\n",
      "Iteration: 103100\n",
      "Gradient: [   2.5793  -11.4429    8.3095   -1.7004 -156.6704]\n",
      "Weights: [-4.8346  0.8599 -1.2812  0.1134  0.1466]\n",
      "MSE loss: 86.9635\n",
      "Iteration: 103200\n",
      "Gradient: [ 5.9752 13.2442 28.2755 43.5112 79.7359]\n",
      "Weights: [-4.8364  0.868  -1.2843  0.1147  0.1466]\n",
      "MSE loss: 87.1007\n",
      "Iteration: 103300\n",
      "Gradient: [ 8.3421  6.1767 39.6821 75.0594 27.7518]\n",
      "Weights: [-4.8536  0.873  -1.2826  0.1142  0.1468]\n",
      "MSE loss: 87.1135\n",
      "Iteration: 103400\n",
      "Gradient: [  5.9478 -10.6732 -15.9636  61.0801  69.8553]\n",
      "Weights: [-4.8527  0.8712 -1.2852  0.1134  0.1468]\n",
      "MSE loss: 87.0083\n",
      "Iteration: 103500\n",
      "Gradient: [   4.2266   -4.522    -9.9584  -20.3483 -278.0075]\n",
      "Weights: [-4.8658  0.8788 -1.2869  0.1129  0.147 ]\n",
      "MSE loss: 87.2971\n",
      "Iteration: 103600\n",
      "Gradient: [   3.2662    4.257     7.6381  117.5749 -263.931 ]\n",
      "Weights: [-4.8536  0.8785 -1.2862  0.113   0.1471]\n",
      "MSE loss: 86.8738\n",
      "Iteration: 103700\n",
      "Gradient: [ -2.9342  -2.3793  -9.3869  62.3934 205.1396]\n",
      "Weights: [-4.8551  0.8747 -1.2836  0.1122  0.1475]\n",
      "MSE loss: 87.0043\n",
      "Iteration: 103800\n",
      "Gradient: [  4.9555 -15.0775  19.2969 -26.8482 -93.7389]\n",
      "Weights: [-4.841   0.866  -1.2823  0.1114  0.1475]\n",
      "MSE loss: 86.8861\n",
      "Iteration: 103900\n",
      "Gradient: [ 1.417  -2.9554 40.8313 55.7599 91.3078]\n",
      "Weights: [-4.8314  0.8547 -1.2813  0.1116  0.1479]\n",
      "MSE loss: 86.9746\n",
      "Iteration: 104000\n",
      "Gradient: [  4.5632 -11.5157 -10.9796  -2.3505 129.8293]\n",
      "Weights: [-4.827   0.8519 -1.2811  0.1114  0.148 ]\n",
      "MSE loss: 87.0412\n",
      "Iteration: 104100\n",
      "Gradient: [ -10.2046   -2.4761  -28.5459    9.1695 -113.1859]\n",
      "Weights: [-4.8355  0.8489 -1.2828  0.1116  0.1478]\n",
      "MSE loss: 87.3952\n",
      "Iteration: 104200\n",
      "Gradient: [  6.2772  -8.7959 -27.233   44.4101 139.941 ]\n",
      "Weights: [-4.8374  0.845  -1.2797  0.1123  0.1478]\n",
      "MSE loss: 87.0315\n",
      "Iteration: 104300\n",
      "Gradient: [ -6.4295 -10.9772  -5.1947  59.8441  32.8926]\n",
      "Weights: [-4.827   0.842  -1.2779  0.1121  0.1475]\n",
      "MSE loss: 86.9818\n",
      "Iteration: 104400\n",
      "Gradient: [ -2.0423   4.6214   3.8398  96.6268 -55.342 ]\n",
      "Weights: [-4.8373  0.8482 -1.2762  0.1122  0.1473]\n",
      "MSE loss: 86.8796\n",
      "Iteration: 104500\n",
      "Gradient: [   2.6351   -0.6036   21.4133 -109.4019  -52.4672]\n",
      "Weights: [-4.8489  0.8595 -1.2783  0.1118  0.1472]\n",
      "MSE loss: 86.8898\n",
      "Iteration: 104600\n",
      "Gradient: [ 11.635    6.2918   0.6562 -54.8705 241.0347]\n",
      "Weights: [-4.8333  0.8513 -1.2802  0.1132  0.147 ]\n",
      "MSE loss: 86.8981\n",
      "Iteration: 104700\n",
      "Gradient: [ -2.6902 -18.5289 -54.1779 -62.1987 -96.2279]\n",
      "Weights: [-4.8504  0.8582 -1.28    0.1129  0.1469]\n",
      "MSE loss: 87.0505\n",
      "Iteration: 104800\n",
      "Gradient: [ -16.0694    2.8506  -10.8386  -53.0722 -277.419 ]\n",
      "Weights: [-4.8577  0.8529 -1.2779  0.1131  0.147 ]\n",
      "MSE loss: 87.3717\n",
      "Iteration: 104900\n",
      "Gradient: [  -7.2872    4.0044   -8.1152   45.136  -129.199 ]\n",
      "Weights: [-4.8433  0.8529 -1.2777  0.1129  0.1471]\n",
      "MSE loss: 86.8533\n",
      "Iteration: 105000\n",
      "Gradient: [  7.6414  -3.4858  30.6516  48.9935 337.1726]\n",
      "Weights: [-4.8584  0.8659 -1.2767  0.112   0.1469]\n",
      "MSE loss: 86.9266\n",
      "Iteration: 105100\n",
      "Gradient: [  1.0804  -1.1539   7.3731  14.4219 462.6996]\n",
      "Weights: [-4.8422  0.8605 -1.2756  0.1125  0.1466]\n",
      "MSE loss: 86.9515\n",
      "Iteration: 105200\n",
      "Gradient: [  5.0535  -5.1264  17.3119  29.6041 189.145 ]\n",
      "Weights: [-4.8439  0.855  -1.2787  0.1142  0.1466]\n",
      "MSE loss: 86.8123\n",
      "Iteration: 105300\n",
      "Gradient: [  3.1438   8.8076 -44.9736 -54.8774 -12.3698]\n",
      "Weights: [-4.8392  0.8509 -1.2789  0.1142  0.1467]\n",
      "MSE loss: 86.8144\n",
      "Iteration: 105400\n",
      "Gradient: [  4.3915  -5.9287  16.449   77.6465 -93.4058]\n",
      "Weights: [-4.8331  0.8479 -1.2771  0.1144  0.1466]\n",
      "MSE loss: 86.8676\n",
      "Iteration: 105500\n",
      "Gradient: [-6.9351 14.9903 31.2735 90.7691 27.3557]\n",
      "Weights: [-4.8412  0.8592 -1.2801  0.1148  0.1466]\n",
      "MSE loss: 87.0402\n",
      "Iteration: 105600\n",
      "Gradient: [-0.5006 12.1392 28.8725 92.1355 85.4844]\n",
      "Weights: [-4.8312  0.8531 -1.2795  0.1144  0.1465]\n",
      "MSE loss: 86.8955\n",
      "Iteration: 105700\n",
      "Gradient: [  0.4038   4.4717 -28.2438 -18.6436 247.3895]\n",
      "Weights: [-4.8406  0.8572 -1.2808  0.1146  0.1465]\n",
      "MSE loss: 86.806\n",
      "Iteration: 105800\n",
      "Gradient: [ -3.8732   0.2867  18.512    9.5544 -22.1962]\n",
      "Weights: [-4.849   0.8593 -1.2786  0.1141  0.1465]\n",
      "MSE loss: 86.8187\n",
      "Iteration: 105900\n",
      "Gradient: [ -4.166  -15.5519 -26.1508  10.3294  64.5375]\n",
      "Weights: [-4.8514  0.8652 -1.2804  0.1146  0.1466]\n",
      "MSE loss: 86.9539\n",
      "Iteration: 106000\n",
      "Gradient: [  -9.3611   -0.9151  -12.5238  -41.3801 -101.4703]\n",
      "Weights: [-4.849   0.8556 -1.2817  0.1147  0.1467]\n",
      "MSE loss: 86.9771\n",
      "Iteration: 106100\n",
      "Gradient: [ 2.2581  7.1604 21.5646 13.4523 12.4052]\n",
      "Weights: [-4.8402  0.8539 -1.2792  0.1148  0.1468]\n",
      "MSE loss: 86.9639\n",
      "Iteration: 106200\n",
      "Gradient: [  4.4384   3.1353   3.2785  71.5043 170.2731]\n",
      "Weights: [-4.8234  0.8479 -1.2815  0.1157  0.1466]\n",
      "MSE loss: 87.0512\n",
      "Iteration: 106300\n",
      "Gradient: [  8.7827  -3.9673 -55.5959  35.588  -17.2213]\n",
      "Weights: [-4.8274  0.8415 -1.28    0.1151  0.1468]\n",
      "MSE loss: 86.8762\n",
      "Iteration: 106400\n",
      "Gradient: [ -0.5097  -0.6809 -32.9204  63.4509  80.6245]\n",
      "Weights: [-4.8276  0.8427 -1.278   0.1148  0.1466]\n",
      "MSE loss: 86.86\n",
      "Iteration: 106500\n",
      "Gradient: [  2.8786  14.5238 -24.4078   5.2433 -31.4071]\n",
      "Weights: [-4.8328  0.8437 -1.2782  0.1145  0.1468]\n",
      "MSE loss: 86.8411\n",
      "Iteration: 106600\n",
      "Gradient: [ 5.9241  0.5853 11.3883 69.8257  8.0172]\n",
      "Weights: [-4.8396  0.8486 -1.2786  0.1146  0.1466]\n",
      "MSE loss: 86.8216\n",
      "Iteration: 106700\n",
      "Gradient: [ 1.149000e-01 -4.733200e+00  4.703600e+00  6.252570e+01 -1.498431e+02]\n",
      "Weights: [-4.8449  0.854  -1.2778  0.1138  0.1465]\n",
      "MSE loss: 86.8598\n",
      "Iteration: 106800\n",
      "Gradient: [  0.8424   5.5197 -12.9616  11.3541 105.0576]\n",
      "Weights: [-4.8399  0.8552 -1.2785  0.1133  0.1465]\n",
      "MSE loss: 86.9829\n",
      "Iteration: 106900\n",
      "Gradient: [  -2.1796    2.1709   -0.6293   15.794  -125.1224]\n",
      "Weights: [-4.846   0.8641 -1.2809  0.1137  0.1467]\n",
      "MSE loss: 86.8333\n",
      "Iteration: 107000\n",
      "Gradient: [  9.5466  15.3309 -28.4108  15.7508 374.2915]\n",
      "Weights: [-4.8495  0.8609 -1.2765  0.1129  0.1469]\n",
      "MSE loss: 86.9901\n",
      "Iteration: 107100\n",
      "Gradient: [  -0.417     0.6366   11.3954   65.4687 -123.049 ]\n",
      "Weights: [-4.8417  0.8556 -1.2739  0.113   0.1466]\n",
      "MSE loss: 86.9816\n",
      "Iteration: 107200\n",
      "Gradient: [ 1.224000e-01 -7.943500e+00  1.019860e+01  5.510030e+01  1.288818e+02]\n",
      "Weights: [-4.8452  0.8447 -1.2725  0.1135  0.1465]\n",
      "MSE loss: 86.8806\n",
      "Iteration: 107300\n",
      "Gradient: [  5.0384   2.7446  22.7129  14.3831 195.827 ]\n",
      "Weights: [-4.8405  0.8489 -1.2769  0.1137  0.1465]\n",
      "MSE loss: 86.9128\n",
      "Iteration: 107400\n",
      "Gradient: [   1.3072    4.4802   24.1054    6.5578 -146.4348]\n",
      "Weights: [-4.8332  0.8476 -1.2761  0.1138  0.1466]\n",
      "MSE loss: 86.839\n",
      "Iteration: 107500\n",
      "Gradient: [-12.486   -2.944  -18.7516  30.544  227.7667]\n",
      "Weights: [-4.8435  0.8439 -1.2712  0.1134  0.1465]\n",
      "MSE loss: 86.8473\n",
      "Iteration: 107600\n",
      "Gradient: [   4.5688   -8.1437   45.577    -7.1712 -105.3665]\n",
      "Weights: [-4.8306  0.843  -1.273   0.113   0.1465]\n",
      "MSE loss: 86.8745\n",
      "Iteration: 107700\n",
      "Gradient: [  7.3813  18.1766  -9.663  112.4055 -59.4173]\n",
      "Weights: [-4.8295  0.8361 -1.2716  0.1134  0.1466]\n",
      "MSE loss: 86.8496\n",
      "Iteration: 107800\n",
      "Gradient: [ -7.5445  -2.1714  -2.0817   5.2761 -62.1568]\n",
      "Weights: [-4.8251  0.834  -1.2723  0.1138  0.1466]\n",
      "MSE loss: 86.8772\n",
      "Iteration: 107900\n",
      "Gradient: [-1.5158  0.5922 20.4821 94.8121 29.887 ]\n",
      "Weights: [-4.8329  0.8379 -1.2714  0.1135  0.1465]\n",
      "MSE loss: 86.83\n",
      "Iteration: 108000\n",
      "Gradient: [ -1.5014 -12.9683   6.4964  66.7432 113.3089]\n",
      "Weights: [-4.8509  0.8497 -1.2721  0.1133  0.1465]\n",
      "MSE loss: 86.9007\n",
      "Iteration: 108100\n",
      "Gradient: [  8.3901  -9.5178  16.2712  -1.3284 108.0038]\n",
      "Weights: [-4.8272  0.8438 -1.2725  0.1137  0.1464]\n",
      "MSE loss: 87.0379\n",
      "Iteration: 108200\n",
      "Gradient: [  2.7139  -6.3205  36.1372 -29.0352 -30.3659]\n",
      "Weights: [-4.8394  0.8422 -1.274   0.114   0.1465]\n",
      "MSE loss: 86.8714\n",
      "Iteration: 108300\n",
      "Gradient: [  -9.0745   18.4635   -4.3938  -37.4377 -112.7506]\n",
      "Weights: [-4.8223  0.8388 -1.2735  0.1139  0.1465]\n",
      "MSE loss: 86.9735\n",
      "Iteration: 108400\n",
      "Gradient: [  -3.6936   -0.134    20.0449  -14.5133 -124.1003]\n",
      "Weights: [-4.8364  0.842  -1.2741  0.114   0.1465]\n",
      "MSE loss: 86.8284\n",
      "Iteration: 108500\n",
      "Gradient: [  -0.3821   -8.4061  -33.1036 -126.3016 -115.7169]\n",
      "Weights: [-4.8176  0.8212 -1.2704  0.1144  0.1465]\n",
      "MSE loss: 86.9746\n",
      "Iteration: 108600\n",
      "Gradient: [-2.4455 13.2258 32.4711 -4.5199 19.7805]\n",
      "Weights: [-4.8224  0.8306 -1.269   0.1139  0.1465]\n",
      "MSE loss: 87.0064\n",
      "Iteration: 108700\n",
      "Gradient: [   1.3939   -2.4593  -57.4237 -130.4638  118.7631]\n",
      "Weights: [-4.8448  0.8344 -1.2687  0.1133  0.1465]\n",
      "MSE loss: 87.0579\n",
      "Iteration: 108800\n",
      "Gradient: [ 10.2663  -2.3437  37.2847 -53.0246  32.6339]\n",
      "Weights: [-4.8141  0.8284 -1.2695  0.1131  0.1465]\n",
      "MSE loss: 87.0641\n",
      "Iteration: 108900\n",
      "Gradient: [ -2.5483  -3.4326   7.4572  45.6058 -48.4567]\n",
      "Weights: [-4.8372  0.8285 -1.2687  0.1143  0.1463]\n",
      "MSE loss: 86.9885\n",
      "Iteration: 109000\n",
      "Gradient: [ -1.4405 -25.3783   6.16    80.4916 336.6317]\n",
      "Weights: [-4.8325  0.8327 -1.2683  0.114   0.1461]\n",
      "MSE loss: 86.8224\n",
      "Iteration: 109100\n",
      "Gradient: [ -5.1906   5.4135 -11.1377  41.5107 -19.2978]\n",
      "Weights: [-4.8464  0.8386 -1.2683  0.1137  0.1462]\n",
      "MSE loss: 86.9214\n",
      "Iteration: 109200\n",
      "Gradient: [   2.7244   -9.7332   34.563     1.5507 -311.2241]\n",
      "Weights: [-4.8426  0.8441 -1.2699  0.1132  0.1463]\n",
      "MSE loss: 86.8237\n",
      "Iteration: 109300\n",
      "Gradient: [   6.4543   -1.9589   12.0218  -43.9219 -243.6721]\n",
      "Weights: [-4.8474  0.842  -1.2705  0.1138  0.1464]\n",
      "MSE loss: 86.9132\n",
      "Iteration: 109400\n",
      "Gradient: [   9.3632   -7.8453   40.4063  -17.4412 -190.4728]\n",
      "Weights: [-4.8278  0.8489 -1.2756  0.1132  0.1464]\n",
      "MSE loss: 86.9751\n",
      "Iteration: 109500\n",
      "Gradient: [ 3.500000e-03  3.157700e+01 -1.784380e+01 -4.368510e+01 -3.786875e+02]\n",
      "Weights: [-4.8276  0.8423 -1.2747  0.1137  0.1464]\n",
      "MSE loss: 86.9206\n",
      "Iteration: 109600\n",
      "Gradient: [ 1.026200e+01 -2.600000e-03 -2.542700e+01  4.355730e+01  1.369528e+02]\n",
      "Weights: [-4.8254  0.8429 -1.2738  0.1131  0.1466]\n",
      "MSE loss: 86.9486\n",
      "Iteration: 109700\n",
      "Gradient: [ -3.7584 -19.4024  -5.9597  72.3497 151.2708]\n",
      "Weights: [-4.8609  0.8551 -1.2732  0.1131  0.1467]\n",
      "MSE loss: 87.1171\n",
      "Iteration: 109800\n",
      "Gradient: [  -1.9638  -10.1735   52.2374    8.7538 -201.7282]\n",
      "Weights: [-4.8568  0.8586 -1.2728  0.1118  0.1468]\n",
      "MSE loss: 86.9461\n",
      "Iteration: 109900\n",
      "Gradient: [ -3.4647   2.2914 -23.7311  35.8219 426.9285]\n",
      "Weights: [-4.8286  0.8465 -1.2744  0.1123  0.1468]\n",
      "MSE loss: 86.9247\n",
      "Iteration: 110000\n",
      "Gradient: [ -8.5583   4.6458 -20.3742  57.2875 -69.8368]\n",
      "Weights: [-4.8439  0.8521 -1.2739  0.1128  0.1467]\n",
      "MSE loss: 86.8594\n",
      "Iteration: 110100\n",
      "Gradient: [   8.7506    0.8032   17.8686 -129.7408    2.4246]\n",
      "Weights: [-4.8198  0.8329 -1.2701  0.1123  0.1466]\n",
      "MSE loss: 87.0151\n",
      "Iteration: 110200\n",
      "Gradient: [  8.1779  -8.2693  22.84   -53.9923 336.6285]\n",
      "Weights: [-4.8141  0.8321 -1.2716  0.1134  0.1467]\n",
      "MSE loss: 87.1712\n",
      "Iteration: 110300\n",
      "Gradient: [ -1.2318   1.2722 -14.9816   9.2996 269.7511]\n",
      "Weights: [-4.8211  0.8336 -1.2684  0.113   0.1466]\n",
      "MSE loss: 87.1905\n",
      "Iteration: 110400\n",
      "Gradient: [ -9.9304   3.7296  -5.4234 -81.9891 -19.1775]\n",
      "Weights: [-4.8505  0.8382 -1.2699  0.113   0.1467]\n",
      "MSE loss: 87.1908\n",
      "Iteration: 110500\n",
      "Gradient: [  9.8002  26.404   -9.8608  32.0468 217.8812]\n",
      "Weights: [-4.8261  0.8389 -1.2704  0.1131  0.1466]\n",
      "MSE loss: 87.0187\n",
      "Iteration: 110600\n",
      "Gradient: [  10.4703  -18.1697    1.5035   32.4204 -108.7258]\n",
      "Weights: [-4.8292  0.8306 -1.2697  0.1137  0.1464]\n",
      "MSE loss: 86.8868\n",
      "Iteration: 110700\n",
      "Gradient: [  2.5546  14.8847   0.2868  70.8053 144.9992]\n",
      "Weights: [-4.8208  0.8402 -1.2708  0.113   0.1465]\n",
      "MSE loss: 87.2055\n",
      "Iteration: 110800\n",
      "Gradient: [   2.4497   20.8676   24.2294  163.4305 -185.6217]\n",
      "Weights: [-4.833   0.8393 -1.2709  0.113   0.1465]\n",
      "MSE loss: 86.8347\n",
      "Iteration: 110900\n",
      "Gradient: [  -7.6667   -6.8493  -66.4038  -15.2255 -273.4806]\n",
      "Weights: [-4.8417  0.8311 -1.2671  0.113   0.1464]\n",
      "MSE loss: 87.0369\n",
      "Iteration: 111000\n",
      "Gradient: [   0.5683   -7.8029   26.4264   -1.3792 -194.5176]\n",
      "Weights: [-4.8282  0.8306 -1.2648  0.1127  0.1462]\n",
      "MSE loss: 86.8633\n",
      "Iteration: 111100\n",
      "Gradient: [  -2.9658  -12.4071   31.6828   -9.6802 -352.4619]\n",
      "Weights: [-4.85    0.8344 -1.2641  0.1124  0.1462]\n",
      "MSE loss: 87.1887\n",
      "Iteration: 111200\n",
      "Gradient: [-16.8034 -19.6622 -20.7558 -28.7852 -11.9785]\n",
      "Weights: [-4.8415  0.8319 -1.2616  0.1124  0.1459]\n",
      "MSE loss: 86.8935\n",
      "Iteration: 111300\n",
      "Gradient: [-13.3845 -14.6695  -7.244   -6.3931 -49.0715]\n",
      "Weights: [-4.8518  0.8315 -1.2603  0.112   0.146 ]\n",
      "MSE loss: 87.2111\n",
      "Iteration: 111400\n",
      "Gradient: [   1.7842   22.819   -16.527    21.0778 -130.7664]\n",
      "Weights: [-4.8194  0.8286 -1.2598  0.1116  0.1459]\n",
      "MSE loss: 87.1521\n",
      "Iteration: 111500\n",
      "Gradient: [ -8.0103   3.9631   6.2682  41.7672 -96.1867]\n",
      "Weights: [-4.8345  0.8244 -1.258   0.1118  0.146 ]\n",
      "MSE loss: 86.8478\n",
      "Iteration: 111600\n",
      "Gradient: [  1.6357   9.9196 -43.8555  52.902   25.9227]\n",
      "Weights: [-4.8354  0.8184 -1.2537  0.1112  0.1461]\n",
      "MSE loss: 86.9108\n",
      "Iteration: 111700\n",
      "Gradient: [   2.6582   10.0039  -31.2797   61.4068 -173.2273]\n",
      "Weights: [-4.8326  0.8103 -1.2541  0.1119  0.1461]\n",
      "MSE loss: 86.9768\n",
      "Iteration: 111800\n",
      "Gradient: [ -9.4444  12.9901  28.5792  72.4222 175.5977]\n",
      "Weights: [-4.8301  0.8125 -1.2514  0.1117  0.1459]\n",
      "MSE loss: 86.9393\n",
      "Iteration: 111900\n",
      "Gradient: [  -2.1009   13.1732   29.2454  107.9586 -123.5468]\n",
      "Weights: [-4.824   0.8173 -1.2547  0.112   0.1457]\n",
      "MSE loss: 86.9064\n",
      "Iteration: 112000\n",
      "Gradient: [  4.3697   2.3164  32.2466 -59.4833  -6.0627]\n",
      "Weights: [-4.8232  0.8168 -1.2549  0.1119  0.1457]\n",
      "MSE loss: 86.9006\n",
      "Iteration: 112100\n",
      "Gradient: [   2.7407    1.8651   -5.4347   84.6574 -198.1121]\n",
      "Weights: [-4.8308  0.8199 -1.2561  0.1119  0.1456]\n",
      "MSE loss: 86.9276\n",
      "Iteration: 112200\n",
      "Gradient: [ -3.614   20.2104  -9.1687  65.3728 127.4683]\n",
      "Weights: [-4.83    0.8222 -1.2566  0.1128  0.1456]\n",
      "MSE loss: 86.8973\n",
      "Iteration: 112300\n",
      "Gradient: [  6.1473   0.1377  40.2482 -63.2822  49.446 ]\n",
      "Weights: [-4.8222  0.8201 -1.2574  0.1122  0.1456]\n",
      "MSE loss: 86.964\n",
      "Iteration: 112400\n",
      "Gradient: [   7.7483    7.8609  -23.6244  -55.3644 -150.5843]\n",
      "Weights: [-4.836   0.8248 -1.2598  0.1132  0.1457]\n",
      "MSE loss: 86.8358\n",
      "Iteration: 112500\n",
      "Gradient: [ -2.4065  -8.5954   6.2716 -59.3976 121.2729]\n",
      "Weights: [-4.8437  0.828  -1.2588  0.1125  0.1458]\n",
      "MSE loss: 86.9127\n",
      "Iteration: 112600\n",
      "Gradient: [  2.791   13.3753   2.522  -44.4755 117.2762]\n",
      "Weights: [-4.8376  0.8327 -1.2594  0.1128  0.1457]\n",
      "MSE loss: 86.9516\n",
      "Iteration: 112700\n",
      "Gradient: [  -1.7902   -5.4869   46.467    -3.5589 -242.5541]\n",
      "Weights: [-4.8404  0.8316 -1.2587  0.113   0.1454]\n",
      "MSE loss: 86.8523\n",
      "Iteration: 112800\n",
      "Gradient: [   5.3108    2.8778   10.3218  -20.7096 -133.454 ]\n",
      "Weights: [-4.8347  0.8243 -1.2603  0.1131  0.1458]\n",
      "MSE loss: 86.8465\n",
      "Iteration: 112900\n",
      "Gradient: [ -0.1961   7.55    -3.6003 -75.7968 -34.4908]\n",
      "Weights: [-4.8262  0.8241 -1.2643  0.1134  0.1461]\n",
      "MSE loss: 86.8645\n",
      "Iteration: 113000\n",
      "Gradient: [   0.7139   20.4574  -16.7328  -27.5153 -187.5215]\n",
      "Weights: [-4.8147  0.8076 -1.2635  0.1148  0.1464]\n",
      "MSE loss: 87.0488\n",
      "Iteration: 113100\n",
      "Gradient: [  8.0967  20.6313   2.4852  57.7145 253.0122]\n",
      "Weights: [-4.816   0.8121 -1.2622  0.1144  0.1462]\n",
      "MSE loss: 87.0649\n",
      "Iteration: 113200\n",
      "Gradient: [-6.50000e-02  1.20420e+00  2.01079e+01  7.59220e+01  8.65219e+01]\n",
      "Weights: [-4.8299  0.8212 -1.2631  0.1139  0.1462]\n",
      "MSE loss: 86.8964\n",
      "Iteration: 113300\n",
      "Gradient: [  -5.6928    5.5954    9.6256 -101.0468 -192.5502]\n",
      "Weights: [-4.8409  0.8321 -1.2601  0.1128  0.1458]\n",
      "MSE loss: 86.8703\n",
      "Iteration: 113400\n",
      "Gradient: [ -6.3166  -3.9352 -46.2156  85.9538 430.8031]\n",
      "Weights: [-4.8708  0.8486 -1.2599  0.1114  0.146 ]\n",
      "MSE loss: 87.3903\n",
      "Iteration: 113500\n",
      "Gradient: [ 23.9619  30.8538   3.8188 -89.4192 561.4778]\n",
      "Weights: [-4.8209  0.8263 -1.258   0.1116  0.1458]\n",
      "MSE loss: 87.0985\n",
      "Iteration: 113600\n",
      "Gradient: [ -9.3699  11.8063 -32.7261  -8.9968  -3.2572]\n",
      "Weights: [-4.8407  0.8343 -1.262   0.112   0.1457]\n",
      "MSE loss: 87.0484\n",
      "Iteration: 113700\n",
      "Gradient: [  8.3961   7.7926 -22.0207  -7.3179 189.5791]\n",
      "Weights: [-4.8317  0.8272 -1.2587  0.1125  0.1457]\n",
      "MSE loss: 86.8594\n",
      "Iteration: 113800\n",
      "Gradient: [   7.4244  -10.6494    9.9726  -66.3968 -158.2547]\n",
      "Weights: [-4.8531  0.8211 -1.256   0.1127  0.1456]\n",
      "MSE loss: 87.5332\n",
      "Iteration: 113900\n",
      "Gradient: [  2.3049  -6.9834 -32.2078  26.4765 102.0441]\n",
      "Weights: [-4.8212  0.8223 -1.2575  0.1128  0.1457]\n",
      "MSE loss: 87.1061\n",
      "Iteration: 114000\n",
      "Gradient: [   1.8071    3.7654  -43.6434  -39.1558 -228.7669]\n",
      "Weights: [-4.8422  0.8169 -1.2548  0.1122  0.1458]\n",
      "MSE loss: 87.1061\n",
      "Iteration: 114100\n",
      "Gradient: [ 4.2938  2.0737 13.6608 48.3918 99.3353]\n",
      "Weights: [-4.8218  0.8084 -1.254   0.1127  0.1457]\n",
      "MSE loss: 86.8699\n",
      "Iteration: 114200\n",
      "Gradient: [   2.6371   -8.5097   -3.2333   29.5387 -105.2403]\n",
      "Weights: [-4.8039  0.8007 -1.2507  0.1127  0.1456]\n",
      "MSE loss: 87.3804\n",
      "Iteration: 114300\n",
      "Gradient: [  -2.6214  -13.4096  -65.8556 -208.0804 -362.3437]\n",
      "Weights: [-4.8276  0.7966 -1.2496  0.1123  0.1457]\n",
      "MSE loss: 87.2267\n",
      "Iteration: 114400\n",
      "Gradient: [  -2.3438   -4.5761  -13.7502  -28.7436 -194.4581]\n",
      "Weights: [-4.8218  0.8021 -1.2507  0.1122  0.1456]\n",
      "MSE loss: 86.9244\n",
      "Iteration: 114500\n",
      "Gradient: [   8.6846   -2.9311   67.3636  -12.9383 -114.494 ]\n",
      "Weights: [-4.8327  0.8162 -1.2518  0.1129  0.1453]\n",
      "MSE loss: 86.8997\n",
      "Iteration: 114600\n",
      "Gradient: [  -7.9742    3.1187  -26.811    -3.9294 -221.5236]\n",
      "Weights: [-4.8398  0.8117 -1.249   0.1129  0.1452]\n",
      "MSE loss: 86.9363\n",
      "Iteration: 114700\n",
      "Gradient: [ -0.5016  -3.4319  12.3534  64.3662 -15.6297]\n",
      "Weights: [-4.8203  0.7996 -1.2494  0.1141  0.1452]\n",
      "MSE loss: 86.9081\n",
      "Iteration: 114800\n",
      "Gradient: [ -4.7948  16.3168  10.7915 188.3089 367.0418]\n",
      "Weights: [-4.8114  0.8054 -1.2501  0.1137  0.1452]\n",
      "MSE loss: 87.501\n",
      "Iteration: 114900\n",
      "Gradient: [   3.762     3.812   -18.1255   31.1823 -129.8156]\n",
      "Weights: [-4.8248  0.7943 -1.2511  0.1144  0.1453]\n",
      "MSE loss: 87.1128\n",
      "Iteration: 115000\n",
      "Gradient: [ -9.5747  16.5601 -16.634   -3.1193 202.3286]\n",
      "Weights: [-4.8234  0.7958 -1.2469  0.114   0.1451]\n",
      "MSE loss: 86.9335\n",
      "Iteration: 115100\n",
      "Gradient: [   6.7971    4.5589   11.892    90.4574 -116.8747]\n",
      "Weights: [-4.8164  0.7989 -1.249   0.1134  0.1452]\n",
      "MSE loss: 86.9029\n",
      "Iteration: 115200\n",
      "Gradient: [  -2.1356   10.6765   -7.0551 -119.2098   82.0425]\n",
      "Weights: [-4.8355  0.8147 -1.2524  0.1133  0.1453]\n",
      "MSE loss: 86.8631\n",
      "Iteration: 115300\n",
      "Gradient: [-13.2291 -11.924  -21.0407  44.3642 -47.1579]\n",
      "Weights: [-4.8246  0.8098 -1.2513  0.1132  0.1451]\n",
      "MSE loss: 86.862\n",
      "Iteration: 115400\n",
      "Gradient: [  0.6883   0.8041  -0.7438 -57.327  140.1267]\n",
      "Weights: [-4.8085  0.799  -1.2491  0.1136  0.1448]\n",
      "MSE loss: 87.0665\n",
      "Iteration: 115500\n",
      "Gradient: [  2.9387   0.5535  16.5167 128.9988  58.0069]\n",
      "Weights: [-4.8179  0.7991 -1.2454  0.1143  0.1448]\n",
      "MSE loss: 87.3394\n",
      "Iteration: 115600\n",
      "Gradient: [   2.8317   -9.1389   -2.8409  -23.7677 -233.3557]\n",
      "Weights: [-4.8159  0.7878 -1.2449  0.1141  0.1449]\n",
      "MSE loss: 86.9004\n",
      "Iteration: 115700\n",
      "Gradient: [-3.639400e+00 -1.762500e+00  7.010000e-02 -5.126070e+01 -1.632543e+02]\n",
      "Weights: [-4.8081  0.7958 -1.2463  0.1139  0.1447]\n",
      "MSE loss: 87.1427\n",
      "Iteration: 115800\n",
      "Gradient: [  5.1991  -7.6825 -39.9158 -29.6538  16.5934]\n",
      "Weights: [-4.8287  0.7853 -1.2413  0.1135  0.1449]\n",
      "MSE loss: 87.1275\n",
      "Iteration: 115900\n",
      "Gradient: [  4.1643  10.4997 -24.4125 -39.9889 -47.3022]\n",
      "Weights: [-4.8148  0.7984 -1.2455  0.1133  0.1446]\n",
      "MSE loss: 86.9819\n",
      "Iteration: 116000\n",
      "Gradient: [  4.2265  -7.3614  13.7991  -1.8346 -33.5402]\n",
      "Weights: [-4.8381  0.8034 -1.2463  0.1135  0.1446]\n",
      "MSE loss: 87.1806\n",
      "Iteration: 116100\n",
      "Gradient: [ -1.4177 -17.3031 -27.1324 -61.1285 -82.6701]\n",
      "Weights: [-4.8378  0.8071 -1.2475  0.1131  0.1447]\n",
      "MSE loss: 87.162\n",
      "Iteration: 116200\n",
      "Gradient: [  5.5627   7.573   -0.29    -1.2306 106.9827]\n",
      "Weights: [-4.8319  0.7979 -1.2479  0.1149  0.1448]\n",
      "MSE loss: 86.9685\n",
      "Iteration: 116300\n",
      "Gradient: [ -6.4072 -10.2763 -34.2513  32.65   -90.6905]\n",
      "Weights: [-4.8201  0.8004 -1.2523  0.1156  0.1448]\n",
      "MSE loss: 86.8409\n",
      "Iteration: 116400\n",
      "Gradient: [ 4.5137  5.669  10.8739 17.951  45.1931]\n",
      "Weights: [-4.8198  0.7996 -1.2527  0.1161  0.1447]\n",
      "MSE loss: 86.8364\n",
      "Iteration: 116500\n",
      "Gradient: [ 5.0486 -1.1541 21.7495 76.3887 32.5605]\n",
      "Weights: [-4.8112  0.8002 -1.251   0.1156  0.1447]\n",
      "MSE loss: 87.0644\n",
      "Iteration: 116600\n",
      "Gradient: [ -6.9006  10.7525  -7.6213  58.537  -63.2931]\n",
      "Weights: [-4.8115  0.7976 -1.2521  0.116   0.1445]\n",
      "MSE loss: 86.9242\n",
      "Iteration: 116700\n",
      "Gradient: [  5.0858  13.2052   9.6291 -41.8374 159.9566]\n",
      "Weights: [-4.8214  0.8096 -1.2531  0.1156  0.1444]\n",
      "MSE loss: 86.8826\n",
      "Iteration: 116800\n",
      "Gradient: [ -3.9279   2.4602 -28.131   -0.6295  15.7404]\n",
      "Weights: [-4.8302  0.8071 -1.2492  0.1157  0.1443]\n",
      "MSE loss: 86.8507\n",
      "Iteration: 116900\n",
      "Gradient: [-18.1628 -10.5283 -36.3297 -98.3496  62.3658]\n",
      "Weights: [-4.8566  0.8155 -1.2507  0.1155  0.1442]\n",
      "MSE loss: 87.553\n",
      "Iteration: 117000\n",
      "Gradient: [   8.9716   -4.021    -6.0582   35.7646 -141.3067]\n",
      "Weights: [-4.8357  0.8138 -1.2514  0.1159  0.1442]\n",
      "MSE loss: 86.8469\n",
      "Iteration: 117100\n",
      "Gradient: [ -2.5921  -7.9542   0.6297  24.212  343.5081]\n",
      "Weights: [-4.8285  0.8057 -1.2498  0.1154  0.1445]\n",
      "MSE loss: 86.8359\n",
      "Iteration: 117200\n",
      "Gradient: [   1.8403   -6.5631   59.4455   12.2096 -185.3498]\n",
      "Weights: [-4.8402  0.8109 -1.2495  0.115   0.1445]\n",
      "MSE loss: 86.9184\n",
      "Iteration: 117300\n",
      "Gradient: [ 16.2444  15.3725  -2.7839  45.2752 146.6913]\n",
      "Weights: [-4.8372  0.8114 -1.2485  0.1146  0.1444]\n",
      "MSE loss: 86.8789\n",
      "Iteration: 117400\n",
      "Gradient: [ -4.8186  -9.2138 -30.103  -10.5201 -86.8547]\n",
      "Weights: [-4.8439  0.8109 -1.2465  0.1145  0.1443]\n",
      "MSE loss: 86.9785\n",
      "Iteration: 117500\n",
      "Gradient: [  6.3778  11.3129 -39.139   38.6479 130.3035]\n",
      "Weights: [-4.8235  0.8074 -1.2464  0.1145  0.1442]\n",
      "MSE loss: 87.0093\n",
      "Iteration: 117600\n",
      "Gradient: [ -5.9189   7.8098   1.8376 -70.0343 160.9812]\n",
      "Weights: [-4.8176  0.8051 -1.2455  0.1153  0.1439]\n",
      "MSE loss: 87.2476\n",
      "Iteration: 117700\n",
      "Gradient: [ 2.85070e+00  7.80000e-02 -7.47060e+00 -1.20087e+01  9.18842e+01]\n",
      "Weights: [-4.8166  0.7934 -1.2443  0.1158  0.144 ]\n",
      "MSE loss: 86.9303\n",
      "Iteration: 117800\n",
      "Gradient: [  -4.9107   -3.6044   43.2075  -61.8864 -148.2117]\n",
      "Weights: [-4.8264  0.7857 -1.2445  0.116   0.1442]\n",
      "MSE loss: 87.1184\n",
      "Iteration: 117900\n",
      "Gradient: [ -3.2686   7.589   -1.9326 -40.2557   0.5948]\n",
      "Weights: [-4.8274  0.7918 -1.2456  0.116   0.1442]\n",
      "MSE loss: 86.9371\n",
      "Iteration: 118000\n",
      "Gradient: [ -2.5382   7.2745  -6.2825 -46.8497  53.8572]\n",
      "Weights: [-4.8207  0.7859 -1.2439  0.1159  0.1444]\n",
      "MSE loss: 86.8977\n",
      "Iteration: 118100\n",
      "Gradient: [   6.3551    6.4907  -16.3958  128.8323 -150.3564]\n",
      "Weights: [-4.8215  0.7907 -1.2452  0.116   0.1443]\n",
      "MSE loss: 86.8694\n",
      "Iteration: 118200\n",
      "Gradient: [  0.5188  11.8165 -22.2768  22.8788 455.7261]\n",
      "Weights: [-4.8067  0.7882 -1.2448  0.1157  0.1443]\n",
      "MSE loss: 87.0883\n",
      "Iteration: 118300\n",
      "Gradient: [ -8.8648  16.043    9.5757 121.541   77.6283]\n",
      "Weights: [-4.8214  0.7952 -1.245   0.1153  0.1444]\n",
      "MSE loss: 86.8912\n",
      "Iteration: 118400\n",
      "Gradient: [ -6.5988   0.6536  37.8173  47.8238 143.832 ]\n",
      "Weights: [-4.8264  0.7912 -1.2448  0.1158  0.1442]\n",
      "MSE loss: 86.9154\n",
      "Iteration: 118500\n",
      "Gradient: [ -1.6134 -17.6683  -8.1028   4.4838 -23.8006]\n",
      "Weights: [-4.8331  0.7863 -1.247   0.1172  0.1442]\n",
      "MSE loss: 87.3939\n",
      "Iteration: 118600\n",
      "Gradient: [  -2.3127    4.6522  -17.0113   19.182  -412.4673]\n",
      "Weights: [-4.8145  0.7763 -1.247   0.1169  0.1444]\n",
      "MSE loss: 87.2752\n",
      "Iteration: 118700\n",
      "Gradient: [-5.7297 -5.8298  2.7372 38.8985 36.6759]\n",
      "Weights: [-4.8281  0.789  -1.2485  0.1172  0.1442]\n",
      "MSE loss: 87.1444\n",
      "Iteration: 118800\n",
      "Gradient: [  -1.2644    3.2786   -7.8646   13.6628 -148.1482]\n",
      "Weights: [-4.8215  0.797  -1.2477  0.117   0.144 ]\n",
      "MSE loss: 86.8663\n",
      "Iteration: 118900\n",
      "Gradient: [  0.3785   6.2411  13.6268 132.0587  46.9633]\n",
      "Weights: [-4.8266  0.7994 -1.2464  0.1174  0.144 ]\n",
      "MSE loss: 87.2653\n",
      "Iteration: 119000\n",
      "Gradient: [-0.918  -8.4579 26.8559  5.6397 -5.8961]\n",
      "Weights: [-4.8309  0.7885 -1.245   0.1176  0.1438]\n",
      "MSE loss: 87.0045\n",
      "Iteration: 119100\n",
      "Gradient: [  8.3268  17.3737   0.8226  -6.7441 -73.7374]\n",
      "Weights: [-4.8117  0.7897 -1.2468  0.1182  0.1437]\n",
      "MSE loss: 87.029\n",
      "Iteration: 119200\n",
      "Gradient: [ -6.2594 -15.8068  16.9339  -5.6364 -60.4501]\n",
      "Weights: [-4.8248  0.785  -1.245   0.1182  0.1435]\n",
      "MSE loss: 87.0315\n",
      "Iteration: 119300\n",
      "Gradient: [  6.5238   2.3816  24.4789 -28.7376  42.7759]\n",
      "Weights: [-4.8221  0.7919 -1.2486  0.1184  0.1435]\n",
      "MSE loss: 86.8713\n",
      "Iteration: 119400\n",
      "Gradient: [  -8.6717   -3.0081  -67.0219 -116.7487 -199.2941]\n",
      "Weights: [-4.8278  0.7987 -1.2521  0.1181  0.1436]\n",
      "MSE loss: 87.1826\n",
      "Iteration: 119500\n",
      "Gradient: [  -4.7414  -11.2276   19.1586  -23.411  -109.5851]\n",
      "Weights: [-4.8343  0.8066 -1.2534  0.1185  0.1437]\n",
      "MSE loss: 86.8689\n",
      "Iteration: 119600\n",
      "Gradient: [ 5.0759  4.7547 26.6183 49.6517 -0.3466]\n",
      "Weights: [-4.8369  0.8039 -1.2514  0.1189  0.1436]\n",
      "MSE loss: 86.8907\n",
      "Iteration: 119700\n",
      "Gradient: [ 3.846600e+00 -1.678000e-01  4.392040e+01  1.424996e+02  2.187605e+02]\n",
      "Weights: [-4.8129  0.8042 -1.2529  0.1188  0.1438]\n",
      "MSE loss: 87.358\n",
      "Iteration: 119800\n",
      "Gradient: [  0.9565 -14.4816  22.3742 -85.9957  -9.0673]\n",
      "Weights: [-4.8204  0.807  -1.2565  0.1191  0.1438]\n",
      "MSE loss: 86.819\n",
      "Iteration: 119900\n",
      "Gradient: [ -5.3339 -20.2782  -3.7914 -47.6471 157.7899]\n",
      "Weights: [-4.8326  0.8058 -1.2566  0.1197  0.1436]\n",
      "MSE loss: 86.8859\n",
      "Iteration: 120000\n",
      "Gradient: [ -7.131   -1.0878 -33.2135   8.5947 -41.5281]\n",
      "Weights: [-4.8293  0.8099 -1.2575  0.1195  0.1438]\n",
      "MSE loss: 86.7687\n",
      "Iteration: 120100\n",
      "Gradient: [ -2.8979   4.4507 -15.4264  55.7136 -43.9646]\n",
      "Weights: [-4.8104  0.8111 -1.2626  0.1199  0.1438]\n",
      "MSE loss: 87.0008\n",
      "Iteration: 120200\n",
      "Gradient: [  -4.8168    9.5427   27.1203  -52.686  -175.5879]\n",
      "Weights: [-4.8325  0.8076 -1.2601  0.1198  0.1441]\n",
      "MSE loss: 86.8841\n",
      "Iteration: 120300\n",
      "Gradient: [ 4.4201 11.0196 11.4133  6.2183 63.7377]\n",
      "Weights: [-4.8223  0.8048 -1.2619  0.1211  0.1437]\n",
      "MSE loss: 86.8526\n",
      "Iteration: 120400\n",
      "Gradient: [ 0.9454 -7.8516 18.5092 39.4607 12.8956]\n",
      "Weights: [-4.821   0.8099 -1.2594  0.1206  0.1437]\n",
      "MSE loss: 86.9598\n",
      "Iteration: 120500\n",
      "Gradient: [   1.4029   15.104   -29.8404  -27.785  -222.7956]\n",
      "Weights: [-4.8246  0.8201 -1.2621  0.12    0.1438]\n",
      "MSE loss: 86.9533\n",
      "Iteration: 120600\n",
      "Gradient: [  9.924   -5.4865 -12.831  -83.8916 -37.7132]\n",
      "Weights: [-4.8307  0.8177 -1.264   0.1204  0.1438]\n",
      "MSE loss: 86.7716\n",
      "Iteration: 120700\n",
      "Gradient: [  7.8359   6.0639   2.3994 120.6051  63.8267]\n",
      "Weights: [-4.8142  0.8188 -1.2644  0.12    0.144 ]\n",
      "MSE loss: 87.1413\n",
      "Iteration: 120800\n",
      "Gradient: [ -5.9741 -11.3453  -5.5876 -28.8299  35.746 ]\n",
      "Weights: [-4.8286  0.8222 -1.2644  0.1196  0.1439]\n",
      "MSE loss: 86.7685\n",
      "Iteration: 120900\n",
      "Gradient: [  3.8959   3.2747 -19.0812  -4.1944 192.6016]\n",
      "Weights: [-4.8157  0.8087 -1.2628  0.1204  0.1439]\n",
      "MSE loss: 86.8186\n",
      "Iteration: 121000\n",
      "Gradient: [  0.7645 -11.0881   3.5418  14.4131 293.3039]\n",
      "Weights: [-4.8142  0.8008 -1.26    0.1205  0.1441]\n",
      "MSE loss: 86.878\n",
      "Iteration: 121100\n",
      "Gradient: [ 11.4082 -10.0524  -7.8673 -48.1585 212.4077]\n",
      "Weights: [-4.8224  0.8124 -1.2634  0.1205  0.1441]\n",
      "MSE loss: 86.7895\n",
      "Iteration: 121200\n",
      "Gradient: [  -0.919    16.7672   28.5262   44.1174 -132.7329]\n",
      "Weights: [-4.8393  0.8236 -1.2644  0.12    0.1441]\n",
      "MSE loss: 86.8125\n",
      "Iteration: 121300\n",
      "Gradient: [  0.8261  -6.7723  47.5716   9.4263 -42.211 ]\n",
      "Weights: [-4.835   0.8212 -1.2647  0.1201  0.1442]\n",
      "MSE loss: 86.7705\n",
      "Iteration: 121400\n",
      "Gradient: [  3.4467 -14.3773 -38.7798 -21.8052  94.5864]\n",
      "Weights: [-4.828   0.8108 -1.2674  0.1208  0.1443]\n",
      "MSE loss: 86.9697\n",
      "Iteration: 121500\n",
      "Gradient: [ 12.5619  -2.5856  39.6754  24.9561 -60.2708]\n",
      "Weights: [-4.8129  0.8039 -1.2617  0.1202  0.1443]\n",
      "MSE loss: 86.8956\n",
      "Iteration: 121600\n",
      "Gradient: [  -2.5066  -13.3835  -40.3842 -127.5503  -26.8794]\n",
      "Weights: [-4.8334  0.8045 -1.2602  0.1193  0.144 ]\n",
      "MSE loss: 87.5112\n",
      "Iteration: 121700\n",
      "Gradient: [  8.2498  -5.8233   9.7426  74.364  143.9469]\n",
      "Weights: [-4.8345  0.8229 -1.2624  0.1198  0.1441]\n",
      "MSE loss: 87.0033\n",
      "Iteration: 121800\n",
      "Gradient: [   3.0051    9.9997    5.8231   22.2901 -159.0472]\n",
      "Weights: [-4.8445  0.8284 -1.264   0.1196  0.144 ]\n",
      "MSE loss: 86.8317\n",
      "Iteration: 121900\n",
      "Gradient: [   2.0938   -9.9789    4.9083  -19.8931 -475.2161]\n",
      "Weights: [-4.8449  0.83   -1.265   0.1189  0.144 ]\n",
      "MSE loss: 86.9715\n",
      "Iteration: 122000\n",
      "Gradient: [  3.3507 -22.4127 -27.477  -25.5943 101.6644]\n",
      "Weights: [-4.8197  0.8272 -1.2661  0.1185  0.144 ]\n",
      "MSE loss: 87.0518\n",
      "Iteration: 122100\n",
      "Gradient: [  1.1165   4.7477   5.2338 139.3723 114.769 ]\n",
      "Weights: [-4.833   0.8249 -1.2636  0.1191  0.1441]\n",
      "MSE loss: 86.7684\n",
      "Iteration: 122200\n",
      "Gradient: [-13.4606  18.7776  25.4573   9.1371 118.4357]\n",
      "Weights: [-4.8235  0.8244 -1.2645  0.1187  0.1442]\n",
      "MSE loss: 86.9007\n",
      "Iteration: 122300\n",
      "Gradient: [ -3.6788   3.0089  23.1337  -7.4698 -34.8768]\n",
      "Weights: [-4.8326  0.8297 -1.2669  0.119   0.1441]\n",
      "MSE loss: 86.7962\n",
      "Iteration: 122400\n",
      "Gradient: [  -3.8647  -11.5606   26.7582    8.8406 -147.3517]\n",
      "Weights: [-4.8286  0.8277 -1.2672  0.1186  0.1444]\n",
      "MSE loss: 86.791\n",
      "Iteration: 122500\n",
      "Gradient: [  1.7772   8.3876  32.2345 -64.0742 411.1519]\n",
      "Weights: [-4.8349  0.8315 -1.2657  0.1185  0.1444]\n",
      "MSE loss: 86.81\n",
      "Iteration: 122600\n",
      "Gradient: [ -6.3991 -22.0835  20.2644  19.3664 -27.0271]\n",
      "Weights: [-4.8332  0.8334 -1.2631  0.1177  0.1442]\n",
      "MSE loss: 86.9764\n",
      "Iteration: 122700\n",
      "Gradient: [  -8.3896  -10.4648   18.46     58.0456 -351.0697]\n",
      "Weights: [-4.8257  0.8169 -1.2611  0.1183  0.1443]\n",
      "MSE loss: 86.7954\n",
      "Iteration: 122800\n",
      "Gradient: [   7.0208   10.0418   -4.8399   60.0233 -143.1447]\n",
      "Weights: [-4.8307  0.8178 -1.2601  0.1184  0.1442]\n",
      "MSE loss: 86.7827\n",
      "Iteration: 122900\n",
      "Gradient: [   6.4669  -15.4936   -7.8665  -15.7872 -152.1347]\n",
      "Weights: [-4.8461  0.8327 -1.2628  0.1181  0.1443]\n",
      "MSE loss: 86.8859\n",
      "Iteration: 123000\n",
      "Gradient: [ -3.3606   6.2057   1.0387  17.2282 -65.3201]\n",
      "Weights: [-4.8413  0.8308 -1.2614  0.1178  0.1443]\n",
      "MSE loss: 86.8966\n",
      "Iteration: 123100\n",
      "Gradient: [ -5.6463   9.4552  10.4471 125.3018  97.3571]\n",
      "Weights: [-4.8248  0.8253 -1.2638  0.1176  0.1446]\n",
      "MSE loss: 86.9131\n",
      "Iteration: 123200\n",
      "Gradient: [  -9.9472  -12.1737  -24.925   107.0419 -419.0666]\n",
      "Weights: [-4.8369  0.8194 -1.2601  0.1179  0.1445]\n",
      "MSE loss: 86.8165\n",
      "Iteration: 123300\n",
      "Gradient: [  0.357    5.1231 -33.5914  37.7898 107.6496]\n",
      "Weights: [-4.8254  0.8173 -1.258   0.1175  0.1442]\n",
      "MSE loss: 86.8862\n",
      "Iteration: 123400\n",
      "Gradient: [  -3.1011   -5.4128  -11.6202   12.0568 -110.1946]\n",
      "Weights: [-4.8395  0.8151 -1.2581  0.1176  0.1443]\n",
      "MSE loss: 86.9178\n",
      "Iteration: 123500\n",
      "Gradient: [ -4.5734 -22.7319 -32.9878 -59.8264 -97.3326]\n",
      "Weights: [-4.8367  0.8102 -1.2567  0.1172  0.1446]\n",
      "MSE loss: 86.9245\n",
      "Iteration: 123600\n",
      "Gradient: [ -4.9718   0.4734  23.6591   8.7909 121.6342]\n",
      "Weights: [-4.8318  0.8216 -1.2611  0.1176  0.1443]\n",
      "MSE loss: 86.795\n",
      "Iteration: 123700\n",
      "Gradient: [  3.2552 -14.0419  13.7103  75.2792 214.5755]\n",
      "Weights: [-4.8283  0.82   -1.2607  0.1171  0.1444]\n",
      "MSE loss: 86.8073\n",
      "Iteration: 123800\n",
      "Gradient: [ -2.4349   0.7209 -16.7214  -3.487   48.2074]\n",
      "Weights: [-4.8247  0.8222 -1.2614  0.1176  0.1444]\n",
      "MSE loss: 86.9326\n",
      "Iteration: 123900\n",
      "Gradient: [ -5.7327 -16.5346 -41.8264 -70.066  -84.4699]\n",
      "Weights: [-4.836   0.8209 -1.2653  0.1182  0.1446]\n",
      "MSE loss: 86.9962\n",
      "Iteration: 124000\n",
      "Gradient: [  9.0622  36.4544  27.3396 -57.9463 442.5662]\n",
      "Weights: [-4.8252  0.8271 -1.2652  0.1183  0.1444]\n",
      "MSE loss: 86.8977\n",
      "Iteration: 124100\n",
      "Gradient: [ -5.0053 -16.9615  13.2453 -15.8082  71.8253]\n",
      "Weights: [-4.8414  0.8249 -1.2666  0.1184  0.1445]\n",
      "MSE loss: 87.1041\n",
      "Iteration: 124200\n",
      "Gradient: [   4.9541   -7.0896  -13.9995   48.2971 -295.6492]\n",
      "Weights: [-4.8318  0.8236 -1.2669  0.1184  0.1445]\n",
      "MSE loss: 86.9123\n",
      "Iteration: 124300\n",
      "Gradient: [ -3.366  -11.8425 -10.605   36.1648 294.395 ]\n",
      "Weights: [-4.8233  0.8246 -1.2672  0.119   0.1447]\n",
      "MSE loss: 86.9615\n",
      "Iteration: 124400\n",
      "Gradient: [  6.6327  15.7782  -5.1701 129.4576  78.646 ]\n",
      "Weights: [-4.8302  0.822  -1.2694  0.1193  0.1448]\n",
      "MSE loss: 86.7824\n",
      "Iteration: 124500\n",
      "Gradient: [ -2.3612  -9.5825  19.4783  71.9604 212.9526]\n",
      "Weights: [-4.8367  0.8293 -1.2704  0.1197  0.1446]\n",
      "MSE loss: 86.7482\n",
      "Iteration: 124600\n",
      "Gradient: [-6.042  -2.2692 10.5686 45.8231 31.3564]\n",
      "Weights: [-4.8209  0.8198 -1.2715  0.1211  0.1444]\n",
      "MSE loss: 86.7587\n",
      "Iteration: 124700\n",
      "Gradient: [  1.7858 -12.2516 -10.3435 -38.2859 -81.0607]\n",
      "Weights: [-4.8181  0.8234 -1.2705  0.1199  0.1444]\n",
      "MSE loss: 86.8544\n",
      "Iteration: 124800\n",
      "Gradient: [-10.5349   7.9916 -19.3596   3.9489  54.8327]\n",
      "Weights: [-4.8335  0.8308 -1.2692  0.1199  0.1443]\n",
      "MSE loss: 86.7658\n",
      "Iteration: 124900\n",
      "Gradient: [  -1.7732   -3.6597  -42.8238  -44.7553 -414.2344]\n",
      "Weights: [-4.8303  0.8191 -1.2684  0.1196  0.1443]\n",
      "MSE loss: 87.0685\n",
      "Iteration: 125000\n",
      "Gradient: [  1.5853 -22.6278 -27.4601 -69.9204 221.0583]\n",
      "Weights: [-4.8223  0.824  -1.2677  0.1196  0.1444]\n",
      "MSE loss: 86.8647\n",
      "Iteration: 125100\n",
      "Gradient: [  4.7216  11.5529  13.1974 -64.3978  75.6676]\n",
      "Weights: [-4.8308  0.8303 -1.2659  0.1194  0.1442]\n",
      "MSE loss: 86.9564\n",
      "Iteration: 125200\n",
      "Gradient: [ -7.4439   5.3548   6.7502 102.7961 -11.4976]\n",
      "Weights: [-4.8362  0.8237 -1.266   0.1197  0.1444]\n",
      "MSE loss: 86.776\n",
      "Iteration: 125300\n",
      "Gradient: [   4.1836    6.1956  -35.5172  -91.8254 -192.4943]\n",
      "Weights: [-4.825   0.8189 -1.2695  0.1199  0.1444]\n",
      "MSE loss: 86.8788\n",
      "Iteration: 125400\n",
      "Gradient: [-18.3486  18.3111  -2.8238  93.8074 116.8049]\n",
      "Weights: [-4.8288  0.8023 -1.2644  0.1204  0.1447]\n",
      "MSE loss: 87.1444\n",
      "Iteration: 125500\n",
      "Gradient: [  9.1476  13.1315 -18.7529  59.0016 -24.2231]\n",
      "Weights: [-4.8091  0.796  -1.2601  0.1192  0.1447]\n",
      "MSE loss: 86.9332\n",
      "Iteration: 125600\n",
      "Gradient: [ -2.6091  -9.6067 -67.6887   0.8111 -89.7454]\n",
      "Weights: [-4.8374  0.8117 -1.2603  0.1186  0.1446]\n",
      "MSE loss: 86.9399\n",
      "Iteration: 125700\n",
      "Gradient: [  4.5394  -4.1222 -29.8735  31.7237 -90.3929]\n",
      "Weights: [-4.8238  0.7985 -1.2579  0.1189  0.1446]\n",
      "MSE loss: 86.906\n",
      "Iteration: 125800\n",
      "Gradient: [-13.4355  -3.777    7.717  -42.4015  13.7634]\n",
      "Weights: [-4.8286  0.8064 -1.258   0.1184  0.1444]\n",
      "MSE loss: 86.8221\n",
      "Iteration: 125900\n",
      "Gradient: [  -0.2461   -5.1912   54.6509   42.3858 -100.9236]\n",
      "Weights: [-4.8343  0.8072 -1.2562  0.1187  0.1444]\n",
      "MSE loss: 86.9291\n",
      "Iteration: 126000\n",
      "Gradient: [ -4.4565 -12.8449  26.514  -69.5057 -61.9583]\n",
      "Weights: [-4.8288  0.8143 -1.2587  0.1183  0.1441]\n",
      "MSE loss: 86.7745\n",
      "Iteration: 126100\n",
      "Gradient: [13.3098  6.033   8.432  53.1025 91.2767]\n",
      "Weights: [-4.8247  0.8099 -1.2585  0.1191  0.144 ]\n",
      "MSE loss: 86.7753\n",
      "Iteration: 126200\n",
      "Gradient: [   2.8366   -4.811     0.5022   86.7804 -176.4062]\n",
      "Weights: [-4.8296  0.8174 -1.2574  0.1195  0.1437]\n",
      "MSE loss: 86.978\n",
      "Iteration: 126300\n",
      "Gradient: [   6.1261  -11.1603    4.1079   50.1604 -166.7072]\n",
      "Weights: [-4.8179  0.8125 -1.2622  0.1195  0.1439]\n",
      "MSE loss: 86.8657\n",
      "Iteration: 126400\n",
      "Gradient: [  3.038   -7.0828 -16.4943  49.7546  65.307 ]\n",
      "Weights: [-4.8453  0.8244 -1.2623  0.1201  0.1437]\n",
      "MSE loss: 86.8543\n",
      "Iteration: 126500\n",
      "Gradient: [  8.3065  26.9834  16.0405 106.0177 239.752 ]\n",
      "Weights: [-4.8226  0.8261 -1.2623  0.1197  0.1438]\n",
      "MSE loss: 87.4415\n",
      "Iteration: 126600\n",
      "Gradient: [ -5.1509  -4.0339 -11.1636  44.1256 159.2549]\n",
      "Weights: [-4.8219  0.8184 -1.2634  0.12    0.1438]\n",
      "MSE loss: 86.8339\n",
      "Iteration: 126700\n",
      "Gradient: [  -9.3663    1.9437  -24.2911  -86.1882 -438.7028]\n",
      "Weights: [-4.844   0.8218 -1.2637  0.1195  0.144 ]\n",
      "MSE loss: 87.072\n",
      "Iteration: 126800\n",
      "Gradient: [  7.8159   7.0066 -19.9157  57.3191  10.1185]\n",
      "Weights: [-4.8454  0.8271 -1.265   0.1204  0.144 ]\n",
      "MSE loss: 86.8675\n",
      "Iteration: 126900\n",
      "Gradient: [12.6376  4.1069  6.7408  7.0802  7.7588]\n",
      "Weights: [-4.8242  0.8218 -1.2631  0.1197  0.1436]\n",
      "MSE loss: 86.8694\n",
      "Iteration: 127000\n",
      "Gradient: [  -0.3529   25.3946   -2.8894  -55.6758 -143.7403]\n",
      "Weights: [-4.8159  0.8083 -1.2634  0.1194  0.1442]\n",
      "MSE loss: 86.8969\n",
      "Iteration: 127100\n",
      "Gradient: [  6.4366   5.2771 -41.4343 -39.1541 227.6973]\n",
      "Weights: [-4.8425  0.8186 -1.2616  0.1197  0.1439]\n",
      "MSE loss: 86.919\n",
      "Iteration: 127200\n",
      "Gradient: [ -3.5761 -15.6061  51.2485   9.0986 -23.076 ]\n",
      "Weights: [-4.8394  0.815  -1.2632  0.1201  0.1441]\n",
      "MSE loss: 86.9951\n",
      "Iteration: 127300\n",
      "Gradient: [ -13.9045  -11.6008   38.655  -113.8555 -208.4589]\n",
      "Weights: [-4.8359  0.8203 -1.2653  0.1203  0.1439]\n",
      "MSE loss: 86.8833\n",
      "Iteration: 127400\n",
      "Gradient: [  -9.4223   11.7639   31.7273   74.124  -218.0511]\n",
      "Weights: [-4.8265  0.8144 -1.2624  0.121   0.1437]\n",
      "MSE loss: 86.7823\n",
      "Iteration: 127500\n",
      "Gradient: [  -0.9171    8.7304   -5.5932  -12.6022 -508.7564]\n",
      "Weights: [-4.8293  0.8071 -1.2622  0.1217  0.1435]\n",
      "MSE loss: 86.8549\n",
      "Iteration: 127600\n",
      "Gradient: [ -7.5375  11.5409 -15.9348 -72.8117 -94.3561]\n",
      "Weights: [-4.8318  0.8159 -1.2646  0.1219  0.1434]\n",
      "MSE loss: 86.7827\n",
      "Iteration: 127700\n",
      "Gradient: [  4.699    8.7795  32.4193  81.8626 -92.816 ]\n",
      "Weights: [-4.8202  0.8232 -1.265   0.1223  0.1435]\n",
      "MSE loss: 87.7792\n",
      "Iteration: 127800\n",
      "Gradient: [  3.3556 -12.0387   6.0064  86.6292 133.4341]\n",
      "Weights: [-4.8289  0.816  -1.2653  0.1229  0.1434]\n",
      "MSE loss: 86.7412\n",
      "Iteration: 127900\n",
      "Gradient: [-4.2202  9.0247  8.1098 32.6729 33.6851]\n",
      "Weights: [-4.8161  0.8142 -1.2683  0.1229  0.1432]\n",
      "MSE loss: 86.9205\n",
      "Iteration: 128000\n",
      "Gradient: [  6.6478  13.1195  32.2462 124.5561 162.833 ]\n",
      "Weights: [-4.8344  0.8175 -1.2676  0.124   0.1433]\n",
      "MSE loss: 86.7437\n",
      "Iteration: 128100\n",
      "Gradient: [ -4.2444 -12.7669   3.7585 -64.0497 285.997 ]\n",
      "Weights: [-4.8237  0.8238 -1.2683  0.1233  0.1431]\n",
      "MSE loss: 86.8772\n",
      "Iteration: 128200\n",
      "Gradient: [  3.5991   1.0825 -26.1537  42.986  -19.0459]\n",
      "Weights: [-4.8391  0.817  -1.2651  0.1228  0.1432]\n",
      "MSE loss: 86.8738\n",
      "Iteration: 128300\n",
      "Gradient: [  -7.9885    3.754   -12.419    -8.7423 -123.914 ]\n",
      "Weights: [-4.8264  0.8128 -1.2672  0.1238  0.1431]\n",
      "MSE loss: 86.7598\n",
      "Iteration: 128400\n",
      "Gradient: [   8.758     3.5285  -50.7567   32.9739 -241.6424]\n",
      "Weights: [-4.8378  0.8151 -1.2653  0.1236  0.1431]\n",
      "MSE loss: 86.8349\n",
      "Iteration: 128500\n",
      "Gradient: [  4.0976   6.4365 -13.664  -29.4323 120.3286]\n",
      "Weights: [-4.8248  0.8187 -1.2641  0.1238  0.1427]\n",
      "MSE loss: 86.9005\n",
      "Iteration: 128600\n",
      "Gradient: [  3.8221   9.5522 -11.0977 -32.1239  99.0707]\n",
      "Weights: [-4.8402  0.8237 -1.2644  0.1237  0.1425]\n",
      "MSE loss: 86.7874\n",
      "Iteration: 128700\n",
      "Gradient: [  5.1521  19.0009  -5.8015 -44.4618 113.5941]\n",
      "Weights: [-4.8203  0.7997 -1.2603  0.1241  0.1428]\n",
      "MSE loss: 86.7309\n",
      "Iteration: 128800\n",
      "Gradient: [  -1.192   -13.5411    1.1992   26.0666 -154.1015]\n",
      "Weights: [-4.828   0.7972 -1.2613  0.1244  0.1429]\n",
      "MSE loss: 86.9655\n",
      "Iteration: 128900\n",
      "Gradient: [  5.6869 -11.8874  14.5754  88.8924 140.2176]\n",
      "Weights: [-4.8163  0.7979 -1.2598  0.1241  0.1429]\n",
      "MSE loss: 86.8097\n",
      "Iteration: 129000\n",
      "Gradient: [  -2.6412    1.2577  -33.0172  -47.582  -251.3658]\n",
      "Weights: [-4.8193  0.8106 -1.2635  0.1234  0.1429]\n",
      "MSE loss: 86.7806\n",
      "Iteration: 129100\n",
      "Gradient: [  1.6935   9.0779  41.7581  33.413  123.6292]\n",
      "Weights: [-4.8328  0.8154 -1.2628  0.1236  0.1429]\n",
      "MSE loss: 86.7861\n",
      "Iteration: 129200\n",
      "Gradient: [  -3.0522    2.181   -27.1898   34.1166 -308.1259]\n",
      "Weights: [-4.8296  0.8195 -1.2652  0.1242  0.1426]\n",
      "MSE loss: 86.7541\n",
      "Iteration: 129300\n",
      "Gradient: [ -4.5204  18.4788  25.0303 109.9364  29.5519]\n",
      "Weights: [-4.8358  0.8167 -1.2643  0.1246  0.1426]\n",
      "MSE loss: 86.7292\n",
      "Iteration: 129400\n",
      "Gradient: [ -5.7618  18.4153 -25.2695  -7.7084   4.3617]\n",
      "Weights: [-4.831   0.8115 -1.266   0.1255  0.1428]\n",
      "MSE loss: 86.7327\n",
      "Iteration: 129500\n",
      "Gradient: [  7.3082   0.9623   7.1061 -41.7024  19.1699]\n",
      "Weights: [-4.8267  0.8199 -1.2682  0.125   0.1427]\n",
      "MSE loss: 86.7253\n",
      "Iteration: 129600\n",
      "Gradient: [ -5.2602  -7.7115   4.0549  -3.5772 -38.6334]\n",
      "Weights: [-4.8333  0.8245 -1.2677  0.1251  0.1427]\n",
      "MSE loss: 86.9133\n",
      "Iteration: 129700\n",
      "Gradient: [  -9.2907   -7.4322  -39.6085   30.8604 -192.765 ]\n",
      "Weights: [-4.8235  0.8183 -1.2725  0.1251  0.1429]\n",
      "MSE loss: 86.8011\n",
      "Iteration: 129800\n",
      "Gradient: [  -1.6207  -25.4109  -23.8345 -100.1396 -191.439 ]\n",
      "Weights: [-4.8502  0.8187 -1.2685  0.1247  0.1427]\n",
      "MSE loss: 87.7572\n",
      "Iteration: 129900\n",
      "Gradient: [   1.5967   -8.4467  -24.5942    2.5239 -381.899 ]\n",
      "Weights: [-4.8404  0.8182 -1.2664  0.125   0.1428]\n",
      "MSE loss: 86.7838\n",
      "Iteration: 130000\n",
      "Gradient: [ -10.9442    7.5168  -14.6749   58.8979 -102.3868]\n",
      "Weights: [-4.8414  0.8114 -1.2641  0.1244  0.1429]\n",
      "MSE loss: 86.991\n",
      "Iteration: 130100\n",
      "Gradient: [  -2.8891   -6.0324  -18.4676  -21.3905 -345.3116]\n",
      "Weights: [-4.8184  0.8071 -1.2654  0.1247  0.1427]\n",
      "MSE loss: 86.785\n",
      "Iteration: 130200\n",
      "Gradient: [  12.1916    8.9769  -59.3081  -14.4013 -129.0497]\n",
      "Weights: [-4.8408  0.8166 -1.2636  0.1246  0.1425]\n",
      "MSE loss: 86.8105\n",
      "Iteration: 130300\n",
      "Gradient: [  9.4308  -5.1763  -4.333   67.4614 245.1866]\n",
      "Weights: [-4.8301  0.8147 -1.2619  0.1244  0.1425]\n",
      "MSE loss: 86.8268\n",
      "Iteration: 130400\n",
      "Gradient: [ 0.5232 13.9537 19.8084 42.6365 37.9223]\n",
      "Weights: [-4.8292  0.8017 -1.2586  0.1253  0.1423]\n",
      "MSE loss: 86.7754\n",
      "Iteration: 130500\n",
      "Gradient: [ 12.1329  11.5928 -36.284  136.6475 124.128 ]\n",
      "Weights: [-4.8072  0.8024 -1.2609  0.1247  0.1425]\n",
      "MSE loss: 87.1952\n",
      "Iteration: 130600\n",
      "Gradient: [  6.5863  12.3849 -19.3775 -20.1427 -41.197 ]\n",
      "Weights: [-4.8206  0.8018 -1.259   0.1244  0.1426]\n",
      "MSE loss: 86.8107\n",
      "Iteration: 130700\n",
      "Gradient: [  -4.9539    6.1618    4.6113  184.9282 -177.4185]\n",
      "Weights: [-4.8231  0.8084 -1.2585  0.1235  0.1425]\n",
      "MSE loss: 86.8453\n",
      "Iteration: 130800\n",
      "Gradient: [ -9.3593  -2.6474 -10.665  -35.6086  21.2747]\n",
      "Weights: [-4.8263  0.804  -1.2565  0.1238  0.1424]\n",
      "MSE loss: 86.7887\n",
      "Iteration: 130900\n",
      "Gradient: [ -5.851  -19.8612  37.2165 -14.81   -91.2149]\n",
      "Weights: [-4.8238  0.7997 -1.2568  0.124   0.1424]\n",
      "MSE loss: 86.7416\n",
      "Iteration: 131000\n",
      "Gradient: [-8.2011  6.2289 30.3277 -4.6463 36.755 ]\n",
      "Weights: [-4.8282  0.8114 -1.2599  0.1239  0.1422]\n",
      "MSE loss: 86.7751\n",
      "Iteration: 131100\n",
      "Gradient: [  2.7513   0.7542  12.2425  40.0481 268.0292]\n",
      "Weights: [-4.8112  0.8035 -1.2587  0.1241  0.1423]\n",
      "MSE loss: 87.0488\n",
      "Iteration: 131200\n",
      "Gradient: [  3.3782   1.086   20.4726  80.7119 449.0726]\n",
      "Weights: [-4.8135  0.8035 -1.2591  0.1243  0.1425]\n",
      "MSE loss: 87.0705\n",
      "Iteration: 131300\n",
      "Gradient: [  3.5307  -4.7829  21.6529  61.4355 169.0805]\n",
      "Weights: [-4.8282  0.8098 -1.2633  0.125   0.1424]\n",
      "MSE loss: 86.7112\n",
      "Iteration: 131400\n",
      "Gradient: [ -2.964   19.9921  23.6256 -84.5682 -90.8667]\n",
      "Weights: [-4.8373  0.8139 -1.2644  0.1243  0.1426]\n",
      "MSE loss: 86.9006\n",
      "Iteration: 131500\n",
      "Gradient: [   1.4131   -1.4373  -20.2427   47.7066 -130.0293]\n",
      "Weights: [-4.8212  0.8097 -1.2665  0.1247  0.1429]\n",
      "MSE loss: 86.7068\n",
      "Iteration: 131600\n",
      "Gradient: [  0.3573  -1.2179  -2.4584 -17.4006 -79.9317]\n",
      "Weights: [-4.8205  0.808  -1.2638  0.1238  0.1428]\n",
      "MSE loss: 86.7464\n",
      "Iteration: 131700\n",
      "Gradient: [-7.797800e+00  1.077000e-01  7.108000e-01  8.170240e+01  2.438299e+02]\n",
      "Weights: [-4.8358  0.8106 -1.2638  0.1249  0.1428]\n",
      "MSE loss: 86.793\n",
      "Iteration: 131800\n",
      "Gradient: [ -8.5665  16.9543  19.9292  29.081  120.5803]\n",
      "Weights: [-4.8269  0.8103 -1.2632  0.1247  0.1427]\n",
      "MSE loss: 86.7365\n",
      "Iteration: 131900\n",
      "Gradient: [ -2.6276  10.3331  45.76   -13.5031  72.2987]\n",
      "Weights: [-4.83    0.809  -1.2631  0.1247  0.1424]\n",
      "MSE loss: 86.8117\n",
      "Iteration: 132000\n",
      "Gradient: [  2.2838  15.7323  -2.4975  51.4609 199.912 ]\n",
      "Weights: [-4.8199  0.8089 -1.2615  0.1248  0.1424]\n",
      "MSE loss: 86.887\n",
      "Iteration: 132100\n",
      "Gradient: [  0.5394  11.2269 -15.8784 -75.5039  89.5365]\n",
      "Weights: [-4.8179  0.8015 -1.2622  0.1251  0.1427]\n",
      "MSE loss: 86.7575\n",
      "Iteration: 132200\n",
      "Gradient: [ -3.5235   4.7737 -63.5546  -8.1867  63.7634]\n",
      "Weights: [-4.8207  0.8087 -1.2633  0.1238  0.1427]\n",
      "MSE loss: 86.7526\n",
      "Iteration: 132300\n",
      "Gradient: [  0.1544  11.7335  45.7141  61.8529 146.9744]\n",
      "Weights: [-4.8299  0.8161 -1.2638  0.1235  0.1429]\n",
      "MSE loss: 86.7442\n",
      "Iteration: 132400\n",
      "Gradient: [-9.28010e+00 -6.94760e+00 -5.15402e+01  1.65900e-01 -5.16377e+02]\n",
      "Weights: [-4.8292  0.8153 -1.2662  0.1233  0.1433]\n",
      "MSE loss: 86.706\n",
      "Iteration: 132500\n",
      "Gradient: [   8.7483    3.7233  -24.5129   -8.1199 -197.0452]\n",
      "Weights: [-4.8363  0.8225 -1.2678  0.123   0.1433]\n",
      "MSE loss: 86.7201\n",
      "Iteration: 132600\n",
      "Gradient: [-10.8613   5.8496 -18.5242 -38.7388 -33.8375]\n",
      "Weights: [-4.8199  0.8057 -1.2627  0.1227  0.1434]\n",
      "MSE loss: 86.7421\n",
      "Iteration: 132700\n",
      "Gradient: [   5.1315    2.8609   13.8294 -119.3093   62.1644]\n",
      "Weights: [-4.8152  0.8085 -1.2632  0.1221  0.1435]\n",
      "MSE loss: 86.842\n",
      "Iteration: 132800\n",
      "Gradient: [   9.2      -6.5151  -17.0375   25.335  -143.0354]\n",
      "Weights: [-4.8271  0.8187 -1.2649  0.1221  0.1434]\n",
      "MSE loss: 86.745\n",
      "Iteration: 132900\n",
      "Gradient: [  7.8259   0.3732 -14.9488 -40.8385 -90.3998]\n",
      "Weights: [-4.8333  0.8292 -1.2686  0.1229  0.1432]\n",
      "MSE loss: 86.7745\n",
      "Iteration: 133000\n",
      "Gradient: [ 0.8893 -3.0153 59.5621 55.0405 -1.1471]\n",
      "Weights: [-4.8349  0.8266 -1.2693  0.1231  0.1434]\n",
      "MSE loss: 86.7326\n",
      "Iteration: 133100\n",
      "Gradient: [ -3.2609  -2.3229   0.7374  29.6502 121.3869]\n",
      "Weights: [-4.8353  0.829  -1.2709  0.1223  0.1435]\n",
      "MSE loss: 86.7908\n",
      "Iteration: 133200\n",
      "Gradient: [  -2.8707  -12.5678  -22.2863   58.395  -132.1648]\n",
      "Weights: [-4.8372  0.8355 -1.27    0.1223  0.1433]\n",
      "MSE loss: 86.7835\n",
      "Iteration: 133300\n",
      "Gradient: [  6.0704 -10.1666  37.6783  62.9516 263.8932]\n",
      "Weights: [-4.8401  0.832  -1.2708  0.1234  0.1434]\n",
      "MSE loss: 86.804\n",
      "Iteration: 133400\n",
      "Gradient: [  -4.4545   -9.824   -14.9038  -36.1492 -202.1687]\n",
      "Weights: [-4.8361  0.8295 -1.2716  0.1229  0.1434]\n",
      "MSE loss: 86.7121\n",
      "Iteration: 133500\n",
      "Gradient: [ 1.951400e+00 -9.593000e+00 -8.069700e+01  7.970000e-02 -1.723395e+02]\n",
      "Weights: [-4.8405  0.8321 -1.2731  0.1237  0.1434]\n",
      "MSE loss: 86.7059\n",
      "Iteration: 133600\n",
      "Gradient: [ -3.3735  -3.5087   0.2118 -23.43    71.5132]\n",
      "Weights: [-4.8411  0.8345 -1.2717  0.1225  0.1435]\n",
      "MSE loss: 86.7263\n",
      "Iteration: 133700\n",
      "Gradient: [  0.2155   4.9046 -28.2511 -60.1079 -12.75  ]\n",
      "Weights: [-4.8439  0.8442 -1.2758  0.1217  0.1437]\n",
      "MSE loss: 86.8347\n",
      "Iteration: 133800\n",
      "Gradient: [-11.747  -26.8585  -2.1319  42.4987 334.0642]\n",
      "Weights: [-4.8516  0.8393 -1.2735  0.122   0.1439]\n",
      "MSE loss: 86.8661\n",
      "Iteration: 133900\n",
      "Gradient: [   4.5674    0.7611   -9.5089   47.8478 -211.6897]\n",
      "Weights: [-4.8348  0.8308 -1.2711  0.1222  0.1437]\n",
      "MSE loss: 86.7154\n",
      "Iteration: 134000\n",
      "Gradient: [ -7.0415  10.5221  26.1059 113.44   187.1678]\n",
      "Weights: [-4.8458  0.8345 -1.2695  0.122   0.1438]\n",
      "MSE loss: 86.8633\n",
      "Iteration: 134100\n",
      "Gradient: [  -0.6803    8.0254   -2.0121   11.1795 -158.5218]\n",
      "Weights: [-4.8227  0.8223 -1.2699  0.1225  0.1436]\n",
      "MSE loss: 86.7761\n",
      "Iteration: 134200\n",
      "Gradient: [  1.2117  12.6672  51.4389  32.9754 323.6569]\n",
      "Weights: [-4.8231  0.8225 -1.2729  0.1238  0.1436]\n",
      "MSE loss: 86.7382\n",
      "Iteration: 134300\n",
      "Gradient: [  6.2583   4.2992  44.396   56.9199 -29.2165]\n",
      "Weights: [-4.8268  0.8282 -1.2729  0.1244  0.1435]\n",
      "MSE loss: 86.9803\n",
      "Iteration: 134400\n",
      "Gradient: [ -7.2026   2.9456 -42.8579 -93.5691 211.0196]\n",
      "Weights: [-4.8397  0.8316 -1.2745  0.1245  0.1435]\n",
      "MSE loss: 86.7527\n",
      "Iteration: 134500\n",
      "Gradient: [ -3.4663 -14.2688  29.8907 -33.7284 154.8483]\n",
      "Weights: [-4.817   0.829  -1.2773  0.1237  0.1436]\n",
      "MSE loss: 86.8765\n",
      "Iteration: 134600\n",
      "Gradient: [  4.8115   5.5145  54.5586 108.636  168.1591]\n",
      "Weights: [-4.8287  0.8346 -1.2774  0.1249  0.1435]\n",
      "MSE loss: 86.9032\n",
      "Iteration: 134700\n",
      "Gradient: [ -5.6558   5.2737  -0.9704  74.0602 138.7158]\n",
      "Weights: [-4.8261  0.8294 -1.2815  0.1263  0.1434]\n",
      "MSE loss: 86.6526\n",
      "Iteration: 134800\n",
      "Gradient: [  0.5399  -6.1615  -2.3654 -27.2665 -66.8685]\n",
      "Weights: [-4.8416  0.8309 -1.2777  0.1257  0.1436]\n",
      "MSE loss: 86.7741\n",
      "Iteration: 134900\n",
      "Gradient: [  7.3402  27.3155  40.2769 156.4434 -30.6154]\n",
      "Weights: [-4.8109  0.8231 -1.2783  0.1256  0.1435]\n",
      "MSE loss: 87.0142\n",
      "Iteration: 135000\n",
      "Gradient: [ -3.9223  -4.4853 -37.3905  24.2189  12.5167]\n",
      "Weights: [-4.8423  0.8352 -1.2808  0.1256  0.1436]\n",
      "MSE loss: 86.7531\n",
      "Iteration: 135100\n",
      "Gradient: [-3.370800e+00 -3.419000e-01  3.682650e+01 -8.700000e-03  1.035244e+02]\n",
      "Weights: [-4.8362  0.8383 -1.2795  0.1251  0.1433]\n",
      "MSE loss: 86.6537\n",
      "Iteration: 135200\n",
      "Gradient: [  1.6896  -4.1237  15.9639  69.6049 -51.4146]\n",
      "Weights: [-4.8452  0.8506 -1.2791  0.1247  0.1432]\n",
      "MSE loss: 86.8219\n",
      "Iteration: 135300\n",
      "Gradient: [-3.7958 16.1045 30.4151 76.5567 52.3258]\n",
      "Weights: [-4.8341  0.8438 -1.2795  0.1249  0.1432]\n",
      "MSE loss: 86.7817\n",
      "Iteration: 135400\n",
      "Gradient: [  -3.0563  -22.4678    5.228   -34.862  -251.1341]\n",
      "Weights: [-4.8319  0.8363 -1.2786  0.125   0.1434]\n",
      "MSE loss: 86.6926\n",
      "Iteration: 135500\n",
      "Gradient: [  2.9562   5.5136   0.3287 -17.7666  56.8836]\n",
      "Weights: [-4.833   0.8398 -1.2783  0.1246  0.1433]\n",
      "MSE loss: 86.726\n",
      "Iteration: 135600\n",
      "Gradient: [  9.8132  10.3415 -24.0356  19.901   58.9088]\n",
      "Weights: [-4.8361  0.8418 -1.2776  0.1244  0.1433]\n",
      "MSE loss: 86.7575\n",
      "Iteration: 135700\n",
      "Gradient: [   1.3828  -10.8637    7.1364   -5.7835 -142.2274]\n",
      "Weights: [-4.8247  0.8222 -1.2761  0.1249  0.1435]\n",
      "MSE loss: 86.7072\n",
      "Iteration: 135800\n",
      "Gradient: [  6.7906   1.0397  19.825  -33.4169 304.5437]\n",
      "Weights: [-4.8373  0.8292 -1.2781  0.1251  0.1436]\n",
      "MSE loss: 86.7411\n",
      "Iteration: 135900\n",
      "Gradient: [  12.6942   -0.4328   42.6175   54.1962 -124.1135]\n",
      "Weights: [-4.8435  0.8394 -1.2796  0.1253  0.1435]\n",
      "MSE loss: 86.689\n",
      "Iteration: 136000\n",
      "Gradient: [ -4.9224  -0.3738  68.16   -25.4466 320.1245]\n",
      "Weights: [-4.8448  0.844  -1.2803  0.1245  0.1436]\n",
      "MSE loss: 86.6854\n",
      "Iteration: 136100\n",
      "Gradient: [ -6.9358  14.852  -50.2893  78.6169 182.763 ]\n",
      "Weights: [-4.8481  0.8529 -1.2827  0.1242  0.1435]\n",
      "MSE loss: 86.7139\n",
      "Iteration: 136200\n",
      "Gradient: [ -3.7557   0.6976 -39.0054   4.6249 -53.5966]\n",
      "Weights: [-4.8413  0.8473 -1.2852  0.1248  0.1436]\n",
      "MSE loss: 86.794\n",
      "Iteration: 136300\n",
      "Gradient: [  4.4297   9.6387 -27.8892  19.8113   3.2966]\n",
      "Weights: [-4.8264  0.8337 -1.2837  0.1252  0.144 ]\n",
      "MSE loss: 86.6693\n",
      "Iteration: 136400\n",
      "Gradient: [  9.128   13.7242  30.0582  82.973  -94.247 ]\n",
      "Weights: [-4.8195  0.8369 -1.2847  0.1247  0.144 ]\n",
      "MSE loss: 86.8066\n",
      "Iteration: 136500\n",
      "Gradient: [  11.0022   -3.9134   62.8318  -50.3225 -183.2471]\n",
      "Weights: [-4.8256  0.8343 -1.2863  0.1254  0.144 ]\n",
      "MSE loss: 86.7519\n",
      "Iteration: 136600\n",
      "Gradient: [ -13.6544    0.51    -35.6015   20.8676 -104.7977]\n",
      "Weights: [-4.8386  0.8345 -1.2865  0.1259  0.1441]\n",
      "MSE loss: 86.9093\n",
      "Iteration: 136700\n",
      "Gradient: [ -2.0291   9.0723 -30.7074  30.4553  52.1984]\n",
      "Weights: [-4.8237  0.8222 -1.2789  0.1255  0.1438]\n",
      "MSE loss: 86.7008\n",
      "Iteration: 136800\n",
      "Gradient: [   0.9759  -12.7021    1.3746  -32.1022 -397.2071]\n",
      "Weights: [-4.8296  0.8298 -1.2797  0.1254  0.1437]\n",
      "MSE loss: 86.6564\n",
      "Iteration: 136900\n",
      "Gradient: [  6.8516   9.7077   0.6382  69.0324 265.3525]\n",
      "Weights: [-4.8284  0.8249 -1.2798  0.1261  0.1437]\n",
      "MSE loss: 86.6964\n",
      "Iteration: 137000\n",
      "Gradient: [   2.9843   -6.1425   20.6007    9.2947 -113.2207]\n",
      "Weights: [-4.828   0.8266 -1.2821  0.1251  0.1441]\n",
      "MSE loss: 86.7356\n",
      "Iteration: 137100\n",
      "Gradient: [  7.0312  -6.8813 -11.5195  -6.4859 -13.8965]\n",
      "Weights: [-4.8251  0.8307 -1.2841  0.1249  0.1444]\n",
      "MSE loss: 86.7131\n",
      "Iteration: 137200\n",
      "Gradient: [ -9.6502   7.2179  37.2579 -22.0833 168.3041]\n",
      "Weights: [-4.8211  0.8263 -1.2843  0.1251  0.1446]\n",
      "MSE loss: 86.8165\n",
      "Iteration: 137300\n",
      "Gradient: [ -9.0067  15.6898  62.3165  95.159  443.0792]\n",
      "Weights: [-4.8158  0.829  -1.2851  0.1257  0.1443]\n",
      "MSE loss: 86.9406\n",
      "Iteration: 137400\n",
      "Gradient: [  -3.0859    4.3227  -20.2931  -56.6608 -159.8696]\n",
      "Weights: [-4.824   0.8287 -1.285   0.1252  0.1444]\n",
      "MSE loss: 86.7312\n",
      "Iteration: 137500\n",
      "Gradient: [  0.5618  -4.975    9.4462 -73.4607  53.0852]\n",
      "Weights: [-4.8343  0.8312 -1.2817  0.1247  0.1443]\n",
      "MSE loss: 86.7593\n",
      "Iteration: 137600\n",
      "Gradient: [ -2.1822 -14.2044  24.1877 -52.4359 306.7632]\n",
      "Weights: [-4.8202  0.8199 -1.281   0.1253  0.1441]\n",
      "MSE loss: 86.7662\n",
      "Iteration: 137700\n",
      "Gradient: [  0.7479   6.8541 -33.4934 -37.789  240.2787]\n",
      "Weights: [-4.8301  0.8372 -1.2806  0.1252  0.1437]\n",
      "MSE loss: 86.7511\n",
      "Iteration: 137800\n",
      "Gradient: [  9.4046  -8.6111   2.0399  87.6941 324.365 ]\n",
      "Weights: [-4.8249  0.8295 -1.2803  0.1253  0.1438]\n",
      "MSE loss: 86.7139\n",
      "Iteration: 137900\n",
      "Gradient: [  -2.0539  -10.2639   14.9124  -13.1962 -255.4838]\n",
      "Weights: [-4.8145  0.8271 -1.2807  0.1257  0.1435]\n",
      "MSE loss: 86.8398\n",
      "Iteration: 138000\n",
      "Gradient: [  1.9343 -19.3266 -23.4475 -44.1622 117.9452]\n",
      "Weights: [-4.8424  0.8328 -1.2791  0.1249  0.1436]\n",
      "MSE loss: 86.8412\n",
      "Iteration: 138100\n",
      "Gradient: [  1.8477  10.4726  11.9619  99.6774 119.2496]\n",
      "Weights: [-4.8229  0.8328 -1.2793  0.1253  0.1435]\n",
      "MSE loss: 86.8801\n",
      "Iteration: 138200\n",
      "Gradient: [  -4.2498    5.2941  -74.8877  -27.6098 -379.9072]\n",
      "Weights: [-4.8485  0.8396 -1.2783  0.1237  0.1436]\n",
      "MSE loss: 86.9896\n",
      "Iteration: 138300\n",
      "Gradient: [  7.4984   6.7052  17.3735 -14.4082 -37.1018]\n",
      "Weights: [-4.8428  0.8503 -1.28    0.1232  0.1436]\n",
      "MSE loss: 86.7247\n",
      "Iteration: 138400\n",
      "Gradient: [ 3.7714 -9.0072  1.4775 -1.6563 79.3398]\n",
      "Weights: [-4.8393  0.8461 -1.2821  0.1243  0.1437]\n",
      "MSE loss: 86.6676\n",
      "Iteration: 138500\n",
      "Gradient: [  9.2422  12.4468 -31.364   42.4451 -18.4277]\n",
      "Weights: [-4.8229  0.8481 -1.2848  0.1247  0.1439]\n",
      "MSE loss: 87.3567\n",
      "Iteration: 138600\n",
      "Gradient: [ -5.7671  14.4139  37.8422  10.0446 -68.1099]\n",
      "Weights: [-4.8485  0.8604 -1.2879  0.1243  0.144 ]\n",
      "MSE loss: 86.6943\n",
      "Iteration: 138700\n",
      "Gradient: [ -2.1331   7.3591 -10.9597  57.9476  64.7436]\n",
      "Weights: [-4.8579  0.8571 -1.2882  0.125   0.144 ]\n",
      "MSE loss: 86.8714\n",
      "Iteration: 138800\n",
      "Gradient: [  6.4349 -13.4787  -1.0308 -63.5474  63.2444]\n",
      "Weights: [-4.8359  0.8516 -1.2878  0.1246  0.144 ]\n",
      "MSE loss: 86.666\n",
      "Iteration: 138900\n",
      "Gradient: [  1.397   -5.8917   2.2844  39.4886 104.3641]\n",
      "Weights: [-4.8438  0.8551 -1.2876  0.1244  0.1441]\n",
      "MSE loss: 86.6553\n",
      "Iteration: 139000\n",
      "Gradient: [-16.8453   5.9706   9.7882 -70.5811 -78.326 ]\n",
      "Weights: [-4.8471  0.8619 -1.2881  0.1242  0.1441]\n",
      "MSE loss: 86.7528\n",
      "Iteration: 139100\n",
      "Gradient: [   7.2033  -10.8989   -7.3174  -15.9438 -347.6472]\n",
      "Weights: [-4.8364  0.8494 -1.2871  0.1244  0.1441]\n",
      "MSE loss: 86.6535\n",
      "Iteration: 139200\n",
      "Gradient: [-1.5149 -5.3786 30.0167 41.3978 56.9686]\n",
      "Weights: [-4.8195  0.8381 -1.2872  0.1254  0.1441]\n",
      "MSE loss: 86.8078\n",
      "Iteration: 139300\n",
      "Gradient: [ -8.34     0.6618  -2.1859  50.2861 267.5767]\n",
      "Weights: [-4.8388  0.8435 -1.2867  0.1254  0.144 ]\n",
      "MSE loss: 86.6578\n",
      "Iteration: 139400\n",
      "Gradient: [  5.1632  -4.7546 -14.0651  44.5204  40.5863]\n",
      "Weights: [-4.844   0.847  -1.2859  0.1255  0.1438]\n",
      "MSE loss: 86.6671\n",
      "Iteration: 139500\n",
      "Gradient: [  5.5802   1.5479   1.2453 -72.8497 368.7724]\n",
      "Weights: [-4.8345  0.8526 -1.2864  0.1251  0.1438]\n",
      "MSE loss: 86.8818\n",
      "Iteration: 139600\n",
      "Gradient: [  4.0277 -12.7616  -8.5451  58.346  -17.3446]\n",
      "Weights: [-4.8366  0.8482 -1.2832  0.1246  0.1436]\n",
      "MSE loss: 86.7001\n",
      "Iteration: 139700\n",
      "Gradient: [ -8.5288   4.5456   4.6518  11.0894 152.2843]\n",
      "Weights: [-4.8336  0.8357 -1.2825  0.1256  0.1437]\n",
      "MSE loss: 86.645\n",
      "Iteration: 139800\n",
      "Gradient: [  10.179   -16.9094   17.5643   78.5902 -109.8286]\n",
      "Weights: [-4.8315  0.8395 -1.2835  0.1264  0.1435]\n",
      "MSE loss: 86.6866\n",
      "Iteration: 139900\n",
      "Gradient: [  -6.9489   -4.2977   40.335  -184.5487  118.5175]\n",
      "Weights: [-4.8292  0.8276 -1.2832  0.1266  0.1437]\n",
      "MSE loss: 86.6985\n",
      "Iteration: 140000\n",
      "Gradient: [  5.7779   3.5651 -15.3236  28.0467 -22.5383]\n",
      "Weights: [-4.8118  0.8234 -1.2829  0.1272  0.1436]\n",
      "MSE loss: 86.9098\n",
      "Iteration: 140100\n",
      "Gradient: [  -6.86    -15.0283   -4.0353  -63.5382 -107.8384]\n",
      "Weights: [-4.8373  0.8351 -1.2842  0.1262  0.1436]\n",
      "MSE loss: 86.7706\n",
      "Iteration: 140200\n",
      "Gradient: [ -6.8401  13.039  -32.4739 -59.8    243.4703]\n",
      "Weights: [-4.8407  0.8402 -1.2844  0.1265  0.1434]\n",
      "MSE loss: 86.6672\n",
      "Iteration: 140300\n",
      "Gradient: [  13.027    -1.1867    6.4425   -6.8213 -197.4351]\n",
      "Weights: [-4.8214  0.8403 -1.2867  0.1263  0.1434]\n",
      "MSE loss: 86.8063\n",
      "Iteration: 140400\n",
      "Gradient: [-10.0008  -5.3536   8.3853  46.8421 -20.8187]\n",
      "Weights: [-4.8469  0.8488 -1.2903  0.1279  0.1433]\n",
      "MSE loss: 86.733\n",
      "Iteration: 140500\n",
      "Gradient: [  9.9057  11.9878 -21.7864 146.5523 270.1477]\n",
      "Weights: [-4.8368  0.8488 -1.2923  0.1295  0.1433]\n",
      "MSE loss: 86.7343\n",
      "Iteration: 140600\n",
      "Gradient: [  2.8527  -4.2947  32.3504 105.3184  -8.2355]\n",
      "Weights: [-4.842   0.8538 -1.2934  0.1282  0.1434]\n",
      "MSE loss: 86.5979\n",
      "Iteration: 140700\n",
      "Gradient: [  1.7018  13.2995 -26.0381 122.3658  95.3234]\n",
      "Weights: [-4.8474  0.8545 -1.2943  0.1273  0.1438]\n",
      "MSE loss: 86.7478\n",
      "Iteration: 140800\n",
      "Gradient: [   5.9453   21.4842  -32.4913   16.8362 -145.3805]\n",
      "Weights: [-4.8418  0.8586 -1.295   0.1275  0.1437]\n",
      "MSE loss: 86.5961\n",
      "Iteration: 140900\n",
      "Gradient: [  0.5578 -21.8024 -12.014    7.2758 -74.1822]\n",
      "Weights: [-4.8325  0.8501 -1.2949  0.1275  0.1437]\n",
      "MSE loss: 86.7126\n",
      "Iteration: 141000\n",
      "Gradient: [12.6352  6.5229  3.1502 37.5311 23.0351]\n",
      "Weights: [-4.8222  0.8485 -1.2953  0.1277  0.1438]\n",
      "MSE loss: 86.7529\n",
      "Iteration: 141100\n",
      "Gradient: [ -4.6439   5.2136  43.1739 111.422   56.4031]\n",
      "Weights: [-4.8476  0.8641 -1.2972  0.1282  0.1439]\n",
      "MSE loss: 86.7485\n",
      "Iteration: 141200\n",
      "Gradient: [ -9.6451  -4.2226  -0.9699 -85.2516 358.006 ]\n",
      "Weights: [-4.8352  0.8599 -1.2982  0.1275  0.1437]\n",
      "MSE loss: 86.6967\n",
      "Iteration: 141300\n",
      "Gradient: [  1.5785  -0.4409  -7.2847 -90.6062 -47.5732]\n",
      "Weights: [-4.8608  0.877  -1.2964  0.1273  0.1434]\n",
      "MSE loss: 86.7276\n",
      "Iteration: 141400\n",
      "Gradient: [-1.8574 14.7306 29.1477 74.044  38.9963]\n",
      "Weights: [-4.8513  0.878  -1.3009  0.1276  0.1436]\n",
      "MSE loss: 86.6516\n",
      "Iteration: 141500\n",
      "Gradient: [ -0.5713   4.1091   7.7883  23.0209 -16.0864]\n",
      "Weights: [-4.8639  0.8721 -1.2992  0.1286  0.1437]\n",
      "MSE loss: 86.8473\n",
      "Iteration: 141600\n",
      "Gradient: [  -2.7639   -2.8165  -15.3772  -28.613  -139.44  ]\n",
      "Weights: [-4.8447  0.8774 -1.3052  0.1278  0.1438]\n",
      "MSE loss: 86.7478\n",
      "Iteration: 141700\n",
      "Gradient: [  1.829   12.9245  40.68   -73.5949 121.9014]\n",
      "Weights: [-4.8338  0.8661 -1.3018  0.1282  0.1439]\n",
      "MSE loss: 86.6853\n",
      "Iteration: 141800\n",
      "Gradient: [ -9.312   16.4005   9.1039 -63.3576   0.3549]\n",
      "Weights: [-4.8485  0.8673 -1.303   0.1289  0.1438]\n",
      "MSE loss: 86.6616\n",
      "Iteration: 141900\n",
      "Gradient: [  2.2956  -8.1676  17.8939 -47.8545  -2.1958]\n",
      "Weights: [-4.8311  0.8591 -1.3035  0.1298  0.1438]\n",
      "MSE loss: 86.606\n",
      "Iteration: 142000\n",
      "Gradient: [ -5.2735   8.1444 -36.7409 -11.1319  66.6739]\n",
      "Weights: [-4.8368  0.864  -1.3077  0.1299  0.1441]\n",
      "MSE loss: 86.6129\n",
      "Iteration: 142100\n",
      "Gradient: [  4.4288  15.0195 -18.7331 -32.4971  36.0943]\n",
      "Weights: [-4.8434  0.8703 -1.3061  0.1303  0.1438]\n",
      "MSE loss: 86.6292\n",
      "Iteration: 142200\n",
      "Gradient: [   0.449    -9.133    21.5332 -108.2574 -170.764 ]\n",
      "Weights: [-4.8445  0.8734 -1.3088  0.1303  0.1437]\n",
      "MSE loss: 86.5733\n",
      "Iteration: 142300\n",
      "Gradient: [ -6.5803 -17.9375   0.4964 -35.0926 -98.3501]\n",
      "Weights: [-4.845   0.8668 -1.3073  0.1312  0.1438]\n",
      "MSE loss: 86.596\n",
      "Iteration: 142400\n",
      "Gradient: [  -4.3609  -17.0524   -1.4813  -39.2369 -163.7209]\n",
      "Weights: [-4.8428  0.8662 -1.3086  0.1306  0.144 ]\n",
      "MSE loss: 86.613\n",
      "Iteration: 142500\n",
      "Gradient: [  0.5874   8.7652  20.4344 -16.1535 122.2971]\n",
      "Weights: [-4.8267  0.8607 -1.3071  0.1308  0.1439]\n",
      "MSE loss: 86.6987\n",
      "Iteration: 142600\n",
      "Gradient: [  -5.9402   -3.0015   11.2968  -96.7512 -120.1804]\n",
      "Weights: [-4.8386  0.8612 -1.3076  0.1305  0.144 ]\n",
      "MSE loss: 86.6385\n",
      "Iteration: 142700\n",
      "Gradient: [  -7.2768    7.7354   12.56     45.5053 -163.7923]\n",
      "Weights: [-4.8399  0.8665 -1.3056  0.1301  0.1438]\n",
      "MSE loss: 86.5708\n",
      "Iteration: 142800\n",
      "Gradient: [ -1.6857  20.5258   2.8468 -20.6184 -77.1424]\n",
      "Weights: [-4.8243  0.8676 -1.3092  0.1305  0.1441]\n",
      "MSE loss: 87.0142\n",
      "Iteration: 142900\n",
      "Gradient: [ -3.1087   2.5635 -25.2488 -89.8741 137.5682]\n",
      "Weights: [-4.8257  0.8522 -1.3092  0.1314  0.1438]\n",
      "MSE loss: 86.9442\n",
      "Iteration: 143000\n",
      "Gradient: [  21.9196  -13.2871   -1.5705  -78.5583 -163.7141]\n",
      "Weights: [-4.8262  0.8535 -1.3078  0.1326  0.1438]\n",
      "MSE loss: 86.6781\n",
      "Iteration: 143100\n",
      "Gradient: [  1.8219 -14.0359 -55.9292  61.8149 131.9581]\n",
      "Weights: [-4.8376  0.8582 -1.3069  0.132   0.1437]\n",
      "MSE loss: 86.5968\n",
      "Iteration: 143200\n",
      "Gradient: [  -6.9296  -11.1937 -112.9643   61.4779 -380.9853]\n",
      "Weights: [-4.8534  0.8547 -1.3052  0.1315  0.1437]\n",
      "MSE loss: 87.4386\n",
      "Iteration: 143300\n",
      "Gradient: [  -5.5947   13.015    -6.1089    1.9493 -423.3139]\n",
      "Weights: [-4.8425  0.8661 -1.3064  0.131   0.1434]\n",
      "MSE loss: 86.6186\n",
      "Iteration: 143400\n",
      "Gradient: [  5.4873   7.7399  66.456  -76.3524 317.2805]\n",
      "Weights: [-4.8433  0.8666 -1.3038  0.13    0.1435]\n",
      "MSE loss: 86.5769\n",
      "Iteration: 143500\n",
      "Gradient: [  1.7418  12.6516  69.6938  -0.9632 144.3638]\n",
      "Weights: [-4.8394  0.8778 -1.304   0.1298  0.1435]\n",
      "MSE loss: 87.1818\n",
      "Iteration: 143600\n",
      "Gradient: [   4.5515   -1.5442   -6.8004  -89.3337 -290.1077]\n",
      "Weights: [-4.8439  0.8802 -1.3054  0.1297  0.1432]\n",
      "MSE loss: 86.7109\n",
      "Iteration: 143700\n",
      "Gradient: [-12.1265 -22.3615 -14.6291  71.0786 -68.8271]\n",
      "Weights: [-4.8479  0.8756 -1.3071  0.1301  0.1433]\n",
      "MSE loss: 86.729\n",
      "Iteration: 143800\n",
      "Gradient: [  -5.9379   -0.8037  -24.8123   -3.948  -373.9229]\n",
      "Weights: [-4.8363  0.8685 -1.3057  0.1306  0.1436]\n",
      "MSE loss: 86.6467\n",
      "Iteration: 143900\n",
      "Gradient: [10.898  -5.2839 47.0117 -3.7621 61.8852]\n",
      "Weights: [-4.8368  0.8645 -1.3045  0.131   0.1433]\n",
      "MSE loss: 86.5672\n",
      "Iteration: 144000\n",
      "Gradient: [ 3.197200e+00  1.671700e+01 -5.074600e+01  2.460000e-02  3.968339e+02]\n",
      "Weights: [-4.8272  0.8631 -1.3058  0.1306  0.1435]\n",
      "MSE loss: 86.7044\n",
      "Iteration: 144100\n",
      "Gradient: [ -5.0358  12.9818  17.7702  26.9442 173.6244]\n",
      "Weights: [-4.8499  0.8693 -1.3061  0.1319  0.1435]\n",
      "MSE loss: 86.7288\n",
      "Iteration: 144200\n",
      "Gradient: [   1.1976   -4.5195   -3.4668  -41.5748 -111.4486]\n",
      "Weights: [-4.846   0.8753 -1.3095  0.1323  0.1432]\n",
      "MSE loss: 86.5334\n",
      "Iteration: 144300\n",
      "Gradient: [ -3.9699  -2.7758 -73.509  -15.7708 -88.2363]\n",
      "Weights: [-4.8519  0.878  -1.3103  0.1324  0.1431]\n",
      "MSE loss: 86.5566\n",
      "Iteration: 144400\n",
      "Gradient: [  -9.2776  -30.8515  -15.9999  -50.3102 -296.4112]\n",
      "Weights: [-4.8634  0.8808 -1.3092  0.1313  0.1431]\n",
      "MSE loss: 86.9982\n",
      "Iteration: 144500\n",
      "Gradient: [  -7.5213   -8.8496  -12.4386   50.9698 -105.316 ]\n",
      "Weights: [-4.856   0.891  -1.3112  0.1314  0.1432]\n",
      "MSE loss: 86.6424\n",
      "Iteration: 144600\n",
      "Gradient: [ -3.0547 -26.651  -32.9742 -63.0058 114.8433]\n",
      "Weights: [-4.8701  0.8973 -1.3124  0.1316  0.1429]\n",
      "MSE loss: 86.7843\n",
      "Iteration: 144700\n",
      "Gradient: [ -1.6983   5.8922  12.4919 -37.6701  70.4663]\n",
      "Weights: [-4.8733  0.8963 -1.312   0.1323  0.1428]\n",
      "MSE loss: 86.8183\n",
      "Iteration: 144800\n",
      "Gradient: [-14.8742   1.5833 -14.7327 -31.1682 -31.3617]\n",
      "Weights: [-4.875   0.908  -1.3132  0.1319  0.1426]\n",
      "MSE loss: 86.8568\n",
      "Iteration: 144900\n",
      "Gradient: [ -2.312  -22.4973  -9.6939  -6.2517  44.6742]\n",
      "Weights: [-4.8594  0.8823 -1.3083  0.1313  0.143 ]\n",
      "MSE loss: 86.6923\n",
      "Iteration: 145000\n",
      "Gradient: [  5.2997  17.319  -25.7385   5.5715  71.0474]\n",
      "Weights: [-4.8586  0.8837 -1.308   0.1317  0.1427]\n",
      "MSE loss: 86.6737\n",
      "Iteration: 145100\n",
      "Gradient: [ -5.5418   4.535    1.2615  -0.9807 139.1568]\n",
      "Weights: [-4.8462  0.8777 -1.3066  0.1314  0.1429]\n",
      "MSE loss: 86.6007\n",
      "Iteration: 145200\n",
      "Gradient: [   6.9749    6.0969    5.5631 -129.0701  -81.6943]\n",
      "Weights: [-4.8575  0.8823 -1.3053  0.131   0.1428]\n",
      "MSE loss: 86.6519\n",
      "Iteration: 145300\n",
      "Gradient: [  8.2245  10.0976   0.6572  67.4804 -12.3804]\n",
      "Weights: [-4.8525  0.8679 -1.303   0.1312  0.1429]\n",
      "MSE loss: 86.6681\n",
      "Iteration: 145400\n",
      "Gradient: [ -10.1518    0.7301   -5.5967  180.0938 -143.2317]\n",
      "Weights: [-4.84    0.8593 -1.3016  0.1316  0.1429]\n",
      "MSE loss: 86.5632\n",
      "Iteration: 145500\n",
      "Gradient: [ 1.141   3.853  19.1709 51.3082 15.3738]\n",
      "Weights: [-4.8344  0.8633 -1.3019  0.1317  0.1429]\n",
      "MSE loss: 86.7044\n",
      "Iteration: 145600\n",
      "Gradient: [-1.378720e+01  2.996000e-01 -9.166800e+00  4.149410e+01 -4.526231e+02]\n",
      "Weights: [-4.8548  0.8618 -1.3058  0.1328  0.143 ]\n",
      "MSE loss: 86.9963\n",
      "Iteration: 145700\n",
      "Gradient: [ -1.712   -4.816    7.7817  70.3736 232.1908]\n",
      "Weights: [-4.8339  0.8567 -1.3037  0.133   0.1429]\n",
      "MSE loss: 86.5512\n",
      "Iteration: 145800\n",
      "Gradient: [  -7.7766  -19.3127  -19.0875  -12.03   -122.5234]\n",
      "Weights: [-4.8542  0.8601 -1.3043  0.1337  0.1429]\n",
      "MSE loss: 86.7606\n",
      "Iteration: 145900\n",
      "Gradient: [  -2.8034  -15.5344  -11.9156 -147.8658  139.3118]\n",
      "Weights: [-4.8509  0.8715 -1.3084  0.1336  0.1428]\n",
      "MSE loss: 86.5373\n",
      "Iteration: 146000\n",
      "Gradient: [  -2.659     0.5781  -23.4656  -58.8433 -104.6439]\n",
      "Weights: [-4.8447  0.8701 -1.3097  0.1339  0.1428]\n",
      "MSE loss: 86.4966\n",
      "Iteration: 146100\n",
      "Gradient: [  -9.0473    1.601    13.6549   54.0677 -251.8679]\n",
      "Weights: [-4.8568  0.8705 -1.3067  0.1328  0.1428]\n",
      "MSE loss: 86.7337\n",
      "Iteration: 146200\n",
      "Gradient: [  6.8425 -14.1596   6.489  -39.634   95.8407]\n",
      "Weights: [-4.8508  0.8762 -1.3046  0.1321  0.1424]\n",
      "MSE loss: 86.6119\n",
      "Iteration: 146300\n",
      "Gradient: [ -9.0264  -5.5674 -23.5502 -68.3974 -48.0735]\n",
      "Weights: [-4.8705  0.875  -1.3035  0.133   0.1424]\n",
      "MSE loss: 86.9898\n",
      "Iteration: 146400\n",
      "Gradient: [  8.2806   9.7852  -9.9214 -38.2169  14.5442]\n",
      "Weights: [-4.8444  0.8695 -1.3022  0.1323  0.1424]\n",
      "MSE loss: 86.6013\n",
      "Iteration: 146500\n",
      "Gradient: [ -8.211  -12.0778  29.5707 -10.9222   4.847 ]\n",
      "Weights: [-4.8514  0.8742 -1.3021  0.1316  0.1425]\n",
      "MSE loss: 86.6104\n",
      "Iteration: 146600\n",
      "Gradient: [  -3.4863   -3.1516  -37.5663  -33.8474 -295.1582]\n",
      "Weights: [-4.8441  0.8691 -1.3028  0.1315  0.1423]\n",
      "MSE loss: 86.8306\n",
      "Iteration: 146700\n",
      "Gradient: [  7.6473  -3.4795  23.5598 103.3031  58.1402]\n",
      "Weights: [-4.8371  0.8656 -1.3022  0.132   0.1426]\n",
      "MSE loss: 86.6599\n",
      "Iteration: 146800\n",
      "Gradient: [   2.2606    6.7506  -39.2906  106.057  -264.6859]\n",
      "Weights: [-4.8326  0.8556 -1.3019  0.1326  0.1428]\n",
      "MSE loss: 86.5606\n",
      "Iteration: 146900\n",
      "Gradient: [  5.8689  21.6003  32.9262 -16.4026 -93.3471]\n",
      "Weights: [-4.8366  0.8643 -1.3031  0.1319  0.1427]\n",
      "MSE loss: 86.5876\n",
      "Iteration: 147000\n",
      "Gradient: [ -4.7933 -10.8198  -3.7699  62.113   43.6847]\n",
      "Weights: [-4.8502  0.8746 -1.304   0.1322  0.1428]\n",
      "MSE loss: 86.6764\n",
      "Iteration: 147100\n",
      "Gradient: [ -1.1455   5.684   10.9913  49.4706 223.3191]\n",
      "Weights: [-4.8379  0.8721 -1.304   0.1324  0.1427]\n",
      "MSE loss: 86.9623\n",
      "Iteration: 147200\n",
      "Gradient: [ -1.644   -3.5324 -12.269   39.717   20.2768]\n",
      "Weights: [-4.8443  0.8711 -1.3065  0.132   0.1428]\n",
      "MSE loss: 86.5677\n",
      "Iteration: 147300\n",
      "Gradient: [ -2.23    -7.7083  18.0311  56.7047 146.2386]\n",
      "Weights: [-4.8348  0.8595 -1.3073  0.1332  0.143 ]\n",
      "MSE loss: 86.5409\n",
      "Iteration: 147400\n",
      "Gradient: [ -4.9144 -12.8163 -21.8161  27.1308  23.9696]\n",
      "Weights: [-4.8324  0.8556 -1.3048  0.1339  0.1425]\n",
      "MSE loss: 86.5415\n",
      "Iteration: 147500\n",
      "Gradient: [ -6.1906  11.4803 -15.7863  28.6342 -69.1143]\n",
      "Weights: [-4.829   0.85   -1.3037  0.1351  0.1425]\n",
      "MSE loss: 86.568\n",
      "Iteration: 147600\n",
      "Gradient: [  -1.0565   -0.7265  -42.7461  -91.0943 -234.9809]\n",
      "Weights: [-4.8365  0.8529 -1.3018  0.1343  0.1423]\n",
      "MSE loss: 86.4976\n",
      "Iteration: 147700\n",
      "Gradient: [ -4.2617   4.4301  -4.1671 -13.354  -32.0326]\n",
      "Weights: [-4.8533  0.8691 -1.3043  0.134   0.1422]\n",
      "MSE loss: 86.5698\n",
      "Iteration: 147800\n",
      "Gradient: [ -0.6098   2.2398  16.3404 -45.4631 141.8083]\n",
      "Weights: [-4.8407  0.8649 -1.3053  0.1339  0.1423]\n",
      "MSE loss: 86.5263\n",
      "Iteration: 147900\n",
      "Gradient: [ 6.2502  6.4177 34.5541 83.7753 -1.2035]\n",
      "Weights: [-4.8437  0.8691 -1.3075  0.1344  0.1426]\n",
      "MSE loss: 86.6036\n",
      "Iteration: 148000\n",
      "Gradient: [   6.3572   -0.691   -12.5376  -39.4357 -265.0395]\n",
      "Weights: [-4.8453  0.871  -1.3095  0.1338  0.1425]\n",
      "MSE loss: 86.5926\n",
      "Iteration: 148100\n",
      "Gradient: [  -9.5892   -3.5616    3.2769  -29.812  -130.8633]\n",
      "Weights: [-4.8632  0.8822 -1.3103  0.134   0.1424]\n",
      "MSE loss: 86.7056\n",
      "Iteration: 148200\n",
      "Gradient: [  -4.6192   -1.3793  -12.3882  -12.8592 -263.8705]\n",
      "Weights: [-4.8383  0.8791 -1.3131  0.1338  0.1422]\n",
      "MSE loss: 86.9377\n",
      "Iteration: 148300\n",
      "Gradient: [ -3.5444  -3.6733 -21.6746 -49.1759  84.8046]\n",
      "Weights: [-4.8457  0.8764 -1.3146  0.1346  0.1427]\n",
      "MSE loss: 86.5425\n",
      "Iteration: 148400\n",
      "Gradient: [  -4.542    -5.8611   39.0458  -30.092  -147.4947]\n",
      "Weights: [-4.8408  0.8756 -1.3151  0.1344  0.143 ]\n",
      "MSE loss: 86.4982\n",
      "Iteration: 148500\n",
      "Gradient: [  -2.8237   12.5515    0.3129  -34.5773 -254.8539]\n",
      "Weights: [-4.8357  0.8753 -1.3152  0.1349  0.143 ]\n",
      "MSE loss: 86.6901\n",
      "Iteration: 148600\n",
      "Gradient: [  11.2678    9.6687   28.2983  -42.7781 -298.7796]\n",
      "Weights: [-4.8301  0.8656 -1.3178  0.1354  0.1433]\n",
      "MSE loss: 86.5567\n",
      "Iteration: 148700\n",
      "Gradient: [ -1.9893 -17.8618   3.4809  94.5231 -90.1347]\n",
      "Weights: [-4.841   0.8691 -1.3203  0.1358  0.1434]\n",
      "MSE loss: 86.6867\n",
      "Iteration: 148800\n",
      "Gradient: [ -3.9719   6.5023 -11.7561 -68.3085 -73.9416]\n",
      "Weights: [-4.8353  0.8706 -1.322   0.1366  0.1433]\n",
      "MSE loss: 86.5335\n",
      "Iteration: 148900\n",
      "Gradient: [ -0.8832  15.3255  19.747   47.3207 219.5514]\n",
      "Weights: [-4.8202  0.865  -1.3197  0.1372  0.143 ]\n",
      "MSE loss: 86.8368\n",
      "Iteration: 149000\n",
      "Gradient: [ 14.3115  -2.5993 -27.4735 -24.1384  48.4445]\n",
      "Weights: [-4.8341  0.8662 -1.3187  0.1379  0.1428]\n",
      "MSE loss: 86.5519\n",
      "Iteration: 149100\n",
      "Gradient: [   0.2995  -20.0496  -33.9333   79.6001 -160.4522]\n",
      "Weights: [-4.8267  0.8663 -1.32    0.1378  0.1425]\n",
      "MSE loss: 86.574\n",
      "Iteration: 149200\n",
      "Gradient: [  3.6644  -4.6851 -11.4675 -58.5357  63.4101]\n",
      "Weights: [-4.839   0.8656 -1.3195  0.1386  0.1425]\n",
      "MSE loss: 86.4998\n",
      "Iteration: 149300\n",
      "Gradient: [  -2.3198   -1.2304   34.6972  -24.2675 -305.3225]\n",
      "Weights: [-4.8216  0.8581 -1.3177  0.1385  0.1426]\n",
      "MSE loss: 86.6579\n",
      "Iteration: 149400\n",
      "Gradient: [ -2.8618   6.577  -44.4256  51.9853 116.4156]\n",
      "Weights: [-4.8167  0.8562 -1.3179  0.1386  0.1424]\n",
      "MSE loss: 86.6697\n",
      "Iteration: 149500\n",
      "Gradient: [  8.3725  -9.2255  39.1681 -40.1556  14.2438]\n",
      "Weights: [-4.8322  0.8621 -1.3175  0.1388  0.1426]\n",
      "MSE loss: 86.6288\n",
      "Iteration: 149600\n",
      "Gradient: [ -5.854   -5.6284  59.4858 -24.5361  63.6293]\n",
      "Weights: [-4.8345  0.8666 -1.3172  0.1383  0.1424]\n",
      "MSE loss: 86.5052\n",
      "Iteration: 149700\n",
      "Gradient: [ 5.058000e+00  4.390000e-02 -3.621660e+01  7.790550e+01  2.686712e+02]\n",
      "Weights: [-4.8354  0.8629 -1.317   0.1385  0.1424]\n",
      "MSE loss: 86.4767\n",
      "Iteration: 149800\n",
      "Gradient: [  4.8674   4.0416  11.4482  27.6427 -90.0953]\n",
      "Weights: [-4.8466  0.8708 -1.3179  0.1384  0.1423]\n",
      "MSE loss: 86.4821\n",
      "Iteration: 149900\n",
      "Gradient: [  9.5915  -2.3205 -26.1945 -19.88    56.8289]\n",
      "Weights: [-4.8447  0.8718 -1.3191  0.1383  0.1424]\n",
      "MSE loss: 86.4708\n",
      "Iteration: 150000\n",
      "Gradient: [ 7.3775  5.9035 36.0506 38.0745 11.9663]\n",
      "Weights: [-4.8536  0.8812 -1.3205  0.138   0.1424]\n",
      "MSE loss: 86.4907\n",
      "Iteration: 150100\n",
      "Gradient: [  -4.1384    9.8975   11.3565   24.3384 -126.5244]\n",
      "Weights: [-4.8473  0.8755 -1.3225  0.1393  0.1424]\n",
      "MSE loss: 86.4757\n",
      "Iteration: 150200\n",
      "Gradient: [  2.6512   7.8004 -16.125   37.7806 255.8045]\n",
      "Weights: [-4.85    0.874  -1.3239  0.1398  0.1425]\n",
      "MSE loss: 86.6045\n",
      "Iteration: 150300\n",
      "Gradient: [ 7.7078  0.6508 11.9866 67.6086  9.8724]\n",
      "Weights: [-4.8301  0.8742 -1.3244  0.1394  0.1425]\n",
      "MSE loss: 86.5633\n",
      "Iteration: 150400\n",
      "Gradient: [ -0.4789  -2.6369   7.6971 102.8393 153.9649]\n",
      "Weights: [-4.8538  0.8804 -1.3231  0.1389  0.1425]\n",
      "MSE loss: 86.5376\n",
      "Iteration: 150500\n",
      "Gradient: [ 2.650000e-02 -2.518700e+00  5.295800e+00 -2.056740e+01  2.311493e+02]\n",
      "Weights: [-4.8491  0.8815 -1.3227  0.1386  0.1421]\n",
      "MSE loss: 86.5432\n",
      "Iteration: 150600\n",
      "Gradient: [  0.4419  10.3591  -1.0649  94.1935 221.7223]\n",
      "Weights: [-4.8452  0.8833 -1.3218  0.1385  0.1422]\n",
      "MSE loss: 86.4493\n",
      "Iteration: 150700\n",
      "Gradient: [  3.0345  -7.2095  -5.5463 121.4264 -39.3522]\n",
      "Weights: [-4.8422  0.8816 -1.3231  0.139   0.1422]\n",
      "MSE loss: 86.444\n",
      "Iteration: 150800\n",
      "Gradient: [  2.1047   2.2168  -1.4347  16.6793 -84.0021]\n",
      "Weights: [-4.8543  0.8946 -1.3224  0.1388  0.1421]\n",
      "MSE loss: 86.8763\n",
      "Iteration: 150900\n",
      "Gradient: [  9.3899  37.7256   7.0488 -19.6536 287.8551]\n",
      "Weights: [-4.8536  0.8991 -1.3238  0.1386  0.1422]\n",
      "MSE loss: 87.0544\n",
      "Iteration: 151000\n",
      "Gradient: [ -6.2254  -8.7206  -8.2738  -6.2383 233.8314]\n",
      "Weights: [-4.853   0.8856 -1.3213  0.1385  0.1421]\n",
      "MSE loss: 86.4469\n",
      "Iteration: 151100\n",
      "Gradient: [ -3.0218  -1.114   30.3493  23.9706 179.2982]\n",
      "Weights: [-4.836   0.8713 -1.3219  0.1395  0.1422]\n",
      "MSE loss: 86.4336\n",
      "Iteration: 151200\n",
      "Gradient: [  -2.5496    2.671    -2.7678    7.8846 -227.1162]\n",
      "Weights: [-4.8523  0.8719 -1.3205  0.1398  0.1421]\n",
      "MSE loss: 86.6077\n",
      "Iteration: 151300\n",
      "Gradient: [ -3.3142  -7.2955  16.4861 -19.0752 194.3774]\n",
      "Weights: [-4.8401  0.8765 -1.3225  0.1399  0.1422]\n",
      "MSE loss: 86.4973\n",
      "Iteration: 151400\n",
      "Gradient: [  -9.7526    2.5595   -8.0079 -126.4656  -91.106 ]\n",
      "Weights: [-4.8482  0.8792 -1.3233  0.1392  0.1421]\n",
      "MSE loss: 86.5702\n",
      "Iteration: 151500\n",
      "Gradient: [ -5.7436  12.8731  26.7703 -23.2231 312.5564]\n",
      "Weights: [-4.845   0.8799 -1.3218  0.1397  0.1419]\n",
      "MSE loss: 86.4251\n",
      "Iteration: 151600\n",
      "Gradient: [ 11.9761 -18.4357 -13.7671 -55.9768 -25.4065]\n",
      "Weights: [-4.8387  0.8763 -1.3225  0.1398  0.1419]\n",
      "MSE loss: 86.4236\n",
      "Iteration: 151700\n",
      "Gradient: [  2.7596   6.8841 -63.5363 -54.8469 -79.6299]\n",
      "Weights: [-4.8415  0.8809 -1.3245  0.1399  0.142 ]\n",
      "MSE loss: 86.4151\n",
      "Iteration: 151800\n",
      "Gradient: [ -8.5661   5.1488  13.6254  62.8129 135.9934]\n",
      "Weights: [-4.8564  0.8779 -1.3211  0.1397  0.142 ]\n",
      "MSE loss: 86.5859\n",
      "Iteration: 151900\n",
      "Gradient: [  3.1325  22.7488  41.4488 -64.6028 -13.162 ]\n",
      "Weights: [-4.8298  0.8764 -1.3201  0.1392  0.1419]\n",
      "MSE loss: 86.7854\n",
      "Iteration: 152000\n",
      "Gradient: [ -5.9141   2.2508 -31.203   17.7669 -34.866 ]\n",
      "Weights: [-4.8483  0.8773 -1.3194  0.139   0.142 ]\n",
      "MSE loss: 86.4303\n",
      "Iteration: 152100\n",
      "Gradient: [ 12.5716  19.0343  24.2288 -36.3141 178.5652]\n",
      "Weights: [-4.8373  0.8825 -1.3226  0.1398  0.1419]\n",
      "MSE loss: 86.6803\n",
      "Iteration: 152200\n",
      "Gradient: [ -3.5674  -8.4134  -7.8115 -27.609  -46.7606]\n",
      "Weights: [-4.8588  0.8808 -1.3227  0.1403  0.1417]\n",
      "MSE loss: 86.7003\n",
      "Iteration: 152300\n",
      "Gradient: [-13.1823 -24.6708 -15.1826 -32.2341 -14.2108]\n",
      "Weights: [-4.8665  0.8915 -1.3248  0.14    0.1418]\n",
      "MSE loss: 86.6919\n",
      "Iteration: 152400\n",
      "Gradient: [ 9.7304  3.102  29.6822 54.3047 52.2523]\n",
      "Weights: [-4.8428  0.8866 -1.3239  0.1398  0.1418]\n",
      "MSE loss: 86.5282\n",
      "Iteration: 152500\n",
      "Gradient: [  -6.0986   -7.5367   -6.1966  -71.7387 -115.9705]\n",
      "Weights: [-4.8497  0.8824 -1.3243  0.14    0.1419]\n",
      "MSE loss: 86.4361\n",
      "Iteration: 152600\n",
      "Gradient: [  -3.9541  -14.8247  -10.7524   16.4734 -140.1091]\n",
      "Weights: [-4.8421  0.8716 -1.3231  0.1407  0.142 ]\n",
      "MSE loss: 86.4295\n",
      "Iteration: 152700\n",
      "Gradient: [  -1.3463   -1.187   -54.2665  129.8972 -265.5617]\n",
      "Weights: [-4.8458  0.874  -1.3241  0.1412  0.142 ]\n",
      "MSE loss: 86.4531\n",
      "Iteration: 152800\n",
      "Gradient: [ 15.8386   5.4627  -9.0119 122.8299 -30.1856]\n",
      "Weights: [-4.8373  0.882  -1.3215  0.1404  0.1415]\n",
      "MSE loss: 86.7198\n",
      "Iteration: 152900\n",
      "Gradient: [  4.2838  29.2804  57.3617 107.9002 197.9147]\n",
      "Weights: [-4.852   0.8869 -1.3205  0.1401  0.1414]\n",
      "MSE loss: 86.4985\n",
      "Iteration: 153000\n",
      "Gradient: [  4.8216  -6.8488  -3.7385 -46.9167  88.4574]\n",
      "Weights: [-4.8489  0.8863 -1.3237  0.1407  0.1413]\n",
      "MSE loss: 86.4447\n",
      "Iteration: 153100\n",
      "Gradient: [16.7125  9.2014 55.6885 73.3245 24.3058]\n",
      "Weights: [-4.8437  0.8852 -1.3204  0.1408  0.1411]\n",
      "MSE loss: 86.6665\n",
      "Iteration: 153200\n",
      "Gradient: [  3.5798   5.4899  -2.013  -57.0633 -43.2473]\n",
      "Weights: [-4.8575  0.8855 -1.3223  0.142   0.1411]\n",
      "MSE loss: 86.4531\n",
      "Iteration: 153300\n",
      "Gradient: [ -4.7118  -5.5122 -20.689  -12.6954 -85.9253]\n",
      "Weights: [-4.8534  0.8805 -1.3235  0.1418  0.1412]\n",
      "MSE loss: 86.5237\n",
      "Iteration: 153400\n",
      "Gradient: [ -2.2864  -8.9741  33.7118  11.3648 -66.1066]\n",
      "Weights: [-4.8347  0.8813 -1.3232  0.1411  0.1411]\n",
      "MSE loss: 86.6093\n",
      "Iteration: 153500\n",
      "Gradient: [   9.2657   10.9709   42.4827   12.1234 -148.9288]\n",
      "Weights: [-4.8453  0.877  -1.3214  0.1418  0.1412]\n",
      "MSE loss: 86.3834\n",
      "Iteration: 153600\n",
      "Gradient: [ 4.088   5.1294 11.39   41.9618 32.2815]\n",
      "Weights: [-4.8434  0.8764 -1.3229  0.1414  0.1414]\n",
      "MSE loss: 86.3912\n",
      "Iteration: 153700\n",
      "Gradient: [  10.2714   25.5139    5.0089  -64.5069 -207.1371]\n",
      "Weights: [-4.8367  0.8749 -1.3229  0.1419  0.1413]\n",
      "MSE loss: 86.4215\n",
      "Iteration: 153800\n",
      "Gradient: [  -4.9552   -8.2797  -53.0958  -61.0861 -303.5555]\n",
      "Weights: [-4.8467  0.8722 -1.324   0.1428  0.1412]\n",
      "MSE loss: 86.5128\n",
      "Iteration: 153900\n",
      "Gradient: [ -3.007  -11.0982 -39.7902  67.0338 106.3115]\n",
      "Weights: [-4.8514  0.8755 -1.3256  0.1431  0.1412]\n",
      "MSE loss: 86.5566\n",
      "Iteration: 154000\n",
      "Gradient: [  2.2751  13.7564  21.6574 -33.9037 193.9102]\n",
      "Weights: [-4.8272  0.8751 -1.3249  0.1433  0.141 ]\n",
      "MSE loss: 86.6817\n",
      "Iteration: 154100\n",
      "Gradient: [  -5.9028   -8.2495   -2.8002  -44.7995 -211.8712]\n",
      "Weights: [-4.833   0.8658 -1.3264  0.1446  0.1411]\n",
      "MSE loss: 86.3764\n",
      "Iteration: 154200\n",
      "Gradient: [ 13.5511   4.0149 -40.7199 -59.6022  34.6475]\n",
      "Weights: [-4.8365  0.8687 -1.3241  0.1439  0.1413]\n",
      "MSE loss: 86.4434\n",
      "Iteration: 154300\n",
      "Gradient: [ 12.9394  -3.4356 -26.5677  -5.5661 219.7624]\n",
      "Weights: [-4.8207  0.876  -1.3271  0.143   0.1413]\n",
      "MSE loss: 86.9547\n",
      "Iteration: 154400\n",
      "Gradient: [ -4.5854   5.2197  43.5478  31.4497 257.625 ]\n",
      "Weights: [-4.8245  0.8746 -1.3261  0.1428  0.1413]\n",
      "MSE loss: 86.7297\n",
      "Iteration: 154500\n",
      "Gradient: [   7.0771  -14.7191    4.1159 -148.8148 -185.5657]\n",
      "Weights: [-4.8585  0.8898 -1.3284  0.1429  0.1413]\n",
      "MSE loss: 86.4366\n",
      "Iteration: 154600\n",
      "Gradient: [ -7.162    0.7835  -4.005  -12.4586 102.9919]\n",
      "Weights: [-4.8302  0.8792 -1.3303  0.1434  0.1414]\n",
      "MSE loss: 86.4907\n",
      "Iteration: 154700\n",
      "Gradient: [ 9.840000e-02 -4.027000e-01  2.858100e+01 -9.372590e+01 -1.034251e+02]\n",
      "Weights: [-4.8464  0.8876 -1.3304  0.1429  0.1417]\n",
      "MSE loss: 86.4177\n",
      "Iteration: 154800\n",
      "Gradient: [   4.1022  -18.2394  -25.6918  -42.5823 -506.5431]\n",
      "Weights: [-4.8215  0.8699 -1.3315  0.1435  0.1416]\n",
      "MSE loss: 86.6319\n",
      "Iteration: 154900\n",
      "Gradient: [   5.3091   23.6731   42.71    131.0489 -183.2117]\n",
      "Weights: [-4.8392  0.8802 -1.3314  0.1435  0.1415]\n",
      "MSE loss: 86.3695\n",
      "Iteration: 155000\n",
      "Gradient: [   4.076    12.2101  -11.3419  -54.6907 -114.0864]\n",
      "Weights: [-4.8407  0.8785 -1.3318  0.1442  0.1415]\n",
      "MSE loss: 86.3631\n",
      "Iteration: 155100\n",
      "Gradient: [ 13.7667   5.1423   5.2562 -87.9749 239.0283]\n",
      "Weights: [-4.8357  0.8811 -1.3318  0.1448  0.1414]\n",
      "MSE loss: 86.4941\n",
      "Iteration: 155200\n",
      "Gradient: [ -3.8346  -2.4612   4.2762 -24.8732 300.6552]\n",
      "Weights: [-4.8444  0.8895 -1.3361  0.1448  0.1414]\n",
      "MSE loss: 86.3341\n",
      "Iteration: 155300\n",
      "Gradient: [ -0.4609   5.2084 -15.9201  30.3094 -55.3081]\n",
      "Weights: [-4.8394  0.8888 -1.3349  0.145   0.1412]\n",
      "MSE loss: 86.39\n",
      "Iteration: 155400\n",
      "Gradient: [  -9.4852    3.8748   -8.296    59.541  -147.7806]\n",
      "Weights: [-4.8508  0.8944 -1.338   0.1451  0.1414]\n",
      "MSE loss: 86.3383\n",
      "Iteration: 155500\n",
      "Gradient: [   3.7271   -3.2759   13.0896  143.076  -248.9983]\n",
      "Weights: [-4.8252  0.8868 -1.3367  0.1447  0.1413]\n",
      "MSE loss: 86.6997\n",
      "Iteration: 155600\n",
      "Gradient: [ -2.7737  -4.137  -21.39   -43.5319 247.6142]\n",
      "Weights: [-4.8445  0.8869 -1.3367  0.1453  0.1414]\n",
      "MSE loss: 86.3358\n",
      "Iteration: 155700\n",
      "Gradient: [  0.2268   0.965    6.8808 -85.4496  24.9131]\n",
      "Weights: [-4.8449  0.8895 -1.3366  0.1445  0.1414]\n",
      "MSE loss: 86.3776\n",
      "Iteration: 155800\n",
      "Gradient: [  -3.8829  -30.7904   14.9821   57.3133 -171.6739]\n",
      "Weights: [-4.8478  0.8882 -1.3323  0.1439  0.1414]\n",
      "MSE loss: 86.3444\n",
      "Iteration: 155900\n",
      "Gradient: [ 9.117  -4.6457 10.091  16.3685 22.3277]\n",
      "Weights: [-4.8446  0.8929 -1.3367  0.1446  0.1413]\n",
      "MSE loss: 86.3479\n",
      "Iteration: 156000\n",
      "Gradient: [ -7.1378   7.036   -6.4776 -33.0187 -71.5556]\n",
      "Weights: [-4.8568  0.8921 -1.3344  0.1449  0.1414]\n",
      "MSE loss: 86.4273\n",
      "Iteration: 156100\n",
      "Gradient: [  -2.157     5.9186   29.6991  -54.2716 -150.8527]\n",
      "Weights: [-4.8304  0.8829 -1.336   0.1451  0.1414]\n",
      "MSE loss: 86.4663\n",
      "Iteration: 156200\n",
      "Gradient: [  4.5608  -7.0208 -16.7584 -45.9338 148.1602]\n",
      "Weights: [-4.8224  0.8784 -1.3373  0.1456  0.1414]\n",
      "MSE loss: 86.5735\n",
      "Iteration: 156300\n",
      "Gradient: [ -6.4114  -3.9251  -8.8911 -53.9309 152.291 ]\n",
      "Weights: [-4.8398  0.8844 -1.3372  0.1453  0.1414]\n",
      "MSE loss: 86.3999\n",
      "Iteration: 156400\n",
      "Gradient: [ -5.5525  -1.5549 -15.3568  21.6688   7.8905]\n",
      "Weights: [-4.8502  0.8889 -1.3361  0.1451  0.1414]\n",
      "MSE loss: 86.374\n",
      "Iteration: 156500\n",
      "Gradient: [ -1.8619   2.7714  11.1061 -78.9614 -15.3199]\n",
      "Weights: [-4.8478  0.8851 -1.3344  0.1452  0.1413]\n",
      "MSE loss: 86.3571\n",
      "Iteration: 156600\n",
      "Gradient: [-10.4733   9.6792 -15.6699 -55.7042 -37.6321]\n",
      "Weights: [-4.853   0.8879 -1.3356  0.1453  0.1413]\n",
      "MSE loss: 86.4286\n",
      "Iteration: 156700\n",
      "Gradient: [ -17.732    -5.7795  -14.2741 -117.3856   82.4809]\n",
      "Weights: [-4.8586  0.886  -1.3353  0.1455  0.1412]\n",
      "MSE loss: 86.8073\n",
      "Iteration: 156800\n",
      "Gradient: [   0.6452   14.0175   26.1972  -10.9371 -439.3721]\n",
      "Weights: [-4.8479  0.8899 -1.3383  0.1468  0.1409]\n",
      "MSE loss: 86.3172\n",
      "Iteration: 156900\n",
      "Gradient: [  7.0641  -2.7635 -13.1608  25.7844 115.8814]\n",
      "Weights: [-4.8505  0.8864 -1.3361  0.1477  0.1405]\n",
      "MSE loss: 86.3613\n",
      "Iteration: 157000\n",
      "Gradient: [-10.4018  -0.3949   0.6728  52.4543 189.7002]\n",
      "Weights: [-4.8517  0.892  -1.3343  0.1471  0.1404]\n",
      "MSE loss: 86.3292\n",
      "Iteration: 157100\n",
      "Gradient: [ -5.6591 -13.1693 -44.4669 -79.7368 268.3754]\n",
      "Weights: [-4.853   0.8842 -1.3363  0.1476  0.1405]\n",
      "MSE loss: 86.6254\n",
      "Iteration: 157200\n",
      "Gradient: [ -1.6538 -10.4754  -0.8094 -64.9352  12.5083]\n",
      "Weights: [-4.8432  0.8867 -1.3353  0.1483  0.1403]\n",
      "MSE loss: 86.3383\n",
      "Iteration: 157300\n",
      "Gradient: [  5.0218   5.5277   7.1436 113.8057 253.4802]\n",
      "Weights: [-4.8465  0.8889 -1.3369  0.1484  0.1404]\n",
      "MSE loss: 86.3105\n",
      "Iteration: 157400\n",
      "Gradient: [-7.5673 -2.6253  4.9524  3.2816 98.3036]\n",
      "Weights: [-4.8585  0.8922 -1.3372  0.1486  0.1403]\n",
      "MSE loss: 86.3706\n",
      "Iteration: 157500\n",
      "Gradient: [ 10.2935  -5.9928  22.1115 -34.6754 125.2886]\n",
      "Weights: [-4.8479  0.8917 -1.3376  0.1483  0.1402]\n",
      "MSE loss: 86.2979\n",
      "Iteration: 157600\n",
      "Gradient: [  2.663   13.0458  14.8773  41.9896 258.5828]\n",
      "Weights: [-4.847   0.8933 -1.3366  0.1482  0.1401]\n",
      "MSE loss: 86.3346\n",
      "Iteration: 157700\n",
      "Gradient: [ -8.2187  -1.5009  30.3505  79.7002 158.5548]\n",
      "Weights: [-4.8609  0.8909 -1.3352  0.1481  0.1402]\n",
      "MSE loss: 86.4577\n",
      "Iteration: 157800\n",
      "Gradient: [  2.364   -0.7332 -27.5361  51.1266 -86.6306]\n",
      "Weights: [-4.8501  0.8911 -1.3364  0.1483  0.1401]\n",
      "MSE loss: 86.3131\n",
      "Iteration: 157900\n",
      "Gradient: [ -5.1697 -20.153    8.837   15.321  -96.7718]\n",
      "Weights: [-4.8645  0.8862 -1.3353  0.1489  0.14  ]\n",
      "MSE loss: 86.8345\n",
      "Iteration: 158000\n",
      "Gradient: [ -4.6615 -11.4084  10.3013 -36.0053 120.8943]\n",
      "Weights: [-4.8486  0.8869 -1.3358  0.1484  0.14  ]\n",
      "MSE loss: 86.354\n",
      "Iteration: 158100\n",
      "Gradient: [  -1.9745    0.4746   -8.6756  -32.0307 -295.9315]\n",
      "Weights: [-4.8501  0.8932 -1.3348  0.148   0.1399]\n",
      "MSE loss: 86.3362\n",
      "Iteration: 158200\n",
      "Gradient: [ -8.1714 -11.779  -29.1757 -68.3755  46.083 ]\n",
      "Weights: [-4.8619  0.8954 -1.3382  0.1482  0.1399]\n",
      "MSE loss: 86.8943\n",
      "Iteration: 158300\n",
      "Gradient: [   7.3743   12.2165   30.4146 -114.4152 -160.2928]\n",
      "Weights: [-4.8518  0.8943 -1.3392  0.1486  0.1402]\n",
      "MSE loss: 86.3195\n",
      "Iteration: 158400\n",
      "Gradient: [ 10.0568  17.3629 -17.6776  73.4233  19.7983]\n",
      "Weights: [-4.8469  0.8942 -1.3395  0.1495  0.1401]\n",
      "MSE loss: 86.3307\n",
      "Iteration: 158500\n",
      "Gradient: [  5.9104  16.9359  -2.9065 -46.668  192.516 ]\n",
      "Weights: [-4.8512  0.8944 -1.3407  0.1502  0.1401]\n",
      "MSE loss: 86.2949\n",
      "Iteration: 158600\n",
      "Gradient: [ -0.7753   1.2462  17.9236  35.3136 193.389 ]\n",
      "Weights: [-4.8427  0.8862 -1.3405  0.1504  0.1402]\n",
      "MSE loss: 86.2609\n",
      "Iteration: 158700\n",
      "Gradient: [  0.1524   0.0659 -11.3466  35.3779 -36.4734]\n",
      "Weights: [-4.8536  0.8897 -1.3388  0.1502  0.1402]\n",
      "MSE loss: 86.4062\n",
      "Iteration: 158800\n",
      "Gradient: [  4.0367   5.0814 -49.4218 -26.9452 100.515 ]\n",
      "Weights: [-4.8453  0.8897 -1.3401  0.1497  0.1403]\n",
      "MSE loss: 86.2879\n",
      "Iteration: 158900\n",
      "Gradient: [-12.753   -9.1177 -18.8621 -37.9639  99.4442]\n",
      "Weights: [-4.8456  0.8834 -1.3384  0.1495  0.1402]\n",
      "MSE loss: 86.3145\n",
      "Iteration: 159000\n",
      "Gradient: [  2.3481  -9.5214 -16.0615   1.9822 146.6335]\n",
      "Weights: [-4.8406  0.8839 -1.3394  0.1498  0.1401]\n",
      "MSE loss: 86.2824\n",
      "Iteration: 159100\n",
      "Gradient: [  -7.3505    5.4903  -37.32   -107.9836  -51.6421]\n",
      "Weights: [-4.8716  0.8933 -1.3384  0.1502  0.1402]\n",
      "MSE loss: 86.7682\n",
      "Iteration: 159200\n",
      "Gradient: [   2.6745    9.3717   31.152  -180.4095   41.4733]\n",
      "Weights: [-4.8451  0.8893 -1.3376  0.149   0.1401]\n",
      "MSE loss: 86.2827\n",
      "Iteration: 159300\n",
      "Gradient: [  5.7761   4.1275 -23.1342  31.2812 109.4543]\n",
      "Weights: [-4.8503  0.8925 -1.3381  0.1494  0.14  ]\n",
      "MSE loss: 86.2839\n",
      "Iteration: 159400\n",
      "Gradient: [ -0.7148  -7.4827 -12.4634  79.1052 -48.3529]\n",
      "Weights: [-4.86    0.8853 -1.3376  0.1504  0.14  ]\n",
      "MSE loss: 86.5385\n",
      "Iteration: 159500\n",
      "Gradient: [  9.3532  -8.9824   3.7594  43.1479 118.4475]\n",
      "Weights: [-4.843   0.8943 -1.3393  0.1498  0.1401]\n",
      "MSE loss: 86.5637\n",
      "Iteration: 159600\n",
      "Gradient: [  8.014    8.3984  18.9541 -57.8783 -97.0616]\n",
      "Weights: [-4.8398  0.8964 -1.3416  0.1496  0.1401]\n",
      "MSE loss: 86.454\n",
      "Iteration: 159700\n",
      "Gradient: [  1.8685 -28.4843  -4.3021 -18.6463 354.0098]\n",
      "Weights: [-4.8533  0.8932 -1.3422  0.15    0.1403]\n",
      "MSE loss: 86.2962\n",
      "Iteration: 159800\n",
      "Gradient: [ -8.2594   5.9942 -37.1921  -1.2907 -72.692 ]\n",
      "Weights: [-4.854   0.8973 -1.342   0.1493  0.1402]\n",
      "MSE loss: 86.2988\n",
      "Iteration: 159900\n",
      "Gradient: [   6.8397   19.2655    5.4675   -1.9282 -106.6394]\n",
      "Weights: [-4.8509  0.8963 -1.3433  0.1493  0.1405]\n",
      "MSE loss: 86.2692\n",
      "Iteration: 160000\n",
      "Gradient: [ -0.2237   6.9163  25.6462 -36.1917 -74.0165]\n",
      "Weights: [-4.8523  0.8998 -1.3444  0.1486  0.1406]\n",
      "MSE loss: 86.3091\n",
      "Iteration: 160100\n",
      "Gradient: [  2.6426 -10.0588 -27.0776 -70.7638 -39.0423]\n",
      "Weights: [-4.8599  0.9019 -1.3435  0.1486  0.1407]\n",
      "MSE loss: 86.3497\n",
      "Iteration: 160200\n",
      "Gradient: [  0.7454  -8.1661  -5.7277 -56.3337 -22.2224]\n",
      "Weights: [-4.839   0.891  -1.3416  0.1491  0.1405]\n",
      "MSE loss: 86.3178\n",
      "Iteration: 160300\n",
      "Gradient: [  -6.0392  -12.5468    4.0462   17.7082 -298.5647]\n",
      "Weights: [-4.8324  0.8807 -1.342   0.1498  0.1405]\n",
      "MSE loss: 86.3191\n",
      "Iteration: 160400\n",
      "Gradient: [-3.9026  7.7023 26.0161 40.5158 59.8675]\n",
      "Weights: [-4.8339  0.8775 -1.3392  0.1505  0.1402]\n",
      "MSE loss: 86.2863\n",
      "Iteration: 160500\n",
      "Gradient: [  4.0878  10.6602  15.4314 -64.1333  95.9578]\n",
      "Weights: [-4.8422  0.8834 -1.3359  0.1499  0.14  ]\n",
      "MSE loss: 86.4003\n",
      "Iteration: 160600\n",
      "Gradient: [  -0.3792   18.9675   14.5108  -54.1949 -233.2234]\n",
      "Weights: [-4.8415  0.8748 -1.3366  0.1503  0.14  ]\n",
      "MSE loss: 86.3133\n",
      "Iteration: 160700\n",
      "Gradient: [  2.1772  10.809   -9.559  -34.8575   2.289 ]\n",
      "Weights: [-4.8405  0.8771 -1.3357  0.1503  0.1399]\n",
      "MSE loss: 86.2579\n",
      "Iteration: 160800\n",
      "Gradient: [ -2.1027  11.1956  -7.1157 -91.4659  -2.6201]\n",
      "Weights: [-4.8267  0.868  -1.3357  0.1503  0.14  ]\n",
      "MSE loss: 86.3445\n",
      "Iteration: 160900\n",
      "Gradient: [   5.7372   16.3616  -17.3851   85.0407 -140.4366]\n",
      "Weights: [-4.8366  0.8663 -1.3337  0.1506  0.1401]\n",
      "MSE loss: 86.339\n",
      "Iteration: 161000\n",
      "Gradient: [-15.829  -14.316  -16.6375 -23.2842 -45.5806]\n",
      "Weights: [-4.8352  0.8584 -1.3287  0.15    0.1399]\n",
      "MSE loss: 86.3506\n",
      "Iteration: 161100\n",
      "Gradient: [   5.756   -14.9292   23.3054   84.3465 -111.784 ]\n",
      "Weights: [-4.8517  0.8843 -1.3318  0.149   0.1398]\n",
      "MSE loss: 86.3243\n",
      "Iteration: 161200\n",
      "Gradient: [  -2.7171  -10.9134  -18.5971   91.0012 -125.7781]\n",
      "Weights: [-4.8538  0.8893 -1.3328  0.1482  0.1397]\n",
      "MSE loss: 86.3638\n",
      "Iteration: 161300\n",
      "Gradient: [ -1.3847 -16.9717 -12.3229 -28.5664 -44.7861]\n",
      "Weights: [-4.862   0.8901 -1.3328  0.1482  0.1398]\n",
      "MSE loss: 86.5308\n",
      "Iteration: 161400\n",
      "Gradient: [-6.4532 20.333  15.7096  1.4081 82.1706]\n",
      "Weights: [-4.8434  0.8896 -1.3318  0.1481  0.1397]\n",
      "MSE loss: 86.4896\n",
      "Iteration: 161500\n",
      "Gradient: [   6.7157   -1.3223  -40.0515   -9.403  -213.0024]\n",
      "Weights: [-4.831   0.8791 -1.3318  0.1487  0.1396]\n",
      "MSE loss: 86.4975\n",
      "Iteration: 161600\n",
      "Gradient: [  0.4956   3.87   -16.4169 104.4611   7.0412]\n",
      "Weights: [-4.8323  0.8741 -1.33    0.1492  0.1396]\n",
      "MSE loss: 86.4323\n",
      "Iteration: 161700\n",
      "Gradient: [ -1.618    6.2801  15.9214  57.3909 174.6335]\n",
      "Weights: [-4.8396  0.8725 -1.3316  0.1504  0.1397]\n",
      "MSE loss: 86.3213\n",
      "Iteration: 161800\n",
      "Gradient: [   7.3657   -8.0431   10.7955   -8.128  -159.0387]\n",
      "Weights: [-4.8416  0.8798 -1.3333  0.1506  0.1397]\n",
      "MSE loss: 86.4586\n",
      "Iteration: 161900\n",
      "Gradient: [ -0.7273   2.2063  -0.8484 -68.02   191.4707]\n",
      "Weights: [-4.8381  0.8709 -1.3328  0.1497  0.1398]\n",
      "MSE loss: 86.3375\n",
      "Iteration: 162000\n",
      "Gradient: [ 9.653   9.6627  5.5091 60.8436 51.717 ]\n",
      "Weights: [-4.8339  0.8679 -1.329   0.1494  0.1398]\n",
      "MSE loss: 86.328\n",
      "Iteration: 162100\n",
      "Gradient: [  3.608    6.2757 -16.6463 147.0309  12.8187]\n",
      "Weights: [-4.833   0.8696 -1.3284  0.1496  0.1396]\n",
      "MSE loss: 86.3823\n",
      "Iteration: 162200\n",
      "Gradient: [   5.2703  -17.7132    2.1178   51.7966 -235.4282]\n",
      "Weights: [-4.8396  0.8728 -1.3303  0.1501  0.1396]\n",
      "MSE loss: 86.3101\n",
      "Iteration: 162300\n",
      "Gradient: [ -9.7244 -13.3465 -11.0226  63.5659 210.6434]\n",
      "Weights: [-4.843   0.8707 -1.3322  0.1504  0.1397]\n",
      "MSE loss: 86.3084\n",
      "Iteration: 162400\n",
      "Gradient: [ -0.8256  13.5677   6.6995  17.3235 322.8666]\n",
      "Weights: [-4.848   0.8717 -1.3336  0.1499  0.14  ]\n",
      "MSE loss: 86.5332\n",
      "Iteration: 162500\n",
      "Gradient: [ -0.15    -1.9543  33.5858  18.515  -64.5456]\n",
      "Weights: [-4.8223  0.8593 -1.3319  0.1509  0.1398]\n",
      "MSE loss: 86.3874\n",
      "Iteration: 162600\n",
      "Gradient: [  0.9893  13.4561  14.5773  56.1869 -31.7717]\n",
      "Weights: [-4.8243  0.8667 -1.3353  0.1517  0.1398]\n",
      "MSE loss: 86.4652\n",
      "Iteration: 162700\n",
      "Gradient: [   0.4668    8.1493  -21.3513   66.2895 -234.1644]\n",
      "Weights: [-4.833   0.8739 -1.3341  0.1499  0.1399]\n",
      "MSE loss: 86.3072\n",
      "Iteration: 162800\n",
      "Gradient: [-16.3142 -10.6727 -44.4807  27.2788 299.038 ]\n",
      "Weights: [-4.833   0.8716 -1.3349  0.1502  0.1399]\n",
      "MSE loss: 86.2892\n",
      "Iteration: 162900\n",
      "Gradient: [ -0.7246 -13.6234 -35.6224  43.423  254.5859]\n",
      "Weights: [-4.8263  0.8657 -1.3359  0.1519  0.1398]\n",
      "MSE loss: 86.3611\n",
      "Iteration: 163000\n",
      "Gradient: [ -3.3758   7.4183 -14.8012 122.4363  57.678 ]\n",
      "Weights: [-4.8302  0.8744 -1.34    0.152   0.1398]\n",
      "MSE loss: 86.2835\n",
      "Iteration: 163100\n",
      "Gradient: [ -3.1542   8.2331 -34.1496   9.2799 218.0926]\n",
      "Weights: [-4.858   0.895  -1.3398  0.1514  0.1396]\n",
      "MSE loss: 86.346\n",
      "Iteration: 163200\n",
      "Gradient: [ -0.2661 -14.6402  15.7112 -56.4689 143.3797]\n",
      "Weights: [-4.8403  0.8919 -1.3431  0.152   0.1397]\n",
      "MSE loss: 86.3536\n",
      "Iteration: 163300\n",
      "Gradient: [   5.5335   -7.062   -14.9947 -134.8422  282.2369]\n",
      "Weights: [-4.8331  0.8761 -1.3437  0.1526  0.1399]\n",
      "MSE loss: 86.3226\n",
      "Iteration: 163400\n",
      "Gradient: [ -5.5841   1.4385   8.8138 -30.9687 126.6427]\n",
      "Weights: [-4.8344  0.8737 -1.3417  0.1523  0.14  ]\n",
      "MSE loss: 86.2907\n",
      "Iteration: 163500\n",
      "Gradient: [  7.7078  12.5576  24.465  103.9145 198.0203]\n",
      "Weights: [-4.821   0.8648 -1.3364  0.1522  0.1398]\n",
      "MSE loss: 86.5287\n",
      "Iteration: 163600\n",
      "Gradient: [-10.2982  -9.7768  22.2746 -31.7663  45.7574]\n",
      "Weights: [-4.8224  0.8616 -1.3376  0.152   0.14  ]\n",
      "MSE loss: 86.3711\n",
      "Iteration: 163700\n",
      "Gradient: [ -4.5662 -10.1731   9.2232  40.0015  43.2664]\n",
      "Weights: [-4.8404  0.8687 -1.3378  0.1523  0.1399]\n",
      "MSE loss: 86.37\n",
      "Iteration: 163800\n",
      "Gradient: [ 2.848000e-01 -1.305000e-01 -1.718380e+01 -1.096920e+02  2.741729e+02]\n",
      "Weights: [-4.8387  0.8767 -1.3392  0.1516  0.1399]\n",
      "MSE loss: 86.2536\n",
      "Iteration: 163900\n",
      "Gradient: [ -0.6924  21.625   -4.084  -12.8266 -13.4518]\n",
      "Weights: [-4.8556  0.8996 -1.3406  0.1507  0.1396]\n",
      "MSE loss: 86.3104\n",
      "Iteration: 164000\n",
      "Gradient: [  -7.8751    5.6985  -14.3599   82.8555 -228.0586]\n",
      "Weights: [-4.8675  0.909  -1.3452  0.151   0.1395]\n",
      "MSE loss: 86.5242\n",
      "Iteration: 164100\n",
      "Gradient: [  7.8121   9.6111  -6.1542 -47.3806 203.3066]\n",
      "Weights: [-4.8514  0.918  -1.3498  0.1514  0.1396]\n",
      "MSE loss: 86.556\n",
      "Iteration: 164200\n",
      "Gradient: [   3.2954   13.9245   14.1445 -111.3405  207.4225]\n",
      "Weights: [-4.8624  0.9187 -1.3532  0.1521  0.1398]\n",
      "MSE loss: 86.307\n",
      "Iteration: 164300\n",
      "Gradient: [ -8.6651   1.989  -43.6832  30.2635 -87.8322]\n",
      "Weights: [-4.8525  0.9175 -1.3559  0.153   0.1397]\n",
      "MSE loss: 86.31\n",
      "Iteration: 164400\n",
      "Gradient: [ 3.8819 -8.3564 17.2126 81.5924 63.6554]\n",
      "Weights: [-4.8556  0.9183 -1.3561  0.1535  0.1398]\n",
      "MSE loss: 86.2984\n",
      "Iteration: 164500\n",
      "Gradient: [ -1.8251   5.5339   3.6805 109.2103 250.3725]\n",
      "Weights: [-4.847   0.9073 -1.3536  0.153   0.1398]\n",
      "MSE loss: 86.2412\n",
      "Iteration: 164600\n",
      "Gradient: [  3.9394   5.4969  26.7113  98.9639 237.4369]\n",
      "Weights: [-4.8593  0.902  -1.3512  0.1535  0.1402]\n",
      "MSE loss: 86.3956\n",
      "Iteration: 164700\n",
      "Gradient: [  3.8889  18.4882   2.4746 110.7595  11.6328]\n",
      "Weights: [-4.8431  0.8928 -1.3498  0.1541  0.1399]\n",
      "MSE loss: 86.2752\n",
      "Iteration: 164800\n",
      "Gradient: [  1.087    7.9908   3.544   71.4442 270.3002]\n",
      "Weights: [-4.837   0.8892 -1.3499  0.1546  0.1397]\n",
      "MSE loss: 86.2686\n",
      "Iteration: 164900\n",
      "Gradient: [  0.3938  -0.9884  16.1661  26.7542 244.6243]\n",
      "Weights: [-4.8395  0.8888 -1.3462  0.1538  0.1398]\n",
      "MSE loss: 86.4469\n",
      "Iteration: 165000\n",
      "Gradient: [   4.3408   -7.8455   -8.8911   14.3785 -163.8997]\n",
      "Weights: [-4.8497  0.8841 -1.3467  0.1541  0.1397]\n",
      "MSE loss: 86.4227\n",
      "Iteration: 165100\n",
      "Gradient: [ -1.384    1.5721  -3.5479   4.2065 271.7503]\n",
      "Weights: [-4.8405  0.8841 -1.3462  0.1541  0.1396]\n",
      "MSE loss: 86.2142\n",
      "Iteration: 165200\n",
      "Gradient: [ -5.2505   6.1543 -45.166   93.3128  13.6686]\n",
      "Weights: [-4.8394  0.8701 -1.3442  0.1542  0.1397]\n",
      "MSE loss: 86.6108\n",
      "Iteration: 165300\n",
      "Gradient: [  7.2907   9.632   -3.6973 115.5027 271.2862]\n",
      "Weights: [-4.8338  0.8774 -1.3444  0.1542  0.1396]\n",
      "MSE loss: 86.2466\n",
      "Iteration: 165400\n",
      "Gradient: [  -4.6197   16.1892  -27.7981   23.6823 -288.2679]\n",
      "Weights: [-4.8439  0.8833 -1.3442  0.1541  0.1392]\n",
      "MSE loss: 86.2754\n",
      "Iteration: 165500\n",
      "Gradient: [   1.8098   -4.3665  -21.661   -33.2972 -144.0016]\n",
      "Weights: [-4.8484  0.8836 -1.341   0.1537  0.1391]\n",
      "MSE loss: 86.2309\n",
      "Iteration: 165600\n",
      "Gradient: [ -0.4134   9.7884 -18.2503  51.4034 157.746 ]\n",
      "Weights: [-4.8257  0.8774 -1.3394  0.1534  0.1389]\n",
      "MSE loss: 86.4483\n",
      "Iteration: 165700\n",
      "Gradient: [ 1.5793  4.7495 24.512  12.7304  5.0888]\n",
      "Weights: [-4.8415  0.8772 -1.3393  0.1542  0.139 ]\n",
      "MSE loss: 86.2047\n",
      "Iteration: 165800\n",
      "Gradient: [ 1.8043 -2.1755 13.8773 29.3963 76.1404]\n",
      "Weights: [-4.8349  0.8806 -1.3403  0.1537  0.139 ]\n",
      "MSE loss: 86.2691\n",
      "Iteration: 165900\n",
      "Gradient: [ -4.7693   2.6533 -21.3048   2.8451 106.2036]\n",
      "Weights: [-4.846   0.8763 -1.3378  0.154   0.139 ]\n",
      "MSE loss: 86.2384\n",
      "Iteration: 166000\n",
      "Gradient: [ -8.2652   3.2262  38.6506  84.0869 100.2269]\n",
      "Weights: [-4.8321  0.8775 -1.3407  0.1548  0.1387]\n",
      "MSE loss: 86.26\n",
      "Iteration: 166100\n",
      "Gradient: [  1.2768   0.9426  26.9023  35.4389 213.3407]\n",
      "Weights: [-4.846   0.8876 -1.3411  0.1549  0.1387]\n",
      "MSE loss: 86.2975\n",
      "Iteration: 166200\n",
      "Gradient: [  -3.0479   11.7117  -31.1524 -102.7112 -384.5603]\n",
      "Weights: [-4.8446  0.8983 -1.3482  0.1548  0.1388]\n",
      "MSE loss: 86.2544\n",
      "Iteration: 166300\n",
      "Gradient: [ 11.0214  12.4023  18.2665 -41.7605 331.4271]\n",
      "Weights: [-4.8426  0.8951 -1.3459  0.1551  0.1388]\n",
      "MSE loss: 86.3805\n",
      "Iteration: 166400\n",
      "Gradient: [  -6.1965    7.6734  -14.2874 -133.2804 -230.0281]\n",
      "Weights: [-4.8471  0.8798 -1.3437  0.1551  0.1387]\n",
      "MSE loss: 86.6803\n",
      "Iteration: 166500\n",
      "Gradient: [  0.5939  -3.9093   2.7076 131.6574  19.1827]\n",
      "Weights: [-4.8472  0.8838 -1.3417  0.1555  0.1385]\n",
      "MSE loss: 86.1968\n",
      "Iteration: 166600\n",
      "Gradient: [ 11.2014  24.5473  31.9618 -59.6729 146.0443]\n",
      "Weights: [-4.8093  0.8696 -1.3427  0.1571  0.1383]\n",
      "MSE loss: 86.9682\n",
      "Iteration: 166700\n",
      "Gradient: [-13.1505   8.664   24.0239 -32.7892 -40.4198]\n",
      "Weights: [-4.8462  0.8774 -1.3404  0.1567  0.1382]\n",
      "MSE loss: 86.1987\n",
      "Iteration: 166800\n",
      "Gradient: [ -3.5556  14.7642   0.6024  88.5031 137.7981]\n",
      "Weights: [-4.8366  0.8838 -1.3423  0.1565  0.138 ]\n",
      "MSE loss: 86.2789\n",
      "Iteration: 166900\n",
      "Gradient: [ -0.8168  11.0221  32.1306 -32.1882 -54.4928]\n",
      "Weights: [-4.8533  0.8875 -1.3419  0.1562  0.1383]\n",
      "MSE loss: 86.2223\n",
      "Iteration: 167000\n",
      "Gradient: [6.050000e-02 2.629800e+00 4.194310e+01 2.231510e+01 1.857998e+02]\n",
      "Weights: [-4.8523  0.8883 -1.3397  0.1558  0.1382]\n",
      "MSE loss: 86.2871\n",
      "Iteration: 167100\n",
      "Gradient: [  2.1097  19.8039  26.5918 -17.954  138.948 ]\n",
      "Weights: [-4.8313  0.8824 -1.3409  0.1554  0.1384]\n",
      "MSE loss: 86.4886\n",
      "Iteration: 167200\n",
      "Gradient: [   2.8754    6.4324  -15.6832  110.9346 -114.3115]\n",
      "Weights: [-4.8263  0.8681 -1.338   0.1563  0.1384]\n",
      "MSE loss: 86.399\n",
      "Iteration: 167300\n",
      "Gradient: [  1.3677  -3.9279  23.3319 138.4526  40.3064]\n",
      "Weights: [-4.8393  0.8756 -1.3391  0.1568  0.1381]\n",
      "MSE loss: 86.1898\n",
      "Iteration: 167400\n",
      "Gradient: [  -2.8495    1.1812   10.4215  -85.6891 -205.1122]\n",
      "Weights: [-4.848   0.8796 -1.3376  0.1562  0.1378]\n",
      "MSE loss: 86.2464\n",
      "Iteration: 167500\n",
      "Gradient: [ -2.8715  -8.1384   6.159  -48.425   60.3358]\n",
      "Weights: [-4.8442  0.8716 -1.3396  0.1571  0.138 ]\n",
      "MSE loss: 86.3605\n",
      "Iteration: 167600\n",
      "Gradient: [ 7.0745 15.6184 14.4352 67.4152 46.5148]\n",
      "Weights: [-4.8183  0.867  -1.3404  0.1575  0.1381]\n",
      "MSE loss: 86.4759\n",
      "Iteration: 167700\n",
      "Gradient: [  2.9638  -8.9734  19.902   90.2848 109.2692]\n",
      "Weights: [-4.8399  0.8751 -1.3413  0.1572  0.1382]\n",
      "MSE loss: 86.1635\n",
      "Iteration: 167800\n",
      "Gradient: [-9.2923  8.914  30.3311 37.0615 84.7987]\n",
      "Weights: [-4.8533  0.8884 -1.3438  0.1575  0.138 ]\n",
      "MSE loss: 86.2033\n",
      "Iteration: 167900\n",
      "Gradient: [ -6.5783  13.8914  -1.7108  95.3304 -47.6231]\n",
      "Weights: [-4.8375  0.8858 -1.3433  0.1579  0.1378]\n",
      "MSE loss: 86.3631\n",
      "Iteration: 168000\n",
      "Gradient: [ 11.2382  -7.6487 -10.704  -38.0467  54.9294]\n",
      "Weights: [-4.8366  0.8812 -1.3432  0.1583  0.1379]\n",
      "MSE loss: 86.3038\n",
      "Iteration: 168100\n",
      "Gradient: [   7.6305    5.1224  -22.6308   19.796  -139.9273]\n",
      "Weights: [-4.8482  0.8781 -1.3396  0.1578  0.1378]\n",
      "MSE loss: 86.1924\n",
      "Iteration: 168200\n",
      "Gradient: [  1.3678  -8.827   22.8753  -4.5231 157.8196]\n",
      "Weights: [-4.8193  0.8656 -1.339   0.1588  0.1378]\n",
      "MSE loss: 86.6998\n",
      "Iteration: 168300\n",
      "Gradient: [   4.427    -8.784   -20.4124  -36.7909 -113.2028]\n",
      "Weights: [-4.8365  0.8678 -1.3411  0.1589  0.1376]\n",
      "MSE loss: 86.301\n",
      "Iteration: 168400\n",
      "Gradient: [  -3.118    -5.2363  -47.5615  -64.2655 -115.5524]\n",
      "Weights: [-4.8514  0.8776 -1.3434  0.1591  0.1376]\n",
      "MSE loss: 86.3721\n",
      "Iteration: 168500\n",
      "Gradient: [   7.8636    3.267    -7.0223  -58.8153 -124.2628]\n",
      "Weights: [-4.8386  0.8809 -1.3444  0.1587  0.1379]\n",
      "MSE loss: 86.1855\n",
      "Iteration: 168600\n",
      "Gradient: [  -3.5065   -1.1313   19.8736   -6.079  -225.0104]\n",
      "Weights: [-4.8469  0.8741 -1.3427  0.159   0.1377]\n",
      "MSE loss: 86.2757\n",
      "Iteration: 168700\n",
      "Gradient: [  5.4041   2.5751  11.1012 -57.3992 102.0373]\n",
      "Weights: [-4.8327  0.8665 -1.342   0.159   0.1379]\n",
      "MSE loss: 86.1705\n",
      "Iteration: 168800\n",
      "Gradient: [-8.954  -3.9457 -8.687   2.874  -8.0676]\n",
      "Weights: [-4.8413  0.8693 -1.342   0.1595  0.1377]\n",
      "MSE loss: 86.2015\n",
      "Iteration: 168900\n",
      "Gradient: [  2.5853  23.6175  -1.55    54.8231 -85.4076]\n",
      "Weights: [-4.8336  0.8818 -1.3454  0.1592  0.1376]\n",
      "MSE loss: 86.2737\n",
      "Iteration: 169000\n",
      "Gradient: [  -6.4214    3.5231   -4.2985    3.2172 -151.2344]\n",
      "Weights: [-4.8389  0.8726 -1.345   0.1598  0.1377]\n",
      "MSE loss: 86.2044\n",
      "Iteration: 169100\n",
      "Gradient: [ -1.8758 -20.0084  -0.4237   3.5797 -47.9396]\n",
      "Weights: [-4.8388  0.8785 -1.3473  0.1598  0.1377]\n",
      "MSE loss: 86.1656\n",
      "Iteration: 169200\n",
      "Gradient: [  -9.9957   18.6061   -5.9696  146.4857 -345.1397]\n",
      "Weights: [-4.8564  0.8962 -1.3506  0.1595  0.1378]\n",
      "MSE loss: 86.1972\n",
      "Iteration: 169300\n",
      "Gradient: [ 1.4879  9.2281 53.3652 75.0617 49.518 ]\n",
      "Weights: [-4.8354  0.8898 -1.3511  0.1606  0.1377]\n",
      "MSE loss: 86.4046\n",
      "Iteration: 169400\n",
      "Gradient: [   0.3675  -11.8218   -2.053   -32.9952 -147.0957]\n",
      "Weights: [-4.8459  0.8851 -1.3487  0.1606  0.1377]\n",
      "MSE loss: 86.1292\n",
      "Iteration: 169500\n",
      "Gradient: [   4.5234   17.2537   45.5805  -23.772  -298.6375]\n",
      "Weights: [-4.8306  0.8871 -1.3535  0.1613  0.1376]\n",
      "MSE loss: 86.2523\n",
      "Iteration: 169600\n",
      "Gradient: [  4.7398  -2.7127 -11.4671  38.9857 265.8264]\n",
      "Weights: [-4.8365  0.8887 -1.3531  0.161   0.1378]\n",
      "MSE loss: 86.1921\n",
      "Iteration: 169700\n",
      "Gradient: [   8.7119    6.3532   11.6205   -1.8175 -186.5813]\n",
      "Weights: [-4.8404  0.8838 -1.3509  0.1606  0.1378]\n",
      "MSE loss: 86.106\n",
      "Iteration: 169800\n",
      "Gradient: [ -0.3899   3.0851   6.4724  85.867  275.4016]\n",
      "Weights: [-4.833   0.8786 -1.3493  0.161   0.1376]\n",
      "MSE loss: 86.1439\n",
      "Iteration: 169900\n",
      "Gradient: [ -9.148  -10.1636   0.1981  25.0373   2.0044]\n",
      "Weights: [-4.851   0.8886 -1.3499  0.1613  0.1375]\n",
      "MSE loss: 86.1467\n",
      "Iteration: 170000\n",
      "Gradient: [  1.0597  15.3654  62.6446  83.416  -12.3196]\n",
      "Weights: [-4.8505  0.8895 -1.3496  0.1609  0.1377]\n",
      "MSE loss: 86.2186\n",
      "Iteration: 170100\n",
      "Gradient: [14.6975 11.5271 19.1693 26.2167 44.155 ]\n",
      "Weights: [-4.8396  0.8856 -1.352   0.1613  0.1378]\n",
      "MSE loss: 86.1431\n",
      "Iteration: 170200\n",
      "Gradient: [ -6.9106 -17.7643 -28.4584  -1.7656  10.0782]\n",
      "Weights: [-4.8527  0.8767 -1.3493  0.1606  0.1378]\n",
      "MSE loss: 86.7855\n",
      "Iteration: 170300\n",
      "Gradient: [  1.7793  -1.8232 -13.221  -47.3329 444.6501]\n",
      "Weights: [-4.8375  0.8835 -1.3515  0.1608  0.1379]\n",
      "MSE loss: 86.1327\n",
      "Iteration: 170400\n",
      "Gradient: [  2.9255 -10.0862  22.2832  -7.0139 106.6908]\n",
      "Weights: [-4.8369  0.8776 -1.3496  0.1607  0.1379]\n",
      "MSE loss: 86.1185\n",
      "Iteration: 170500\n",
      "Gradient: [   1.6884    2.3342   -1.3795   60.3169 -369.5473]\n",
      "Weights: [-4.828   0.876  -1.3503  0.161   0.1377]\n",
      "MSE loss: 86.1788\n",
      "Iteration: 170600\n",
      "Gradient: [  -5.0956  -13.7928   12.5736 -160.7822   38.0504]\n",
      "Weights: [-4.8605  0.8852 -1.3516  0.1612  0.1378]\n",
      "MSE loss: 86.6582\n",
      "Iteration: 170700\n",
      "Gradient: [  -1.2483  -27.903   -43.9378 -115.9394  -74.7218]\n",
      "Weights: [-4.8443  0.8773 -1.3508  0.1613  0.1378]\n",
      "MSE loss: 86.2888\n",
      "Iteration: 170800\n",
      "Gradient: [ -11.2882  -15.2093   34.2065  -41.7137 -149.298 ]\n",
      "Weights: [-4.8484  0.8854 -1.35    0.1609  0.1377]\n",
      "MSE loss: 86.1392\n",
      "Iteration: 170900\n",
      "Gradient: [  0.7566  25.756  -22.2279   2.5723 -79.934 ]\n",
      "Weights: [-4.8419  0.8945 -1.3528  0.1606  0.1376]\n",
      "MSE loss: 86.1815\n",
      "Iteration: 171000\n",
      "Gradient: [  1.4714  -0.3614  44.7991 -13.613   32.1968]\n",
      "Weights: [-4.8406  0.8876 -1.3532  0.1599  0.1379]\n",
      "MSE loss: 86.2413\n",
      "Iteration: 171100\n",
      "Gradient: [ -3.1954   3.2873 -23.1205  19.5955 243.1841]\n",
      "Weights: [-4.8445  0.8965 -1.3563  0.1607  0.138 ]\n",
      "MSE loss: 86.1043\n",
      "Iteration: 171200\n",
      "Gradient: [ -5.6644   5.9465  10.1631 -67.0948 228.549 ]\n",
      "Weights: [-4.8421  0.8992 -1.3572  0.1605  0.1382]\n",
      "MSE loss: 86.213\n",
      "Iteration: 171300\n",
      "Gradient: [ -6.2851  -9.1149 -34.5289 -37.4985 171.6476]\n",
      "Weights: [-4.8602  0.8953 -1.3543  0.1599  0.1381]\n",
      "MSE loss: 86.4152\n",
      "Iteration: 171400\n",
      "Gradient: [ -3.6704 -14.0359  28.0967 -30.4403   0.3916]\n",
      "Weights: [-4.8498  0.8947 -1.3559  0.1609  0.138 ]\n",
      "MSE loss: 86.1145\n",
      "Iteration: 171500\n",
      "Gradient: [  -5.418     9.8141   28.6326  -78.84   -211.6211]\n",
      "Weights: [-4.8566  0.901  -1.3557  0.1608  0.138 ]\n",
      "MSE loss: 86.1952\n",
      "Iteration: 171600\n",
      "Gradient: [ -3.5488   4.3253  -8.8728  94.013  136.44  ]\n",
      "Weights: [-4.8458  0.8895 -1.3535  0.1608  0.1381]\n",
      "MSE loss: 86.1375\n",
      "Iteration: 171700\n",
      "Gradient: [ -4.4098  -7.1701 -21.8085  27.1781 -78.4823]\n",
      "Weights: [-4.837   0.8851 -1.3561  0.1612  0.138 ]\n",
      "MSE loss: 86.1935\n",
      "Iteration: 171800\n",
      "Gradient: [  2.8876   4.5693 -30.6597  -2.0625 -28.3932]\n",
      "Weights: [-4.8323  0.8851 -1.3574  0.1621  0.1381]\n",
      "MSE loss: 86.1576\n",
      "Iteration: 171900\n",
      "Gradient: [ -4.2732   2.8902   8.2763  15.7061 143.4645]\n",
      "Weights: [-4.8172  0.8726 -1.3555  0.1623  0.1382]\n",
      "MSE loss: 86.3685\n",
      "Iteration: 172000\n",
      "Gradient: [ -2.2382  -0.2221 -16.493  -48.6864 -85.8915]\n",
      "Weights: [-4.8325  0.8771 -1.3604  0.1629  0.1382]\n",
      "MSE loss: 86.3622\n",
      "Iteration: 172100\n",
      "Gradient: [ -5.7406  -9.5817 -14.7219  -7.6447 206.4395]\n",
      "Weights: [-4.8309  0.879  -1.3585  0.1629  0.1382]\n",
      "MSE loss: 86.1771\n",
      "Iteration: 172200\n",
      "Gradient: [ 3.9188 -5.3345  3.4563 10.4107 76.0405]\n",
      "Weights: [-4.8255  0.8752 -1.3578  0.1626  0.1381]\n",
      "MSE loss: 86.1993\n",
      "Iteration: 172300\n",
      "Gradient: [ 14.8802 -14.0543  12.2668  86.8348  -0.9159]\n",
      "Weights: [-4.8164  0.8684 -1.3566  0.1632  0.1381]\n",
      "MSE loss: 86.3253\n",
      "Iteration: 172400\n",
      "Gradient: [  7.3505  23.5059  24.5779 -16.7377 333.2431]\n",
      "Weights: [-4.8437  0.888  -1.3554  0.1618  0.138 ]\n",
      "MSE loss: 86.134\n",
      "Iteration: 172500\n",
      "Gradient: [-11.6914  -9.5374  -7.9484 -91.5387 112.0002]\n",
      "Weights: [-4.8424  0.8877 -1.3559  0.162   0.1379]\n",
      "MSE loss: 86.092\n",
      "Iteration: 172600\n",
      "Gradient: [ 10.8916  -0.933   -2.387  110.4451  15.6037]\n",
      "Weights: [-4.85    0.8885 -1.3549  0.1622  0.1377]\n",
      "MSE loss: 86.152\n",
      "Iteration: 172700\n",
      "Gradient: [  -6.7003    5.5409  -53.5213  -13.8232 -312.1575]\n",
      "Weights: [-4.8442  0.8814 -1.3551  0.1619  0.1377]\n",
      "MSE loss: 86.496\n",
      "Iteration: 172800\n",
      "Gradient: [  -5.1067    9.7692   14.7461  -97.4906 -332.2775]\n",
      "Weights: [-4.8484  0.8846 -1.3541  0.1617  0.138 ]\n",
      "MSE loss: 86.206\n",
      "Iteration: 172900\n",
      "Gradient: [ -4.1278  -2.812   36.6695 -41.4249 135.4804]\n",
      "Weights: [-4.8557  0.899  -1.3563  0.1613  0.1378]\n",
      "MSE loss: 86.1485\n",
      "Iteration: 173000\n",
      "Gradient: [ -5.9248  -8.9534   9.2299 -44.9908 398.101 ]\n",
      "Weights: [-4.8583  0.9074 -1.3602  0.1621  0.1378]\n",
      "MSE loss: 86.1332\n",
      "Iteration: 173100\n",
      "Gradient: [  8.4115  -9.3459  44.6071 122.7362 -37.7138]\n",
      "Weights: [-4.8533  0.9108 -1.3596  0.1623  0.1375]\n",
      "MSE loss: 86.3079\n",
      "Iteration: 173200\n",
      "Gradient: [   4.2952   23.5017  -26.1143  -19.3491 -134.0499]\n",
      "Weights: [-4.8463  0.9034 -1.3628  0.1625  0.1377]\n",
      "MSE loss: 86.1139\n",
      "Iteration: 173300\n",
      "Gradient: [   8.1742   -4.6317   34.8114 -142.6655  201.4885]\n",
      "Weights: [-4.8476  0.9027 -1.3624  0.1629  0.1378]\n",
      "MSE loss: 86.0753\n",
      "Iteration: 173400\n",
      "Gradient: [ 10.5034  -0.1328 -13.9147  35.0116   5.6391]\n",
      "Weights: [-4.8416  0.8999 -1.3623  0.1629  0.138 ]\n",
      "MSE loss: 86.1704\n",
      "Iteration: 173500\n",
      "Gradient: [  4.0766   1.5781 -27.4694  23.8251   0.7559]\n",
      "Weights: [-4.8592  0.9108 -1.3634  0.1624  0.1378]\n",
      "MSE loss: 86.1182\n",
      "Iteration: 173600\n",
      "Gradient: [  8.4734   4.6287  -1.3081   1.3844 145.5569]\n",
      "Weights: [-4.8312  0.9015 -1.3621  0.1624  0.138 ]\n",
      "MSE loss: 86.6354\n",
      "Iteration: 173700\n",
      "Gradient: [ -1.215   -7.3083  -5.5605  62.5583 -72.563 ]\n",
      "Weights: [-4.8327  0.8999 -1.363   0.1631  0.1379]\n",
      "MSE loss: 86.4517\n",
      "Iteration: 173800\n",
      "Gradient: [  9.0307   5.1457  14.9475 -37.0187 -61.9889]\n",
      "Weights: [-4.8444  0.9006 -1.3634  0.1628  0.138 ]\n",
      "MSE loss: 86.0646\n",
      "Iteration: 173900\n",
      "Gradient: [ 12.1662  -3.7558  11.5428 142.671   11.5264]\n",
      "Weights: [-4.8401  0.8949 -1.3624  0.1632  0.138 ]\n",
      "MSE loss: 86.0828\n",
      "Iteration: 174000\n",
      "Gradient: [ -5.1501   2.8298  11.2565  85.146  -40.2639]\n",
      "Weights: [-4.8461  0.8928 -1.3621  0.1631  0.138 ]\n",
      "MSE loss: 86.1421\n",
      "Iteration: 174100\n",
      "Gradient: [   0.9101    3.652    19.0446  100.4159 -139.2816]\n",
      "Weights: [-4.8518  0.8999 -1.3624  0.1627  0.1381]\n",
      "MSE loss: 86.1267\n",
      "Iteration: 174200\n",
      "Gradient: [ -1.9136   8.2852  -8.8962  39.0807 169.477 ]\n",
      "Weights: [-4.839   0.9091 -1.3686  0.1634  0.1379]\n",
      "MSE loss: 86.1591\n",
      "Iteration: 174300\n",
      "Gradient: [ -8.0226   9.3323  20.2242 -35.4193 -43.4829]\n",
      "Weights: [-4.8631  0.9106 -1.367   0.1635  0.1379]\n",
      "MSE loss: 86.2445\n",
      "Iteration: 174400\n",
      "Gradient: [ -3.1998   3.259    4.3002 -59.0501 -29.3563]\n",
      "Weights: [-4.8326  0.9077 -1.3668  0.1629  0.1379]\n",
      "MSE loss: 86.4424\n",
      "Iteration: 174500\n",
      "Gradient: [ -5.4215  -0.1864  12.5775 136.3668  60.2528]\n",
      "Weights: [-4.8404  0.9041 -1.3656  0.1635  0.1378]\n",
      "MSE loss: 86.1123\n",
      "Iteration: 174600\n",
      "Gradient: [-20.8212  18.3673   1.3399  21.1934 -18.0942]\n",
      "Weights: [-4.8642  0.9076 -1.3628  0.1634  0.1376]\n",
      "MSE loss: 86.2397\n",
      "Iteration: 174700\n",
      "Gradient: [   5.9453    8.7875  -19.1621 -125.613   295.0023]\n",
      "Weights: [-4.845   0.9001 -1.3633  0.1636  0.1375]\n",
      "MSE loss: 86.085\n",
      "Iteration: 174800\n",
      "Gradient: [   1.4162   17.3505  -27.9613  -62.231  -114.2967]\n",
      "Weights: [-4.8383  0.891  -1.3617  0.164   0.1375]\n",
      "MSE loss: 86.0992\n",
      "Iteration: 174900\n",
      "Gradient: [ -8.3272  -6.9783 -16.6505  82.7052 -30.1047]\n",
      "Weights: [-4.829   0.8837 -1.3603  0.1641  0.1375]\n",
      "MSE loss: 86.1453\n",
      "Iteration: 175000\n",
      "Gradient: [   4.0995   -1.5842   28.909   -50.6811 -167.437 ]\n",
      "Weights: [-4.8307  0.883  -1.359   0.1636  0.1374]\n",
      "MSE loss: 86.206\n",
      "Iteration: 175100\n",
      "Gradient: [  4.4355 -14.9825 -33.5628  -8.0512 -18.7453]\n",
      "Weights: [-4.8224  0.876  -1.3571  0.1642  0.1374]\n",
      "MSE loss: 86.204\n",
      "Iteration: 175200\n",
      "Gradient: [ -2.3621  -2.0559   6.8816 112.7855  -3.4961]\n",
      "Weights: [-4.8202  0.8677 -1.353   0.163   0.1376]\n",
      "MSE loss: 86.2316\n",
      "Iteration: 175300\n",
      "Gradient: [ -13.442     1.8345   12.2861   15.5121 -420.5955]\n",
      "Weights: [-4.8417  0.8741 -1.3527  0.1638  0.1375]\n",
      "MSE loss: 86.1895\n",
      "Iteration: 175400\n",
      "Gradient: [  4.7393   9.3487 -14.7688  21.1859  25.7321]\n",
      "Weights: [-4.8413  0.8772 -1.3517  0.163   0.1376]\n",
      "MSE loss: 86.1521\n",
      "Iteration: 175500\n",
      "Gradient: [  6.523    0.9093 -43.2279 101.1028 -20.0657]\n",
      "Weights: [-4.837   0.8748 -1.3523  0.1632  0.1375]\n",
      "MSE loss: 86.1151\n",
      "Iteration: 175600\n",
      "Gradient: [12.5467  9.7484 37.623   0.8966 54.1895]\n",
      "Weights: [-4.8195  0.8751 -1.3511  0.1627  0.1373]\n",
      "MSE loss: 86.4688\n",
      "Iteration: 175700\n",
      "Gradient: [  0.8893  -4.7226  41.0031  86.095  217.917 ]\n",
      "Weights: [-4.8372  0.8723 -1.3506  0.1631  0.1374]\n",
      "MSE loss: 86.1245\n",
      "Iteration: 175800\n",
      "Gradient: [ -4.9396  -4.2105 -31.498   29.999   98.7216]\n",
      "Weights: [-4.8333  0.8757 -1.3497  0.1628  0.1373]\n",
      "MSE loss: 86.1688\n",
      "Iteration: 175900\n",
      "Gradient: [ -1.6981   7.5585   8.5406  36.6295 325.0093]\n",
      "Weights: [-4.8417  0.8771 -1.3501  0.1628  0.1373]\n",
      "MSE loss: 86.1086\n",
      "Iteration: 176000\n",
      "Gradient: [  0.1633   3.5456 -40.1567  -8.7933 -95.4037]\n",
      "Weights: [-4.8197  0.8625 -1.3504  0.1626  0.1374]\n",
      "MSE loss: 86.3317\n",
      "Iteration: 176100\n",
      "Gradient: [  2.8894 -10.9343  10.5687 -81.4005 194.9505]\n",
      "Weights: [-4.8336  0.8764 -1.3494  0.1626  0.1374]\n",
      "MSE loss: 86.2819\n",
      "Iteration: 176200\n",
      "Gradient: [-3.148   4.873  40.2687 78.2346  5.8426]\n",
      "Weights: [-4.851   0.8806 -1.3472  0.1624  0.1373]\n",
      "MSE loss: 86.2512\n",
      "Iteration: 176300\n",
      "Gradient: [   6.8579    3.5146  -48.7698   -8.6557 -124.1033]\n",
      "Weights: [-4.839   0.8699 -1.3469  0.162   0.1375]\n",
      "MSE loss: 86.1523\n",
      "Iteration: 176400\n",
      "Gradient: [  3.3283   5.5064  -7.9893 -22.2518 -26.7715]\n",
      "Weights: [-4.8295  0.866  -1.3474  0.1626  0.1372]\n",
      "MSE loss: 86.1394\n",
      "Iteration: 176500\n",
      "Gradient: [   1.1517  -18.3088  -41.5248   17.1625 -196.6545]\n",
      "Weights: [-4.8315  0.8738 -1.3475  0.1617  0.1371]\n",
      "MSE loss: 86.165\n",
      "Iteration: 176600\n",
      "Gradient: [ -6.665   -0.9461 -12.3394  40.4052 258.6243]\n",
      "Weights: [-4.8353  0.875  -1.3489  0.1618  0.1373]\n",
      "MSE loss: 86.1309\n",
      "Iteration: 176700\n",
      "Gradient: [   0.5142   -5.6695  -35.5031 -161.1037  118.8156]\n",
      "Weights: [-4.843   0.8707 -1.3486  0.1621  0.1375]\n",
      "MSE loss: 86.3457\n",
      "Iteration: 176800\n",
      "Gradient: [12.8745 -9.5035 -3.0693 65.0521 81.311 ]\n",
      "Weights: [-4.8389  0.8736 -1.3467  0.1615  0.1376]\n",
      "MSE loss: 86.1611\n",
      "Iteration: 176900\n",
      "Gradient: [  2.8101  -3.7054  37.8683 -49.4958  75.163 ]\n",
      "Weights: [-4.8393  0.8725 -1.3475  0.1621  0.1373]\n",
      "MSE loss: 86.1311\n",
      "Iteration: 177000\n",
      "Gradient: [ -5.1349  -5.3607 -12.2532  44.1368 -53.0371]\n",
      "Weights: [-4.8388  0.8721 -1.3461  0.1612  0.1372]\n",
      "MSE loss: 86.2575\n",
      "Iteration: 177100\n",
      "Gradient: [ -2.567   -8.2253 -16.9267  29.7394 -58.9121]\n",
      "Weights: [-4.8437  0.8802 -1.3479  0.1613  0.1373]\n",
      "MSE loss: 86.1312\n",
      "Iteration: 177200\n",
      "Gradient: [ 1.525   0.7816  1.8707  4.5755 22.1484]\n",
      "Weights: [-4.8457  0.8849 -1.3508  0.1617  0.1372]\n",
      "MSE loss: 86.161\n",
      "Iteration: 177300\n",
      "Gradient: [  1.8371   9.7684  10.0989  69.7211 132.7559]\n",
      "Weights: [-4.8283  0.8772 -1.3508  0.1624  0.1372]\n",
      "MSE loss: 86.1788\n",
      "Iteration: 177400\n",
      "Gradient: [  -3.6612   -0.2116  -33.1981  -82.7787 -178.5372]\n",
      "Weights: [-4.8396  0.8845 -1.3519  0.1622  0.1373]\n",
      "MSE loss: 86.0923\n",
      "Iteration: 177500\n",
      "Gradient: [  10.5995   -2.3377  -22.4539 -120.1523  -17.9719]\n",
      "Weights: [-4.8431  0.8762 -1.3504  0.1632  0.1373]\n",
      "MSE loss: 86.1345\n",
      "Iteration: 177600\n",
      "Gradient: [  3.1078 -22.0898   4.0059 -70.657   72.1289]\n",
      "Weights: [-4.8491  0.8902 -1.3524  0.1632  0.1368]\n",
      "MSE loss: 86.106\n",
      "Iteration: 177700\n",
      "Gradient: [  -0.2191   -7.9406  -21.746   -23.1704 -201.3453]\n",
      "Weights: [-4.8468  0.8911 -1.3551  0.1637  0.1366]\n",
      "MSE loss: 86.3209\n",
      "Iteration: 177800\n",
      "Gradient: [ -7.36    -4.9351  -3.7977  23.453  -68.4708]\n",
      "Weights: [-4.8407  0.8838 -1.3553  0.1644  0.137 ]\n",
      "MSE loss: 86.0582\n",
      "Iteration: 177900\n",
      "Gradient: [   2.1341   17.0248   12.9949    3.5415 -114.4183]\n",
      "Weights: [-4.8359  0.8923 -1.3596  0.1641  0.1372]\n",
      "MSE loss: 86.114\n",
      "Iteration: 178000\n",
      "Gradient: [ -7.1192  -4.7062  -3.649   34.939  -79.4734]\n",
      "Weights: [-4.8669  0.9033 -1.359   0.1633  0.1373]\n",
      "MSE loss: 86.3825\n",
      "Iteration: 178100\n",
      "Gradient: [ -5.1655  -8.058    6.5828 -40.1584 -50.8785]\n",
      "Weights: [-4.8677  0.9146 -1.3569  0.1615  0.1373]\n",
      "MSE loss: 86.2683\n",
      "Iteration: 178200\n",
      "Gradient: [   0.4315  -19.5124  -14.4019  -19.2959 -205.1526]\n",
      "Weights: [-4.8574  0.8994 -1.357   0.162   0.1373]\n",
      "MSE loss: 86.3494\n",
      "Iteration: 178300\n",
      "Gradient: [  9.7218  -9.625   -8.1183  26.7146 -56.6497]\n",
      "Weights: [-4.8377  0.8885 -1.3562  0.1623  0.1375]\n",
      "MSE loss: 86.1268\n",
      "Iteration: 178400\n",
      "Gradient: [  -6.7799  -19.0055  -30.7897  -99.7297 -227.4743]\n",
      "Weights: [-4.8491  0.8974 -1.3552  0.1621  0.1374]\n",
      "MSE loss: 86.1129\n",
      "Iteration: 178500\n",
      "Gradient: [   5.1133    5.3819   42.4348  -19.4543 -185.4818]\n",
      "Weights: [-4.8418  0.8966 -1.3552  0.1616  0.1375]\n",
      "MSE loss: 86.184\n",
      "Iteration: 178600\n",
      "Gradient: [-9.62100e-01 -8.99000e-02 -1.49565e+01  5.38885e+01  1.16637e+02]\n",
      "Weights: [-4.8689  0.9037 -1.3567  0.1627  0.1375]\n",
      "MSE loss: 86.3812\n",
      "Iteration: 178700\n",
      "Gradient: [   0.2976    0.2658  -40.9958  -63.7538 -226.0183]\n",
      "Weights: [-4.8422  0.9005 -1.3579  0.162   0.1373]\n",
      "MSE loss: 86.204\n",
      "Iteration: 178800\n",
      "Gradient: [  -0.6667   -8.9829    3.4468  -19.0398 -171.2433]\n",
      "Weights: [-4.8293  0.8899 -1.3567  0.1628  0.1374]\n",
      "MSE loss: 86.3495\n",
      "Iteration: 178900\n",
      "Gradient: [-0.8279  3.146  -5.7983 34.0254 60.1826]\n",
      "Weights: [-4.8439  0.8953 -1.3599  0.1627  0.1376]\n",
      "MSE loss: 86.1161\n",
      "Iteration: 179000\n",
      "Gradient: [-10.0268   6.056   15.4939 -32.7564 -46.6873]\n",
      "Weights: [-4.8315  0.8859 -1.3612  0.1637  0.1376]\n",
      "MSE loss: 86.1571\n",
      "Iteration: 179100\n",
      "Gradient: [ 12.7894  -3.6592  19.5467  80.9772 326.0675]\n",
      "Weights: [-4.8279  0.8939 -1.3616  0.1635  0.1377]\n",
      "MSE loss: 86.3869\n",
      "Iteration: 179200\n",
      "Gradient: [   3.3282    9.2195  -20.0015  -99.9956 -203.2318]\n",
      "Weights: [-4.8517  0.8976 -1.36    0.1638  0.1375]\n",
      "MSE loss: 86.1148\n",
      "Iteration: 179300\n",
      "Gradient: [  -5.3974  -13.6014  -43.1598  -83.8962 -329.2545]\n",
      "Weights: [-4.8363  0.8854 -1.3607  0.1641  0.1374]\n",
      "MSE loss: 86.2378\n",
      "Iteration: 179400\n",
      "Gradient: [  0.6923  -4.501  -24.6613 -61.073   99.0176]\n",
      "Weights: [-4.8453  0.8717 -1.3572  0.1652  0.1376]\n",
      "MSE loss: 86.6219\n",
      "Iteration: 179500\n",
      "Gradient: [  0.0459   3.0925   1.1691 -18.0845  -9.9891]\n",
      "Weights: [-4.8381  0.8805 -1.3594  0.1647  0.1374]\n",
      "MSE loss: 86.1929\n",
      "Iteration: 179600\n",
      "Gradient: [ 12.0214  14.3956 -16.2937  31.3631 331.98  ]\n",
      "Weights: [-4.8198  0.8791 -1.3608  0.1655  0.1375]\n",
      "MSE loss: 86.3388\n",
      "Iteration: 179700\n",
      "Gradient: [ -4.3444 -27.5201  -8.8304   6.3553 200.4022]\n",
      "Weights: [-4.8321  0.8775 -1.3647  0.1667  0.1374]\n",
      "MSE loss: 86.299\n",
      "Iteration: 179800\n",
      "Gradient: [  -1.6482    0.7792  -30.1654  -48.1105 -231.0931]\n",
      "Weights: [-4.8532  0.8928 -1.3631  0.1667  0.1371]\n",
      "MSE loss: 86.1663\n",
      "Iteration: 179900\n",
      "Gradient: [   2.0451   -6.523   -45.8752 -204.2325 -171.9008]\n",
      "Weights: [-4.85    0.8914 -1.3653  0.1672  0.1368]\n",
      "MSE loss: 86.3531\n",
      "Iteration: 180000\n",
      "Gradient: [ -0.7484  13.305  -32.395   36.5372 155.8441]\n",
      "Weights: [-4.8383  0.8861 -1.366   0.1679  0.1369]\n",
      "MSE loss: 86.1062\n",
      "Iteration: 180100\n",
      "Gradient: [ -3.1436  -1.6565   1.2885 -63.4598 -26.438 ]\n",
      "Weights: [-4.8306  0.8906 -1.3677  0.168   0.1369]\n",
      "MSE loss: 86.0674\n",
      "Iteration: 180200\n",
      "Gradient: [  3.5334  10.2316  43.5481  36.4586 -74.4655]\n",
      "Weights: [-4.8282  0.8968 -1.3673  0.1684  0.137 ]\n",
      "MSE loss: 87.1069\n",
      "Iteration: 180300\n",
      "Gradient: [ -3.5301 -17.6689 -30.0793 -22.8369  65.0185]\n",
      "Weights: [-4.8418  0.885  -1.3646  0.1674  0.137 ]\n",
      "MSE loss: 86.1994\n",
      "Iteration: 180400\n",
      "Gradient: [  1.9895  -8.2055 -17.0351  -3.8687 -38.0129]\n",
      "Weights: [-4.8353  0.8823 -1.3639  0.1675  0.1369]\n",
      "MSE loss: 86.0853\n",
      "Iteration: 180500\n",
      "Gradient: [   2.7983    2.1665  -73.9233  -49.982  -354.9626]\n",
      "Weights: [-4.8266  0.8742 -1.3595  0.1671  0.1368]\n",
      "MSE loss: 86.0914\n",
      "Iteration: 180600\n",
      "Gradient: [ -7.5487   8.8182 -11.9069  21.5964  54.644 ]\n",
      "Weights: [-4.8357  0.8772 -1.3585  0.1664  0.1368]\n",
      "MSE loss: 86.1058\n",
      "Iteration: 180700\n",
      "Gradient: [  -3.8206    1.9491  -24.5671 -110.1738  107.9158]\n",
      "Weights: [-4.8198  0.8674 -1.3569  0.1662  0.1369]\n",
      "MSE loss: 86.2023\n",
      "Iteration: 180800\n",
      "Gradient: [ 0.2891  2.9223 46.9788 36.9004 87.8071]\n",
      "Weights: [-4.835   0.8682 -1.3539  0.1665  0.1368]\n",
      "MSE loss: 86.1254\n",
      "Iteration: 180900\n",
      "Gradient: [-6.002000e-01  1.231200e+00  3.458450e+01 -1.460000e-01 -1.490437e+02]\n",
      "Weights: [-4.8182  0.8702 -1.3588  0.1668  0.1368]\n",
      "MSE loss: 86.2231\n",
      "Iteration: 181000\n",
      "Gradient: [-11.6478  -1.8426  -6.8887  -1.3075 -97.2278]\n",
      "Weights: [-4.849   0.8878 -1.3603  0.1669  0.1367]\n",
      "MSE loss: 86.1053\n",
      "Iteration: 181100\n",
      "Gradient: [  8.8607  -4.0903   5.4683  50.5516 165.0777]\n",
      "Weights: [-4.8424  0.8887 -1.36    0.1662  0.1368]\n",
      "MSE loss: 86.0328\n",
      "Iteration: 181200\n",
      "Gradient: [   5.0691   -3.9391   56.2307   38.872  -249.5264]\n",
      "Weights: [-4.8397  0.8883 -1.361   0.1659  0.1368]\n",
      "MSE loss: 86.0904\n",
      "Iteration: 181300\n",
      "Gradient: [  1.7295  12.3956  24.1438  51.0949 226.0416]\n",
      "Weights: [-4.8437  0.8981 -1.3635  0.1667  0.1368]\n",
      "MSE loss: 86.0817\n",
      "Iteration: 181400\n",
      "Gradient: [ -2.7933   5.1237  -1.7085 -51.9066 202.6014]\n",
      "Weights: [-4.8578  0.8981 -1.3639  0.1673  0.1365]\n",
      "MSE loss: 86.2406\n",
      "Iteration: 181500\n",
      "Gradient: [  4.4476 -16.3663  13.1071 -18.3278 -23.0518]\n",
      "Weights: [-4.8534  0.8938 -1.3629  0.1676  0.1364]\n",
      "MSE loss: 86.1823\n",
      "Iteration: 181600\n",
      "Gradient: [  4.946   10.7281 -11.4485  -6.4092  50.886 ]\n",
      "Weights: [-4.843   0.8939 -1.362   0.1674  0.1363]\n",
      "MSE loss: 86.0208\n",
      "Iteration: 181700\n",
      "Gradient: [  -7.7102    4.4884   -5.9933  -71.8771 -121.623 ]\n",
      "Weights: [-4.8447  0.8921 -1.3607  0.1675  0.1364]\n",
      "MSE loss: 86.0414\n",
      "Iteration: 181800\n",
      "Gradient: [-3.2003  1.206  -8.3189 10.8053 10.9366]\n",
      "Weights: [-4.8589  0.8988 -1.362   0.1677  0.1361]\n",
      "MSE loss: 86.1815\n",
      "Iteration: 181900\n",
      "Gradient: [   0.7282    5.2298  -10.5096    4.929  -199.819 ]\n",
      "Weights: [-4.836   0.8914 -1.3652  0.1687  0.1362]\n",
      "MSE loss: 86.044\n",
      "Iteration: 182000\n",
      "Gradient: [   0.9939    2.721    10.2157  -49.5848 -107.7249]\n",
      "Weights: [-4.8364  0.898  -1.3667  0.169   0.1362]\n",
      "MSE loss: 86.1434\n",
      "Iteration: 182100\n",
      "Gradient: [  5.2248  13.848    5.0009  60.8022 115.706 ]\n",
      "Weights: [-4.8449  0.8973 -1.3652  0.1688  0.1364]\n",
      "MSE loss: 86.092\n",
      "Iteration: 182200\n",
      "Gradient: [ -0.982    5.4942 -51.9491 -48.7266  81.9255]\n",
      "Weights: [-4.8376  0.891  -1.3638  0.1694  0.1359]\n",
      "MSE loss: 86.0205\n",
      "Iteration: 182300\n",
      "Gradient: [  3.1569 -16.9662   7.2594  16.9964  15.1961]\n",
      "Weights: [-4.8508  0.8966 -1.3647  0.1698  0.1359]\n",
      "MSE loss: 86.0057\n",
      "Iteration: 182400\n",
      "Gradient: [  -2.7614   -0.8938   14.3906   56.11   -166.6056]\n",
      "Weights: [-4.8617  0.8976 -1.3617  0.1694  0.1358]\n",
      "MSE loss: 86.1783\n",
      "Iteration: 182500\n",
      "Gradient: [  6.4417  -5.263  -10.3024 -41.8626 254.134 ]\n",
      "Weights: [-4.8515  0.8955 -1.3624  0.1691  0.1358]\n",
      "MSE loss: 86.0283\n",
      "Iteration: 182600\n",
      "Gradient: [ -11.9998   -6.6398   -4.2165  -84.4984 -205.8745]\n",
      "Weights: [-4.8524  0.8849 -1.3593  0.1692  0.1357]\n",
      "MSE loss: 86.2981\n",
      "Iteration: 182700\n",
      "Gradient: [ -8.3409  -2.3913 -56.8833  18.3     70.8994]\n",
      "Weights: [-4.8432  0.877  -1.3581  0.1699  0.1356]\n",
      "MSE loss: 86.1156\n",
      "Iteration: 182800\n",
      "Gradient: [-0.5135 19.041  -2.8282  6.351  11.874 ]\n",
      "Weights: [-4.8315  0.8791 -1.3557  0.1695  0.1356]\n",
      "MSE loss: 86.3264\n",
      "Iteration: 182900\n",
      "Gradient: [ -2.2279  -5.9153   8.4285 -66.6711 -20.9635]\n",
      "Weights: [-4.8458  0.869  -1.3525  0.1696  0.1356]\n",
      "MSE loss: 86.1867\n",
      "Iteration: 183000\n",
      "Gradient: [  1.255   -3.0905   4.6279 -35.3702 236.3841]\n",
      "Weights: [-4.832   0.8772 -1.3554  0.1695  0.1354]\n",
      "MSE loss: 86.1028\n",
      "Iteration: 183100\n",
      "Gradient: [  -4.1812   21.196   -25.9168    0.6977 -176.1796]\n",
      "Weights: [-4.8423  0.8881 -1.3586  0.1698  0.1356]\n",
      "MSE loss: 86.1526\n",
      "Iteration: 183200\n",
      "Gradient: [   2.2086  -20.0566   31.1545   74.8349 -159.4156]\n",
      "Weights: [-4.8546  0.8836 -1.3545  0.1695  0.1354]\n",
      "MSE loss: 86.1654\n",
      "Iteration: 183300\n",
      "Gradient: [ -2.2476   5.465  -41.126  -36.4931 213.9709]\n",
      "Weights: [-4.8367  0.8838 -1.3564  0.1693  0.1352]\n",
      "MSE loss: 86.1022\n",
      "Iteration: 183400\n",
      "Gradient: [  2.3232   2.6943   7.7142 -29.3143 -32.401 ]\n",
      "Weights: [-4.8408  0.8891 -1.3589  0.1698  0.1352]\n",
      "MSE loss: 86.0685\n",
      "Iteration: 183500\n",
      "Gradient: [ -8.544   -3.3872 -12.0718 -28.8796 -25.5872]\n",
      "Weights: [-4.8378  0.8818 -1.3569  0.1706  0.1352]\n",
      "MSE loss: 86.1089\n",
      "Iteration: 183600\n",
      "Gradient: [  2.9546  10.1095  -5.9381 142.8022  54.6739]\n",
      "Weights: [-4.8262  0.8766 -1.3591  0.1708  0.1354]\n",
      "MSE loss: 86.1261\n",
      "Iteration: 183700\n",
      "Gradient: [  8.763   -2.8406  64.0403  62.4138 274.0522]\n",
      "Weights: [-4.8103  0.8613 -1.3524  0.1694  0.1354]\n",
      "MSE loss: 86.4322\n",
      "Iteration: 183800\n",
      "Gradient: [   0.7693   -5.9137   45.5214   86.8868 -186.4563]\n",
      "Weights: [-4.8313  0.8727 -1.3552  0.1694  0.1355]\n",
      "MSE loss: 86.032\n",
      "Iteration: 183900\n",
      "Gradient: [ -1.0021   6.4574  27.2365 -83.3338 229.8444]\n",
      "Weights: [-4.8317  0.8765 -1.3578  0.1695  0.1357]\n",
      "MSE loss: 86.0296\n",
      "Iteration: 184000\n",
      "Gradient: [ -9.7727   1.9173  -6.5277  23.89   157.8532]\n",
      "Weights: [-4.8376  0.8735 -1.3564  0.1702  0.1356]\n",
      "MSE loss: 86.0154\n",
      "Iteration: 184100\n",
      "Gradient: [  3.2992  -9.1291  -6.897  -34.0004 332.6708]\n",
      "Weights: [-4.8258  0.867  -1.3552  0.1702  0.1356]\n",
      "MSE loss: 86.0701\n",
      "Iteration: 184200\n",
      "Gradient: [  -2.4287   -3.7981   -1.1933  -29.7722 -158.1227]\n",
      "Weights: [-4.831   0.8659 -1.3539  0.1702  0.1352]\n",
      "MSE loss: 86.0833\n",
      "Iteration: 184300\n",
      "Gradient: [  -7.0038    6.8645   22.994   -46.0933 -132.8383]\n",
      "Weights: [-4.8398  0.8662 -1.3545  0.1703  0.1355]\n",
      "MSE loss: 86.1605\n",
      "Iteration: 184400\n",
      "Gradient: [  1.9582  20.729   50.1737  54.1173 247.2957]\n",
      "Weights: [-4.823   0.8603 -1.3515  0.1704  0.1356]\n",
      "MSE loss: 86.3469\n",
      "Iteration: 184500\n",
      "Gradient: [ -1.9059 -17.3102   6.8806 -70.7184 287.1495]\n",
      "Weights: [-4.8357  0.8638 -1.3517  0.1702  0.1354]\n",
      "MSE loss: 86.0542\n",
      "Iteration: 184600\n",
      "Gradient: [  -2.9174    1.9116  -14.0548  132.1771 -259.5491]\n",
      "Weights: [-4.8363  0.8643 -1.3536  0.1715  0.1351]\n",
      "MSE loss: 86.0477\n",
      "Iteration: 184700\n",
      "Gradient: [   0.9689  -13.2543   59.899  -135.6382  -17.1769]\n",
      "Weights: [-4.8468  0.8717 -1.3568  0.1716  0.1351]\n",
      "MSE loss: 86.2853\n",
      "Iteration: 184800\n",
      "Gradient: [  -9.4145  -11.2908  -29.651    21.0239 -106.3219]\n",
      "Weights: [-4.8419  0.8755 -1.3594  0.1725  0.1351]\n",
      "MSE loss: 86.0129\n",
      "Iteration: 184900\n",
      "Gradient: [  -2.8415   24.0157   19.2512 -110.7987 -182.7875]\n",
      "Weights: [-4.8366  0.8883 -1.361   0.1722  0.1347]\n",
      "MSE loss: 86.1482\n",
      "Iteration: 185000\n",
      "Gradient: [  7.8858  -0.3894  -7.0117  35.5355 -32.9568]\n",
      "Weights: [-4.8325  0.8873 -1.363   0.1727  0.1348]\n",
      "MSE loss: 86.1773\n",
      "Iteration: 185100\n",
      "Gradient: [ -5.2438 -12.1762  18.1994  37.4331   9.9143]\n",
      "Weights: [-4.8439  0.894  -1.3656  0.1732  0.1347]\n",
      "MSE loss: 85.9965\n",
      "Iteration: 185200\n",
      "Gradient: [  9.3879  -5.3601   0.3619  12.1178 186.1291]\n",
      "Weights: [-4.8364  0.8867 -1.3671  0.1738  0.1347]\n",
      "MSE loss: 86.0084\n",
      "Iteration: 185300\n",
      "Gradient: [  3.8021  11.0611  27.9036 -18.8721  15.59  ]\n",
      "Weights: [-4.8379  0.8869 -1.3639  0.1744  0.1348]\n",
      "MSE loss: 86.3413\n",
      "Iteration: 185400\n",
      "Gradient: [  5.9561  12.1502 -24.052   12.5311 215.1277]\n",
      "Weights: [-4.835   0.8776 -1.3621  0.1742  0.1346]\n",
      "MSE loss: 85.9587\n",
      "Iteration: 185500\n",
      "Gradient: [  -5.0146   -9.7127   -8.      -87.7012 -172.3259]\n",
      "Weights: [-4.843   0.8777 -1.3639  0.1739  0.1346]\n",
      "MSE loss: 86.3787\n",
      "Iteration: 185600\n",
      "Gradient: [ -8.4175  -1.3749  10.3931 -30.5659 -74.4336]\n",
      "Weights: [-4.8336  0.8785 -1.3646  0.174   0.1346]\n",
      "MSE loss: 86.0944\n",
      "Iteration: 185700\n",
      "Gradient: [ -9.9397 -11.7655 -18.7895 -55.7609   3.972 ]\n",
      "Weights: [-4.8408  0.8833 -1.3663  0.175   0.1347]\n",
      "MSE loss: 85.9346\n",
      "Iteration: 185800\n",
      "Gradient: [   3.4173   -0.6398  -14.5021  -27.0886 -205.0198]\n",
      "Weights: [-4.8263  0.8758 -1.3654  0.1748  0.1347]\n",
      "MSE loss: 86.0038\n",
      "Iteration: 185900\n",
      "Gradient: [ -0.8452  -9.7854 -11.0334 -14.4988 112.9247]\n",
      "Weights: [-4.8472  0.875  -1.363   0.1753  0.1347]\n",
      "MSE loss: 86.1357\n",
      "Iteration: 186000\n",
      "Gradient: [ -7.1514   7.2238 -39.0456 -77.8068 -58.212 ]\n",
      "Weights: [-4.8388  0.8717 -1.3631  0.1752  0.1346]\n",
      "MSE loss: 86.0333\n",
      "Iteration: 186100\n",
      "Gradient: [ -6.5961  21.2949   7.0843 -51.9776   2.781 ]\n",
      "Weights: [-4.8337  0.8719 -1.3626  0.1755  0.1346]\n",
      "MSE loss: 85.9861\n",
      "Iteration: 186200\n",
      "Gradient: [ 13.5886  -3.8369  10.6266 -83.5537 162.7835]\n",
      "Weights: [-4.8284  0.8704 -1.3634  0.1755  0.1345]\n",
      "MSE loss: 85.9621\n",
      "Iteration: 186300\n",
      "Gradient: [  2.582   23.8295 -27.8165 127.2468 -14.682 ]\n",
      "Weights: [-4.8385  0.8663 -1.3626  0.1763  0.1344]\n",
      "MSE loss: 86.1237\n",
      "Iteration: 186400\n",
      "Gradient: [ -4.5472  11.6362   2.3904  37.173  -94.8392]\n",
      "Weights: [-4.8434  0.8635 -1.3629  0.1764  0.1345]\n",
      "MSE loss: 86.5723\n",
      "Iteration: 186500\n",
      "Gradient: [   3.7675  -10.938    47.6311   39.0276 -116.7229]\n",
      "Weights: [-4.8373  0.8719 -1.3653  0.1765  0.1343]\n",
      "MSE loss: 86.0943\n",
      "Iteration: 186600\n",
      "Gradient: [ -15.2587    2.8141    8.448  -106.1385 -136.9892]\n",
      "Weights: [-4.8363  0.8804 -1.3666  0.1754  0.1345]\n",
      "MSE loss: 85.9474\n",
      "Iteration: 186700\n",
      "Gradient: [   5.7169   25.0109   28.9476   24.5525 -163.6139]\n",
      "Weights: [-4.8417  0.8898 -1.3681  0.1752  0.1346]\n",
      "MSE loss: 85.9523\n",
      "Iteration: 186800\n",
      "Gradient: [  2.9512  10.6182   1.8421  73.0464 107.3871]\n",
      "Weights: [-4.8573  0.8909 -1.3638  0.1743  0.1345]\n",
      "MSE loss: 86.0993\n",
      "Iteration: 186900\n",
      "Gradient: [ -3.6163  -8.6547 -21.0994 -15.4933 278.697 ]\n",
      "Weights: [-4.8395  0.8818 -1.3635  0.1737  0.1345]\n",
      "MSE loss: 86.0963\n",
      "Iteration: 187000\n",
      "Gradient: [  0.2273  -0.4273 -20.4462 -51.7559   1.7215]\n",
      "Weights: [-4.8472  0.8939 -1.3653  0.173   0.1345]\n",
      "MSE loss: 86.1262\n",
      "Iteration: 187100\n",
      "Gradient: [ -8.3098  -1.3384 -20.5603  82.7186  30.9249]\n",
      "Weights: [-4.8563  0.8912 -1.363   0.1731  0.1345]\n",
      "MSE loss: 86.194\n",
      "Iteration: 187200\n",
      "Gradient: [  -8.34      6.817   -14.7751 -149.3278  -87.466 ]\n",
      "Weights: [-4.8556  0.8944 -1.364   0.1724  0.1347]\n",
      "MSE loss: 86.1956\n",
      "Iteration: 187300\n",
      "Gradient: [ 2.082000e-01 -1.057050e+01  3.582370e+01  4.829700e+00 -2.214899e+02]\n",
      "Weights: [-4.8358  0.8838 -1.3609  0.1728  0.1346]\n",
      "MSE loss: 86.0418\n",
      "Iteration: 187400\n",
      "Gradient: [ -0.2949   4.4623  37.726   -1.3334 -77.7588]\n",
      "Weights: [-4.843   0.8897 -1.3603  0.172   0.1346]\n",
      "MSE loss: 86.0588\n",
      "Iteration: 187500\n",
      "Gradient: [   2.8933   17.8276    2.5614  -42.0146 -140.8168]\n",
      "Weights: [-4.8347  0.8828 -1.3607  0.1722  0.1347]\n",
      "MSE loss: 86.0645\n",
      "Iteration: 187600\n",
      "Gradient: [  -6.2195    3.3124    3.0966  -41.6786 -138.2613]\n",
      "Weights: [-4.839   0.8787 -1.3581  0.1722  0.1349]\n",
      "MSE loss: 86.045\n",
      "Iteration: 187700\n",
      "Gradient: [ 12.542   19.2919   3.4542  52.2805 -26.43  ]\n",
      "Weights: [-4.8163  0.8722 -1.36    0.1728  0.1348]\n",
      "MSE loss: 86.3669\n",
      "Iteration: 187800\n",
      "Gradient: [  1.5176  -7.5761 -17.6013  42.933  -26.0555]\n",
      "Weights: [-4.8465  0.8776 -1.36    0.1731  0.1348]\n",
      "MSE loss: 86.0738\n",
      "Iteration: 187900\n",
      "Gradient: [  4.7092 -13.6048   3.6424  75.299  -47.6022]\n",
      "Weights: [-4.8357  0.873  -1.3594  0.1737  0.1347]\n",
      "MSE loss: 85.9848\n",
      "Iteration: 188000\n",
      "Gradient: [  4.6379  -8.8339  -0.4927  17.1235 -38.2832]\n",
      "Weights: [-4.8439  0.8838 -1.3605  0.1733  0.1345]\n",
      "MSE loss: 85.9872\n",
      "Iteration: 188100\n",
      "Gradient: [  -3.2914   -5.2841   21.4353  111.1848 -136.5458]\n",
      "Weights: [-4.8392  0.877  -1.3599  0.1737  0.1344]\n",
      "MSE loss: 86.0126\n",
      "Iteration: 188200\n",
      "Gradient: [   2.7337   11.0919  -35.7786   -9.9602 -173.6151]\n",
      "Weights: [-4.8507  0.8864 -1.3614  0.1732  0.1343]\n",
      "MSE loss: 86.2366\n",
      "Iteration: 188300\n",
      "Gradient: [ -5.2901  -2.9837 -26.4314 101.424  190.9342]\n",
      "Weights: [-4.8449  0.8889 -1.361   0.1734  0.1343]\n",
      "MSE loss: 86.041\n",
      "Iteration: 188400\n",
      "Gradient: [  -8.3539   11.0593  -53.3478  -13.7414 -129.7384]\n",
      "Weights: [-4.8431  0.8807 -1.3581  0.1722  0.1345]\n",
      "MSE loss: 86.0486\n",
      "Iteration: 188500\n",
      "Gradient: [ 8.96080e+00  6.64640e+00  4.96502e+01 -7.10000e-03 -3.97190e+00]\n",
      "Weights: [-4.8292  0.881  -1.3587  0.1729  0.1346]\n",
      "MSE loss: 86.47\n",
      "Iteration: 188600\n",
      "Gradient: [-2.502000e-01 -4.457100e+00  9.608300e+00 -7.161970e+01  2.576261e+02]\n",
      "Weights: [-4.8481  0.8861 -1.3607  0.1733  0.1347]\n",
      "MSE loss: 86.0482\n",
      "Iteration: 188700\n",
      "Gradient: [  -1.8055   -5.0731  -53.044   -11.2726 -180.1021]\n",
      "Weights: [-4.8411  0.8825 -1.3649  0.1738  0.1347]\n",
      "MSE loss: 86.0413\n",
      "Iteration: 188800\n",
      "Gradient: [   1.5672   -2.1698   -9.5712  -79.8893 -142.2646]\n",
      "Weights: [-4.8517  0.8886 -1.3628  0.1735  0.1349]\n",
      "MSE loss: 86.1375\n",
      "Iteration: 188900\n",
      "Gradient: [  5.2474   5.7015 -24.0886  50.5032   4.6452]\n",
      "Weights: [-4.8399  0.8929 -1.3657  0.1727  0.1348]\n",
      "MSE loss: 86.0171\n",
      "Iteration: 189000\n",
      "Gradient: [  0.3978  -4.3003   4.6461  32.027  226.6417]\n",
      "Weights: [-4.8543  0.898  -1.3675  0.1727  0.1351]\n",
      "MSE loss: 86.0235\n",
      "Iteration: 189100\n",
      "Gradient: [ -5.6373   6.5104 -20.2421 128.5874  73.4767]\n",
      "Weights: [-4.8336  0.8854 -1.3631  0.1727  0.1348]\n",
      "MSE loss: 86.0559\n",
      "Iteration: 189200\n",
      "Gradient: [  3.5066   4.3051  15.4161  18.8022 -68.5474]\n",
      "Weights: [-4.8419  0.8776 -1.362   0.1735  0.135 ]\n",
      "MSE loss: 85.9938\n",
      "Iteration: 189300\n",
      "Gradient: [-1.0126  1.9874 34.6226 73.2821 32.3602]\n",
      "Weights: [-4.8399  0.8798 -1.3633  0.1738  0.1353]\n",
      "MSE loss: 86.2877\n",
      "Iteration: 189400\n",
      "Gradient: [ 9.3626 11.6134  5.0459  8.6952 -6.1659]\n",
      "Weights: [-4.8314  0.8786 -1.363   0.1732  0.1351]\n",
      "MSE loss: 86.0731\n",
      "Iteration: 189500\n",
      "Gradient: [   5.3908  -13.5581  -31.2087  -13.488  -193.7542]\n",
      "Weights: [-4.8381  0.8862 -1.3663  0.1729  0.135 ]\n",
      "MSE loss: 85.996\n",
      "Iteration: 189600\n",
      "Gradient: [  1.3147  -3.6469 -25.0806  63.4908 -66.8705]\n",
      "Weights: [-4.8412  0.8907 -1.3672  0.1732  0.1352]\n",
      "MSE loss: 85.9699\n",
      "Iteration: 189700\n",
      "Gradient: [  2.4858 -23.5038   1.0544  42.0417 117.2315]\n",
      "Weights: [-4.8498  0.8888 -1.3655  0.1745  0.1349]\n",
      "MSE loss: 86.0718\n",
      "Iteration: 189800\n",
      "Gradient: [  4.9087 -15.6035 -42.7178  23.8566   8.8966]\n",
      "Weights: [-4.845   0.8938 -1.3661  0.1728  0.1349]\n",
      "MSE loss: 85.9773\n",
      "Iteration: 189900\n",
      "Gradient: [  1.5778 -11.5143  -3.5872 -81.7918 -10.0389]\n",
      "Weights: [-4.8452  0.8846 -1.3648  0.1731  0.1351]\n",
      "MSE loss: 85.9942\n",
      "Iteration: 190000\n",
      "Gradient: [  9.3566 -14.6754 -52.5015 -54.1042 260.5141]\n",
      "Weights: [-4.8483  0.8941 -1.3694  0.1731  0.1351]\n",
      "MSE loss: 86.0619\n",
      "Iteration: 190100\n",
      "Gradient: [  1.2458 -16.7504  17.785   -8.403   27.5636]\n",
      "Weights: [-4.84    0.8889 -1.3678  0.1736  0.135 ]\n",
      "MSE loss: 85.9406\n",
      "Iteration: 190200\n",
      "Gradient: [ -0.7028 -21.0577 -12.7037   1.7779  61.2487]\n",
      "Weights: [-4.8358  0.8812 -1.3685  0.1743  0.1351]\n",
      "MSE loss: 86.0009\n",
      "Iteration: 190300\n",
      "Gradient: [   9.8444    7.0136  -29.9374   51.8908 -158.7206]\n",
      "Weights: [-4.8286  0.878  -1.3674  0.1744  0.1353]\n",
      "MSE loss: 86.0303\n",
      "Iteration: 190400\n",
      "Gradient: [  -3.4341   13.0789   -4.9955   64.6434 -144.1214]\n",
      "Weights: [-4.8363  0.8847 -1.3709  0.1737  0.1354]\n",
      "MSE loss: 86.0391\n",
      "Iteration: 190500\n",
      "Gradient: [  2.0745  10.4029  13.5576 133.3461 -22.908 ]\n",
      "Weights: [-4.8485  0.9048 -1.3738  0.1734  0.1356]\n",
      "MSE loss: 86.0424\n",
      "Iteration: 190600\n",
      "Gradient: [  3.4899   3.9926  19.8704 -77.1704 -84.2725]\n",
      "Weights: [-4.8409  0.8979 -1.3737  0.1732  0.1355]\n",
      "MSE loss: 85.9298\n",
      "Iteration: 190700\n",
      "Gradient: [-11.0979  19.624   49.7658  98.3793  43.6671]\n",
      "Weights: [-4.8372  0.8958 -1.3752  0.1748  0.1355]\n",
      "MSE loss: 86.0148\n",
      "Iteration: 190800\n",
      "Gradient: [-15.2666 -19.858  -19.4694  93.5956  99.9314]\n",
      "Weights: [-4.8553  0.8953 -1.3776  0.1759  0.1354]\n",
      "MSE loss: 86.2876\n",
      "Iteration: 190900\n",
      "Gradient: [-13.9133  15.0409  10.4624 -39.8592 113.5132]\n",
      "Weights: [-4.8501  0.904  -1.3784  0.1751  0.1355]\n",
      "MSE loss: 85.944\n",
      "Iteration: 191000\n",
      "Gradient: [   3.1668    1.6463  -26.0335    3.0782 -102.8081]\n",
      "Weights: [-4.8465  0.903  -1.3803  0.1755  0.1355]\n",
      "MSE loss: 85.9121\n",
      "Iteration: 191100\n",
      "Gradient: [  -0.4203   11.783   -43.8304  -56.5644 -112.5486]\n",
      "Weights: [-4.8431  0.916  -1.3847  0.1749  0.1354]\n",
      "MSE loss: 85.9711\n",
      "Iteration: 191200\n",
      "Gradient: [  3.4796  -1.506  -31.3443  25.9808 -54.6633]\n",
      "Weights: [-4.8542  0.9167 -1.3812  0.1749  0.1355]\n",
      "MSE loss: 85.9653\n",
      "Iteration: 191300\n",
      "Gradient: [   8.2571  -12.6317  -12.6232  -72.1178 -123.9313]\n",
      "Weights: [-4.8496  0.9048 -1.3805  0.1756  0.1353]\n",
      "MSE loss: 85.9615\n",
      "Iteration: 191400\n",
      "Gradient: [  3.0362  14.6731   1.8726  30.3785 -59.1543]\n",
      "Weights: [-4.8346  0.9095 -1.3838  0.1755  0.1356]\n",
      "MSE loss: 86.0675\n",
      "Iteration: 191500\n",
      "Gradient: [  9.0367  15.9378  14.3633  32.7877 -77.1573]\n",
      "Weights: [-4.8521  0.9119 -1.3831  0.1749  0.1356]\n",
      "MSE loss: 85.9424\n",
      "Iteration: 191600\n",
      "Gradient: [  7.6846  -6.918  -56.5042  37.1919  88.5776]\n",
      "Weights: [-4.8577  0.9135 -1.3821  0.1747  0.1357]\n",
      "MSE loss: 85.9717\n",
      "Iteration: 191700\n",
      "Gradient: [ 13.0401  -9.2034  -0.6253  -9.9087 -63.4671]\n",
      "Weights: [-4.8436  0.9063 -1.3807  0.1743  0.1356]\n",
      "MSE loss: 85.9511\n",
      "Iteration: 191800\n",
      "Gradient: [-8.5329 -4.3407 11.9131 72.4392 96.5139]\n",
      "Weights: [-4.8628  0.9162 -1.3826  0.1749  0.1357]\n",
      "MSE loss: 86.0462\n",
      "Iteration: 191900\n",
      "Gradient: [ -3.7929 -10.1145   1.3748 -90.0481  20.2681]\n",
      "Weights: [-4.8539  0.9188 -1.3832  0.1747  0.1354]\n",
      "MSE loss: 85.9321\n",
      "Iteration: 192000\n",
      "Gradient: [ -5.4131 -23.0966 -25.6451 -59.8868 166.7073]\n",
      "Weights: [-4.8612  0.9189 -1.3827  0.1749  0.1354]\n",
      "MSE loss: 85.9823\n",
      "Iteration: 192100\n",
      "Gradient: [  2.8653  19.1676 -27.4174  26.727   14.8503]\n",
      "Weights: [-4.8474  0.9204 -1.3827  0.1741  0.1353]\n",
      "MSE loss: 86.0081\n",
      "Iteration: 192200\n",
      "Gradient: [  1.471   -6.1255 -25.4616 -27.7445  82.1192]\n",
      "Weights: [-4.8466  0.9229 -1.3856  0.1744  0.1354]\n",
      "MSE loss: 86.0546\n",
      "Iteration: 192300\n",
      "Gradient: [ 13.8159  -0.8618  34.4686 -15.4362 261.417 ]\n",
      "Weights: [-4.8477  0.9156 -1.3819  0.1752  0.1353]\n",
      "MSE loss: 85.968\n",
      "Iteration: 192400\n",
      "Gradient: [-1.007460e+01  1.116000e-01 -2.269620e+01  7.264970e+01 -1.589196e+02]\n",
      "Weights: [-4.8577  0.9189 -1.3819  0.1746  0.1352]\n",
      "MSE loss: 86.0267\n",
      "Iteration: 192500\n",
      "Gradient: [  0.4912  -2.4346  -9.2957  17.3214 222.5494]\n",
      "Weights: [-4.8481  0.9105 -1.3804  0.1754  0.1352]\n",
      "MSE loss: 85.8964\n",
      "Iteration: 192600\n",
      "Gradient: [  0.6243  15.2506 -16.9877 -75.0551  52.3625]\n",
      "Weights: [-4.8376  0.9053 -1.3793  0.1758  0.1349]\n",
      "MSE loss: 85.9835\n",
      "Iteration: 192700\n",
      "Gradient: [   0.6308    0.7039    1.8106   58.0144 -106.8213]\n",
      "Weights: [-4.8459  0.8962 -1.3769  0.1765  0.1352]\n",
      "MSE loss: 85.9445\n",
      "Iteration: 192800\n",
      "Gradient: [ -2.0966 -13.4294   2.9363 -23.1756 408.1091]\n",
      "Weights: [-4.8479  0.9042 -1.3782  0.1761  0.135 ]\n",
      "MSE loss: 85.9072\n",
      "Iteration: 192900\n",
      "Gradient: [  -4.1505    2.8834    6.946   -97.8998 -174.8843]\n",
      "Weights: [-4.8339  0.8986 -1.3829  0.177   0.1351]\n",
      "MSE loss: 85.9412\n",
      "Iteration: 193000\n",
      "Gradient: [ -2.261   -9.5747 -14.2001 205.023  318.3619]\n",
      "Weights: [-4.8291  0.8938 -1.3816  0.1776  0.1351]\n",
      "MSE loss: 85.9533\n",
      "Iteration: 193100\n",
      "Gradient: [ -3.1856  -3.9874 -32.5896 -74.7085 -98.5084]\n",
      "Weights: [-4.8342  0.8888 -1.3801  0.1775  0.135 ]\n",
      "MSE loss: 86.0083\n",
      "Iteration: 193200\n",
      "Gradient: [ -1.1559  11.8132 -29.5033   1.6532  -1.6733]\n",
      "Weights: [-4.8279  0.898  -1.3829  0.1778  0.135 ]\n",
      "MSE loss: 86.0402\n",
      "Iteration: 193300\n",
      "Gradient: [ -2.5985  15.5576  23.9431  54.2864 -37.5493]\n",
      "Weights: [-4.8282  0.8987 -1.384   0.1786  0.135 ]\n",
      "MSE loss: 86.0924\n",
      "Iteration: 193400\n",
      "Gradient: [  6.2581  14.7444  -5.5852   9.3264 171.8355]\n",
      "Weights: [-4.842   0.8983 -1.3849  0.1799  0.1347]\n",
      "MSE loss: 85.8679\n",
      "Iteration: 193500\n",
      "Gradient: [   3.4947   17.8899   -4.5425   -6.2224 -298.5336]\n",
      "Weights: [-4.8369  0.9011 -1.3868  0.1792  0.1349]\n",
      "MSE loss: 85.8589\n",
      "Iteration: 193600\n",
      "Gradient: [ -5.769  -12.5612 -45.6258 -18.4178 -10.1214]\n",
      "Weights: [-4.8461  0.9012 -1.3877  0.1798  0.135 ]\n",
      "MSE loss: 85.9537\n",
      "Iteration: 193700\n",
      "Gradient: [ -1.2123   8.3144 -37.2523  34.0046  82.0806]\n",
      "Weights: [-4.8378  0.8987 -1.3861  0.1794  0.1349]\n",
      "MSE loss: 85.8633\n",
      "Iteration: 193800\n",
      "Gradient: [ 12.6742  -0.2636   4.4035  96.0295 206.2975]\n",
      "Weights: [-4.8266  0.8953 -1.3851  0.1793  0.1348]\n",
      "MSE loss: 85.9596\n",
      "Iteration: 193900\n",
      "Gradient: [10.7859 23.8422 35.4859 -0.5351 52.6025]\n",
      "Weights: [-4.8252  0.8977 -1.3842  0.1786  0.1348]\n",
      "MSE loss: 86.0514\n",
      "Iteration: 194000\n",
      "Gradient: [ -8.2943  -8.8126  16.9303 -26.0731 123.7891]\n",
      "Weights: [-4.844   0.8982 -1.3865  0.179   0.1349]\n",
      "MSE loss: 86.2516\n",
      "Iteration: 194100\n",
      "Gradient: [   2.2047  -20.0454   -6.6089   63.0293 -159.0443]\n",
      "Weights: [-4.8319  0.8972 -1.3831  0.1785  0.1348]\n",
      "MSE loss: 85.9097\n",
      "Iteration: 194200\n",
      "Gradient: [ -4.1649  11.2022  24.9543  13.1177 -24.6049]\n",
      "Weights: [-4.8603  0.909  -1.3847  0.1788  0.1349]\n",
      "MSE loss: 86.0594\n",
      "Iteration: 194300\n",
      "Gradient: [  4.8772  10.2884  33.8903 -65.0231 209.676 ]\n",
      "Weights: [-4.8361  0.9037 -1.3848  0.1783  0.1351]\n",
      "MSE loss: 86.051\n",
      "Iteration: 194400\n",
      "Gradient: [ -1.9377   2.0826  47.877  -16.6477  73.2726]\n",
      "Weights: [-4.8273  0.8983 -1.3845  0.1776  0.1351]\n",
      "MSE loss: 85.9965\n",
      "Iteration: 194500\n",
      "Gradient: [ 1.2605 -0.517  -2.0774 19.5825  4.2568]\n",
      "Weights: [-4.8438  0.9019 -1.3824  0.1777  0.135 ]\n",
      "MSE loss: 85.8661\n",
      "Iteration: 194600\n",
      "Gradient: [ 7.4181 20.5511 12.5559 15.0215 19.9711]\n",
      "Weights: [-4.8318  0.9061 -1.3846  0.1782  0.1349]\n",
      "MSE loss: 86.1275\n",
      "Iteration: 194700\n",
      "Gradient: [  7.4588  -9.7021 -30.9308 -14.1427 263.6141]\n",
      "Weights: [-4.8392  0.9034 -1.3844  0.1781  0.1351]\n",
      "MSE loss: 85.8836\n",
      "Iteration: 194800\n",
      "Gradient: [  -4.3884    4.2054   -8.7182 -105.5231  190.4693]\n",
      "Weights: [-4.8412  0.9067 -1.3857  0.1782  0.1349]\n",
      "MSE loss: 85.8581\n",
      "Iteration: 194900\n",
      "Gradient: [   1.3196    9.5645  -53.16     80.8985 -136.2354]\n",
      "Weights: [-4.8624  0.9205 -1.3874  0.1782  0.1348]\n",
      "MSE loss: 85.9553\n",
      "Iteration: 195000\n",
      "Gradient: [ 3.2976 14.2514 24.773  27.4126 90.5859]\n",
      "Weights: [-4.8534  0.9201 -1.3905  0.1786  0.135 ]\n",
      "MSE loss: 85.843\n",
      "Iteration: 195100\n",
      "Gradient: [  -2.7619   18.1123   34.8048  -62.8716 -182.2725]\n",
      "Weights: [-4.8666  0.9255 -1.3889  0.1783  0.1351]\n",
      "MSE loss: 86.0407\n",
      "Iteration: 195200\n",
      "Gradient: [  9.0211  17.8446 -19.4796   9.6077 -55.5   ]\n",
      "Weights: [-4.8497  0.9161 -1.3889  0.1781  0.1352]\n",
      "MSE loss: 85.8525\n",
      "Iteration: 195300\n",
      "Gradient: [  7.7019  -1.0613   5.6632 -58.2722  17.8596]\n",
      "Weights: [-4.855   0.9237 -1.3896  0.1784  0.135 ]\n",
      "MSE loss: 85.8972\n",
      "Iteration: 195400\n",
      "Gradient: [  4.7408   5.3174  11.6824  71.2801 -54.4671]\n",
      "Weights: [-4.845   0.9248 -1.391   0.1791  0.1348]\n",
      "MSE loss: 86.137\n",
      "Iteration: 195500\n",
      "Gradient: [ -0.3465   6.4603  27.2941 -10.5323  52.4119]\n",
      "Weights: [-4.8514  0.9157 -1.3877  0.1792  0.1345]\n",
      "MSE loss: 85.8418\n",
      "Iteration: 195600\n",
      "Gradient: [  -0.968    -9.9951   25.2898   46.6381 -263.7859]\n",
      "Weights: [-4.8422  0.9074 -1.3888  0.1797  0.1347]\n",
      "MSE loss: 85.8561\n",
      "Iteration: 195700\n",
      "Gradient: [ -0.8256 -17.0848  44.3993 -55.771  352.5521]\n",
      "Weights: [-4.8523  0.9125 -1.3882  0.1802  0.1348]\n",
      "MSE loss: 85.9984\n",
      "Iteration: 195800\n",
      "Gradient: [ 11.5295 -11.0384  -8.4768  -0.7818 116.4033]\n",
      "Weights: [-4.853   0.9192 -1.3926  0.1804  0.1346]\n",
      "MSE loss: 85.8357\n",
      "Iteration: 195900\n",
      "Gradient: [ -6.8692  -5.6528  50.6595  27.7636 -10.6455]\n",
      "Weights: [-4.8423  0.9077 -1.392   0.1809  0.1348]\n",
      "MSE loss: 85.8322\n",
      "Iteration: 196000\n",
      "Gradient: [   7.6454    7.8569   34.9673  -91.7253 -242.0384]\n",
      "Weights: [-4.8406  0.916  -1.3918  0.1803  0.1347]\n",
      "MSE loss: 85.9626\n",
      "Iteration: 196100\n",
      "Gradient: [  -4.9811    3.0126   10.906    25.2851 -154.3153]\n",
      "Weights: [-4.8392  0.9226 -1.3948  0.1799  0.1346]\n",
      "MSE loss: 85.9673\n",
      "Iteration: 196200\n",
      "Gradient: [  -1.8431   -4.8473  -44.227    87.6468 -358.1331]\n",
      "Weights: [-4.8511  0.9262 -1.3965  0.1803  0.1348]\n",
      "MSE loss: 85.8095\n",
      "Iteration: 196300\n",
      "Gradient: [ -4.5881  11.8053   1.1416 -41.0162 -83.3632]\n",
      "Weights: [-4.8241  0.9191 -1.3956  0.1801  0.1349]\n",
      "MSE loss: 86.5474\n",
      "Iteration: 196400\n",
      "Gradient: [  -2.1643   -7.7622    2.5499   15.5916 -135.4836]\n",
      "Weights: [-4.8608  0.9217 -1.3973  0.1807  0.135 ]\n",
      "MSE loss: 86.1823\n",
      "Iteration: 196500\n",
      "Gradient: [  6.6883 -18.5059   7.089  -41.97   -55.5433]\n",
      "Weights: [-4.843   0.9277 -1.3965  0.1798  0.1349]\n",
      "MSE loss: 85.986\n",
      "Iteration: 196600\n",
      "Gradient: [-11.3941  -8.4158 -16.9934 -30.1375 -10.7096]\n",
      "Weights: [-4.8586  0.9283 -1.3972  0.1797  0.1349]\n",
      "MSE loss: 85.9868\n",
      "Iteration: 196700\n",
      "Gradient: [  -7.6223  -13.3498   23.7193  -82.8093 -103.2627]\n",
      "Weights: [-4.8481  0.9276 -1.3992  0.1797  0.135 ]\n",
      "MSE loss: 85.9712\n",
      "Iteration: 196800\n",
      "Gradient: [-10.3635   1.2252  -6.6223 -41.2187 335.0697]\n",
      "Weights: [-4.8644  0.9428 -1.3999  0.1795  0.135 ]\n",
      "MSE loss: 85.8711\n",
      "Iteration: 196900\n",
      "Gradient: [  2.731   -5.7176 -23.4318   1.9412  -7.771 ]\n",
      "Weights: [-4.8793  0.9439 -1.3988  0.1802  0.1348]\n",
      "MSE loss: 86.1179\n",
      "Iteration: 197000\n",
      "Gradient: [  6.896   -7.9688  12.9043 -13.0201 -23.2008]\n",
      "Weights: [-4.8663  0.9373 -1.4007  0.1794  0.1349]\n",
      "MSE loss: 86.5401\n",
      "Iteration: 197100\n",
      "Gradient: [ -2.1156   3.7333 -10.7306 -62.5388  19.6584]\n",
      "Weights: [-4.8635  0.9413 -1.4005  0.1801  0.1348]\n",
      "MSE loss: 85.8659\n",
      "Iteration: 197200\n",
      "Gradient: [ -3.3391   7.0956 -41.0417 -52.2989 -14.5327]\n",
      "Weights: [-4.8615  0.9326 -1.3966  0.1798  0.1348]\n",
      "MSE loss: 85.8593\n",
      "Iteration: 197300\n",
      "Gradient: [ 12.7461   7.2416  19.6106  39.732  138.9931]\n",
      "Weights: [-4.8478  0.932  -1.3971  0.1803  0.1349]\n",
      "MSE loss: 86.1433\n",
      "Iteration: 197400\n",
      "Gradient: [  5.606    6.3651  27.6944 -61.5987 -16.2532]\n",
      "Weights: [-4.8458  0.9282 -1.3958  0.1798  0.1348]\n",
      "MSE loss: 85.9166\n",
      "Iteration: 197500\n",
      "Gradient: [ -1.7469 -11.3268  14.5618 -47.3261  90.4235]\n",
      "Weights: [-4.8441  0.9092 -1.3915  0.1801  0.1349]\n",
      "MSE loss: 85.844\n",
      "Iteration: 197600\n",
      "Gradient: [ -6.2072 -14.4607   7.0239 -81.1609  80.9571]\n",
      "Weights: [-4.8437  0.9056 -1.3936  0.1807  0.1349]\n",
      "MSE loss: 86.0696\n",
      "Iteration: 197700\n",
      "Gradient: [  -7.1456   -7.5719  -17.7794  -18.1201 -221.9082]\n",
      "Weights: [-4.8375  0.9097 -1.3916  0.181   0.1346]\n",
      "MSE loss: 85.8755\n",
      "Iteration: 197800\n",
      "Gradient: [  4.5032  10.5664 -36.6582  66.0269 101.6482]\n",
      "Weights: [-4.8361  0.9161 -1.3932  0.1813  0.1345]\n",
      "MSE loss: 86.0721\n",
      "Iteration: 197900\n",
      "Gradient: [ -13.9256    0.3399   -8.4965  -21.8738 -232.5172]\n",
      "Weights: [-4.8607  0.9163 -1.3895  0.1808  0.1342]\n",
      "MSE loss: 85.9761\n",
      "Iteration: 198000\n",
      "Gradient: [  2.8734   3.1593   5.6955  83.3761 313.0376]\n",
      "Weights: [-4.8416  0.92   -1.3909  0.1804  0.1342]\n",
      "MSE loss: 86.0021\n",
      "Iteration: 198100\n",
      "Gradient: [-3.870000e-02  1.334300e+01  3.315560e+01 -1.319908e+02 -4.681570e+01]\n",
      "Weights: [-4.8524  0.9164 -1.3918  0.1807  0.1344]\n",
      "MSE loss: 85.8641\n",
      "Iteration: 198200\n",
      "Gradient: [ -5.9276 -10.5575 -49.4624 -33.39   195.1093]\n",
      "Weights: [-4.8591  0.9149 -1.3916  0.181   0.1345]\n",
      "MSE loss: 86.0345\n",
      "Iteration: 198300\n",
      "Gradient: [-12.7711   1.8109  15.2141 -46.7799 -29.9097]\n",
      "Weights: [-4.8613  0.9179 -1.391   0.1814  0.1344]\n",
      "MSE loss: 85.9369\n",
      "Iteration: 198400\n",
      "Gradient: [ -0.5293  -7.9423   5.4308 -30.1763 -65.1374]\n",
      "Weights: [-4.8456  0.9201 -1.3934  0.1812  0.1343]\n",
      "MSE loss: 85.8383\n",
      "Iteration: 198500\n",
      "Gradient: [   1.3121   15.8044   -1.8026  -70.7155 -176.7454]\n",
      "Weights: [-4.85    0.9189 -1.3942  0.181   0.1346]\n",
      "MSE loss: 85.806\n",
      "Iteration: 198600\n",
      "Gradient: [ -5.834   14.1556   2.924   -5.8866 -19.9571]\n",
      "Weights: [-4.8677  0.9276 -1.394   0.1803  0.1348]\n",
      "MSE loss: 86.0\n",
      "Iteration: 198700\n",
      "Gradient: [  -5.8262   -8.9986  -16.0595   28.0339 -310.3672]\n",
      "Weights: [-4.8578  0.9221 -1.3949  0.1799  0.1348]\n",
      "MSE loss: 86.1058\n",
      "Iteration: 198800\n",
      "Gradient: [  8.7675   5.7912  15.3044 -47.6924 123.0911]\n",
      "Weights: [-4.8439  0.9178 -1.3936  0.1803  0.1348]\n",
      "MSE loss: 85.8467\n",
      "Iteration: 198900\n",
      "Gradient: [ -2.8429  18.1401  18.474  -14.7198 125.1936]\n",
      "Weights: [-4.8465  0.9419 -1.4004  0.1798  0.1349]\n",
      "MSE loss: 86.3079\n",
      "Iteration: 199000\n",
      "Gradient: [ -3.6865  -4.7437 -33.0931  -2.3718 -70.1499]\n",
      "Weights: [-4.8548  0.94   -1.4037  0.1802  0.1351]\n",
      "MSE loss: 85.8361\n",
      "Iteration: 199100\n",
      "Gradient: [  2.0373  24.5152  -4.1762  41.6666 153.632 ]\n",
      "Weights: [-4.8718  0.9471 -1.4026  0.1804  0.1352]\n",
      "MSE loss: 85.9722\n",
      "Iteration: 199200\n",
      "Gradient: [  1.566    4.4963  13.6659 -26.0813 -90.2068]\n",
      "Weights: [-4.8534  0.9418 -1.4025  0.1797  0.1351]\n",
      "MSE loss: 85.8794\n",
      "Iteration: 199300\n",
      "Gradient: [  2.943   -7.6242  27.0244 -57.4897 180.7128]\n",
      "Weights: [-4.8501  0.93   -1.401   0.1808  0.1353]\n",
      "MSE loss: 85.9377\n",
      "Iteration: 199400\n",
      "Gradient: [ -6.7282 -17.6139 -45.7524  56.7943 110.6525]\n",
      "Weights: [-4.8505  0.9299 -1.4022  0.18    0.1355]\n",
      "MSE loss: 85.8205\n",
      "Iteration: 199500\n",
      "Gradient: [   6.0496    4.8975  -24.3886  171.0103 -143.9325]\n",
      "Weights: [-4.8444  0.9211 -1.4009  0.1801  0.1355]\n",
      "MSE loss: 85.8678\n",
      "Iteration: 199600\n",
      "Gradient: [  13.344    -6.3619  -15.4715    5.2285 -353.5978]\n",
      "Weights: [-4.868   0.9373 -1.4023  0.1796  0.1355]\n",
      "MSE loss: 86.0487\n",
      "Iteration: 199700\n",
      "Gradient: [ 2.8309  2.5703 -2.2772 11.8367 62.445 ]\n",
      "Weights: [-4.863   0.9488 -1.4024  0.1784  0.1355]\n",
      "MSE loss: 85.903\n",
      "Iteration: 199800\n",
      "Gradient: [  1.0123   9.5092  13.0355 -55.0991 110.7898]\n",
      "Weights: [-4.8624  0.9432 -1.4014  0.1787  0.1355]\n",
      "MSE loss: 85.8554\n",
      "Iteration: 199900\n",
      "Gradient: [ -15.649     1.0664   47.0347   58.9857 -470.2225]\n",
      "Weights: [-4.8675  0.946  -1.4032  0.1798  0.1353]\n",
      "MSE loss: 85.8877\n"
     ]
    }
   ],
   "source": [
    "weights_5, losses_5, iter_final_5, fit_time_5 = model_fit(X_train, y_train,\n",
    "                                                          learning_rate=[1e-3, 1e-4, 1e-5, 1e-6, 1e-7],\n",
    "                                                          tolerance=(0.2**2 * N_points),\n",
    "                                                          batch_ratio=0.1,\n",
    "                                                          beta=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45310014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Gradient: [ 3276.3593  4449.2347  7700.183  14530.9014 32023.0087]\n",
      "Weights: [-0.0008 -0.0001 -0.     -0.     -0.    ]\n",
      "MSE loss: 40431.4898\n",
      "Iteration: 100\n",
      "Gradient: [   -8.4616     7.7936   240.8192  -319.9867 -1193.3659]\n",
      "Weights: [-4.7874 -0.7348  0.1149  0.0848  0.042 ]\n",
      "MSE loss: 444.2081\n",
      "Iteration: 200\n",
      "Gradient: [ -15.0197    5.1756   51.9554  102.8173 -549.894 ]\n",
      "Weights: [-4.5304 -0.9762  0.0758  0.1001  0.0524]\n",
      "MSE loss: 298.4307\n",
      "Iteration: 300\n",
      "Gradient: [ -11.845    13.5649   57.2902 -102.8141 -569.1688]\n",
      "Weights: [-4.3721 -1.1449  0.0415  0.1079  0.0598]\n",
      "MSE loss: 237.6613\n",
      "Iteration: 400\n",
      "Gradient: [  1.0888  10.8448 -48.8712 -85.4495 241.408 ]\n",
      "Weights: [-4.2594 -1.2221  0.0151  0.1121  0.0645]\n",
      "MSE loss: 217.0077\n",
      "Iteration: 500\n",
      "Gradient: [ -13.8854   52.4613   74.6133  351.4121 -162.808 ]\n",
      "Weights: [-4.2453 -1.2365 -0.0079  0.1147  0.0673]\n",
      "MSE loss: 208.6468\n",
      "Iteration: 600\n",
      "Gradient: [   3.9225   17.4136   91.36    206.6976 -211.803 ]\n",
      "Weights: [-4.2146 -1.2351 -0.0312  0.1153  0.0699]\n",
      "MSE loss: 202.7859\n",
      "Iteration: 700\n",
      "Gradient: [  16.4684   45.7665   39.4513  236.4676 1221.0829]\n",
      "Weights: [-4.2065 -1.2076 -0.0487  0.1138  0.0717]\n",
      "MSE loss: 199.672\n",
      "Iteration: 800\n",
      "Gradient: [  30.1713   -2.4914   66.8886  -20.6998 -582.743 ]\n",
      "Weights: [-4.2116 -1.1892 -0.0672  0.1127  0.0734]\n",
      "MSE loss: 194.9973\n",
      "Iteration: 900\n",
      "Gradient: [ -21.8598   -1.6729 -129.3592 -255.6537 -616.743 ]\n",
      "Weights: [-4.2463 -1.1601 -0.0894  0.1115  0.0752]\n",
      "MSE loss: 192.1842\n",
      "Iteration: 1000\n",
      "Gradient: [ 22.5791  -6.8268  10.5423 -72.6888  16.3108]\n",
      "Weights: [-4.2241 -1.1315 -0.108   0.1127  0.0768]\n",
      "MSE loss: 187.691\n",
      "Iteration: 1100\n",
      "Gradient: [-25.7905  11.8608  35.0267 222.6623 -73.3331]\n",
      "Weights: [-4.234  -1.1136 -0.1249  0.1117  0.0782]\n",
      "MSE loss: 184.1066\n",
      "Iteration: 1200\n",
      "Gradient: [ 21.6422  20.645  -34.5739 180.9748 -79.4068]\n",
      "Weights: [-4.2513 -1.0659 -0.1412  0.1088  0.0795]\n",
      "MSE loss: 180.5054\n",
      "Iteration: 1300\n",
      "Gradient: [   4.885   -16.5413  -12.9551   19.2561 -366.9789]\n",
      "Weights: [-4.2666 -1.0615 -0.1482  0.107   0.0808]\n",
      "MSE loss: 179.2674\n",
      "Iteration: 1400\n",
      "Gradient: [ -3.7282  20.8862  67.9804 -78.2632  62.4552]\n",
      "Weights: [-4.2613 -1.0296 -0.1659  0.1062  0.0821]\n",
      "MSE loss: 175.5979\n",
      "Iteration: 1500\n",
      "Gradient: [ -0.5301  -4.4128 -20.1274   1.0975 351.3599]\n",
      "Weights: [-4.2768 -0.9983 -0.1828  0.1078  0.0827]\n",
      "MSE loss: 173.1962\n",
      "Iteration: 1600\n",
      "Gradient: [ -20.3113  -28.2136  -70.2187 -228.4557 -790.6342]\n",
      "Weights: [-4.2947 -0.9561 -0.2014  0.1052  0.0839]\n",
      "MSE loss: 169.9244\n",
      "Iteration: 1700\n",
      "Gradient: [ -11.5515  -24.0809   -4.8978   57.8162 -150.3148]\n",
      "Weights: [-4.2834 -0.9439 -0.2113  0.1041  0.0853]\n",
      "MSE loss: 167.8251\n",
      "Iteration: 1800\n",
      "Gradient: [ 14.5229 -25.1642  64.6478 -95.8296 -81.0141]\n",
      "Weights: [-4.2935 -0.9174 -0.2227  0.1017  0.0862]\n",
      "MSE loss: 165.656\n",
      "Iteration: 1900\n",
      "Gradient: [  -3.1004  -27.0241 -109.1624 -246.9803 -328.0809]\n",
      "Weights: [-4.2953 -0.9067 -0.2296  0.1006  0.0872]\n",
      "MSE loss: 164.163\n",
      "Iteration: 2000\n",
      "Gradient: [-10.3278 -16.4616 -15.8489 239.1774 253.8458]\n",
      "Weights: [-4.341  -0.8687 -0.2415  0.0997  0.0882]\n",
      "MSE loss: 162.1346\n",
      "Iteration: 2100\n",
      "Gradient: [ -15.7021  -28.224   -79.2359  -17.2306 -251.085 ]\n",
      "Weights: [-4.3172 -0.8486 -0.2617  0.0982  0.0898]\n",
      "MSE loss: 158.6955\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m weights_6, losses_6, iter_final_6, fit_time_6 = \u001b[43mmodel_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m                                                          \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1e-6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1e-7\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m                                                          \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_points\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m                                                          \u001b[49m\u001b[43mbatch_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m                                                          \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mmodel_fit\u001b[39m\u001b[34m(X_train, y_train, learning_rate, tolerance, batch_ratio, beta)\u001b[39m\n\u001b[32m     36\u001b[39m y_train_batch = [y_train[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m batch_indices]\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Формируем массив значений функции потерь, для вычисления градиента. Массив состоит из 3х элементов.\u001b[39;00m\n\u001b[32m     39\u001b[39m loss_func_grad = [loss_func(X_train_batch, y_train_batch, w_coeff  ),\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m                   \u001b[43mloss_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_coeff_1\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     41\u001b[39m                   loss_func(X_train_batch, y_train_batch, w_coeff_2)]\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Массив градиента состоит из 3х чисел с индексами [0, 1, 2]. Берем предпоследнее число.\u001b[39;00m\n\u001b[32m     43\u001b[39m grad[\u001b[32m0\u001b[39m,k] = np.gradient(loss_func_grad, w_step)[\u001b[32m1\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mloss_func\u001b[39m\u001b[34m(X, y, w_coeff)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mloss_func\u001b[39m(X, y, w_coeff):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.sum((y - \u001b[43mf_poly\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_coeff\u001b[49m\u001b[43m)\u001b[49m)**\u001b[32m2\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mf_poly\u001b[39m\u001b[34m(X, w_coeff)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m n, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(X):\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m idx, w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(w_coeff.tolist()[\u001b[32m0\u001b[39m]):\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m         Y[n] = Y[n] + w * x**idx\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Y\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "weights_6, losses_6, iter_final_6, fit_time_6 = model_fit(X_train, y_train,\n",
    "                                                          learning_rate=[1e-3, 1e-4, 1e-5, 1e-6, 1e-7],\n",
    "                                                          tolerance=(0.2**2 * N_points),\n",
    "                                                          batch_ratio=0.2,\n",
    "                                                          beta=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be7e7b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses_1, color='magenta', linestyle='--', linewidth=2)\n",
    "plt.plot(losses_2, color='orange', linestyle='-.', linewidth=2)\n",
    "plt.plot(losses_3, color='blue', linestyle='--', linewidth=2)\n",
    "plt.plot(losses_4, color='green', linestyle='-', linewidth=2)\n",
    "plt.plot(losses_5, color='brown', linestyle='--', linewidth=1)\n",
    "plt.plot(losses_6, color='red', linestyle=':', linewidth=2)\n",
    "plt.ylim(70,800)\n",
    "plt.legend(['lr=[5e-7, 5e-7, 5e-7, 5e-7, 5e-7], batch=1.0, beta=0.0',\n",
    "            'lr=[1e-5, 1e-6, 1e-7, 1e-8, 1e-9], batch=1.0, beta=0.0',\n",
    "            'lr=[1e-3, 1e-4, 1e-5, 1e-6, 1e-7], batch=0.1, beta=0.0',\n",
    "            'lr=[1e-3, 1e-4, 1e-5, 1e-6, 1e-7], batch=0.1, beta=0.8',\n",
    "            'lr=[1e-3, 1e-4, 1e-5, 1e-6, 1e-7], batch=0.1, beta=0.4',\n",
    "            'lr=[1e-3, 1e-4, 1e-5, 1e-6, 1e-7], batch=0.2, beta=0.0'])\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc74e00d",
   "metadata": {},
   "source": [
    "Кривые для batch=0.1, beta=[0.0, 0.4, 0.8] практически накладываются друг на друга. Следовательно, в рассмотренной частной задачи оптимизации параметр beta незначим. Фильтрация градиента (Momentum) не требуется."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11143885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "legend_list =  ['lr=[5e-7, 5e-7, 5e-7, 5e-7, 5e-7], batch=1.0, beta=0.0',\n",
    "                'lr=[1e-5, 1e-6, 1e-7, 1e-8, 1e-9], batch=1.0, beta=0.0',\n",
    "                'lr=[1e-3, 1e-4, 1e-5, 1e-6, 1e-7], batch=0.1, beta=0.0',\n",
    "                'lr=[1e-3, 1e-4, 1e-5, 1e-6, 1e-7], batch=0.2, beta=0.0']\n",
    "\n",
    "plt.plot(losses_1, color='magenta', linestyle='--', linewidth=2)\n",
    "plt.plot(losses_2, color='orange', linestyle='-.', linewidth=2)\n",
    "plt.plot(losses_3, color='blue', linestyle='--', linewidth=2)\n",
    "plt.plot(losses_6, color='red', linestyle=':', linewidth=2)\n",
    "plt.ylim(70,800)\n",
    "plt.legend(legend_list)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "#plt.savefig('plots/MSE_plot.png')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a399437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_train, y_train)\n",
    "plt.plot(np.sort(X_train), f_trend(np.sort(X_train)), color='yellow')\n",
    "plt.plot(np.sort(X_train), f_poly(np.sort(X_train), weights_1), color='magenta', linestyle='--', linewidth=2)\n",
    "plt.plot(np.sort(X_train), f_poly(np.sort(X_train), weights_2), color='orange', linestyle='-.', linewidth=2)\n",
    "plt.plot(np.sort(X_train), f_poly(np.sort(X_train), weights_3), color='blue', linestyle='--', linewidth=2)\n",
    "plt.plot(np.sort(X_train), f_poly(np.sort(X_train), weights_6), color='red', linestyle=':', linewidth=2)\n",
    "plt.legend(['train dataset', 'exact trend line'] + legend_list)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "#plt.savefig('plots/trend_line_plot.png')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f829b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fit_time = [fit_time_1, fit_time_2, fit_time_3, fit_time_6]\n",
    "bar_label = ['Opt. 1', 'Opt. 2', 'Opt. 3', 'Opt. 6']\n",
    "bar_colors = ['magenta', 'orange', 'blue', 'red']\n",
    "plt.bar(bar_label, fit_time, color=bar_colors)\n",
    "plt.ylabel('Time, sec.')\n",
    "#splt.savefig('plots/time_plot.png')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8df437f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17800.521367788315, 17925.368300437927, 3114.852860212326, 5091.532459259033]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit_time = [17800.521367788315, 17925.368300437927, 3114.852860212326, 5091.532459259033]\n",
    "fit_time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

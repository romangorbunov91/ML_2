{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "453d2819",
   "metadata": {},
   "source": [
    "# Градиентный спуск\n",
    "## Задача поиска оптимальных коэффициентов полиномиальной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3d6c553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from livelossplot import PlotLosses\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a770ef8",
   "metadata": {},
   "source": [
    "Формируем синтетический датасет на основе полиномиальной функции с добавлением случайного шума:\n",
    "\n",
    "${f(x) = \\sum_{k=0}^{K-1}{w_k \\cdot x^k} + \\epsilon}$,\n",
    "\n",
    "где ${w}$ - массив весов размера ${K}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05952c8",
   "metadata": {},
   "source": [
    "Определим полиномиальную функцию, в частном и в общем виде:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee933d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_trend(x):\n",
    "    return x**3 - 3*x**2 + 2*x - 5\n",
    "\n",
    "def f_poly(X, w_coeff):\n",
    "    Y = np.zeros_like(X)\n",
    "    for n, x in enumerate(X):\n",
    "        for idx, w in enumerate(w_coeff.tolist()[0]):\n",
    "            Y[n] = Y[n] + w * x**idx\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaf5f32",
   "metadata": {},
   "source": [
    "Сгенерируем синтетический датасет и построим графики."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37d9e5cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_min = 0\n",
    "x_max = 3\n",
    "N_points = 2000\n",
    "# Случайный набор x-ов.\n",
    "X_train = np.random.uniform(low=x_min, high=x_max, size=(N_points,))\n",
    "# Отклики с добавлением шума.\n",
    "y_train = f_trend(X_train) + np.random.normal(0,0.2,N_points)\n",
    "\n",
    "plt.scatter(X_train, y_train)\n",
    "plt.plot(np.sort(X_train), f_trend(np.sort(X_train)), color='red')\n",
    "plt.plot(np.sort(X_train), f_poly(np.sort(X_train), np.array([[-5, 2, -3, 1]])), '--', color='black')\n",
    "plt.legend(['train dataset', 'trend line', 'poly line'])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a4f4da",
   "metadata": {},
   "source": [
    "Функция ошибки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "858c2928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(X, y, w_coeff):\n",
    "    return np.sum((y - f_poly(X, w_coeff))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca17f1a9",
   "metadata": {},
   "source": [
    "Функция поиска параметров методом градиентного спуска."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "551da401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit(X_train, y_train, learning_rate, tolerance, beta, batch_ratio, lr_scaling, liveplot):\n",
    "    t_init = time.time()\n",
    "    if liveplot:\n",
    "        liveloss = PlotLosses()\n",
    "    batch_size = int(batch_ratio * len(X_train))\n",
    "    \n",
    "    iteration_max = 100000\n",
    "    # Коэффициенты (веса) инициилизируются нулями.\n",
    "    w_coeff = np.zeros((1,len(learning_rate)))\n",
    "    # Буфер коэффициентов.\n",
    "    w_coeff_buff = w_coeff.copy()\n",
    "    \n",
    "    # Градиенты инициализируются ненулевыми значениями.\n",
    "    grad = np.ones_like(w_coeff)\n",
    "    \n",
    "    # Аккумулятор для фильтра градиента.\n",
    "    grad_filter = np.zeros_like(grad)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    loss_filter = tolerance\n",
    "    # Масштабирующий коэффициент, обратно пропорциональный значению ошибки.\n",
    "    lr_scale = 1 / np.sqrt(1 + loss_filter)\n",
    "    \n",
    "    for i in range(iteration_max):\n",
    "        for k, w in enumerate(w_coeff.tolist()[0]):\n",
    "            # Накопление \"момента\".\n",
    "            grad_filter[0,k] = beta * grad_filter[0,k] + (1 - beta) * grad[0,k]\n",
    "            # Шаг градиентного спуска.\n",
    "            if lr_scaling:\n",
    "                w_step = - learning_rate[k] * lr_scale * grad_filter[0,k]\n",
    "            else:\n",
    "                w_step = - learning_rate[k] * grad_filter[0,k]\n",
    "            # Сбрасываем коэффициенты до необновленных значений.\n",
    "            w_coeff_1 = w_coeff.copy()\n",
    "            w_coeff_2 = w_coeff.copy()\n",
    "            # Модификация только k-го коэффициента:\n",
    "            w_coeff_1[0,k] = w + w_step\n",
    "            w_coeff_2[0,k] = w + 2*w_step\n",
    "\n",
    "            # Выборка случайных элементов (по индексам).\n",
    "            batch_indices = np.random.choice(len(X_train), size=batch_size, replace=True)\n",
    "            X_train_batch = [X_train[idx] for idx in batch_indices]\n",
    "            y_train_batch = [y_train[idx] for idx in batch_indices]\n",
    "            \n",
    "            # Формируем массив значений функции потерь, для вычисления градиента. Массив состоит из 3х элементов.\n",
    "            loss_func_grad = [loss_func(X_train_batch, y_train_batch, w_coeff  ),\n",
    "                              loss_func(X_train_batch, y_train_batch, w_coeff_1),\n",
    "                              loss_func(X_train_batch, y_train_batch, w_coeff_2)]\n",
    "            # Массив градиента состоит из 3х точек с индексами [0, 1, 2]. Берем предпоследнюю.\n",
    "            grad[0,k] = np.gradient(loss_func_grad, w_step)[1]\n",
    "            \n",
    "            # Обновление оного коэффициента в итоговом массиве.\n",
    "            w_coeff_buff[0,k] = w_coeff_1[0,k]\n",
    "        # Обновление всех коэффициентов.\n",
    "        w_coeff = w_coeff_buff.copy()\n",
    "        # Вычисление ошибки на полном датасете, при обновленных коэффициентах.\n",
    "        loss = loss_func(X_train, y_train, w_coeff)\n",
    "        if lr_scaling:\n",
    "            # Фильтр ошибки.\n",
    "            loss_filter = 0.6 * loss_filter + (1 - 0.6) * loss\n",
    "            lr_scale = 1 / np.sqrt(1 + loss_filter)\n",
    "        # Накопление ошибки в отдельный массив для дальнейшей визуализации.\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print('Iteration:', i)\n",
    "            print('Gradient:', np.round(grad[0],4))\n",
    "            print('Weights:', np.round(w_coeff[0],4))\n",
    "            print('MSE loss:', np.round(loss,4))\n",
    "            if liveplot:\n",
    "                if loss < 1000:\n",
    "                    liveloss.update({'MSE loss': loss})\n",
    "                    liveloss.draw()\n",
    "        if (loss < tolerance):\n",
    "            print('Tolerance reached')\n",
    "            break\n",
    "    iter_final = i\n",
    "    fit_time = time.time() - t_init\n",
    "    return w_coeff, losses, iter_final, fit_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "add33c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Gradient: [ 1672.2531  2246.4173  3958.2487  7058.4725 15402.4836]\n",
      "Weights: [-0. -0. -0. -0. -0.]\n",
      "MSE loss: 40788.3676\n",
      "Iteration: 100\n",
      "Gradient: [1355.3437 1455.1483 2630.8387 4866.6976 8043.6375]\n",
      "Weights: [-8.061e-01 -1.020e-01 -1.700e-02 -3.300e-03 -7.000e-04]\n",
      "MSE loss: 25923.8596\n",
      "Iteration: 200\n",
      "Gradient: [ 915.5299  861.3624 1228.5616 1763.9763 2245.3854]\n",
      "Weights: [-1.5845e+00 -1.8970e-01 -3.0100e-02 -5.4000e-03 -1.0000e-03]\n",
      "MSE loss: 15941.6381\n",
      "Iteration: 300\n",
      "Gradient: [  545.0633   430.1931   554.0144 -1237.7973 -2077.1514]\n",
      "Weights: [-2.2479e+00 -2.5140e-01 -3.6100e-02 -5.6000e-03 -8.0000e-04]\n",
      "MSE loss: 10468.3157\n",
      "Iteration: 400\n",
      "Gradient: [   346.1719    294.9181   -634.9538  -2527.0544 -11063.5848]\n",
      "Weights: [-2.7647e+00 -2.8410e-01 -3.4500e-02 -3.9000e-03 -1.0000e-04]\n",
      "MSE loss: 7760.0785\n",
      "Iteration: 500\n",
      "Gradient: [  237.7716  -130.1463  -964.0348 -3455.1009 -9194.9528]\n",
      "Weights: [-3.1455e+00 -2.8810e-01 -2.8000e-02 -9.0000e-04  9.0000e-04]\n",
      "MSE loss: 6339.264\n",
      "Iteration: 600\n",
      "Gradient: [  149.9473   -53.8412  -684.129  -3901.8617 -8645.4808]\n",
      "Weights: [-3.4216e+00 -2.7740e-01 -1.7000e-02  3.0000e-03  2.2000e-03]\n",
      "MSE loss: 5397.0254\n",
      "Iteration: 700\n",
      "Gradient: [   144.9185    -74.8826   -814.2916  -2521.5004 -11209.2241]\n",
      "Weights: [-3.6429e+00 -2.5680e-01 -5.1000e-03  7.7000e-03  3.6000e-03]\n",
      "MSE loss: 4628.0089\n",
      "Iteration: 800\n",
      "Gradient: [   150.4492    -66.6211   -958.7658  -3319.6961 -13480.8377]\n",
      "Weights: [-3.8332 -0.2337  0.0084  0.0123  0.005 ]\n",
      "MSE loss: 3959.0599\n",
      "Iteration: 900\n",
      "Gradient: [   95.9053  -102.7756 -1015.5105 -2935.1697 -7114.044 ]\n",
      "Weights: [-4.0096 -0.2085  0.0217  0.0169  0.0065]\n",
      "MSE loss: 3373.3222\n",
      "Iteration: 1000\n",
      "Gradient: [  154.6876   -83.6186 -1067.4105 -1518.4285 -9184.3545]\n",
      "Weights: [-4.1669 -0.1835  0.0337  0.0215  0.008 ]\n",
      "MSE loss: 2885.3073\n",
      "Iteration: 1100\n",
      "Gradient: [   22.2952  -197.2751  -258.1393 -3043.2875 -8168.1138]\n",
      "Weights: [-4.3136 -0.162   0.0463  0.026   0.0093]\n",
      "MSE loss: 2465.5456\n",
      "Iteration: 1200\n",
      "Gradient: [    82.0462    -73.7687   -472.7307  -1474.275  -10997.9823]\n",
      "Weights: [-4.4583 -0.1448  0.0579  0.0303  0.0107]\n",
      "MSE loss: 2113.7394\n",
      "Iteration: 1300\n",
      "Gradient: [   51.7544    -9.6489   -35.5395  -676.6353 -3866.0625]\n",
      "Weights: [-4.589  -0.1288  0.0684  0.0343  0.0121]\n",
      "MSE loss: 1832.1234\n",
      "Iteration: 1400\n",
      "Gradient: [   27.1481  -169.7298  -129.2882 -1217.5051 -6783.8154]\n",
      "Weights: [-4.6933 -0.1148  0.0774  0.0379  0.0133]\n",
      "MSE loss: 1617.3876\n",
      "Iteration: 1500\n",
      "Gradient: [   26.772    -91.8962  -453.032  -1864.3004 -3272.7737]\n",
      "Weights: [-4.7949 -0.1054  0.0858  0.0412  0.0145]\n",
      "MSE loss: 1446.1397\n",
      "Iteration: 1600\n",
      "Gradient: [   43.6533   -80.5612  -215.1319 -1202.3575 -4389.5791]\n",
      "Weights: [-4.8853 -0.0985  0.0935  0.0444  0.0156]\n",
      "MSE loss: 1311.8134\n",
      "Iteration: 1700\n",
      "Gradient: [   26.4611   -35.7954  -245.5431 -1431.3154 -3345.247 ]\n",
      "Weights: [-4.9517 -0.0946  0.0996  0.0473  0.0166]\n",
      "MSE loss: 1215.0724\n",
      "Iteration: 1800\n",
      "Gradient: [   32.9785    20.9531  -271.4457 -1208.8606 -4016.991 ]\n",
      "Weights: [-5.0143 -0.0955  0.1054  0.0498  0.0175]\n",
      "MSE loss: 1138.4568\n",
      "Iteration: 1900\n",
      "Gradient: [   14.5942   -12.9266   -90.2874  -593.89   -3093.6944]\n",
      "Weights: [-5.0702 -0.0989  0.1103  0.0521  0.0184]\n",
      "MSE loss: 1079.7908\n",
      "Iteration: 2000\n",
      "Gradient: [   19.8221    87.1866  -121.5558  -764.1572 -1584.4104]\n",
      "Weights: [-5.1026 -0.1027  0.1139  0.0542  0.0192]\n",
      "MSE loss: 1036.2502\n",
      "Iteration: 2100\n",
      "Gradient: [  -21.4007    -3.7205   -25.6565  -850.1403 -2144.7235]\n",
      "Weights: [-5.1342 -0.1112  0.1173  0.0562  0.0199]\n",
      "MSE loss: 999.3938\n",
      "Iteration: 2200\n",
      "Gradient: [  -21.1254   102.4003   -80.1614  -110.6416 -3111.4956]\n",
      "Weights: [-5.1595 -0.1204  0.1191  0.0578  0.0206]\n",
      "MSE loss: 971.2572\n",
      "Iteration: 2300\n",
      "Gradient: [-1.6140000e-01  3.1888300e+01 -6.6222500e+01 -3.8449070e+02\n",
      " -1.7963639e+03]\n",
      "Weights: [-5.1696 -0.1292  0.1204  0.0593  0.0213]\n",
      "MSE loss: 947.2107\n",
      "Iteration: 2400\n",
      "Gradient: [   30.5111    41.0823  -128.5431 -1033.6507  -822.6241]\n",
      "Weights: [-5.1799 -0.1405  0.1214  0.0607  0.0219]\n",
      "MSE loss: 925.8457\n",
      "Iteration: 2500\n",
      "Gradient: [   12.7434    22.7181    22.4154  -245.7745 -1406.8271]\n",
      "Weights: [-5.1858 -0.1534  0.122   0.062   0.0225]\n",
      "MSE loss: 906.2942\n",
      "Iteration: 2600\n",
      "Gradient: [  31.2684   -3.5755   36.3772 -292.2197 -539.4342]\n",
      "Weights: [-5.1835 -0.1664  0.1223  0.0632  0.023 ]\n",
      "MSE loss: 888.4071\n",
      "Iteration: 2700\n",
      "Gradient: [  -9.2588   99.7138   49.4324 -158.9307 -723.2816]\n",
      "Weights: [-5.1838 -0.1816  0.1225  0.0643  0.0235]\n",
      "MSE loss: 870.6569\n",
      "Iteration: 2800\n",
      "Gradient: [ 5.6440000e-01  8.2574900e+01 -1.0673210e+02 -2.0957990e+02\n",
      " -2.2292808e+03]\n",
      "Weights: [-5.1764 -0.1958  0.1228  0.0652  0.0241]\n",
      "MSE loss: 853.504\n",
      "Iteration: 2900\n",
      "Gradient: [  -15.6476    91.6502  -227.9467  -527.9067 -2686.559 ]\n",
      "Weights: [-5.174  -0.2094  0.1231  0.0664  0.0246]\n",
      "MSE loss: 836.8797\n",
      "Iteration: 3000\n",
      "Gradient: [ -33.6092   53.4589  150.5547 -282.5842 -988.6012]\n",
      "Weights: [-5.167  -0.2247  0.123   0.0674  0.0251]\n",
      "MSE loss: 820.6441\n",
      "Iteration: 3100\n",
      "Gradient: [-1.7122000e+00  8.7051300e+01  8.3086500e+01 -3.7748610e+02\n",
      " -2.2963055e+03]\n",
      "Weights: [-5.1553 -0.2386  0.1222  0.0683  0.0255]\n",
      "MSE loss: 806.5242\n",
      "Iteration: 3200\n",
      "Gradient: [   16.4948    74.0584   205.802   -215.9942 -1434.6218]\n",
      "Weights: [-5.1398 -0.2535  0.1216  0.0691  0.0259]\n",
      "MSE loss: 791.934\n",
      "Iteration: 3300\n",
      "Gradient: [ -27.8864    2.6477   13.8456 -603.9207 -884.8705]\n",
      "Weights: [-5.1381 -0.2676  0.1209  0.0699  0.0264]\n",
      "MSE loss: 777.6007\n",
      "Iteration: 3400\n",
      "Gradient: [   2.9715   38.9179  223.4565  -13.7684 -836.1379]\n",
      "Weights: [-5.1243 -0.2839  0.1202  0.0708  0.0269]\n",
      "MSE loss: 762.6288\n",
      "Iteration: 3500\n",
      "Gradient: [   1.4762   68.1859   55.2075   23.5604 -839.0728]\n",
      "Weights: [-5.1093 -0.2986  0.1193  0.0717  0.0273]\n",
      "MSE loss: 747.9526\n",
      "Iteration: 3600\n",
      "Gradient: [  15.7105  122.2531   50.6469 -523.0478 -574.9659]\n",
      "Weights: [-5.0975 -0.3136  0.1185  0.0726  0.0278]\n",
      "MSE loss: 733.5277\n",
      "Iteration: 3700\n",
      "Gradient: [   -2.7276    73.4948    14.1494  -339.4018 -1342.5714]\n",
      "Weights: [-5.077  -0.3294  0.1171  0.0735  0.0282]\n",
      "MSE loss: 719.3152\n",
      "Iteration: 3800\n",
      "Gradient: [   11.3667    43.0645    40.1462  -715.8712 -1080.4866]\n",
      "Weights: [-5.0727 -0.3453  0.1162  0.0741  0.0287]\n",
      "MSE loss: 706.1531\n",
      "Iteration: 3900\n",
      "Gradient: [  -12.6634    67.8125   -37.2025  -670.2935 -1232.1268]\n",
      "Weights: [-5.0599 -0.3584  0.1158  0.0749  0.0291]\n",
      "MSE loss: 693.3338\n",
      "Iteration: 4000\n",
      "Gradient: [   45.606     49.2588    57.3037  -125.7508 -1445.7439]\n",
      "Weights: [-5.0561 -0.3719  0.1155  0.0757  0.0296]\n",
      "MSE loss: 680.9176\n",
      "Iteration: 4100\n",
      "Gradient: [   -8.7368    -9.5944   -44.9119  -730.8576 -1569.8607]\n",
      "Weights: [-5.0442 -0.3868  0.1146  0.0764  0.03  ]\n",
      "MSE loss: 668.7913\n",
      "Iteration: 4200\n",
      "Gradient: [  -21.7261    39.3317    12.8449  -267.5006 -1004.0193]\n",
      "Weights: [-5.0271 -0.4027  0.1145  0.0771  0.0305]\n",
      "MSE loss: 655.3407\n",
      "Iteration: 4300\n",
      "Gradient: [ -33.5457   37.9239  229.6173   30.8069 -576.6505]\n",
      "Weights: [-5.016  -0.4146  0.1136  0.0777  0.0309]\n",
      "MSE loss: 644.3353\n",
      "Iteration: 4400\n",
      "Gradient: [   28.2569    17.893     66.1854   177.083  -1574.9975]\n",
      "Weights: [-5.0077 -0.4277  0.1127  0.0785  0.0314]\n",
      "MSE loss: 632.7194\n",
      "Iteration: 4500\n",
      "Gradient: [  -23.9344   -20.2559    -7.9081  -234.7919 -1191.6043]\n",
      "Weights: [-4.9993 -0.4401  0.1117  0.0792  0.0318]\n",
      "MSE loss: 621.8464\n",
      "Iteration: 4600\n",
      "Gradient: [  25.7138    3.5433  -97.4645 -274.745  -139.6961]\n",
      "Weights: [-4.9929 -0.4518  0.1109  0.08    0.0322]\n",
      "MSE loss: 611.4627\n",
      "Iteration: 4700\n",
      "Gradient: [-4.996000e-01  1.935450e+01  9.034400e+00 -6.877916e+02  3.395441e+02]\n",
      "Weights: [-4.9765 -0.4664  0.1098  0.0809  0.0327]\n",
      "MSE loss: 599.6449\n",
      "Iteration: 4800\n",
      "Gradient: [ -37.005    59.2602   79.0798  -76.112  -941.3899]\n",
      "Weights: [-4.9684 -0.4827  0.1092  0.0817  0.0331]\n",
      "MSE loss: 587.8601\n",
      "Iteration: 4900\n",
      "Gradient: [ 9.3100000e-01  3.1108300e+01  1.0347750e+02 -5.1152400e+01\n",
      " -1.5492969e+03]\n",
      "Weights: [-4.9577 -0.4962  0.1081  0.0823  0.0335]\n",
      "MSE loss: 577.4479\n",
      "Iteration: 5000\n",
      "Gradient: [  -2.0156   20.1888  -12.1041   27.5986 -980.9297]\n",
      "Weights: [-4.9424 -0.5099  0.1078  0.083   0.0339]\n",
      "MSE loss: 567.2898\n",
      "Iteration: 5100\n",
      "Gradient: [  37.823    34.701  -152.4268  -21.6217 -288.6965]\n",
      "Weights: [-4.9299 -0.5232  0.1072  0.0837  0.0343]\n",
      "MSE loss: 557.2895\n",
      "Iteration: 5200\n",
      "Gradient: [  -24.0738    21.3182   -27.3153   -94.8611 -2045.3753]\n",
      "Weights: [-4.913  -0.5372  0.1062  0.0845  0.0346]\n",
      "MSE loss: 547.168\n",
      "Iteration: 5300\n",
      "Gradient: [   16.9407    93.1865    54.4879  -458.7864 -1010.0897]\n",
      "Weights: [-4.8988 -0.5517  0.1052  0.0852  0.0351]\n",
      "MSE loss: 536.8267\n",
      "Iteration: 5400\n",
      "Gradient: [   9.4674    2.6489  -26.5174  124.756  -218.8285]\n",
      "Weights: [-4.8959 -0.5663  0.1041  0.0858  0.0355]\n",
      "MSE loss: 526.9831\n",
      "Iteration: 5500\n",
      "Gradient: [    9.4776    15.8146  -120.7017  -278.9473 -1049.0833]\n",
      "Weights: [-4.8812 -0.5781  0.1036  0.0865  0.0359]\n",
      "MSE loss: 517.882\n",
      "Iteration: 5600\n",
      "Gradient: [  -13.3839    74.0179    70.8457  -114.2969 -1351.0357]\n",
      "Weights: [-4.8683 -0.5918  0.1031  0.0871  0.0363]\n",
      "MSE loss: 508.7711\n",
      "Iteration: 5700\n",
      "Gradient: [    9.1531    67.8695   -17.4971  -134.414  -2112.6244]\n",
      "Weights: [-4.8582 -0.6063  0.1025  0.0879  0.0367]\n",
      "MSE loss: 499.2545\n",
      "Iteration: 5800\n",
      "Gradient: [  -3.9466   31.6156  -39.2796  106.1419 -186.2211]\n",
      "Weights: [-4.8455 -0.6201  0.1021  0.0886  0.037 ]\n",
      "MSE loss: 490.6048\n",
      "Iteration: 5900\n",
      "Gradient: [    3.0398    -5.999    114.082   -203.5435 -1579.764 ]\n",
      "Weights: [-4.8354 -0.6323  0.1011  0.0893  0.0374]\n",
      "MSE loss: 482.2338\n",
      "Iteration: 6000\n",
      "Gradient: [  21.4531  -28.0002  -13.7385 -163.4082 -729.8033]\n",
      "Weights: [-4.8274 -0.6448  0.1003  0.0899  0.0378]\n",
      "MSE loss: 474.15\n",
      "Iteration: 6100\n",
      "Gradient: [   -3.7187    48.0891   -36.302   -100.5052 -1333.5184]\n",
      "Weights: [-4.8208 -0.6567  0.0998  0.0906  0.0382]\n",
      "MSE loss: 466.01\n",
      "Iteration: 6200\n",
      "Gradient: [   1.937    22.0348  112.6762 -468.0486 -537.3889]\n",
      "Weights: [-4.8114 -0.6664  0.0986  0.0912  0.0386]\n",
      "MSE loss: 458.8661\n",
      "Iteration: 6300\n",
      "Gradient: [  -7.4361   -9.865  -109.539  -296.4913  318.5025]\n",
      "Weights: [-4.8005 -0.679   0.0978  0.0919  0.0389]\n",
      "MSE loss: 451.2255\n",
      "Iteration: 6400\n",
      "Gradient: [-2.604000e-01 -3.793400e+00  2.606670e+01 -2.793691e+02 -3.016836e+02]\n",
      "Weights: [-4.781  -0.6923  0.0966  0.0923  0.0393]\n",
      "MSE loss: 443.3528\n",
      "Iteration: 6500\n",
      "Gradient: [ 6.486000e-01  1.655060e+01 -1.824920e+01 -4.176322e+02 -6.902862e+02]\n",
      "Weights: [-4.7717 -0.705   0.0964  0.0929  0.0397]\n",
      "MSE loss: 435.9152\n",
      "Iteration: 6600\n",
      "Gradient: [ -18.4687   48.1866  -44.3448 -415.8672 -560.3965]\n",
      "Weights: [-4.7598 -0.7183  0.0955  0.0936  0.04  ]\n",
      "MSE loss: 428.7026\n",
      "Iteration: 6700\n",
      "Gradient: [   9.4256   27.6492    6.2553  -15.9411 -659.8014]\n",
      "Weights: [-4.7478 -0.7312  0.0952  0.0942  0.0404]\n",
      "MSE loss: 421.5581\n",
      "Iteration: 6800\n",
      "Gradient: [ -10.5746   23.9128  145.0672 -155.8495 -751.3632]\n",
      "Weights: [-4.7352 -0.744   0.0941  0.0949  0.0407]\n",
      "MSE loss: 414.4501\n",
      "Iteration: 6900\n",
      "Gradient: [ 6.3670000e-01  7.8936000e+00  1.0568830e+02 -6.3311620e+02\n",
      " -1.1235933e+03]\n",
      "Weights: [-4.7236 -0.7558  0.0929  0.0957  0.0411]\n",
      "MSE loss: 407.2747\n",
      "Iteration: 7000\n",
      "Gradient: [  -13.6641    20.4009    57.1332    -3.2457 -1670.7099]\n",
      "Weights: [-4.718  -0.769   0.0921  0.0964  0.0414]\n",
      "MSE loss: 400.4003\n",
      "Iteration: 7100\n",
      "Gradient: [   -5.0689   -25.1722   100.4524   181.9857 -1184.5741]\n",
      "Weights: [-4.708  -0.7807  0.092   0.0969  0.0418]\n",
      "MSE loss: 394.2924\n",
      "Iteration: 7200\n",
      "Gradient: [  -29.6561    30.9379    45.3342  -189.6751 -1331.211 ]\n",
      "Weights: [-4.6929 -0.7918  0.0913  0.0975  0.0421]\n",
      "MSE loss: 388.1098\n",
      "Iteration: 7300\n",
      "Gradient: [  -11.1741    40.7218   115.1107    89.0924 -1295.6535]\n",
      "Weights: [-4.6858 -0.8031  0.0897  0.0981  0.0425]\n",
      "MSE loss: 382.1288\n",
      "Iteration: 7400\n",
      "Gradient: [ -11.1366   20.1798   77.5892  -78.2028 -775.9768]\n",
      "Weights: [-4.6783 -0.8142  0.0889  0.0987  0.0429]\n",
      "MSE loss: 376.0283\n",
      "Iteration: 7500\n",
      "Gradient: [-18.4667  44.7613 -76.7601  29.5827 381.1559]\n",
      "Weights: [-4.6708 -0.8249  0.0889  0.0992  0.0432]\n",
      "MSE loss: 370.668\n",
      "Iteration: 7600\n",
      "Gradient: [  -15.2569    24.5672    29.6089   -24.3989 -1410.3544]\n",
      "Weights: [-4.6611 -0.8357  0.0877  0.0996  0.0436]\n",
      "MSE loss: 365.0221\n",
      "Iteration: 7700\n",
      "Gradient: [   -8.0247    26.7644    87.3389  -154.8719 -2109.9626]\n",
      "Weights: [-4.6513 -0.8473  0.0873  0.1002  0.0439]\n",
      "MSE loss: 359.7585\n",
      "Iteration: 7800\n",
      "Gradient: [  23.9509    9.8509  -45.1453 -115.7941 -773.6129]\n",
      "Weights: [-4.6408 -0.8578  0.0867  0.1008  0.0442]\n",
      "MSE loss: 354.4489\n",
      "Iteration: 7900\n",
      "Gradient: [   8.5714  -13.2306   12.4624  -45.2039 -264.1827]\n",
      "Weights: [-4.6362 -0.8678  0.0865  0.1014  0.0446]\n",
      "MSE loss: 349.5624\n",
      "Iteration: 8000\n",
      "Gradient: [    2.1082    14.3195   -16.5934   -99.6724 -1351.2968]\n",
      "Weights: [-4.6238 -0.8769  0.0853  0.1019  0.0449]\n",
      "MSE loss: 344.648\n",
      "Iteration: 8100\n",
      "Gradient: [-6.261000e-01  3.820550e+01 -3.505270e+01  2.618848e+02 -6.428473e+02]\n",
      "Weights: [-4.611  -0.8889  0.084   0.1025  0.0452]\n",
      "MSE loss: 339.5719\n",
      "Iteration: 8200\n",
      "Gradient: [  -0.9379  -37.8311   14.1548  -32.393  -649.2035]\n",
      "Weights: [-4.6059 -0.9     0.0838  0.1029  0.0456]\n",
      "MSE loss: 334.9017\n",
      "Iteration: 8300\n",
      "Gradient: [  -8.0401    2.4229  -17.1326  -79.8943 -686.0426]\n",
      "Weights: [-4.5983 -0.9097  0.0828  0.1035  0.046 ]\n",
      "MSE loss: 330.0433\n",
      "Iteration: 8400\n",
      "Gradient: [ 27.7452  10.7759  29.6319 -70.5547  33.7233]\n",
      "Weights: [-4.5887 -0.9187  0.0812  0.1041  0.0463]\n",
      "MSE loss: 325.6198\n",
      "Iteration: 8500\n",
      "Gradient: [  26.2772   43.7689   63.7587  -18.1369 -267.8409]\n",
      "Weights: [-4.5809 -0.9291  0.0802  0.1046  0.0466]\n",
      "MSE loss: 321.3005\n",
      "Iteration: 8600\n",
      "Gradient: [ -12.9014   15.8988  -81.0225  142.8194 -523.6288]\n",
      "Weights: [-4.5655 -0.9379  0.0794  0.1049  0.0469]\n",
      "MSE loss: 317.3647\n",
      "Iteration: 8700\n",
      "Gradient: [ -3.8488  -0.3892  65.6803  10.6577 101.4049]\n",
      "Weights: [-4.5538 -0.9477  0.0785  0.1052  0.0473]\n",
      "MSE loss: 313.3377\n",
      "Iteration: 8800\n",
      "Gradient: [   16.0669    -1.5044   -44.1459   -45.7407 -1081.7499]\n",
      "Weights: [-4.5489 -0.9578  0.0776  0.1056  0.0476]\n",
      "MSE loss: 309.349\n",
      "Iteration: 8900\n",
      "Gradient: [   3.9074   14.0308  116.649   -98.474  -311.4151]\n",
      "Weights: [-4.5419 -0.9677  0.0768  0.1062  0.0479]\n",
      "MSE loss: 305.4375\n",
      "Iteration: 9000\n",
      "Gradient: [   2.0167   20.1259   26.4417  -38.673  -999.021 ]\n",
      "Weights: [-4.5305 -0.9764  0.076   0.1067  0.0482]\n",
      "MSE loss: 301.825\n",
      "Iteration: 9100\n",
      "Gradient: [ -12.9864   65.8823    0.875  -103.8362 -540.5617]\n",
      "Weights: [-4.5271 -0.9856  0.0754  0.1071  0.0485]\n",
      "MSE loss: 298.4257\n",
      "Iteration: 9200\n",
      "Gradient: [  14.2834   18.9445  -10.1235  -43.6752 -500.6469]\n",
      "Weights: [-4.5165 -0.9935  0.0744  0.1077  0.0488]\n",
      "MSE loss: 295.0003\n",
      "Iteration: 9300\n",
      "Gradient: [   4.9082  -12.472   -20.1845 -237.8403 -389.44  ]\n",
      "Weights: [-4.5156 -1.0024  0.0735  0.1082  0.0491]\n",
      "MSE loss: 291.8178\n",
      "Iteration: 9400\n",
      "Gradient: [ -11.9453   18.1237   34.9113  -73.9392 -185.9491]\n",
      "Weights: [-4.509  -1.0096  0.073   0.1086  0.0494]\n",
      "MSE loss: 288.7456\n",
      "Iteration: 9500\n",
      "Gradient: [   2.2707    9.5647  -82.768  -214.9556  -27.7545]\n",
      "Weights: [-4.4975 -1.0176  0.0719  0.1091  0.0496]\n",
      "MSE loss: 285.8423\n",
      "Iteration: 9600\n",
      "Gradient: [  -6.74      9.3764   33.6426 -330.5508 -432.4356]\n",
      "Weights: [-4.4818 -1.0278  0.0708  0.1094  0.0499]\n",
      "MSE loss: 282.7713\n",
      "Iteration: 9700\n",
      "Gradient: [ -14.6054   19.0229   -5.5561 -236.5475  116.4647]\n",
      "Weights: [-4.4848 -1.0354  0.0701  0.1098  0.0502]\n",
      "MSE loss: 280.0017\n",
      "Iteration: 9800\n",
      "Gradient: [  -7.321     8.8366   -6.5817 -179.0421 -724.5308]\n",
      "Weights: [-4.4727 -1.0429  0.0693  0.1103  0.0505]\n",
      "MSE loss: 277.207\n",
      "Iteration: 9900\n",
      "Gradient: [  21.1489   11.8465  -59.6113  -42.8641 -298.1032]\n",
      "Weights: [-4.4638 -1.0514  0.0686  0.1108  0.0508]\n",
      "MSE loss: 274.3725\n",
      "Iteration: 10000\n",
      "Gradient: [   5.937    -0.6908   15.0765 -130.2112    5.305 ]\n",
      "Weights: [-4.4571 -1.0585  0.0676  0.1113  0.051 ]\n",
      "MSE loss: 271.8075\n",
      "Iteration: 10100\n",
      "Gradient: [   8.1303   18.6913   26.8404 -262.4535 -422.4806]\n",
      "Weights: [-4.4512 -1.0678  0.0664  0.1117  0.0513]\n",
      "MSE loss: 269.303\n",
      "Iteration: 10200\n",
      "Gradient: [   9.6864   -6.0064  -14.5017   77.1154 -806.3906]\n",
      "Weights: [-4.4443 -1.0751  0.0657  0.1119  0.0515]\n",
      "MSE loss: 267.0429\n",
      "Iteration: 10300\n",
      "Gradient: [ -12.8395    7.9605    0.5591 -279.5689 -128.208 ]\n",
      "Weights: [-4.4392 -1.0805  0.0653  0.1124  0.0518]\n",
      "MSE loss: 264.9408\n",
      "Iteration: 10400\n",
      "Gradient: [  -25.1167    23.0396    83.2885  -125.7462 -1128.1866]\n",
      "Weights: [-4.435  -1.0884  0.0648  0.1128  0.0521]\n",
      "MSE loss: 262.5771\n",
      "Iteration: 10500\n",
      "Gradient: [ -13.6797  -15.1866   38.7167   23.6278 -157.2658]\n",
      "Weights: [-4.435  -1.0944  0.0643  0.1133  0.0523]\n",
      "MSE loss: 260.6275\n",
      "Iteration: 10600\n",
      "Gradient: [-13.2224 -40.0461  54.4498 -13.4746 350.239 ]\n",
      "Weights: [-4.4294 -1.1005  0.0635  0.1135  0.0526]\n",
      "MSE loss: 258.7422\n",
      "Iteration: 10700\n",
      "Gradient: [ 1.693000e-01  4.092080e+01 -6.759900e+00  9.948420e+01 -5.178509e+02]\n",
      "Weights: [-4.419  -1.1053  0.0628  0.1138  0.0528]\n",
      "MSE loss: 256.9938\n",
      "Iteration: 10800\n",
      "Gradient: [  -2.2413   15.4091   -3.1844   35.7424 -322.9815]\n",
      "Weights: [-4.4163 -1.1136  0.0617  0.1141  0.0531]\n",
      "MSE loss: 254.8787\n",
      "Iteration: 10900\n",
      "Gradient: [ -22.644    31.3296   82.9807  133.3968 -732.4269]\n",
      "Weights: [-4.4074 -1.1198  0.0609  0.1143  0.0533]\n",
      "MSE loss: 253.113\n",
      "Iteration: 11000\n",
      "Gradient: [  -3.7801   38.1971   -3.6267  -86.3608 -978.596 ]\n",
      "Weights: [-4.4031 -1.1254  0.0604  0.1146  0.0535]\n",
      "MSE loss: 251.517\n",
      "Iteration: 11100\n",
      "Gradient: [  -9.3069   -5.5235   32.3173 -197.8598 -188.9925]\n",
      "Weights: [-4.3984 -1.1305  0.0595  0.1148  0.0538]\n",
      "MSE loss: 249.9839\n",
      "Iteration: 11200\n",
      "Gradient: [  -6.3238   59.8426   29.7777   68.8553 -898.7879]\n",
      "Weights: [-4.3892 -1.1359  0.0585  0.1152  0.054 ]\n",
      "MSE loss: 248.3212\n",
      "Iteration: 11300\n",
      "Gradient: [1.020780e+01 2.874000e-01 5.378800e+01 7.207560e+01 4.043449e+02]\n",
      "Weights: [-4.3792 -1.1439  0.0575  0.1154  0.0542]\n",
      "MSE loss: 246.5909\n",
      "Iteration: 11400\n",
      "Gradient: [   5.3719   13.0084  -36.0801   13.6418 -804.3908]\n",
      "Weights: [-4.3753 -1.1492  0.0572  0.1156  0.0545]\n",
      "MSE loss: 245.256\n",
      "Iteration: 11500\n",
      "Gradient: [  -9.1751   17.8742  -90.68    -10.4113 -646.4212]\n",
      "Weights: [-4.3752 -1.1534  0.0561  0.1159  0.0547]\n",
      "MSE loss: 243.8036\n",
      "Iteration: 11600\n",
      "Gradient: [-14.3204  29.1201  38.2681  34.0737 497.6164]\n",
      "Weights: [-4.3707 -1.157   0.0554  0.1162  0.0549]\n",
      "MSE loss: 242.6742\n",
      "Iteration: 11700\n",
      "Gradient: [   9.1296   14.4567   39.0384  146.2123 -245.3892]\n",
      "Weights: [-4.3669 -1.1616  0.0546  0.1165  0.0551]\n",
      "MSE loss: 241.4912\n",
      "Iteration: 11800\n",
      "Gradient: [ -1.4062 -21.3117  26.7311 -12.3108  53.1861]\n",
      "Weights: [-4.367  -1.1655  0.0535  0.1167  0.0553]\n",
      "MSE loss: 240.4309\n",
      "Iteration: 11900\n",
      "Gradient: [  -5.3319   27.4195  -42.4181 -174.9479 -543.4402]\n",
      "Weights: [-4.3583 -1.1693  0.0527  0.1171  0.0554]\n",
      "MSE loss: 239.3576\n",
      "Iteration: 12000\n",
      "Gradient: [   2.7486    1.678   -31.8873 -104.0842 -582.4734]\n",
      "Weights: [-4.3494 -1.1745  0.0519  0.1173  0.0556]\n",
      "MSE loss: 238.1885\n",
      "Iteration: 12100\n",
      "Gradient: [ -15.261    13.6046   82.8406 -234.1324 -259.4497]\n",
      "Weights: [-4.3509 -1.1787  0.0508  0.1175  0.0559]\n",
      "MSE loss: 236.9366\n",
      "Iteration: 12200\n",
      "Gradient: [ -13.9553  -10.9899  -28.3609  -20.5935 -297.2504]\n",
      "Weights: [-4.3445 -1.184   0.05    0.1177  0.056 ]\n",
      "MSE loss: 235.8367\n",
      "Iteration: 12300\n",
      "Gradient: [   1.1656   11.9914   17.3496  -55.2541 -149.1743]\n",
      "Weights: [-4.3444 -1.1875  0.0491  0.1181  0.0563]\n",
      "MSE loss: 234.7382\n",
      "Iteration: 12400\n",
      "Gradient: [  -5.6054   41.5775   20.7698 -211.2524 -164.2318]\n",
      "Weights: [-4.3421 -1.191   0.0486  0.1183  0.0565]\n",
      "MSE loss: 233.7968\n",
      "Iteration: 12500\n",
      "Gradient: [ -23.3664    6.8317   25.6125  109.1087 -135.823 ]\n",
      "Weights: [-4.3378 -1.1947  0.0479  0.1185  0.0567]\n",
      "MSE loss: 232.8856\n",
      "Iteration: 12600\n",
      "Gradient: [  1.347  -19.1326 -11.5273   1.9928  95.5233]\n",
      "Weights: [-4.3307 -1.1991  0.0468  0.1187  0.0569]\n",
      "MSE loss: 231.7982\n",
      "Iteration: 12700\n",
      "Gradient: [   2.5346   20.331   -18.2617  -47.8599 -724.6595]\n",
      "Weights: [-4.3236 -1.2027  0.0459  0.119   0.057 ]\n",
      "MSE loss: 230.9847\n",
      "Iteration: 12800\n",
      "Gradient: [ 16.6155  18.8324  64.9831 -64.0947  45.2544]\n",
      "Weights: [-4.3256 -1.2067  0.0454  0.1191  0.0572]\n",
      "MSE loss: 230.1979\n",
      "Iteration: 12900\n",
      "Gradient: [ -7.61    10.2925 -21.0674 103.8396 107.9091]\n",
      "Weights: [-4.315  -1.2108  0.0446  0.1191  0.0574]\n",
      "MSE loss: 229.3294\n",
      "Iteration: 13000\n",
      "Gradient: [ -20.529   -12.2065  -44.9361    6.2745 -396.9013]\n",
      "Weights: [-4.3211 -1.2145  0.044   0.1192  0.0576]\n",
      "MSE loss: 228.6109\n",
      "Iteration: 13100\n",
      "Gradient: [  -4.6046   -4.6189   -4.3007  165.7665 -427.7501]\n",
      "Weights: [-4.3169 -1.2165  0.0436  0.1194  0.0578]\n",
      "MSE loss: 227.817\n",
      "Iteration: 13200\n",
      "Gradient: [ -3.025    1.9099   2.3104 -52.5131 -96.8147]\n",
      "Weights: [-4.313  -1.2193  0.0425  0.1196  0.058 ]\n",
      "MSE loss: 227.0373\n",
      "Iteration: 13300\n",
      "Gradient: [  -0.543     3.6213   71.9584  -45.0488 -314.3794]\n",
      "Weights: [-4.3117 -1.2229  0.0414  0.1199  0.0581]\n",
      "MSE loss: 226.3779\n",
      "Iteration: 13400\n",
      "Gradient: [  18.7367   14.7505    9.0117  -41.5425 -609.0159]\n",
      "Weights: [-4.3058 -1.2254  0.041   0.12    0.0583]\n",
      "MSE loss: 225.7377\n",
      "Iteration: 13500\n",
      "Gradient: [   1.6856  -21.6425   16.8646  163.9971 -119.5221]\n",
      "Weights: [-4.2977 -1.2285  0.0404  0.12    0.0584]\n",
      "MSE loss: 225.2201\n",
      "Iteration: 13600\n",
      "Gradient: [   3.5783  -12.6441  -35.5026 -203.3739 -193.0973]\n",
      "Weights: [-4.2984 -1.2324  0.0395  0.1202  0.0585]\n",
      "MSE loss: 224.5706\n",
      "Iteration: 13700\n",
      "Gradient: [   1.1184  -11.2917  -52.3386  269.0412 -181.5035]\n",
      "Weights: [-4.293  -1.2331  0.0391  0.1204  0.0587]\n",
      "MSE loss: 224.1276\n",
      "Iteration: 13800\n",
      "Gradient: [   4.0835   -7.087    -4.3345 -134.0917 -238.1279]\n",
      "Weights: [-4.2944 -1.2366  0.038   0.1205  0.0588]\n",
      "MSE loss: 223.4948\n",
      "Iteration: 13900\n",
      "Gradient: [   8.8711   28.2648   23.3999   50.0407 -511.6355]\n",
      "Weights: [-4.2947 -1.2383  0.0373  0.1207  0.059 ]\n",
      "MSE loss: 222.9535\n",
      "Iteration: 14000\n",
      "Gradient: [  10.9191    8.2431   25.9646   88.7978 -723.6499]\n",
      "Weights: [-4.2886 -1.2419  0.0365  0.1208  0.0592]\n",
      "MSE loss: 222.2699\n",
      "Iteration: 14100\n",
      "Gradient: [ 13.743  -16.38    -1.775  -86.6669 271.1654]\n",
      "Weights: [-4.286  -1.2451  0.0354  0.1208  0.0593]\n",
      "MSE loss: 221.752\n",
      "Iteration: 14200\n",
      "Gradient: [   2.5157   25.3751   66.8414 -180.2844  106.1932]\n",
      "Weights: [-4.2826 -1.2465  0.0347  0.121   0.0594]\n",
      "MSE loss: 221.269\n",
      "Iteration: 14300\n",
      "Gradient: [  11.8821  -12.7203   16.1619   80.7946 -288.5171]\n",
      "Weights: [-4.2764 -1.25    0.034   0.121   0.0596]\n",
      "MSE loss: 220.7887\n",
      "Iteration: 14400\n",
      "Gradient: [  4.2741  24.6382   5.121  132.9028  23.8515]\n",
      "Weights: [-4.2696 -1.2531  0.0332  0.1211  0.0597]\n",
      "MSE loss: 220.2854\n",
      "Iteration: 14500\n",
      "Gradient: [ -15.7416  -11.9218    5.4718 -156.2981   63.4739]\n",
      "Weights: [-4.2764 -1.2557  0.0331  0.1212  0.0598]\n",
      "MSE loss: 219.9844\n",
      "Iteration: 14600\n",
      "Gradient: [  -7.9751  -18.4153    0.4661 -134.7935    7.8797]\n",
      "Weights: [-4.2779 -1.2525  0.0326  0.1214  0.06  ]\n",
      "MSE loss: 219.6522\n",
      "Iteration: 14700\n",
      "Gradient: [  1.038    4.844   30.1248 134.6707 -41.5642]\n",
      "Weights: [-4.2745 -1.254   0.0317  0.1214  0.0601]\n",
      "MSE loss: 219.2436\n",
      "Iteration: 14800\n",
      "Gradient: [ -16.5634  -25.3479  -28.3611  -29.3925 -818.54  ]\n",
      "Weights: [-4.2731 -1.2559  0.0311  0.1215  0.0602]\n",
      "MSE loss: 218.8812\n",
      "Iteration: 14900\n",
      "Gradient: [ -10.2516   23.3528    3.7987   41.9222 -113.5439]\n",
      "Weights: [-4.2704 -1.2567  0.0301  0.1216  0.0604]\n",
      "MSE loss: 218.4106\n",
      "Iteration: 15000\n",
      "Gradient: [ -5.0411  -3.7306 -42.9276  91.3559  28.1148]\n",
      "Weights: [-4.2717 -1.2579  0.0295  0.1217  0.0605]\n",
      "MSE loss: 218.1137\n",
      "Iteration: 15100\n",
      "Gradient: [   1.8055   15.1874   -2.6364  112.9718 -111.569 ]\n",
      "Weights: [-4.2683 -1.2612  0.0288  0.1219  0.0606]\n",
      "MSE loss: 217.6542\n",
      "Iteration: 15200\n",
      "Gradient: [   2.3869   21.3204   12.7747 -210.6396 -159.3529]\n",
      "Weights: [-4.2619 -1.2635  0.028   0.122   0.0607]\n",
      "MSE loss: 217.2613\n",
      "Iteration: 15300\n",
      "Gradient: [  15.6183   11.6304  -26.403  -115.4571 -680.9895]\n",
      "Weights: [-4.2619 -1.2661  0.0271  0.1221  0.0609]\n",
      "MSE loss: 216.888\n",
      "Iteration: 15400\n",
      "Gradient: [ -17.7045  -15.3376   31.8473  136.1717 -361.3346]\n",
      "Weights: [-4.2574 -1.268   0.0268  0.1221  0.061 ]\n",
      "MSE loss: 216.4926\n",
      "Iteration: 15500\n",
      "Gradient: [  -2.4287  -37.1852   -9.0605  -76.5041 -367.0599]\n",
      "Weights: [-4.2586 -1.2691  0.0263  0.1221  0.0611]\n",
      "MSE loss: 216.2424\n",
      "Iteration: 15600\n",
      "Gradient: [ -1.4133  14.5024  10.5033 -35.2762 229.7449]\n",
      "Weights: [-4.2505 -1.2706  0.0252  0.1223  0.0612]\n",
      "MSE loss: 215.8274\n",
      "Iteration: 15700\n",
      "Gradient: [  10.3081    6.2767   50.2458   31.6574 -270.7672]\n",
      "Weights: [-4.2497 -1.2721  0.0247  0.1224  0.0613]\n",
      "MSE loss: 215.5718\n",
      "Iteration: 15800\n",
      "Gradient: [ -21.1118   -1.683    24.3126  -13.6004 -121.4858]\n",
      "Weights: [-4.2548 -1.2736  0.024   0.1225  0.0615]\n",
      "MSE loss: 215.2883\n",
      "Iteration: 15900\n",
      "Gradient: [  2.0446  20.4665 -32.5213 201.2539 -33.1494]\n",
      "Weights: [-4.2544 -1.2733  0.0234  0.1226  0.0616]\n",
      "MSE loss: 214.9385\n",
      "Iteration: 16000\n",
      "Gradient: [  9.6527  -9.968   30.2401  30.7149 162.2211]\n",
      "Weights: [-4.2515 -1.2748  0.0225  0.1227  0.0617]\n",
      "MSE loss: 214.5796\n",
      "Iteration: 16100\n",
      "Gradient: [  12.0046   16.1836  -56.3883  -36.759  -695.0952]\n",
      "Weights: [-4.2471 -1.2762  0.0217  0.1227  0.0619]\n",
      "MSE loss: 214.2496\n",
      "Iteration: 16200\n",
      "Gradient: [  -2.413   -14.0191   72.5545   12.6155 -425.1128]\n",
      "Weights: [-4.2449 -1.2754  0.0209  0.1227  0.062 ]\n",
      "MSE loss: 214.0319\n",
      "Iteration: 16300\n",
      "Gradient: [ 4.232500e+00 -2.601400e+00  3.178000e-01  7.146140e+01 -5.508187e+02]\n",
      "Weights: [-4.242  -1.2775  0.0199  0.1228  0.062 ]\n",
      "MSE loss: 213.7523\n",
      "Iteration: 16400\n",
      "Gradient: [  6.8759   2.6041 -40.3812 -31.2306 271.4802]\n",
      "Weights: [-4.2427 -1.2784  0.0195  0.1228  0.0621]\n",
      "MSE loss: 213.5441\n",
      "Iteration: 16500\n",
      "Gradient: [-10.1659   6.7116  38.5271  31.0433 -13.9758]\n",
      "Weights: [-4.2428 -1.2787  0.0188  0.1228  0.0622]\n",
      "MSE loss: 213.3291\n",
      "Iteration: 16600\n",
      "Gradient: [ 10.0965  27.8545  12.1938 -57.3865  69.0254]\n",
      "Weights: [-4.2359 -1.2812  0.0182  0.1228  0.0623]\n",
      "MSE loss: 213.0291\n",
      "Iteration: 16700\n",
      "Gradient: [ -5.5609  10.3039 -17.5573  93.9864 108.812 ]\n",
      "Weights: [-4.2365 -1.2818  0.0176  0.123   0.0624]\n",
      "MSE loss: 212.7736\n",
      "Iteration: 16800\n",
      "Gradient: [-15.4231 -26.8289 -35.6127  16.236  209.4379]\n",
      "Weights: [-4.2344 -1.2812  0.0167  0.123   0.0626]\n",
      "MSE loss: 212.5263\n",
      "Iteration: 16900\n",
      "Gradient: [  -8.6885   -5.166   -12.0295   84.5138 -285.7245]\n",
      "Weights: [-4.2313 -1.2834  0.0161  0.1229  0.0627]\n",
      "MSE loss: 212.2897\n",
      "Iteration: 17000\n",
      "Gradient: [  -8.2035   19.2098  -41.0004  104.0034 -480.6612]\n",
      "Weights: [-4.2312 -1.2846  0.0152  0.1231  0.0627]\n",
      "MSE loss: 212.0361\n",
      "Iteration: 17100\n",
      "Gradient: [  -2.9731   10.4869  -41.4705  195.2152 -119.0833]\n",
      "Weights: [-4.229  -1.286   0.0143  0.1231  0.0628]\n",
      "MSE loss: 211.8153\n",
      "Iteration: 17200\n",
      "Gradient: [  -1.2557    0.4714   34.1904 -181.2133   54.9005]\n",
      "Weights: [-4.2255 -1.2867  0.0138  0.1232  0.0629]\n",
      "MSE loss: 211.6266\n",
      "Iteration: 17300\n",
      "Gradient: [   7.1654   19.9975   -5.6448  -79.3807 -240.5906]\n",
      "Weights: [-4.2281 -1.287   0.0133  0.1232  0.063 ]\n",
      "MSE loss: 211.3695\n",
      "Iteration: 17400\n",
      "Gradient: [   5.7951   -9.0051   -0.6208  -50.5536 -476.6405]\n",
      "Weights: [-4.2275 -1.2862  0.0128  0.1233  0.0632]\n",
      "MSE loss: 211.1633\n",
      "Iteration: 17500\n",
      "Gradient: [   3.9492    7.0239  -25.9712  -80.4678 -415.6367]\n",
      "Weights: [-4.2278 -1.2886  0.012   0.1233  0.0633]\n",
      "MSE loss: 210.9283\n",
      "Iteration: 17600\n",
      "Gradient: [  12.5651    9.4009   51.9954 -153.7952  -37.8814]\n",
      "Weights: [-4.229  -1.2883  0.0111  0.1233  0.0633]\n",
      "MSE loss: 210.7516\n",
      "Iteration: 17700\n",
      "Gradient: [   7.1196   12.1472    5.5599   23.101  -609.6443]\n",
      "Weights: [-4.227  -1.2882  0.0109  0.1235  0.0634]\n",
      "MSE loss: 210.5707\n",
      "Iteration: 17800\n",
      "Gradient: [  5.944    1.1282 -29.0311  12.4559  11.9064]\n",
      "Weights: [-4.2267 -1.2887  0.0099  0.1235  0.0635]\n",
      "MSE loss: 210.3752\n",
      "Iteration: 17900\n",
      "Gradient: [  -3.2107    1.4048   75.1569  -49.3697 -158.0993]\n",
      "Weights: [-4.2268 -1.2882  0.0093  0.1234  0.0636]\n",
      "MSE loss: 210.1775\n",
      "Iteration: 18000\n",
      "Gradient: [  -3.3653    2.0278   -8.886    62.5675 -260.2674]\n",
      "Weights: [-4.2243 -1.2882  0.0086  0.1234  0.0637]\n",
      "MSE loss: 209.9847\n",
      "Iteration: 18100\n",
      "Gradient: [ -10.7761  -22.4991  -13.0212  178.7258 -216.1549]\n",
      "Weights: [-4.226  -1.2899  0.0078  0.1234  0.0638]\n",
      "MSE loss: 209.8138\n",
      "Iteration: 18200\n",
      "Gradient: [   8.3042    6.0157   35.6111   54.6542 -721.0347]\n",
      "Weights: [-4.2275 -1.2885  0.0075  0.1235  0.0639]\n",
      "MSE loss: 209.6278\n",
      "Iteration: 18300\n",
      "Gradient: [ -3.3308 -17.3335 -27.3896 -55.0688 358.9659]\n",
      "Weights: [-4.2209 -1.2883  0.0065  0.1234  0.064 ]\n",
      "MSE loss: 209.3662\n",
      "Iteration: 18400\n",
      "Gradient: [ -11.0903    1.0519   73.1338 -196.6895 -398.3412]\n",
      "Weights: [-4.2248 -1.2874  0.006   0.1233  0.064 ]\n",
      "MSE loss: 209.2194\n",
      "Iteration: 18500\n",
      "Gradient: [   1.9161   -6.641    11.1381 -211.0093 -105.162 ]\n",
      "Weights: [-4.2213 -1.2866  0.0051  0.1233  0.0642]\n",
      "MSE loss: 208.9788\n",
      "Iteration: 18600\n",
      "Gradient: [   1.5961    4.1277   54.9261   20.7046 -354.4892]\n",
      "Weights: [-4.2237 -1.2876  0.0045  0.1233  0.0642]\n",
      "MSE loss: 208.8477\n",
      "Iteration: 18700\n",
      "Gradient: [   2.7333    6.2301  -27.2938  124.7715 -115.0144]\n",
      "Weights: [-4.2257e+00 -1.2860e+00  3.4000e-03  1.2330e-01  6.4300e-02]\n",
      "MSE loss: 208.6793\n",
      "Iteration: 18800\n",
      "Gradient: [   9.8466   30.3968  -51.4245   64.773  -223.7569]\n",
      "Weights: [-4.2145e+00 -1.2850e+00  2.9000e-03  1.2330e-01  6.4400e-02]\n",
      "MSE loss: 208.5689\n",
      "Iteration: 18900\n",
      "Gradient: [-17.5233  40.9165 -14.0598  -8.8979  36.6629]\n",
      "Weights: [-4.2223e+00 -1.2857e+00  2.5000e-03  1.2330e-01  6.4400e-02]\n",
      "MSE loss: 208.3558\n",
      "Iteration: 19000\n",
      "Gradient: [  -3.8716   14.2744  -32.6004   85.0696 -250.0235]\n",
      "Weights: [-4.2150e+00 -1.2874e+00  1.5000e-03  1.2330e-01  6.4500e-02]\n",
      "MSE loss: 208.1707\n",
      "Iteration: 19100\n",
      "Gradient: [   1.6271   -4.1818   49.8218 -115.7945   35.4953]\n",
      "Weights: [-4.2164e+00 -1.2853e+00  6.0000e-04  1.2340e-01  6.4600e-02]\n",
      "MSE loss: 207.9645\n",
      "Iteration: 19200\n",
      "Gradient: [   2.3084   -0.6554  -32.7524 -195.0351  -45.2957]\n",
      "Weights: [-4.2128e+00 -1.2848e+00 -3.0000e-04  1.2330e-01  6.4700e-02]\n",
      "MSE loss: 207.8067\n",
      "Iteration: 19300\n",
      "Gradient: [  15.7324   -6.2807    3.1469 -205.2036  213.6916]\n",
      "Weights: [-4.2171e+00 -1.2839e+00 -1.0000e-03  1.2330e-01  6.4800e-02]\n",
      "MSE loss: 207.574\n",
      "Iteration: 19400\n",
      "Gradient: [  3.9048  10.1203  20.2591  23.7938 187.4404]\n",
      "Weights: [-4.2183e+00 -1.2846e+00 -1.8000e-03  1.2330e-01  6.4800e-02]\n",
      "MSE loss: 207.3882\n",
      "Iteration: 19500\n",
      "Gradient: [  7.3069   9.5046 -21.6727 -20.9713 -56.0351]\n",
      "Weights: [-4.2160e+00 -1.2846e+00 -2.5000e-03  1.2330e-01  6.5000e-02]\n",
      "MSE loss: 207.1851\n",
      "Iteration: 19600\n",
      "Gradient: [-1.284060e+01  1.191000e-01  6.950000e+00 -1.107170e+01 -4.224958e+02]\n",
      "Weights: [-4.2157e+00 -1.2839e+00 -3.4000e-03  1.2340e-01  6.5100e-02]\n",
      "MSE loss: 206.9569\n",
      "Iteration: 19700\n",
      "Gradient: [ -6.4046  -6.6997  43.8767 150.0943 299.0223]\n",
      "Weights: [-4.2167e+00 -1.2827e+00 -4.1000e-03  1.2340e-01  6.5200e-02]\n",
      "MSE loss: 206.7561\n",
      "Iteration: 19800\n",
      "Gradient: [ -7.9745  13.3065  43.2194 -50.8242  41.1338]\n",
      "Weights: [-4.2243 -1.2815 -0.0045  0.1234  0.0652]\n",
      "MSE loss: 206.6982\n",
      "Iteration: 19900\n",
      "Gradient: [  -6.9276  -43.9576  -38.3468  -25.7757 -287.428 ]\n",
      "Weights: [-4.2211 -1.2804 -0.0055  0.1234  0.0653]\n",
      "MSE loss: 206.4393\n",
      "Iteration: 20000\n",
      "Gradient: [ -15.7752   20.8649  -17.8703   -5.8804 -434.2761]\n",
      "Weights: [-4.2229 -1.2786 -0.0065  0.1233  0.0654]\n",
      "MSE loss: 206.2259\n",
      "Iteration: 20100\n",
      "Gradient: [   3.9273  -12.4585   54.9695   32.2912 -383.0137]\n",
      "Weights: [-4.2188 -1.2791 -0.0072  0.1233  0.0655]\n",
      "MSE loss: 206.0348\n",
      "Iteration: 20200\n",
      "Gradient: [  -2.6215   -1.8387    7.1149 -118.955    11.8656]\n",
      "Weights: [-4.2202 -1.277  -0.0082  0.1232  0.0656]\n",
      "MSE loss: 205.8277\n",
      "Iteration: 20300\n",
      "Gradient: [-4.4313 -4.2621 28.0062 63.3354 63.7736]\n",
      "Weights: [-4.2213 -1.2758 -0.009   0.1232  0.0656]\n",
      "MSE loss: 205.6718\n",
      "Iteration: 20400\n",
      "Gradient: [  -2.6588   -8.8736  -41.005    88.8086 -360.2655]\n",
      "Weights: [-4.2178 -1.2757 -0.0097  0.1232  0.0657]\n",
      "MSE loss: 205.4937\n",
      "Iteration: 20500\n",
      "Gradient: [ 4.511000e-01 -1.084540e+01  3.219030e+01  1.339380e+01  4.517912e+02]\n",
      "Weights: [-4.2125 -1.2752 -0.0107  0.1231  0.0658]\n",
      "MSE loss: 205.3576\n",
      "Iteration: 20600\n",
      "Gradient: [-11.1609  25.8687  25.4779 176.063  106.0387]\n",
      "Weights: [-4.2179 -1.2738 -0.0113  0.123   0.0658]\n",
      "MSE loss: 205.1828\n",
      "Iteration: 20700\n",
      "Gradient: [  7.631  -15.3908  34.6729   6.2219 161.0101]\n",
      "Weights: [-4.213  -1.2749 -0.0119  0.1231  0.0659]\n",
      "MSE loss: 205.0481\n",
      "Iteration: 20800\n",
      "Gradient: [ 11.5804 -13.2588 -25.4975 209.3877 135.4814]\n",
      "Weights: [-4.2137 -1.2741 -0.0124  0.123   0.066 ]\n",
      "MSE loss: 204.9088\n",
      "Iteration: 20900\n",
      "Gradient: [ 12.9774  -3.1728  79.675   51.9961 -85.0401]\n",
      "Weights: [-4.2124 -1.2726 -0.0134  0.123   0.0661]\n",
      "MSE loss: 204.7359\n",
      "Iteration: 21000\n",
      "Gradient: [-18.2472 -32.8383  25.5491  -0.8108 -78.1666]\n",
      "Weights: [-4.2179 -1.2712 -0.0143  0.123   0.0661]\n",
      "MSE loss: 204.4986\n",
      "Iteration: 21100\n",
      "Gradient: [   3.4338   17.5146  -53.7661   36.0122 -175.0453]\n",
      "Weights: [-4.2106 -1.2715 -0.0154  0.123   0.0662]\n",
      "MSE loss: 204.3465\n",
      "Iteration: 21200\n",
      "Gradient: [ -9.3628  -6.3138 -11.4843 108.2471 -17.1824]\n",
      "Weights: [-4.2166 -1.271  -0.0156  0.123   0.0663]\n",
      "MSE loss: 204.1836\n",
      "Iteration: 21300\n",
      "Gradient: [ 12.0784  29.9306  42.4707 -85.8787 111.6112]\n",
      "Weights: [-4.217  -1.2706 -0.0163  0.1231  0.0664]\n",
      "MSE loss: 204.0131\n",
      "Iteration: 21400\n",
      "Gradient: [  1.6493 -19.9473   0.4809  10.4722 -39.7038]\n",
      "Weights: [-4.22   -1.2697 -0.0173  0.1231  0.0664]\n",
      "MSE loss: 203.8911\n",
      "Iteration: 21500\n",
      "Gradient: [  8.9239   1.3991 -31.8122  17.0935 227.2597]\n",
      "Weights: [-4.2165 -1.2701 -0.0178  0.1231  0.0665]\n",
      "MSE loss: 203.7199\n",
      "Iteration: 21600\n",
      "Gradient: [ 1.218000e-01 -2.550170e+01 -4.889780e+01  6.550950e+01 -6.404385e+02]\n",
      "Weights: [-4.2175 -1.2682 -0.0184  0.1231  0.0666]\n",
      "MSE loss: 203.6127\n",
      "Iteration: 21700\n",
      "Gradient: [ -5.3629  34.3954 -35.491   20.3582  -7.6816]\n",
      "Weights: [-4.2174 -1.2673 -0.0191  0.1231  0.0666]\n",
      "MSE loss: 203.441\n",
      "Iteration: 21800\n",
      "Gradient: [ -6.1728  24.2543  86.8507  23.9478 -61.8438]\n",
      "Weights: [-4.2144 -1.267  -0.0201  0.1231  0.0667]\n",
      "MSE loss: 203.2727\n",
      "Iteration: 21900\n",
      "Gradient: [ -7.71     1.2842  26.417  156.9658 127.1382]\n",
      "Weights: [-4.2157 -1.2668 -0.0205  0.123   0.0668]\n",
      "MSE loss: 203.1499\n",
      "Iteration: 22000\n",
      "Gradient: [   2.8673   -9.9894   49.4626  -24.6395 -465.6786]\n",
      "Weights: [-4.2173 -1.2642 -0.0214  0.1229  0.0668]\n",
      "MSE loss: 202.981\n",
      "Iteration: 22100\n",
      "Gradient: [   3.7309    9.6678   32.3303    6.6581 -205.2593]\n",
      "Weights: [-4.2166 -1.262  -0.0222  0.1229  0.0669]\n",
      "MSE loss: 202.787\n",
      "Iteration: 22200\n",
      "Gradient: [-11.6216  -9.9134  -5.7342 104.1695 168.0533]\n",
      "Weights: [-4.2181 -1.2601 -0.023   0.1228  0.067 ]\n",
      "MSE loss: 202.612\n",
      "Iteration: 22300\n",
      "Gradient: [  6.8803  -6.8021  25.7799 148.0396   0.4545]\n",
      "Weights: [-4.2176 -1.26   -0.024   0.1228  0.067 ]\n",
      "MSE loss: 202.4315\n",
      "Iteration: 22400\n",
      "Gradient: [-12.5134 -10.4641 -18.709   10.2543 -20.7271]\n",
      "Weights: [-4.217  -1.2586 -0.025   0.1228  0.0671]\n",
      "MSE loss: 202.2661\n",
      "Iteration: 22500\n",
      "Gradient: [   2.0682    5.336   -12.7432  114.6877 -191.0638]\n",
      "Weights: [-4.2201 -1.2577 -0.0258  0.1228  0.0671]\n",
      "MSE loss: 202.1209\n",
      "Iteration: 22600\n",
      "Gradient: [  -5.7023   -0.2954   21.1681  181.7219 -195.0803]\n",
      "Weights: [-4.2186 -1.2581 -0.0261  0.1229  0.0672]\n",
      "MSE loss: 202.0227\n",
      "Iteration: 22700\n",
      "Gradient: [ 14.6977  -2.7306  -9.7689   3.9609 256.5295]\n",
      "Weights: [-4.2153 -1.2567 -0.027   0.1229  0.0673]\n",
      "MSE loss: 201.8788\n",
      "Iteration: 22800\n",
      "Gradient: [ -6.7459  -6.6007  43.8058  53.4884 195.6798]\n",
      "Weights: [-4.2169 -1.2573 -0.0276  0.1228  0.0673]\n",
      "MSE loss: 201.7675\n",
      "Iteration: 22900\n",
      "Gradient: [   1.9247    3.7488  -22.1629 -182.0286   54.4183]\n",
      "Weights: [-4.2201 -1.2552 -0.028   0.1229  0.0674]\n",
      "MSE loss: 201.6429\n",
      "Iteration: 23000\n",
      "Gradient: [  -2.49     22.1783   45.3603  141.1408 -216.4175]\n",
      "Weights: [-4.2158 -1.2553 -0.0287  0.1228  0.0674]\n",
      "MSE loss: 201.4842\n",
      "Iteration: 23100\n",
      "Gradient: [ -17.6104   -6.6438   40.6829   88.1334 -565.5141]\n",
      "Weights: [-4.2145 -1.2545 -0.0297  0.1227  0.0675]\n",
      "MSE loss: 201.285\n",
      "Iteration: 23200\n",
      "Gradient: [   2.6196   20.3922   46.4293  -85.1787 -430.1484]\n",
      "Weights: [-4.2148 -1.254  -0.0306  0.1226  0.0676]\n",
      "MSE loss: 201.1114\n",
      "Iteration: 23300\n",
      "Gradient: [  8.0803  -4.4641 -17.1102 -52.1227 -71.7977]\n",
      "Weights: [-4.219  -1.2515 -0.0312  0.1226  0.0677]\n",
      "MSE loss: 200.9201\n",
      "Iteration: 23400\n",
      "Gradient: [ 1.755000e-01  2.061300e+00  1.211910e+01  5.626490e+01 -4.807924e+02]\n",
      "Weights: [-4.2192 -1.2499 -0.032   0.1225  0.0678]\n",
      "MSE loss: 200.7432\n",
      "Iteration: 23500\n",
      "Gradient: [  10.3497    8.7541   59.2027  123.1229 -503.87  ]\n",
      "Weights: [-4.2224 -1.2484 -0.0325  0.1225  0.0679]\n",
      "MSE loss: 200.6065\n",
      "Iteration: 23600\n",
      "Gradient: [   7.5682  -31.8307   21.8996  -88.5434 -285.2669]\n",
      "Weights: [-4.2199 -1.2497 -0.0333  0.1224  0.0679]\n",
      "MSE loss: 200.4764\n",
      "Iteration: 23700\n",
      "Gradient: [  -7.0519   19.1607  -49.0043 -125.1963   69.6514]\n",
      "Weights: [-4.2193 -1.2481 -0.0339  0.1223  0.068 ]\n",
      "MSE loss: 200.2701\n",
      "Iteration: 23800\n",
      "Gradient: [  -6.5318   15.5512   53.295  -117.3085 -290.9403]\n",
      "Weights: [-4.2163 -1.2484 -0.0348  0.1223  0.0681]\n",
      "MSE loss: 200.145\n",
      "Iteration: 23900\n",
      "Gradient: [ -9.2927 -16.354    1.9015 179.6787 105.8056]\n",
      "Weights: [-4.2246 -1.2474 -0.0348  0.1222  0.0681]\n",
      "MSE loss: 200.1308\n",
      "Iteration: 24000\n",
      "Gradient: [ -4.4162  18.6226 -31.6542 -23.8123  66.0234]\n",
      "Weights: [-4.2204 -1.2448 -0.0356  0.1222  0.0682]\n",
      "MSE loss: 199.8846\n",
      "Iteration: 24100\n",
      "Gradient: [-4.8238 12.3623 17.9686 45.9691 40.1381]\n",
      "Weights: [-4.2195 -1.2446 -0.0365  0.1222  0.0683]\n",
      "MSE loss: 199.7193\n",
      "Iteration: 24200\n",
      "Gradient: [ -7.1409  -6.4047 -20.1799  79.1165 -68.0836]\n",
      "Weights: [-4.2144 -1.2453 -0.0377  0.1221  0.0683]\n",
      "MSE loss: 199.5653\n",
      "Iteration: 24300\n",
      "Gradient: [  -8.6438   20.7237  -36.3737   49.1206 -108.46  ]\n",
      "Weights: [-4.2124 -1.244  -0.0384  0.1221  0.0684]\n",
      "MSE loss: 199.4404\n",
      "Iteration: 24400\n",
      "Gradient: [  1.6768 -24.0263   8.8887 -71.9012  32.0143]\n",
      "Weights: [-4.2128 -1.2431 -0.0394  0.122   0.0684]\n",
      "MSE loss: 199.2712\n",
      "Iteration: 24500\n",
      "Gradient: [-1.995300e+00  1.664000e-01 -2.466090e+01  5.256470e+01 -2.491547e+02]\n",
      "Weights: [-4.2168 -1.2415 -0.0398  0.122   0.0685]\n",
      "MSE loss: 199.1469\n",
      "Iteration: 24600\n",
      "Gradient: [   6.9132   -4.0503   -5.8661  -51.9122 -319.8694]\n",
      "Weights: [-4.2144 -1.2409 -0.0402  0.122   0.0686]\n",
      "MSE loss: 199.0297\n",
      "Iteration: 24700\n",
      "Gradient: [  -5.9299    6.0994  -46.1899   55.8818 -217.7999]\n",
      "Weights: [-4.2176 -1.2392 -0.0412  0.122   0.0687]\n",
      "MSE loss: 198.8023\n",
      "Iteration: 24800\n",
      "Gradient: [-4.135200e+00 -1.372000e-01  3.902160e+01  6.598550e+01 -5.881609e+02]\n",
      "Weights: [-4.2199 -1.237  -0.0417  0.122   0.0688]\n",
      "MSE loss: 198.657\n",
      "Iteration: 24900\n",
      "Gradient: [  -6.2676   -1.6617    7.2758   -2.4324 -554.4937]\n",
      "Weights: [-4.2241 -1.2343 -0.0429  0.122   0.0688]\n",
      "MSE loss: 198.4393\n",
      "Iteration: 25000\n",
      "Gradient: [  -9.7632   -1.0097  -46.5425  214.6874 -209.4294]\n",
      "Weights: [-4.2208 -1.2342 -0.0433  0.1219  0.0689]\n",
      "MSE loss: 198.332\n",
      "Iteration: 25100\n",
      "Gradient: [  5.9868  -5.5761   5.1824 -40.2486  71.3586]\n",
      "Weights: [-4.2239 -1.2337 -0.0444  0.1219  0.0689]\n",
      "MSE loss: 198.1523\n",
      "Iteration: 25200\n",
      "Gradient: [ -13.082     6.4335  -24.9724  -27.7116 -291.7367]\n",
      "Weights: [-4.228  -1.2315 -0.0451  0.1219  0.069 ]\n",
      "MSE loss: 198.0118\n",
      "Iteration: 25300\n",
      "Gradient: [ -6.0378   2.7067  17.3178 -36.8974 -82.9544]\n",
      "Weights: [-4.2281 -1.2286 -0.0457  0.1218  0.0691]\n",
      "MSE loss: 197.8418\n",
      "Iteration: 25400\n",
      "Gradient: [  1.9544   8.0417  40.6785  85.0686 -13.8493]\n",
      "Weights: [-4.2216 -1.2278 -0.0462  0.1216  0.0691]\n",
      "MSE loss: 197.7352\n",
      "Iteration: 25500\n",
      "Gradient: [ -5.3515  -8.4788  23.9565  -6.9037 150.3754]\n",
      "Weights: [-4.2158 -1.2293 -0.0472  0.1216  0.0692]\n",
      "MSE loss: 197.6132\n",
      "Iteration: 25600\n",
      "Gradient: [10.3047 -8.7096 -4.3381 69.1228 47.8229]\n",
      "Weights: [-4.2218 -1.2288 -0.0478  0.1215  0.0692]\n",
      "MSE loss: 197.4396\n",
      "Iteration: 25700\n",
      "Gradient: [-10.46     9.1801 -17.3383  50.9659 117.4543]\n",
      "Weights: [-4.2279 -1.2259 -0.0483  0.1216  0.0693]\n",
      "MSE loss: 197.323\n",
      "Iteration: 25800\n",
      "Gradient: [  -3.8486   -5.942    50.4229   -9.0893 -420.1337]\n",
      "Weights: [-4.2204 -1.2242 -0.0494  0.1214  0.0694]\n",
      "MSE loss: 197.1197\n",
      "Iteration: 25900\n",
      "Gradient: [   2.9964   -4.8786   25.0542    3.9156 -368.5563]\n",
      "Weights: [-4.229  -1.2221 -0.0499  0.1213  0.0694]\n",
      "MSE loss: 197.0019\n",
      "Iteration: 26000\n",
      "Gradient: [   8.8663   16.3997   65.5619  -53.335  -147.0439]\n",
      "Weights: [-4.2228 -1.2221 -0.0504  0.1212  0.0694]\n",
      "MSE loss: 196.8772\n",
      "Iteration: 26100\n",
      "Gradient: [ 12.439    0.5217 -25.129  147.6538 -19.8294]\n",
      "Weights: [-4.225  -1.2222 -0.0512  0.1212  0.0695]\n",
      "MSE loss: 196.7185\n",
      "Iteration: 26200\n",
      "Gradient: [ -0.9757  17.7021  14.8829  47.3387 -12.0755]\n",
      "Weights: [-4.2247 -1.2212 -0.0519  0.1212  0.0696]\n",
      "MSE loss: 196.5449\n",
      "Iteration: 26300\n",
      "Gradient: [   4.8217  -24.9635   56.0476  -44.1822 -563.3738]\n",
      "Weights: [-4.2222 -1.2207 -0.0528  0.1211  0.0697]\n",
      "MSE loss: 196.3785\n",
      "Iteration: 26400\n",
      "Gradient: [   9.2584    1.5081  -28.3913   48.5094 -411.5461]\n",
      "Weights: [-4.2224 -1.2191 -0.0535  0.1211  0.0698]\n",
      "MSE loss: 196.2231\n",
      "Iteration: 26500\n",
      "Gradient: [ -6.5841   8.5554 -43.638  -24.2844 -86.5979]\n",
      "Weights: [-4.2239 -1.2186 -0.0542  0.1211  0.0698]\n",
      "MSE loss: 196.0765\n",
      "Iteration: 26600\n",
      "Gradient: [ 13.1582  -7.479   19.9283 -63.0827 236.5262]\n",
      "Weights: [-4.225  -1.2178 -0.0547  0.121   0.0699]\n",
      "MSE loss: 195.9535\n",
      "Iteration: 26700\n",
      "Gradient: [ -4.6431  10.9522  51.031  147.0266 328.1929]\n",
      "Weights: [-4.2265 -1.2146 -0.0555  0.121   0.07  ]\n",
      "MSE loss: 195.7689\n",
      "Iteration: 26800\n",
      "Gradient: [-12.6722  21.7791   8.2691 101.6879 392.9377]\n",
      "Weights: [-4.2216 -1.2136 -0.0566  0.121   0.07  ]\n",
      "MSE loss: 195.6406\n",
      "Iteration: 26900\n",
      "Gradient: [  -3.409    -2.1132   -9.5714 -167.7087  -56.4323]\n",
      "Weights: [-4.2244 -1.2139 -0.0572  0.121   0.0701]\n",
      "MSE loss: 195.4941\n",
      "Iteration: 27000\n",
      "Gradient: [ 13.3184   9.4417 -62.9597   8.0643  40.4181]\n",
      "Weights: [-4.2214 -1.213  -0.0577  0.1209  0.0701]\n",
      "MSE loss: 195.3925\n",
      "Iteration: 27100\n",
      "Gradient: [ -24.0716    1.1545   19.52    -81.9074 -289.9968]\n",
      "Weights: [-4.2244 -1.2118 -0.0584  0.1209  0.0702]\n",
      "MSE loss: 195.2349\n",
      "Iteration: 27200\n",
      "Gradient: [  -7.4244  -13.9148   -2.0995   38.1274 -159.8052]\n",
      "Weights: [-4.2271 -1.2108 -0.0587  0.1209  0.0703]\n",
      "MSE loss: 195.1324\n",
      "Iteration: 27300\n",
      "Gradient: [-2.8312 12.3754 -6.4346 26.9544 10.5961]\n",
      "Weights: [-4.228  -1.2097 -0.0593  0.1209  0.0703]\n",
      "MSE loss: 195.014\n",
      "Iteration: 27400\n",
      "Gradient: [  3.7808 -17.7432  37.4105 -22.4726   5.8896]\n",
      "Weights: [-4.2286 -1.2089 -0.0595  0.1209  0.0704]\n",
      "MSE loss: 194.9467\n",
      "Iteration: 27500\n",
      "Gradient: [  17.5381    4.2087   17.6111  -11.2801 -280.0379]\n",
      "Weights: [-4.2289 -1.2065 -0.0607  0.1207  0.0704]\n",
      "MSE loss: 194.7418\n",
      "Iteration: 27600\n",
      "Gradient: [-11.2958  21.0449 -65.4848  65.8089 383.2747]\n",
      "Weights: [-4.2289 -1.2057 -0.0613  0.1207  0.0705]\n",
      "MSE loss: 194.6137\n",
      "Iteration: 27700\n",
      "Gradient: [  17.6321    0.861   -17.8681  -13.1922 -338.7005]\n",
      "Weights: [-4.231  -1.205  -0.0622  0.1207  0.0705]\n",
      "MSE loss: 194.5173\n",
      "Iteration: 27800\n",
      "Gradient: [  2.235    7.6382 -12.0907 -34.6558 148.2351]\n",
      "Weights: [-4.2248 -1.2036 -0.0631  0.1207  0.0706]\n",
      "MSE loss: 194.3352\n",
      "Iteration: 27900\n",
      "Gradient: [-4.499700e+00 -1.641900e+01  3.423000e-01 -2.549000e-01  3.151338e+02]\n",
      "Weights: [-4.2278 -1.2039 -0.0641  0.1207  0.0707]\n",
      "MSE loss: 194.1438\n",
      "Iteration: 28000\n",
      "Gradient: [  13.4292   -9.2541   -4.6759  -60.9637 -173.0532]\n",
      "Weights: [-4.2253 -1.2026 -0.0646  0.1205  0.0707]\n",
      "MSE loss: 194.0007\n",
      "Iteration: 28100\n",
      "Gradient: [   3.3932    2.2978  -27.241     1.1942 -209.5724]\n",
      "Weights: [-4.2201 -1.2005 -0.0655  0.1204  0.0708]\n",
      "MSE loss: 193.934\n",
      "Iteration: 28200\n",
      "Gradient: [ 1.512090e+01 -1.831810e+01  1.418000e-01  1.452413e+02 -7.095344e+02]\n",
      "Weights: [-4.229  -1.1995 -0.0662  0.1202  0.0709]\n",
      "MSE loss: 193.6953\n",
      "Iteration: 28300\n",
      "Gradient: [  6.5993   8.7716   9.4651  46.7701 236.2117]\n",
      "Weights: [-4.2215 -1.1975 -0.0671  0.1201  0.0709]\n",
      "MSE loss: 193.5565\n",
      "Iteration: 28400\n",
      "Gradient: [  3.4823   0.1896   4.016  -25.6915 -71.0707]\n",
      "Weights: [-4.2253 -1.1969 -0.0682  0.1201  0.071 ]\n",
      "MSE loss: 193.3246\n",
      "Iteration: 28500\n",
      "Gradient: [ -4.5719  -8.859   -1.9775 -15.2231 -80.0553]\n",
      "Weights: [-4.226  -1.196  -0.0689  0.12    0.0711]\n",
      "MSE loss: 193.1927\n",
      "Iteration: 28600\n",
      "Gradient: [-10.658  -15.479   19.0594  41.3523 -72.364 ]\n",
      "Weights: [-4.2303 -1.1939 -0.0695  0.12    0.0711]\n",
      "MSE loss: 193.0217\n",
      "Iteration: 28700\n",
      "Gradient: [-4.834600e+00  6.480000e-02  4.330750e+01 -6.420660e+01 -3.223747e+02]\n",
      "Weights: [-4.2304 -1.1914 -0.0703  0.12    0.0712]\n",
      "MSE loss: 192.8425\n",
      "Iteration: 28800\n",
      "Gradient: [ -6.8997 -23.0517  -4.0459 139.4006 228.9168]\n",
      "Weights: [-4.2307 -1.1911 -0.071   0.1199  0.0713]\n",
      "MSE loss: 192.6948\n",
      "Iteration: 28900\n",
      "Gradient: [  -3.9019   -6.323   -68.3572  133.2925 -214.3011]\n",
      "Weights: [-4.2317 -1.1902 -0.0715  0.1198  0.0713]\n",
      "MSE loss: 192.5741\n",
      "Iteration: 29000\n",
      "Gradient: [   0.8968  -13.1339   44.2104 -107.2869 -135.8303]\n",
      "Weights: [-4.2284 -1.1888 -0.0722  0.1199  0.0714]\n",
      "MSE loss: 192.4614\n",
      "Iteration: 29100\n",
      "Gradient: [-11.572  -14.6245 -13.2691  21.2688  95.9177]\n",
      "Weights: [-4.2331 -1.1873 -0.0729  0.1198  0.0715]\n",
      "MSE loss: 192.2881\n",
      "Iteration: 29200\n",
      "Gradient: [ -4.2193 -15.8019 -37.1001  31.5453  78.2078]\n",
      "Weights: [-4.2356 -1.1835 -0.0736  0.1197  0.0715]\n",
      "MSE loss: 192.1193\n",
      "Iteration: 29300\n",
      "Gradient: [  -8.6531    2.6389    7.8724   35.2643 -123.3484]\n",
      "Weights: [-4.2381 -1.1815 -0.0738  0.1196  0.0716]\n",
      "MSE loss: 192.0121\n",
      "Iteration: 29400\n",
      "Gradient: [   7.1552    4.1135   35.2768  119.6834 -266.6521]\n",
      "Weights: [-4.2332 -1.1812 -0.0744  0.1195  0.0716]\n",
      "MSE loss: 191.9174\n",
      "Iteration: 29500\n",
      "Gradient: [  0.2613  -8.8419 -72.4201  30.8719  51.1681]\n",
      "Weights: [-4.2367 -1.182  -0.075   0.1194  0.0716]\n",
      "MSE loss: 191.8372\n",
      "Iteration: 29600\n",
      "Gradient: [  0.1923 -16.5119  -0.9891 -77.0772  57.885 ]\n",
      "Weights: [-4.229  -1.1822 -0.0758  0.1193  0.0717]\n",
      "MSE loss: 191.6815\n",
      "Iteration: 29700\n",
      "Gradient: [   1.7131   -2.3147    7.1129   96.2615 -256.1715]\n",
      "Weights: [-4.2348 -1.1806 -0.0764  0.1193  0.0718]\n",
      "MSE loss: 191.5217\n",
      "Iteration: 29800\n",
      "Gradient: [   1.315     3.586    -9.5801 -157.3142 -226.4953]\n",
      "Weights: [-4.2354 -1.1801 -0.0771  0.1192  0.0719]\n",
      "MSE loss: 191.4206\n",
      "Iteration: 29900\n",
      "Gradient: [   5.1343  -11.9656   66.6284 -121.3443  -93.058 ]\n",
      "Weights: [-4.2296 -1.1803 -0.0775  0.1191  0.0719]\n",
      "MSE loss: 191.3235\n",
      "Iteration: 30000\n",
      "Gradient: [-18.8291   1.2121  15.3551 -15.2877 103.6257]\n",
      "Weights: [-4.2345 -1.18   -0.078   0.1192  0.072 ]\n",
      "MSE loss: 191.2262\n",
      "Iteration: 30100\n",
      "Gradient: [  3.8863  17.6228  -3.0263  -0.8485 -69.7188]\n",
      "Weights: [-4.2309 -1.177  -0.0783  0.1192  0.072 ]\n",
      "MSE loss: 191.1261\n",
      "Iteration: 30200\n",
      "Gradient: [  16.3168   18.7619   59.3295  -39.2679 -119.1177]\n",
      "Weights: [-4.231  -1.1763 -0.079   0.1191  0.0721]\n",
      "MSE loss: 190.9775\n",
      "Iteration: 30300\n",
      "Gradient: [  11.2235    1.7689    7.7957  -11.9921 -110.5629]\n",
      "Weights: [-4.2318 -1.1765 -0.0798  0.119   0.0722]\n",
      "MSE loss: 190.8095\n",
      "Iteration: 30400\n",
      "Gradient: [ -8.309   -1.7722  41.8224 -71.9299 141.3611]\n",
      "Weights: [-4.2365 -1.1746 -0.0805  0.119   0.0723]\n",
      "MSE loss: 190.639\n",
      "Iteration: 30500\n",
      "Gradient: [  7.5146   5.9646  12.2824  55.2285 -30.6911]\n",
      "Weights: [-4.2333 -1.1736 -0.0815  0.119   0.0723]\n",
      "MSE loss: 190.5094\n",
      "Iteration: 30600\n",
      "Gradient: [  -1.6285   -2.8723   42.7406   -5.3241 -481.9946]\n",
      "Weights: [-4.2333 -1.1715 -0.0821  0.119   0.0723]\n",
      "MSE loss: 190.3949\n",
      "Iteration: 30700\n",
      "Gradient: [  -9.6668   15.5153   31.4922   -4.2953 -129.5161]\n",
      "Weights: [-4.2312 -1.1712 -0.0833  0.119   0.0724]\n",
      "MSE loss: 190.2244\n",
      "Iteration: 30800\n",
      "Gradient: [-19.0169 -28.1231  42.6924  22.5299 -71.4895]\n",
      "Weights: [-4.2363 -1.1698 -0.0841  0.1189  0.0724]\n",
      "MSE loss: 190.0763\n",
      "Iteration: 30900\n",
      "Gradient: [  0.3202 -14.5978  44.175  -35.659   51.1758]\n",
      "Weights: [-4.2325 -1.1693 -0.085   0.1189  0.0725]\n",
      "MSE loss: 189.9139\n",
      "Iteration: 31000\n",
      "Gradient: [  -7.8235   16.6649  -59.9666 -126.0878 -450.9887]\n",
      "Weights: [-4.2314 -1.1671 -0.0854  0.1188  0.0726]\n",
      "MSE loss: 189.7869\n",
      "Iteration: 31100\n",
      "Gradient: [  -4.1311    6.3121  -85.8529 -108.8907  225.1727]\n",
      "Weights: [-4.2356 -1.1658 -0.0863  0.1188  0.0726]\n",
      "MSE loss: 189.6207\n",
      "Iteration: 31200\n",
      "Gradient: [  -3.5382   15.4652   62.9135   -4.8866 -533.2555]\n",
      "Weights: [-4.2402 -1.1647 -0.0871  0.1188  0.0727]\n",
      "MSE loss: 189.5572\n",
      "Iteration: 31300\n",
      "Gradient: [   9.1128   -0.6773  -45.0297   98.6103 -141.1031]\n",
      "Weights: [-4.2362 -1.1621 -0.0878  0.1187  0.0727]\n",
      "MSE loss: 189.3349\n",
      "Iteration: 31400\n",
      "Gradient: [ 20.0058  -6.8954  25.6852  82.454  -69.8761]\n",
      "Weights: [-4.2372 -1.1599 -0.0885  0.1187  0.0728]\n",
      "MSE loss: 189.2025\n",
      "Iteration: 31500\n",
      "Gradient: [ -2.8072  -1.9845   8.3886 -34.9299 -56.4695]\n",
      "Weights: [-4.2342 -1.1594 -0.0895  0.1185  0.0728]\n",
      "MSE loss: 189.0713\n",
      "Iteration: 31600\n",
      "Gradient: [   0.6392    8.8909  -11.7683  -55.2714 -139.8579]\n",
      "Weights: [-4.2343 -1.159  -0.0903  0.1185  0.0729]\n",
      "MSE loss: 188.9481\n",
      "Iteration: 31700\n",
      "Gradient: [ -17.6324    7.8665  -24.5486   87.8514 -362.7403]\n",
      "Weights: [-4.2377 -1.158  -0.0907  0.1186  0.0729]\n",
      "MSE loss: 188.7976\n",
      "Iteration: 31800\n",
      "Gradient: [  -9.8518    5.8787   20.7634 -119.8492 -169.7957]\n",
      "Weights: [-4.2362 -1.1573 -0.0915  0.1185  0.073 ]\n",
      "MSE loss: 188.6493\n",
      "Iteration: 31900\n",
      "Gradient: [  -4.1644   27.4924   -1.5247  -31.3285 -241.8379]\n",
      "Weights: [-4.2373 -1.1561 -0.092   0.1185  0.0731]\n",
      "MSE loss: 188.4957\n",
      "Iteration: 32000\n",
      "Gradient: [  -1.8696   -3.3262  -27.8321   33.2537 -158.4239]\n",
      "Weights: [-4.24   -1.1551 -0.0928  0.1184  0.0732]\n",
      "MSE loss: 188.3451\n",
      "Iteration: 32100\n",
      "Gradient: [   0.3242  -12.3577  -10.5693 -101.928   -46.2794]\n",
      "Weights: [-4.2407 -1.1535 -0.0936  0.1183  0.0732]\n",
      "MSE loss: 188.2125\n",
      "Iteration: 32200\n",
      "Gradient: [   2.5985    2.7101   -6.2281  -75.4572 -537.1824]\n",
      "Weights: [-4.2398 -1.1525 -0.0941  0.1183  0.0733]\n",
      "MSE loss: 188.0632\n",
      "Iteration: 32300\n",
      "Gradient: [  8.6868 -16.4321   7.7116 -35.9612 -42.362 ]\n",
      "Weights: [-4.2384 -1.151  -0.0948  0.1182  0.0734]\n",
      "MSE loss: 187.9164\n",
      "Iteration: 32400\n",
      "Gradient: [   4.3411   -6.4398   20.7407  -14.1807 -451.2043]\n",
      "Weights: [-4.2415 -1.1495 -0.0954  0.1182  0.0734]\n",
      "MSE loss: 187.791\n",
      "Iteration: 32500\n",
      "Gradient: [-12.1434   3.7562 -39.2971  14.5244 165.866 ]\n",
      "Weights: [-4.2467 -1.1478 -0.0958  0.1183  0.0735]\n",
      "MSE loss: 187.7124\n",
      "Iteration: 32600\n",
      "Gradient: [ -4.9442  13.1135 -30.3272 102.5201  89.0071]\n",
      "Weights: [-4.2482 -1.1465 -0.0963  0.1183  0.0735]\n",
      "MSE loss: 187.6299\n",
      "Iteration: 32700\n",
      "Gradient: [   8.7581    9.3531    8.6748 -102.639   -30.6818]\n",
      "Weights: [-4.2447 -1.1455 -0.097   0.1182  0.0735]\n",
      "MSE loss: 187.4752\n",
      "Iteration: 32800\n",
      "Gradient: [  7.327  -15.4381  17.0141 -48.3515 303.9529]\n",
      "Weights: [-4.2389 -1.1459 -0.098   0.1181  0.0736]\n",
      "MSE loss: 187.3671\n",
      "Iteration: 32900\n",
      "Gradient: [  -8.6431    9.9181   15.9692  -28.2095 -377.6067]\n",
      "Weights: [-4.2445 -1.1442 -0.0988  0.1181  0.0736]\n",
      "MSE loss: 187.2241\n",
      "Iteration: 33000\n",
      "Gradient: [  10.9237  -18.9011   35.4728   85.0623 -550.1462]\n",
      "Weights: [-4.2429 -1.1425 -0.0991  0.1182  0.0737]\n",
      "MSE loss: 187.1192\n",
      "Iteration: 33100\n",
      "Gradient: [  -7.9501  -25.5548   25.9751  -68.5622 -165.1405]\n",
      "Weights: [-4.2467 -1.1422 -0.0995  0.1182  0.0738]\n",
      "MSE loss: 187.0155\n",
      "Iteration: 33200\n",
      "Gradient: [  -8.1615  -23.6644   53.4381   40.2465 -597.9791]\n",
      "Weights: [-4.2428 -1.1425 -0.1001  0.1181  0.0738]\n",
      "MSE loss: 186.9162\n",
      "Iteration: 33300\n",
      "Gradient: [  5.8379 -11.0792 -26.6903  58.915  198.5776]\n",
      "Weights: [-4.2478 -1.142  -0.1005  0.1181  0.0739]\n",
      "MSE loss: 186.832\n",
      "Iteration: 33400\n",
      "Gradient: [  7.9464   3.6041  49.4744 142.0594 -25.3049]\n",
      "Weights: [-4.2463 -1.1398 -0.1011  0.118   0.0739]\n",
      "MSE loss: 186.6848\n",
      "Iteration: 33500\n",
      "Gradient: [ -10.0025    2.5791   32.8351  163.9622 -495.6176]\n",
      "Weights: [-4.2379 -1.1394 -0.1022  0.1179  0.074 ]\n",
      "MSE loss: 186.5764\n",
      "Iteration: 33600\n",
      "Gradient: [ -11.2933   10.7573   72.989   104.0007 -240.8243]\n",
      "Weights: [-4.2439 -1.1382 -0.1026  0.1178  0.074 ]\n",
      "MSE loss: 186.4167\n",
      "Iteration: 33700\n",
      "Gradient: [12.5294  4.3483 29.9315 29.0283 51.4033]\n",
      "Weights: [-4.2409 -1.1367 -0.1033  0.1178  0.0741]\n",
      "MSE loss: 186.2954\n",
      "Iteration: 33800\n",
      "Gradient: [  2.233    8.4519  94.0981 -41.3488  83.398 ]\n",
      "Weights: [-4.2384 -1.1365 -0.104   0.1177  0.0742]\n",
      "MSE loss: 186.2076\n",
      "Iteration: 33900\n",
      "Gradient: [   1.3868   24.2103   11.9739   79.0955 -420.1987]\n",
      "Weights: [-4.2435 -1.1346 -0.1046  0.1178  0.0742]\n",
      "MSE loss: 186.0259\n",
      "Iteration: 34000\n",
      "Gradient: [  5.6213 -15.2924  19.8242  15.3783 119.6786]\n",
      "Weights: [-4.2523 -1.1327 -0.1051  0.1178  0.0743]\n",
      "MSE loss: 185.9298\n",
      "Iteration: 34100\n",
      "Gradient: [ -7.9002   3.0539 -16.5739  23.728  163.5046]\n",
      "Weights: [-4.249  -1.1307 -0.1061  0.1177  0.0743]\n",
      "MSE loss: 185.7219\n",
      "Iteration: 34200\n",
      "Gradient: [ -7.9081  -9.1448  36.8398 123.9174 311.6925]\n",
      "Weights: [-4.2464 -1.1291 -0.1069  0.1177  0.0744]\n",
      "MSE loss: 185.5937\n",
      "Iteration: 34300\n",
      "Gradient: [ -12.3314    2.719    37.9365  -54.0457 -440.5174]\n",
      "Weights: [-4.2508 -1.1283 -0.1077  0.1176  0.0745]\n",
      "MSE loss: 185.4358\n",
      "Iteration: 34400\n",
      "Gradient: [-4.2909 -1.8156 12.5603 38.3162  3.3673]\n",
      "Weights: [-4.2545 -1.1252 -0.1086  0.1176  0.0745]\n",
      "MSE loss: 185.2656\n",
      "Iteration: 34500\n",
      "Gradient: [ -1.3553  12.822   33.8102  28.0596 -73.3261]\n",
      "Weights: [-4.252  -1.1231 -0.1093  0.1175  0.0746]\n",
      "MSE loss: 185.1082\n",
      "Iteration: 34600\n",
      "Gradient: [  -8.1161   29.4411   16.8083  -24.5902 -474.9172]\n",
      "Weights: [-4.253  -1.1229 -0.11    0.1175  0.0746]\n",
      "MSE loss: 184.9973\n",
      "Iteration: 34700\n",
      "Gradient: [ -4.0674  -6.8195  29.536    7.643  214.9573]\n",
      "Weights: [-4.2535 -1.122  -0.1104  0.1175  0.0747]\n",
      "MSE loss: 184.899\n",
      "Iteration: 34800\n",
      "Gradient: [  3.6676  -4.7676  31.9089  46.0185 383.0969]\n",
      "Weights: [-4.2467 -1.1216 -0.1114  0.1174  0.0747]\n",
      "MSE loss: 184.7807\n",
      "Iteration: 34900\n",
      "Gradient: [   5.7562    5.7975    6.8119   98.7103 -206.0996]\n",
      "Weights: [-4.2434 -1.1211 -0.1121  0.1173  0.0747]\n",
      "MSE loss: 184.6903\n",
      "Iteration: 35000\n",
      "Gradient: [   0.5994   -9.5938   13.1923  147.2909 -124.1539]\n",
      "Weights: [-4.2404 -1.1216 -0.113   0.1172  0.0748]\n",
      "MSE loss: 184.6011\n",
      "Iteration: 35100\n",
      "Gradient: [   1.2889    6.5067  -28.0471  114.602  -160.9983]\n",
      "Weights: [-4.2436 -1.1203 -0.1137  0.1171  0.0749]\n",
      "MSE loss: 184.4025\n",
      "Iteration: 35200\n",
      "Gradient: [ -0.3408   7.0759 -28.958   -2.0125  47.3491]\n",
      "Weights: [-4.2436 -1.1199 -0.114   0.1169  0.0749]\n",
      "MSE loss: 184.3092\n",
      "Iteration: 35300\n",
      "Gradient: [  10.0764  -11.5779   14.444   -24.9543 -281.3007]\n",
      "Weights: [-4.2477 -1.1193 -0.1143  0.1169  0.075 ]\n",
      "MSE loss: 184.2188\n",
      "Iteration: 35400\n",
      "Gradient: [  -4.3345  -15.1318   43.939  -107.7443  242.2242]\n",
      "Weights: [-4.2481 -1.1177 -0.1149  0.1169  0.0751]\n",
      "MSE loss: 184.068\n",
      "Iteration: 35500\n",
      "Gradient: [  8.3846  13.3356  14.8022  -3.1868 160.7224]\n",
      "Weights: [-4.2496 -1.1147 -0.1157  0.117   0.0751]\n",
      "MSE loss: 183.8923\n",
      "Iteration: 35600\n",
      "Gradient: [  8.9206   8.4628  17.5656 -58.5072 -37.5857]\n",
      "Weights: [-4.2498 -1.1148 -0.1162  0.1169  0.0752]\n",
      "MSE loss: 183.8054\n",
      "Iteration: 35700\n",
      "Gradient: [  5.8834 -10.6438  21.1144 -37.6766 -34.2194]\n",
      "Weights: [-4.2533 -1.1126 -0.1171  0.117   0.0752]\n",
      "MSE loss: 183.6724\n",
      "Iteration: 35800\n",
      "Gradient: [ 18.1739 -27.4925  35.2185 -22.6938 -22.0517]\n",
      "Weights: [-4.2483 -1.1112 -0.1179  0.117   0.0752]\n",
      "MSE loss: 183.5675\n",
      "Iteration: 35900\n",
      "Gradient: [  16.9539   12.0126   45.2064   48.7034 -236.7883]\n",
      "Weights: [-4.2483 -1.1106 -0.119   0.1169  0.0753]\n",
      "MSE loss: 183.4053\n",
      "Iteration: 36000\n",
      "Gradient: [   5.5832    8.1756  -34.3354  128.4866 -708.6401]\n",
      "Weights: [-4.248  -1.1093 -0.1196  0.1169  0.0753]\n",
      "MSE loss: 183.2866\n",
      "Iteration: 36100\n",
      "Gradient: [-12.0013   6.2013   6.9483  52.7533  43.7012]\n",
      "Weights: [-4.2552 -1.1074 -0.1198  0.1168  0.0754]\n",
      "MSE loss: 183.1599\n",
      "Iteration: 36200\n",
      "Gradient: [ -2.2535 -21.0775  18.2779  97.1898 204.0721]\n",
      "Weights: [-4.2526 -1.1059 -0.1204  0.1167  0.0754]\n",
      "MSE loss: 183.0436\n",
      "Iteration: 36300\n",
      "Gradient: [ -12.2985   13.3313   24.3856   -3.2481 -148.8174]\n",
      "Weights: [-4.2584 -1.1034 -0.1212  0.1167  0.0755]\n",
      "MSE loss: 182.8838\n",
      "Iteration: 36400\n",
      "Gradient: [-10.0215   4.2484  40.0427 -15.8301 -28.0598]\n",
      "Weights: [-4.2538 -1.102  -0.1218  0.1166  0.0755]\n",
      "MSE loss: 182.7716\n",
      "Iteration: 36500\n",
      "Gradient: [  2.7354 -24.3908 -30.7624 -30.7009  44.3369]\n",
      "Weights: [-4.2545 -1.1009 -0.1226  0.1166  0.0756]\n",
      "MSE loss: 182.6351\n",
      "Iteration: 36600\n",
      "Gradient: [  -9.3194    7.1889   14.0402  -11.0373 -295.69  ]\n",
      "Weights: [-4.2561 -1.0997 -0.1235  0.1164  0.0756]\n",
      "MSE loss: 182.478\n",
      "Iteration: 36700\n",
      "Gradient: [ -6.6346   8.5583 -72.5337  48.8369   2.6462]\n",
      "Weights: [-4.2595 -1.0976 -0.1242  0.1165  0.0757]\n",
      "MSE loss: 182.3465\n",
      "Iteration: 36800\n",
      "Gradient: [  -8.3696   21.2151   35.0063   10.9465 -472.0521]\n",
      "Weights: [-4.2584 -1.0953 -0.1251  0.1165  0.0758]\n",
      "MSE loss: 182.1671\n",
      "Iteration: 36900\n",
      "Gradient: [ -4.4794 -16.5186 -17.1729 114.6326 435.321 ]\n",
      "Weights: [-4.2592 -1.0936 -0.1259  0.1164  0.0758]\n",
      "MSE loss: 182.0116\n",
      "Iteration: 37000\n",
      "Gradient: [   4.8014   -7.626    68.0072  101.4776 -286.0705]\n",
      "Weights: [-4.2608 -1.0919 -0.1266  0.1163  0.0759]\n",
      "MSE loss: 181.8654\n",
      "Iteration: 37100\n",
      "Gradient: [ -4.3515  -6.7102  -5.4208  61.2455 -85.4942]\n",
      "Weights: [-4.258  -1.0904 -0.1275  0.1162  0.0759]\n",
      "MSE loss: 181.7177\n",
      "Iteration: 37200\n",
      "Gradient: [-14.3187  -0.8812   7.7729  92.4522  38.263 ]\n",
      "Weights: [-4.2572 -1.0911 -0.1281  0.1161  0.076 ]\n",
      "MSE loss: 181.6139\n",
      "Iteration: 37300\n",
      "Gradient: [ -1.8943  -0.2565  64.8416 -99.3468 171.9452]\n",
      "Weights: [-4.2585 -1.09   -0.1287  0.116   0.0761]\n",
      "MSE loss: 181.4756\n",
      "Iteration: 37400\n",
      "Gradient: [  1.7756   4.3994  77.6842   0.9211 297.0084]\n",
      "Weights: [-4.2586 -1.0886 -0.1293  0.116   0.0761]\n",
      "MSE loss: 181.362\n",
      "Iteration: 37500\n",
      "Gradient: [  16.1092   -2.9834   58.4844   44.5772 -144.6316]\n",
      "Weights: [-4.2584 -1.0879 -0.1296  0.1159  0.0762]\n",
      "MSE loss: 181.2867\n",
      "Iteration: 37600\n",
      "Gradient: [  -7.2081   -8.4469   35.1794  136.9672 -152.6021]\n",
      "Weights: [-4.2586 -1.0877 -0.1304  0.1159  0.0762]\n",
      "MSE loss: 181.1657\n",
      "Iteration: 37700\n",
      "Gradient: [ -3.3295  -4.9146  34.9135 -19.5671 215.156 ]\n",
      "Weights: [-4.2593 -1.0852 -0.131   0.1158  0.0763]\n",
      "MSE loss: 181.0125\n",
      "Iteration: 37800\n",
      "Gradient: [-13.3401   7.9662  -1.291  142.6208  83.0989]\n",
      "Weights: [-4.2622 -1.0836 -0.1318  0.1158  0.0764]\n",
      "MSE loss: 180.8533\n",
      "Iteration: 37900\n",
      "Gradient: [  -2.0736    8.2449  -17.0425  -67.1577 -388.0955]\n",
      "Weights: [-4.2665 -1.0817 -0.1322  0.1157  0.0764]\n",
      "MSE loss: 180.7602\n",
      "Iteration: 38000\n",
      "Gradient: [  5.7356 -18.1656 -11.2674  26.4558 -35.673 ]\n",
      "Weights: [-4.2623 -1.0804 -0.1328  0.1156  0.0765]\n",
      "MSE loss: 180.6333\n",
      "Iteration: 38100\n",
      "Gradient: [   6.0307   -0.4492    6.0445  -30.7983 -318.4612]\n",
      "Weights: [-4.2645 -1.0798 -0.1336  0.1156  0.0765]\n",
      "MSE loss: 180.4883\n",
      "Iteration: 38200\n",
      "Gradient: [-10.9199  -9.2784  37.86   -65.1102 374.2293]\n",
      "Weights: [-4.2595 -1.0804 -0.1341  0.1156  0.0766]\n",
      "MSE loss: 180.4158\n",
      "Iteration: 38300\n",
      "Gradient: [  18.2361   31.1546  -11.0894  -26.7254 -157.5572]\n",
      "Weights: [-4.2595 -1.0797 -0.1349  0.1155  0.0766]\n",
      "MSE loss: 180.2937\n",
      "Iteration: 38400\n",
      "Gradient: [   5.6664   10.3559  -43.0735  -48.6826 -259.9899]\n",
      "Weights: [-4.2631 -1.0781 -0.1355  0.1155  0.0767]\n",
      "MSE loss: 180.1388\n",
      "Iteration: 38500\n",
      "Gradient: [ 4.540000e-02  1.329260e+01  3.806080e+01 -1.229901e+02 -2.323065e+02]\n",
      "Weights: [-4.259  -1.0782 -0.1361  0.1154  0.0768]\n",
      "MSE loss: 180.0545\n",
      "Iteration: 38600\n",
      "Gradient: [   0.2369   -2.7562   12.0665  -33.8575 -107.4489]\n",
      "Weights: [-4.2654 -1.0774 -0.1369  0.1155  0.0768]\n",
      "MSE loss: 179.9779\n",
      "Iteration: 38700\n",
      "Gradient: [ 11.1396   1.813   11.1911 103.7021 -28.3107]\n",
      "Weights: [-4.2621 -1.0746 -0.1376  0.1154  0.0769]\n",
      "MSE loss: 179.7795\n",
      "Iteration: 38800\n",
      "Gradient: [  13.7887    9.1769  -51.7962  -96.9977 -118.0116]\n",
      "Weights: [-4.2647 -1.0732 -0.1381  0.1154  0.0769]\n",
      "MSE loss: 179.6634\n",
      "Iteration: 38900\n",
      "Gradient: [-1.679   4.6732 17.1331 -9.8956 46.4687]\n",
      "Weights: [-4.2709 -1.0703 -0.1387  0.1154  0.077 ]\n",
      "MSE loss: 179.5439\n",
      "Iteration: 39000\n",
      "Gradient: [ 14.3805 -14.8663   0.3609  76.9743 -55.4036]\n",
      "Weights: [-4.2627 -1.0704 -0.1396  0.1153  0.077 ]\n",
      "MSE loss: 179.4077\n",
      "Iteration: 39100\n",
      "Gradient: [   2.7943   -6.1974   10.2813 -152.0202  377.5482]\n",
      "Weights: [-4.2651 -1.0675 -0.1402  0.1152  0.0771]\n",
      "MSE loss: 179.2666\n",
      "Iteration: 39200\n",
      "Gradient: [  0.4649   9.4692  63.6113 -64.2804  80.3126]\n",
      "Weights: [-4.2649 -1.0653 -0.141   0.1151  0.0771]\n",
      "MSE loss: 179.1285\n",
      "Iteration: 39300\n",
      "Gradient: [ -2.217    8.7657  29.3259 -24.6261 -13.7351]\n",
      "Weights: [-4.2675 -1.0645 -0.1419  0.1151  0.0772]\n",
      "MSE loss: 178.9725\n",
      "Iteration: 39400\n",
      "Gradient: [ -3.1985   2.9823   4.7155 -34.9172 -86.0037]\n",
      "Weights: [-4.2704 -1.0637 -0.1423  0.115   0.0772]\n",
      "MSE loss: 178.8987\n",
      "Iteration: 39500\n",
      "Gradient: [  3.4741 -12.9022 -12.1764 -74.5199 -47.5255]\n",
      "Weights: [-4.2684 -1.0635 -0.1429  0.1149  0.0773]\n",
      "MSE loss: 178.7655\n",
      "Iteration: 39600\n",
      "Gradient: [  -3.3764    5.6538   40.2867  -53.17   -161.5046]\n",
      "Weights: [-4.2685 -1.0625 -0.1438  0.1149  0.0773]\n",
      "MSE loss: 178.6211\n",
      "Iteration: 39700\n",
      "Gradient: [  -4.2038   -2.4796   14.6357 -147.4893 -386.8846]\n",
      "Weights: [-4.267  -1.0603 -0.1444  0.1148  0.0774]\n",
      "MSE loss: 178.4786\n",
      "Iteration: 39800\n",
      "Gradient: [  -1.4061    3.0029   -8.0515  -61.3248 -124.8363]\n",
      "Weights: [-4.2631 -1.0609 -0.1453  0.1147  0.0775]\n",
      "MSE loss: 178.3805\n",
      "Iteration: 39900\n",
      "Gradient: [ -4.0485  -6.9263 -10.37    -5.3289 156.9707]\n",
      "Weights: [-4.2637 -1.0595 -0.1459  0.1147  0.0775]\n",
      "MSE loss: 178.2347\n",
      "Iteration: 40000\n",
      "Gradient: [ -5.3103  12.4     38.8881 -14.3397   5.0575]\n",
      "Weights: [-4.2678 -1.0581 -0.1466  0.1147  0.0776]\n",
      "MSE loss: 178.0905\n",
      "Iteration: 40100\n",
      "Gradient: [  7.931  -10.8468  30.4411   9.4736 305.2669]\n",
      "Weights: [-4.2736 -1.0558 -0.1473  0.1146  0.0777]\n",
      "MSE loss: 177.9549\n",
      "Iteration: 40200\n",
      "Gradient: [ -1.9913  -6.5676 -17.3254  90.7792   4.8453]\n",
      "Weights: [-4.2717 -1.0541 -0.1477  0.1146  0.0777]\n",
      "MSE loss: 177.816\n",
      "Iteration: 40300\n",
      "Gradient: [ -0.3582  -4.7622  46.1299  12.7082 -54.7876]\n",
      "Weights: [-4.2693 -1.0541 -0.1484  0.1144  0.0778]\n",
      "MSE loss: 177.7112\n",
      "Iteration: 40400\n",
      "Gradient: [  1.4192 -13.5887  -5.9126  -1.4936 245.6423]\n",
      "Weights: [-4.2691 -1.0522 -0.149   0.1144  0.0778]\n",
      "MSE loss: 177.5946\n",
      "Iteration: 40500\n",
      "Gradient: [   3.436    -7.5849    6.9224  119.8782 -246.7795]\n",
      "Weights: [-4.276  -1.0515 -0.1497  0.1144  0.0779]\n",
      "MSE loss: 177.5099\n",
      "Iteration: 40600\n",
      "Gradient: [ -1.4927 -18.4074  -7.3678  20.1472 166.0339]\n",
      "Weights: [-4.2762 -1.0488 -0.1502  0.1144  0.0779]\n",
      "MSE loss: 177.3471\n",
      "Iteration: 40700\n",
      "Gradient: [10.0303 13.0736 16.5291 78.655  36.7222]\n",
      "Weights: [-4.2752 -1.0491 -0.1507  0.1143  0.078 ]\n",
      "MSE loss: 177.261\n",
      "Iteration: 40800\n",
      "Gradient: [  2.4644  12.536   13.4764  71.7134 117.8103]\n",
      "Weights: [-4.2704 -1.0475 -0.1514  0.1142  0.0781]\n",
      "MSE loss: 177.1303\n",
      "Iteration: 40900\n",
      "Gradient: [  4.0377 -11.2345 -65.7621  14.5961 -16.5186]\n",
      "Weights: [-4.2729 -1.046  -0.1521  0.1141  0.0781]\n",
      "MSE loss: 176.9817\n",
      "Iteration: 41000\n",
      "Gradient: [ 13.6325  28.7937  -9.5916 -61.4755 190.2143]\n",
      "Weights: [-4.2721 -1.0447 -0.1526  0.114   0.0782]\n",
      "MSE loss: 176.8767\n",
      "Iteration: 41100\n",
      "Gradient: [  14.5287  -33.8683   31.9832  115.0486 -275.9921]\n",
      "Weights: [-4.2718 -1.0432 -0.1534  0.1141  0.0782]\n",
      "MSE loss: 176.7617\n",
      "Iteration: 41200\n",
      "Gradient: [ -4.2157 -16.6937  60.2897 181.7352  89.7354]\n",
      "Weights: [-4.2723 -1.042  -0.154   0.1139  0.0782]\n",
      "MSE loss: 176.6414\n",
      "Iteration: 41300\n",
      "Gradient: [   3.4098   28.4618    6.6783   84.6863 -316.0156]\n",
      "Weights: [-4.2775 -1.0409 -0.1544  0.1139  0.0783]\n",
      "MSE loss: 176.5388\n",
      "Iteration: 41400\n",
      "Gradient: [ 1.799000e-01 -1.453800e+01  1.199620e+01  2.587220e+01 -4.120077e+02]\n",
      "Weights: [-4.276  -1.04   -0.1551  0.1138  0.0783]\n",
      "MSE loss: 176.4301\n",
      "Iteration: 41500\n",
      "Gradient: [-1.040000e-01  5.795400e+00  1.551450e+01  6.746560e+01  1.391175e+02]\n",
      "Weights: [-4.2741 -1.0388 -0.1557  0.1138  0.0784]\n",
      "MSE loss: 176.3038\n",
      "Iteration: 41600\n",
      "Gradient: [   8.8287   -1.9495   -6.2592    6.3277 -206.9201]\n",
      "Weights: [-4.2709 -1.0379 -0.1568  0.1137  0.0784]\n",
      "MSE loss: 176.1802\n",
      "Iteration: 41700\n",
      "Gradient: [-16.112   -3.9785  14.1123 -11.936  168.5596]\n",
      "Weights: [-4.2753 -1.0373 -0.1573  0.1136  0.0785]\n",
      "MSE loss: 176.0398\n",
      "Iteration: 41800\n",
      "Gradient: [  5.5126  19.9201 -31.6802  65.5059  65.4584]\n",
      "Weights: [-4.2776 -1.0345 -0.1579  0.1136  0.0786]\n",
      "MSE loss: 175.8984\n",
      "Iteration: 41900\n",
      "Gradient: [   5.6284  -33.3931   -2.408   177.665  -152.3206]\n",
      "Weights: [-4.2786 -1.033  -0.1589  0.1135  0.0786]\n",
      "MSE loss: 175.7347\n",
      "Iteration: 42000\n",
      "Gradient: [  2.9618  -2.93    41.0957 -36.1056  10.2903]\n",
      "Weights: [-4.2846 -1.031  -0.1589  0.1134  0.0787]\n",
      "MSE loss: 175.6703\n",
      "Iteration: 42100\n",
      "Gradient: [  7.3022  17.7101  20.5605 -51.6216 212.554 ]\n",
      "Weights: [-4.2888 -1.028  -0.1595  0.1134  0.0788]\n",
      "MSE loss: 175.5568\n",
      "Iteration: 42200\n",
      "Gradient: [ -2.5111  -9.8357  74.9962 -67.8246 299.5226]\n",
      "Weights: [-4.2858 -1.0262 -0.1603  0.1133  0.0788]\n",
      "MSE loss: 175.3937\n",
      "Iteration: 42300\n",
      "Gradient: [   0.7289   -1.7252   39.6731  152.0085 -482.884 ]\n",
      "Weights: [-4.2786 -1.0253 -0.1614  0.1131  0.0789]\n",
      "MSE loss: 175.2341\n",
      "Iteration: 42400\n",
      "Gradient: [   1.0313  -14.8655   57.7996  -81.5192 -410.7742]\n",
      "Weights: [-4.2819 -1.0254 -0.1621  0.1132  0.0789]\n",
      "MSE loss: 175.1066\n",
      "Iteration: 42500\n",
      "Gradient: [  11.9594   11.8303  -20.035    13.4447 -137.4615]\n",
      "Weights: [-4.2822 -1.025  -0.1625  0.1132  0.079 ]\n",
      "MSE loss: 175.0039\n",
      "Iteration: 42600\n",
      "Gradient: [ -8.7223 -19.457   60.363   18.9228 -45.9471]\n",
      "Weights: [-4.2807 -1.0242 -0.1633  0.113   0.079 ]\n",
      "MSE loss: 174.8689\n",
      "Iteration: 42700\n",
      "Gradient: [  -6.9904    0.9915   11.7682  -91.3978 -189.6363]\n",
      "Weights: [-4.2788 -1.022  -0.1644  0.1129  0.0791]\n",
      "MSE loss: 174.6925\n",
      "Iteration: 42800\n",
      "Gradient: [  -0.7876    0.9581   23.3256 -177.1075 -384.5193]\n",
      "Weights: [-4.2788 -1.0225 -0.1651  0.1129  0.0792]\n",
      "MSE loss: 174.6016\n",
      "Iteration: 42900\n",
      "Gradient: [  -2.8382   -8.5042   17.1967 -128.5118 -146.5215]\n",
      "Weights: [-4.2844 -1.0213 -0.1653  0.1129  0.0792]\n",
      "MSE loss: 174.5318\n",
      "Iteration: 43000\n",
      "Gradient: [ -10.1828   -9.6968  -19.3147   83.5242 -486.9148]\n",
      "Weights: [-4.2798 -1.0207 -0.166   0.1129  0.0793]\n",
      "MSE loss: 174.3889\n",
      "Iteration: 43100\n",
      "Gradient: [ -6.4746  10.7408  43.1669 -14.9849  89.0788]\n",
      "Weights: [-4.2812 -1.0198 -0.1664  0.1128  0.0794]\n",
      "MSE loss: 174.2579\n",
      "Iteration: 43200\n",
      "Gradient: [ 10.7037  -8.1865 -20.2623 -52.3581  27.259 ]\n",
      "Weights: [-4.2824 -1.0193 -0.167   0.1128  0.0794]\n",
      "MSE loss: 174.1483\n",
      "Iteration: 43300\n",
      "Gradient: [   2.9342   10.3296   23.2022  101.3536 -136.6913]\n",
      "Weights: [-4.2763 -1.0188 -0.1678  0.1127  0.0795]\n",
      "MSE loss: 174.0705\n",
      "Iteration: 43400\n",
      "Gradient: [  -7.1382  -20.2108   29.0125 -100.7066   54.2164]\n",
      "Weights: [-4.2774 -1.0182 -0.1686  0.1127  0.0796]\n",
      "MSE loss: 173.9009\n",
      "Iteration: 43500\n",
      "Gradient: [ 1.941000e-01 -1.654000e+01  2.600100e+01 -3.826970e+01 -2.522314e+02]\n",
      "Weights: [-4.2757 -1.019  -0.1693  0.1127  0.0796]\n",
      "MSE loss: 173.8288\n",
      "Iteration: 43600\n",
      "Gradient: [  -6.8278   10.7772   44.1497   23.7852 -111.6767]\n",
      "Weights: [-4.2819 -1.0166 -0.1702  0.1127  0.0797]\n",
      "MSE loss: 173.6498\n",
      "Iteration: 43700\n",
      "Gradient: [10.5287 -4.4777 -4.8469 89.7789 30.6474]\n",
      "Weights: [-4.2764 -1.0152 -0.1708  0.1126  0.0798]\n",
      "MSE loss: 173.5475\n",
      "Iteration: 43800\n",
      "Gradient: [  -1.9209    8.876    -8.8748   78.7806 -163.8066]\n",
      "Weights: [-4.2829 -1.0148 -0.1714  0.1125  0.0798]\n",
      "MSE loss: 173.4207\n",
      "Iteration: 43900\n",
      "Gradient: [   4.5254   -3.4291   23.9138   53.6042 -144.7442]\n",
      "Weights: [-4.2729 -1.0152 -0.1722  0.1125  0.0799]\n",
      "MSE loss: 173.351\n",
      "Iteration: 44000\n",
      "Gradient: [ -11.281    -6.9167   33.2909 -102.13     84.9826]\n",
      "Weights: [-4.2796 -1.0135 -0.1727  0.1125  0.08  ]\n",
      "MSE loss: 173.1728\n",
      "Iteration: 44100\n",
      "Gradient: [ -15.3397    6.5566   33.4134   -3.2053 -243.0419]\n",
      "Weights: [-4.2798 -1.0111 -0.1734  0.1125  0.08  ]\n",
      "MSE loss: 173.0381\n",
      "Iteration: 44200\n",
      "Gradient: [  -4.3831   -6.3296   35.6277 -188.0283 -246.2438]\n",
      "Weights: [-4.2737 -1.0115 -0.1745  0.1124  0.0801]\n",
      "MSE loss: 172.9582\n",
      "Iteration: 44300\n",
      "Gradient: [ -11.3096  -16.3256   -5.6439   31.9212 -373.2499]\n",
      "Weights: [-4.2794 -1.0096 -0.1751  0.1124  0.0801]\n",
      "MSE loss: 172.7694\n",
      "Iteration: 44400\n",
      "Gradient: [ -19.6732   21.2987  -28.8551  -33.0029 -230.6559]\n",
      "Weights: [-4.2809 -1.0076 -0.1759  0.1123  0.0802]\n",
      "MSE loss: 172.6358\n",
      "Iteration: 44500\n",
      "Gradient: [  14.1858    4.5289   58.0637  -41.2374 -490.327 ]\n",
      "Weights: [-4.2793 -1.0058 -0.1763  0.1123  0.0802]\n",
      "MSE loss: 172.5794\n",
      "Iteration: 44600\n",
      "Gradient: [  -4.0448   18.8092   41.685    -2.6367 -157.5306]\n",
      "Weights: [-4.2783 -1.0063 -0.1769  0.1123  0.0802]\n",
      "MSE loss: 172.4965\n",
      "Iteration: 44700\n",
      "Gradient: [ 0.1043 -1.09   18.8712 75.8282 41.7184]\n",
      "Weights: [-4.2779 -1.0052 -0.1771  0.1122  0.0803]\n",
      "MSE loss: 172.4409\n",
      "Iteration: 44800\n",
      "Gradient: [ -7.252   -3.3451  14.4127  83.4148 221.5813]\n",
      "Weights: [-4.2857 -1.0037 -0.1777  0.1122  0.0803]\n",
      "MSE loss: 172.3012\n",
      "Iteration: 44900\n",
      "Gradient: [  13.8884  -11.155    44.4531   -4.3595 -393.775 ]\n",
      "Weights: [-4.2813 -1.0003 -0.1783  0.1121  0.0804]\n",
      "MSE loss: 172.2092\n",
      "Iteration: 45000\n",
      "Gradient: [   0.4558  -11.5648   19.6393 -122.6045 -426.4216]\n",
      "Weights: [-4.2828 -0.9989 -0.1793  0.112   0.0804]\n",
      "MSE loss: 172.0266\n",
      "Iteration: 45100\n",
      "Gradient: [11.5914 16.2131 74.9382 58.3057 54.7106]\n",
      "Weights: [-4.2811 -0.9978 -0.1805  0.112   0.0805]\n",
      "MSE loss: 171.8813\n",
      "Iteration: 45200\n",
      "Gradient: [  0.9982 -22.1687 -12.2647  58.791  334.7616]\n",
      "Weights: [-4.285  -0.9966 -0.1809  0.112   0.0805]\n",
      "MSE loss: 171.7799\n",
      "Iteration: 45300\n",
      "Gradient: [  8.7026 -10.7397   4.5488  74.3847  12.2677]\n",
      "Weights: [-4.2856 -0.9948 -0.1816  0.112   0.0805]\n",
      "MSE loss: 171.6425\n",
      "Iteration: 45400\n",
      "Gradient: [ -15.911    19.4485   73.189    97.3407 -282.0831]\n",
      "Weights: [-4.2912 -0.9937 -0.1822  0.112   0.0806]\n",
      "MSE loss: 171.5542\n",
      "Iteration: 45500\n",
      "Gradient: [   6.4323   14.5806    8.7343 -188.2029   -8.4815]\n",
      "Weights: [-4.2885 -0.9914 -0.1827  0.1119  0.0807]\n",
      "MSE loss: 171.3998\n",
      "Iteration: 45600\n",
      "Gradient: [  6.6249   3.302   -4.7148  32.0271 149.9611]\n",
      "Weights: [-4.2863 -0.9907 -0.1836  0.1119  0.0807]\n",
      "MSE loss: 171.2985\n",
      "Iteration: 45700\n",
      "Gradient: [  2.0567  11.4721  51.7147  74.2232 -27.6572]\n",
      "Weights: [-4.2884 -0.9892 -0.1844  0.1118  0.0808]\n",
      "MSE loss: 171.1456\n",
      "Iteration: 45800\n",
      "Gradient: [-16.5867  -9.6695 -12.7976 153.6217 -50.6571]\n",
      "Weights: [-4.2916 -0.9869 -0.185   0.1117  0.0808]\n",
      "MSE loss: 171.0074\n",
      "Iteration: 45900\n",
      "Gradient: [-15.0183   1.8765  -8.5845 -58.0288 121.7463]\n",
      "Weights: [-4.2867 -0.9866 -0.1859  0.1117  0.0808]\n",
      "MSE loss: 170.9161\n",
      "Iteration: 46000\n",
      "Gradient: [12.0679 -4.9008 49.077  15.517  24.5362]\n",
      "Weights: [-4.2886 -0.986  -0.1864  0.1116  0.0809]\n",
      "MSE loss: 170.8047\n",
      "Iteration: 46100\n",
      "Gradient: [-16.2666  -6.3521   0.5273  10.9533 300.1884]\n",
      "Weights: [-4.2951 -0.9848 -0.1869  0.1116  0.081 ]\n",
      "MSE loss: 170.736\n",
      "Iteration: 46200\n",
      "Gradient: [  2.9708  15.233   56.5408  29.7361 289.4567]\n",
      "Weights: [-4.2922 -0.9822 -0.1875  0.1116  0.081 ]\n",
      "MSE loss: 170.5521\n",
      "Iteration: 46300\n",
      "Gradient: [  -5.8755    8.1961    1.5589  -62.8749 -147.4683]\n",
      "Weights: [-4.2932 -0.9815 -0.1886  0.1114  0.0811]\n",
      "MSE loss: 170.4168\n",
      "Iteration: 46400\n",
      "Gradient: [  -3.8778  -13.4016   -5.5749   21.9953 -142.9674]\n",
      "Weights: [-4.2924 -0.9786 -0.1893  0.1113  0.0811]\n",
      "MSE loss: 170.257\n",
      "Iteration: 46500\n",
      "Gradient: [   7.0652   17.0995   39.4484    5.4886 -245.4989]\n",
      "Weights: [-4.2973 -0.9746 -0.1904  0.1113  0.0812]\n",
      "MSE loss: 170.056\n",
      "Iteration: 46600\n",
      "Gradient: [  16.7408   -0.3062   39.4003  123.9266 -235.9283]\n",
      "Weights: [-4.2933 -0.9758 -0.1908  0.1113  0.0812]\n",
      "MSE loss: 169.9969\n",
      "Iteration: 46700\n",
      "Gradient: [  -4.4752   12.7035  -29.4833  -55.1865 -188.2203]\n",
      "Weights: [-4.2972 -0.9745 -0.1914  0.1113  0.0813]\n",
      "MSE loss: 169.8937\n",
      "Iteration: 46800\n",
      "Gradient: [ 4.04000e-02  5.43030e+00 -1.82847e+01 -6.52650e+00 -4.22429e+01]\n",
      "Weights: [-4.2969 -0.9742 -0.1923  0.1113  0.0814]\n",
      "MSE loss: 169.7599\n",
      "Iteration: 46900\n",
      "Gradient: [  1.4162  -4.2987   8.1099  30.9984 157.621 ]\n",
      "Weights: [-4.2985 -0.9732 -0.1928  0.1113  0.0814]\n",
      "MSE loss: 169.6881\n",
      "Iteration: 47000\n",
      "Gradient: [  -0.5086  -15.7524   -0.965   137.4017 -347.2482]\n",
      "Weights: [-4.293  -0.9722 -0.1933  0.1112  0.0814]\n",
      "MSE loss: 169.5784\n",
      "Iteration: 47100\n",
      "Gradient: [ 1.920700e+00 -2.108000e-01  9.297400e+00  8.928330e+01 -3.368019e+02]\n",
      "Weights: [-4.2941 -0.9708 -0.194   0.1111  0.0815]\n",
      "MSE loss: 169.4649\n",
      "Iteration: 47200\n",
      "Gradient: [ -6.5698 -21.501  -34.2577  20.2108 -89.5799]\n",
      "Weights: [-4.2954 -0.9689 -0.1946  0.1112  0.0816]\n",
      "MSE loss: 169.3439\n",
      "Iteration: 47300\n",
      "Gradient: [  6.6812 -29.5974  39.9831 -51.541  131.6888]\n",
      "Weights: [-4.2965 -0.9689 -0.1954  0.1112  0.0816]\n",
      "MSE loss: 169.2226\n",
      "Iteration: 47400\n",
      "Gradient: [  6.4772   4.4    -27.3785 -27.541  -15.1046]\n",
      "Weights: [-4.2988 -0.9676 -0.1957  0.1111  0.0816]\n",
      "MSE loss: 169.1565\n",
      "Iteration: 47500\n",
      "Gradient: [ -7.4981   1.9687  31.6425   8.0965 157.1751]\n",
      "Weights: [-4.3043 -0.9651 -0.1963  0.111   0.0817]\n",
      "MSE loss: 169.0688\n",
      "Iteration: 47600\n",
      "Gradient: [ -5.4157  -1.6058  52.7438  57.2444 -99.9425]\n",
      "Weights: [-4.2982 -0.9638 -0.1969  0.111   0.0818]\n",
      "MSE loss: 168.913\n",
      "Iteration: 47700\n",
      "Gradient: [ 2.041000e-01  1.080340e+01 -4.572100e+00 -2.195458e+02 -1.672918e+02]\n",
      "Weights: [-4.2987 -0.9642 -0.1978  0.111   0.0818]\n",
      "MSE loss: 168.796\n",
      "Iteration: 47800\n",
      "Gradient: [ -0.8749  -7.0295  40.1288  27.2382 149.6835]\n",
      "Weights: [-4.2963 -0.963  -0.1986  0.1109  0.0819]\n",
      "MSE loss: 168.6574\n",
      "Iteration: 47900\n",
      "Gradient: [  13.2362   11.2822   13.9414   63.4636 -289.7994]\n",
      "Weights: [-4.3005 -0.9615 -0.199   0.1108  0.0819]\n",
      "MSE loss: 168.5639\n",
      "Iteration: 48000\n",
      "Gradient: [   5.9524   10.0175  -11.7311  -15.0799 -299.341 ]\n",
      "Weights: [-4.298  -0.961  -0.1999  0.1108  0.082 ]\n",
      "MSE loss: 168.4357\n",
      "Iteration: 48100\n",
      "Gradient: [ -9.375  -23.7888   6.3551  38.2369  60.421 ]\n",
      "Weights: [-4.301  -0.9593 -0.2002  0.1107  0.0821]\n",
      "MSE loss: 168.3194\n",
      "Iteration: 48200\n",
      "Gradient: [ 5.65600e-01 -1.08550e+00 -1.81000e-02  6.30831e+01 -3.50681e+01]\n",
      "Weights: [-4.3023 -0.9568 -0.2009  0.1107  0.0821]\n",
      "MSE loss: 168.22\n",
      "Iteration: 48300\n",
      "Gradient: [-13.2385  -1.827   22.3342  24.5477  66.3366]\n",
      "Weights: [-4.2998 -0.9569 -0.2013  0.1106  0.0821]\n",
      "MSE loss: 168.1523\n",
      "Iteration: 48400\n",
      "Gradient: [  -3.184    -8.2267  -41.9917  -17.9573 -115.8747]\n",
      "Weights: [-4.3008 -0.9561 -0.2021  0.1105  0.0822]\n",
      "MSE loss: 168.014\n",
      "Iteration: 48500\n",
      "Gradient: [ -17.2344   -1.6945   32.1641  -42.4865 -522.9264]\n",
      "Weights: [-4.2965 -0.9539 -0.2031  0.1103  0.0822]\n",
      "MSE loss: 167.8745\n",
      "Iteration: 48600\n",
      "Gradient: [  -1.9448    7.3731   55.6034   16.2277 -312.4654]\n",
      "Weights: [-4.2985 -0.9521 -0.2039  0.1102  0.0823]\n",
      "MSE loss: 167.7014\n",
      "Iteration: 48700\n",
      "Gradient: [  -8.8104   13.4646   50.5583  -46.0159 -102.2817]\n",
      "Weights: [-4.3016 -0.9507 -0.2044  0.1103  0.0824]\n",
      "MSE loss: 167.5888\n",
      "Iteration: 48800\n",
      "Gradient: [ -18.894    -8.7366   49.0222 -108.8474  -22.5061]\n",
      "Weights: [-4.3059 -0.9489 -0.2051  0.1101  0.0824]\n",
      "MSE loss: 167.4753\n",
      "Iteration: 48900\n",
      "Gradient: [  4.1792 -20.9394 -20.9415  89.7455 270.1839]\n",
      "Weights: [-4.306  -0.9459 -0.2054  0.1101  0.0825]\n",
      "MSE loss: 167.3619\n",
      "Iteration: 49000\n",
      "Gradient: [   9.3824   -3.3555   13.0548   69.4    -128.3687]\n",
      "Weights: [-4.3048 -0.9457 -0.206   0.1101  0.0825]\n",
      "MSE loss: 167.2907\n",
      "Iteration: 49100\n",
      "Gradient: [-6.3006  6.4177 -8.4001 16.1282 -8.7258]\n",
      "Weights: [-4.3039 -0.9459 -0.2068  0.11    0.0825]\n",
      "MSE loss: 167.1932\n",
      "Iteration: 49200\n",
      "Gradient: [  -2.3739   15.2463    0.6064  -22.3785 -349.0905]\n",
      "Weights: [-4.3007 -0.9463 -0.2077  0.1099  0.0826]\n",
      "MSE loss: 167.0849\n",
      "Iteration: 49300\n",
      "Gradient: [   7.8932    3.3782   12.0657  -22.1705 -132.3918]\n",
      "Weights: [-4.3018 -0.9464 -0.208   0.1099  0.0827]\n",
      "MSE loss: 167.0168\n",
      "Iteration: 49400\n",
      "Gradient: [ 6.820300e+00  4.480000e-02 -1.379460e+01 -2.304290e+01  1.107427e+02]\n",
      "Weights: [-4.3022 -0.9446 -0.2088  0.1099  0.0827]\n",
      "MSE loss: 166.8593\n",
      "Iteration: 49500\n",
      "Gradient: [  2.5874   1.4311 -52.8285 -56.1623  47.1844]\n",
      "Weights: [-4.3018 -0.9432 -0.2093  0.1098  0.0828]\n",
      "MSE loss: 166.7443\n",
      "Iteration: 49600\n",
      "Gradient: [   3.5205    5.2074   14.4443  -79.0428 -147.8193]\n",
      "Weights: [-4.3008 -0.9426 -0.2094  0.1097  0.0828]\n",
      "MSE loss: 166.6923\n",
      "Iteration: 49700\n",
      "Gradient: [ -4.7398   3.6667  80.0599 132.6499 -92.2094]\n",
      "Weights: [-4.303  -0.9417 -0.2105  0.1097  0.0829]\n",
      "MSE loss: 166.5211\n",
      "Iteration: 49800\n",
      "Gradient: [-6.278900e+00  3.266000e-01  3.924430e+01 -1.722590e+01 -4.454044e+02]\n",
      "Weights: [-4.3041 -0.941  -0.2112  0.1097  0.083 ]\n",
      "MSE loss: 166.4128\n",
      "Iteration: 49900\n",
      "Gradient: [  -1.7196   -0.6563   -3.1128   -4.4716 -173.1296]\n",
      "Weights: [-4.305  -0.9387 -0.2115  0.1097  0.083 ]\n",
      "MSE loss: 166.315\n",
      "Iteration: 50000\n",
      "Gradient: [   1.8221    5.3365   50.1906   87.9838 -587.4364]\n",
      "Weights: [-4.3064 -0.9386 -0.2118  0.1096  0.0831]\n",
      "MSE loss: 166.244\n",
      "Iteration: 50100\n",
      "Gradient: [   0.6061    2.1715   29.1375    8.8801 -132.4161]\n",
      "Weights: [-4.303  -0.9375 -0.2124  0.1096  0.0831]\n",
      "MSE loss: 166.1583\n",
      "Iteration: 50200\n",
      "Gradient: [   9.3638   -4.8913    9.3362 -100.8787   32.3764]\n",
      "Weights: [-4.3053 -0.9356 -0.2132  0.1096  0.0831]\n",
      "MSE loss: 166.0286\n",
      "Iteration: 50300\n",
      "Gradient: [ -6.0273  10.6101  20.143   11.0366 217.678 ]\n",
      "Weights: [-4.3037 -0.9343 -0.214   0.1095  0.0832]\n",
      "MSE loss: 165.911\n",
      "Iteration: 50400\n",
      "Gradient: [  -5.2421  -14.2238  -10.8199   36.1147 -144.583 ]\n",
      "Weights: [-4.3072 -0.9338 -0.2143  0.1094  0.0833]\n",
      "MSE loss: 165.8142\n",
      "Iteration: 50500\n",
      "Gradient: [  7.5253 -11.4393  10.7998 -85.0395 -97.8671]\n",
      "Weights: [-4.3098 -0.932  -0.2147  0.1094  0.0833]\n",
      "MSE loss: 165.7187\n",
      "Iteration: 50600\n",
      "Gradient: [   2.6273   14.5973   23.8012  -94.7743 -129.5189]\n",
      "Weights: [-4.3073 -0.9308 -0.2154  0.1094  0.0834]\n",
      "MSE loss: 165.6052\n",
      "Iteration: 50700\n",
      "Gradient: [ 6.9788 -2.9685 31.3983 -7.9368 -7.7579]\n",
      "Weights: [-4.3112 -0.929  -0.2163  0.1094  0.0835]\n",
      "MSE loss: 165.4478\n",
      "Iteration: 50800\n",
      "Gradient: [ -7.0339 -16.7423  26.9457 113.6101 109.2698]\n",
      "Weights: [-4.3175 -0.9271 -0.2165  0.1093  0.0835]\n",
      "MSE loss: 165.4092\n",
      "Iteration: 50900\n",
      "Gradient: [ -0.6613  -3.0929 -29.4332 113.6771 199.9347]\n",
      "Weights: [-4.3081 -0.9268 -0.2176  0.1091  0.0835]\n",
      "MSE loss: 165.2296\n",
      "Iteration: 51000\n",
      "Gradient: [   3.9952    6.5745   37.3826   23.1562 -129.5171]\n",
      "Weights: [-4.312  -0.9248 -0.2185  0.1091  0.0836]\n",
      "MSE loss: 165.0918\n",
      "Iteration: 51100\n",
      "Gradient: [ -10.7806   10.332    34.8156   15.1845 -413.8737]\n",
      "Weights: [-4.3083 -0.9242 -0.2195  0.1091  0.0836]\n",
      "MSE loss: 164.9528\n",
      "Iteration: 51200\n",
      "Gradient: [ -6.6197  -1.4743 -16.212   -3.0585  37.8944]\n",
      "Weights: [-4.3102 -0.9236 -0.2202  0.109   0.0837]\n",
      "MSE loss: 164.8935\n",
      "Iteration: 51300\n",
      "Gradient: [ 12.7774   0.9057 -10.1657  75.3415 105.0737]\n",
      "Weights: [-4.3128 -0.9216 -0.2206  0.109   0.0837]\n",
      "MSE loss: 164.7502\n",
      "Iteration: 51400\n",
      "Gradient: [ -0.9811  -9.3816 -37.325   88.1093 -84.5502]\n",
      "Weights: [-4.3141 -0.9193 -0.2209  0.109   0.0838]\n",
      "MSE loss: 164.6628\n",
      "Iteration: 51500\n",
      "Gradient: [ 16.0724   7.2438 -46.3805 -78.799  108.8983]\n",
      "Weights: [-4.3091 -0.9193 -0.2219  0.1089  0.0838]\n",
      "MSE loss: 164.5437\n",
      "Iteration: 51600\n",
      "Gradient: [ 11.6733 -14.3416  35.6901 -47.2915  29.5519]\n",
      "Weights: [-4.3066 -0.9194 -0.2225  0.1088  0.0839]\n",
      "MSE loss: 164.4765\n",
      "Iteration: 51700\n",
      "Gradient: [  -0.8208   -6.0785  -25.9361    9.538  -127.2715]\n",
      "Weights: [-4.312  -0.918  -0.2231  0.1088  0.0839]\n",
      "MSE loss: 164.3511\n",
      "Iteration: 51800\n",
      "Gradient: [  13.6415   -7.186    31.7017 -158.2814 -209.4603]\n",
      "Weights: [-4.3114 -0.9168 -0.2236  0.1088  0.084 ]\n",
      "MSE loss: 164.2293\n",
      "Iteration: 51900\n",
      "Gradient: [ 16.3515   7.819   19.7386 -57.3171 156.3806]\n",
      "Weights: [-4.3157 -0.915  -0.224   0.1089  0.084 ]\n",
      "MSE loss: 164.1585\n",
      "Iteration: 52000\n",
      "Gradient: [ -10.2628   -6.7756    3.7196  140.2848 -411.5064]\n",
      "Weights: [-4.3127 -0.9152 -0.2245  0.1088  0.0841]\n",
      "MSE loss: 164.0768\n",
      "Iteration: 52100\n",
      "Gradient: [  -9.4202  -18.3041   -0.4347  102.3305 -102.4152]\n",
      "Weights: [-4.3116 -0.9136 -0.2253  0.1087  0.0842]\n",
      "MSE loss: 163.9428\n",
      "Iteration: 52200\n",
      "Gradient: [ 10.0564   1.2091   5.9352 -90.1744  34.8804]\n",
      "Weights: [-4.313  -0.9139 -0.2258  0.1086  0.0842]\n",
      "MSE loss: 163.8612\n",
      "Iteration: 52300\n",
      "Gradient: [ -2.747    6.6253 -11.9547  15.0185  -4.7798]\n",
      "Weights: [-4.3087 -0.9113 -0.2267  0.1087  0.0842]\n",
      "MSE loss: 163.7876\n",
      "Iteration: 52400\n",
      "Gradient: [  -7.3564   20.7491    6.2482  -46.7691 -255.3361]\n",
      "Weights: [-4.3135 -0.9107 -0.2272  0.1086  0.0843]\n",
      "MSE loss: 163.6305\n",
      "Iteration: 52500\n",
      "Gradient: [   0.3535    6.5295    9.3582  -57.6481 -231.9155]\n",
      "Weights: [-4.317  -0.9097 -0.2274  0.1086  0.0843]\n",
      "MSE loss: 163.5749\n",
      "Iteration: 52600\n",
      "Gradient: [   0.7815   24.8917   21.0869  -53.2385 -244.761 ]\n",
      "Weights: [-4.3152 -0.9077 -0.2281  0.1086  0.0844]\n",
      "MSE loss: 163.4599\n",
      "Iteration: 52700\n",
      "Gradient: [  5.1523 -15.3605  16.6702  42.8017  64.1953]\n",
      "Weights: [-4.3142 -0.9072 -0.2292  0.1084  0.0845]\n",
      "MSE loss: 163.3002\n",
      "Iteration: 52800\n",
      "Gradient: [ -12.8265  -16.7389   -3.3381  -58.2219 -168.2947]\n",
      "Weights: [-4.3175 -0.9058 -0.2295  0.1084  0.0845]\n",
      "MSE loss: 163.2268\n",
      "Iteration: 52900\n",
      "Gradient: [  -4.4256  -10.6286   16.1931  -40.2441 -763.0428]\n",
      "Weights: [-4.3158 -0.904  -0.2301  0.1083  0.0846]\n",
      "MSE loss: 163.0977\n",
      "Iteration: 53000\n",
      "Gradient: [   5.8463   11.5317  -14.5607 -121.7046  230.8848]\n",
      "Weights: [-4.3166 -0.9023 -0.2306  0.1082  0.0846]\n",
      "MSE loss: 163.0041\n",
      "Iteration: 53100\n",
      "Gradient: [  -0.8188   18.4212   -1.0519    7.5913 -161.1272]\n",
      "Weights: [-4.3208 -0.9015 -0.2314  0.1082  0.0847]\n",
      "MSE loss: 162.882\n",
      "Iteration: 53200\n",
      "Gradient: [ -23.564    -4.4217   23.8033   63.0163 -198.2821]\n",
      "Weights: [-4.3177 -0.9016 -0.2319  0.1081  0.0847]\n",
      "MSE loss: 162.7799\n",
      "Iteration: 53300\n",
      "Gradient: [   0.7606    2.1821   -3.9685   19.9421 -112.7521]\n",
      "Weights: [-4.3167 -0.9014 -0.2324  0.1081  0.0848]\n",
      "MSE loss: 162.6982\n",
      "Iteration: 53400\n",
      "Gradient: [  -7.8946   -1.1618   27.7988  -41.2895 -166.1731]\n",
      "Weights: [-4.3214 -0.9    -0.2328  0.1081  0.0849]\n",
      "MSE loss: 162.6146\n",
      "Iteration: 53500\n",
      "Gradient: [  -3.6966   10.3461  -26.2754 -117.6056  313.4193]\n",
      "Weights: [-4.3162 -0.8988 -0.2334  0.108   0.0849]\n",
      "MSE loss: 162.5089\n",
      "Iteration: 53600\n",
      "Gradient: [ 7.6126 10.9372 19.8695 26.2304 14.4618]\n",
      "Weights: [-4.3176 -0.8973 -0.2341  0.1079  0.0849]\n",
      "MSE loss: 162.3848\n",
      "Iteration: 53700\n",
      "Gradient: [  16.8141  -21.5724   36.679    79.8985 -107.1376]\n",
      "Weights: [-4.3117 -0.8961 -0.235   0.1078  0.085 ]\n",
      "MSE loss: 162.3297\n",
      "Iteration: 53800\n",
      "Gradient: [   9.4452   -5.655    -9.8676   12.4033 -114.2456]\n",
      "Weights: [-4.3227 -0.8942 -0.2357  0.1077  0.085 ]\n",
      "MSE loss: 162.1535\n",
      "Iteration: 53900\n",
      "Gradient: [ -0.7057   7.7325 -17.949  110.013  197.5017]\n",
      "Weights: [-4.3159 -0.8935 -0.2365  0.1077  0.0851]\n",
      "MSE loss: 162.0211\n",
      "Iteration: 54000\n",
      "Gradient: [ -1.8797   4.4095 -13.0285   6.1734 -97.19  ]\n",
      "Weights: [-4.3214 -0.8924 -0.2369  0.1076  0.0852]\n",
      "MSE loss: 161.9223\n",
      "Iteration: 54100\n",
      "Gradient: [  6.4685  -9.9758  19.3999 -70.6911 121.988 ]\n",
      "Weights: [-4.3209 -0.8902 -0.2375  0.1076  0.0852]\n",
      "MSE loss: 161.7884\n",
      "Iteration: 54200\n",
      "Gradient: [-10.5781  13.9587  48.1655 -68.2757  25.9497]\n",
      "Weights: [-4.3257 -0.8894 -0.2379  0.1076  0.0853]\n",
      "MSE loss: 161.732\n",
      "Iteration: 54300\n",
      "Gradient: [   7.2424  -11.3988  -21.0662 -161.619  -101.6378]\n",
      "Weights: [-4.3195 -0.8886 -0.2388  0.1075  0.0853]\n",
      "MSE loss: 161.6006\n",
      "Iteration: 54400\n",
      "Gradient: [   3.7361   17.005   -19.6059 -115.1988   28.2439]\n",
      "Weights: [-4.32   -0.8885 -0.2395  0.1074  0.0854]\n",
      "MSE loss: 161.4968\n",
      "Iteration: 54500\n",
      "Gradient: [  -9.0115    1.4441   65.862   -11.1546 -233.8703]\n",
      "Weights: [-4.3257 -0.8869 -0.2398  0.1074  0.0854]\n",
      "MSE loss: 161.4218\n",
      "Iteration: 54600\n",
      "Gradient: [   9.6007   -8.117    24.644  -114.124  -325.3782]\n",
      "Weights: [-4.3268 -0.8861 -0.2403  0.1074  0.0855]\n",
      "MSE loss: 161.357\n",
      "Iteration: 54700\n",
      "Gradient: [   4.854    -4.6846  -11.2949   34.4957 -410.5681]\n",
      "Weights: [-4.3222 -0.8853 -0.2408  0.1074  0.0855]\n",
      "MSE loss: 161.2189\n",
      "Iteration: 54800\n",
      "Gradient: [   0.432     9.7632   11.2679   29.0926 -198.1466]\n",
      "Weights: [-4.3298 -0.8823 -0.2413  0.1074  0.0856]\n",
      "MSE loss: 161.1423\n",
      "Iteration: 54900\n",
      "Gradient: [   2.6486   17.3862  -22.5468  152.0088 -110.4209]\n",
      "Weights: [-4.3267 -0.8809 -0.2419  0.1073  0.0856]\n",
      "MSE loss: 161.0122\n",
      "Iteration: 55000\n",
      "Gradient: [  18.0376   11.8504   32.7688  -19.6946 -426.4486]\n",
      "Weights: [-4.3222 -0.8809 -0.243   0.1072  0.0857]\n",
      "MSE loss: 160.8738\n",
      "Iteration: 55100\n",
      "Gradient: [-14.6732  12.4584 -38.7918  54.6497  39.6999]\n",
      "Weights: [-4.3238 -0.8801 -0.2435  0.1071  0.0857]\n",
      "MSE loss: 160.7744\n",
      "Iteration: 55200\n",
      "Gradient: [-10.6453   2.6045  60.8282 -68.7697 -17.89  ]\n",
      "Weights: [-4.3213 -0.8794 -0.244   0.1071  0.0858]\n",
      "MSE loss: 160.7071\n",
      "Iteration: 55300\n",
      "Gradient: [  5.0147   8.8411  24.1273  35.5074 -80.5238]\n",
      "Weights: [-4.32   -0.8782 -0.2449  0.107   0.0858]\n",
      "MSE loss: 160.6005\n",
      "Iteration: 55400\n",
      "Gradient: [  -0.5783    0.2509   74.7766   -7.8462 -196.9559]\n",
      "Weights: [-4.3205 -0.8779 -0.2457  0.1069  0.0858]\n",
      "MSE loss: 160.5012\n",
      "Iteration: 55500\n",
      "Gradient: [ 1.1795  3.5316 49.554  39.4817 17.6141]\n",
      "Weights: [-4.3245 -0.8746 -0.2466  0.1069  0.0859]\n",
      "MSE loss: 160.3181\n",
      "Iteration: 55600\n",
      "Gradient: [   2.5023   -5.0993   -5.7996  -32.7637 -342.5915]\n",
      "Weights: [-4.3253 -0.8746 -0.247   0.1069  0.0859]\n",
      "MSE loss: 160.2668\n",
      "Iteration: 55700\n",
      "Gradient: [   2.2568  -10.9834  -50.056   -49.1308 -655.0756]\n",
      "Weights: [-4.3299 -0.872  -0.2476  0.1068  0.086 ]\n",
      "MSE loss: 160.1365\n",
      "Iteration: 55800\n",
      "Gradient: [-2.973000e-01  2.025470e+01  5.609100e+00 -1.854700e+01 -5.089434e+02]\n",
      "Weights: [-4.322  -0.8714 -0.2483  0.1068  0.0861]\n",
      "MSE loss: 160.0562\n",
      "Iteration: 55900\n",
      "Gradient: [   7.0691   -4.7094  -23.6882  -73.8276 -183.1752]\n",
      "Weights: [-4.3188 -0.873  -0.2492  0.1067  0.0861]\n",
      "MSE loss: 159.9779\n",
      "Iteration: 56000\n",
      "Gradient: [ 2.323300e+00  1.961000e-01  1.354330e+01 -4.355400e+00 -2.228269e+02]\n",
      "Weights: [-4.3203 -0.8718 -0.2499  0.1068  0.0862]\n",
      "MSE loss: 159.8494\n",
      "Iteration: 56100\n",
      "Gradient: [   0.4254  -14.0922   27.388   133.9105 -159.6105]\n",
      "Weights: [-4.3189 -0.8714 -0.2503  0.1068  0.0863]\n",
      "MSE loss: 159.7686\n",
      "Iteration: 56200\n",
      "Gradient: [  8.4745 -18.4896 -52.8805  41.4091 -18.0804]\n",
      "Weights: [-4.3214 -0.8712 -0.251   0.1068  0.0863]\n",
      "MSE loss: 159.6797\n",
      "Iteration: 56300\n",
      "Gradient: [ -3.6352 -13.8273   1.2228   8.6141 176.6957]\n",
      "Weights: [-4.321  -0.87   -0.2513  0.1068  0.0863]\n",
      "MSE loss: 159.5974\n",
      "Iteration: 56400\n",
      "Gradient: [  12.76    -27.0895   -8.5351   28.1227 -121.9399]\n",
      "Weights: [-4.3222 -0.868  -0.2524  0.1068  0.0863]\n",
      "MSE loss: 159.4751\n",
      "Iteration: 56500\n",
      "Gradient: [ -18.9473  -14.4153  -38.8291  -35.9739 -240.0062]\n",
      "Weights: [-4.3234 -0.8667 -0.253   0.1068  0.0864]\n",
      "MSE loss: 159.3664\n",
      "Iteration: 56600\n",
      "Gradient: [  0.1099  11.4917  73.1201 -33.6884 -73.6307]\n",
      "Weights: [-4.3231 -0.864  -0.2538  0.1068  0.0865]\n",
      "MSE loss: 159.2256\n",
      "Iteration: 56700\n",
      "Gradient: [   1.0653   11.3817    0.7271   96.3094 -270.2985]\n",
      "Weights: [-4.3226 -0.8635 -0.2546  0.1067  0.0865]\n",
      "MSE loss: 159.1278\n",
      "Iteration: 56800\n",
      "Gradient: [  13.8147  -18.6293   -1.9664  -39.6571 -196.8208]\n",
      "Weights: [-4.3213 -0.8642 -0.2553  0.1067  0.0866]\n",
      "MSE loss: 159.0545\n",
      "Iteration: 56900\n",
      "Gradient: [  1.9529  20.3526 -15.9217  63.6597 106.801 ]\n",
      "Weights: [-4.3227 -0.8623 -0.2558  0.1067  0.0866]\n",
      "MSE loss: 158.9309\n",
      "Iteration: 57000\n",
      "Gradient: [ -19.0097   13.4432  -29.6795   56.5991 -143.0314]\n",
      "Weights: [-4.3253 -0.8624 -0.2565  0.1066  0.0867]\n",
      "MSE loss: 158.8152\n",
      "Iteration: 57100\n",
      "Gradient: [   2.3129  -12.5777    2.4213   -4.8546 -164.8283]\n",
      "Weights: [-4.3235 -0.8622 -0.257   0.1067  0.0868]\n",
      "MSE loss: 158.7345\n",
      "Iteration: 57200\n",
      "Gradient: [  1.0659 -13.4866  37.492  -51.5931  -8.3458]\n",
      "Weights: [-4.3233 -0.8603 -0.2576  0.1068  0.0868]\n",
      "MSE loss: 158.6519\n",
      "Iteration: 57300\n",
      "Gradient: [ -4.5144  12.1326  20.8906  73.3793 138.4285]\n",
      "Weights: [-4.3264 -0.8583 -0.2586  0.1068  0.0868]\n",
      "MSE loss: 158.4879\n",
      "Iteration: 57400\n",
      "Gradient: [   1.342     5.9781   33.2658   -4.1071 -262.1376]\n",
      "Weights: [-4.3295 -0.8571 -0.2588  0.1068  0.0869]\n",
      "MSE loss: 158.3987\n",
      "Iteration: 57500\n",
      "Gradient: [  -4.6635  -14.745     3.493    15.7642 -379.3364]\n",
      "Weights: [-4.3267 -0.8562 -0.2597  0.1067  0.087 ]\n",
      "MSE loss: 158.2774\n",
      "Iteration: 57600\n",
      "Gradient: [ 11.6317  -9.3756   8.3452  45.2375 -95.0858]\n",
      "Weights: [-4.3274 -0.8533 -0.2606  0.1067  0.087 ]\n",
      "MSE loss: 158.1428\n",
      "Iteration: 57700\n",
      "Gradient: [ -1.0235  -1.8142  -5.1989  20.6034 174.4003]\n",
      "Weights: [-4.3312 -0.8521 -0.2613  0.1066  0.0871]\n",
      "MSE loss: 157.9977\n",
      "Iteration: 57800\n",
      "Gradient: [-20.8295   2.4873  36.9317 -25.9624 214.3676]\n",
      "Weights: [-4.3291 -0.8507 -0.2622  0.1066  0.0871]\n",
      "MSE loss: 157.8915\n",
      "Iteration: 57900\n",
      "Gradient: [ -15.4731    7.8398  -16.583    44.2413 -156.9674]\n",
      "Weights: [-4.3325 -0.8488 -0.2629  0.1066  0.0872]\n",
      "MSE loss: 157.7706\n",
      "Iteration: 58000\n",
      "Gradient: [   1.5983   22.4186   13.0433  -36.5376 -141.2672]\n",
      "Weights: [-4.3322 -0.8461 -0.2635  0.1065  0.0872]\n",
      "MSE loss: 157.6578\n",
      "Iteration: 58100\n",
      "Gradient: [ -2.4872 -20.7452  39.9533 -50.6531 193.3643]\n",
      "Weights: [-4.3327 -0.8459 -0.2644  0.1065  0.0873]\n",
      "MSE loss: 157.5426\n",
      "Iteration: 58200\n",
      "Gradient: [   5.6567   -1.9795    1.3995 -117.094  -192.0926]\n",
      "Weights: [-4.3367 -0.8443 -0.2651  0.1064  0.0873]\n",
      "MSE loss: 157.4397\n",
      "Iteration: 58300\n",
      "Gradient: [-15.8536  10.5506 -10.3837 -39.8291 115.6711]\n",
      "Weights: [-4.3371 -0.8416 -0.2652  0.1062  0.0874]\n",
      "MSE loss: 157.3407\n",
      "Iteration: 58400\n",
      "Gradient: [  -9.0162    2.4995  -15.0414   54.9031 -582.9772]\n",
      "Weights: [-4.3347 -0.8415 -0.2661  0.1062  0.0874]\n",
      "MSE loss: 157.2359\n",
      "Iteration: 58500\n",
      "Gradient: [ -4.2647   7.9049 -31.6012 -44.4822  10.711 ]\n",
      "Weights: [-4.3303 -0.8419 -0.2669  0.1062  0.0874]\n",
      "MSE loss: 157.1766\n",
      "Iteration: 58600\n",
      "Gradient: [-13.2636   3.7481  10.8694 -30.8933 -42.5462]\n",
      "Weights: [-4.3334 -0.8404 -0.2674  0.1062  0.0875]\n",
      "MSE loss: 157.0676\n",
      "Iteration: 58700\n",
      "Gradient: [ 19.8705 -19.3394  -7.2503  82.9034  31.0076]\n",
      "Weights: [-4.3339 -0.8389 -0.2679  0.1062  0.0875]\n",
      "MSE loss: 156.9702\n",
      "Iteration: 58800\n",
      "Gradient: [ -4.6165 -21.345   17.5166   9.1655 224.5673]\n",
      "Weights: [-4.3389 -0.8377 -0.2683  0.1062  0.0876]\n",
      "MSE loss: 156.9021\n",
      "Iteration: 58900\n",
      "Gradient: [-14.3899   5.0437 -12.2205 121.5968  50.0301]\n",
      "Weights: [-4.3365 -0.8362 -0.2693  0.1061  0.0876]\n",
      "MSE loss: 156.7571\n",
      "Iteration: 59000\n",
      "Gradient: [  20.6955   -1.945     9.023    91.6824 -214.739 ]\n",
      "Weights: [-4.3322 -0.8352 -0.2699  0.1061  0.0877]\n",
      "MSE loss: 156.69\n",
      "Iteration: 59100\n",
      "Gradient: [   0.3693    3.3296    9.7354  123.8521 -213.4049]\n",
      "Weights: [-4.3405 -0.834  -0.2703  0.1062  0.0878]\n",
      "MSE loss: 156.5733\n",
      "Iteration: 59200\n",
      "Gradient: [   8.3872  -14.2687    7.7183  -55.9179 -155.3994]\n",
      "Weights: [-4.3357 -0.8332 -0.2712  0.106   0.0878]\n",
      "MSE loss: 156.4424\n",
      "Iteration: 59300\n",
      "Gradient: [   2.9677  -15.3001    3.1865  -51.6601 -246.7719]\n",
      "Weights: [-4.3359 -0.8323 -0.272   0.1058  0.0878]\n",
      "MSE loss: 156.3452\n",
      "Iteration: 59400\n",
      "Gradient: [ -0.7891 -12.3536  71.2838 -17.6946 -46.7393]\n",
      "Weights: [-4.3345 -0.83   -0.2725  0.1057  0.0879]\n",
      "MSE loss: 156.2234\n",
      "Iteration: 59500\n",
      "Gradient: [-4.3484 -0.6469 42.0676 46.5736 74.7189]\n",
      "Weights: [-4.3352 -0.8306 -0.2729  0.1057  0.088 ]\n",
      "MSE loss: 156.1536\n",
      "Iteration: 59600\n",
      "Gradient: [   8.3993   -5.286    46.3654  -81.825  -247.8641]\n",
      "Weights: [-4.3388 -0.8301 -0.2732  0.1056  0.088 ]\n",
      "MSE loss: 156.1094\n",
      "Iteration: 59700\n",
      "Gradient: [   0.7191  -10.9268    8.4591    3.5512 -217.0059]\n",
      "Weights: [-4.342  -0.8285 -0.2736  0.1056  0.0881]\n",
      "MSE loss: 156.0138\n",
      "Iteration: 59800\n",
      "Gradient: [  7.4801  -4.4403  31.1267  47.19   -47.3322]\n",
      "Weights: [-4.3372 -0.8261 -0.2744  0.1055  0.0881]\n",
      "MSE loss: 155.8533\n",
      "Iteration: 59900\n",
      "Gradient: [  11.1353   -3.4815   31.5618  132.0332 -389.5735]\n",
      "Weights: [-4.3327 -0.8261 -0.2755  0.1054  0.0882]\n",
      "MSE loss: 155.7693\n",
      "Iteration: 60000\n",
      "Gradient: [  4.2033   3.6318  -3.0783  42.6154 -83.2738]\n",
      "Weights: [-4.3413 -0.8244 -0.2757  0.1054  0.0882]\n",
      "MSE loss: 155.663\n",
      "Iteration: 60100\n",
      "Gradient: [  -4.0027    7.4562    3.6143 -110.1916  -69.2756]\n",
      "Weights: [-4.3414 -0.8222 -0.2762  0.1054  0.0883]\n",
      "MSE loss: 155.555\n",
      "Iteration: 60200\n",
      "Gradient: [  12.6406   -0.5212   -8.8526 -166.5692 -241.0347]\n",
      "Weights: [-4.3412 -0.8218 -0.2769  0.1052  0.0883]\n",
      "MSE loss: 155.4545\n",
      "Iteration: 60300\n",
      "Gradient: [  -5.2628   -7.2105  -44.7047 -156.8123  -38.0098]\n",
      "Weights: [-4.3357 -0.8212 -0.2777  0.1052  0.0884]\n",
      "MSE loss: 155.3634\n",
      "Iteration: 60400\n",
      "Gradient: [   2.8453    1.2391   16.4879   36.7064 -219.9473]\n",
      "Weights: [-4.3367 -0.8197 -0.2783  0.1051  0.0884]\n",
      "MSE loss: 155.2776\n",
      "Iteration: 60500\n",
      "Gradient: [ 12.9699  -8.8778  22.1411  55.5984 -89.0441]\n",
      "Weights: [-4.3362 -0.8176 -0.2789  0.1051  0.0884]\n",
      "MSE loss: 155.2077\n",
      "Iteration: 60600\n",
      "Gradient: [ -7.6576 -25.6195  33.0033 -10.3643 -54.868 ]\n",
      "Weights: [-4.3456 -0.8164 -0.2791  0.1051  0.0885]\n",
      "MSE loss: 155.0993\n",
      "Iteration: 60700\n",
      "Gradient: [ 7.1622  4.4936 41.5983 13.6302 80.5009]\n",
      "Weights: [-4.3483 -0.8135 -0.2797  0.105   0.0885]\n",
      "MSE loss: 154.9746\n",
      "Iteration: 60800\n",
      "Gradient: [  15.8312  -23.2287  -30.5689   -3.1051 -127.6449]\n",
      "Weights: [-4.3491 -0.8116 -0.2806  0.105   0.0886]\n",
      "MSE loss: 154.8359\n",
      "Iteration: 60900\n",
      "Gradient: [   5.9469   -3.5176   34.3447   54.6993 -242.9647]\n",
      "Weights: [-4.3446 -0.8118 -0.2812  0.105   0.0886]\n",
      "MSE loss: 154.7524\n",
      "Iteration: 61000\n",
      "Gradient: [ -9.1176  -1.5181  28.231  -45.7928 -42.6208]\n",
      "Weights: [-4.3391 -0.8133 -0.2821  0.1049  0.0887]\n",
      "MSE loss: 154.6781\n",
      "Iteration: 61100\n",
      "Gradient: [-2.700000e-03 -3.146000e+00 -2.136240e+01  1.143319e+02 -1.263714e+02]\n",
      "Weights: [-4.3369 -0.8117 -0.2825  0.1049  0.0888]\n",
      "MSE loss: 154.6448\n",
      "Iteration: 61200\n",
      "Gradient: [  -1.845    11.4187   -0.2591 -154.6159    7.6084]\n",
      "Weights: [-4.3448 -0.8106 -0.2829  0.1049  0.0888]\n",
      "MSE loss: 154.5062\n",
      "Iteration: 61300\n",
      "Gradient: [ -10.3338    3.9577   10.1615  107.1897 -230.7736]\n",
      "Weights: [-4.3434 -0.8088 -0.2836  0.1047  0.0888]\n",
      "MSE loss: 154.3974\n",
      "Iteration: 61400\n",
      "Gradient: [ -15.6238   -1.1503   17.3447   86.9828 -187.1943]\n",
      "Weights: [-4.3468 -0.8083 -0.2842  0.1047  0.0889]\n",
      "MSE loss: 154.3309\n",
      "Iteration: 61500\n",
      "Gradient: [   4.7112  -17.9046   27.3353 -116.3689 -153.7887]\n",
      "Weights: [-4.3421 -0.8057 -0.2846  0.1046  0.089 ]\n",
      "MSE loss: 154.2251\n",
      "Iteration: 61600\n",
      "Gradient: [-16.1702  -9.5078  45.3121  58.6354 -39.0057]\n",
      "Weights: [-4.3452 -0.8047 -0.2854  0.1045  0.089 ]\n",
      "MSE loss: 154.0812\n",
      "Iteration: 61700\n",
      "Gradient: [  18.5573   13.2383    6.1638  134.9037 -107.6441]\n",
      "Weights: [-4.3428 -0.8031 -0.2861  0.1045  0.089 ]\n",
      "MSE loss: 154.0087\n",
      "Iteration: 61800\n",
      "Gradient: [  4.3851  -3.3681  10.4613 185.0645 182.131 ]\n",
      "Weights: [-4.347  -0.8028 -0.2864  0.1045  0.0891]\n",
      "MSE loss: 153.9374\n",
      "Iteration: 61900\n",
      "Gradient: [  2.518    4.8931 -42.1551 -14.1411 191.5503]\n",
      "Weights: [-4.3463 -0.8004 -0.2873  0.1045  0.0891]\n",
      "MSE loss: 153.8111\n",
      "Iteration: 62000\n",
      "Gradient: [  10.1219   18.5312   29.5123   44.8461 -182.8946]\n",
      "Weights: [-4.3442 -0.7999 -0.2882  0.1044  0.0891]\n",
      "MSE loss: 153.7205\n",
      "Iteration: 62100\n",
      "Gradient: [ 9.36150e+00  4.95340e+00 -7.45770e+00  2.15300e-01 -2.58391e+02]\n",
      "Weights: [-4.3492 -0.798  -0.2886  0.1043  0.0892]\n",
      "MSE loss: 153.5937\n",
      "Iteration: 62200\n",
      "Gradient: [   7.3807   -9.3095    4.3117  -38.2551 -150.7443]\n",
      "Weights: [-4.3535 -0.7957 -0.289   0.1043  0.0892]\n",
      "MSE loss: 153.5088\n",
      "Iteration: 62300\n",
      "Gradient: [   6.902   -13.9244   10.7141  104.8592 -191.2244]\n",
      "Weights: [-4.351  -0.7937 -0.2897  0.1043  0.0893]\n",
      "MSE loss: 153.3857\n",
      "Iteration: 62400\n",
      "Gradient: [ 11.0497   1.1154 -30.284    1.4114 227.2605]\n",
      "Weights: [-4.3503 -0.7939 -0.2906  0.1042  0.0893]\n",
      "MSE loss: 153.2774\n",
      "Iteration: 62500\n",
      "Gradient: [  1.9624   1.1323 -13.3323  32.2859 408.777 ]\n",
      "Weights: [-4.3498 -0.7923 -0.2914  0.1041  0.0894]\n",
      "MSE loss: 153.1667\n",
      "Iteration: 62600\n",
      "Gradient: [  -3.8233   14.9344   -3.6933   15.3957 -274.514 ]\n",
      "Weights: [-4.3467 -0.7916 -0.2922  0.1041  0.0895]\n",
      "MSE loss: 153.0677\n",
      "Iteration: 62700\n",
      "Gradient: [   8.6035   -6.1207  -16.8572   67.0941 -409.8351]\n",
      "Weights: [-4.3476 -0.7912 -0.293   0.1041  0.0895]\n",
      "MSE loss: 152.9605\n",
      "Iteration: 62800\n",
      "Gradient: [-4.2842 15.2467 21.1106 96.4828 22.1337]\n",
      "Weights: [-4.35   -0.7895 -0.2934  0.104   0.0896]\n",
      "MSE loss: 152.8607\n",
      "Iteration: 62900\n",
      "Gradient: [  -1.5458   -5.9326   33.2044 -146.2575   72.9557]\n",
      "Weights: [-4.3477 -0.7886 -0.294   0.104   0.0896]\n",
      "MSE loss: 152.7747\n",
      "Iteration: 63000\n",
      "Gradient: [  21.5655    3.4319   18.6637  108.2114 -133.0312]\n",
      "Weights: [-4.3487 -0.7878 -0.2948  0.1039  0.0897]\n",
      "MSE loss: 152.6538\n",
      "Iteration: 63100\n",
      "Gradient: [  -3.7462   27.8681   44.4951   48.9812 -295.0157]\n",
      "Weights: [-4.3495 -0.7871 -0.2952  0.1039  0.0898]\n",
      "MSE loss: 152.5559\n",
      "Iteration: 63200\n",
      "Gradient: [  2.6949   4.9219  28.839   21.2642 -16.0411]\n",
      "Weights: [-4.3576 -0.785  -0.2955  0.1039  0.0898]\n",
      "MSE loss: 152.4725\n",
      "Iteration: 63300\n",
      "Gradient: [-14.157   -3.7323  38.6093 120.9998 -82.8617]\n",
      "Weights: [-4.3533 -0.7826 -0.2967  0.1038  0.0899]\n",
      "MSE loss: 152.2998\n",
      "Iteration: 63400\n",
      "Gradient: [ 10.0158  -8.846  -14.3236  57.6764 -48.9091]\n",
      "Weights: [-4.3527 -0.7806 -0.2977  0.1038  0.0899]\n",
      "MSE loss: 152.1866\n",
      "Iteration: 63500\n",
      "Gradient: [-12.7045   3.8563   3.3657 -48.2804 337.207 ]\n",
      "Weights: [-4.3563 -0.7797 -0.2984  0.1039  0.0899]\n",
      "MSE loss: 152.0829\n",
      "Iteration: 63600\n",
      "Gradient: [-9.911  15.4941 76.8315 78.1189 34.6926]\n",
      "Weights: [-4.3559 -0.779  -0.2989  0.1038  0.09  ]\n",
      "MSE loss: 152.0029\n",
      "Iteration: 63700\n",
      "Gradient: [ -7.2149  -4.7977   8.7204 -72.6948  72.1574]\n",
      "Weights: [-4.3572 -0.7785 -0.2995  0.1039  0.09  ]\n",
      "MSE loss: 151.9272\n",
      "Iteration: 63800\n",
      "Gradient: [-14.0503   8.6186  15.8624  86.5918 -64.0613]\n",
      "Weights: [-4.3628 -0.7768 -0.3     0.1037  0.0901]\n",
      "MSE loss: 151.8778\n",
      "Iteration: 63900\n",
      "Gradient: [ -3.0545  -8.5828   2.0005  51.758  105.4802]\n",
      "Weights: [-4.3558 -0.7755 -0.3003  0.1037  0.0902]\n",
      "MSE loss: 151.7606\n",
      "Iteration: 64000\n",
      "Gradient: [  9.5119   8.911   -8.3956 -66.2144 242.4464]\n",
      "Weights: [-4.3569 -0.7753 -0.3009  0.1037  0.0902]\n",
      "MSE loss: 151.6823\n",
      "Iteration: 64100\n",
      "Gradient: [  -1.9435  -16.0434   15.216    99.2943 -108.3386]\n",
      "Weights: [-4.3548 -0.775  -0.302   0.1036  0.0902]\n",
      "MSE loss: 151.554\n",
      "Iteration: 64200\n",
      "Gradient: [  3.4618  10.938  -16.1999   9.0444 129.6628]\n",
      "Weights: [-4.353  -0.775  -0.3026  0.1036  0.0903]\n",
      "MSE loss: 151.4692\n",
      "Iteration: 64300\n",
      "Gradient: [  17.234    -6.9221   -8.4066  -33.348  -141.5018]\n",
      "Weights: [-4.3566 -0.7719 -0.3033  0.1035  0.0903]\n",
      "MSE loss: 151.3333\n",
      "Iteration: 64400\n",
      "Gradient: [  3.1446 -25.8284 -17.3643  70.8394  83.6028]\n",
      "Weights: [-4.362  -0.7699 -0.3035  0.1035  0.0904]\n",
      "MSE loss: 151.2589\n",
      "Iteration: 64500\n",
      "Gradient: [  13.0243   -3.2786  -28.1686  -55.6086 -409.0457]\n",
      "Weights: [-4.3606 -0.769  -0.3042  0.1034  0.0904]\n",
      "MSE loss: 151.1685\n",
      "Iteration: 64600\n",
      "Gradient: [   3.6494    2.4915   -1.8003   61.8344 -171.6322]\n",
      "Weights: [-4.3609 -0.7682 -0.3046  0.1033  0.0905]\n",
      "MSE loss: 151.0687\n",
      "Iteration: 64700\n",
      "Gradient: [  -6.7245   13.226   -22.3652   13.2736 -229.6549]\n",
      "Weights: [-4.3593 -0.7667 -0.3052  0.1033  0.0905]\n",
      "MSE loss: 150.9781\n",
      "Iteration: 64800\n",
      "Gradient: [-6.000000e-04  4.498000e+00 -9.129000e-01  8.601010e+01  1.453286e+02]\n",
      "Weights: [-4.357  -0.7669 -0.3058  0.1033  0.0906]\n",
      "MSE loss: 150.9159\n",
      "Iteration: 64900\n",
      "Gradient: [ -2.8484  -3.2419  26.5279 120.5169 180.8272]\n",
      "Weights: [-4.3589 -0.765  -0.3063  0.1031  0.0906]\n",
      "MSE loss: 150.8195\n",
      "Iteration: 65000\n",
      "Gradient: [  5.0592  -3.2558 -20.5765   2.4828 109.5587]\n",
      "Weights: [-4.3554 -0.7656 -0.307   0.1031  0.0906]\n",
      "MSE loss: 150.7746\n",
      "Iteration: 65100\n",
      "Gradient: [ -0.708  -10.7962  38.1748  54.2059 -55.6232]\n",
      "Weights: [-4.3543 -0.7647 -0.308   0.1031  0.0907]\n",
      "MSE loss: 150.6516\n",
      "Iteration: 65200\n",
      "Gradient: [ -2.8021  15.0872  21.8069 -40.57    80.029 ]\n",
      "Weights: [-4.357  -0.7631 -0.3086  0.103   0.0908]\n",
      "MSE loss: 150.538\n",
      "Iteration: 65300\n",
      "Gradient: [ -4.7224  10.3894 -26.5858 112.1589 -43.4143]\n",
      "Weights: [-4.356  -0.762  -0.309   0.1028  0.0908]\n",
      "MSE loss: 150.4413\n",
      "Iteration: 65400\n",
      "Gradient: [  -4.9935  -24.6165   75.3832    2.947  -473.3154]\n",
      "Weights: [-4.3651 -0.7606 -0.3093  0.1028  0.0909]\n",
      "MSE loss: 150.4325\n",
      "Iteration: 65500\n",
      "Gradient: [   2.4821   -0.8589   -4.4559  -29.1022 -176.3136]\n",
      "Weights: [-4.36   -0.7588 -0.3095  0.1029  0.0909]\n",
      "MSE loss: 150.2908\n",
      "Iteration: 65600\n",
      "Gradient: [-9.15200e-01 -1.07600e-01  3.29000e-02  3.61756e+01 -8.25213e+01]\n",
      "Weights: [-4.3616 -0.7575 -0.3099  0.1028  0.091 ]\n",
      "MSE loss: 150.2156\n",
      "Iteration: 65700\n",
      "Gradient: [ -4.2867  -3.0755  -4.0865  17.9506 221.2131]\n",
      "Weights: [-4.3582 -0.7574 -0.3109  0.1027  0.091 ]\n",
      "MSE loss: 150.1169\n",
      "Iteration: 65800\n",
      "Gradient: [   3.8392    9.0657  -26.9347   46.3023 -164.3579]\n",
      "Weights: [-4.3617 -0.7555 -0.3115  0.1028  0.0911]\n",
      "MSE loss: 150.0048\n",
      "Iteration: 65900\n",
      "Gradient: [   6.5008   -6.2668   -9.4629   25.901  -359.2892]\n",
      "Weights: [-4.3654 -0.7528 -0.3122  0.1027  0.0911]\n",
      "MSE loss: 149.8881\n",
      "Iteration: 66000\n",
      "Gradient: [  4.3425   8.0515 -20.822  -86.3959  28.5252]\n",
      "Weights: [-4.3652 -0.7515 -0.3128  0.1028  0.0911]\n",
      "MSE loss: 149.805\n",
      "Iteration: 66100\n",
      "Gradient: [  4.6068   0.8061  23.6469  84.151  -95.5286]\n",
      "Weights: [-4.3649 -0.7514 -0.3133  0.1028  0.0912]\n",
      "MSE loss: 149.7234\n",
      "Iteration: 66200\n",
      "Gradient: [   5.604    -5.2443    3.9021  -31.1138 -126.935 ]\n",
      "Weights: [-4.3669 -0.751  -0.3138  0.1027  0.0912]\n",
      "MSE loss: 149.6584\n",
      "Iteration: 66300\n",
      "Gradient: [  -0.9162   -7.7089   41.6585  109.0227 -498.3077]\n",
      "Weights: [-4.3666 -0.7504 -0.3145  0.1025  0.0913]\n",
      "MSE loss: 149.5602\n",
      "Iteration: 66400\n",
      "Gradient: [ -7.2762   9.6936   0.3267 -12.2612 116.652 ]\n",
      "Weights: [-4.3619 -0.7501 -0.315   0.1025  0.0914]\n",
      "MSE loss: 149.473\n",
      "Iteration: 66500\n",
      "Gradient: [  -7.2505   11.1254   16.8062   13.9179 -119.2193]\n",
      "Weights: [-4.364  -0.7482 -0.3158  0.1025  0.0914]\n",
      "MSE loss: 149.3556\n",
      "Iteration: 66600\n",
      "Gradient: [  3.6924  -5.8823   9.4347 -27.1515 -42.3263]\n",
      "Weights: [-4.3645 -0.7475 -0.3167  0.1024  0.0915]\n",
      "MSE loss: 149.2271\n",
      "Iteration: 66700\n",
      "Gradient: [   0.9228  -21.9336  -58.7039  -44.3499 -367.6671]\n",
      "Weights: [-4.3663 -0.7441 -0.3174  0.1024  0.0915]\n",
      "MSE loss: 149.0856\n",
      "Iteration: 66800\n",
      "Gradient: [  -2.1932    2.7086   -3.6469 -138.2003   56.511 ]\n",
      "Weights: [-4.3674 -0.743  -0.3181  0.1023  0.0916]\n",
      "MSE loss: 148.9912\n",
      "Iteration: 66900\n",
      "Gradient: [  8.7051   5.9658  42.0907  20.1974 -99.904 ]\n",
      "Weights: [-4.3647 -0.7419 -0.3189  0.1023  0.0916]\n",
      "MSE loss: 148.8889\n",
      "Iteration: 67000\n",
      "Gradient: [ -10.5586    6.62    -25.3265   99.2768 -319.3428]\n",
      "Weights: [-4.3661 -0.7428 -0.3197  0.1023  0.0917]\n",
      "MSE loss: 148.8045\n",
      "Iteration: 67100\n",
      "Gradient: [ -0.2282   7.0875  32.8442 -61.4019 -75.5238]\n",
      "Weights: [-4.3627 -0.7427 -0.3205  0.1022  0.0918]\n",
      "MSE loss: 148.7082\n",
      "Iteration: 67200\n",
      "Gradient: [  -9.498     1.7266   15.6677  -28.4829 -350.9857]\n",
      "Weights: [-4.369  -0.7402 -0.3209  0.1022  0.0918]\n",
      "MSE loss: 148.6006\n",
      "Iteration: 67300\n",
      "Gradient: [ -5.9257 -10.0525 -25.5506  -0.2212 -98.4216]\n",
      "Weights: [-4.3702 -0.7387 -0.3214  0.102   0.0919]\n",
      "MSE loss: 148.504\n",
      "Iteration: 67400\n",
      "Gradient: [  -7.1525  -12.6541    9.8394 -101.7962  272.8741]\n",
      "Weights: [-4.3708 -0.7365 -0.3221  0.102   0.0919]\n",
      "MSE loss: 148.3769\n",
      "Iteration: 67500\n",
      "Gradient: [  -4.9185  -10.7729  -11.3104   -0.6172 -245.002 ]\n",
      "Weights: [-4.3738 -0.7353 -0.3225  0.102   0.092 ]\n",
      "MSE loss: 148.3268\n",
      "Iteration: 67600\n",
      "Gradient: [  6.0782 -14.9877  23.5917   0.5154 -26.4724]\n",
      "Weights: [-4.3717 -0.7342 -0.323   0.102   0.092 ]\n",
      "MSE loss: 148.2163\n",
      "Iteration: 67700\n",
      "Gradient: [  -3.1223   -2.5117   15.1162  101.2729 -281.9152]\n",
      "Weights: [-4.3732 -0.7323 -0.3238  0.1019  0.0921]\n",
      "MSE loss: 148.1122\n",
      "Iteration: 67800\n",
      "Gradient: [ -0.2934 -11.8021  79.9621  54.735  193.4297]\n",
      "Weights: [-4.3715 -0.7311 -0.3243  0.1019  0.0921]\n",
      "MSE loss: 148.0245\n",
      "Iteration: 67900\n",
      "Gradient: [ -0.7573  12.8889   7.0355  22.3661 -20.637 ]\n",
      "Weights: [-4.3709 -0.7306 -0.3248  0.1019  0.0922]\n",
      "MSE loss: 147.9657\n",
      "Iteration: 68000\n",
      "Gradient: [   5.3128  -30.6482   47.4905  -10.299  -234.6984]\n",
      "Weights: [-4.369  -0.7309 -0.3256  0.1018  0.0922]\n",
      "MSE loss: 147.8691\n",
      "Iteration: 68100\n",
      "Gradient: [-3.713000e+00  3.529000e-01 -2.550810e+01  2.115590e+01 -4.673497e+02]\n",
      "Weights: [-4.3721 -0.7302 -0.3262  0.1018  0.0923]\n",
      "MSE loss: 147.7925\n",
      "Iteration: 68200\n",
      "Gradient: [   6.6611    3.9351    8.1931  -46.8356 -371.1553]\n",
      "Weights: [-4.3731 -0.7285 -0.3269  0.1017  0.0923]\n",
      "MSE loss: 147.7101\n",
      "Iteration: 68300\n",
      "Gradient: [  2.4242   5.418   36.7197  51.0151 -89.877 ]\n",
      "Weights: [-4.3676 -0.7297 -0.3274  0.1017  0.0924]\n",
      "MSE loss: 147.6201\n",
      "Iteration: 68400\n",
      "Gradient: [-13.6741 -22.1366 -14.7125  21.2918   4.0765]\n",
      "Weights: [-4.3703 -0.7273 -0.3275  0.1017  0.0924]\n",
      "MSE loss: 147.5509\n",
      "Iteration: 68500\n",
      "Gradient: [ -13.0452   13.0292   25.7265  -15.366  -223.7576]\n",
      "Weights: [-4.3759 -0.7259 -0.328   0.1017  0.0925]\n",
      "MSE loss: 147.484\n",
      "Iteration: 68600\n",
      "Gradient: [   9.3714   12.9552   -2.0107   -1.4575 -189.1657]\n",
      "Weights: [-4.3683 -0.7268 -0.3291  0.1016  0.0925]\n",
      "MSE loss: 147.3669\n",
      "Iteration: 68700\n",
      "Gradient: [  -0.6733   15.4631   -3.7113   66.4861 -121.6211]\n",
      "Weights: [-4.3677 -0.7261 -0.3293  0.1015  0.0926]\n",
      "MSE loss: 147.2992\n",
      "Iteration: 68800\n",
      "Gradient: [  5.0597 -11.0763 -34.7752 -42.2945 -52.1006]\n",
      "Weights: [-4.369  -0.7253 -0.3299  0.1014  0.0926]\n",
      "MSE loss: 147.2201\n",
      "Iteration: 68900\n",
      "Gradient: [  -6.2239   -3.9561   30.6695  -49.8216 -123.3957]\n",
      "Weights: [-4.3734 -0.7242 -0.3308  0.1014  0.0927]\n",
      "MSE loss: 147.1278\n",
      "Iteration: 69000\n",
      "Gradient: [   1.6038   -4.1933   -6.8051  -29.0125 -229.9638]\n",
      "Weights: [-4.3755 -0.722  -0.331   0.1015  0.0927]\n",
      "MSE loss: 147.0144\n",
      "Iteration: 69100\n",
      "Gradient: [  -5.4463   -6.0312    9.2078   40.067  -210.9383]\n",
      "Weights: [-4.373  -0.72   -0.3314  0.1015  0.0928]\n",
      "MSE loss: 146.9463\n",
      "Iteration: 69200\n",
      "Gradient: [   3.7136   -2.8537   42.1193   71.8958 -298.3407]\n",
      "Weights: [-4.3778 -0.7199 -0.3318  0.1014  0.0928]\n",
      "MSE loss: 146.9106\n",
      "Iteration: 69300\n",
      "Gradient: [   5.3875  -12.59      1.774    37.3614 -302.4185]\n",
      "Weights: [-4.3777 -0.7172 -0.3322  0.1013  0.0929]\n",
      "MSE loss: 146.7978\n",
      "Iteration: 69400\n",
      "Gradient: [  1.8636  14.0807   4.2612  41.454  245.7707]\n",
      "Weights: [-4.3742 -0.7162 -0.3329  0.1012  0.0929]\n",
      "MSE loss: 146.7117\n",
      "Iteration: 69500\n",
      "Gradient: [-13.8519  -9.9648  12.5012 -45.6395 -30.6916]\n",
      "Weights: [-4.3771 -0.7153 -0.3339  0.1012  0.0929]\n",
      "MSE loss: 146.5943\n",
      "Iteration: 69600\n",
      "Gradient: [   4.629   -13.5077   68.0537  146.6543 -299.5329]\n",
      "Weights: [-4.3704 -0.7145 -0.3347  0.1011  0.093 ]\n",
      "MSE loss: 146.5021\n",
      "Iteration: 69700\n",
      "Gradient: [  3.0941   4.2628  28.4943  24.3593 -90.2978]\n",
      "Weights: [-4.3711 -0.7138 -0.335   0.1009  0.093 ]\n",
      "MSE loss: 146.4286\n",
      "Iteration: 69800\n",
      "Gradient: [ 12.8917  -4.3274 -59.3041 -83.4993  98.7532]\n",
      "Weights: [-4.3723 -0.7133 -0.3357  0.1009  0.0931]\n",
      "MSE loss: 146.3351\n",
      "Iteration: 69900\n",
      "Gradient: [-14.5473  16.0442   3.6483   2.6736  97.1392]\n",
      "Weights: [-4.3756 -0.7113 -0.3362  0.1008  0.0931]\n",
      "MSE loss: 146.254\n",
      "Iteration: 70000\n",
      "Gradient: [  5.0407  -5.4039  44.8013 -39.8341 -86.6548]\n",
      "Weights: [-4.3772 -0.7093 -0.3367  0.1007  0.0931]\n",
      "MSE loss: 146.1538\n",
      "Iteration: 70100\n",
      "Gradient: [  9.0349   3.5543  -0.2077 -74.8941   0.2341]\n",
      "Weights: [-4.3724 -0.7083 -0.3372  0.1007  0.0931]\n",
      "MSE loss: 146.1212\n",
      "Iteration: 70200\n",
      "Gradient: [-4.5841  2.4593 -4.3047 25.0124  3.0092]\n",
      "Weights: [-4.3828 -0.7069 -0.3375  0.1007  0.0932]\n",
      "MSE loss: 146.0372\n",
      "Iteration: 70300\n",
      "Gradient: [ -4.3468   8.9494  34.6787 121.2985 231.0225]\n",
      "Weights: [-4.3792 -0.7056 -0.3382  0.1007  0.0933]\n",
      "MSE loss: 145.8878\n",
      "Iteration: 70400\n",
      "Gradient: [  7.5414 -16.9542  26.9456  90.1947 -30.4941]\n",
      "Weights: [-4.3794 -0.704  -0.3387  0.1006  0.0934]\n",
      "MSE loss: 145.7955\n",
      "Iteration: 70500\n",
      "Gradient: [ 6.2627 -0.8021 28.4248 13.7475  0.6134]\n",
      "Weights: [-4.3764 -0.7042 -0.3394  0.1005  0.0934]\n",
      "MSE loss: 145.7091\n",
      "Iteration: 70600\n",
      "Gradient: [  -2.5235    8.4851   23.7301   52.246  -564.5449]\n",
      "Weights: [-4.3787 -0.7037 -0.3398  0.1005  0.0935]\n",
      "MSE loss: 145.6403\n",
      "Iteration: 70700\n",
      "Gradient: [  2.5691  -9.7809 -22.6529 -49.369   30.521 ]\n",
      "Weights: [-4.3747 -0.7027 -0.3404  0.1004  0.0935]\n",
      "MSE loss: 145.5786\n",
      "Iteration: 70800\n",
      "Gradient: [-11.5916  26.4805 -17.9794  57.6045 -46.7918]\n",
      "Weights: [-4.3768 -0.7031 -0.3406  0.1003  0.0935]\n",
      "MSE loss: 145.5177\n",
      "Iteration: 70900\n",
      "Gradient: [   5.9784  -22.7735  -15.1886   68.3632 -105.9685]\n",
      "Weights: [-4.3765 -0.7029 -0.3411  0.1002  0.0936]\n",
      "MSE loss: 145.4477\n",
      "Iteration: 71000\n",
      "Gradient: [ -5.2574  16.7708 -30.0981 -13.3631  25.9861]\n",
      "Weights: [-4.3751 -0.701  -0.3418  0.1002  0.0936]\n",
      "MSE loss: 145.3567\n",
      "Iteration: 71100\n",
      "Gradient: [  -7.9888    6.9447  -32.9405  -47.503  -208.564 ]\n",
      "Weights: [-4.3819 -0.6997 -0.3423  0.1002  0.0937]\n",
      "MSE loss: 145.2821\n",
      "Iteration: 71200\n",
      "Gradient: [ -9.581  -12.2613  59.9809 -64.9772 -40.0692]\n",
      "Weights: [-4.3829 -0.6971 -0.3427  0.1002  0.0937]\n",
      "MSE loss: 145.1787\n",
      "Iteration: 71300\n",
      "Gradient: [ -5.162   -9.2562  22.4294   8.795  -86.5437]\n",
      "Weights: [-4.3796 -0.6977 -0.3431  0.1002  0.0938]\n",
      "MSE loss: 145.1307\n",
      "Iteration: 71400\n",
      "Gradient: [   5.0511   -3.3376    5.4905   88.8413 -129.057 ]\n",
      "Weights: [-4.3783 -0.6973 -0.3438  0.1001  0.0938]\n",
      "MSE loss: 145.0373\n",
      "Iteration: 71500\n",
      "Gradient: [ -3.1448  -6.9779 -21.6194  50.54    76.6949]\n",
      "Weights: [-4.3797 -0.6956 -0.3446  0.1001  0.0939]\n",
      "MSE loss: 144.9341\n",
      "Iteration: 71600\n",
      "Gradient: [ -4.6181 -11.602   -4.1408 -28.455   75.7407]\n",
      "Weights: [-4.3778 -0.694  -0.345   0.1001  0.0939]\n",
      "MSE loss: 144.8937\n",
      "Iteration: 71700\n",
      "Gradient: [  -3.6245    3.8993   16.0213   76.7664 -279.3415]\n",
      "Weights: [-4.3894 -0.6926 -0.3452  0.1     0.0939]\n",
      "MSE loss: 144.8801\n",
      "Iteration: 71800\n",
      "Gradient: [  6.1941   0.4015   5.5644  98.0141 192.6007]\n",
      "Weights: [-4.3888 -0.6898 -0.346   0.1     0.094 ]\n",
      "MSE loss: 144.696\n",
      "Iteration: 71900\n",
      "Gradient: [   0.1885   -5.2011  -15.4127   32.2704 -102.3734]\n",
      "Weights: [-4.3846 -0.6895 -0.3466  0.0998  0.094 ]\n",
      "MSE loss: 144.5968\n",
      "Iteration: 72000\n",
      "Gradient: [   2.1223  -13.3022  -14.9577   61.5423 -241.9103]\n",
      "Weights: [-4.3825 -0.6889 -0.347   0.0997  0.0941]\n",
      "MSE loss: 144.5186\n",
      "Iteration: 72100\n",
      "Gradient: [   2.9689   -4.0266   26.7146  -14.6502 -103.7352]\n",
      "Weights: [-4.381  -0.6884 -0.3481  0.0996  0.0941]\n",
      "MSE loss: 144.4\n",
      "Iteration: 72200\n",
      "Gradient: [  13.3507   -8.4226    5.0773  -56.5281 -273.9117]\n",
      "Weights: [-4.3792 -0.6874 -0.3486  0.0998  0.0942]\n",
      "MSE loss: 144.3507\n",
      "Iteration: 72300\n",
      "Gradient: [-4.0175  4.7531 42.1254  7.5091 84.2869]\n",
      "Weights: [-4.3854 -0.6876 -0.3492  0.0997  0.0943]\n",
      "MSE loss: 144.2536\n",
      "Iteration: 72400\n",
      "Gradient: [   4.7638   -2.1847   -1.1608   -7.1666 -379.905 ]\n",
      "Weights: [-4.3788 -0.6872 -0.3499  0.0997  0.0943]\n",
      "MSE loss: 144.1779\n",
      "Iteration: 72500\n",
      "Gradient: [  7.7001  -1.344  -11.1006 -52.3182  98.7567]\n",
      "Weights: [-4.3834 -0.686  -0.3502  0.0998  0.0943]\n",
      "MSE loss: 144.1023\n",
      "Iteration: 72600\n",
      "Gradient: [ -5.6185 -16.1061 -11.8304 -35.4842   2.1435]\n",
      "Weights: [-4.3879 -0.6832 -0.3507  0.0998  0.0944]\n",
      "MSE loss: 144.0213\n",
      "Iteration: 72700\n",
      "Gradient: [ -10.4578   -5.0521  -67.9851  -30.837  -233.2601]\n",
      "Weights: [-4.3903 -0.6817 -0.3515  0.0997  0.0944]\n",
      "MSE loss: 143.9329\n",
      "Iteration: 72800\n",
      "Gradient: [ 10.4312   3.084   40.8716 119.5076 -42.2658]\n",
      "Weights: [-4.3916 -0.6798 -0.3522  0.0998  0.0945]\n",
      "MSE loss: 143.824\n",
      "Iteration: 72900\n",
      "Gradient: [   2.9491  -16.6122   14.3235   -5.7989 -355.3965]\n",
      "Weights: [-4.3898 -0.6794 -0.3528  0.0997  0.0945]\n",
      "MSE loss: 143.7232\n",
      "Iteration: 73000\n",
      "Gradient: [  -3.1247    8.8484   12.4007   75.6076 -324.6937]\n",
      "Weights: [-4.3936 -0.678  -0.3535  0.0998  0.0946]\n",
      "MSE loss: 143.6807\n",
      "Iteration: 73100\n",
      "Gradient: [  -8.4239  -14.3629   18.1701  -40.7578 -181.478 ]\n",
      "Weights: [-4.3914 -0.6763 -0.354   0.0996  0.0946]\n",
      "MSE loss: 143.5525\n",
      "Iteration: 73200\n",
      "Gradient: [  8.0847  -2.2692  36.3037 -48.0179  90.8439]\n",
      "Weights: [-4.3842 -0.6766 -0.3549  0.0994  0.0946]\n",
      "MSE loss: 143.4533\n",
      "Iteration: 73300\n",
      "Gradient: [  0.3541   2.138  -11.3893  84.6459 -65.3679]\n",
      "Weights: [-4.3851 -0.6758 -0.355   0.0994  0.0947]\n",
      "MSE loss: 143.4043\n",
      "Iteration: 73400\n",
      "Gradient: [-11.785   -7.1721  24.5175 -35.9341  60.4324]\n",
      "Weights: [-4.3888 -0.6756 -0.3554  0.0994  0.0947]\n",
      "MSE loss: 143.344\n",
      "Iteration: 73500\n",
      "Gradient: [  -3.2781    2.5775    9.2554 -104.9366   -6.1156]\n",
      "Weights: [-4.3858 -0.6729 -0.3559  0.0992  0.0948]\n",
      "MSE loss: 143.2581\n",
      "Iteration: 73600\n",
      "Gradient: [  3.8225 -20.5171  -5.5201  37.4271 -56.602 ]\n",
      "Weights: [-4.3969 -0.671  -0.3562  0.0991  0.0948]\n",
      "MSE loss: 143.2232\n",
      "Iteration: 73700\n",
      "Gradient: [-5.337000e-01  1.660000e-02 -2.273380e+01  1.254028e+02  2.201380e+02]\n",
      "Weights: [-4.3888 -0.6691 -0.3566  0.099   0.0949]\n",
      "MSE loss: 143.1201\n",
      "Iteration: 73800\n",
      "Gradient: [ 11.3902   9.8356 -28.2966 -52.3689 165.8131]\n",
      "Weights: [-4.3947 -0.6684 -0.3573  0.0989  0.0949]\n",
      "MSE loss: 142.998\n",
      "Iteration: 73900\n",
      "Gradient: [   3.9271    1.7495   28.5612   55.5704 -108.7686]\n",
      "Weights: [-4.3931 -0.6666 -0.3581  0.0989  0.095 ]\n",
      "MSE loss: 142.8868\n",
      "Iteration: 74000\n",
      "Gradient: [ -1.176   -3.5299   3.0159 -47.7456 220.3397]\n",
      "Weights: [-4.3914 -0.6649 -0.359   0.0989  0.095 ]\n",
      "MSE loss: 142.7702\n",
      "Iteration: 74100\n",
      "Gradient: [  0.2206   1.3234 -11.3863 132.3035  -8.4423]\n",
      "Weights: [-4.3952 -0.664  -0.3598  0.0987  0.0951]\n",
      "MSE loss: 142.6781\n",
      "Iteration: 74200\n",
      "Gradient: [ 10.5453 -13.0494  -7.31   -76.3628 211.9377]\n",
      "Weights: [-4.3961 -0.6624 -0.3599  0.0987  0.0951]\n",
      "MSE loss: 142.5865\n",
      "Iteration: 74300\n",
      "Gradient: [  4.4999 -17.7293  59.2299  15.1228 340.0349]\n",
      "Weights: [-4.3994 -0.6601 -0.3607  0.0986  0.0952]\n",
      "MSE loss: 142.5085\n",
      "Iteration: 74400\n",
      "Gradient: [  -3.3094   -0.7201   14.9689  -80.2524 -205.7955]\n",
      "Weights: [-4.3972 -0.6587 -0.3614  0.0986  0.0952]\n",
      "MSE loss: 142.3863\n",
      "Iteration: 74500\n",
      "Gradient: [ -4.9226  -5.1065 -24.9251 -89.0877 139.6591]\n",
      "Weights: [-4.396  -0.6574 -0.3622  0.0986  0.0953]\n",
      "MSE loss: 142.2919\n",
      "Iteration: 74600\n",
      "Gradient: [ 6.7548 -7.1933 44.4055 12.562   1.3248]\n",
      "Weights: [-4.3989 -0.6558 -0.3631  0.0985  0.0953]\n",
      "MSE loss: 142.1971\n",
      "Iteration: 74700\n",
      "Gradient: [  5.7606  12.0581  16.9117 -28.5037 -94.6344]\n",
      "Weights: [-4.3979 -0.6541 -0.3638  0.0983  0.0953]\n",
      "MSE loss: 142.0882\n",
      "Iteration: 74800\n",
      "Gradient: [  4.303   -6.9879  21.0553 -52.1501 -30.8077]\n",
      "Weights: [-4.3948 -0.6529 -0.3646  0.0983  0.0954]\n",
      "MSE loss: 141.9911\n",
      "Iteration: 74900\n",
      "Gradient: [   2.39     -2.1088  -16.6065 -123.4952  100.7453]\n",
      "Weights: [-4.3953 -0.6537 -0.365   0.0983  0.0954]\n",
      "MSE loss: 141.9316\n",
      "Iteration: 75000\n",
      "Gradient: [ -3.8161  -0.2339  15.8741 162.7557 -22.2248]\n",
      "Weights: [-4.3916 -0.6532 -0.3654  0.0983  0.0955]\n",
      "MSE loss: 141.9068\n",
      "Iteration: 75100\n",
      "Gradient: [ -2.6585 -15.4147  20.1843 -75.6041 -42.769 ]\n",
      "Weights: [-4.3938 -0.6515 -0.3662  0.0983  0.0955]\n",
      "MSE loss: 141.7738\n",
      "Iteration: 75200\n",
      "Gradient: [ 11.1703  16.4312  -5.9336 -32.4005 -55.5595]\n",
      "Weights: [-4.3949 -0.6504 -0.3669  0.0982  0.0956]\n",
      "MSE loss: 141.6732\n",
      "Iteration: 75300\n",
      "Gradient: [  8.1572  -6.2409  45.7628 124.4227  12.4333]\n",
      "Weights: [-4.3957 -0.6503 -0.3672  0.0982  0.0956]\n",
      "MSE loss: 141.6011\n",
      "Iteration: 75400\n",
      "Gradient: [ -4.3993 -29.7517   5.2369 -72.3233   3.3349]\n",
      "Weights: [-4.3981 -0.6479 -0.3677  0.0982  0.0957]\n",
      "MSE loss: 141.4928\n",
      "Iteration: 75500\n",
      "Gradient: [   1.9396    2.9962  -44.1985  -35.2449 -255.9379]\n",
      "Weights: [-4.396  -0.6481 -0.3687  0.0981  0.0958]\n",
      "MSE loss: 141.3941\n",
      "Iteration: 75600\n",
      "Gradient: [-3.824000e+00 -1.092030e+01 -2.358000e-01 -1.249408e+02 -2.540043e+02]\n",
      "Weights: [-4.3954 -0.6486 -0.3693  0.098   0.0958]\n",
      "MSE loss: 141.3422\n",
      "Iteration: 75700\n",
      "Gradient: [  -1.527   -13.4383  -21.7255   86.5923 -224.4844]\n",
      "Weights: [-4.3962 -0.6464 -0.3696  0.098   0.0958]\n",
      "MSE loss: 141.257\n",
      "Iteration: 75800\n",
      "Gradient: [ -5.8462  11.7128  35.2443 -46.7722  26.1438]\n",
      "Weights: [-4.3924 -0.6457 -0.3698  0.0978  0.0959]\n",
      "MSE loss: 141.2409\n",
      "Iteration: 75900\n",
      "Gradient: [ -2.0363   6.3613 -14.6576  -9.5239 -89.1754]\n",
      "Weights: [-4.3964 -0.6445 -0.3707  0.0977  0.0959]\n",
      "MSE loss: 141.0916\n",
      "Iteration: 76000\n",
      "Gradient: [  0.352   -0.7869  12.4188  -9.3371 185.5678]\n",
      "Weights: [-4.3934 -0.644  -0.3714  0.0977  0.096 ]\n",
      "MSE loss: 141.0293\n",
      "Iteration: 76100\n",
      "Gradient: [  -1.0998   11.1786    9.0203   12.8269 -396.2581]\n",
      "Weights: [-4.3974 -0.6417 -0.372   0.0977  0.0961]\n",
      "MSE loss: 140.9008\n",
      "Iteration: 76200\n",
      "Gradient: [-7.680000e-02  1.332690e+01  5.351300e+00  1.131375e+02 -1.120890e+02]\n",
      "Weights: [-4.3974 -0.6399 -0.3722  0.0976  0.0961]\n",
      "MSE loss: 140.839\n",
      "Iteration: 76300\n",
      "Gradient: [ 14.547   -1.8883  37.4459 123.4395  12.3323]\n",
      "Weights: [-4.3972 -0.6401 -0.3726  0.0976  0.0961]\n",
      "MSE loss: 140.7968\n",
      "Iteration: 76400\n",
      "Gradient: [-1.4177 -8.041  -6.8905 86.7285 95.9318]\n",
      "Weights: [-4.3967 -0.6399 -0.3727  0.0976  0.0962]\n",
      "MSE loss: 140.7713\n",
      "Iteration: 76500\n",
      "Gradient: [  4.1421  16.6196  -4.0584 -15.2092 -57.5334]\n",
      "Weights: [-4.398  -0.6394 -0.3733  0.0975  0.0962]\n",
      "MSE loss: 140.6717\n",
      "Iteration: 76600\n",
      "Gradient: [  8.4462 -14.3518 -29.4446  23.4085  12.9414]\n",
      "Weights: [-4.4005 -0.638  -0.3739  0.0974  0.0963]\n",
      "MSE loss: 140.5666\n",
      "Iteration: 76700\n",
      "Gradient: [  8.818   -8.6603   0.5005  44.2503 -23.4095]\n",
      "Weights: [-4.4035 -0.6357 -0.3743  0.0974  0.0964]\n",
      "MSE loss: 140.4824\n",
      "Iteration: 76800\n",
      "Gradient: [ -6.9655 -10.9971 -25.5209  71.1629 179.3903]\n",
      "Weights: [-4.4046 -0.6345 -0.375   0.0973  0.0964]\n",
      "MSE loss: 140.4022\n",
      "Iteration: 76900\n",
      "Gradient: [ 2.439000e-01  7.244100e+00  2.180370e+01 -2.462920e+01 -3.083606e+02]\n",
      "Weights: [-4.4011 -0.6328 -0.3754  0.0973  0.0964]\n",
      "MSE loss: 140.3451\n",
      "Iteration: 77000\n",
      "Gradient: [   8.3567  -25.0322   13.7189   -6.3209 -281.0259]\n",
      "Weights: [-4.4035 -0.6322 -0.3762  0.0972  0.0965]\n",
      "MSE loss: 140.2371\n",
      "Iteration: 77100\n",
      "Gradient: [   6.8028   13.3729   33.1497   -3.5033 -217.1127]\n",
      "Weights: [-4.4    -0.632  -0.3769  0.0971  0.0965]\n",
      "MSE loss: 140.1485\n",
      "Iteration: 77200\n",
      "Gradient: [ -1.9258  13.3854 -19.7519  26.8059  51.384 ]\n",
      "Weights: [-4.3989 -0.6333 -0.3774  0.0971  0.0966]\n",
      "MSE loss: 140.1015\n",
      "Iteration: 77300\n",
      "Gradient: [ 1.7523 -4.1606 17.3536 -2.7385 24.2104]\n",
      "Weights: [-4.4036 -0.6326 -0.3777  0.0971  0.0967]\n",
      "MSE loss: 140.0444\n",
      "Iteration: 77400\n",
      "Gradient: [  5.4558   7.2201 -55.8602  10.0728  48.9237]\n",
      "Weights: [-4.402  -0.6302 -0.3782  0.0971  0.0967]\n",
      "MSE loss: 139.9303\n",
      "Iteration: 77500\n",
      "Gradient: [ -3.675   20.7566   1.2135 106.0976 -12.2884]\n",
      "Weights: [-4.4029 -0.6291 -0.3786  0.097   0.0967]\n",
      "MSE loss: 139.859\n",
      "Iteration: 77600\n",
      "Gradient: [-6.86000e-02  5.60730e+00 -3.85480e+00  2.10874e+01  1.67294e+02]\n",
      "Weights: [-4.397  -0.6285 -0.3793  0.0969  0.0968]\n",
      "MSE loss: 139.8262\n",
      "Iteration: 77700\n",
      "Gradient: [   1.0043   21.4557   39.1577   84.1397 -633.3399]\n",
      "Weights: [-4.3986 -0.628  -0.3803  0.0969  0.0968]\n",
      "MSE loss: 139.7061\n",
      "Iteration: 77800\n",
      "Gradient: [-12.1115 -19.6208  12.9914 -81.2358 147.8917]\n",
      "Weights: [-4.4057 -0.6256 -0.3811  0.097   0.0969]\n",
      "MSE loss: 139.5914\n",
      "Iteration: 77900\n",
      "Gradient: [   8.2766   -1.1507   13.805   180.3443 -377.6643]\n",
      "Weights: [-4.4047 -0.625  -0.3817  0.097   0.0969]\n",
      "MSE loss: 139.5083\n",
      "Iteration: 78000\n",
      "Gradient: [  7.187    2.5932  53.5775 -30.4983  76.467 ]\n",
      "Weights: [-4.4022 -0.6242 -0.3826  0.0969  0.097 ]\n",
      "MSE loss: 139.401\n",
      "Iteration: 78100\n",
      "Gradient: [  9.4662   5.6224  46.2573 -49.4173 103.3971]\n",
      "Weights: [-4.3988 -0.6239 -0.383   0.0969  0.097 ]\n",
      "MSE loss: 139.3575\n",
      "Iteration: 78200\n",
      "Gradient: [ -4.4991   2.1961 -15.6843  97.7537 199.4266]\n",
      "Weights: [-4.4094 -0.6213 -0.3834  0.097   0.0971]\n",
      "MSE loss: 139.2514\n",
      "Iteration: 78300\n",
      "Gradient: [ -0.4128 -23.9849  62.8679  -1.0739 109.6798]\n",
      "Weights: [-4.408  -0.6198 -0.3835  0.0969  0.0971]\n",
      "MSE loss: 139.1871\n",
      "Iteration: 78400\n",
      "Gradient: [ -2.364  -19.0966 -19.8641 -25.6519 -89.1346]\n",
      "Weights: [-4.4079 -0.6193 -0.3844  0.0968  0.0972]\n",
      "MSE loss: 139.0737\n",
      "Iteration: 78500\n",
      "Gradient: [ -6.5574  -2.6435 -12.3818 -65.3354 210.0731]\n",
      "Weights: [-4.4091 -0.6166 -0.3849  0.0968  0.0972]\n",
      "MSE loss: 138.9808\n",
      "Iteration: 78600\n",
      "Gradient: [  -2.2087   -3.5776  -20.031    47.0278 -259.3775]\n",
      "Weights: [-4.4109 -0.6144 -0.3855  0.0968  0.0973]\n",
      "MSE loss: 138.9122\n",
      "Iteration: 78700\n",
      "Gradient: [   4.5039  -12.9827  -10.8305  106.027  -173.9047]\n",
      "Weights: [-4.4103 -0.6137 -0.3862  0.0967  0.0973]\n",
      "MSE loss: 138.8111\n",
      "Iteration: 78800\n",
      "Gradient: [-10.0276   3.5351  14.8787  39.4669 278.3996]\n",
      "Weights: [-4.4083 -0.613  -0.3868  0.0965  0.0974]\n",
      "MSE loss: 138.7362\n",
      "Iteration: 78900\n",
      "Gradient: [ -2.7703   3.2358  -0.2898 -48.1011 101.411 ]\n",
      "Weights: [-4.4054 -0.6133 -0.3878  0.0965  0.0974]\n",
      "MSE loss: 138.6396\n",
      "Iteration: 79000\n",
      "Gradient: [ -0.2008   5.7116  26.9651 184.7693  76.6605]\n",
      "Weights: [-4.4016 -0.6124 -0.3886  0.0964  0.0974]\n",
      "MSE loss: 138.5808\n",
      "Iteration: 79100\n",
      "Gradient: [ -8.1106 -10.0864  44.0717  38.7136  16.1584]\n",
      "Weights: [-4.4017 -0.6125 -0.3891  0.0962  0.0975]\n",
      "MSE loss: 138.5236\n",
      "Iteration: 79200\n",
      "Gradient: [  8.3158  -3.1938  13.3003 -25.61   -76.2611]\n",
      "Weights: [-4.4062 -0.6125 -0.3895  0.0963  0.0975]\n",
      "MSE loss: 138.4447\n",
      "Iteration: 79300\n",
      "Gradient: [ -3.9678  11.1538   4.3444  65.1949 -74.1848]\n",
      "Weights: [-4.4048 -0.6113 -0.3898  0.0963  0.0976]\n",
      "MSE loss: 138.338\n",
      "Iteration: 79400\n",
      "Gradient: [  -3.1034   20.3139   33.9916  -23.6505 -148.006 ]\n",
      "Weights: [-4.4034 -0.6097 -0.3902  0.0962  0.0977]\n",
      "MSE loss: 138.2833\n",
      "Iteration: 79500\n",
      "Gradient: [  3.5972  -5.928  -11.6353  89.6892 -21.661 ]\n",
      "Weights: [-4.4076 -0.6095 -0.3909  0.0961  0.0977]\n",
      "MSE loss: 138.1828\n",
      "Iteration: 79600\n",
      "Gradient: [ -2.4096  10.4271  45.2774 -96.3221  79.2973]\n",
      "Weights: [-4.4108 -0.6072 -0.3909  0.0961  0.0978]\n",
      "MSE loss: 138.1071\n",
      "Iteration: 79700\n",
      "Gradient: [ -12.6483   -4.1765   47.5338  -48.9494 -299.0945]\n",
      "Weights: [-4.4107 -0.6072 -0.3916  0.096   0.0978]\n",
      "MSE loss: 138.0353\n",
      "Iteration: 79800\n",
      "Gradient: [ -18.1979   -1.8649  -24.3765  -10.6678 -386.2397]\n",
      "Weights: [-4.4129 -0.6055 -0.3921  0.0962  0.0979]\n",
      "MSE loss: 137.9742\n",
      "Iteration: 79900\n",
      "Gradient: [   4.9611   -7.869     5.6928   96.7612 -163.8509]\n",
      "Weights: [-4.4076 -0.6048 -0.3928  0.0961  0.0979]\n",
      "MSE loss: 137.8968\n",
      "Iteration: 80000\n",
      "Gradient: [   6.       -4.2378   22.4312   40.8284 -142.1041]\n",
      "Weights: [-4.414  -0.6026 -0.3934  0.0961  0.098 ]\n",
      "MSE loss: 137.7886\n",
      "Iteration: 80100\n",
      "Gradient: [  7.564  -12.7161 -15.4931 -35.7387  86.3529]\n",
      "Weights: [-4.4134 -0.6008 -0.3943  0.096   0.098 ]\n",
      "MSE loss: 137.671\n",
      "Iteration: 80200\n",
      "Gradient: [ -16.6409   16.8072   11.3985   55.5589 -241.3817]\n",
      "Weights: [-4.413  -0.5987 -0.3952  0.096   0.098 ]\n",
      "MSE loss: 137.5677\n",
      "Iteration: 80300\n",
      "Gradient: [ 6.2554 11.022  21.5436 76.0539 19.6663]\n",
      "Weights: [-4.4105 -0.5981 -0.396   0.0959  0.0981]\n",
      "MSE loss: 137.4808\n",
      "Iteration: 80400\n",
      "Gradient: [  0.8986   5.9089  50.0659  16.9034 -12.5829]\n",
      "Weights: [-4.4156 -0.5964 -0.3968  0.0959  0.0981]\n",
      "MSE loss: 137.3852\n",
      "Iteration: 80500\n",
      "Gradient: [   4.6108  -17.21      1.2152   -8.3857 -264.7189]\n",
      "Weights: [-4.4175 -0.5943 -0.3973  0.0959  0.0982]\n",
      "MSE loss: 137.3057\n",
      "Iteration: 80600\n",
      "Gradient: [  12.0702    0.4596   60.9984   17.334  -312.1224]\n",
      "Weights: [-4.4071 -0.5943 -0.398   0.0959  0.0982]\n",
      "MSE loss: 137.3088\n",
      "Iteration: 80700\n",
      "Gradient: [ 16.0675  -3.7263   5.0903 104.7608  31.8597]\n",
      "Weights: [-4.4121 -0.5957 -0.3986  0.0959  0.0983]\n",
      "MSE loss: 137.1841\n",
      "Iteration: 80800\n",
      "Gradient: [  10.6184   21.0912   14.4192  -33.0868 -169.4148]\n",
      "Weights: [-4.4143 -0.5931 -0.3991  0.0959  0.0983]\n",
      "MSE loss: 137.0879\n",
      "Iteration: 80900\n",
      "Gradient: [ -0.7845  21.0335 -57.7641 -74.1864 145.7536]\n",
      "Weights: [-4.4216 -0.5906 -0.3995  0.0959  0.0983]\n",
      "MSE loss: 137.0629\n",
      "Iteration: 81000\n",
      "Gradient: [  -7.112   -19.4414    4.9112   56.048  -438.2088]\n",
      "Weights: [-4.4174 -0.5884 -0.3999  0.0959  0.0984]\n",
      "MSE loss: 136.9688\n",
      "Iteration: 81100\n",
      "Gradient: [  -3.7638  -15.7983   32.5527   30.5916 -274.2424]\n",
      "Weights: [-4.4211 -0.5876 -0.4008  0.0959  0.0984]\n",
      "MSE loss: 136.8672\n",
      "Iteration: 81200\n",
      "Gradient: [  -3.8309   17.3738   31.739   -12.4412 -161.2742]\n",
      "Weights: [-4.4182 -0.5863 -0.4014  0.0958  0.0985]\n",
      "MSE loss: 136.7645\n",
      "Iteration: 81300\n",
      "Gradient: [ 13.4254   3.997   26.0378  88.6659 -29.9528]\n",
      "Weights: [-4.4206 -0.5838 -0.402   0.0957  0.0985]\n",
      "MSE loss: 136.672\n",
      "Iteration: 81400\n",
      "Gradient: [   1.104    -3.1184   19.5926  -11.7368 -143.4053]\n",
      "Weights: [-4.4159 -0.5853 -0.4026  0.0956  0.0985]\n",
      "MSE loss: 136.6213\n",
      "Iteration: 81500\n",
      "Gradient: [-8.12120e+00 -2.17000e-02  5.45732e+01  6.26656e+01 -9.36781e+01]\n",
      "Weights: [-4.4178 -0.5833 -0.4034  0.0956  0.0986]\n",
      "MSE loss: 136.5134\n",
      "Iteration: 81600\n",
      "Gradient: [  -2.0795   11.4133   28.9525   -6.0221 -231.9123]\n",
      "Weights: [-4.4178 -0.5829 -0.4039  0.0955  0.0986]\n",
      "MSE loss: 136.4484\n",
      "Iteration: 81700\n",
      "Gradient: [ 14.1266  13.6463 -16.7784 -70.8204 226.6051]\n",
      "Weights: [-4.4201 -0.5816 -0.4046  0.0956  0.0987]\n",
      "MSE loss: 136.3721\n",
      "Iteration: 81800\n",
      "Gradient: [  8.5744   5.48   -15.6457  22.9617 -34.0225]\n",
      "Weights: [-4.4222 -0.5805 -0.4048  0.0956  0.0987]\n",
      "MSE loss: 136.3305\n",
      "Iteration: 81900\n",
      "Gradient: [ -3.565   16.689   -3.8528  48.763  -31.122 ]\n",
      "Weights: [-4.4205 -0.5795 -0.4054  0.0956  0.0988]\n",
      "MSE loss: 136.2306\n",
      "Iteration: 82000\n",
      "Gradient: [  24.7575   14.5876  -50.7967   12.123  -210.5924]\n",
      "Weights: [-4.4242 -0.5773 -0.406   0.0955  0.0988]\n",
      "MSE loss: 136.152\n",
      "Iteration: 82100\n",
      "Gradient: [ -6.4004   1.3698   3.3758 -34.1367  60.4503]\n",
      "Weights: [-4.4228 -0.5758 -0.4065  0.0954  0.0989]\n",
      "MSE loss: 136.0693\n",
      "Iteration: 82200\n",
      "Gradient: [   4.5953  -18.6279   56.0175   40.0449 -234.9198]\n",
      "Weights: [-4.4214 -0.5744 -0.4073  0.0953  0.0989]\n",
      "MSE loss: 135.9693\n",
      "Iteration: 82300\n",
      "Gradient: [ -1.7241 -27.3246  -5.0595 -26.0449 -57.7466]\n",
      "Weights: [-4.424  -0.5746 -0.4078  0.0953  0.099 ]\n",
      "MSE loss: 135.8932\n",
      "Iteration: 82400\n",
      "Gradient: [   3.4571   19.1378   -9.3711  170.1143 -157.559 ]\n",
      "Weights: [-4.424  -0.5725 -0.4084  0.0952  0.099 ]\n",
      "MSE loss: 135.8013\n",
      "Iteration: 82500\n",
      "Gradient: [  -3.3975  -12.2062  -13.7727  -54.9418 -138.5926]\n",
      "Weights: [-4.4237 -0.5711 -0.409   0.0951  0.0991]\n",
      "MSE loss: 135.7253\n",
      "Iteration: 82600\n",
      "Gradient: [ -1.9345   1.3227  10.8239  94.8878 200.6108]\n",
      "Weights: [-4.4221 -0.5716 -0.4095  0.0951  0.0991]\n",
      "MSE loss: 135.6608\n",
      "Iteration: 82700\n",
      "Gradient: [ -0.5914  -0.1957  -0.7054   3.0207 -81.2642]\n",
      "Weights: [-4.4193 -0.5702 -0.4102  0.0949  0.0991]\n",
      "MSE loss: 135.5849\n",
      "Iteration: 82800\n",
      "Gradient: [  0.7565 -31.1963  24.6651 -30.7894 172.886 ]\n",
      "Weights: [-4.4239 -0.5687 -0.411   0.0949  0.0992]\n",
      "MSE loss: 135.4712\n",
      "Iteration: 82900\n",
      "Gradient: [ -3.3211 -10.3916   5.8078 -62.2227  31.625 ]\n",
      "Weights: [-4.426  -0.5671 -0.4117  0.095   0.0993]\n",
      "MSE loss: 135.3807\n",
      "Iteration: 83000\n",
      "Gradient: [   3.6085  -13.0951  -21.2855   40.797  -230.331 ]\n",
      "Weights: [-4.4238 -0.5674 -0.4121  0.0949  0.0993]\n",
      "MSE loss: 135.331\n",
      "Iteration: 83100\n",
      "Gradient: [   8.5045   -4.4581   11.0546  -16.1399 -365.4988]\n",
      "Weights: [-4.4249 -0.565  -0.4125  0.0949  0.0993]\n",
      "MSE loss: 135.2568\n",
      "Iteration: 83200\n",
      "Gradient: [  4.4018  18.2432   8.4184 -39.0616 -28.024 ]\n",
      "Weights: [-4.4267 -0.565  -0.4131  0.0948  0.0994]\n",
      "MSE loss: 135.2095\n",
      "Iteration: 83300\n",
      "Gradient: [   6.5006    0.4969   -0.7319  -42.5293 -253.7589]\n",
      "Weights: [-4.425  -0.5638 -0.4137  0.0948  0.0994]\n",
      "MSE loss: 135.091\n",
      "Iteration: 83400\n",
      "Gradient: [-4.12200e+00  6.82000e-02  6.48820e+00 -8.75100e-01 -3.29286e+02]\n",
      "Weights: [-4.4231 -0.5616 -0.4141  0.0947  0.0995]\n",
      "MSE loss: 135.0537\n",
      "Iteration: 83500\n",
      "Gradient: [   1.0745  -13.2252   -8.9171   51.4069 -110.3233]\n",
      "Weights: [-4.4232 -0.5619 -0.4147  0.0947  0.0995]\n",
      "MSE loss: 134.9755\n",
      "Iteration: 83600\n",
      "Gradient: [   4.737     9.3657   -6.6512  -18.5776 -282.0674]\n",
      "Weights: [-4.4241 -0.5608 -0.4154  0.0946  0.0996]\n",
      "MSE loss: 134.8802\n",
      "Iteration: 83700\n",
      "Gradient: [ -1.2006  -7.6565  -3.2906   2.6887 -96.075 ]\n",
      "Weights: [-4.4266 -0.5606 -0.4159  0.0946  0.0996]\n",
      "MSE loss: 134.8242\n",
      "Iteration: 83800\n",
      "Gradient: [-10.8682 -15.0619  -5.7389 -79.1292 -61.0577]\n",
      "Weights: [-4.4268 -0.5598 -0.4167  0.0946  0.0997]\n",
      "MSE loss: 134.7341\n",
      "Iteration: 83900\n",
      "Gradient: [   8.1811   -0.9459  -27.7643   -9.584  -155.9544]\n",
      "Weights: [-4.4271 -0.5554 -0.4172  0.0946  0.0997]\n",
      "MSE loss: 134.6273\n",
      "Iteration: 84000\n",
      "Gradient: [ -4.9067 -18.4519  31.4659  68.8488  53.2948]\n",
      "Weights: [-4.4255 -0.5554 -0.4181  0.0945  0.0998]\n",
      "MSE loss: 134.5275\n",
      "Iteration: 84100\n",
      "Gradient: [  -4.6866  -10.7137   -3.2208 -127.5069   90.3163]\n",
      "Weights: [-4.4264 -0.5548 -0.4188  0.0944  0.0998]\n",
      "MSE loss: 134.4368\n",
      "Iteration: 84200\n",
      "Gradient: [  8.354   -1.6001  -1.1838  51.8823 288.0833]\n",
      "Weights: [-4.4265 -0.5545 -0.4198  0.0945  0.0999]\n",
      "MSE loss: 134.3456\n",
      "Iteration: 84300\n",
      "Gradient: [   1.5574   -4.3937  -52.4727   81.1224 -292.6334]\n",
      "Weights: [-4.4289 -0.5529 -0.4202  0.0945  0.0999]\n",
      "MSE loss: 134.2619\n",
      "Iteration: 84400\n",
      "Gradient: [  0.0623 -24.7722  51.0679 -18.8803  15.0267]\n",
      "Weights: [-4.4284 -0.5511 -0.4209  0.0944  0.1   ]\n",
      "MSE loss: 134.162\n",
      "Iteration: 84500\n",
      "Gradient: [  7.6985  -0.4896  62.9395  61.0337 -55.5688]\n",
      "Weights: [-4.4266 -0.5507 -0.4215  0.0944  0.1001]\n",
      "MSE loss: 134.0879\n",
      "Iteration: 84600\n",
      "Gradient: [-11.0184  -7.6672  27.6347  -5.5362  14.0532]\n",
      "Weights: [-4.4253 -0.5494 -0.4223  0.0944  0.1001]\n",
      "MSE loss: 133.9955\n",
      "Iteration: 84700\n",
      "Gradient: [   8.3123    9.0724   37.2346   18.7064 -102.6543]\n",
      "Weights: [-4.4272 -0.5489 -0.4229  0.0943  0.1002]\n",
      "MSE loss: 133.8985\n",
      "Iteration: 84800\n",
      "Gradient: [   1.9681   11.066    26.1566   78.993  -251.8788]\n",
      "Weights: [-4.4248 -0.5482 -0.4235  0.0942  0.1002]\n",
      "MSE loss: 133.8242\n",
      "Iteration: 84900\n",
      "Gradient: [  9.6114   7.787   32.8674 -31.6278 -61.2054]\n",
      "Weights: [-4.4226 -0.5473 -0.4239  0.0941  0.1003]\n",
      "MSE loss: 133.7995\n",
      "Iteration: 85000\n",
      "Gradient: [ -3.7728  -0.7775  55.8887 -13.4408 -73.9307]\n",
      "Weights: [-4.4218 -0.5483 -0.4243  0.0941  0.1003]\n",
      "MSE loss: 133.7557\n",
      "Iteration: 85100\n",
      "Gradient: [  -0.6212  -22.536   -18.6959 -130.8249 -399.9979]\n",
      "Weights: [-4.4298 -0.5471 -0.4244  0.0941  0.1004]\n",
      "MSE loss: 133.683\n",
      "Iteration: 85200\n",
      "Gradient: [-16.8761 -14.7661  28.5299 -50.5079   2.9299]\n",
      "Weights: [-4.4271 -0.5459 -0.425   0.094   0.1004]\n",
      "MSE loss: 133.5786\n",
      "Iteration: 85300\n",
      "Gradient: [  -2.2403  -13.2623   16.3996   45.0287 -212.3874]\n",
      "Weights: [-4.4257 -0.5459 -0.426   0.094   0.1004]\n",
      "MSE loss: 133.5077\n",
      "Iteration: 85400\n",
      "Gradient: [  1.7252  -4.8707 -14.2204 -46.6172 -60.6427]\n",
      "Weights: [-4.4262 -0.544  -0.4266  0.094   0.1005]\n",
      "MSE loss: 133.4172\n",
      "Iteration: 85500\n",
      "Gradient: [  1.9735  10.1094   9.2114  69.1277 -18.5847]\n",
      "Weights: [-4.4301 -0.5418 -0.4274  0.0939  0.1006]\n",
      "MSE loss: 133.2901\n",
      "Iteration: 85600\n",
      "Gradient: [13.921  10.5469 19.194  72.3842 98.5184]\n",
      "Weights: [-4.4289 -0.5423 -0.428   0.0938  0.1006]\n",
      "MSE loss: 133.2323\n",
      "Iteration: 85700\n",
      "Gradient: [  10.4289    5.4247   -3.4711    2.9329 -123.7261]\n",
      "Weights: [-4.4279 -0.5399 -0.4286  0.0938  0.1007]\n",
      "MSE loss: 133.1333\n",
      "Iteration: 85800\n",
      "Gradient: [  2.391  -25.4813  32.3393 -22.1984 -54.4354]\n",
      "Weights: [-4.4298 -0.5381 -0.4291  0.0938  0.1007]\n",
      "MSE loss: 133.0504\n",
      "Iteration: 85900\n",
      "Gradient: [ -2.9019  17.6579 -21.9711 -24.3542  68.99  ]\n",
      "Weights: [-4.432  -0.5377 -0.4297  0.0938  0.1008]\n",
      "MSE loss: 132.9795\n",
      "Iteration: 86000\n",
      "Gradient: [  -3.2251   -9.6986  -45.3735  112.7396 -338.6338]\n",
      "Weights: [-4.434  -0.5356 -0.4301  0.0939  0.1008]\n",
      "MSE loss: 132.9188\n",
      "Iteration: 86100\n",
      "Gradient: [  2.6456  -0.8461  19.46    20.3826 145.3061]\n",
      "Weights: [-4.4314 -0.5359 -0.4307  0.0939  0.1009]\n",
      "MSE loss: 132.8582\n",
      "Iteration: 86200\n",
      "Gradient: [   7.6606   11.7681   63.4603  121.2381 -115.3228]\n",
      "Weights: [-4.4303 -0.5367 -0.4315  0.0939  0.1009]\n",
      "MSE loss: 132.7933\n",
      "Iteration: 86300\n",
      "Gradient: [   6.3168   -9.2068    5.3378   78.259  -139.3831]\n",
      "Weights: [-4.4273 -0.5361 -0.432   0.0939  0.101 ]\n",
      "MSE loss: 132.7233\n",
      "Iteration: 86400\n",
      "Gradient: [   2.7428    8.682    50.5149  128.0069 -437.8833]\n",
      "Weights: [-4.4283 -0.5347 -0.4327  0.0938  0.101 ]\n",
      "MSE loss: 132.6159\n",
      "Iteration: 86500\n",
      "Gradient: [  1.5365  14.956   -9.0623   4.4128 169.8906]\n",
      "Weights: [-4.4281 -0.5337 -0.4333  0.0936  0.1011]\n",
      "MSE loss: 132.5443\n",
      "Iteration: 86600\n",
      "Gradient: [ -0.5499   7.0932   5.1652 139.0392 -88.1338]\n",
      "Weights: [-4.4296 -0.5324 -0.4337  0.0937  0.1012]\n",
      "MSE loss: 132.459\n",
      "Iteration: 86700\n",
      "Gradient: [  -7.2556    5.9157   12.5539  -90.7328 -142.4236]\n",
      "Weights: [-4.4358 -0.5313 -0.434   0.0936  0.1012]\n",
      "MSE loss: 132.4244\n",
      "Iteration: 86800\n",
      "Gradient: [-10.5757   1.333   12.698   36.7518  14.723 ]\n",
      "Weights: [-4.428  -0.5308 -0.4345  0.0935  0.1012]\n",
      "MSE loss: 132.3683\n",
      "Iteration: 86900\n",
      "Gradient: [ -5.8061   6.1307  -3.1447 -46.4168 -67.7147]\n",
      "Weights: [-4.4329 -0.5321 -0.4348  0.0935  0.1013]\n",
      "MSE loss: 132.3313\n",
      "Iteration: 87000\n",
      "Gradient: [   6.2455   -5.5418   18.9574 -112.1047  -59.8826]\n",
      "Weights: [-4.4283 -0.5305 -0.4354  0.0936  0.1013]\n",
      "MSE loss: 132.2541\n",
      "Iteration: 87100\n",
      "Gradient: [  7.4943   1.3964  44.1779  13.7306 -77.5052]\n",
      "Weights: [-4.4338 -0.5292 -0.4358  0.0935  0.1014]\n",
      "MSE loss: 132.1565\n",
      "Iteration: 87200\n",
      "Gradient: [  5.6963   2.7244  49.3474 103.8147  -3.9279]\n",
      "Weights: [-4.4359 -0.5249 -0.4367  0.0934  0.1015]\n",
      "MSE loss: 132.036\n",
      "Iteration: 87300\n",
      "Gradient: [   6.2788   -5.6478   21.1957  -38.9289 -292.1221]\n",
      "Weights: [-4.4363 -0.5241 -0.4377  0.0933  0.1015]\n",
      "MSE loss: 131.9191\n",
      "Iteration: 87400\n",
      "Gradient: [  -1.5669   23.4827  -19.6074  -93.1696 -251.5162]\n",
      "Weights: [-4.4421 -0.5217 -0.4378  0.0932  0.1015]\n",
      "MSE loss: 131.9101\n",
      "Iteration: 87500\n",
      "Gradient: [ 8.0671 28.7373 17.411  45.6246 79.7124]\n",
      "Weights: [-4.4296 -0.5227 -0.4389  0.0932  0.1015]\n",
      "MSE loss: 131.8217\n",
      "Iteration: 87600\n",
      "Gradient: [ 3.5747 -5.7169 15.6115 47.8311 15.4724]\n",
      "Weights: [-4.4318 -0.5225 -0.4397  0.0932  0.1016]\n",
      "MSE loss: 131.7235\n",
      "Iteration: 87700\n",
      "Gradient: [ -3.201  -31.9541 -12.1263 -14.0372 -51.1835]\n",
      "Weights: [-4.4323 -0.5216 -0.4403  0.0933  0.1016]\n",
      "MSE loss: 131.659\n",
      "Iteration: 87800\n",
      "Gradient: [ -5.4491 -16.7196  14.4245 -13.4953 125.1437]\n",
      "Weights: [-4.4352 -0.5194 -0.4406  0.0932  0.1017]\n",
      "MSE loss: 131.5782\n",
      "Iteration: 87900\n",
      "Gradient: [ -10.8742   -2.9951   14.245   -12.2786 -100.8779]\n",
      "Weights: [-4.4342 -0.5189 -0.4411  0.0931  0.1017]\n",
      "MSE loss: 131.5156\n",
      "Iteration: 88000\n",
      "Gradient: [  -3.6332    2.0767  -14.6706   64.4544 -183.2558]\n",
      "Weights: [-4.4359 -0.5183 -0.4417  0.093   0.1018]\n",
      "MSE loss: 131.4368\n",
      "Iteration: 88100\n",
      "Gradient: [ 5.180000e-02 -8.223600e+00  4.658120e+01 -3.412380e+01  1.220033e+02]\n",
      "Weights: [-4.4348 -0.5168 -0.4422  0.093   0.1018]\n",
      "MSE loss: 131.3442\n",
      "Iteration: 88200\n",
      "Gradient: [-2.57010e+00  2.16240e+00  2.26000e-02 -4.56058e+01  4.86570e+01]\n",
      "Weights: [-4.4369 -0.5173 -0.4427  0.0929  0.1019]\n",
      "MSE loss: 131.304\n",
      "Iteration: 88300\n",
      "Gradient: [ -11.4047    5.4438   21.4236  132.1723 -246.71  ]\n",
      "Weights: [-4.4375 -0.5155 -0.4435  0.0929  0.1019]\n",
      "MSE loss: 131.2184\n",
      "Iteration: 88400\n",
      "Gradient: [   4.5342    8.881    -0.9931   44.111  -194.0583]\n",
      "Weights: [-4.4331 -0.5145 -0.4441  0.0929  0.102 ]\n",
      "MSE loss: 131.1403\n",
      "Iteration: 88500\n",
      "Gradient: [ -2.9619 -11.1088 -28.1435 -49.1525 171.2309]\n",
      "Weights: [-4.4338 -0.5148 -0.4445  0.093   0.102 ]\n",
      "MSE loss: 131.0833\n",
      "Iteration: 88600\n",
      "Gradient: [-15.3022  -7.0457  38.8577   7.8312 201.8835]\n",
      "Weights: [-4.4326 -0.515  -0.4449  0.0929  0.1021]\n",
      "MSE loss: 131.0351\n",
      "Iteration: 88700\n",
      "Gradient: [  6.2675 -10.8398  31.8924  -0.4747 -22.0355]\n",
      "Weights: [-4.4338 -0.5127 -0.4456  0.0928  0.1022]\n",
      "MSE loss: 130.9299\n",
      "Iteration: 88800\n",
      "Gradient: [ -1.6976  -6.9205 -27.9976  -1.7268  76.7918]\n",
      "Weights: [-4.4379 -0.5114 -0.4461  0.0928  0.1022]\n",
      "MSE loss: 130.8578\n",
      "Iteration: 88900\n",
      "Gradient: [ -4.4377  20.3001  13.5279   4.1246 320.8686]\n",
      "Weights: [-4.4351 -0.5111 -0.4463  0.0928  0.1022]\n",
      "MSE loss: 130.8169\n",
      "Iteration: 89000\n",
      "Gradient: [  10.5368  -11.0474   -5.947    25.0917 -378.7699]\n",
      "Weights: [-4.4361 -0.5109 -0.4471  0.0928  0.1023]\n",
      "MSE loss: 130.7375\n",
      "Iteration: 89100\n",
      "Gradient: [ -11.0621    2.8857  -22.7479  -72.9402 -169.026 ]\n",
      "Weights: [-4.4435 -0.5096 -0.4476  0.0928  0.1023]\n",
      "MSE loss: 130.7397\n",
      "Iteration: 89200\n",
      "Gradient: [-1.16000e-02  2.70930e+00  2.76208e+01  1.55929e+01  2.39089e+01]\n",
      "Weights: [-4.4366 -0.508  -0.4479  0.0928  0.1024]\n",
      "MSE loss: 130.6202\n",
      "Iteration: 89300\n",
      "Gradient: [  5.1502 -12.5887 -22.99    41.8229  77.8058]\n",
      "Weights: [-4.4417 -0.5074 -0.4487  0.0927  0.1024]\n",
      "MSE loss: 130.5455\n",
      "Iteration: 89400\n",
      "Gradient: [-12.9792   7.0317 -11.3031  91.267  -22.3803]\n",
      "Weights: [-4.4392 -0.5067 -0.4491  0.0928  0.1025]\n",
      "MSE loss: 130.4727\n",
      "Iteration: 89500\n",
      "Gradient: [  3.8505  16.6731  30.6656 -20.1273 -62.5951]\n",
      "Weights: [-4.4358 -0.5054 -0.4497  0.0927  0.1025]\n",
      "MSE loss: 130.4226\n",
      "Iteration: 89600\n",
      "Gradient: [-11.3989  -4.7576  27.5156 -13.9078  25.7005]\n",
      "Weights: [-4.4363 -0.5039 -0.45    0.0926  0.1025]\n",
      "MSE loss: 130.3865\n",
      "Iteration: 89700\n",
      "Gradient: [-11.3642   4.6876 -27.3474 -39.4502 332.766 ]\n",
      "Weights: [-4.4397 -0.5037 -0.4506  0.0927  0.1026]\n",
      "MSE loss: 130.2775\n",
      "Iteration: 89800\n",
      "Gradient: [  -5.4087    6.9407  -21.6278   46.61   -223.5412]\n",
      "Weights: [-4.4381 -0.5039 -0.4511  0.0927  0.1026]\n",
      "MSE loss: 130.2388\n",
      "Iteration: 89900\n",
      "Gradient: [  9.2     -0.7425  62.5078  37.8675 -49.8432]\n",
      "Weights: [-4.4391 -0.5028 -0.452   0.0927  0.1027]\n",
      "MSE loss: 130.1429\n",
      "Iteration: 90000\n",
      "Gradient: [  5.9494   2.4133  31.6042  34.5695 436.4374]\n",
      "Weights: [-4.4358 -0.5018 -0.4525  0.0926  0.1027]\n",
      "MSE loss: 130.0915\n",
      "Iteration: 90100\n",
      "Gradient: [  -1.4374  -18.6984   12.4107   37.7107 -604.0773]\n",
      "Weights: [-4.442  -0.5015 -0.4531  0.0926  0.1027]\n",
      "MSE loss: 130.0457\n",
      "Iteration: 90200\n",
      "Gradient: [  2.5338   3.9375  11.6607 -35.938  246.5344]\n",
      "Weights: [-4.4415 -0.5003 -0.4533  0.0926  0.1028]\n",
      "MSE loss: 129.9559\n",
      "Iteration: 90300\n",
      "Gradient: [ 14.3319 -12.6651  48.0268  95.6709  -4.1439]\n",
      "Weights: [-4.4366 -0.499  -0.454   0.0925  0.1028]\n",
      "MSE loss: 129.8982\n",
      "Iteration: 90400\n",
      "Gradient: [  -5.4278  -15.8118   29.0757  -15.377  -294.9019]\n",
      "Weights: [-4.4406 -0.4974 -0.4545  0.0924  0.1029]\n",
      "MSE loss: 129.8116\n",
      "Iteration: 90500\n",
      "Gradient: [  9.2313  -4.2374  12.187  124.8534 103.3909]\n",
      "Weights: [-4.4374 -0.4972 -0.4551  0.0923  0.1029]\n",
      "MSE loss: 129.7699\n",
      "Iteration: 90600\n",
      "Gradient: [   7.0657   -3.7586    5.3793  -39.4547 -332.8522]\n",
      "Weights: [-4.4362 -0.4964 -0.4559  0.0923  0.1029]\n",
      "MSE loss: 129.7036\n",
      "Iteration: 90700\n",
      "Gradient: [  3.0943  -3.3429 -32.2668  -4.1326 129.8231]\n",
      "Weights: [-4.4384 -0.4946 -0.4564  0.0923  0.103 ]\n",
      "MSE loss: 129.6042\n",
      "Iteration: 90800\n",
      "Gradient: [  -9.3459    8.5189    4.9829   43.9664 -102.632 ]\n",
      "Weights: [-4.4384 -0.4941 -0.457   0.0923  0.103 ]\n",
      "MSE loss: 129.5464\n",
      "Iteration: 90900\n",
      "Gradient: [  -2.0831   -3.3529  -24.2706   46.5072 -346.0898]\n",
      "Weights: [-4.4405 -0.4933 -0.4572  0.0923  0.1031]\n",
      "MSE loss: 129.4738\n",
      "Iteration: 91000\n",
      "Gradient: [  -2.8438   13.9454  -14.4311  -19.3625 -139.4654]\n",
      "Weights: [-4.444  -0.4919 -0.4577  0.0922  0.1031]\n",
      "MSE loss: 129.4116\n",
      "Iteration: 91100\n",
      "Gradient: [   6.7961   11.6164   -9.5311  -80.3671 -336.8395]\n",
      "Weights: [-4.4449 -0.4908 -0.458   0.0922  0.1032]\n",
      "MSE loss: 129.3577\n",
      "Iteration: 91200\n",
      "Gradient: [11.6671  8.3382 26.8874 90.259  57.4537]\n",
      "Weights: [-4.4418 -0.4907 -0.4585  0.0922  0.1032]\n",
      "MSE loss: 129.3056\n",
      "Iteration: 91300\n",
      "Gradient: [   9.4914    2.587    -4.8243   58.4692 -369.9676]\n",
      "Weights: [-4.4482 -0.4891 -0.4591  0.0923  0.1032]\n",
      "MSE loss: 129.2779\n",
      "Iteration: 91400\n",
      "Gradient: [ -11.195    -1.3152  -50.9725   29.6289 -409.8202]\n",
      "Weights: [-4.4417 -0.4876 -0.4596  0.0922  0.1032]\n",
      "MSE loss: 129.2007\n",
      "Iteration: 91500\n",
      "Gradient: [ 13.49    -3.2778  37.6562  28.0461 148.5455]\n",
      "Weights: [-4.4402 -0.4867 -0.4604  0.0921  0.1032]\n",
      "MSE loss: 129.1286\n",
      "Iteration: 91600\n",
      "Gradient: [  9.3668  25.9301  -6.407   27.2787 241.5123]\n",
      "Weights: [-4.4376 -0.4855 -0.4614  0.092   0.1033]\n",
      "MSE loss: 129.0637\n",
      "Iteration: 91700\n",
      "Gradient: [   7.9649  -11.1586   -0.72     11.1408 -280.8434]\n",
      "Weights: [-4.4445 -0.4831 -0.4618  0.0921  0.1033]\n",
      "MSE loss: 128.9371\n",
      "Iteration: 91800\n",
      "Gradient: [  4.3283  -9.1952 -19.3041  57.1823 -30.1669]\n",
      "Weights: [-4.4468 -0.4817 -0.462   0.0921  0.1034]\n",
      "MSE loss: 128.8968\n",
      "Iteration: 91900\n",
      "Gradient: [  3.673    3.4474 -12.3369 117.6924 197.7554]\n",
      "Weights: [-4.4466 -0.4817 -0.4626  0.0921  0.1034]\n",
      "MSE loss: 128.8387\n",
      "Iteration: 92000\n",
      "Gradient: [ -1.1019   2.4414 -14.7439 -78.1935 -29.1998]\n",
      "Weights: [-4.4481 -0.4814 -0.4631  0.0921  0.1034]\n",
      "MSE loss: 128.7938\n",
      "Iteration: 92100\n",
      "Gradient: [   3.417     6.5177   10.6581   10.7331 -334.502 ]\n",
      "Weights: [-4.4462 -0.48   -0.4636  0.092   0.1035]\n",
      "MSE loss: 128.7124\n",
      "Iteration: 92200\n",
      "Gradient: [  -7.7037    1.1162  -23.845    79.9107 -271.9521]\n",
      "Weights: [-4.4518 -0.4781 -0.4638  0.0919  0.1035]\n",
      "MSE loss: 128.6703\n",
      "Iteration: 92300\n",
      "Gradient: [ -3.1015  -3.043   31.6514 -73.9459  32.1833]\n",
      "Weights: [-4.4572 -0.4771 -0.4642  0.0919  0.1036]\n",
      "MSE loss: 128.7027\n",
      "Iteration: 92400\n",
      "Gradient: [   1.4625  -14.9699   33.6039  -55.2149 -259.887 ]\n",
      "Weights: [-4.4533 -0.4736 -0.4649  0.0918  0.1036]\n",
      "MSE loss: 128.5065\n",
      "Iteration: 92500\n",
      "Gradient: [ -4.3978 -15.0283  41.5678 -37.6405 149.312 ]\n",
      "Weights: [-4.451  -0.4728 -0.4659  0.0917  0.1037]\n",
      "MSE loss: 128.3994\n",
      "Iteration: 92600\n",
      "Gradient: [  -5.655   -11.5618   24.8046   -2.6189 -234.9538]\n",
      "Weights: [-4.4503 -0.4724 -0.4669  0.0917  0.1037]\n",
      "MSE loss: 128.3194\n",
      "Iteration: 92700\n",
      "Gradient: [-13.2828   4.6134  -3.0145  60.5422  45.6608]\n",
      "Weights: [-4.4479 -0.4711 -0.4675  0.0916  0.1037]\n",
      "MSE loss: 128.2585\n",
      "Iteration: 92800\n",
      "Gradient: [-10.8708 -15.8186   8.0839 -58.2294 163.3702]\n",
      "Weights: [-4.4493 -0.4711 -0.4678  0.0915  0.1038]\n",
      "MSE loss: 128.201\n",
      "Iteration: 92900\n",
      "Gradient: [ -6.139   23.2596  17.4601 102.3942  90.6217]\n",
      "Weights: [-4.4481 -0.4698 -0.4681  0.0915  0.1038]\n",
      "MSE loss: 128.1409\n",
      "Iteration: 93000\n",
      "Gradient: [  -1.0364    8.05    -30.6396   37.0689 -182.0962]\n",
      "Weights: [-4.4477 -0.4683 -0.469   0.0914  0.1039]\n",
      "MSE loss: 128.0505\n",
      "Iteration: 93100\n",
      "Gradient: [  3.8467  -2.1142  -8.2937  76.8819 -51.0251]\n",
      "Weights: [-4.4517 -0.4672 -0.4695  0.0913  0.1039]\n",
      "MSE loss: 127.9733\n",
      "Iteration: 93200\n",
      "Gradient: [ -1.8072 -17.3891   4.8431  10.9896 218.8392]\n",
      "Weights: [-4.4454 -0.4663 -0.4701  0.0913  0.1039]\n",
      "MSE loss: 127.9625\n",
      "Iteration: 93300\n",
      "Gradient: [   0.6874   -4.7321   -9.6888 -116.9221  170.8031]\n",
      "Weights: [-4.4482 -0.4658 -0.4707  0.0913  0.104 ]\n",
      "MSE loss: 127.8636\n",
      "Iteration: 93400\n",
      "Gradient: [ -4.3871   8.7079 -32.931   47.632   26.9825]\n",
      "Weights: [-4.4465 -0.4662 -0.4713  0.0914  0.104 ]\n",
      "MSE loss: 127.8201\n",
      "Iteration: 93500\n",
      "Gradient: [ -1.4144  17.2015  -1.0784  58.1346 -17.1683]\n",
      "Weights: [-4.4497 -0.4663 -0.4714  0.0913  0.1041]\n",
      "MSE loss: 127.785\n",
      "Iteration: 93600\n",
      "Gradient: [ -6.507   10.8958 -85.1126  11.0949 222.3463]\n",
      "Weights: [-4.4504 -0.465  -0.4717  0.0914  0.1041]\n",
      "MSE loss: 127.7192\n",
      "Iteration: 93700\n",
      "Gradient: [ -7.1504  13.4604 -23.4489  13.3537 -59.2147]\n",
      "Weights: [-4.4533 -0.4621 -0.4726  0.0914  0.1042]\n",
      "MSE loss: 127.5941\n",
      "Iteration: 93800\n",
      "Gradient: [  -2.6119    1.1255  -17.8374   51.8541 -122.397 ]\n",
      "Weights: [-4.4573 -0.4603 -0.4732  0.0913  0.1042]\n",
      "MSE loss: 127.5252\n",
      "Iteration: 93900\n",
      "Gradient: [  4.6181  -4.6101 -27.1498  36.5828  29.7136]\n",
      "Weights: [-4.4591 -0.4588 -0.4739  0.0913  0.1043]\n",
      "MSE loss: 127.4415\n",
      "Iteration: 94000\n",
      "Gradient: [-6.3691  1.8526 13.14   66.8648 -5.0356]\n",
      "Weights: [-4.456  -0.4567 -0.4742  0.0913  0.1043]\n",
      "MSE loss: 127.392\n",
      "Iteration: 94100\n",
      "Gradient: [ 19.2455 -11.2178  19.848   74.9549 -76.1099]\n",
      "Weights: [-4.4578 -0.4557 -0.4751  0.0912  0.1043]\n",
      "MSE loss: 127.2869\n",
      "Iteration: 94200\n",
      "Gradient: [   3.3905   -5.8844   40.4351  -37.3131 -321.8793]\n",
      "Weights: [-4.4584 -0.4553 -0.4755  0.0912  0.1044]\n",
      "MSE loss: 127.2409\n",
      "Iteration: 94300\n",
      "Gradient: [ -16.3401    5.202    -3.4744  -23.9634 -124.3183]\n",
      "Weights: [-4.4525 -0.4556 -0.4763  0.0911  0.1044]\n",
      "MSE loss: 127.1651\n",
      "Iteration: 94400\n",
      "Gradient: [ -2.621   -2.4418  65.7838 -76.7483 143.2491]\n",
      "Weights: [-4.4571 -0.4534 -0.4767  0.091   0.1045]\n",
      "MSE loss: 127.0841\n",
      "Iteration: 94500\n",
      "Gradient: [-0.9835 -0.7465  1.386  62.4985 59.0139]\n",
      "Weights: [-4.4597 -0.4509 -0.4773  0.091   0.1045]\n",
      "MSE loss: 127.0143\n",
      "Iteration: 94600\n",
      "Gradient: [ 16.4989 -19.0834 -64.1293 -21.6143 -42.6025]\n",
      "Weights: [-4.4546 -0.4515 -0.4777  0.091   0.1045]\n",
      "MSE loss: 126.9884\n",
      "Iteration: 94700\n",
      "Gradient: [ -0.1719   5.0195  17.1933 -45.4576  43.1539]\n",
      "Weights: [-4.4558 -0.4501 -0.4782  0.0909  0.1046]\n",
      "MSE loss: 126.923\n",
      "Iteration: 94800\n",
      "Gradient: [-10.7724  22.6462  39.8235 -65.5      2.1817]\n",
      "Weights: [-4.4548 -0.4516 -0.4787  0.0909  0.1046]\n",
      "MSE loss: 126.8938\n",
      "Iteration: 94900\n",
      "Gradient: [ -7.4102   7.2863  33.3256 168.732  155.9561]\n",
      "Weights: [-4.4578 -0.4506 -0.4791  0.0909  0.1047]\n",
      "MSE loss: 126.833\n",
      "Iteration: 95000\n",
      "Gradient: [-5.6659  1.1172 15.8492 37.2075 59.5903]\n",
      "Weights: [-4.4547 -0.4498 -0.4794  0.0909  0.1047]\n",
      "MSE loss: 126.7794\n",
      "Iteration: 95100\n",
      "Gradient: [ 5.42000e-02 -3.06100e+00  6.15400e-01  3.12020e+01 -6.94963e+01]\n",
      "Weights: [-4.4532 -0.4507 -0.48    0.0907  0.1047]\n",
      "MSE loss: 126.7503\n",
      "Iteration: 95200\n",
      "Gradient: [ -2.4162  -5.2116  23.3654  39.9717 -23.7828]\n",
      "Weights: [-4.4532 -0.4511 -0.4803  0.0908  0.1048]\n",
      "MSE loss: 126.7034\n",
      "Iteration: 95300\n",
      "Gradient: [  1.0879 -22.4173 -14.9037 -97.3967  25.3474]\n",
      "Weights: [-4.4553 -0.449  -0.4807  0.0908  0.1048]\n",
      "MSE loss: 126.6174\n",
      "Iteration: 95400\n",
      "Gradient: [  1.1723 -10.8945 -30.2578 -55.8485 114.1054]\n",
      "Weights: [-4.4552 -0.4491 -0.4811  0.0908  0.1049]\n",
      "MSE loss: 126.5764\n",
      "Iteration: 95500\n",
      "Gradient: [  -5.1403   25.4721   18.8683  -17.1346 -245.6031]\n",
      "Weights: [-4.4563 -0.4483 -0.4812  0.0907  0.1049]\n",
      "MSE loss: 126.5272\n",
      "Iteration: 95600\n",
      "Gradient: [  -1.0409    0.9433   68.2233   12.5668 -180.3918]\n",
      "Weights: [-4.4559 -0.448  -0.4819  0.0908  0.105 ]\n",
      "MSE loss: 126.4578\n",
      "Iteration: 95700\n",
      "Gradient: [  8.0155  15.1837  21.9861  70.4182 -88.6095]\n",
      "Weights: [-4.4565 -0.4473 -0.4826  0.0908  0.105 ]\n",
      "MSE loss: 126.3971\n",
      "Iteration: 95800\n",
      "Gradient: [   1.6987    2.0538   -4.574  -100.9558   59.1789]\n",
      "Weights: [-4.4546 -0.4471 -0.4828  0.0907  0.1051]\n",
      "MSE loss: 126.3504\n",
      "Iteration: 95900\n",
      "Gradient: [-4.080600e+00  3.957000e-01  1.583470e+01  7.525820e+01 -4.889783e+02]\n",
      "Weights: [-4.452  -0.4478 -0.4833  0.0907  0.1051]\n",
      "MSE loss: 126.3174\n",
      "Iteration: 96000\n",
      "Gradient: [ -10.8955    4.546   -11.2337 -100.4175  -95.0106]\n",
      "Weights: [-4.4582 -0.4458 -0.4835  0.0906  0.1052]\n",
      "MSE loss: 126.2538\n",
      "Iteration: 96100\n",
      "Gradient: [  2.3672  12.8173  -5.2414  58.3885 -19.1579]\n",
      "Weights: [-4.4584 -0.4446 -0.4842  0.0906  0.1052]\n",
      "MSE loss: 126.1858\n",
      "Iteration: 96200\n",
      "Gradient: [  15.7447    5.6864   42.1394   -4.0835 -145.1669]\n",
      "Weights: [-4.456  -0.4443 -0.4845  0.0906  0.1052]\n",
      "MSE loss: 126.1361\n",
      "Iteration: 96300\n",
      "Gradient: [-5.99450e+00  8.60000e-03  3.70445e+01  4.30652e+01 -3.66465e+01]\n",
      "Weights: [-4.4588 -0.4439 -0.4851  0.0907  0.1053]\n",
      "MSE loss: 126.0912\n",
      "Iteration: 96400\n",
      "Gradient: [  7.4674  -3.5729  67.951  -13.6298 -90.8123]\n",
      "Weights: [-4.4528 -0.4437 -0.4855  0.0906  0.1053]\n",
      "MSE loss: 126.0381\n",
      "Iteration: 96500\n",
      "Gradient: [ -11.6309    2.491    44.0148  -63.139  -338.8156]\n",
      "Weights: [-4.4564 -0.4424 -0.4859  0.0905  0.1054]\n",
      "MSE loss: 125.9713\n",
      "Iteration: 96600\n",
      "Gradient: [   3.278    12.2711   10.8404  -94.5572 -134.4721]\n",
      "Weights: [-4.4583 -0.4394 -0.4865  0.0904  0.1054]\n",
      "MSE loss: 125.8754\n",
      "Iteration: 96700\n",
      "Gradient: [ -18.7848  -11.6962   14.5365   43.593  -337.2394]\n",
      "Weights: [-4.4606 -0.4396 -0.4869  0.0905  0.1054]\n",
      "MSE loss: 125.8504\n",
      "Iteration: 96800\n",
      "Gradient: [   8.1492   -3.6284   -5.1826  -21.3497 -295.1393]\n",
      "Weights: [-4.4633 -0.4367 -0.4878  0.0904  0.1055]\n",
      "MSE loss: 125.7552\n",
      "Iteration: 96900\n",
      "Gradient: [  4.3057   1.514   13.1044 124.6405 132.112 ]\n",
      "Weights: [-4.4642 -0.4366 -0.4881  0.0904  0.1055]\n",
      "MSE loss: 125.7361\n",
      "Iteration: 97000\n",
      "Gradient: [  3.5142   1.6199  23.1999 -10.0539 114.3915]\n",
      "Weights: [-4.4581 -0.4348 -0.4886  0.0904  0.1055]\n",
      "MSE loss: 125.656\n",
      "Iteration: 97100\n",
      "Gradient: [  -9.5655   -5.328    21.8138   49.8627 -189.2974]\n",
      "Weights: [-4.4618 -0.434  -0.4893  0.0903  0.1056]\n",
      "MSE loss: 125.5596\n",
      "Iteration: 97200\n",
      "Gradient: [ 1.4149 14.6275 43.6901 25.4904 18.3224]\n",
      "Weights: [-4.4623 -0.4328 -0.4898  0.0903  0.1056]\n",
      "MSE loss: 125.5115\n",
      "Iteration: 97300\n",
      "Gradient: [ -2.8909   7.7411  62.058   -1.3558 -14.7836]\n",
      "Weights: [-4.4626 -0.4313 -0.4903  0.0903  0.1056]\n",
      "MSE loss: 125.4416\n",
      "Iteration: 97400\n",
      "Gradient: [  2.2934  30.6756  40.5887 -59.9106 277.311 ]\n",
      "Weights: [-4.4583 -0.4318 -0.4909  0.0902  0.1057]\n",
      "MSE loss: 125.3925\n",
      "Iteration: 97500\n",
      "Gradient: [-13.1036  -3.6989  19.8769 -30.4393 -62.1396]\n",
      "Weights: [-4.4593 -0.4306 -0.4914  0.0902  0.1057]\n",
      "MSE loss: 125.3332\n",
      "Iteration: 97600\n",
      "Gradient: [  2.1905  -8.7946 -11.3126 -65.3459 -50.479 ]\n",
      "Weights: [-4.4627 -0.4291 -0.4922  0.0902  0.1058]\n",
      "MSE loss: 125.2391\n",
      "Iteration: 97700\n",
      "Gradient: [  7.3788 -14.4106  21.624   -4.5249 -21.7834]\n",
      "Weights: [-4.4617 -0.428  -0.4926  0.0902  0.1058]\n",
      "MSE loss: 125.2\n",
      "Iteration: 97800\n",
      "Gradient: [ -4.4571   2.1024   4.4502 141.1158  99.6227]\n",
      "Weights: [-4.4642 -0.4263 -0.4933  0.0902  0.1058]\n",
      "MSE loss: 125.1282\n",
      "Iteration: 97900\n",
      "Gradient: [ -1.1195   7.2415  13.1574  -1.1224 241.8904]\n",
      "Weights: [-4.4659 -0.4244 -0.4934  0.0901  0.1058]\n",
      "MSE loss: 125.0853\n",
      "Iteration: 98000\n",
      "Gradient: [ -3.117    0.2398 -11.6846 -50.2753  96.3186]\n",
      "Weights: [-4.4657 -0.4232 -0.4941  0.0901  0.1059]\n",
      "MSE loss: 125.0077\n",
      "Iteration: 98100\n",
      "Gradient: [-16.2662   6.2749  25.4892  93.1076 284.7599]\n",
      "Weights: [-4.4645 -0.4224 -0.495   0.0899  0.1059]\n",
      "MSE loss: 124.9235\n",
      "Iteration: 98200\n",
      "Gradient: [  9.1548   2.5779 -16.7624  36.4105 -36.9545]\n",
      "Weights: [-4.4667 -0.4207 -0.4954  0.0899  0.1059]\n",
      "MSE loss: 124.8691\n",
      "Iteration: 98300\n",
      "Gradient: [  7.2257  12.0357  27.3395 -70.8596  -2.7418]\n",
      "Weights: [-4.4694 -0.4186 -0.4958  0.0898  0.106 ]\n",
      "MSE loss: 124.7947\n",
      "Iteration: 98400\n",
      "Gradient: [ -6.1946  -4.4092 -37.3217  75.2145 -15.9862]\n",
      "Weights: [-4.4647 -0.4191 -0.4966  0.0897  0.106 ]\n",
      "MSE loss: 124.7256\n",
      "Iteration: 98500\n",
      "Gradient: [  -4.8029   -0.7185    6.7443   25.0247 -140.2764]\n",
      "Weights: [-4.4688 -0.4175 -0.4972  0.0898  0.1061]\n",
      "MSE loss: 124.6602\n",
      "Iteration: 98600\n",
      "Gradient: [-10.1796  15.6519  -9.0322  28.2034 -14.2718]\n",
      "Weights: [-4.4655 -0.416  -0.4978  0.0898  0.1061]\n",
      "MSE loss: 124.5859\n",
      "Iteration: 98700\n",
      "Gradient: [ -2.5207  -1.7552 -23.8183 -29.9942 -76.99  ]\n",
      "Weights: [-4.4646 -0.4162 -0.4986  0.0898  0.1062]\n",
      "MSE loss: 124.5095\n",
      "Iteration: 98800\n",
      "Gradient: [   3.5065  -11.4445   -3.7492  -39.6555 -113.9907]\n",
      "Weights: [-4.4677 -0.4157 -0.4994  0.0899  0.1062]\n",
      "MSE loss: 124.4421\n",
      "Iteration: 98900\n",
      "Gradient: [  -6.5709   15.2455   35.4708  -12.3086 -133.2894]\n",
      "Weights: [-4.4649 -0.4148 -0.4999  0.0899  0.1063]\n",
      "MSE loss: 124.3793\n",
      "Iteration: 99000\n",
      "Gradient: [ -3.7678 -17.5633 -21.8534   8.5972 -67.5133]\n",
      "Weights: [-4.4648 -0.414  -0.5008  0.0899  0.1063]\n",
      "MSE loss: 124.305\n",
      "Iteration: 99100\n",
      "Gradient: [  -1.6582  -12.5378   -2.6453   89.0399 -177.3231]\n",
      "Weights: [-4.4688 -0.4123 -0.501   0.0898  0.1064]\n",
      "MSE loss: 124.2531\n",
      "Iteration: 99200\n",
      "Gradient: [   2.1173   -1.293   -57.3479 -156.4105  166.6318]\n",
      "Weights: [-4.4694 -0.4104 -0.5011  0.0897  0.1064]\n",
      "MSE loss: 124.2057\n",
      "Iteration: 99300\n",
      "Gradient: [ -3.2416 -23.3659  13.28   -34.3109 -53.198 ]\n",
      "Weights: [-4.4686 -0.4084 -0.5016  0.0897  0.1064]\n",
      "MSE loss: 124.16\n",
      "Iteration: 99400\n",
      "Gradient: [ -3.3927  28.766  -35.4276  79.0618 185.2157]\n",
      "Weights: [-4.4731 -0.408  -0.5022  0.0897  0.1065]\n",
      "MSE loss: 124.0972\n",
      "Iteration: 99500\n",
      "Gradient: [-12.7621 -27.5181  20.2242 -91.0124 -62.8137]\n",
      "Weights: [-4.4739 -0.4074 -0.5028  0.0897  0.1065]\n",
      "MSE loss: 124.0451\n",
      "Iteration: 99600\n",
      "Gradient: [ -10.2677   -6.4559   16.6149  -43.7634 -235.7128]\n",
      "Weights: [-4.4719 -0.4081 -0.5032  0.0897  0.1066]\n",
      "MSE loss: 123.9897\n",
      "Iteration: 99700\n",
      "Gradient: [   4.4615    5.3407   56.5925    2.519  -507.2675]\n",
      "Weights: [-4.4735 -0.4082 -0.5033  0.0897  0.1066]\n",
      "MSE loss: 123.9704\n",
      "Iteration: 99800\n",
      "Gradient: [  -0.2012   15.7402  -24.6715   26.2594 -121.5707]\n",
      "Weights: [-4.4682 -0.408  -0.5037  0.0896  0.1066]\n",
      "MSE loss: 123.9066\n",
      "Iteration: 99900\n",
      "Gradient: [-10.6669  -4.0517   7.3007 -11.7785  -7.9863]\n",
      "Weights: [-4.4652 -0.4083 -0.5044  0.0896  0.1067]\n",
      "MSE loss: 123.8579\n"
     ]
    }
   ],
   "source": [
    "# Отключен Momentum.\n",
    "# Обучение на полном датасете.\n",
    "'''\n",
    "weights_1, losses_1, iter_final_1, fit_time_1 = model_fit(X_train, y_train,\n",
    "                                                          learning_rate=5*[1e-7],\n",
    "                                                          tolerance=(0.1**2 * N_points),\n",
    "                                                          beta=0,\n",
    "                                                          batch_ratio=1.0,\n",
    "                                                          lr_scaling=False,\n",
    "                                                          liveplot=False)\n",
    "'''\n",
    "weights_1, losses_1, iter_final_1, fit_time_1 = model_fit(X_train, y_train,\n",
    "                                                          learning_rate=[1e-3, 1e-4, 1e-5, 1e-6, 1e-7],\n",
    "                                                          tolerance=(0.1**2 * N_points),\n",
    "                                                          beta=0.8,\n",
    "                                                          batch_ratio=0.1,\n",
    "                                                          lr_scaling=True,\n",
    "                                                          liveplot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91fc4314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Gradient: [ 3400.8052  4570.6796  7765.9205 15435.8184 30668.746 ]\n",
      "Weights: [-0.0002 -0.     -0.     -0.     -0.    ]\n",
      "MSE loss: 40785.2674\n",
      "Iteration: 100\n",
      "Gradient: [   -8.7985    59.1038    36.3973  -304.8356 -2858.9992]\n",
      "Weights: [-4.7679 -0.7461  0.1252  0.0871  0.04  ]\n",
      "MSE loss: 435.23\n",
      "Iteration: 200\n",
      "Gradient: [ 18.6524  69.7329 157.7635 312.1449 217.6594]\n",
      "Weights: [-4.4697 -1.0794  0.0994  0.1066  0.0506]\n",
      "MSE loss: 280.3761\n",
      "Iteration: 300\n",
      "Gradient: [-19.0364 -28.8112 -40.9804  -7.6327  46.2884]\n",
      "Weights: [-4.3384 -1.2305  0.0681  0.1142  0.0568]\n",
      "MSE loss: 235.0955\n",
      "Iteration: 400\n",
      "Gradient: [  -5.2188   14.2047   57.7758  -29.6317 -316.2252]\n",
      "Weights: [-4.2446 -1.2839  0.0423  0.1179  0.0612]\n",
      "MSE loss: 218.2936\n",
      "Iteration: 500\n",
      "Gradient: [   2.3743    2.8088  -57.5475 -143.5241  -72.5136]\n",
      "Weights: [-4.2114 -1.3099  0.0223  0.1191  0.0643]\n",
      "MSE loss: 210.8083\n",
      "Iteration: 600\n",
      "Gradient: [   7.2547    3.933   -13.1975   82.296  -772.141 ]\n",
      "Weights: [-4.2078 -1.3107  0.0074  0.1187  0.0661]\n",
      "MSE loss: 207.5414\n",
      "Iteration: 700\n",
      "Gradient: [-1.572000e-01 -3.363400e+00  5.537360e+01 -1.474310e+01 -3.989111e+02]\n",
      "Weights: [-4.1929 -1.286  -0.0084  0.118   0.0678]\n",
      "MSE loss: 203.7735\n",
      "Iteration: 800\n",
      "Gradient: [   7.3106    5.2438  -51.7755   65.8592 -588.4166]\n",
      "Weights: [-4.2008 -1.2507 -0.0322  0.1156  0.0696]\n",
      "MSE loss: 199.0507\n",
      "Iteration: 900\n",
      "Gradient: [  19.5575  -30.6605  -63.3635  103.2762 -320.5881]\n",
      "Weights: [-4.2308 -1.213  -0.0507  0.1156  0.0715]\n",
      "MSE loss: 194.7007\n",
      "Iteration: 1000\n",
      "Gradient: [ -13.529   -17.6443  -82.7597 -309.1775 -384.5348]\n",
      "Weights: [-4.2049 -1.2094 -0.0677  0.1154  0.0723]\n",
      "MSE loss: 193.2025\n",
      "Iteration: 1100\n",
      "Gradient: [   4.756   -32.9673   14.344  -143.195  -537.6941]\n",
      "Weights: [-4.2393 -1.1659 -0.0817  0.114   0.0739]\n",
      "MSE loss: 188.7123\n",
      "Iteration: 1200\n",
      "Gradient: [ -2.0475   7.2703  62.2347 240.5627  27.8411]\n",
      "Weights: [-4.257  -1.1199 -0.1027  0.1112  0.0758]\n",
      "MSE loss: 184.3446\n",
      "Iteration: 1300\n",
      "Gradient: [-14.588   15.9073  10.009   -8.3816 133.9372]\n",
      "Weights: [-4.2692 -1.0843 -0.1233  0.1113  0.0771]\n",
      "MSE loss: 180.8371\n",
      "Iteration: 1400\n",
      "Gradient: [  -8.7811  -28.3618    9.3807 -159.4072 -529.1619]\n",
      "Weights: [-4.2805 -1.0621 -0.1414  0.1097  0.0786]\n",
      "MSE loss: 178.5436\n",
      "Iteration: 1500\n",
      "Gradient: [  3.8819 -11.1141 -12.4961  48.6012 -44.1033]\n",
      "Weights: [-4.2795 -1.0189 -0.1624  0.1097  0.0801]\n",
      "MSE loss: 174.0041\n",
      "Iteration: 1600\n",
      "Gradient: [  -5.0414    1.7351  -32.6927  -43.1982 -349.7997]\n",
      "Weights: [-4.2879 -0.9957 -0.1762  0.1076  0.0814]\n",
      "MSE loss: 171.3251\n",
      "Iteration: 1700\n",
      "Gradient: [  6.1928  42.7902  31.9854 244.2459 828.4661]\n",
      "Weights: [-4.2924 -0.9509 -0.1952  0.1062  0.0826]\n",
      "MSE loss: 168.3896\n",
      "Iteration: 1800\n",
      "Gradient: [  7.4048  23.4143  24.2746 -29.6024 -76.957 ]\n",
      "Weights: [-4.2886 -0.9332 -0.2115  0.1055  0.0842]\n",
      "MSE loss: 165.7095\n",
      "Iteration: 1900\n",
      "Gradient: [  3.1523 -18.7901  48.2424 195.4651 400.5341]\n",
      "Weights: [-4.3075 -0.9106 -0.225   0.1051  0.0854]\n",
      "MSE loss: 162.9808\n",
      "Iteration: 2000\n",
      "Gradient: [   3.9454   19.0278   26.7771  -59.5056 -254.2408]\n",
      "Weights: [-4.3286 -0.87   -0.2426  0.1034  0.0865]\n",
      "MSE loss: 159.9657\n",
      "Iteration: 2100\n",
      "Gradient: [  -4.4111  -30.0848   58.6072  -13.512  -281.5994]\n",
      "Weights: [-4.3523 -0.8368 -0.2591  0.1025  0.0878]\n",
      "MSE loss: 157.3928\n",
      "Iteration: 2200\n",
      "Gradient: [-6.522900e+00  2.680000e-02 -5.054880e+01 -9.530470e+01 -5.258701e+02]\n",
      "Weights: [-4.3403 -0.8204 -0.2725  0.1018  0.0891]\n",
      "MSE loss: 155.1638\n",
      "Iteration: 2300\n",
      "Gradient: [ 14.3817  43.2544 103.1521 155.7308 391.9386]\n",
      "Weights: [-4.3305 -0.7913 -0.2885  0.1007  0.0904]\n",
      "MSE loss: 153.6326\n",
      "Iteration: 2400\n",
      "Gradient: [ 2.445000e-01 -4.127900e+00 -5.050040e+01  2.633470e+01 -3.223833e+02]\n",
      "Weights: [-4.354  -0.776  -0.3062  0.0999  0.0923]\n",
      "MSE loss: 149.9705\n",
      "Iteration: 2500\n",
      "Gradient: [  -3.5223  -32.6295   56.2965  -80.3816 -123.6106]\n",
      "Weights: [-4.3739 -0.7478 -0.3194  0.0992  0.0934]\n",
      "MSE loss: 148.0668\n",
      "Iteration: 2600\n",
      "Gradient: [  23.2754   21.9894  -25.4802 -166.271  -159.0307]\n",
      "Weights: [-4.365  -0.717  -0.3303  0.0969  0.0942]\n",
      "MSE loss: 146.136\n",
      "Iteration: 2700\n",
      "Gradient: [  -1.8404  -23.2056   23.5681   40.8568 -191.3715]\n",
      "Weights: [-4.3821 -0.6785 -0.3462  0.0951  0.0952]\n",
      "MSE loss: 143.674\n",
      "Iteration: 2800\n",
      "Gradient: [ 18.5099 -26.3387  -0.6508 142.5185 -84.3217]\n",
      "Weights: [-4.396  -0.6523 -0.3551  0.094   0.0961]\n",
      "MSE loss: 142.5123\n",
      "Iteration: 2900\n",
      "Gradient: [  -2.7849   -1.1118   -8.143    15.3782 -154.4512]\n",
      "Weights: [-4.4149 -0.6326 -0.3691  0.0921  0.0976]\n",
      "MSE loss: 139.968\n",
      "Iteration: 3000\n",
      "Gradient: [   2.9826   17.3884  -11.9292  -36.4004 -417.1429]\n",
      "Weights: [-4.4006 -0.6284 -0.3758  0.0913  0.0982]\n",
      "MSE loss: 138.9971\n",
      "Iteration: 3100\n",
      "Gradient: [ 16.6369  -2.3223   7.3351 -61.8797 122.1116]\n",
      "Weights: [-4.3931 -0.6159 -0.3862  0.0916  0.0992]\n",
      "MSE loss: 137.8036\n",
      "Iteration: 3200\n",
      "Gradient: [ 19.5954   1.1209  -1.6527 214.4114 309.353 ]\n",
      "Weights: [-4.4077 -0.5933 -0.3961  0.0913  0.0998]\n",
      "MSE loss: 136.4384\n",
      "Iteration: 3300\n",
      "Gradient: [-6.046000e-01 -6.289970e+01  7.472800e+00 -1.330198e+02 -8.067155e+02]\n",
      "Weights: [-4.4119 -0.5816 -0.4086  0.0906  0.1003]\n",
      "MSE loss: 135.6305\n",
      "Iteration: 3400\n",
      "Gradient: [  0.8716 -29.4672  68.3476 200.3261  40.2013]\n",
      "Weights: [-4.4097 -0.5587 -0.4178  0.0903  0.1014]\n",
      "MSE loss: 133.8125\n",
      "Iteration: 3500\n",
      "Gradient: [-14.6757 -33.4554  57.7756 -85.6663 -33.5386]\n",
      "Weights: [-4.4268 -0.5379 -0.4291  0.0896  0.1025]\n",
      "MSE loss: 132.0831\n",
      "Iteration: 3600\n",
      "Gradient: [ 27.9015  32.1387   9.0785 211.3881 442.7675]\n",
      "Weights: [-4.4196 -0.5224 -0.4391  0.089   0.1035]\n",
      "MSE loss: 131.3916\n",
      "Iteration: 3700\n",
      "Gradient: [  -0.9976  -17.0386   63.2026 -130.4634 -398.4337]\n",
      "Weights: [-4.4301 -0.4999 -0.4542  0.0882  0.1045]\n",
      "MSE loss: 128.9954\n",
      "Iteration: 3800\n",
      "Gradient: [  -1.6491   17.6708   50.0191  -20.9903 -188.3378]\n",
      "Weights: [-4.4464 -0.4646 -0.4682  0.0863  0.1055]\n",
      "MSE loss: 127.0907\n",
      "Iteration: 3900\n",
      "Gradient: [  -3.6286    7.3938   19.4427 -106.2807 -465.1798]\n",
      "Weights: [-4.4658 -0.4343 -0.4806  0.0849  0.1064]\n",
      "MSE loss: 125.4744\n",
      "Iteration: 4000\n",
      "Gradient: [ 4.1011  4.9149 -9.0145  6.9203 98.8818]\n",
      "Weights: [-4.461  -0.4186 -0.4925  0.0846  0.1074]\n",
      "MSE loss: 124.1615\n",
      "Iteration: 4100\n",
      "Gradient: [  -1.8519  -11.9312  -22.9984  -79.4557 -248.0443]\n",
      "Weights: [-4.4809 -0.3976 -0.505   0.084   0.1083]\n",
      "MSE loss: 122.9876\n",
      "Iteration: 4200\n",
      "Gradient: [ 12.1056  -4.6549  45.5919 -95.5845 -65.9892]\n",
      "Weights: [-4.4586 -0.389  -0.5104  0.0831  0.1089]\n",
      "MSE loss: 122.357\n",
      "Iteration: 4300\n",
      "Gradient: [-1.943390e+01  7.663900e+00  4.115000e-01  1.607561e+02 -5.605856e+02]\n",
      "Weights: [-4.4725 -0.3709 -0.5227  0.0825  0.1099]\n",
      "MSE loss: 120.7843\n",
      "Iteration: 4400\n",
      "Gradient: [  10.9749   20.6506   -6.2528   -4.7866 -273.8089]\n",
      "Weights: [-4.4745 -0.362  -0.5293  0.0817  0.1109]\n",
      "MSE loss: 119.8473\n",
      "Iteration: 4500\n",
      "Gradient: [ -3.2194  -5.351   59.821   10.9838 526.5866]\n",
      "Weights: [-4.4839 -0.3373 -0.5374  0.0806  0.1117]\n",
      "MSE loss: 118.9886\n",
      "Iteration: 4600\n",
      "Gradient: [  0.8027   8.7648  33.0319  25.4829 225.4761]\n",
      "Weights: [-4.4955 -0.3236 -0.5489  0.0815  0.1122]\n",
      "MSE loss: 117.8568\n",
      "Iteration: 4700\n",
      "Gradient: [ -22.2984  -21.1121   -5.4469   21.215  -285.7417]\n",
      "Weights: [-4.5243 -0.2903 -0.5593  0.0797  0.1125]\n",
      "MSE loss: 117.3799\n",
      "Iteration: 4800\n",
      "Gradient: [ 23.4737 -11.225   53.8329  10.5107 -62.3926]\n",
      "Weights: [-4.4977 -0.2804 -0.5684  0.08    0.1132]\n",
      "MSE loss: 116.0405\n",
      "Iteration: 4900\n",
      "Gradient: [  -7.4446   -8.6625  -29.5358  -71.06   -212.0455]\n",
      "Weights: [-4.5198 -0.2655 -0.5785  0.0807  0.1138]\n",
      "MSE loss: 115.0967\n",
      "Iteration: 5000\n",
      "Gradient: [   6.9401    5.2327  -28.4157    8.7406 -392.3094]\n",
      "Weights: [-4.5112 -0.2488 -0.588   0.08    0.1141]\n",
      "MSE loss: 114.3978\n",
      "Iteration: 5100\n",
      "Gradient: [  -1.8045   17.6047   28.3736   59.5006 -170.7314]\n",
      "Weights: [-4.5132 -0.2269 -0.5987  0.0795  0.1153]\n",
      "MSE loss: 113.3663\n",
      "Iteration: 5200\n",
      "Gradient: [  0.9024  28.4096 -26.0783  38.1197 534.7041]\n",
      "Weights: [-4.5401 -0.2036 -0.6049  0.0774  0.1159]\n",
      "MSE loss: 112.221\n",
      "Iteration: 5300\n",
      "Gradient: [14.5611 20.6391 65.0929 44.5454 53.33  ]\n",
      "Weights: [-4.5195 -0.2117 -0.6069  0.0775  0.1166]\n",
      "MSE loss: 112.3144\n",
      "Iteration: 5400\n",
      "Gradient: [ 16.6729  -7.8918  41.3242 101.3942 -98.489 ]\n",
      "Weights: [-4.5393 -0.2001 -0.6103  0.0752  0.1173]\n",
      "MSE loss: 111.3715\n",
      "Iteration: 5500\n",
      "Gradient: [ -20.4586  -10.898  -110.6531 -155.6554 -152.1981]\n",
      "Weights: [-4.575  -0.1676 -0.6192  0.0744  0.1173]\n",
      "MSE loss: 111.6821\n",
      "Iteration: 5600\n",
      "Gradient: [ -9.8208 -16.9772  14.5706 -51.3329  29.846 ]\n",
      "Weights: [-4.5514 -0.1553 -0.6292  0.0747  0.1182]\n",
      "MSE loss: 109.7894\n",
      "Iteration: 5700\n",
      "Gradient: [  -9.6247  -13.8147  -54.6861   27.6675 -346.4943]\n",
      "Weights: [-4.5714 -0.1436 -0.6392  0.074   0.1193]\n",
      "MSE loss: 109.1717\n",
      "Iteration: 5800\n",
      "Gradient: [ -16.5057  -38.5446  -15.8472 -160.8773 -317.2343]\n",
      "Weights: [-4.5741 -0.1312 -0.6447  0.0732  0.1198]\n",
      "MSE loss: 108.5347\n",
      "Iteration: 5900\n",
      "Gradient: [  12.4776   23.4361   19.5995  182.1183 -201.7887]\n",
      "Weights: [-4.5559 -0.1248 -0.651   0.0723  0.1205]\n",
      "MSE loss: 107.6119\n",
      "Iteration: 6000\n",
      "Gradient: [ 18.8027  -7.36   -72.1018 158.3488 323.9074]\n",
      "Weights: [-4.5522 -0.1171 -0.6558  0.0714  0.1211]\n",
      "MSE loss: 107.1695\n",
      "Iteration: 6100\n",
      "Gradient: [ 20.5576   4.045   55.8317  20.6013 -42.2477]\n",
      "Weights: [-4.5524 -0.1032 -0.6576  0.0707  0.1216]\n",
      "MSE loss: 108.1547\n",
      "Iteration: 6200\n",
      "Gradient: [  8.5842  15.6885  11.4968  85.6738 272.7431]\n",
      "Weights: [-4.558  -0.1022 -0.6643  0.0698  0.1224]\n",
      "MSE loss: 106.2882\n",
      "Iteration: 6300\n",
      "Gradient: [  3.3386 -16.0872  46.0812  -9.3922  40.9355]\n",
      "Weights: [-4.5722 -0.0846 -0.6716  0.0684  0.1233]\n",
      "MSE loss: 105.4771\n",
      "Iteration: 6400\n",
      "Gradient: [ -6.8398 -22.503   -6.6594 -66.7746 -71.9109]\n",
      "Weights: [-4.5665 -0.0737 -0.68    0.0671  0.1239]\n",
      "MSE loss: 104.7932\n",
      "Iteration: 6500\n",
      "Gradient: [ -16.2672  -17.2933  -85.0564 -173.996  -100.4771]\n",
      "Weights: [-4.5762 -0.0749 -0.683   0.0654  0.1246]\n",
      "MSE loss: 105.5307\n",
      "Iteration: 6600\n",
      "Gradient: [  5.1743  -3.733   -1.54   101.9384 334.801 ]\n",
      "Weights: [-4.5827 -0.0536 -0.6833  0.0651  0.1248]\n",
      "MSE loss: 104.2552\n",
      "Iteration: 6700\n",
      "Gradient: [ -4.0638   8.2333  20.5433 -42.7849 220.8359]\n",
      "Weights: [-4.5857 -0.0376 -0.6925  0.0638  0.1257]\n",
      "MSE loss: 103.4303\n",
      "Iteration: 6800\n",
      "Gradient: [ -16.7045  -47.3941  -40.1671 -203.7752 -253.0786]\n",
      "Weights: [-4.5906 -0.0236 -0.7017  0.0629  0.1263]\n",
      "MSE loss: 102.7123\n",
      "Iteration: 6900\n",
      "Gradient: [-7.764000e-01 -3.852490e+01 -9.144450e+01 -6.794300e+01 -8.206554e+02]\n",
      "Weights: [-4.6005 -0.0099 -0.7103  0.0622  0.1268]\n",
      "MSE loss: 102.7769\n",
      "Iteration: 7000\n",
      "Gradient: [-18.8152 -13.1044  21.0587  14.005  117.1188]\n",
      "Weights: [-4.5983e+00 -3.0000e-04 -7.1690e-01  6.2400e-02  1.2760e-01]\n",
      "MSE loss: 101.6063\n",
      "Iteration: 7100\n",
      "Gradient: [  2.2117  -1.3631  20.474  -50.5904 -38.2427]\n",
      "Weights: [-4.5902e+00  3.8000e-03 -7.2400e-01  6.3000e-02  1.2810e-01]\n",
      "MSE loss: 101.2008\n",
      "Iteration: 7200\n",
      "Gradient: [ -9.3687   7.9256  21.0012  78.4763 204.6861]\n",
      "Weights: [-4.5917  0.0261 -0.7322  0.0625  0.1284]\n",
      "MSE loss: 100.781\n",
      "Iteration: 7300\n",
      "Gradient: [ 11.4694   9.1545  32.689   20.2931 -55.289 ]\n",
      "Weights: [-4.5934  0.0276 -0.7356  0.0628  0.1288]\n",
      "MSE loss: 100.5239\n",
      "Iteration: 7400\n",
      "Gradient: [  8.932   12.1549 -22.5711  58.4114 -58.7651]\n",
      "Weights: [-4.6025  0.0424 -0.7415  0.0616  0.1295]\n",
      "MSE loss: 99.9691\n",
      "Iteration: 7500\n",
      "Gradient: [ 29.0581  20.4639   1.6083 163.6149 730.6596]\n",
      "Weights: [-4.5801  0.0517 -0.7485  0.0614  0.1299]\n",
      "MSE loss: 100.9203\n",
      "Iteration: 7600\n",
      "Gradient: [ 8.6485 -3.9842 80.262  89.5087 34.5383]\n",
      "Weights: [-4.6159  0.0791 -0.7572  0.0613  0.1301]\n",
      "MSE loss: 99.0744\n",
      "Iteration: 7700\n",
      "Gradient: [  -5.6385    8.5576   -8.8965   54.218  -438.9343]\n",
      "Weights: [-4.6169  0.0815 -0.7636  0.0612  0.131 ]\n",
      "MSE loss: 98.5905\n",
      "Iteration: 7800\n",
      "Gradient: [  6.5141  -1.7235 -25.7204 -80.707  -77.1414]\n",
      "Weights: [-4.6199  0.0895 -0.7671  0.0594  0.1313]\n",
      "MSE loss: 98.2291\n",
      "Iteration: 7900\n",
      "Gradient: [-9.1554  5.9047 38.7595 10.9718 40.5019]\n",
      "Weights: [-4.6285  0.0959 -0.77    0.0583  0.1318]\n",
      "MSE loss: 98.1416\n",
      "Iteration: 8000\n",
      "Gradient: [  0.8968  -3.1379   3.974  198.77   126.8726]\n",
      "Weights: [-4.6152  0.1145 -0.7758  0.0578  0.1323]\n",
      "MSE loss: 98.1746\n",
      "Iteration: 8100\n",
      "Gradient: [  3.2673  -4.5149   1.6662 -35.8828 194.1564]\n",
      "Weights: [-4.6453  0.1286 -0.7826  0.057   0.1329]\n",
      "MSE loss: 97.1343\n",
      "Iteration: 8200\n",
      "Gradient: [  -5.0508  -17.8605  -40.0974   96.3903 -171.0487]\n",
      "Weights: [-4.6513  0.1499 -0.79    0.0562  0.1329]\n",
      "MSE loss: 96.9517\n",
      "Iteration: 8300\n",
      "Gradient: [ 19.8554  31.1803  -6.824   95.1851 430.7751]\n",
      "Weights: [-4.6299  0.1466 -0.7928  0.0571  0.1335]\n",
      "MSE loss: 96.9805\n",
      "Iteration: 8400\n",
      "Gradient: [ 9.9711  7.1039 33.4905 48.7455 13.4141]\n",
      "Weights: [-4.6278  0.1388 -0.7959  0.0579  0.1339]\n",
      "MSE loss: 96.5376\n",
      "Iteration: 8500\n",
      "Gradient: [ 12.4145 -15.4445  17.0848 128.0621 -95.6937]\n",
      "Weights: [-4.6314  0.1428 -0.7981  0.0575  0.1341]\n",
      "MSE loss: 96.3105\n",
      "Iteration: 8600\n",
      "Gradient: [ -5.9949  18.1216  -7.383   82.0057 114.9777]\n",
      "Weights: [-4.6234  0.143  -0.8011  0.0568  0.1346]\n",
      "MSE loss: 96.2135\n",
      "Iteration: 8700\n",
      "Gradient: [ -12.6889    5.5254  -43.7193 -143.0465   88.6504]\n",
      "Weights: [-4.6501  0.1648 -0.808   0.0566  0.1346]\n",
      "MSE loss: 95.856\n",
      "Iteration: 8800\n",
      "Gradient: [ -7.6435  21.8207  29.1459 130.2191 365.0087]\n",
      "Weights: [-4.6407  0.1739 -0.81    0.056   0.1352]\n",
      "MSE loss: 95.8937\n",
      "Iteration: 8900\n",
      "Gradient: [  12.1056  -23.8716   16.8145   29.8645 -605.2988]\n",
      "Weights: [-4.6446  0.178  -0.8138  0.0553  0.1352]\n",
      "MSE loss: 95.3939\n",
      "Iteration: 9000\n",
      "Gradient: [2.800000e-02 1.470160e+01 5.106780e+01 1.846391e+02 4.956481e+02]\n",
      "Weights: [-4.6432  0.1871 -0.8138  0.0542  0.1356]\n",
      "MSE loss: 95.5995\n",
      "Iteration: 9100\n",
      "Gradient: [ 15.9417  42.625   67.923  109.8091 353.8834]\n",
      "Weights: [-4.6603  0.2064 -0.8193  0.0537  0.1358]\n",
      "MSE loss: 95.0888\n",
      "Iteration: 9200\n",
      "Gradient: [  2.8572 -22.6374  35.3403 -65.407   40.1199]\n",
      "Weights: [-4.658   0.2154 -0.8291  0.0536  0.1364]\n",
      "MSE loss: 94.5142\n",
      "Iteration: 9300\n",
      "Gradient: [ -9.2834  37.1864  -1.6284 -11.1257 180.1555]\n",
      "Weights: [-4.6734  0.2322 -0.8366  0.0534  0.1367]\n",
      "MSE loss: 94.1964\n",
      "Iteration: 9400\n",
      "Gradient: [ -13.5545  -11.197   -43.2788 -111.8189 -133.8375]\n",
      "Weights: [-4.6859  0.2397 -0.8368  0.0524  0.1372]\n",
      "MSE loss: 94.0772\n",
      "Iteration: 9500\n",
      "Gradient: [ -3.1763   3.2847   4.5261 -45.3733  39.3909]\n",
      "Weights: [-4.692   0.2536 -0.8439  0.0525  0.1372]\n",
      "MSE loss: 94.1248\n",
      "Iteration: 9600\n",
      "Gradient: [ 11.4213  10.5819  67.9022 -92.6815 174.1125]\n",
      "Weights: [-4.6746  0.2545 -0.8462  0.0523  0.1378]\n",
      "MSE loss: 93.7618\n",
      "Iteration: 9700\n",
      "Gradient: [ -16.7836  -38.1257  -81.8525  -79.1406 -350.4413]\n",
      "Weights: [-4.6814  0.2437 -0.8487  0.0524  0.1384]\n",
      "MSE loss: 93.6215\n",
      "Iteration: 9800\n",
      "Gradient: [  18.6911   -7.9912   50.0804 -112.3856 -356.328 ]\n",
      "Weights: [-4.6794  0.265  -0.8543  0.051   0.1386]\n",
      "MSE loss: 93.1719\n",
      "Iteration: 9900\n",
      "Gradient: [  2.9109   7.7082  -3.1028 178.571  566.6296]\n",
      "Weights: [-4.6908  0.2682 -0.8584  0.0522  0.139 ]\n",
      "MSE loss: 93.0501\n",
      "Iteration: 10000\n",
      "Gradient: [  -9.3076  -25.905   -78.5135 -106.0273 -157.5829]\n",
      "Weights: [-4.6813  0.2703 -0.8628  0.0517  0.1392]\n",
      "MSE loss: 92.8816\n",
      "Iteration: 10100\n",
      "Gradient: [   7.8763   10.792    -0.8704  163.3868 -242.9167]\n",
      "Weights: [-4.6884  0.2948 -0.8666  0.0507  0.1392]\n",
      "MSE loss: 92.7585\n",
      "Iteration: 10200\n",
      "Gradient: [   0.2834   32.479    10.4106   91.1367 -115.429 ]\n",
      "Weights: [-4.705   0.3087 -0.8742  0.0519  0.1393]\n",
      "MSE loss: 92.4958\n",
      "Iteration: 10300\n",
      "Gradient: [  14.0496  -11.1368   12.6985   27.0816 -173.2816]\n",
      "Weights: [-4.7035  0.3184 -0.8829  0.0525  0.1397]\n",
      "MSE loss: 92.1315\n",
      "Iteration: 10400\n",
      "Gradient: [   9.5257    1.7518   44.315     9.0116 -424.462 ]\n",
      "Weights: [-4.7002  0.3194 -0.8889  0.0531  0.1404]\n",
      "MSE loss: 91.8175\n",
      "Iteration: 10500\n",
      "Gradient: [ -14.3767  -38.7674  -90.2133 -355.4454 -652.624 ]\n",
      "Weights: [-4.713   0.3211 -0.8944  0.0527  0.1406]\n",
      "MSE loss: 93.1944\n",
      "Iteration: 10600\n",
      "Gradient: [ 16.9973  17.4423 -28.6326 -39.8911 -55.6431]\n",
      "Weights: [-4.6868  0.3217 -0.8943  0.0531  0.141 ]\n",
      "MSE loss: 91.7599\n",
      "Iteration: 10700\n",
      "Gradient: [  13.0454   -7.0417  -53.724   -62.1565 -359.0834]\n",
      "Weights: [-4.6893  0.3203 -0.8967  0.0532  0.1411]\n",
      "MSE loss: 91.5282\n",
      "Iteration: 10800\n",
      "Gradient: [ -0.6769  16.5796 -17.1148  -5.2054 267.2768]\n",
      "Weights: [-4.677   0.3281 -0.8998  0.0522  0.1414]\n",
      "MSE loss: 91.7969\n",
      "Iteration: 10900\n",
      "Gradient: [  0.7002 -16.1545   1.3213 -37.3886  94.8452]\n",
      "Weights: [-4.7053  0.3408 -0.9048  0.0527  0.1418]\n",
      "MSE loss: 91.1896\n",
      "Iteration: 11000\n",
      "Gradient: [ -5.1231  12.8496  14.1152  48.3414 -44.1437]\n",
      "Weights: [-4.7092  0.3551 -0.9069  0.052   0.1418]\n",
      "MSE loss: 91.1627\n",
      "Iteration: 11100\n",
      "Gradient: [ 10.1174   6.562  122.3418 304.8409 395.6237]\n",
      "Weights: [-4.7029  0.3623 -0.9112  0.0515  0.1422]\n",
      "MSE loss: 91.2318\n",
      "Iteration: 11200\n",
      "Gradient: [  -7.5063   18.3971  -37.108  -114.8735 -199.0719]\n",
      "Weights: [-4.7074  0.3552 -0.9123  0.0515  0.1423]\n",
      "MSE loss: 90.9403\n",
      "Iteration: 11300\n",
      "Gradient: [-11.6201  -7.1782 -27.3212 -72.815  220.715 ]\n",
      "Weights: [-4.6996  0.3558 -0.916   0.0519  0.1425]\n",
      "MSE loss: 90.8325\n",
      "Iteration: 11400\n",
      "Gradient: [ -6.6917   2.1031  20.2689 223.2855  43.9101]\n",
      "Weights: [-4.7006  0.3535 -0.9148  0.0519  0.1429]\n",
      "MSE loss: 90.8515\n",
      "Iteration: 11500\n",
      "Gradient: [  7.4991  34.6978  32.0891 273.7886 127.0468]\n",
      "Weights: [-4.6822  0.362  -0.9216  0.0518  0.1433]\n",
      "MSE loss: 91.4613\n",
      "Iteration: 11600\n",
      "Gradient: [ 20.151  -16.8919  19.5472 -53.7167 123.5703]\n",
      "Weights: [-4.705   0.379  -0.9277  0.0513  0.1434]\n",
      "MSE loss: 90.3944\n",
      "Iteration: 11700\n",
      "Gradient: [ 1.49300e-01  4.90300e-01 -1.06195e+01  4.85214e+01 -5.95972e+02]\n",
      "Weights: [-4.7159  0.3899 -0.9311  0.0516  0.1435]\n",
      "MSE loss: 90.2649\n",
      "Iteration: 11800\n",
      "Gradient: [ -21.1917  -17.0153  -41.357  -318.5497 -279.0064]\n",
      "Weights: [-4.7122  0.3861 -0.9357  0.052   0.1438]\n",
      "MSE loss: 90.2597\n",
      "Iteration: 11900\n",
      "Gradient: [ 10.8497  17.1071  67.2991 216.9671 174.0783]\n",
      "Weights: [-4.698   0.401  -0.9433  0.0537  0.1439]\n",
      "MSE loss: 90.8208\n",
      "Iteration: 12000\n",
      "Gradient: [  -7.8042   -3.8188  -59.872   -36.8493 -106.7522]\n",
      "Weights: [-4.7272  0.4135 -0.9467  0.0528  0.144 ]\n",
      "MSE loss: 89.8635\n",
      "Iteration: 12100\n",
      "Gradient: [  2.8132   5.5586  54.244   46.2635 -20.8469]\n",
      "Weights: [-4.7052  0.4019 -0.946   0.0525  0.1445]\n",
      "MSE loss: 89.9959\n",
      "Iteration: 12200\n",
      "Gradient: [  -0.5438    5.4186   11.7197   39.5228 -326.527 ]\n",
      "Weights: [-4.7269  0.4148 -0.9471  0.0521  0.1445]\n",
      "MSE loss: 89.7763\n",
      "Iteration: 12300\n",
      "Gradient: [ 13.5089 -21.0733  64.2401  61.7195 442.3884]\n",
      "Weights: [-4.7272  0.4298 -0.9509  0.0515  0.1447]\n",
      "MSE loss: 89.929\n",
      "Iteration: 12400\n",
      "Gradient: [-16.9771  15.9619  26.5958 -55.8996 109.4712]\n",
      "Weights: [-4.7413  0.4379 -0.9577  0.0528  0.1447]\n",
      "MSE loss: 89.5591\n",
      "Iteration: 12500\n",
      "Gradient: [ 3.6371 -5.9923 43.6223 36.504  72.4645]\n",
      "Weights: [-4.7214  0.4367 -0.9595  0.0527  0.1447]\n",
      "MSE loss: 89.6256\n",
      "Iteration: 12600\n",
      "Gradient: [   1.7756  -18.1838  -82.5048  -53.6004 -106.2924]\n",
      "Weights: [-4.7257  0.435  -0.9638  0.0526  0.1452]\n",
      "MSE loss: 89.3947\n",
      "Iteration: 12700\n",
      "Gradient: [-1.300000e-02  9.423200e+00 -7.555200e+00  6.881400e+00  1.098315e+02]\n",
      "Weights: [-4.7258  0.4422 -0.9628  0.0521  0.1455]\n",
      "MSE loss: 89.6365\n",
      "Iteration: 12800\n",
      "Gradient: [  -4.4516   -1.71    -63.7146   43.7573 -217.2223]\n",
      "Weights: [-4.729   0.4285 -0.9652  0.0524  0.1459]\n",
      "MSE loss: 89.544\n",
      "Iteration: 12900\n",
      "Gradient: [  1.9094   5.9948  11.7194 109.0626 253.4574]\n",
      "Weights: [-4.7194  0.429  -0.967   0.0519  0.1463]\n",
      "MSE loss: 89.3375\n",
      "Iteration: 13000\n",
      "Gradient: [  -3.7495  -12.0974  -27.5418    4.1582 -214.1447]\n",
      "Weights: [-4.7268  0.44   -0.971   0.0521  0.1464]\n",
      "MSE loss: 89.1767\n",
      "Iteration: 13100\n",
      "Gradient: [  1.1263  -9.9061 -28.5345  48.7617 -37.8226]\n",
      "Weights: [-4.722   0.4375 -0.9741  0.0524  0.1466]\n",
      "MSE loss: 89.2531\n",
      "Iteration: 13200\n",
      "Gradient: [ 12.6492   5.8813  12.583  -59.3057  58.3588]\n",
      "Weights: [-4.7049  0.4429 -0.9763  0.0524  0.1467]\n",
      "MSE loss: 89.4944\n",
      "Iteration: 13300\n",
      "Gradient: [-11.4511  16.9268  32.392   31.6219  90.4463]\n",
      "Weights: [-4.7245  0.4579 -0.9798  0.0519  0.1467]\n",
      "MSE loss: 88.9509\n",
      "Iteration: 13400\n",
      "Gradient: [ 19.2158  24.4155  17.8044 199.3577 391.7323]\n",
      "Weights: [-4.7199  0.4807 -0.9851  0.0519  0.1466]\n",
      "MSE loss: 89.9763\n",
      "Iteration: 13500\n",
      "Gradient: [   6.6071   24.3791  -12.1531  -61.8402 -230.0788]\n",
      "Weights: [-4.7324  0.4763 -0.9849  0.051   0.1471]\n",
      "MSE loss: 88.8493\n",
      "Iteration: 13600\n",
      "Gradient: [  1.9384  -7.1433  96.0397 -10.0667 226.0772]\n",
      "Weights: [-4.735   0.4855 -0.9886  0.0512  0.1474]\n",
      "MSE loss: 89.0547\n",
      "Iteration: 13700\n",
      "Gradient: [  2.7496 -17.3558  35.3149  32.3405 104.9609]\n",
      "Weights: [-4.7234  0.4707 -0.9917  0.0526  0.1476]\n",
      "MSE loss: 88.7857\n",
      "Iteration: 13800\n",
      "Gradient: [ -10.9836  -38.3267  -34.8136 -166.8107 -346.8329]\n",
      "Weights: [-4.7398  0.4803 -0.9933  0.0518  0.1474]\n",
      "MSE loss: 88.8362\n",
      "Iteration: 13900\n",
      "Gradient: [  -3.7535   -5.3854   13.2803  108.1254 -305.0418]\n",
      "Weights: [-4.7426  0.4882 -0.9966  0.0517  0.1476]\n",
      "MSE loss: 88.6273\n",
      "Iteration: 14000\n",
      "Gradient: [  2.0731   9.7098  23.0939 301.3757 437.289 ]\n",
      "Weights: [-4.7695  0.5168 -0.9987  0.0503  0.1479]\n",
      "MSE loss: 88.532\n",
      "Iteration: 14100\n",
      "Gradient: [ 0.7368 -9.6224 40.364  68.4122 93.1007]\n",
      "Weights: [-4.7427  0.5021 -0.9998  0.0503  0.1482]\n",
      "MSE loss: 88.4127\n",
      "Iteration: 14200\n",
      "Gradient: [  -3.4298  -13.2211  -20.9603   23.1827 -209.6444]\n",
      "Weights: [-4.7475  0.5094 -1.0035  0.05    0.1485]\n",
      "MSE loss: 88.2917\n",
      "Iteration: 14300\n",
      "Gradient: [-11.8729 -23.5325 -63.1936 -72.9797  -3.8434]\n",
      "Weights: [-4.7702  0.5094 -1.0033  0.0504  0.1484]\n",
      "MSE loss: 88.992\n",
      "Iteration: 14400\n",
      "Gradient: [  6.6112  23.1548  34.1285  38.8133 136.6762]\n",
      "Weights: [-4.7594  0.5212 -1.0057  0.0505  0.1483]\n",
      "MSE loss: 88.2654\n",
      "Iteration: 14500\n",
      "Gradient: [ 25.0454  52.1117  33.5026 177.6385 124.3675]\n",
      "Weights: [-4.7563  0.5303 -1.005   0.0505  0.1482]\n",
      "MSE loss: 89.154\n",
      "Iteration: 14600\n",
      "Gradient: [ -0.7553  -9.8222  24.2551 -90.9377 453.6916]\n",
      "Weights: [-4.7678  0.5153 -1.0055  0.0513  0.1484]\n",
      "MSE loss: 88.4566\n",
      "Iteration: 14700\n",
      "Gradient: [ -10.3573   -1.1155  -53.0648    9.2758 -118.0025]\n",
      "Weights: [-4.7712  0.5205 -1.0083  0.052   0.1484]\n",
      "MSE loss: 88.4523\n",
      "Iteration: 14800\n",
      "Gradient: [ 14.5102  21.9277  75.118   32.5259 352.9424]\n",
      "Weights: [-4.7451  0.5298 -1.0134  0.0519  0.1485]\n",
      "MSE loss: 88.8604\n",
      "Iteration: 14900\n",
      "Gradient: [ 17.4699  22.9009  44.565  109.1395 217.3301]\n",
      "Weights: [-4.7728  0.5352 -1.0156  0.0511  0.149 ]\n",
      "MSE loss: 88.1944\n",
      "Iteration: 15000\n",
      "Gradient: [  18.9919   18.6447    4.2224 -122.4395  205.6492]\n",
      "Weights: [-4.7487  0.5391 -1.0226  0.0513  0.1495]\n",
      "MSE loss: 88.3021\n",
      "Iteration: 15100\n",
      "Gradient: [  6.2845  -9.1441  18.8036  81.324  196.3948]\n",
      "Weights: [-4.7433  0.5416 -1.0273  0.0514  0.1494]\n",
      "MSE loss: 88.1086\n",
      "Iteration: 15200\n",
      "Gradient: [ 17.9174  24.1041  43.2555 -23.8823 380.5181]\n",
      "Weights: [-4.7482  0.5474 -1.0295  0.0519  0.1496]\n",
      "MSE loss: 88.0754\n",
      "Iteration: 15300\n",
      "Gradient: [ 14.6673  19.6885  19.747   46.8494 220.0797]\n",
      "Weights: [-4.7604  0.5613 -1.0316  0.0516  0.1495]\n",
      "MSE loss: 88.0068\n",
      "Iteration: 15400\n",
      "Gradient: [ -15.8432   11.4171    4.4261 -109.4182 -523.3971]\n",
      "Weights: [-4.7868  0.574  -1.0351  0.051   0.1494]\n",
      "MSE loss: 88.2221\n",
      "Iteration: 15500\n",
      "Gradient: [ 16.6752  25.7491 114.123   96.5514  65.4008]\n",
      "Weights: [-4.749   0.5621 -1.0351  0.0528  0.1497]\n",
      "MSE loss: 88.8153\n",
      "Iteration: 15600\n",
      "Gradient: [ -0.9249  11.6419  23.7709 137.5755 254.4688]\n",
      "Weights: [-4.7738  0.5634 -1.0339  0.0524  0.1496]\n",
      "MSE loss: 87.7717\n",
      "Iteration: 15700\n",
      "Gradient: [ -3.5161 -13.2774 -31.1721 114.4057 -58.2669]\n",
      "Weights: [-4.7795  0.5604 -1.0363  0.053   0.1499]\n",
      "MSE loss: 87.9336\n",
      "Iteration: 15800\n",
      "Gradient: [  -1.1045   26.2154    2.6633   28.2603 -448.2475]\n",
      "Weights: [-4.7614  0.5574 -1.0382  0.0532  0.1499]\n",
      "MSE loss: 87.7153\n",
      "Iteration: 15900\n",
      "Gradient: [-2.49751e+01 -5.64400e+00  3.29900e-01 -3.78657e+01 -6.24896e+02]\n",
      "Weights: [-4.7753  0.5658 -1.0416  0.0536  0.1499]\n",
      "MSE loss: 87.78\n",
      "Iteration: 16000\n",
      "Gradient: [  6.5994  24.8331  57.4309  56.0809 216.911 ]\n",
      "Weights: [-4.7679  0.5714 -1.0445  0.0544  0.1502]\n",
      "MSE loss: 87.9643\n",
      "Iteration: 16100\n",
      "Gradient: [ -3.938   17.0457 -15.0817 351.048  276.1449]\n",
      "Weights: [-4.7749  0.5852 -1.0448  0.0524  0.1502]\n",
      "MSE loss: 87.7836\n",
      "Iteration: 16200\n",
      "Gradient: [  7.5695  10.8099  22.9814  14.3326 137.124 ]\n",
      "Weights: [-4.7686  0.5763 -1.043   0.0521  0.1501]\n",
      "MSE loss: 87.6282\n",
      "Iteration: 16300\n",
      "Gradient: [14.4705 -7.8318 31.0779 39.0212 80.7291]\n",
      "Weights: [-4.7633  0.5791 -1.0425  0.0518  0.1502]\n",
      "MSE loss: 87.9431\n",
      "Iteration: 16400\n",
      "Gradient: [  -8.8477   -0.3469   -6.0816  -39.4586 -164.8236]\n",
      "Weights: [-4.7845  0.5801 -1.0433  0.0519  0.1503]\n",
      "MSE loss: 87.6973\n",
      "Iteration: 16500\n",
      "Gradient: [ -3.7626 -15.4698 -59.0224  -5.9994  82.734 ]\n",
      "Weights: [-4.7839  0.5795 -1.0452  0.0514  0.1504]\n",
      "MSE loss: 88.0258\n",
      "Iteration: 16600\n",
      "Gradient: [ 13.7879  21.2581  57.7321  23.1199 -41.6198]\n",
      "Weights: [-4.7573  0.5811 -1.0483  0.0516  0.1507]\n",
      "MSE loss: 87.8433\n",
      "Iteration: 16700\n",
      "Gradient: [ 12.9643  25.3203  54.2783 135.2045 641.6848]\n",
      "Weights: [-4.7693  0.5853 -1.0481  0.0523  0.1507]\n",
      "MSE loss: 87.7815\n",
      "Iteration: 16800\n",
      "Gradient: [ -16.4753  -27.168    62.2378 -128.6873  -67.9006]\n",
      "Weights: [-4.7803  0.5801 -1.0458  0.0523  0.1504]\n",
      "MSE loss: 87.6212\n",
      "Iteration: 16900\n",
      "Gradient: [  4.0187   4.6634  32.0398 -87.3991 244.0484]\n",
      "Weights: [-4.7743  0.5926 -1.0503  0.0522  0.1504]\n",
      "MSE loss: 87.5635\n",
      "Iteration: 17000\n",
      "Gradient: [ -4.6293   5.7761  21.3597 207.5945 139.4569]\n",
      "Weights: [-4.78    0.5889 -1.0509  0.0536  0.1503]\n",
      "MSE loss: 87.5146\n",
      "Iteration: 17100\n",
      "Gradient: [-4.328300e+00  1.660000e-01  4.480380e+01 -2.242750e+01 -1.758172e+02]\n",
      "Weights: [-4.7708  0.5797 -1.0499  0.0539  0.1503]\n",
      "MSE loss: 87.526\n",
      "Iteration: 17200\n",
      "Gradient: [   1.0158    7.6483   32.3273 -158.2775 -382.4821]\n",
      "Weights: [-4.7638  0.5854 -1.0531  0.0532  0.1506]\n",
      "MSE loss: 87.5738\n",
      "Iteration: 17300\n",
      "Gradient: [ 17.8816  13.9343 -13.4873  54.6968 328.2886]\n",
      "Weights: [-4.7771  0.6045 -1.0558  0.0528  0.1504]\n",
      "MSE loss: 87.6005\n",
      "Iteration: 17400\n",
      "Gradient: [ -1.9525  -2.1916 -12.104  -10.5922 190.9768]\n",
      "Weights: [-4.8048  0.6147 -1.0591  0.0525  0.1507]\n",
      "MSE loss: 87.7062\n",
      "Iteration: 17500\n",
      "Gradient: [ 16.4396 -14.0914  32.1593 -35.5686   8.3562]\n",
      "Weights: [-4.7717  0.604  -1.0595  0.0528  0.1507]\n",
      "MSE loss: 87.5146\n",
      "Iteration: 17600\n",
      "Gradient: [ -0.561    5.2512   5.25   106.4527 336.669 ]\n",
      "Weights: [-4.7711  0.6024 -1.0585  0.0526  0.151 ]\n",
      "MSE loss: 87.6737\n",
      "Iteration: 17700\n",
      "Gradient: [ -13.8026  -27.1129   33.853    17.6381 -139.3408]\n",
      "Weights: [-4.7905  0.599  -1.0578  0.0519  0.1513]\n",
      "MSE loss: 87.6172\n",
      "Iteration: 17800\n",
      "Gradient: [  7.9938  31.6599 -41.7359  40.9799 -14.8034]\n",
      "Weights: [-4.782   0.6043 -1.0601  0.0516  0.1513]\n",
      "MSE loss: 87.3703\n",
      "Iteration: 17900\n",
      "Gradient: [-11.7446  12.0758  27.1753  48.1168   3.1442]\n",
      "Weights: [-4.7827  0.6098 -1.0637  0.0522  0.1512]\n",
      "MSE loss: 87.3923\n",
      "Iteration: 18000\n",
      "Gradient: [  2.4628   6.4963  11.3872 -80.7366 -64.2656]\n",
      "Weights: [-4.773   0.6155 -1.0676  0.0526  0.1515]\n",
      "MSE loss: 87.4483\n",
      "Iteration: 18100\n",
      "Gradient: [   5.282   -22.7204  -41.3605  -38.8938 -206.8247]\n",
      "Weights: [-4.7888  0.6278 -1.0724  0.0529  0.1516]\n",
      "MSE loss: 87.221\n",
      "Iteration: 18200\n",
      "Gradient: [   6.0632  -28.4476  -40.4109  -43.3501 -358.6679]\n",
      "Weights: [-4.7928  0.6336 -1.0768  0.0528  0.152 ]\n",
      "MSE loss: 87.1684\n",
      "Iteration: 18300\n",
      "Gradient: [ 11.2318  10.9171  78.64   203.3237 349.2882]\n",
      "Weights: [-4.7886  0.6361 -1.0763  0.0528  0.152 ]\n",
      "MSE loss: 87.3417\n",
      "Iteration: 18400\n",
      "Gradient: [ -7.1811  -2.4773 -70.6259 143.3624 117.4668]\n",
      "Weights: [-4.8013  0.6296 -1.079   0.0533  0.1521]\n",
      "MSE loss: 87.6787\n",
      "Iteration: 18500\n",
      "Gradient: [  12.6439   -3.9121  -49.4903 -134.5825  -71.1212]\n",
      "Weights: [-4.7945  0.6436 -1.0835  0.0541  0.152 ]\n",
      "MSE loss: 87.102\n",
      "Iteration: 18600\n",
      "Gradient: [  8.9379  13.5709 -15.9526 -74.8681 267.6202]\n",
      "Weights: [-4.8034  0.6579 -1.0861  0.0542  0.1518]\n",
      "MSE loss: 87.161\n",
      "Iteration: 18700\n",
      "Gradient: [ 21.97    15.3476  92.2361  91.8929 244.5898]\n",
      "Weights: [-4.7813  0.6433 -1.0846  0.0555  0.1519]\n",
      "MSE loss: 87.8112\n",
      "Iteration: 18800\n",
      "Gradient: [ 23.1092  33.1312  42.8648 132.7682 124.5644]\n",
      "Weights: [-4.79    0.6472 -1.0865  0.056   0.1517]\n",
      "MSE loss: 87.3098\n",
      "Iteration: 18900\n",
      "Gradient: [ -9.1915  -7.82    -8.117  -93.5188 -22.8382]\n",
      "Weights: [-4.8034  0.649  -1.0869  0.0554  0.1517]\n",
      "MSE loss: 87.1817\n",
      "Iteration: 19000\n",
      "Gradient: [   3.0572   -5.2998   17.0772 -144.8475  151.6555]\n",
      "Weights: [-4.7931  0.6439 -1.0858  0.0557  0.1516]\n",
      "MSE loss: 87.0976\n",
      "Iteration: 19100\n",
      "Gradient: [  12.542   -20.2148   29.9674  -73.4686 -119.3439]\n",
      "Weights: [-4.7886  0.6314 -1.0855  0.0565  0.1516]\n",
      "MSE loss: 87.2264\n",
      "Iteration: 19200\n",
      "Gradient: [  0.382   17.0756  24.7532  -4.3124 299.9185]\n",
      "Weights: [-4.7948  0.6511 -1.0891  0.0566  0.1515]\n",
      "MSE loss: 87.0959\n",
      "Iteration: 19300\n",
      "Gradient: [ -6.7733 -33.417  -73.9817 -63.7232 -66.644 ]\n",
      "Weights: [-4.7956  0.6405 -1.0942  0.0574  0.152 ]\n",
      "MSE loss: 87.5295\n",
      "Iteration: 19400\n",
      "Gradient: [-0.4915 30.321  46.9094 22.1266 19.9706]\n",
      "Weights: [-4.7896  0.6515 -1.0948  0.0577  0.1518]\n",
      "MSE loss: 87.0504\n",
      "Iteration: 19500\n",
      "Gradient: [  5.9444  18.8359  42.5654 107.7598 -44.4371]\n",
      "Weights: [-4.7914  0.6559 -1.0961  0.0574  0.1518]\n",
      "MSE loss: 87.0132\n",
      "Iteration: 19600\n",
      "Gradient: [ -6.6323 -19.8545  29.3554 -36.3524  18.844 ]\n",
      "Weights: [-4.7854  0.6438 -1.098   0.059   0.152 ]\n",
      "MSE loss: 87.0096\n",
      "Iteration: 19700\n",
      "Gradient: [   2.8058    0.917   -27.9787   41.4611 -126.9315]\n",
      "Weights: [-4.7834  0.6392 -1.0957  0.0595  0.1515]\n",
      "MSE loss: 87.0837\n",
      "Iteration: 19800\n",
      "Gradient: [-17.2102   7.6779 -55.5585 -25.227  244.9162]\n",
      "Weights: [-4.7958  0.6464 -1.0967  0.0595  0.1515]\n",
      "MSE loss: 87.1147\n",
      "Iteration: 19900\n",
      "Gradient: [  -6.7207    0.356    32.6045  -86.655  -216.91  ]\n",
      "Weights: [-4.7882  0.648  -1.1007  0.0601  0.1517]\n",
      "MSE loss: 86.9932\n",
      "Iteration: 20000\n",
      "Gradient: [ 12.06     1.5518   5.5709 -13.2381  10.286 ]\n",
      "Weights: [-4.7751  0.6517 -1.1055  0.0604  0.1521]\n",
      "MSE loss: 87.158\n",
      "Iteration: 20100\n",
      "Gradient: [  0.6051  17.8433  27.4118 -27.8574 259.4262]\n",
      "Weights: [-4.796   0.6624 -1.1065  0.0605  0.1522]\n",
      "MSE loss: 87.0147\n",
      "Iteration: 20200\n",
      "Gradient: [  -4.4709  -12.6316   20.2754  -45.4178 -351.5291]\n",
      "Weights: [-4.8103  0.6662 -1.1058  0.0596  0.152 ]\n",
      "MSE loss: 87.1725\n",
      "Iteration: 20300\n",
      "Gradient: [  15.0194    5.0832   40.7532 -114.9807  298.706 ]\n",
      "Weights: [-4.7856  0.6562 -1.1034  0.0597  0.1518]\n",
      "MSE loss: 86.9731\n",
      "Iteration: 20400\n",
      "Gradient: [ 10.726    4.0528  19.0571  45.986  105.3718]\n",
      "Weights: [-4.8059  0.6544 -1.0981  0.0592  0.1519]\n",
      "MSE loss: 87.1419\n",
      "Iteration: 20500\n",
      "Gradient: [  -2.7436    3.4048  -16.3376   51.181  -427.5859]\n",
      "Weights: [-4.7982  0.657  -1.099   0.059   0.1515]\n",
      "MSE loss: 87.0324\n",
      "Iteration: 20600\n",
      "Gradient: [-1.065410e+01 -3.035080e+01 -6.052940e+01 -2.120000e-02 -3.174049e+02]\n",
      "Weights: [-4.7946  0.6467 -1.0997  0.0602  0.1517]\n",
      "MSE loss: 87.0766\n",
      "Iteration: 20700\n",
      "Gradient: [  1.735   -1.0153 -26.3431 -12.4871 -99.9385]\n",
      "Weights: [-4.7926  0.6452 -1.102   0.0599  0.1521]\n",
      "MSE loss: 87.203\n",
      "Iteration: 20800\n",
      "Gradient: [ -1.4756 -18.5819  -7.9565 -45.6685 192.6556]\n",
      "Weights: [-4.7913  0.6493 -1.0997  0.0589  0.1519]\n",
      "MSE loss: 87.065\n",
      "Iteration: 20900\n",
      "Gradient: [  -2.8257   10.2285   16.663  -113.8971  516.5172]\n",
      "Weights: [-4.7914  0.6422 -1.0993  0.0601  0.1519]\n",
      "MSE loss: 87.1235\n",
      "Iteration: 21000\n",
      "Gradient: [  -9.3601   -8.2839    0.7563 -145.3862  -93.768 ]\n",
      "Weights: [-4.7924  0.6402 -1.0986  0.0594  0.1521]\n",
      "MSE loss: 87.2597\n",
      "Iteration: 21100\n",
      "Gradient: [ -16.0491   -7.9612   31.459  -118.0499   93.436 ]\n",
      "Weights: [-4.7902  0.6472 -1.0998  0.0594  0.1517]\n",
      "MSE loss: 87.1122\n",
      "Iteration: 21200\n",
      "Gradient: [   2.718    -4.4288   -7.7126   -6.4821 -137.2506]\n",
      "Weights: [-4.7988  0.6493 -1.099   0.0606  0.1515]\n",
      "MSE loss: 87.0606\n",
      "Iteration: 21300\n",
      "Gradient: [ -11.3779  -11.4431  -79.9998 -145.8623 -444.2889]\n",
      "Weights: [-4.8073  0.6478 -1.1012  0.0611  0.1513]\n",
      "MSE loss: 88.0429\n",
      "Iteration: 21400\n",
      "Gradient: [-11.7213  13.8428  -0.2044 -10.3686 201.6887]\n",
      "Weights: [-4.8027  0.6594 -1.1008  0.0605  0.1513]\n",
      "MSE loss: 86.9894\n",
      "Iteration: 21500\n",
      "Gradient: [  -0.8799  -25.4501   21.9861   21.3206 -263.0743]\n",
      "Weights: [-4.7972  0.6472 -1.0984  0.061   0.1515]\n",
      "MSE loss: 87.0648\n",
      "Iteration: 21600\n",
      "Gradient: [ 12.3767   7.5343  12.0457 100.6571  47.9979]\n",
      "Weights: [-4.7895  0.6611 -1.1043  0.0605  0.1516]\n",
      "MSE loss: 86.9984\n",
      "Iteration: 21700\n",
      "Gradient: [-6.761900e+00 -6.650000e-02  3.313850e+01  1.260110e+02 -2.068743e+02]\n",
      "Weights: [-4.7907  0.6642 -1.1047  0.0599  0.1516]\n",
      "MSE loss: 86.9711\n",
      "Iteration: 21800\n",
      "Gradient: [ -4.3967  11.7156  24.1739 -80.1996 -63.259 ]\n",
      "Weights: [-4.8102  0.6676 -1.1056  0.0605  0.1516]\n",
      "MSE loss: 87.0653\n",
      "Iteration: 21900\n",
      "Gradient: [-27.3197 -10.3613 -90.626  -38.4176 118.9473]\n",
      "Weights: [-4.8209  0.6676 -1.104   0.0592  0.1516]\n",
      "MSE loss: 87.9614\n",
      "Iteration: 22000\n",
      "Gradient: [  9.0207  12.9959  55.1798 127.811  106.2965]\n",
      "Weights: [-4.7839  0.6677 -1.1055  0.0602  0.1518]\n",
      "MSE loss: 87.6431\n",
      "Iteration: 22100\n",
      "Gradient: [  0.1402  13.827   24.9712 124.9071 101.5827]\n",
      "Weights: [-4.783   0.6578 -1.1095  0.0603  0.1525]\n",
      "MSE loss: 86.9759\n",
      "Iteration: 22200\n",
      "Gradient: [ -16.0959    3.8286   26.3181 -136.6197 -443.8253]\n",
      "Weights: [-4.7937  0.6578 -1.1108  0.0607  0.1522]\n",
      "MSE loss: 87.2564\n",
      "Iteration: 22300\n",
      "Gradient: [ -12.5894  -24.5109   -3.2089 -167.4307 -315.1144]\n",
      "Weights: [-4.7984  0.6663 -1.1075  0.0593  0.1521]\n",
      "MSE loss: 86.9578\n",
      "Iteration: 22400\n",
      "Gradient: [  -3.3195   -6.9503  -69.7485   -9.6662 -203.9307]\n",
      "Weights: [-4.8116  0.6642 -1.1058  0.0588  0.1521]\n",
      "MSE loss: 87.6343\n",
      "Iteration: 22500\n",
      "Gradient: [  3.4835 -10.2354  15.5295  51.7585 -17.1114]\n",
      "Weights: [-4.7842  0.6586 -1.1053  0.0586  0.1524]\n",
      "MSE loss: 86.9774\n",
      "Iteration: 22600\n",
      "Gradient: [  -7.6926  -17.7524   11.8739   30.4877 -210.2144]\n",
      "Weights: [-4.8032  0.6727 -1.1088  0.0584  0.1524]\n",
      "MSE loss: 86.9142\n",
      "Iteration: 22700\n",
      "Gradient: [  2.3573  -6.5099 -30.8053 -14.5246 298.5612]\n",
      "Weights: [-4.8052  0.6871 -1.1154  0.0585  0.1525]\n",
      "MSE loss: 86.8945\n",
      "Iteration: 22800\n",
      "Gradient: [  -8.0926  -12.1974  -21.9962 -167.764  -227.3886]\n",
      "Weights: [-4.812   0.6811 -1.1167  0.0587  0.1529]\n",
      "MSE loss: 87.227\n",
      "Iteration: 22900\n",
      "Gradient: [ -21.0498   -1.0083  -26.4849  -80.2408 -249.2797]\n",
      "Weights: [-4.81    0.6849 -1.1171  0.0583  0.1529]\n",
      "MSE loss: 87.0746\n",
      "Iteration: 23000\n",
      "Gradient: [  -7.2901    2.6883    1.2082  -81.0729 -334.3046]\n",
      "Weights: [-4.807   0.7061 -1.1239  0.0584  0.1529]\n",
      "MSE loss: 86.8624\n",
      "Iteration: 23100\n",
      "Gradient: [ -20.715   -20.1249  -38.3431 -130.3225  -54.9469]\n",
      "Weights: [-4.8185  0.7009 -1.1222  0.0579  0.1532]\n",
      "MSE loss: 86.9381\n",
      "Iteration: 23200\n",
      "Gradient: [  10.4742   10.8721   55.7451 -167.2352  188.0159]\n",
      "Weights: [-4.8262  0.7095 -1.1203  0.0585  0.1529]\n",
      "MSE loss: 87.0296\n",
      "Iteration: 23300\n",
      "Gradient: [  -6.7878  -19.2357  -42.1189   16.7499 -230.7502]\n",
      "Weights: [-4.8315  0.6932 -1.1171  0.0586  0.1529]\n",
      "MSE loss: 87.5053\n",
      "Iteration: 23400\n",
      "Gradient: [ 11.1762   6.1675 -41.3555  -9.5892 399.9364]\n",
      "Weights: [-4.7967  0.6925 -1.1171  0.0582  0.1527]\n",
      "MSE loss: 87.0489\n",
      "Iteration: 23500\n",
      "Gradient: [  7.9932  30.5269  17.5068 122.6145 502.4172]\n",
      "Weights: [-4.803   0.6912 -1.1139  0.0572  0.1528]\n",
      "MSE loss: 86.9488\n",
      "Iteration: 23600\n",
      "Gradient: [   8.8168  -40.5275  -27.5048 -138.6404  114.0622]\n",
      "Weights: [-4.8164  0.684  -1.1123  0.0576  0.1529]\n",
      "MSE loss: 87.0212\n",
      "Iteration: 23700\n",
      "Gradient: [ -21.3063  -34.5328  -52.0105 -164.4304 -382.5631]\n",
      "Weights: [-4.8012  0.6684 -1.1103  0.057   0.1532]\n",
      "MSE loss: 87.0805\n",
      "Iteration: 23800\n",
      "Gradient: [  -7.3825   -5.0701  -10.2296 -206.8171   44.4406]\n",
      "Weights: [-4.8049  0.6787 -1.113   0.058   0.1529]\n",
      "MSE loss: 86.8832\n",
      "Iteration: 23900\n",
      "Gradient: [ -3.8472  26.998    4.1576  61.1298 448.6746]\n",
      "Weights: [-4.8106  0.6834 -1.1145  0.0578  0.153 ]\n",
      "MSE loss: 86.9429\n",
      "Iteration: 24000\n",
      "Gradient: [  7.8432 -22.7155  30.9044 -62.5939  83.029 ]\n",
      "Weights: [-4.812   0.6921 -1.1187  0.0585  0.153 ]\n",
      "MSE loss: 86.8482\n",
      "Iteration: 24100\n",
      "Gradient: [-1.024510e+01  1.995600e+01  1.075000e-01  3.813720e+01 -2.311975e+02]\n",
      "Weights: [-4.8107  0.6965 -1.1215  0.0589  0.153 ]\n",
      "MSE loss: 86.8009\n",
      "Iteration: 24200\n",
      "Gradient: [-21.5679  -6.7774 -67.812  -75.2641 185.1345]\n",
      "Weights: [-4.808   0.6884 -1.1217  0.0607  0.1525]\n",
      "MSE loss: 86.8912\n",
      "Iteration: 24300\n",
      "Gradient: [  -1.8177   -3.4352   -9.4391  -85.402  -283.9855]\n",
      "Weights: [-4.8171  0.6911 -1.1163  0.0609  0.1519]\n",
      "MSE loss: 86.9116\n",
      "Iteration: 24400\n",
      "Gradient: [   6.8278    8.2884    3.623   117.8505 -156.4447]\n",
      "Weights: [-4.8171  0.7052 -1.1213  0.0603  0.1522]\n",
      "MSE loss: 86.8832\n",
      "Iteration: 24500\n",
      "Gradient: [  3.0292   6.9711 -78.514    0.8551 225.5793]\n",
      "Weights: [-4.8015  0.6995 -1.1267  0.0614  0.1525]\n",
      "MSE loss: 86.8095\n",
      "Iteration: 24600\n",
      "Gradient: [ -0.3983  12.1665 -37.2348 -27.1077 -51.4358]\n",
      "Weights: [-4.8059  0.698  -1.1293  0.0622  0.1525]\n",
      "MSE loss: 86.7988\n",
      "Iteration: 24700\n",
      "Gradient: [ 10.4557   2.3786 100.3354  49.6926 305.5876]\n",
      "Weights: [-4.8056  0.7103 -1.1335  0.0621  0.1527]\n",
      "MSE loss: 86.7636\n",
      "Iteration: 24800\n",
      "Gradient: [ -5.9039  -6.9136 -29.2714 -66.1121 -32.3401]\n",
      "Weights: [-4.8058  0.7059 -1.1386  0.0638  0.153 ]\n",
      "MSE loss: 86.6898\n",
      "Iteration: 24900\n",
      "Gradient: [  -3.6354   -8.624    17.6989 -145.4045 -207.1727]\n",
      "Weights: [-4.8232  0.7178 -1.1428  0.065   0.1529]\n",
      "MSE loss: 86.8049\n",
      "Iteration: 25000\n",
      "Gradient: [  7.5521   3.0329  36.2188  82.4353 526.1688]\n",
      "Weights: [-4.7909  0.6989 -1.1393  0.0655  0.1528]\n",
      "MSE loss: 86.937\n",
      "Iteration: 25100\n",
      "Gradient: [ 20.0809 -10.8287  38.2208  74.0261 106.4738]\n",
      "Weights: [-4.7886  0.7085 -1.1423  0.0654  0.1526]\n",
      "MSE loss: 87.0447\n",
      "Iteration: 25200\n",
      "Gradient: [-16.1812 -20.8333  -2.8382  -8.6596 -29.9461]\n",
      "Weights: [-4.812   0.706  -1.1416  0.0652  0.1528]\n",
      "MSE loss: 86.9111\n",
      "Iteration: 25300\n",
      "Gradient: [   2.8062   34.2575   11.803  -102.4793  -26.1527]\n",
      "Weights: [-4.7981  0.7031 -1.1393  0.0649  0.1526]\n",
      "MSE loss: 86.7155\n",
      "Iteration: 25400\n",
      "Gradient: [ -16.1881  -26.3927   -6.7133 -131.4991  107.6321]\n",
      "Weights: [-4.8158  0.7176 -1.1424  0.0651  0.1524]\n",
      "MSE loss: 86.748\n",
      "Iteration: 25500\n",
      "Gradient: [  2.2549  20.0067  62.7014  13.1428 -68.1159]\n",
      "Weights: [-4.8014  0.718  -1.1408  0.0646  0.1525]\n",
      "MSE loss: 86.9578\n",
      "Iteration: 25600\n",
      "Gradient: [   9.6726   -5.7147  -11.9093  153.368  -252.4815]\n",
      "Weights: [-4.7993  0.7055 -1.1403  0.0655  0.1525]\n",
      "MSE loss: 86.6971\n",
      "Iteration: 25700\n",
      "Gradient: [  -4.9668  -32.6632  -64.1428   21.434  -346.8339]\n",
      "Weights: [-4.8047  0.7175 -1.1468  0.0655  0.1527]\n",
      "MSE loss: 86.7146\n",
      "Iteration: 25800\n",
      "Gradient: [   7.9269    9.1156  -14.9875   69.5724 -243.1209]\n",
      "Weights: [-4.8066  0.7206 -1.148   0.0653  0.1529]\n",
      "MSE loss: 86.6445\n",
      "Iteration: 25900\n",
      "Gradient: [   6.4997   29.1532   45.855  -122.4901  345.7634]\n",
      "Weights: [-4.8174  0.7388 -1.1495  0.0645  0.1531]\n",
      "MSE loss: 86.851\n",
      "Iteration: 26000\n",
      "Gradient: [ 14.2004   9.2593  11.0378 100.1371 -10.1162]\n",
      "Weights: [-4.8152  0.7422 -1.1533  0.0657  0.153 ]\n",
      "MSE loss: 86.9644\n",
      "Iteration: 26100\n",
      "Gradient: [  -1.201     8.9084   -0.8148  -40.3571 -114.7465]\n",
      "Weights: [-4.8146  0.7327 -1.1551  0.0649  0.1535]\n",
      "MSE loss: 86.6782\n",
      "Iteration: 26200\n",
      "Gradient: [  -3.4959  -21.1785  -43.4188  -32.3121 -129.5552]\n",
      "Weights: [-4.8178  0.7326 -1.1546  0.0654  0.1535]\n",
      "MSE loss: 86.6209\n",
      "Iteration: 26300\n",
      "Gradient: [   9.768    -7.0821   22.1056   93.9442 -112.2553]\n",
      "Weights: [-4.8056  0.735  -1.1564  0.0664  0.153 ]\n",
      "MSE loss: 86.6979\n",
      "Iteration: 26400\n",
      "Gradient: [  -7.099     4.1651   37.0709   16.0188 -305.6392]\n",
      "Weights: [-4.8179  0.7411 -1.1555  0.0669  0.1529]\n",
      "MSE loss: 86.7523\n",
      "Iteration: 26500\n",
      "Gradient: [  -9.752    -8.584   -36.2608  -65.9266 -163.3889]\n",
      "Weights: [-4.8293  0.7397 -1.1548  0.0658  0.1528]\n",
      "MSE loss: 86.9033\n",
      "Iteration: 26600\n",
      "Gradient: [ 12.8562   8.4714  93.5914 122.8912  -3.3861]\n",
      "Weights: [-4.8232  0.7379 -1.1515  0.0661  0.1529]\n",
      "MSE loss: 86.7539\n",
      "Iteration: 26700\n",
      "Gradient: [  0.8313   7.5246  81.6116  61.0272 338.2295]\n",
      "Weights: [-4.8308  0.7453 -1.1519  0.066   0.1529]\n",
      "MSE loss: 86.9143\n",
      "Iteration: 26800\n",
      "Gradient: [   9.811    -6.9606  -15.7685 -124.4128 -335.8179]\n",
      "Weights: [-4.8168  0.7353 -1.1531  0.0658  0.1529]\n",
      "MSE loss: 86.614\n",
      "Iteration: 26900\n",
      "Gradient: [ 14.7303  15.8889  53.0621 155.3163 252.3891]\n",
      "Weights: [-4.7976  0.7349 -1.1526  0.0659  0.1529]\n",
      "MSE loss: 87.4507\n",
      "Iteration: 27000\n",
      "Gradient: [  8.4553  24.5774  30.2348 -95.9743 156.4647]\n",
      "Weights: [-4.8107  0.7374 -1.1552  0.0663  0.153 ]\n",
      "MSE loss: 86.6896\n",
      "Iteration: 27100\n",
      "Gradient: [  11.3063    0.4936   20.1384  -67.7552 -125.9888]\n",
      "Weights: [-4.8292  0.7349 -1.1538  0.0667  0.1528]\n",
      "MSE loss: 86.7865\n",
      "Iteration: 27200\n",
      "Gradient: [  3.8928  21.1082 -25.9636   5.0397 383.2327]\n",
      "Weights: [-4.8194  0.7348 -1.1509  0.0659  0.1528]\n",
      "MSE loss: 86.6251\n",
      "Iteration: 27300\n",
      "Gradient: [ -18.9281  -11.3871   51.6322  109.0707 -409.2179]\n",
      "Weights: [-4.8267  0.7343 -1.1527  0.0657  0.1528]\n",
      "MSE loss: 86.9116\n",
      "Iteration: 27400\n",
      "Gradient: [ -18.2982  -15.0371  -10.1653   16.1732 -597.1223]\n",
      "Weights: [-4.8379  0.7418 -1.154   0.0647  0.1532]\n",
      "MSE loss: 87.1924\n",
      "Iteration: 27500\n",
      "Gradient: [   8.4267    3.9763   -6.698  -163.4057 -202.2274]\n",
      "Weights: [-4.8008  0.7376 -1.1532  0.0642  0.1533]\n",
      "MSE loss: 87.0846\n",
      "Iteration: 27600\n",
      "Gradient: [  -5.0224   21.1751    5.0417  112.8725 -302.9597]\n",
      "Weights: [-4.8139  0.736  -1.1536  0.065   0.1532]\n",
      "MSE loss: 86.6139\n",
      "Iteration: 27700\n",
      "Gradient: [ -14.9526  -15.9632   -3.7573 -203.21   -282.1976]\n",
      "Weights: [-4.8222  0.7306 -1.1544  0.0654  0.1533]\n",
      "MSE loss: 86.8987\n",
      "Iteration: 27800\n",
      "Gradient: [ -8.1669 -12.8134  70.717  -38.4571 -20.6189]\n",
      "Weights: [-4.818   0.7321 -1.1556  0.0655  0.1534]\n",
      "MSE loss: 86.6991\n",
      "Iteration: 27900\n",
      "Gradient: [ -12.7877  -17.0464  -50.8858   -5.6885 -176.9677]\n",
      "Weights: [-4.826   0.7386 -1.1576  0.0658  0.1533]\n",
      "MSE loss: 86.8372\n",
      "Iteration: 28000\n",
      "Gradient: [  5.2303  -8.5702 -74.7734  84.9835 126.4351]\n",
      "Weights: [-4.8068  0.7283 -1.1525  0.0651  0.1534]\n",
      "MSE loss: 86.672\n",
      "Iteration: 28100\n",
      "Gradient: [ 12.1016  50.564   76.9591  40.0953 428.3471]\n",
      "Weights: [-4.7984  0.7309 -1.1535  0.0643  0.1538]\n",
      "MSE loss: 87.0983\n",
      "Iteration: 28200\n",
      "Gradient: [ 14.284   26.9901  69.5351  84.3713 140.6455]\n",
      "Weights: [-4.8246  0.7602 -1.1567  0.0629  0.1537]\n",
      "MSE loss: 87.022\n",
      "Iteration: 28300\n",
      "Gradient: [ -16.9774   -5.1307  -32.4007 -175.2436 -129.0207]\n",
      "Weights: [-4.831   0.7509 -1.1561  0.0628  0.1539]\n",
      "MSE loss: 86.6645\n",
      "Iteration: 28400\n",
      "Gradient: [  1.0203   7.1194  64.4794 305.2436 466.4754]\n",
      "Weights: [-4.8269  0.7416 -1.1547  0.0629  0.1539]\n",
      "MSE loss: 86.7542\n",
      "Iteration: 28500\n",
      "Gradient: [   0.5682   -3.3962    0.5562  107.7208 -269.2212]\n",
      "Weights: [-4.8287  0.7535 -1.1574  0.0619  0.154 ]\n",
      "MSE loss: 86.7703\n",
      "Iteration: 28600\n",
      "Gradient: [ -8.6936 -12.0163   7.9294  68.1875 104.8373]\n",
      "Weights: [-4.8326  0.7628 -1.1637  0.0629  0.1542]\n",
      "MSE loss: 86.6494\n",
      "Iteration: 28700\n",
      "Gradient: [ -14.1639   -5.9575   36.997   -82.4332 -116.2813]\n",
      "Weights: [-4.8284  0.7564 -1.1658  0.0643  0.1542]\n",
      "MSE loss: 86.7069\n",
      "Iteration: 28800\n",
      "Gradient: [ -7.4074   5.4915  14.6877   2.0787 -68.9759]\n",
      "Weights: [-4.8137  0.7534 -1.1671  0.0645  0.1542]\n",
      "MSE loss: 86.6392\n",
      "Iteration: 28900\n",
      "Gradient: [   8.7056   -8.2068  -19.7032  -20.6848 -247.0572]\n",
      "Weights: [-4.8147  0.7416 -1.1633  0.0648  0.1541]\n",
      "MSE loss: 86.7683\n",
      "Iteration: 29000\n",
      "Gradient: [  7.1874 -30.2222  -0.5936  53.0448 -39.5138]\n",
      "Weights: [-4.8073  0.7416 -1.161   0.0644  0.1539]\n",
      "MSE loss: 86.6884\n",
      "Iteration: 29100\n",
      "Gradient: [  -4.5037    5.4638  -30.3841  -41.304  -213.0422]\n",
      "Weights: [-4.8255  0.7477 -1.1574  0.0642  0.1536]\n",
      "MSE loss: 86.6187\n",
      "Iteration: 29200\n",
      "Gradient: [  7.0331  19.3574 -74.6573 172.4047 278.9158]\n",
      "Weights: [-4.8119  0.7441 -1.1602  0.0654  0.1535]\n",
      "MSE loss: 86.6308\n",
      "Iteration: 29300\n",
      "Gradient: [ -1.735    0.5642 -45.7686 -59.1181 -32.0552]\n",
      "Weights: [-4.81    0.7467 -1.164   0.0661  0.1536]\n",
      "MSE loss: 86.6423\n",
      "Iteration: 29400\n",
      "Gradient: [  2.447    8.7588  32.0855 149.2389 193.0739]\n",
      "Weights: [-4.8247  0.7596 -1.1653  0.0659  0.1538]\n",
      "MSE loss: 86.7245\n",
      "Iteration: 29500\n",
      "Gradient: [ -2.2077  -4.9864  14.1221 110.3967   8.1323]\n",
      "Weights: [-4.8198  0.7462 -1.1644  0.0659  0.154 ]\n",
      "MSE loss: 86.5922\n",
      "Iteration: 29600\n",
      "Gradient: [ -20.1297  -15.9197  -20.4368 -108.5356 -273.5793]\n",
      "Weights: [-4.8509  0.7717 -1.1692  0.0647  0.1538]\n",
      "MSE loss: 87.5571\n",
      "Iteration: 29700\n",
      "Gradient: [-1.97   -0.6609 25.902  80.7896 82.605 ]\n",
      "Weights: [-4.8321  0.7616 -1.1663  0.0666  0.1537]\n",
      "MSE loss: 86.7115\n",
      "Iteration: 29800\n",
      "Gradient: [   5.8523   14.3346   19.037  -149.7556  -45.1981]\n",
      "Weights: [-4.8176  0.7483 -1.166   0.0657  0.1538]\n",
      "MSE loss: 86.7404\n",
      "Iteration: 29900\n",
      "Gradient: [ -17.0898  -40.406   -42.043   -18.8722 -128.6979]\n",
      "Weights: [-4.8533  0.7543 -1.1651  0.0659  0.1537]\n",
      "MSE loss: 88.3436\n",
      "Iteration: 30000\n",
      "Gradient: [   3.5735  -18.5979   -7.7931 -107.808  -293.5518]\n",
      "Weights: [-4.8244  0.7653 -1.1682  0.0655  0.1538]\n",
      "MSE loss: 86.6101\n",
      "Iteration: 30100\n",
      "Gradient: [ 2.341170e+01  2.559090e+01 -1.102000e-01  1.138008e+02 -5.051800e+01]\n",
      "Weights: [-4.8105  0.7675 -1.1696  0.0661  0.1537]\n",
      "MSE loss: 87.4007\n",
      "Iteration: 30200\n",
      "Gradient: [  20.29    -15.1085  -61.1372 -186.5228  -84.183 ]\n",
      "Weights: [-4.8118  0.7545 -1.1671  0.0657  0.1537]\n",
      "MSE loss: 86.675\n",
      "Iteration: 30300\n",
      "Gradient: [-12.3332  -4.9531 -52.1576 226.8744 116.747 ]\n",
      "Weights: [-4.827   0.7578 -1.1673  0.0664  0.1537]\n",
      "MSE loss: 86.559\n",
      "Iteration: 30400\n",
      "Gradient: [  3.7938   7.1676  52.3529 -14.8384 -46.9256]\n",
      "Weights: [-4.8241  0.747  -1.1663  0.0674  0.1538]\n",
      "MSE loss: 86.6248\n",
      "Iteration: 30500\n",
      "Gradient: [  -0.6416    2.7671   28.6345  -35.638  -156.2823]\n",
      "Weights: [-4.8088  0.7476 -1.1699  0.0675  0.1539]\n",
      "MSE loss: 86.6165\n",
      "Iteration: 30600\n",
      "Gradient: [  7.6842  13.3606  50.8419  79.3333 169.8618]\n",
      "Weights: [-4.8042  0.7539 -1.1728  0.0676  0.1539]\n",
      "MSE loss: 86.7589\n",
      "Iteration: 30700\n",
      "Gradient: [  1.0875  -3.0521 -17.2634 -63.6375 138.9349]\n",
      "Weights: [-4.8173  0.7576 -1.1736  0.068   0.1538]\n",
      "MSE loss: 86.5432\n",
      "Iteration: 30800\n",
      "Gradient: [ 14.872   13.191   -9.5993 113.0683 390.0974]\n",
      "Weights: [-4.813   0.7547 -1.1691  0.0679  0.1537]\n",
      "MSE loss: 86.8703\n",
      "Iteration: 30900\n",
      "Gradient: [-7.5525 12.5531 66.3826  3.0509 71.0738]\n",
      "Weights: [-4.8084  0.7362 -1.1656  0.068   0.1539]\n",
      "MSE loss: 86.7011\n",
      "Iteration: 31000\n",
      "Gradient: [  -2.3623  -37.7022    6.3698 -242.2995 -392.6434]\n",
      "Weights: [-4.8184  0.745  -1.1666  0.068   0.1533]\n",
      "MSE loss: 86.6598\n",
      "Iteration: 31100\n",
      "Gradient: [ -16.5162  -14.2823  -43.0287  -97.9112 -624.5341]\n",
      "Weights: [-4.8175  0.7453 -1.1666  0.0686  0.1531]\n",
      "MSE loss: 86.586\n",
      "Iteration: 31200\n",
      "Gradient: [ -0.6978   2.2189  10.6199 -49.7641  65.0997]\n",
      "Weights: [-4.8097  0.7389 -1.1646  0.0694  0.153 ]\n",
      "MSE loss: 86.5665\n",
      "Iteration: 31300\n",
      "Gradient: [ -15.2555   15.941    11.9169 -108.0985 -447.3297]\n",
      "Weights: [-4.8175  0.7387 -1.1631  0.0687  0.1528]\n",
      "MSE loss: 86.6789\n",
      "Iteration: 31400\n",
      "Gradient: [  20.5989   16.5078  -72.0086   80.6234 -160.5451]\n",
      "Weights: [-4.82    0.7429 -1.1612  0.0687  0.1526]\n",
      "MSE loss: 86.552\n",
      "Iteration: 31500\n",
      "Gradient: [ -0.6864  -0.8356  17.2854  30.3583 105.0543]\n",
      "Weights: [-4.8257  0.744  -1.158   0.068   0.1528]\n",
      "MSE loss: 86.6917\n",
      "Iteration: 31600\n",
      "Gradient: [ 18.4678  17.6645  77.8067 -15.2631  -0.4729]\n",
      "Weights: [-4.8122  0.7453 -1.1644  0.0689  0.1531]\n",
      "MSE loss: 86.7226\n",
      "Iteration: 31700\n",
      "Gradient: [   2.1771  -12.8154  -57.1279  -37.1586 -264.6355]\n",
      "Weights: [-4.8044  0.7253 -1.1621  0.0689  0.1531]\n",
      "MSE loss: 86.765\n",
      "Iteration: 31800\n",
      "Gradient: [ -15.3109  -26.2157   26.6484   49.9828 -149.7466]\n",
      "Weights: [-4.804   0.7256 -1.1608  0.0693  0.153 ]\n",
      "MSE loss: 86.6151\n",
      "Iteration: 31900\n",
      "Gradient: [  -4.3727   -4.472   -62.7899  -53.7142 -324.8141]\n",
      "Weights: [-4.8209  0.7375 -1.1625  0.0696  0.1526]\n",
      "MSE loss: 86.6548\n",
      "Iteration: 32000\n",
      "Gradient: [  -6.918   -24.388   -39.9999  -70.4657 -465.6008]\n",
      "Weights: [-4.8328  0.7493 -1.1632  0.069   0.1524]\n",
      "MSE loss: 86.821\n",
      "Iteration: 32100\n",
      "Gradient: [ -4.7843 -18.6398  34.6594 -51.7589 -41.9116]\n",
      "Weights: [-4.824   0.745  -1.1627  0.0689  0.1527]\n",
      "MSE loss: 86.5657\n",
      "Iteration: 32200\n",
      "Gradient: [  -6.4742    3.2921    1.894    78.9387 -336.5267]\n",
      "Weights: [-4.8188  0.7432 -1.1667  0.0705  0.1528]\n",
      "MSE loss: 86.5292\n",
      "Iteration: 32300\n",
      "Gradient: [ -12.5501  -11.0843   -5.0651  -88.5809 -316.0691]\n",
      "Weights: [-4.8158  0.7258 -1.1615  0.0692  0.1528]\n",
      "MSE loss: 87.2484\n",
      "Iteration: 32400\n",
      "Gradient: [   3.9708  -27.139    45.4837  -39.7546 -236.6763]\n",
      "Weights: [-4.8145  0.7404 -1.1661  0.069   0.1532]\n",
      "MSE loss: 86.5788\n",
      "Iteration: 32500\n",
      "Gradient: [ -8.7763  17.7895 -19.5284  80.3921 122.1106]\n",
      "Weights: [-4.8111  0.7455 -1.1681  0.0699  0.153 ]\n",
      "MSE loss: 86.5865\n",
      "Iteration: 32600\n",
      "Gradient: [  9.9431  10.9054  33.7691 230.7364 295.3013]\n",
      "Weights: [-4.8077  0.7524 -1.1703  0.0698  0.153 ]\n",
      "MSE loss: 86.822\n",
      "Iteration: 32700\n",
      "Gradient: [ 10.5349   9.6896  42.2946  15.9343 -99.2455]\n",
      "Weights: [-4.8108  0.7549 -1.1733  0.0699  0.1532]\n",
      "MSE loss: 86.6235\n",
      "Iteration: 32800\n",
      "Gradient: [  5.2092  -5.2708  12.736  119.0395  25.2757]\n",
      "Weights: [-4.8245  0.7606 -1.1753  0.0693  0.1535]\n",
      "MSE loss: 86.5318\n",
      "Iteration: 32900\n",
      "Gradient: [ -5.9794 -14.0768 -19.0209  58.5027 196.9375]\n",
      "Weights: [-4.8307  0.7586 -1.1716  0.0687  0.1535]\n",
      "MSE loss: 86.59\n",
      "Iteration: 33000\n",
      "Gradient: [   9.9424  -14.7919  -55.0067   19.0178 -574.0103]\n",
      "Weights: [-4.8219  0.7609 -1.1739  0.0689  0.1535]\n",
      "MSE loss: 86.5154\n",
      "Iteration: 33100\n",
      "Gradient: [   4.4261   14.6843  -44.9563 -116.3796 -289.1751]\n",
      "Weights: [-4.8177  0.7602 -1.1753  0.0697  0.1533]\n",
      "MSE loss: 86.5245\n",
      "Iteration: 33200\n",
      "Gradient: [   5.263    18.7471    5.2939 -103.5416 -116.0975]\n",
      "Weights: [-4.8086  0.7497 -1.1725  0.0691  0.1535]\n",
      "MSE loss: 86.5952\n",
      "Iteration: 33300\n",
      "Gradient: [  -3.8119  -25.7968   28.7808  -22.4328 -150.612 ]\n",
      "Weights: [-4.8213  0.7544 -1.173   0.0687  0.1538]\n",
      "MSE loss: 86.5604\n",
      "Iteration: 33400\n",
      "Gradient: [  5.4015  -0.6127  19.0068 114.8718 -77.6295]\n",
      "Weights: [-4.8243  0.7556 -1.1694  0.0691  0.1533]\n",
      "MSE loss: 86.5824\n",
      "Iteration: 33500\n",
      "Gradient: [ 8.7526  5.4038 -3.0286  3.8669 59.3094]\n",
      "Weights: [-4.8129  0.7521 -1.1666  0.0685  0.1529]\n",
      "MSE loss: 86.6474\n",
      "Iteration: 33600\n",
      "Gradient: [  21.9546   17.8838  -23.43      8.8841 -107.5763]\n",
      "Weights: [-4.8068  0.7459 -1.1675  0.0694  0.1533]\n",
      "MSE loss: 86.9089\n",
      "Iteration: 33700\n",
      "Gradient: [  4.773   22.0586   0.1094 -87.0168 103.6182]\n",
      "Weights: [-4.8105  0.7357 -1.1604  0.0683  0.153 ]\n",
      "MSE loss: 86.5855\n",
      "Iteration: 33800\n",
      "Gradient: [ -6.2952   0.5522  35.0347 -10.4231 452.211 ]\n",
      "Weights: [-4.8239  0.7436 -1.1607  0.0685  0.1527]\n",
      "MSE loss: 86.5629\n",
      "Iteration: 33900\n",
      "Gradient: [  -8.6468  -20.7106  -38.3088  -92.2291 -311.9048]\n",
      "Weights: [-4.8526  0.7597 -1.1644  0.0683  0.1526]\n",
      "MSE loss: 87.4703\n",
      "Iteration: 34000\n",
      "Gradient: [ -3.5525  12.9758   0.8172 -30.102   69.0825]\n",
      "Weights: [-4.8331  0.7585 -1.1654  0.0685  0.1529]\n",
      "MSE loss: 86.6023\n",
      "Iteration: 34100\n",
      "Gradient: [ -2.1867  -7.159   -4.8678  91.1465 176.354 ]\n",
      "Weights: [-4.8131  0.748  -1.1649  0.0686  0.1527]\n",
      "MSE loss: 86.6109\n",
      "Iteration: 34200\n",
      "Gradient: [ -18.7698  -66.2318  -93.0244 -199.7214 -325.679 ]\n",
      "Weights: [-4.8372  0.7459 -1.1656  0.0685  0.153 ]\n",
      "MSE loss: 87.6086\n",
      "Iteration: 34300\n",
      "Gradient: [  -9.6182  -19.4481  -34.9854 -140.2098 -128.6324]\n",
      "Weights: [-4.803   0.7376 -1.1671  0.0698  0.1529]\n",
      "MSE loss: 86.6539\n",
      "Iteration: 34400\n",
      "Gradient: [  -6.2231  -36.9078   41.913    16.7712 -159.8801]\n",
      "Weights: [-4.8147  0.7402 -1.1663  0.0693  0.1528]\n",
      "MSE loss: 86.8048\n",
      "Iteration: 34500\n",
      "Gradient: [   7.2505   -0.459   -24.1291  -26.817  -294.7307]\n",
      "Weights: [-4.8161  0.7384 -1.1644  0.07    0.1528]\n",
      "MSE loss: 86.5443\n",
      "Iteration: 34600\n",
      "Gradient: [ -7.5765 -49.3897 -28.7919 -49.0667 183.9204]\n",
      "Weights: [-4.8256  0.7464 -1.1689  0.0705  0.1524]\n",
      "MSE loss: 87.1575\n",
      "Iteration: 34700\n",
      "Gradient: [ -2.9376   5.2336 -12.3395 -34.6647 267.7712]\n",
      "Weights: [-4.8235  0.7597 -1.1698  0.0696  0.1525]\n",
      "MSE loss: 86.5754\n",
      "Iteration: 34800\n",
      "Gradient: [  -9.0796  -28.3172  -44.1946 -197.4047   14.0347]\n",
      "Weights: [-4.8438  0.7653 -1.1715  0.0696  0.1526]\n",
      "MSE loss: 87.0764\n",
      "Iteration: 34900\n",
      "Gradient: [  4.5222 -10.5668  23.6208   6.8097  53.245 ]\n",
      "Weights: [-4.8379  0.7751 -1.1766  0.0699  0.1528]\n",
      "MSE loss: 86.6077\n",
      "Iteration: 35000\n",
      "Gradient: [  -8.3486  -55.0765  -39.7958  -59.9097 -199.017 ]\n",
      "Weights: [-4.8665  0.7768 -1.1731  0.0698  0.1527]\n",
      "MSE loss: 87.7551\n",
      "Iteration: 35100\n",
      "Gradient: [ 29.5551  27.7487  60.3901 -44.2717 274.5597]\n",
      "Weights: [-4.818   0.7699 -1.1723  0.07    0.1527]\n",
      "MSE loss: 87.1818\n",
      "Iteration: 35200\n",
      "Gradient: [ 10.5146  -1.4099  -4.8775  95.6516 144.0012]\n",
      "Weights: [-4.8195  0.7715 -1.1763  0.0701  0.1531]\n",
      "MSE loss: 86.9822\n",
      "Iteration: 35300\n",
      "Gradient: [  9.3794  26.2216 -28.384  -54.8092 -84.0737]\n",
      "Weights: [-4.8106  0.7643 -1.1798  0.0711  0.153 ]\n",
      "MSE loss: 86.6582\n",
      "Iteration: 35400\n",
      "Gradient: [ -15.8241  -11.6044    9.4952 -282.5676 -113.7298]\n",
      "Weights: [-4.8187  0.7453 -1.1717  0.0708  0.1532]\n",
      "MSE loss: 86.572\n",
      "Iteration: 35500\n",
      "Gradient: [  3.5697  20.4397  47.0388 158.3614 102.6668]\n",
      "Weights: [-4.8     0.7327 -1.1657  0.0711  0.1528]\n",
      "MSE loss: 86.7373\n",
      "Iteration: 35600\n",
      "Gradient: [   9.174   -18.618   -30.6574 -127.2222    8.8247]\n",
      "Weights: [-4.8154  0.7416 -1.1657  0.0711  0.1524]\n",
      "MSE loss: 86.513\n",
      "Iteration: 35700\n",
      "Gradient: [  -6.1972   16.4479   -9.9201 -127.4029  -39.9944]\n",
      "Weights: [-4.8143  0.7404 -1.1675  0.071   0.1524]\n",
      "MSE loss: 86.6314\n",
      "Iteration: 35800\n",
      "Gradient: [  2.5999 -20.3042 -57.6079 -85.889  318.5009]\n",
      "Weights: [-4.8119  0.7329 -1.1649  0.0729  0.1521]\n",
      "MSE loss: 86.5436\n",
      "Iteration: 35900\n",
      "Gradient: [ -10.4437   -6.0505  -23.7291 -141.9417 -297.6759]\n",
      "Weights: [-4.8307  0.7426 -1.1638  0.0718  0.1519]\n",
      "MSE loss: 86.7921\n",
      "Iteration: 36000\n",
      "Gradient: [  1.2536 -25.6824 -21.7049 137.0584 282.3986]\n",
      "Weights: [-4.8158  0.7312 -1.1592  0.0729  0.1513]\n",
      "MSE loss: 86.5359\n",
      "Iteration: 36100\n",
      "Gradient: [ -5.7169  -8.4929 -39.5674  -0.7981 112.3938]\n",
      "Weights: [-4.8181  0.7359 -1.1618  0.0734  0.1513]\n",
      "MSE loss: 86.512\n",
      "Iteration: 36200\n",
      "Gradient: [ -3.7715 -21.3019   6.3171 -95.6138 317.1908]\n",
      "Weights: [-4.8175  0.7457 -1.1641  0.0729  0.1513]\n",
      "MSE loss: 86.5708\n",
      "Iteration: 36300\n",
      "Gradient: [   4.1165    3.0717   34.1735   60.2137 -492.6225]\n",
      "Weights: [-4.8213  0.752  -1.1668  0.0731  0.1513]\n",
      "MSE loss: 86.567\n",
      "Iteration: 36400\n",
      "Gradient: [  9.8508   8.6254  26.1238  17.5197 189.1479]\n",
      "Weights: [-4.8233  0.7486 -1.1636  0.0731  0.1513]\n",
      "MSE loss: 86.64\n",
      "Iteration: 36500\n",
      "Gradient: [ -5.62     2.2052 -18.2236   7.2267 171.3974]\n",
      "Weights: [-4.8351  0.7488 -1.1643  0.0726  0.1514]\n",
      "MSE loss: 86.7161\n",
      "Iteration: 36600\n",
      "Gradient: [-18.3195   7.9965  -2.3747 -50.8644 110.3866]\n",
      "Weights: [-4.8265  0.7482 -1.1618  0.0719  0.1515]\n",
      "MSE loss: 86.5838\n",
      "Iteration: 36700\n",
      "Gradient: [ 14.4312  16.4069  51.0179 237.8511 396.353 ]\n",
      "Weights: [-4.8187  0.7444 -1.1621  0.0721  0.1516]\n",
      "MSE loss: 86.6381\n",
      "Iteration: 36800\n",
      "Gradient: [ -11.6643    9.0845    4.7961  177.7346 -106.2771]\n",
      "Weights: [-4.8214  0.7461 -1.1628  0.0734  0.1513]\n",
      "MSE loss: 86.7082\n",
      "Iteration: 36900\n",
      "Gradient: [ 25.2821  47.9874  51.077  155.3234 268.9512]\n",
      "Weights: [-4.8043  0.7478 -1.1629  0.0732  0.1513]\n",
      "MSE loss: 88.0134\n",
      "Iteration: 37000\n",
      "Gradient: [ -3.4781  33.0746  20.6344  -7.6729 157.2556]\n",
      "Weights: [-4.8143  0.746  -1.164   0.0732  0.1514]\n",
      "MSE loss: 86.7878\n",
      "Iteration: 37100\n",
      "Gradient: [ -0.7961 -27.5323  -0.2967 128.1014 267.1785]\n",
      "Weights: [-4.8072  0.7394 -1.1672  0.0738  0.1515]\n",
      "MSE loss: 86.5739\n",
      "Iteration: 37200\n",
      "Gradient: [ 30.787   32.1432  67.0595 210.3625 496.5222]\n",
      "Weights: [-4.7968  0.7341 -1.1653  0.0755  0.151 ]\n",
      "MSE loss: 87.0162\n",
      "Iteration: 37300\n",
      "Gradient: [  6.3102  25.1852 -20.7628  76.7623 270.4051]\n",
      "Weights: [-4.8124  0.7441 -1.164   0.075   0.1507]\n",
      "MSE loss: 86.8121\n",
      "Iteration: 37400\n",
      "Gradient: [ -8.145   51.4034  28.6739   6.205  113.6412]\n",
      "Weights: [-4.8243  0.7427 -1.1657  0.0746  0.1511]\n",
      "MSE loss: 86.5147\n",
      "Iteration: 37500\n",
      "Gradient: [  6.757   27.7514  24.1018 -31.4951 424.6169]\n",
      "Weights: [-4.8183  0.7379 -1.163   0.0752  0.1509]\n",
      "MSE loss: 86.5711\n",
      "Iteration: 37600\n",
      "Gradient: [ 4.658400e+00 -7.880000e-02  1.824300e+00 -3.516600e+00  1.440199e+02]\n",
      "Weights: [-4.8068  0.7311 -1.1641  0.076   0.1509]\n",
      "MSE loss: 86.6244\n",
      "Iteration: 37700\n",
      "Gradient: [ -15.6432   -1.9967  -32.4862 -138.6029 -252.8953]\n",
      "Weights: [-4.8219  0.7224 -1.163   0.0762  0.1509]\n",
      "MSE loss: 87.0189\n",
      "Iteration: 37800\n",
      "Gradient: [  3.857   12.0534  53.9879 -98.5246 425.0233]\n",
      "Weights: [-4.8004  0.7318 -1.1656  0.0756  0.151 ]\n",
      "MSE loss: 86.681\n",
      "Iteration: 37900\n",
      "Gradient: [ -9.6896  -2.987    9.0025 130.9764 -49.2166]\n",
      "Weights: [-4.8298  0.7321 -1.1637  0.0754  0.1512]\n",
      "MSE loss: 86.8395\n",
      "Iteration: 38000\n",
      "Gradient: [-7.3657  1.6899 23.6988 46.3798 20.9379]\n",
      "Weights: [-4.8162  0.7316 -1.1635  0.0747  0.1514]\n",
      "MSE loss: 86.5128\n",
      "Iteration: 38100\n",
      "Gradient: [  -2.2355  -30.3891    3.4789  -88.2461 -226.2428]\n",
      "Weights: [-4.825   0.7325 -1.1637  0.0744  0.1513]\n",
      "MSE loss: 86.7829\n",
      "Iteration: 38200\n",
      "Gradient: [  7.5561   9.4356  27.7447 162.6968 166.8906]\n",
      "Weights: [-4.805   0.7325 -1.1646  0.0744  0.1516]\n",
      "MSE loss: 86.7255\n",
      "Iteration: 38300\n",
      "Gradient: [   3.3851  -12.7732  -50.8817 -111.6798  132.9647]\n",
      "Weights: [-4.8165  0.7273 -1.1606  0.0726  0.1518]\n",
      "MSE loss: 86.5919\n",
      "Iteration: 38400\n",
      "Gradient: [ -4.5387  28.7835  89.4381 112.882  600.2032]\n",
      "Weights: [-4.81    0.7375 -1.1581  0.0717  0.1517]\n",
      "MSE loss: 87.0326\n",
      "Iteration: 38500\n",
      "Gradient: [ -2.4788  19.5656  25.5489 -39.8236  75.0586]\n",
      "Weights: [-4.8024  0.7247 -1.1579  0.0723  0.1518]\n",
      "MSE loss: 86.7864\n",
      "Iteration: 38600\n",
      "Gradient: [   5.9612  -16.2827   -2.452   173.5156 -145.0523]\n",
      "Weights: [-4.816   0.7258 -1.1563  0.0714  0.1517]\n",
      "MSE loss: 86.5787\n",
      "Iteration: 38700\n",
      "Gradient: [   3.9294    2.8422  -12.3232   11.1802 -125.5401]\n",
      "Weights: [-4.8206  0.728  -1.1548  0.0713  0.1516]\n",
      "MSE loss: 86.5933\n",
      "Iteration: 38800\n",
      "Gradient: [ 7.0179 23.1947  3.0488 84.8004 93.4123]\n",
      "Weights: [-4.8114  0.736  -1.1559  0.0705  0.1517]\n",
      "MSE loss: 86.7849\n",
      "Iteration: 38900\n",
      "Gradient: [ -1.865   22.7789  61.893  156.4798 223.9096]\n",
      "Weights: [-4.8202  0.7438 -1.1589  0.0711  0.1517]\n",
      "MSE loss: 86.7682\n",
      "Iteration: 39000\n",
      "Gradient: [  -8.3478  -16.2103  -35.167   -69.9492 -353.2319]\n",
      "Weights: [-4.8266  0.7502 -1.1652  0.0718  0.1515]\n",
      "MSE loss: 86.6444\n",
      "Iteration: 39100\n",
      "Gradient: [ -10.0474    1.5829   18.9485  -52.7133 -229.551 ]\n",
      "Weights: [-4.8242  0.7431 -1.1663  0.0723  0.1517]\n",
      "MSE loss: 86.7748\n",
      "Iteration: 39200\n",
      "Gradient: [ 22.658   33.2324  35.2677  99.3953 139.9539]\n",
      "Weights: [-4.801   0.7381 -1.1678  0.0725  0.1522]\n",
      "MSE loss: 86.7228\n",
      "Iteration: 39300\n",
      "Gradient: [  -0.8179  -32.716   -55.1102  -62.0617 -240.5148]\n",
      "Weights: [-4.8135  0.7377 -1.1705  0.0731  0.1523]\n",
      "MSE loss: 86.5961\n",
      "Iteration: 39400\n",
      "Gradient: [   4.4727  -27.639   -20.6956  -57.1669 -452.4969]\n",
      "Weights: [-4.8201  0.7429 -1.1697  0.0727  0.1523]\n",
      "MSE loss: 86.5624\n",
      "Iteration: 39500\n",
      "Gradient: [ -1.9954 -16.0191  19.3898  42.7346 -38.5403]\n",
      "Weights: [-4.8272  0.7557 -1.1705  0.0721  0.1524]\n",
      "MSE loss: 86.5796\n",
      "Iteration: 39600\n",
      "Gradient: [-13.9302 -13.5646 -53.4782 -34.3689  14.5112]\n",
      "Weights: [-4.8233  0.7572 -1.1759  0.0718  0.1526]\n",
      "MSE loss: 86.6496\n",
      "Iteration: 39700\n",
      "Gradient: [ -6.4428  22.8345 -22.3314 -87.966   32.0291]\n",
      "Weights: [-4.8367  0.7677 -1.1783  0.0725  0.1526]\n",
      "MSE loss: 86.5801\n",
      "Iteration: 39800\n",
      "Gradient: [ -2.8215 -16.9881   5.9436  83.3434  70.6319]\n",
      "Weights: [-4.8299  0.771  -1.1804  0.0724  0.1524]\n",
      "MSE loss: 86.533\n",
      "Iteration: 39900\n",
      "Gradient: [   0.5314  -18.467   -29.7923 -125.15    145.8723]\n",
      "Weights: [-4.8434  0.7651 -1.1785  0.0728  0.1527]\n",
      "MSE loss: 86.9301\n",
      "Iteration: 40000\n",
      "Gradient: [ -13.2405    2.8172   16.1613  -78.6614 -109.3245]\n",
      "Weights: [-4.8276  0.7513 -1.1785  0.074   0.1526]\n",
      "MSE loss: 86.7926\n",
      "Iteration: 40100\n",
      "Gradient: [  8.7343   0.8206 -10.9031 202.6181 302.2911]\n",
      "Weights: [-4.8145  0.7602 -1.1814  0.074   0.1528]\n",
      "MSE loss: 86.6212\n",
      "Iteration: 40200\n",
      "Gradient: [-14.0583   4.736   26.3967  46.2406 285.5644]\n",
      "Weights: [-4.8303  0.7688 -1.1794  0.072   0.153 ]\n",
      "MSE loss: 86.5061\n",
      "Iteration: 40300\n",
      "Gradient: [  0.3412   5.7425  37.694  -36.2439 113.2598]\n",
      "Weights: [-4.8064  0.7569 -1.1791  0.0717  0.1532]\n",
      "MSE loss: 86.6695\n",
      "Iteration: 40400\n",
      "Gradient: [-12.6972   5.6511 -29.8388 -58.3437  64.4293]\n",
      "Weights: [-4.8342  0.7636 -1.1797  0.072   0.1531]\n",
      "MSE loss: 86.6767\n",
      "Iteration: 40500\n",
      "Gradient: [  8.2332   3.5889  52.3065 -78.8669 254.3095]\n",
      "Weights: [-4.819   0.7645 -1.1777  0.0724  0.1527]\n",
      "MSE loss: 86.6596\n",
      "Iteration: 40600\n",
      "Gradient: [  7.7882  24.978   39.4336 211.6143  99.5886]\n",
      "Weights: [-4.8373  0.7832 -1.1803  0.0718  0.1526]\n",
      "MSE loss: 86.6514\n",
      "Iteration: 40700\n",
      "Gradient: [  3.574  -17.6988 -38.0756 -20.6014 -19.7577]\n",
      "Weights: [-4.8345  0.7669 -1.1762  0.0713  0.1525]\n",
      "MSE loss: 86.6741\n",
      "Iteration: 40800\n",
      "Gradient: [ -2.6403 -15.0699   8.209   43.388  108.404 ]\n",
      "Weights: [-4.8437  0.7753 -1.1809  0.0727  0.1524]\n",
      "MSE loss: 86.7646\n",
      "Iteration: 40900\n",
      "Gradient: [   5.3884    4.6712  -22.9169   56.828  -220.9911]\n",
      "Weights: [-4.8238  0.7705 -1.1838  0.0729  0.1528]\n",
      "MSE loss: 86.4657\n",
      "Iteration: 41000\n",
      "Gradient: [ 11.9782 -14.1016  84.8028 191.8733 -38.7799]\n",
      "Weights: [-4.8231  0.7748 -1.1841  0.074   0.1527]\n",
      "MSE loss: 86.8418\n",
      "Iteration: 41100\n",
      "Gradient: [ -6.2059  -0.4643 -54.7564  52.7934 158.6285]\n",
      "Weights: [-4.8241  0.7779 -1.1867  0.0733  0.1528]\n",
      "MSE loss: 86.4863\n",
      "Iteration: 41200\n",
      "Gradient: [  7.6032  16.8944 -22.6125  80.7394  60.0092]\n",
      "Weights: [-4.8256  0.7786 -1.1877  0.0739  0.1528]\n",
      "MSE loss: 86.498\n",
      "Iteration: 41300\n",
      "Gradient: [   4.0128   23.738   -19.1859   27.8098 -105.726 ]\n",
      "Weights: [-4.8277  0.7795 -1.1857  0.0742  0.1526]\n",
      "MSE loss: 86.6228\n",
      "Iteration: 41400\n",
      "Gradient: [  -4.7058  -11.1097  -44.8448 -116.7222 -320.1456]\n",
      "Weights: [-4.8397  0.7782 -1.1864  0.0738  0.1524]\n",
      "MSE loss: 86.8389\n",
      "Iteration: 41500\n",
      "Gradient: [  -1.2894   24.0001   36.0677   83.4521 -583.6964]\n",
      "Weights: [-4.8173  0.7792 -1.187   0.0738  0.1526]\n",
      "MSE loss: 86.7607\n",
      "Iteration: 41600\n",
      "Gradient: [ -10.6315  -10.1479  -24.104    47.0873 -446.0766]\n",
      "Weights: [-4.8258  0.7712 -1.1892  0.0741  0.1528]\n",
      "MSE loss: 86.7123\n",
      "Iteration: 41700\n",
      "Gradient: [  -3.2622  -23.2614  -25.9035   71.9024 -495.9549]\n",
      "Weights: [-4.8136  0.7514 -1.1851  0.0752  0.1527]\n",
      "MSE loss: 86.6915\n",
      "Iteration: 41800\n",
      "Gradient: [  -4.3308  -10.7869  -17.7687 -133.2752 -229.8168]\n",
      "Weights: [-4.8257  0.757  -1.1839  0.0743  0.1528]\n",
      "MSE loss: 86.7932\n",
      "Iteration: 41900\n",
      "Gradient: [  3.3728  -6.5457  -2.1676 -54.2843  51.8868]\n",
      "Weights: [-4.817   0.7617 -1.1854  0.074   0.1529]\n",
      "MSE loss: 86.5328\n",
      "Iteration: 42000\n",
      "Gradient: [-15.1739  -0.1314  30.8696  71.5583  -2.9601]\n",
      "Weights: [-4.8304  0.7659 -1.1838  0.0742  0.153 ]\n",
      "MSE loss: 86.5508\n",
      "Iteration: 42100\n",
      "Gradient: [ -4.4635  -0.2733  44.191  -41.8927  61.1481]\n",
      "Weights: [-4.8422  0.7873 -1.1916  0.074   0.1529]\n",
      "MSE loss: 86.5554\n",
      "Iteration: 42200\n",
      "Gradient: [  13.6142   -2.7487  -17.7089  -54.7136 -307.7054]\n",
      "Weights: [-4.8261  0.7888 -1.1914  0.0734  0.1528]\n",
      "MSE loss: 86.5518\n",
      "Iteration: 42300\n",
      "Gradient: [-12.5494   6.0516 -36.7926  -8.0274 -31.8374]\n",
      "Weights: [-4.8358  0.7812 -1.1868  0.0737  0.1527]\n",
      "MSE loss: 86.4687\n",
      "Iteration: 42400\n",
      "Gradient: [ -2.2192   3.0174  64.4238 104.01   -53.2339]\n",
      "Weights: [-4.8074  0.7683 -1.1906  0.0755  0.1528]\n",
      "MSE loss: 86.685\n",
      "Iteration: 42500\n",
      "Gradient: [ -6.2296  -5.4595 -27.1093 -19.5784 210.532 ]\n",
      "Weights: [-4.8227  0.7722 -1.1898  0.0747  0.1527]\n",
      "MSE loss: 86.4913\n",
      "Iteration: 42600\n",
      "Gradient: [  -4.5751   -6.8757  -11.6575 -118.3311 -352.248 ]\n",
      "Weights: [-4.8267  0.769  -1.1905  0.0757  0.1527]\n",
      "MSE loss: 86.5681\n",
      "Iteration: 42700\n",
      "Gradient: [  2.5958  24.2242  56.5649  34.3459 215.7618]\n",
      "Weights: [-4.823   0.7757 -1.1921  0.0759  0.1527]\n",
      "MSE loss: 86.427\n",
      "Iteration: 42800\n",
      "Gradient: [  1.5629  -1.0495  -2.8447 -45.1211 -34.3339]\n",
      "Weights: [-4.8252  0.7705 -1.1896  0.0763  0.1525]\n",
      "MSE loss: 86.4184\n",
      "Iteration: 42900\n",
      "Gradient: [ -8.6635  -3.7225  -3.5821 -62.3693 -48.116 ]\n",
      "Weights: [-4.8352  0.7757 -1.1902  0.0761  0.1523]\n",
      "MSE loss: 86.5647\n",
      "Iteration: 43000\n",
      "Gradient: [-1.916550e+01 -3.654780e+01 -1.702310e+01 -4.187000e-01 -5.060806e+02]\n",
      "Weights: [-4.8233  0.7641 -1.1881  0.0768  0.1521]\n",
      "MSE loss: 86.5615\n",
      "Iteration: 43100\n",
      "Gradient: [  -4.0127   16.3751   -6.6525 -153.8279  426.1911]\n",
      "Weights: [-4.7975  0.7488 -1.1881  0.0785  0.152 ]\n",
      "MSE loss: 86.6286\n",
      "Iteration: 43200\n",
      "Gradient: [  6.913  -21.6452  16.4847 -57.8764 254.4488]\n",
      "Weights: [-4.8266  0.7634 -1.1888  0.0783  0.152 ]\n",
      "MSE loss: 86.4577\n",
      "Iteration: 43300\n",
      "Gradient: [  30.0252   30.6363   26.7473  109.311  -279.1254]\n",
      "Weights: [-4.8111  0.7645 -1.1901  0.0786  0.152 ]\n",
      "MSE loss: 86.5819\n",
      "Iteration: 43400\n",
      "Gradient: [   2.1291  -17.7298 -100.7528  -46.838  -176.3533]\n",
      "Weights: [-4.8263  0.7669 -1.1878  0.0777  0.1515]\n",
      "MSE loss: 86.6224\n",
      "Iteration: 43500\n",
      "Gradient: [  0.523    0.6085  -7.6676  59.7768 140.0735]\n",
      "Weights: [-4.8309  0.766  -1.1886  0.079   0.1515]\n",
      "MSE loss: 86.5357\n",
      "Iteration: 43600\n",
      "Gradient: [  -9.1451  -17.719    17.5768 -188.0849  137.6338]\n",
      "Weights: [-4.8335  0.7712 -1.1891  0.0794  0.1513]\n",
      "MSE loss: 86.4223\n",
      "Iteration: 43700\n",
      "Gradient: [  -1.0744  -27.1941 -101.2428  -77.2563 -675.6865]\n",
      "Weights: [-4.8277  0.7785 -1.1937  0.079   0.1514]\n",
      "MSE loss: 86.4567\n",
      "Iteration: 43800\n",
      "Gradient: [-1.456000e-01  7.005800e+00 -4.520450e+01 -1.438616e+02 -4.482506e+02]\n",
      "Weights: [-4.8524  0.7982 -1.1974  0.0787  0.1515]\n",
      "MSE loss: 86.634\n",
      "Iteration: 43900\n",
      "Gradient: [  9.3226  30.483   22.8615  57.1452 192.0816]\n",
      "Weights: [-4.8219  0.785  -1.1929  0.0781  0.1516]\n",
      "MSE loss: 86.6967\n",
      "Iteration: 44000\n",
      "Gradient: [  -3.5132  -26.197    -5.375   110.4686 -416.4765]\n",
      "Weights: [-4.8311  0.7741 -1.1946  0.0801  0.1518]\n",
      "MSE loss: 86.4302\n",
      "Iteration: 44100\n",
      "Gradient: [ -13.3325   -0.3654   35.0575   22.6916 -277.8789]\n",
      "Weights: [-4.8387  0.784  -1.1973  0.0801  0.1516]\n",
      "MSE loss: 86.4324\n",
      "Iteration: 44200\n",
      "Gradient: [  -8.6799   20.481    -8.3908   95.8623 -101.8309]\n",
      "Weights: [-4.8274  0.7808 -1.1988  0.0803  0.1519]\n",
      "MSE loss: 86.3559\n",
      "Iteration: 44300\n",
      "Gradient: [-23.0235   2.6796 -34.926  -99.9301  81.6748]\n",
      "Weights: [-4.8465  0.787  -1.2003  0.0793  0.152 ]\n",
      "MSE loss: 86.9313\n",
      "Iteration: 44400\n",
      "Gradient: [  7.4092   6.4455 127.4773 138.3314 221.7452]\n",
      "Weights: [-4.8234  0.7871 -1.1968  0.0786  0.1519]\n",
      "MSE loss: 86.625\n",
      "Iteration: 44500\n",
      "Gradient: [  14.5925    3.8944  -12.3123 -109.079  -184.5783]\n",
      "Weights: [-4.8288  0.7855 -1.1983  0.0784  0.1522]\n",
      "MSE loss: 86.3713\n",
      "Iteration: 44600\n",
      "Gradient: [ -5.6909 -17.2576 -27.4641 -67.6201 -70.3352]\n",
      "Weights: [-4.8205  0.7809 -1.2021  0.0789  0.1523]\n",
      "MSE loss: 86.5378\n",
      "Iteration: 44700\n",
      "Gradient: [   2.0321    8.8128   36.7543   14.2452 -140.2386]\n",
      "Weights: [-4.8188  0.7812 -1.2029  0.08    0.1523]\n",
      "MSE loss: 86.4045\n",
      "Iteration: 44800\n",
      "Gradient: [ 4.1842 11.9621 73.5623 27.9575 52.4768]\n",
      "Weights: [-4.8261  0.7842 -1.1998  0.0796  0.1519]\n",
      "MSE loss: 86.3589\n",
      "Iteration: 44900\n",
      "Gradient: [  -1.5475  -18.1697  -25.8225  156.2459 -341.2525]\n",
      "Weights: [-4.8349  0.7934 -1.1983  0.0784  0.1518]\n",
      "MSE loss: 86.4074\n",
      "Iteration: 45000\n",
      "Gradient: [ 10.6717   8.5656  34.414   10.1091 160.6923]\n",
      "Weights: [-4.8271  0.7845 -1.1952  0.0779  0.1521]\n",
      "MSE loss: 86.4753\n",
      "Iteration: 45100\n",
      "Gradient: [   3.595    -0.3116  -11.417   208.1122 -263.1138]\n",
      "Weights: [-4.8168  0.7766 -1.197   0.0786  0.1522]\n",
      "MSE loss: 86.4468\n",
      "Iteration: 45200\n",
      "Gradient: [ -9.4915   2.9556  43.3958  -1.5457 486.7016]\n",
      "Weights: [-4.8226  0.7805 -1.203   0.0792  0.1525]\n",
      "MSE loss: 86.4585\n",
      "Iteration: 45300\n",
      "Gradient: [ -10.5617  -17.899   -28.215   -21.3478 -184.9073]\n",
      "Weights: [-4.8158  0.7737 -1.2046  0.0804  0.1524]\n",
      "MSE loss: 86.6157\n",
      "Iteration: 45400\n",
      "Gradient: [ -2.0474   4.2352  28.7757 110.3778 453.9629]\n",
      "Weights: [-4.8216  0.7876 -1.1998  0.0795  0.1522]\n",
      "MSE loss: 86.8079\n",
      "Iteration: 45500\n",
      "Gradient: [  0.8295   4.4869 -54.6725 -85.5293 169.6797]\n",
      "Weights: [-4.829   0.7901 -1.2045  0.0804  0.1519]\n",
      "MSE loss: 86.3634\n",
      "Iteration: 45600\n",
      "Gradient: [   1.5333  -29.3399  -29.5284  -92.5523 -170.7808]\n",
      "Weights: [-4.8194  0.7851 -1.2044  0.0806  0.152 ]\n",
      "MSE loss: 86.4022\n",
      "Iteration: 45700\n",
      "Gradient: [  4.0026   8.0081  62.1255 -26.8157 195.0054]\n",
      "Weights: [-4.8322  0.797  -1.2039  0.0804  0.1516]\n",
      "MSE loss: 86.3797\n",
      "Iteration: 45800\n",
      "Gradient: [  6.586   10.4648   2.8701 108.482  330.2821]\n",
      "Weights: [-4.83    0.7891 -1.2002  0.0807  0.1514]\n",
      "MSE loss: 86.3643\n",
      "Iteration: 45900\n",
      "Gradient: [  7.0977   0.1797  76.0281 113.0202  41.8319]\n",
      "Weights: [-4.8356  0.7987 -1.2031  0.0815  0.1515]\n",
      "MSE loss: 86.6281\n",
      "Iteration: 46000\n",
      "Gradient: [  2.5654  -3.6673 -22.1857 -26.6779  28.4032]\n",
      "Weights: [-4.8319  0.7924 -1.2061  0.0817  0.1517]\n",
      "MSE loss: 86.3267\n",
      "Iteration: 46100\n",
      "Gradient: [  -2.5152   13.6496   -9.8122  -66.6616 -534.7674]\n",
      "Weights: [-4.8152  0.7782 -1.2059  0.0818  0.1519]\n",
      "MSE loss: 86.4306\n",
      "Iteration: 46200\n",
      "Gradient: [  7.8929 -10.0616  -6.8595 -40.4303 334.0981]\n",
      "Weights: [-4.8185  0.7825 -1.2075  0.0816  0.1518]\n",
      "MSE loss: 86.5689\n",
      "Iteration: 46300\n",
      "Gradient: [-13.1184   0.5389 -77.597  -97.965  -52.3264]\n",
      "Weights: [-4.8225  0.776  -1.2026  0.0811  0.1523]\n",
      "MSE loss: 86.399\n",
      "Iteration: 46400\n",
      "Gradient: [   2.7861  -11.7869  -62.6832 -102.437  -286.4355]\n",
      "Weights: [-4.8187  0.7689 -1.2003  0.0808  0.1521]\n",
      "MSE loss: 86.4636\n",
      "Iteration: 46500\n",
      "Gradient: [ -8.0254  15.9197  40.7493 189.9889   0.7367]\n",
      "Weights: [-4.8146  0.7634 -1.1985  0.0806  0.1524]\n",
      "MSE loss: 86.4811\n",
      "Iteration: 46600\n",
      "Gradient: [ -10.1055  -29.8564  -49.2684 -190.2349 -543.6208]\n",
      "Weights: [-4.8252  0.7718 -1.2033  0.0801  0.1525]\n",
      "MSE loss: 86.9568\n",
      "Iteration: 46700\n",
      "Gradient: [  5.6133  18.7342  18.3643 -33.6092 246.0617]\n",
      "Weights: [-4.8235  0.79   -1.2044  0.0794  0.1526]\n",
      "MSE loss: 86.5045\n",
      "Iteration: 46800\n",
      "Gradient: [-2.2249  0.8373 -6.9876 83.9684 39.8129]\n",
      "Weights: [-4.8503  0.8039 -1.2036  0.0786  0.1526]\n",
      "MSE loss: 86.6199\n",
      "Iteration: 46900\n",
      "Gradient: [ -21.1591   -8.3055  -41.555  -138.8286 -310.6704]\n",
      "Weights: [-4.8498  0.8    -1.2099  0.0795  0.1527]\n",
      "MSE loss: 86.9805\n",
      "Iteration: 47000\n",
      "Gradient: [ -25.3537   -9.0197  -57.5122  170.8727 -226.0421]\n",
      "Weights: [-4.8426  0.7894 -1.2097  0.0803  0.1528]\n",
      "MSE loss: 87.042\n",
      "Iteration: 47100\n",
      "Gradient: [  -9.4649    7.5974   24.5193  -63.2412 -117.6288]\n",
      "Weights: [-4.824   0.7862 -1.2092  0.0804  0.1526]\n",
      "MSE loss: 86.4958\n",
      "Iteration: 47200\n",
      "Gradient: [ 22.2974  41.0407  63.6977  77.674  493.668 ]\n",
      "Weights: [-4.8242  0.8091 -1.2101  0.0795  0.1524]\n",
      "MSE loss: 86.9541\n",
      "Iteration: 47300\n",
      "Gradient: [ -9.02   -16.4344 -21.7631 -56.4931  39.8523]\n",
      "Weights: [-4.8332  0.7971 -1.2089  0.0803  0.1525]\n",
      "MSE loss: 86.356\n",
      "Iteration: 47400\n",
      "Gradient: [  1.2389  10.9542  29.4472 170.8393   5.2087]\n",
      "Weights: [-4.8348  0.8092 -1.2099  0.0791  0.1525]\n",
      "MSE loss: 86.4305\n",
      "Iteration: 47500\n",
      "Gradient: [-27.1088 -18.7406 -36.3377 -69.8107 -13.272 ]\n",
      "Weights: [-4.8438  0.8018 -1.2117  0.0802  0.1527]\n",
      "MSE loss: 86.5166\n",
      "Iteration: 47600\n",
      "Gradient: [ -5.2838  -1.8628 -35.9054 -26.2997 -23.1375]\n",
      "Weights: [-4.8366  0.8001 -1.2139  0.0809  0.1528]\n",
      "MSE loss: 86.4153\n",
      "Iteration: 47700\n",
      "Gradient: [  2.0976 -36.7021 -28.7463 -66.1927 184.0803]\n",
      "Weights: [-4.8343  0.8012 -1.2135  0.0802  0.1528]\n",
      "MSE loss: 86.3975\n",
      "Iteration: 47800\n",
      "Gradient: [  1.3163 -13.5617 -16.1693 -66.4456 199.6375]\n",
      "Weights: [-4.8273  0.7945 -1.211   0.0804  0.1527]\n",
      "MSE loss: 86.3654\n",
      "Iteration: 47900\n",
      "Gradient: [   4.863    -4.7235   -8.4529 -133.024   115.4236]\n",
      "Weights: [-4.8263  0.7962 -1.2091  0.0795  0.1525]\n",
      "MSE loss: 86.3876\n",
      "Iteration: 48000\n",
      "Gradient: [ 15.9661   4.8239 -19.9826  77.7097 105.3745]\n",
      "Weights: [-4.8207  0.7972 -1.2074  0.0796  0.1524]\n",
      "MSE loss: 86.6194\n",
      "Iteration: 48100\n",
      "Gradient: [   1.7468   -6.3536   19.2141 -120.8937  -20.9233]\n",
      "Weights: [-4.8245  0.7936 -1.2065  0.0806  0.1521]\n",
      "MSE loss: 86.4331\n",
      "Iteration: 48200\n",
      "Gradient: [ -6.38    -1.8202 -15.1941  -6.2457  41.8058]\n",
      "Weights: [-4.8309  0.8059 -1.2086  0.0806  0.152 ]\n",
      "MSE loss: 86.6125\n",
      "Iteration: 48300\n",
      "Gradient: [  -5.0883   19.8498  -31.2266 -100.3708   36.1601]\n",
      "Weights: [-4.8377  0.8139 -1.2092  0.079   0.152 ]\n",
      "MSE loss: 86.4987\n",
      "Iteration: 48400\n",
      "Gradient: [  3.6892 -31.0949  29.4691 -18.2511 188.9655]\n",
      "Weights: [-4.8372  0.8136 -1.2122  0.0793  0.1525]\n",
      "MSE loss: 86.4256\n",
      "Iteration: 48500\n",
      "Gradient: [ -7.988   11.0042  49.0226 -60.836  202.3802]\n",
      "Weights: [-4.8292  0.8046 -1.2132  0.0804  0.1526]\n",
      "MSE loss: 86.3958\n",
      "Iteration: 48600\n",
      "Gradient: [  -5.0678  -11.6392  -26.9651  -58.5994 -380.3099]\n",
      "Weights: [-4.8521  0.804  -1.2111  0.081   0.1522]\n",
      "MSE loss: 86.8058\n",
      "Iteration: 48700\n",
      "Gradient: [ -13.4901  -12.9486    3.0141   38.688  -468.4415]\n",
      "Weights: [-4.8143  0.7906 -1.2127  0.0811  0.1522]\n",
      "MSE loss: 86.6662\n",
      "Iteration: 48800\n",
      "Gradient: [ -2.3307  17.4314   7.9509 317.85   520.4625]\n",
      "Weights: [-4.8241  0.79   -1.2097  0.0823  0.1523]\n",
      "MSE loss: 86.522\n",
      "Iteration: 48900\n",
      "Gradient: [ -9.1743  -3.8819 -27.4874   7.7273  72.7486]\n",
      "Weights: [-4.8396  0.7883 -1.2052  0.0819  0.1519]\n",
      "MSE loss: 86.4642\n",
      "Iteration: 49000\n",
      "Gradient: [-10.4101  10.2837  -9.456  -28.2886 -74.1098]\n",
      "Weights: [-4.8327  0.7843 -1.2054  0.0821  0.1518]\n",
      "MSE loss: 86.4336\n",
      "Iteration: 49100\n",
      "Gradient: [-12.3527  38.3116  70.6514 136.6061 443.9949]\n",
      "Weights: [-4.8186  0.7778 -1.2045  0.083   0.1516]\n",
      "MSE loss: 86.3439\n",
      "Iteration: 49200\n",
      "Gradient: [  5.2731  -0.9887  73.9998 -51.312  118.1541]\n",
      "Weights: [-4.8316  0.7964 -1.206   0.0825  0.1514]\n",
      "MSE loss: 86.3957\n",
      "Iteration: 49300\n",
      "Gradient: [ 10.8851   6.353   67.3657 119.0476 236.482 ]\n",
      "Weights: [-4.8114  0.7802 -1.2048  0.084   0.1513]\n",
      "MSE loss: 86.7961\n",
      "Iteration: 49400\n",
      "Gradient: [-8.520000e-02 -2.018000e-01 -3.295210e+01 -8.395600e+01  2.224453e+02]\n",
      "Weights: [-4.8297  0.7805 -1.207   0.0847  0.1512]\n",
      "MSE loss: 86.3929\n",
      "Iteration: 49500\n",
      "Gradient: [  18.4691    7.9754   36.7092    8.3712 -222.4465]\n",
      "Weights: [-4.8287  0.794  -1.2081  0.0844  0.1508]\n",
      "MSE loss: 86.3248\n",
      "Iteration: 49600\n",
      "Gradient: [  -5.8503   -3.6945  -46.3365  -15.2794 -362.7734]\n",
      "Weights: [-4.8426  0.8029 -1.21    0.085   0.1506]\n",
      "MSE loss: 86.3299\n",
      "Iteration: 49700\n",
      "Gradient: [-1.57969e+01 -1.36919e+01  4.72240e+00 -8.81000e-02  2.37498e+02]\n",
      "Weights: [-4.8272  0.7821 -1.2066  0.0846  0.1509]\n",
      "MSE loss: 86.4359\n",
      "Iteration: 49800\n",
      "Gradient: [-1.900000e-03 -2.114360e+01 -3.832580e+01 -2.253123e+02 -2.402250e+01]\n",
      "Weights: [-4.822   0.7865 -1.2052  0.0834  0.1511]\n",
      "MSE loss: 86.3555\n",
      "Iteration: 49900\n",
      "Gradient: [  -0.7384  -21.8305  -11.611   -19.4435 -164.7535]\n",
      "Weights: [-4.8309  0.783  -1.2069  0.084   0.1511]\n",
      "MSE loss: 86.5165\n",
      "Iteration: 50000\n",
      "Gradient: [  8.8189  18.7781  26.8007  97.5679 107.6582]\n",
      "Weights: [-4.8166  0.793  -1.2101  0.0841  0.1513]\n",
      "MSE loss: 86.6064\n",
      "Iteration: 50100\n",
      "Gradient: [ 14.8109 -25.531  -62.1339 -84.7503 220.9602]\n",
      "Weights: [-4.8268  0.7943 -1.2129  0.0847  0.1512]\n",
      "MSE loss: 86.326\n",
      "Iteration: 50200\n",
      "Gradient: [ -16.47     -8.483   -16.0593 -428.4959 -157.8792]\n",
      "Weights: [-4.8464  0.7979 -1.2129  0.0856  0.151 ]\n",
      "MSE loss: 86.5561\n",
      "Iteration: 50300\n",
      "Gradient: [  6.9615  28.0898  46.0584 183.7255 621.5321]\n",
      "Weights: [-4.8166  0.7844 -1.211   0.0871  0.1507]\n",
      "MSE loss: 86.4188\n",
      "Iteration: 50400\n",
      "Gradient: [ 2.180900e+00  3.550000e-02 -2.317940e+01 -5.679080e+01  1.050062e+02]\n",
      "Weights: [-4.8309  0.7868 -1.2093  0.086   0.1508]\n",
      "MSE loss: 86.2997\n",
      "Iteration: 50500\n",
      "Gradient: [   5.735    12.0379  -15.8061  -24.0974 -245.0868]\n",
      "Weights: [-4.8267  0.7821 -1.208   0.0862  0.1505]\n",
      "MSE loss: 86.3703\n",
      "Iteration: 50600\n",
      "Gradient: [ 1.536800e+00 -1.651950e+01  7.100000e-03 -3.754470e+01 -2.625138e+02]\n",
      "Weights: [-4.8337  0.7889 -1.2101  0.0868  0.1502]\n",
      "MSE loss: 86.4577\n",
      "Iteration: 50700\n",
      "Gradient: [ -4.8426  19.8009   5.6246 -95.7096 441.9373]\n",
      "Weights: [-4.8355  0.7901 -1.2066  0.0867  0.1498]\n",
      "MSE loss: 86.397\n",
      "Iteration: 50800\n",
      "Gradient: [  14.4448   23.6583  -15.0601   74.3213 -143.1063]\n",
      "Weights: [-4.8218  0.7894 -1.2092  0.0875  0.15  ]\n",
      "MSE loss: 86.371\n",
      "Iteration: 50900\n",
      "Gradient: [  11.2665   11.2023   24.6391 -106.2599  -64.8709]\n",
      "Weights: [-4.8344  0.7955 -1.2101  0.0878  0.1501]\n",
      "MSE loss: 86.3377\n",
      "Iteration: 51000\n",
      "Gradient: [  0.8241  21.9312  46.0534 -33.3003 -64.8223]\n",
      "Weights: [-4.8283  0.7912 -1.2104  0.0877  0.1499]\n",
      "MSE loss: 86.2928\n",
      "Iteration: 51100\n",
      "Gradient: [ 13.4533  11.9545  16.2812  10.9836 522.0648]\n",
      "Weights: [-4.8252  0.7961 -1.2121  0.0878  0.1502]\n",
      "MSE loss: 86.4611\n",
      "Iteration: 51200\n",
      "Gradient: [ -6.7947  25.965   24.7128 201.2163 479.4449]\n",
      "Weights: [-4.843   0.7964 -1.2094  0.0871  0.1502]\n",
      "MSE loss: 86.3303\n",
      "Iteration: 51300\n",
      "Gradient: [  -8.033    -9.2375   12.6342 -116.1973 -114.366 ]\n",
      "Weights: [-4.823   0.7851 -1.2125  0.0881  0.1504]\n",
      "MSE loss: 86.2444\n",
      "Iteration: 51400\n",
      "Gradient: [ 12.8295  30.5484  29.4525 201.562  200.3476]\n",
      "Weights: [-4.813   0.786  -1.2131  0.0892  0.1503]\n",
      "MSE loss: 86.7615\n",
      "Iteration: 51500\n",
      "Gradient: [2.122000e-01 2.119080e+01 4.384290e+01 3.473300e+00 3.952077e+02]\n",
      "Weights: [-4.8335  0.7882 -1.2144  0.0899  0.1503]\n",
      "MSE loss: 86.329\n",
      "Iteration: 51600\n",
      "Gradient: [   1.6142   -8.621   -22.7612  -84.972  -392.7825]\n",
      "Weights: [-4.8225  0.7785 -1.2187  0.0906  0.1505]\n",
      "MSE loss: 86.4878\n",
      "Iteration: 51700\n",
      "Gradient: [ 21.1451   5.6556 -25.7031  90.7707 650.1308]\n",
      "Weights: [-4.8116  0.7878 -1.22    0.0904  0.1505]\n",
      "MSE loss: 86.4505\n",
      "Iteration: 51800\n",
      "Gradient: [ -11.147    -5.3656  -14.6434 -290.9016 -415.2075]\n",
      "Weights: [-4.8277  0.7844 -1.2188  0.0903  0.1501]\n",
      "MSE loss: 86.7853\n",
      "Iteration: 51900\n",
      "Gradient: [  5.3791   4.8735 -66.8216 -42.2351  78.5265]\n",
      "Weights: [-4.813   0.7763 -1.2184  0.0912  0.1504]\n",
      "MSE loss: 86.3058\n",
      "Iteration: 52000\n",
      "Gradient: [-17.3528  30.1142   2.8624 -49.6883 104.7252]\n",
      "Weights: [-4.8354  0.7948 -1.2178  0.0909  0.15  ]\n",
      "MSE loss: 86.2376\n",
      "Iteration: 52100\n",
      "Gradient: [  -6.7581  -28.0633 -138.1174  -78.6983 -327.6049]\n",
      "Weights: [-4.8278  0.7754 -1.2157  0.0902  0.1503]\n",
      "MSE loss: 86.8409\n",
      "Iteration: 52200\n",
      "Gradient: [ -13.3424    4.8914  -74.8407  -29.6166 -272.0562]\n",
      "Weights: [-4.8336  0.7865 -1.2162  0.0901  0.15  ]\n",
      "MSE loss: 86.5173\n",
      "Iteration: 52300\n",
      "Gradient: [  6.403  -12.8356  31.9067 -18.1977 -86.4499]\n",
      "Weights: [-4.8205  0.7799 -1.2158  0.0907  0.1499]\n",
      "MSE loss: 86.3496\n",
      "Iteration: 52400\n",
      "Gradient: [-1.7912  3.4203 36.9372 12.8585  6.1744]\n",
      "Weights: [-4.8276  0.79   -1.2159  0.0903  0.1499]\n",
      "MSE loss: 86.2043\n",
      "Iteration: 52500\n",
      "Gradient: [  11.8623  -21.3842   29.8629 -238.864   216.0014]\n",
      "Weights: [-4.8147  0.7866 -1.2178  0.0905  0.15  ]\n",
      "MSE loss: 86.3064\n",
      "Iteration: 52600\n",
      "Gradient: [   7.2718   -6.5333   14.2471 -105.4327  -32.0058]\n",
      "Weights: [-4.8253  0.7906 -1.2173  0.0907  0.1499]\n",
      "MSE loss: 86.2054\n",
      "Iteration: 52700\n",
      "Gradient: [   8.0238   24.5461   -3.6139 -120.912   314.8033]\n",
      "Weights: [-4.8263  0.7937 -1.2199  0.0913  0.15  ]\n",
      "MSE loss: 86.1987\n",
      "Iteration: 52800\n",
      "Gradient: [  8.9378  19.6212   9.7534  21.8324 146.7153]\n",
      "Weights: [-4.8083  0.7944 -1.2222  0.0916  0.1498]\n",
      "MSE loss: 86.6494\n",
      "Iteration: 52900\n",
      "Gradient: [  8.2966  22.8878   2.6302 -31.5341 147.7238]\n",
      "Weights: [-4.8403  0.8068 -1.2231  0.0916  0.1499]\n",
      "MSE loss: 86.213\n",
      "Iteration: 53000\n",
      "Gradient: [ -14.146     2.4463   16.7231 -129.9074  -77.8464]\n",
      "Weights: [-4.8338  0.8061 -1.2235  0.0917  0.1498]\n",
      "MSE loss: 86.199\n",
      "Iteration: 53100\n",
      "Gradient: [  -9.9995  -32.9242  -44.1642 -117.7904 -430.221 ]\n",
      "Weights: [-4.8513  0.8097 -1.2266  0.0918  0.1497]\n",
      "MSE loss: 86.9871\n",
      "Iteration: 53200\n",
      "Gradient: [ -5.5616  -8.3395  57.3971 -92.8024 -41.4072]\n",
      "Weights: [-4.8351  0.8135 -1.2277  0.0921  0.1499]\n",
      "MSE loss: 86.2177\n",
      "Iteration: 53300\n",
      "Gradient: [ 4.64300e-01 -1.39448e+01 -8.28806e+01 -2.23520e+02 -5.28766e+02]\n",
      "Weights: [-4.8313  0.805  -1.2277  0.0913  0.1503]\n",
      "MSE loss: 86.2311\n",
      "Iteration: 53400\n",
      "Gradient: [  10.7591   -5.2292   33.759    10.5709 -111.3029]\n",
      "Weights: [-4.8325  0.8064 -1.2254  0.0916  0.1499]\n",
      "MSE loss: 86.1916\n",
      "Iteration: 53500\n",
      "Gradient: [ -15.0435   -0.8034  -54.5459  -59.6857 -177.8299]\n",
      "Weights: [-4.8376  0.7973 -1.2215  0.0913  0.1501]\n",
      "MSE loss: 86.273\n",
      "Iteration: 53600\n",
      "Gradient: [ -3.0449   9.0273  56.4471  81.7297 323.3996]\n",
      "Weights: [-4.821   0.8021 -1.2239  0.0901  0.1505]\n",
      "MSE loss: 86.3469\n",
      "Iteration: 53700\n",
      "Gradient: [  18.5757   19.7117  -14.3053   57.4493 -253.2791]\n",
      "Weights: [-4.8114  0.8013 -1.2273  0.0919  0.1503]\n",
      "MSE loss: 86.6633\n",
      "Iteration: 53800\n",
      "Gradient: [  2.6019  -5.9832  -8.3889 151.9254 457.0502]\n",
      "Weights: [-4.8407  0.8074 -1.2256  0.0913  0.1506]\n",
      "MSE loss: 86.334\n",
      "Iteration: 53900\n",
      "Gradient: [  -8.4062    0.4917  -20.6743   42.1048 -146.9804]\n",
      "Weights: [-4.8233  0.7992 -1.2254  0.0902  0.1504]\n",
      "MSE loss: 86.3595\n",
      "Iteration: 54000\n",
      "Gradient: [ 10.7668  26.1323  36.3301 124.9889 302.8288]\n",
      "Weights: [-4.8267  0.8066 -1.2231  0.0905  0.1506]\n",
      "MSE loss: 86.9688\n",
      "Iteration: 54100\n",
      "Gradient: [ 16.0938 -21.3253 -15.3871 -12.0407 417.629 ]\n",
      "Weights: [-4.8338  0.802  -1.2265  0.0915  0.1504]\n",
      "MSE loss: 86.2214\n",
      "Iteration: 54200\n",
      "Gradient: [-10.2235  10.5578  -4.2241 -33.2677 158.104 ]\n",
      "Weights: [-4.8265  0.8092 -1.2297  0.0918  0.1503]\n",
      "MSE loss: 86.2353\n",
      "Iteration: 54300\n",
      "Gradient: [   6.6021    9.0067   -9.8722   82.9956 -217.1675]\n",
      "Weights: [-4.8336  0.8139 -1.2318  0.0925  0.1504]\n",
      "MSE loss: 86.226\n",
      "Iteration: 54400\n",
      "Gradient: [  2.7185   8.2069  27.6845 203.458   38.89  ]\n",
      "Weights: [-4.8419  0.8279 -1.2321  0.0918  0.1504]\n",
      "MSE loss: 86.6246\n",
      "Iteration: 54500\n",
      "Gradient: [ -10.1819   -0.8537   29.5491 -123.9055   72.4533]\n",
      "Weights: [-4.8536  0.8283 -1.2327  0.0912  0.1502]\n",
      "MSE loss: 86.3569\n",
      "Iteration: 54600\n",
      "Gradient: [-11.9762  -5.1902  27.0862 -24.9169 151.2342]\n",
      "Weights: [-4.8452  0.8276 -1.2351  0.092   0.1505]\n",
      "MSE loss: 86.2355\n",
      "Iteration: 54700\n",
      "Gradient: [ -2.4301  25.3606  -8.6894 -75.9256 -54.2598]\n",
      "Weights: [-4.8369  0.8201 -1.2374  0.0933  0.1504]\n",
      "MSE loss: 86.1583\n",
      "Iteration: 54800\n",
      "Gradient: [ 3.362  16.3924 36.3298 72.7467 98.7446]\n",
      "Weights: [-4.8365  0.8296 -1.2371  0.093   0.1503]\n",
      "MSE loss: 86.459\n",
      "Iteration: 54900\n",
      "Gradient: [-18.0583   0.7533  28.8515 -77.8135 -64.2716]\n",
      "Weights: [-4.8332  0.818  -1.2346  0.0927  0.1501]\n",
      "MSE loss: 86.2219\n",
      "Iteration: 55000\n",
      "Gradient: [  -6.8275   -7.7044  -47.3577    2.0131 -291.0462]\n",
      "Weights: [-4.8499  0.8151 -1.2351  0.0937  0.1503]\n",
      "MSE loss: 86.5308\n",
      "Iteration: 55100\n",
      "Gradient: [  -5.3069  -40.5361  -68.0258 -458.0701 -289.8817]\n",
      "Weights: [-4.8401  0.814  -1.2373  0.0929  0.1505]\n",
      "MSE loss: 86.6915\n",
      "Iteration: 55200\n",
      "Gradient: [ -21.0609  -34.1734  -18.5195 -165.1948    9.9983]\n",
      "Weights: [-4.8429  0.8151 -1.2367  0.0933  0.1503]\n",
      "MSE loss: 86.521\n",
      "Iteration: 55300\n",
      "Gradient: [-14.1662 -11.4528 -12.8945 -89.2329 -48.0436]\n",
      "Weights: [-4.8251  0.8065 -1.2354  0.0938  0.1503]\n",
      "MSE loss: 86.2221\n",
      "Iteration: 55400\n",
      "Gradient: [ -6.117  -13.9369  62.979  132.9067  48.5807]\n",
      "Weights: [-4.8268  0.809  -1.2328  0.0941  0.1499]\n",
      "MSE loss: 86.1897\n",
      "Iteration: 55500\n",
      "Gradient: [ -5.7226 -13.2096   1.696   32.0867 131.4119]\n",
      "Weights: [-4.8336  0.818  -1.235   0.0939  0.1499]\n",
      "MSE loss: 86.1647\n",
      "Iteration: 55600\n",
      "Gradient: [  1.352  -11.6583   2.3465  41.7116 -91.4296]\n",
      "Weights: [-4.8541  0.8266 -1.2343  0.0948  0.1493]\n",
      "MSE loss: 86.2886\n",
      "Iteration: 55700\n",
      "Gradient: [  0.3811  28.5639 -15.1457  27.3352 182.2426]\n",
      "Weights: [-4.8366  0.8283 -1.2369  0.0957  0.1492]\n",
      "MSE loss: 86.3785\n",
      "Iteration: 55800\n",
      "Gradient: [  8.3228  -5.2792 -29.0094 -57.8515  84.1251]\n",
      "Weights: [-4.8408  0.8199 -1.2395  0.0961  0.1496]\n",
      "MSE loss: 86.1764\n",
      "Iteration: 55900\n",
      "Gradient: [ 10.246   13.4031  11.9222 257.8352 538.8019]\n",
      "Weights: [-4.8378  0.8296 -1.2396  0.0959  0.1497]\n",
      "MSE loss: 86.4374\n",
      "Iteration: 56000\n",
      "Gradient: [  6.6856  -4.5982  -2.5307   7.0831 -52.897 ]\n",
      "Weights: [-4.8289  0.82   -1.2379  0.0956  0.1496]\n",
      "MSE loss: 86.2461\n",
      "Iteration: 56100\n",
      "Gradient: [ 15.0153  17.9641 -14.1094 -86.0843 310.889 ]\n",
      "Weights: [-4.8284  0.8142 -1.2373  0.096   0.1495]\n",
      "MSE loss: 86.1507\n",
      "Iteration: 56200\n",
      "Gradient: [ -10.482   -12.0125   23.3705 -186.1093  -58.5202]\n",
      "Weights: [-4.8571  0.8261 -1.2396  0.0959  0.1496]\n",
      "MSE loss: 86.5318\n",
      "Iteration: 56300\n",
      "Gradient: [  -1.7124   30.2322  -28.0084   50.732  -203.6618]\n",
      "Weights: [-4.8359  0.8202 -1.2372  0.0959  0.1495]\n",
      "MSE loss: 86.1647\n",
      "Iteration: 56400\n",
      "Gradient: [  -2.6583   -1.0689   21.316  -134.0727 -257.0853]\n",
      "Weights: [-4.8324  0.8138 -1.2345  0.0948  0.1495]\n",
      "MSE loss: 86.1807\n",
      "Iteration: 56500\n",
      "Gradient: [ 13.6742  15.6828 -52.7173 -95.6384  -7.3491]\n",
      "Weights: [-4.8228  0.8182 -1.2348  0.0942  0.1498]\n",
      "MSE loss: 86.545\n",
      "Iteration: 56600\n",
      "Gradient: [  1.4339  -3.7292  29.1604 -41.3614 217.2504]\n",
      "Weights: [-4.8275  0.8129 -1.2367  0.0954  0.1498]\n",
      "MSE loss: 86.1736\n",
      "Iteration: 56700\n",
      "Gradient: [   1.3439  -22.0001    7.0691   35.6638 -224.6127]\n",
      "Weights: [-4.8254  0.8131 -1.2375  0.0962  0.1496]\n",
      "MSE loss: 86.202\n",
      "Iteration: 56800\n",
      "Gradient: [ 16.541   17.9521  66.7251  23.3374 -10.4643]\n",
      "Weights: [-4.8373  0.8276 -1.2402  0.0955  0.1497]\n",
      "MSE loss: 86.1778\n",
      "Iteration: 56900\n",
      "Gradient: [-2.079470e+01 -3.775000e-01  6.754790e+01 -1.755738e+02 -5.777250e+02]\n",
      "Weights: [-4.8486  0.8298 -1.2421  0.0966  0.1494]\n",
      "MSE loss: 86.2069\n",
      "Iteration: 57000\n",
      "Gradient: [  5.7332   8.8189  25.1733  -2.763  581.6046]\n",
      "Weights: [-4.8361  0.8117 -1.2321  0.0963  0.1493]\n",
      "MSE loss: 86.2447\n",
      "Iteration: 57100\n",
      "Gradient: [ -5.0625   5.014   -5.1059 -79.5167  70.2695]\n",
      "Weights: [-4.84    0.8088 -1.2325  0.0958  0.1493]\n",
      "MSE loss: 86.2207\n",
      "Iteration: 57200\n",
      "Gradient: [ -18.6847  -18.0619    6.505  -120.8561 -107.6402]\n",
      "Weights: [-4.8556  0.8192 -1.2325  0.0964  0.1492]\n",
      "MSE loss: 86.3729\n",
      "Iteration: 57300\n",
      "Gradient: [   8.6292   36.2362   18.5251   79.428  -111.8214]\n",
      "Weights: [-4.8253  0.8111 -1.2334  0.0963  0.1492]\n",
      "MSE loss: 86.3584\n",
      "Iteration: 57400\n",
      "Gradient: [ -7.0403 -17.0948 -48.9963  75.0797 163.0084]\n",
      "Weights: [-4.8319  0.8094 -1.2333  0.0961  0.1494]\n",
      "MSE loss: 86.1254\n",
      "Iteration: 57500\n",
      "Gradient: [ -6.203  -23.1122 -49.4686  92.2743 163.0112]\n",
      "Weights: [-4.8434  0.8052 -1.232   0.0967  0.1491]\n",
      "MSE loss: 86.3657\n",
      "Iteration: 57600\n",
      "Gradient: [   9.4492  -10.8308   54.1513  141.5149 -349.1865]\n",
      "Weights: [-4.8262  0.798  -1.2292  0.0976  0.1491]\n",
      "MSE loss: 86.4662\n",
      "Iteration: 57700\n",
      "Gradient: [   4.4929   19.8827   29.0987   29.9685 -103.4085]\n",
      "Weights: [-4.8154  0.787  -1.2291  0.0968  0.1494]\n",
      "MSE loss: 86.2007\n",
      "Iteration: 57800\n",
      "Gradient: [ -4.505    6.456    1.8843 -68.1088 -54.6388]\n",
      "Weights: [-4.8226  0.7899 -1.2302  0.0963  0.1495]\n",
      "MSE loss: 86.2269\n",
      "Iteration: 57900\n",
      "Gradient: [   3.6711    3.8174   16.231  -140.545  -221.0868]\n",
      "Weights: [-4.8195  0.7957 -1.2315  0.0969  0.1493]\n",
      "MSE loss: 86.1854\n",
      "Iteration: 58000\n",
      "Gradient: [  2.9501  43.6948  27.6407 -32.889  462.5199]\n",
      "Weights: [-4.8128  0.7952 -1.2318  0.0981  0.149 ]\n",
      "MSE loss: 86.481\n",
      "Iteration: 58100\n",
      "Gradient: [ 16.0696   5.4389  65.4874 -26.2294 -58.8122]\n",
      "Weights: [-4.8213  0.7969 -1.2339  0.0982  0.1491]\n",
      "MSE loss: 86.1404\n",
      "Iteration: 58200\n",
      "Gradient: [ 10.5781  29.1426  45.5933 243.8556 335.0248]\n",
      "Weights: [-4.8337  0.8172 -1.2385  0.0985  0.1489]\n",
      "MSE loss: 86.1725\n",
      "Iteration: 58300\n",
      "Gradient: [  -0.5742   16.1803   -5.0575 -102.1108  -43.3806]\n",
      "Weights: [-4.8387  0.814  -1.2365  0.0973  0.149 ]\n",
      "MSE loss: 86.1695\n",
      "Iteration: 58400\n",
      "Gradient: [ -2.4194 -28.3067 -90.2624 -35.2771 -80.7176]\n",
      "Weights: [-4.8391  0.8104 -1.2376  0.0972  0.1493]\n",
      "MSE loss: 86.2617\n",
      "Iteration: 58500\n",
      "Gradient: [   3.9626   34.0571   54.0668   50.9291 -138.1322]\n",
      "Weights: [-4.8284  0.8111 -1.2365  0.0971  0.1494]\n",
      "MSE loss: 86.2069\n",
      "Iteration: 58600\n",
      "Gradient: [   1.0154   14.114    -0.4145  129.4498 -121.041 ]\n",
      "Weights: [-4.8375  0.8129 -1.2339  0.0976  0.1489]\n",
      "MSE loss: 86.1755\n",
      "Iteration: 58700\n",
      "Gradient: [ 11.1883  16.3327  -7.0086 -27.8805  24.7733]\n",
      "Weights: [-4.831   0.8075 -1.2347  0.0985  0.1488]\n",
      "MSE loss: 86.1228\n",
      "Iteration: 58800\n",
      "Gradient: [ 2.201000e+00  1.703780e+01  3.798030e+01 -3.952000e-01  6.762766e+02]\n",
      "Weights: [-4.8193  0.8002 -1.2385  0.0997  0.149 ]\n",
      "MSE loss: 86.16\n",
      "Iteration: 58900\n",
      "Gradient: [  0.5667  32.947   36.9922  62.2001 455.3725]\n",
      "Weights: [-4.8165  0.8042 -1.2385  0.1002  0.1489]\n",
      "MSE loss: 86.5141\n",
      "Iteration: 59000\n",
      "Gradient: [ 14.5889  -8.9985  18.3589  33.8241 194.9849]\n",
      "Weights: [-4.8128  0.7944 -1.2346  0.1     0.1487]\n",
      "MSE loss: 86.3712\n",
      "Iteration: 59100\n",
      "Gradient: [  14.7272   34.1498   33.8943   95.6341 -236.6119]\n",
      "Weights: [-4.8232  0.8093 -1.2354  0.1002  0.1482]\n",
      "MSE loss: 86.4342\n",
      "Iteration: 59200\n",
      "Gradient: [   4.6264    1.3279  -39.7401   -1.0271 -370.0178]\n",
      "Weights: [-4.8255  0.7969 -1.2339  0.1005  0.1481]\n",
      "MSE loss: 86.1108\n",
      "Iteration: 59300\n",
      "Gradient: [ 18.5447  -0.4513  71.6861 -39.5709  -0.117 ]\n",
      "Weights: [-4.8212  0.7994 -1.234   0.1006  0.148 ]\n",
      "MSE loss: 86.1239\n",
      "Iteration: 59400\n",
      "Gradient: [ -8.9787  20.0437   4.3275 192.1225 257.655 ]\n",
      "Weights: [-4.8535  0.818  -1.2365  0.1008  0.148 ]\n",
      "MSE loss: 86.2625\n",
      "Iteration: 59500\n",
      "Gradient: [ -20.428   -11.4655  -37.5155 -159.0795 -260.2216]\n",
      "Weights: [-4.8547  0.8251 -1.2393  0.1     0.1479]\n",
      "MSE loss: 86.3914\n",
      "Iteration: 59600\n",
      "Gradient: [ -16.6981  -28.7396  -32.8312 -152.6935 -172.6465]\n",
      "Weights: [-4.8478  0.812  -1.2407  0.1013  0.1481]\n",
      "MSE loss: 86.6598\n",
      "Iteration: 59700\n",
      "Gradient: [ -11.8316  -26.5021  -78.0068   26.5095 -446.4235]\n",
      "Weights: [-4.843   0.8183 -1.2419  0.1016  0.1481]\n",
      "MSE loss: 86.0946\n",
      "Iteration: 59800\n",
      "Gradient: [  1.3511  13.6187   8.5746 -25.4225 327.5981]\n",
      "Weights: [-4.8395  0.8197 -1.2402  0.1009  0.1481]\n",
      "MSE loss: 86.0774\n",
      "Iteration: 59900\n",
      "Gradient: [  9.3742   0.5799  23.1937  -0.3051 -83.7486]\n",
      "Weights: [-4.837   0.8218 -1.2415  0.101   0.1479]\n",
      "MSE loss: 86.1427\n",
      "Iteration: 60000\n",
      "Gradient: [  6.0322  -6.5682   1.5331  66.1331 212.0661]\n",
      "Weights: [-4.8278  0.8176 -1.2447  0.1027  0.1481]\n",
      "MSE loss: 86.1753\n",
      "Iteration: 60100\n",
      "Gradient: [  7.8751  -7.5126 -43.7496  -1.3411  98.8166]\n",
      "Weights: [-4.8416  0.817  -1.245   0.1025  0.1483]\n",
      "MSE loss: 86.0847\n",
      "Iteration: 60200\n",
      "Gradient: [   9.564   -20.1462  -48.1334  -79.6357 -168.5891]\n",
      "Weights: [-4.8394  0.8225 -1.2501  0.1031  0.1482]\n",
      "MSE loss: 86.1289\n",
      "Iteration: 60300\n",
      "Gradient: [ -11.7629   -5.6101  -59.8663   46.1121 -238.0807]\n",
      "Weights: [-4.8381  0.828  -1.2477  0.1013  0.1483]\n",
      "MSE loss: 86.0831\n",
      "Iteration: 60400\n",
      "Gradient: [  -2.8512  -24.3743  -30.1724  -64.0276 -204.5778]\n",
      "Weights: [-4.8417  0.819  -1.2429  0.1008  0.1484]\n",
      "MSE loss: 86.1185\n",
      "Iteration: 60500\n",
      "Gradient: [  3.4684   5.2712 -15.9752  45.009  100.9939]\n",
      "Weights: [-4.8266  0.8207 -1.2451  0.1005  0.1486]\n",
      "MSE loss: 86.1649\n",
      "Iteration: 60600\n",
      "Gradient: [  4.9566   4.1231 -13.5459 -40.5684 -99.3605]\n",
      "Weights: [-4.8389  0.8238 -1.245   0.1007  0.1487]\n",
      "MSE loss: 86.0876\n",
      "Iteration: 60700\n",
      "Gradient: [ -15.2965   -4.2277  -26.4368  -61.2606 -303.8926]\n",
      "Weights: [-4.8414  0.8284 -1.2473  0.101   0.1485]\n",
      "MSE loss: 86.0589\n",
      "Iteration: 60800\n",
      "Gradient: [  -8.1021   -7.4848  -17.9562  -92.0392 -109.444 ]\n",
      "Weights: [-4.854   0.8307 -1.2504  0.1024  0.1485]\n",
      "MSE loss: 86.2745\n",
      "Iteration: 60900\n",
      "Gradient: [ -1.0959 -30.8158 -40.0821 139.6841 -92.4221]\n",
      "Weights: [-4.8481  0.8441 -1.2524  0.1014  0.1483]\n",
      "MSE loss: 86.1323\n",
      "Iteration: 61000\n",
      "Gradient: [  -1.3876   18.7582   46.1475   96.5017 -189.848 ]\n",
      "Weights: [-4.8506  0.8413 -1.2499  0.1006  0.1486]\n",
      "MSE loss: 86.1229\n",
      "Iteration: 61100\n",
      "Gradient: [ -1.4123  12.8037 -33.1587   8.3935 515.2546]\n",
      "Weights: [-4.8277  0.8275 -1.2545  0.1022  0.149 ]\n",
      "MSE loss: 86.1101\n",
      "Iteration: 61200\n",
      "Gradient: [ 20.4095 -14.6252  45.2883 171.9865  82.5576]\n",
      "Weights: [-4.8241  0.8309 -1.2544  0.1015  0.1492]\n",
      "MSE loss: 86.4065\n",
      "Iteration: 61300\n",
      "Gradient: [ -11.0217  -23.947   -18.6495   46.1964 -113.2771]\n",
      "Weights: [-4.8281  0.8259 -1.2531  0.101   0.1493]\n",
      "MSE loss: 86.0972\n",
      "Iteration: 61400\n",
      "Gradient: [  -9.2402  -30.2995  -29.6542 -107.9583  -17.4967]\n",
      "Weights: [-4.8541  0.8307 -1.2528  0.101   0.1492]\n",
      "MSE loss: 86.5825\n",
      "Iteration: 61500\n",
      "Gradient: [  -4.2813   20.4376   11.6054  -47.3408 -206.3882]\n",
      "Weights: [-4.8405  0.8362 -1.254   0.1017  0.149 ]\n",
      "MSE loss: 86.0661\n",
      "Iteration: 61600\n",
      "Gradient: [ -3.2603  -2.4154 -34.5648 -37.079  345.2521]\n",
      "Weights: [-4.8505  0.8479 -1.2598  0.1025  0.1489]\n",
      "MSE loss: 86.0612\n",
      "Iteration: 61700\n",
      "Gradient: [  3.3857 -11.3059  -9.0717 -45.0845 108.576 ]\n",
      "Weights: [-4.8316  0.8401 -1.2641  0.1039  0.1492]\n",
      "MSE loss: 86.1071\n",
      "Iteration: 61800\n",
      "Gradient: [ -11.4109   -4.7713  -27.0628 -154.5129   75.3758]\n",
      "Weights: [-4.8543  0.8533 -1.264   0.1042  0.1485]\n",
      "MSE loss: 86.1101\n",
      "Iteration: 61900\n",
      "Gradient: [   5.6199  -18.5658  -58.7368 -125.8813 -184.1524]\n",
      "Weights: [-4.8419  0.8327 -1.2577  0.1031  0.1488]\n",
      "MSE loss: 86.1961\n",
      "Iteration: 62000\n",
      "Gradient: [   8.474    -7.4006  -23.0903  -97.6343 -427.1547]\n",
      "Weights: [-4.838   0.8434 -1.2597  0.1027  0.1487]\n",
      "MSE loss: 86.0857\n",
      "Iteration: 62100\n",
      "Gradient: [ -3.8481  -7.5722 -15.9392 -42.9552 131.2978]\n",
      "Weights: [-4.8469  0.8333 -1.2566  0.1038  0.1489]\n",
      "MSE loss: 86.1205\n",
      "Iteration: 62200\n",
      "Gradient: [  -4.4465   -3.5431   34.774  -165.1644  276.5408]\n",
      "Weights: [-4.8385  0.8323 -1.2609  0.1047  0.1487]\n",
      "MSE loss: 86.0827\n",
      "Iteration: 62300\n",
      "Gradient: [  8.0948  29.4811  37.4292  62.0828 262.3428]\n",
      "Weights: [-4.8336  0.8412 -1.2643  0.1048  0.1491]\n",
      "MSE loss: 86.2349\n",
      "Iteration: 62400\n",
      "Gradient: [ -1.0117  10.8265  39.3372  -8.9352 344.8232]\n",
      "Weights: [-4.8403  0.8561 -1.2666  0.1036  0.1489]\n",
      "MSE loss: 86.1631\n",
      "Iteration: 62500\n",
      "Gradient: [  7.0795  28.2437  25.7229  79.0451 128.0143]\n",
      "Weights: [-4.8479  0.8512 -1.2635  0.1033  0.149 ]\n",
      "MSE loss: 86.0392\n",
      "Iteration: 62600\n",
      "Gradient: [  -1.749    -7.2088  -39.79      5.8069 -319.0646]\n",
      "Weights: [-4.8451  0.852  -1.2626  0.103   0.1487]\n",
      "MSE loss: 86.0689\n",
      "Iteration: 62700\n",
      "Gradient: [  3.1025 -14.6725  36.555   26.8314 -66.5435]\n",
      "Weights: [-4.8322  0.8327 -1.2574  0.1026  0.1489]\n",
      "MSE loss: 86.0574\n",
      "Iteration: 62800\n",
      "Gradient: [   1.7498   -5.5195   -2.5361 -106.777    73.0959]\n",
      "Weights: [-4.8406  0.8476 -1.2656  0.1032  0.1493]\n",
      "MSE loss: 86.0385\n",
      "Iteration: 62900\n",
      "Gradient: [   6.5769    1.6378    7.4897 -110.1623 -404.3719]\n",
      "Weights: [-4.831   0.8417 -1.2683  0.1038  0.1494]\n",
      "MSE loss: 86.153\n",
      "Iteration: 63000\n",
      "Gradient: [ 16.832   -0.3568  -2.2234 156.0211  82.2898]\n",
      "Weights: [-4.8261  0.8382 -1.267   0.1044  0.1494]\n",
      "MSE loss: 86.1564\n",
      "Iteration: 63100\n",
      "Gradient: [  0.1339  -5.1431 -15.1886  76.0317 -43.1219]\n",
      "Weights: [-4.8473  0.8476 -1.2678  0.1047  0.1493]\n",
      "MSE loss: 86.0624\n",
      "Iteration: 63200\n",
      "Gradient: [  -5.4184   -3.2017  -35.3589  -76.6116 -261.3302]\n",
      "Weights: [-4.8455  0.8435 -1.2685  0.1059  0.1489]\n",
      "MSE loss: 86.1193\n",
      "Iteration: 63300\n",
      "Gradient: [  9.4237   3.8189 -50.9678 128.8582 -62.803 ]\n",
      "Weights: [-4.8413  0.8508 -1.2686  0.1057  0.1486]\n",
      "MSE loss: 86.0052\n",
      "Iteration: 63400\n",
      "Gradient: [  7.5148  18.1222 -13.4889   7.2664  87.6326]\n",
      "Weights: [-4.8293  0.8303 -1.2603  0.1054  0.1487]\n",
      "MSE loss: 86.1451\n",
      "Iteration: 63500\n",
      "Gradient: [  1.4347  -9.6878 -58.7485 -82.2074 -11.6923]\n",
      "Weights: [-4.8376  0.837  -1.2617  0.1057  0.1484]\n",
      "MSE loss: 86.0035\n",
      "Iteration: 63600\n",
      "Gradient: [ -4.883  -15.8114  -3.9501 -96.149  153.5998]\n",
      "Weights: [-4.8556  0.8462 -1.2621  0.1047  0.1482]\n",
      "MSE loss: 86.4323\n",
      "Iteration: 63700\n",
      "Gradient: [ -5.5583  35.6699  25.3463 162.7638  85.6302]\n",
      "Weights: [-4.8441  0.8468 -1.2613  0.1051  0.1482]\n",
      "MSE loss: 86.0555\n",
      "Iteration: 63800\n",
      "Gradient: [ -9.3043   1.1964  -1.8579 174.9039 -98.9338]\n",
      "Weights: [-4.8483  0.849  -1.2629  0.1052  0.1485]\n",
      "MSE loss: 86.1238\n",
      "Iteration: 63900\n",
      "Gradient: [  -1.2542   30.1646   15.5    -122.8692 -624.5078]\n",
      "Weights: [-4.8389  0.8396 -1.2621  0.1045  0.1486]\n",
      "MSE loss: 86.0157\n",
      "Iteration: 64000\n",
      "Gradient: [ -14.8156   -4.8469  -43.3974  162.6665 -132.8149]\n",
      "Weights: [-4.8427  0.8339 -1.2587  0.1046  0.1485]\n",
      "MSE loss: 86.0319\n",
      "Iteration: 64100\n",
      "Gradient: [  -1.6206   17.1375   20.9248 -172.036  -102.826 ]\n",
      "Weights: [-4.8578  0.8376 -1.2573  0.1041  0.1486]\n",
      "MSE loss: 86.3434\n",
      "Iteration: 64200\n",
      "Gradient: [ -13.1355  -25.5131   -4.132   114.8801 -165.3751]\n",
      "Weights: [-4.8404  0.8273 -1.2539  0.1035  0.1485]\n",
      "MSE loss: 86.0485\n",
      "Iteration: 64300\n",
      "Gradient: [ 11.2831  15.9166   7.6757  32.5024 -25.3528]\n",
      "Weights: [-4.8252  0.8236 -1.2506  0.1031  0.1484]\n",
      "MSE loss: 86.2292\n",
      "Iteration: 64400\n",
      "Gradient: [   2.6958  -44.069     2.5343   10.3832 -233.505 ]\n",
      "Weights: [-4.8335  0.823  -1.2511  0.1033  0.1482]\n",
      "MSE loss: 86.0806\n",
      "Iteration: 64500\n",
      "Gradient: [-14.6563 -12.6609  -2.6986 -78.5381 -32.7758]\n",
      "Weights: [-4.856   0.8337 -1.2509  0.104   0.1481]\n",
      "MSE loss: 86.1949\n",
      "Iteration: 64600\n",
      "Gradient: [ 10.6701 -14.6027 -53.622  -89.6221 205.7987]\n",
      "Weights: [-4.8264  0.8334 -1.2566  0.1041  0.1482]\n",
      "MSE loss: 86.2224\n",
      "Iteration: 64700\n",
      "Gradient: [  4.3308  -3.3555  37.4307  75.8409 376.8092]\n",
      "Weights: [-4.8506  0.85   -1.2569  0.1041  0.148 ]\n",
      "MSE loss: 86.2112\n",
      "Iteration: 64800\n",
      "Gradient: [  -1.6323    8.693   -67.7256  -29.2494 -100.832 ]\n",
      "Weights: [-4.8477  0.8447 -1.2579  0.1042  0.1479]\n",
      "MSE loss: 86.1359\n",
      "Iteration: 64900\n",
      "Gradient: [  6.2247  19.791   60.7395  31.5341 348.1349]\n",
      "Weights: [-4.8251  0.8387 -1.2607  0.1054  0.148 ]\n",
      "MSE loss: 86.3391\n",
      "Iteration: 65000\n",
      "Gradient: [  3.5777  35.2613 -96.9116  82.8766 319.5516]\n",
      "Weights: [-4.8392  0.8409 -1.2601  0.1066  0.1478]\n",
      "MSE loss: 86.1426\n",
      "Iteration: 65100\n",
      "Gradient: [ 10.0772  37.3217  85.0307 120.6312 410.6378]\n",
      "Weights: [-4.8332  0.8439 -1.2616  0.1069  0.1476]\n",
      "MSE loss: 86.2929\n",
      "Iteration: 65200\n",
      "Gradient: [   4.0557  -24.7509   -0.8579   24.8525 -435.7458]\n",
      "Weights: [-4.842   0.8399 -1.263   0.1063  0.1478]\n",
      "MSE loss: 86.2138\n",
      "Iteration: 65300\n",
      "Gradient: [   9.9334   -9.4098  -19.567    15.3861 -128.2578]\n",
      "Weights: [-4.8385  0.8319 -1.2622  0.1069  0.1481]\n",
      "MSE loss: 86.014\n",
      "Iteration: 65400\n",
      "Gradient: [ -11.9793  -24.3256   11.3817   51.2381 -492.3167]\n",
      "Weights: [-4.837   0.8372 -1.2638  0.1062  0.1484]\n",
      "MSE loss: 85.9817\n",
      "Iteration: 65500\n",
      "Gradient: [  -3.2302  -17.3852 -128.682  -170.8148 -401.1508]\n",
      "Weights: [-4.846   0.8392 -1.2622  0.1053  0.1483]\n",
      "MSE loss: 86.1512\n",
      "Iteration: 65600\n",
      "Gradient: [  -6.1068   -5.1879  -46.3642  -32.6067 -223.3591]\n",
      "Weights: [-4.8464  0.8407 -1.261   0.1052  0.1483]\n",
      "MSE loss: 86.0144\n",
      "Iteration: 65700\n",
      "Gradient: [ 13.8017  -8.0329 -27.262   -4.1213 127.5833]\n",
      "Weights: [-4.8522  0.8482 -1.2619  0.1048  0.1483]\n",
      "MSE loss: 86.0455\n",
      "Iteration: 65800\n",
      "Gradient: [ -2.3393  -4.9969  39.6516  23.4126 -94.3545]\n",
      "Weights: [-4.8508  0.848  -1.2627  0.1046  0.1483]\n",
      "MSE loss: 86.141\n",
      "Iteration: 65900\n",
      "Gradient: [  10.5696  -13.256     4.5063 -106.0164   29.0611]\n",
      "Weights: [-4.8435  0.8515 -1.2645  0.1051  0.1481]\n",
      "MSE loss: 86.0841\n",
      "Iteration: 66000\n",
      "Gradient: [ 22.0563  19.2771  34.5612   2.3192 267.2655]\n",
      "Weights: [-4.8413  0.8485 -1.263   0.1053  0.1482]\n",
      "MSE loss: 86.1\n",
      "Iteration: 66100\n",
      "Gradient: [ 14.1327   5.1216 -46.3718  91.1033  70.9481]\n",
      "Weights: [-4.828   0.8326 -1.2641  0.1058  0.1484]\n",
      "MSE loss: 86.1093\n",
      "Iteration: 66200\n",
      "Gradient: [  -4.9092    1.804   -31.9275 -113.7976  172.769 ]\n",
      "Weights: [-4.843   0.8418 -1.2647  0.1055  0.1486]\n",
      "MSE loss: 86.0091\n",
      "Iteration: 66300\n",
      "Gradient: [ 20.2284  -2.2976  42.3962  12.6187 -41.7914]\n",
      "Weights: [-4.7999  0.8218 -1.2655  0.1066  0.1484]\n",
      "MSE loss: 86.7816\n",
      "Iteration: 66400\n",
      "Gradient: [  2.534   15.3761  15.428   -1.4967 -73.3934]\n",
      "Weights: [-4.8244  0.8297 -1.2613  0.1071  0.1482]\n",
      "MSE loss: 86.2343\n",
      "Iteration: 66500\n",
      "Gradient: [  1.2598 -10.5403  29.8673  81.5466 -72.1568]\n",
      "Weights: [-4.8557  0.8509 -1.2665  0.1075  0.1481]\n",
      "MSE loss: 86.0666\n",
      "Iteration: 66600\n",
      "Gradient: [-13.2177   4.9003 -39.869   39.6069 223.8369]\n",
      "Weights: [-4.8417  0.835  -1.2634  0.1076  0.1481]\n",
      "MSE loss: 85.9818\n",
      "Iteration: 66700\n",
      "Gradient: [  -7.4903    6.3412  -51.414  -130.838  -399.2692]\n",
      "Weights: [-4.8288  0.8365 -1.2655  0.1079  0.1478]\n",
      "MSE loss: 86.0385\n",
      "Iteration: 66800\n",
      "Gradient: [ -7.7857  -9.7268  37.5364 -85.9271 204.0854]\n",
      "Weights: [-4.8391  0.8328 -1.2643  0.1084  0.1479]\n",
      "MSE loss: 85.9817\n",
      "Iteration: 66900\n",
      "Gradient: [ -0.2771 -13.5765  15.8649 -60.6915   0.4512]\n",
      "Weights: [-4.8307  0.8164 -1.2572  0.1085  0.1477]\n",
      "MSE loss: 85.9906\n",
      "Iteration: 67000\n",
      "Gradient: [-9.3429 17.5003 36.0479 71.1765 60.3241]\n",
      "Weights: [-4.817   0.8227 -1.2579  0.1082  0.1476]\n",
      "MSE loss: 86.5329\n",
      "Iteration: 67100\n",
      "Gradient: [   1.8449   24.7057  -18.3049 -119.6243   95.0135]\n",
      "Weights: [-4.8297  0.8219 -1.2569  0.1092  0.1473]\n",
      "MSE loss: 86.0958\n",
      "Iteration: 67200\n",
      "Gradient: [ 1.135110e+01 -2.578000e-01  2.663680e+01  2.396300e+02  7.155626e+02]\n",
      "Weights: [-4.8224  0.8133 -1.2541  0.1096  0.1471]\n",
      "MSE loss: 86.1716\n",
      "Iteration: 67300\n",
      "Gradient: [ -14.055    15.3924   -2.182   -29.0264 -354.5697]\n",
      "Weights: [-4.8191  0.8106 -1.2575  0.1102  0.147 ]\n",
      "MSE loss: 86.017\n",
      "Iteration: 67400\n",
      "Gradient: [  8.7349  19.8022   2.8654 143.9144  95.7224]\n",
      "Weights: [-4.8328  0.8223 -1.2583  0.1102  0.1473]\n",
      "MSE loss: 86.1518\n",
      "Iteration: 67500\n",
      "Gradient: [ 10.8294 -15.9871 -18.7213  -8.6331 122.0957]\n",
      "Weights: [-4.8369  0.8266 -1.2581  0.1093  0.1472]\n",
      "MSE loss: 85.9727\n",
      "Iteration: 67600\n",
      "Gradient: [ -2.6697   5.6206  39.0801 205.7011 232.2315]\n",
      "Weights: [-4.8372  0.829  -1.2591  0.109   0.1473]\n",
      "MSE loss: 85.9508\n",
      "Iteration: 67700\n",
      "Gradient: [  0.9856  17.1448  19.7309  40.2083 310.683 ]\n",
      "Weights: [-4.8349  0.8325 -1.2616  0.1095  0.1473]\n",
      "MSE loss: 86.0579\n",
      "Iteration: 67800\n",
      "Gradient: [  -2.2303   31.2179   35.7824   61.475  -272.3501]\n",
      "Weights: [-4.8505  0.8402 -1.2645  0.1101  0.1472]\n",
      "MSE loss: 85.9966\n",
      "Iteration: 67900\n",
      "Gradient: [ -14.4375  -20.1046 -111.7167 -193.3424 -289.3164]\n",
      "Weights: [-4.8309  0.8189 -1.2612  0.1096  0.147 ]\n",
      "MSE loss: 86.5072\n",
      "Iteration: 68000\n",
      "Gradient: [ 14.2425   5.5008  14.9594 -98.4944 254.6327]\n",
      "Weights: [-4.8263  0.8259 -1.26    0.1102  0.1468]\n",
      "MSE loss: 86.0267\n",
      "Iteration: 68100\n",
      "Gradient: [ -16.2418  -21.8442 -104.4704  -93.1179  144.514 ]\n",
      "Weights: [-4.8366  0.8361 -1.2651  0.1109  0.1466]\n",
      "MSE loss: 86.0484\n",
      "Iteration: 68200\n",
      "Gradient: [  7.1999   8.426   60.1258 223.713  107.4504]\n",
      "Weights: [-4.8263  0.8342 -1.264   0.111   0.1469]\n",
      "MSE loss: 86.2925\n",
      "Iteration: 68300\n",
      "Gradient: [  16.3242  -13.9303    5.4947 -150.9518   54.1941]\n",
      "Weights: [-4.8356  0.8332 -1.2643  0.1113  0.1471]\n",
      "MSE loss: 86.081\n",
      "Iteration: 68400\n",
      "Gradient: [ -0.4432   5.7173  32.4276  98.1979 394.0974]\n",
      "Weights: [-4.8424  0.8383 -1.2655  0.1115  0.1469]\n",
      "MSE loss: 85.9654\n",
      "Iteration: 68500\n",
      "Gradient: [ 15.752   15.7058 111.1954 124.0926 283.4522]\n",
      "Weights: [-4.8367  0.8466 -1.2681  0.1115  0.1471]\n",
      "MSE loss: 86.5067\n",
      "Iteration: 68600\n",
      "Gradient: [ -20.0185  -10.5727   -4.6953 -176.7606  -46.0137]\n",
      "Weights: [-4.8609  0.8482 -1.267   0.111   0.1468]\n",
      "MSE loss: 86.1653\n",
      "Iteration: 68700\n",
      "Gradient: [ -14.9882  -26.3177 -103.8879 -154.0962 -362.1403]\n",
      "Weights: [-4.8581  0.845  -1.2713  0.1117  0.1468]\n",
      "MSE loss: 86.8086\n",
      "Iteration: 68800\n",
      "Gradient: [ 15.3674  14.8334  30.8324  65.9851 187.6773]\n",
      "Weights: [-4.8409  0.8489 -1.2725  0.1118  0.1472]\n",
      "MSE loss: 85.9695\n",
      "Iteration: 68900\n",
      "Gradient: [  5.6983   8.2533  27.7299 314.0463 461.5787]\n",
      "Weights: [-4.8319  0.8402 -1.2675  0.1116  0.1468]\n",
      "MSE loss: 86.0716\n",
      "Iteration: 69000\n",
      "Gradient: [ -15.9993  -26.8684  -35.7468 -133.3199  421.0927]\n",
      "Weights: [-4.8441  0.8343 -1.2667  0.1116  0.1469]\n",
      "MSE loss: 86.0474\n",
      "Iteration: 69100\n",
      "Gradient: [  -0.9524  -20.8921   32.7334  207.1124 -172.1958]\n",
      "Weights: [-4.8352  0.8326 -1.268   0.1117  0.1473]\n",
      "MSE loss: 85.9306\n",
      "Iteration: 69200\n",
      "Gradient: [-27.6052 -33.0991 -54.5253 -12.8569 412.0886]\n",
      "Weights: [-4.8458  0.835  -1.2693  0.1105  0.1474]\n",
      "MSE loss: 86.4158\n",
      "Iteration: 69300\n",
      "Gradient: [  6.1702  15.9464  78.6789  44.0298 710.6951]\n",
      "Weights: [-4.8483  0.8468 -1.2673  0.1097  0.1476]\n",
      "MSE loss: 86.0492\n",
      "Iteration: 69400\n",
      "Gradient: [ -3.236   -2.114   26.8038  23.4971 179.9904]\n",
      "Weights: [-4.8372  0.8478 -1.2672  0.1094  0.1475]\n",
      "MSE loss: 86.2523\n",
      "Iteration: 69500\n",
      "Gradient: [-10.2645  12.0569  -6.1523 -25.9475 199.9996]\n",
      "Weights: [-4.8373  0.8377 -1.2684  0.1093  0.1478]\n",
      "MSE loss: 85.9548\n",
      "Iteration: 69600\n",
      "Gradient: [  -9.2923  -38.1586  -42.0349 -201.3813 -183.7481]\n",
      "Weights: [-4.8428  0.8287 -1.2674  0.1096  0.148 ]\n",
      "MSE loss: 86.2954\n",
      "Iteration: 69700\n",
      "Gradient: [  -3.8234    5.7161   17.6656   14.463  -123.491 ]\n",
      "Weights: [-4.8351  0.8281 -1.2673  0.1093  0.1482]\n",
      "MSE loss: 86.0434\n",
      "Iteration: 69800\n",
      "Gradient: [   7.8009  -14.805   -68.3809  -99.2223 -144.8591]\n",
      "Weights: [-4.8277  0.8225 -1.2654  0.1094  0.1478]\n",
      "MSE loss: 86.0856\n",
      "Iteration: 69900\n",
      "Gradient: [  13.3236   11.4991  -28.8735 -120.2762 -166.6852]\n",
      "Weights: [-4.8335  0.8323 -1.267   0.1107  0.1476]\n",
      "MSE loss: 85.9709\n",
      "Iteration: 70000\n",
      "Gradient: [ 14.7778  18.8295 -16.1741 181.208  405.8854]\n",
      "Weights: [-4.8356  0.8422 -1.267   0.1105  0.1476]\n",
      "MSE loss: 86.5847\n",
      "Iteration: 70100\n",
      "Gradient: [  -2.1906   -1.7387   58.1686 -196.8762  170.8801]\n",
      "Weights: [-4.861   0.8453 -1.2667  0.11    0.1472]\n",
      "MSE loss: 86.3176\n",
      "Iteration: 70200\n",
      "Gradient: [  2.6769  10.1605   8.8496  75.611  -97.7889]\n",
      "Weights: [-4.8426  0.8461 -1.2673  0.1094  0.1472]\n",
      "MSE loss: 85.9764\n",
      "Iteration: 70300\n",
      "Gradient: [  -4.3438  -25.4494  -18.875  -216.4623 -118.7117]\n",
      "Weights: [-4.8616  0.8367 -1.2607  0.11    0.1468]\n",
      "MSE loss: 86.4777\n",
      "Iteration: 70400\n",
      "Gradient: [ -17.0431  -28.3247   -1.2611  -17.1861 -145.6591]\n",
      "Weights: [-4.8461  0.8234 -1.2622  0.1107  0.1471]\n",
      "MSE loss: 86.3843\n",
      "Iteration: 70500\n",
      "Gradient: [  -5.1099   -7.8151  -33.8978 -158.4671  131.5812]\n",
      "Weights: [-4.8188  0.8204 -1.2609  0.1103  0.147 ]\n",
      "MSE loss: 86.0754\n",
      "Iteration: 70600\n",
      "Gradient: [  7.0902  12.778   14.5092  24.0802 -34.5204]\n",
      "Weights: [-4.8298  0.8257 -1.2635  0.1112  0.147 ]\n",
      "MSE loss: 85.925\n",
      "Iteration: 70700\n",
      "Gradient: [-12.3662 -17.5428 -18.3047  93.6345 -11.2923]\n",
      "Weights: [-4.8419  0.8419 -1.2664  0.1111  0.1468]\n",
      "MSE loss: 85.9289\n",
      "Iteration: 70800\n",
      "Gradient: [-10.4056  -2.7861   8.7573  -0.846   24.046 ]\n",
      "Weights: [-4.8347  0.8294 -1.2635  0.1109  0.1467]\n",
      "MSE loss: 85.9978\n",
      "Iteration: 70900\n",
      "Gradient: [  -9.1722  -15.1082  -57.7335   43.5637 -104.9269]\n",
      "Weights: [-4.8404  0.8319 -1.2653  0.1118  0.147 ]\n",
      "MSE loss: 85.9165\n",
      "Iteration: 71000\n",
      "Gradient: [14.6652 22.4422 24.6829 57.166  -9.7473]\n",
      "Weights: [-4.8194  0.8197 -1.2612  0.1106  0.147 ]\n",
      "MSE loss: 86.0458\n",
      "Iteration: 71100\n",
      "Gradient: [  0.6016  28.8214  22.4576  28.7935 397.8369]\n",
      "Weights: [-4.815   0.8213 -1.2608  0.1107  0.1471]\n",
      "MSE loss: 86.4545\n",
      "Iteration: 71200\n",
      "Gradient: [-5.815000e-01 -6.174000e-01 -1.815340e+01  8.002800e+00 -7.909784e+02]\n",
      "Weights: [-4.8254  0.8153 -1.2622  0.1112  0.1472]\n",
      "MSE loss: 85.9829\n",
      "Iteration: 71300\n",
      "Gradient: [   0.3317  -38.6573  -30.6706 -116.1383 -105.9366]\n",
      "Weights: [-4.8546  0.8317 -1.2615  0.1102  0.147 ]\n",
      "MSE loss: 86.3697\n",
      "Iteration: 71400\n",
      "Gradient: [ -14.2538   -9.7712  -86.914    88.751  -512.0508]\n",
      "Weights: [-4.8604  0.8415 -1.262   0.1095  0.1469]\n",
      "MSE loss: 86.3594\n",
      "Iteration: 71500\n",
      "Gradient: [ -0.7904   6.0128   8.9319  34.0008 414.2457]\n",
      "Weights: [-4.8445  0.8376 -1.2625  0.1107  0.1469]\n",
      "MSE loss: 85.988\n",
      "Iteration: 71600\n",
      "Gradient: [  0.4843 -27.4382 -10.7584 -41.3205 -27.7662]\n",
      "Weights: [-4.8381  0.8271 -1.2609  0.1099  0.1472]\n",
      "MSE loss: 85.9355\n",
      "Iteration: 71700\n",
      "Gradient: [ 11.3572  16.6417  52.1924 -29.2915 251.5991]\n",
      "Weights: [-4.8277  0.8197 -1.261   0.1098  0.1476]\n",
      "MSE loss: 85.9857\n",
      "Iteration: 71800\n",
      "Gradient: [  5.3398  40.7214  17.4813  12.281  496.651 ]\n",
      "Weights: [-4.8242  0.8256 -1.2628  0.1104  0.1472]\n",
      "MSE loss: 86.0414\n",
      "Iteration: 71900\n",
      "Gradient: [ -3.2742  33.2395  52.972  336.078   35.876 ]\n",
      "Weights: [-4.8458  0.8281 -1.2612  0.1107  0.1471]\n",
      "MSE loss: 86.0282\n",
      "Iteration: 72000\n",
      "Gradient: [  -2.2236    3.829    -7.0082 -151.3759  260.0775]\n",
      "Weights: [-4.8314  0.8304 -1.2644  0.1099  0.1476]\n",
      "MSE loss: 86.0334\n",
      "Iteration: 72100\n",
      "Gradient: [  5.9822   8.653  -21.1598 -49.8096 317.9443]\n",
      "Weights: [-4.8381  0.8422 -1.2636  0.1082  0.1473]\n",
      "MSE loss: 86.0084\n",
      "Iteration: 72200\n",
      "Gradient: [   5.0635  -11.9995   13.1811   -4.3487 -117.0557]\n",
      "Weights: [-4.8374  0.8276 -1.2619  0.1088  0.1477]\n",
      "MSE loss: 85.9729\n",
      "Iteration: 72300\n",
      "Gradient: [ -4.5467   4.1191  13.7243 -50.6851 -26.9256]\n",
      "Weights: [-4.8313  0.8284 -1.2634  0.1084  0.148 ]\n",
      "MSE loss: 86.0067\n",
      "Iteration: 72400\n",
      "Gradient: [  -8.1107  -13.9783   33.7009  126.0272 -117.5814]\n",
      "Weights: [-4.862   0.8429 -1.2659  0.1094  0.1477]\n",
      "MSE loss: 86.3714\n",
      "Iteration: 72500\n",
      "Gradient: [  5.602    7.6801 -33.1986 126.4973  63.532 ]\n",
      "Weights: [-4.842   0.8484 -1.2688  0.1089  0.1477]\n",
      "MSE loss: 85.9862\n",
      "Iteration: 72600\n",
      "Gradient: [ -0.1987  42.6284  91.474  112.1369  81.1838]\n",
      "Weights: [-4.8366  0.8387 -1.2662  0.1084  0.1481]\n",
      "MSE loss: 86.0474\n",
      "Iteration: 72700\n",
      "Gradient: [ -7.6388  -4.6216 -16.5649   8.2574 170.2605]\n",
      "Weights: [-4.8362  0.8366 -1.2706  0.1093  0.148 ]\n",
      "MSE loss: 86.0261\n",
      "Iteration: 72800\n",
      "Gradient: [-13.4865 -15.8016 -23.418   12.6085 107.8804]\n",
      "Weights: [-4.8392  0.8409 -1.2702  0.1092  0.1478]\n",
      "MSE loss: 86.0331\n",
      "Iteration: 72900\n",
      "Gradient: [   0.9844  -16.1339  -25.4656   -6.8212 -273.5651]\n",
      "Weights: [-4.8423  0.837  -1.2673  0.1092  0.1478]\n",
      "MSE loss: 85.9896\n",
      "Iteration: 73000\n",
      "Gradient: [  -2.4424   -1.2806   48.8097   -5.4625 -167.5484]\n",
      "Weights: [-4.8213  0.8299 -1.266   0.1091  0.1477]\n",
      "MSE loss: 86.0927\n",
      "Iteration: 73100\n",
      "Gradient: [ 27.7528   0.8624  40.068  168.587   62.8078]\n",
      "Weights: [-4.8249  0.8419 -1.2673  0.1087  0.1478]\n",
      "MSE loss: 86.4863\n",
      "Iteration: 73200\n",
      "Gradient: [   3.1553  -12.1869   24.7646   -5.7673 -103.3883]\n",
      "Weights: [-4.849   0.8452 -1.2651  0.108   0.1477]\n",
      "MSE loss: 85.9814\n",
      "Iteration: 73300\n",
      "Gradient: [  -2.4644   -8.2252  -14.3353 -191.3182   31.7558]\n",
      "Weights: [-4.8408  0.8425 -1.2662  0.1086  0.1476]\n",
      "MSE loss: 85.9474\n",
      "Iteration: 73400\n",
      "Gradient: [ 17.0891  29.6911  19.3791 -11.7084  -7.6503]\n",
      "Weights: [-4.8121  0.8253 -1.265   0.1083  0.1479]\n",
      "MSE loss: 86.3107\n",
      "Iteration: 73500\n",
      "Gradient: [  2.1015  25.4366  39.0484 -11.996  300.2854]\n",
      "Weights: [-4.8307  0.8325 -1.2627  0.1079  0.1479]\n",
      "MSE loss: 86.0168\n",
      "Iteration: 73600\n",
      "Gradient: [ -0.1178 -22.6142 -40.5746  58.1195  54.2836]\n",
      "Weights: [-4.8354  0.8282 -1.263   0.1075  0.1481]\n",
      "MSE loss: 86.069\n",
      "Iteration: 73700\n",
      "Gradient: [  -7.6816  -11.6657  -19.9442  -67.2733 -366.8757]\n",
      "Weights: [-4.8446  0.8323 -1.2632  0.1081  0.148 ]\n",
      "MSE loss: 86.0691\n",
      "Iteration: 73800\n",
      "Gradient: [-3.074000e-01 -1.159960e+01  3.265890e+01  1.524069e+02  6.256849e+02]\n",
      "Weights: [-4.8393  0.8378 -1.2624  0.1073  0.1481]\n",
      "MSE loss: 86.0921\n",
      "Iteration: 73900\n",
      "Gradient: [   7.0129  -12.2424    7.1448  -63.3258 -157.9841]\n",
      "Weights: [-4.8461  0.8471 -1.2678  0.1079  0.1479]\n",
      "MSE loss: 85.9781\n",
      "Iteration: 74000\n",
      "Gradient: [  -9.9504  -36.2698  -35.3999 -260.8916 -615.8472]\n",
      "Weights: [-4.8497  0.8457 -1.267   0.1066  0.148 ]\n",
      "MSE loss: 86.4772\n",
      "Iteration: 74100\n",
      "Gradient: [ -12.5986   -2.2203  -34.5594 -183.6939  115.5855]\n",
      "Weights: [-4.8414  0.8352 -1.2621  0.1074  0.148 ]\n",
      "MSE loss: 85.9681\n",
      "Iteration: 74200\n",
      "Gradient: [  -6.7047    9.0867   38.1415 -115.2286   74.6149]\n",
      "Weights: [-4.8412  0.8448 -1.265   0.1075  0.1478]\n",
      "MSE loss: 85.9852\n",
      "Iteration: 74300\n",
      "Gradient: [ -9.609   -2.4271   2.7791  -8.4661 -78.1168]\n",
      "Weights: [-4.8447  0.8385 -1.2641  0.1086  0.1476]\n",
      "MSE loss: 85.9611\n",
      "Iteration: 74400\n",
      "Gradient: [  -7.6218  -24.5719  -93.1129 -183.7885 -204.2368]\n",
      "Weights: [-4.8371  0.8269 -1.2664  0.1101  0.1476]\n",
      "MSE loss: 86.1827\n",
      "Iteration: 74500\n",
      "Gradient: [  9.6064 -13.5859 -42.7376  10.2167 159.2364]\n",
      "Weights: [-4.8272  0.8269 -1.2697  0.1109  0.1478]\n",
      "MSE loss: 85.9874\n",
      "Iteration: 74600\n",
      "Gradient: [  2.9979  25.4074   6.4168 168.9597 192.9045]\n",
      "Weights: [-4.8501  0.8565 -1.2716  0.1111  0.1475]\n",
      "MSE loss: 86.4581\n",
      "Iteration: 74700\n",
      "Gradient: [ -2.2869  -1.1721 -17.4953  -5.552  -47.2532]\n",
      "Weights: [-4.8432  0.8592 -1.2777  0.1117  0.1473]\n",
      "MSE loss: 85.964\n",
      "Iteration: 74800\n",
      "Gradient: [ -2.8515  11.7286  -5.1396  99.776  156.2615]\n",
      "Weights: [-4.8454  0.8495 -1.277   0.1119  0.1474]\n",
      "MSE loss: 86.0184\n",
      "Iteration: 74900\n",
      "Gradient: [ -8.9543   2.909   19.2354   1.2508 151.9803]\n",
      "Weights: [-4.8353  0.8453 -1.2757  0.1123  0.1474]\n",
      "MSE loss: 85.9205\n",
      "Iteration: 75000\n",
      "Gradient: [  -6.2483   -8.5107   33.3549 -133.7786 -313.711 ]\n",
      "Weights: [-4.8476  0.8483 -1.2753  0.1129  0.1471]\n",
      "MSE loss: 85.9212\n",
      "Iteration: 75100\n",
      "Gradient: [  -2.8015   -6.4465   30.2319   43.1902 -411.1742]\n",
      "Weights: [-4.8421  0.8462 -1.2796  0.1143  0.147 ]\n",
      "MSE loss: 85.98\n",
      "Iteration: 75200\n",
      "Gradient: [   8.2959   -9.3833   -0.3184   44.7093 -284.5021]\n",
      "Weights: [-4.8391  0.8449 -1.2786  0.1139  0.1472]\n",
      "MSE loss: 85.8871\n",
      "Iteration: 75300\n",
      "Gradient: [ -0.6079  16.8709 -34.4698 -49.0894  47.7638]\n",
      "Weights: [-4.8382  0.8425 -1.2787  0.1136  0.1473]\n",
      "MSE loss: 85.9939\n",
      "Iteration: 75400\n",
      "Gradient: [ -8.0049 -18.442  -22.2708 -65.7268  38.2506]\n",
      "Weights: [-4.8366  0.8313 -1.2751  0.114   0.1471]\n",
      "MSE loss: 86.1779\n",
      "Iteration: 75500\n",
      "Gradient: [   5.1662  -15.591    47.5208  -69.4087 -140.2623]\n",
      "Weights: [-4.8352  0.8382 -1.2809  0.1159  0.1471]\n",
      "MSE loss: 85.9386\n",
      "Iteration: 75600\n",
      "Gradient: [  5.7862  30.6388  25.4271 -24.5895 308.2382]\n",
      "Weights: [-4.8326  0.8474 -1.2809  0.1158  0.1469]\n",
      "MSE loss: 86.0074\n",
      "Iteration: 75700\n",
      "Gradient: [ 27.4913  13.1637 -50.8569  44.5134 204.3087]\n",
      "Weights: [-4.8414  0.8517 -1.2816  0.1155  0.1468]\n",
      "MSE loss: 85.85\n",
      "Iteration: 75800\n",
      "Gradient: [-17.1818 -24.3841   6.9158 -44.0299  83.2728]\n",
      "Weights: [-4.852   0.8621 -1.2814  0.1154  0.1464]\n",
      "MSE loss: 85.8855\n",
      "Iteration: 75900\n",
      "Gradient: [  -3.9969  -16.9042  -77.5943 -105.1664  249.5762]\n",
      "Weights: [-4.8562  0.8606 -1.2816  0.116   0.1462]\n",
      "MSE loss: 85.9454\n",
      "Iteration: 76000\n",
      "Gradient: [-11.7683 -36.7616 -64.2022 -78.4054 -32.2287]\n",
      "Weights: [-4.8402  0.8466 -1.2819  0.1163  0.1462]\n",
      "MSE loss: 86.2792\n",
      "Iteration: 76100\n",
      "Gradient: [ 17.8633  12.0534  31.3617 157.8409 448.2462]\n",
      "Weights: [-4.8394  0.8485 -1.2771  0.116   0.1464]\n",
      "MSE loss: 86.0171\n",
      "Iteration: 76200\n",
      "Gradient: [ -9.4437 -20.0305  17.8046  49.1225 -26.7988]\n",
      "Weights: [-4.8353  0.8402 -1.2768  0.1161  0.1462]\n",
      "MSE loss: 85.8517\n",
      "Iteration: 76300\n",
      "Gradient: [-1.732000e-01 -1.928500e+00 -2.728000e-01  1.355500e+02 -4.829818e+02]\n",
      "Weights: [-4.83    0.8379 -1.2745  0.1164  0.1461]\n",
      "MSE loss: 85.9336\n",
      "Iteration: 76400\n",
      "Gradient: [17.4207  3.1156 45.6469 -3.8357 36.7034]\n",
      "Weights: [-4.8341  0.8479 -1.2793  0.1163  0.1462]\n",
      "MSE loss: 85.9092\n",
      "Iteration: 76500\n",
      "Gradient: [ 17.8567  14.0291  28.2898 130.1669 329.8866]\n",
      "Weights: [-4.8346  0.8545 -1.2782  0.1158  0.1464]\n",
      "MSE loss: 86.5487\n",
      "Iteration: 76600\n",
      "Gradient: [   8.9563   -5.2873  -38.9598  200.1474 -231.3461]\n",
      "Weights: [-4.8357  0.8459 -1.2829  0.1166  0.1464]\n",
      "MSE loss: 85.9567\n",
      "Iteration: 76700\n",
      "Gradient: [ -2.9925   1.2409  24.3111 -79.0096 -13.6974]\n",
      "Weights: [-4.8553  0.863  -1.2846  0.1162  0.1465]\n",
      "MSE loss: 85.9308\n",
      "Iteration: 76800\n",
      "Gradient: [   4.01     -1.1673   91.2751    8.5445 -385.4273]\n",
      "Weights: [-4.8351  0.8621 -1.286   0.1173  0.1464]\n",
      "MSE loss: 86.3423\n",
      "Iteration: 76900\n",
      "Gradient: [-2.306090e+01 -2.010760e+01  3.030000e-01 -5.786400e+00 -5.195783e+02]\n",
      "Weights: [-4.8541  0.8649 -1.291   0.1173  0.1467]\n",
      "MSE loss: 86.0045\n",
      "Iteration: 77000\n",
      "Gradient: [ 5.004   1.1827 10.8017 70.9947 10.9412]\n",
      "Weights: [-4.8371  0.8664 -1.2899  0.1175  0.1466]\n",
      "MSE loss: 86.1349\n",
      "Iteration: 77100\n",
      "Gradient: [  3.2185  14.381   52.3693  34.7985 160.3163]\n",
      "Weights: [-4.8396  0.8675 -1.2922  0.1174  0.1469]\n",
      "MSE loss: 86.0294\n",
      "Iteration: 77200\n",
      "Gradient: [  -1.5251   -8.8862  -61.7918  -84.578  -226.6997]\n",
      "Weights: [-4.8409  0.8703 -1.2957  0.1172  0.1469]\n",
      "MSE loss: 85.9046\n",
      "Iteration: 77300\n",
      "Gradient: [   4.1089  -12.0239  -12.7637  -78.8594 -194.1409]\n",
      "Weights: [-4.8397  0.8706 -1.2947  0.1163  0.147 ]\n",
      "MSE loss: 86.0064\n",
      "Iteration: 77400\n",
      "Gradient: [  5.7778  15.0202  65.5753  70.1446 321.8352]\n",
      "Weights: [-4.8248  0.8597 -1.2914  0.1172  0.147 ]\n",
      "MSE loss: 86.3345\n",
      "Iteration: 77500\n",
      "Gradient: [ -10.7502  -26.5828  -48.8134    4.4478 -183.8766]\n",
      "Weights: [-4.8431  0.8529 -1.2918  0.1183  0.1468]\n",
      "MSE loss: 86.0649\n",
      "Iteration: 77600\n",
      "Gradient: [ 13.7267  19.9559 -20.0964 -38.3911 323.1328]\n",
      "Weights: [-4.8309  0.8485 -1.2859  0.1187  0.1464]\n",
      "MSE loss: 85.9365\n",
      "Iteration: 77700\n",
      "Gradient: [ 13.183    6.8248   9.7708  46.4854 185.503 ]\n",
      "Weights: [-4.8375  0.8549 -1.2892  0.1197  0.1463]\n",
      "MSE loss: 85.9308\n",
      "Iteration: 77800\n",
      "Gradient: [ -3.827   -1.9267  -4.9359 -63.9333 211.6728]\n",
      "Weights: [-4.8361  0.8462 -1.2878  0.1197  0.1462]\n",
      "MSE loss: 85.8309\n",
      "Iteration: 77900\n",
      "Gradient: [ -9.7042  11.6783   2.8011 -12.6732 159.0382]\n",
      "Weights: [-4.8408  0.8555 -1.2922  0.12    0.146 ]\n",
      "MSE loss: 85.9949\n",
      "Iteration: 78000\n",
      "Gradient: [-10.4316  -2.813  -71.6039 -45.9959 -63.7679]\n",
      "Weights: [-4.8503  0.8606 -1.2933  0.1208  0.146 ]\n",
      "MSE loss: 85.8635\n",
      "Iteration: 78100\n",
      "Gradient: [-10.8686 -15.0941 -19.7006 -56.1482  18.5904]\n",
      "Weights: [-4.8558  0.8682 -1.2947  0.1207  0.1459]\n",
      "MSE loss: 85.9077\n",
      "Iteration: 78200\n",
      "Gradient: [ -11.8921    3.3685   -9.942   -23.7275 -279.6886]\n",
      "Weights: [-4.8402  0.8604 -1.2949  0.1206  0.1463]\n",
      "MSE loss: 85.7927\n",
      "Iteration: 78300\n",
      "Gradient: [  2.6412 -17.4859 -21.0228  19.4996  20.4679]\n",
      "Weights: [-4.8648  0.8697 -1.2952  0.1202  0.146 ]\n",
      "MSE loss: 86.4031\n",
      "Iteration: 78400\n",
      "Gradient: [   6.2926   11.1337   14.8878 -149.9506   -7.8505]\n",
      "Weights: [-4.8509  0.8767 -1.2967  0.1204  0.146 ]\n",
      "MSE loss: 85.8526\n",
      "Iteration: 78500\n",
      "Gradient: [ 9.860000e-02  1.422570e+01 -3.522120e+01 -1.556804e+02 -4.152129e+02]\n",
      "Weights: [-4.8412  0.8673 -1.2975  0.1204  0.1462]\n",
      "MSE loss: 85.8139\n",
      "Iteration: 78600\n",
      "Gradient: [  6.6955 -18.2964 -13.9444  94.5891  19.8865]\n",
      "Weights: [-4.8561  0.8812 -1.2988  0.1206  0.1459]\n",
      "MSE loss: 85.8508\n",
      "Iteration: 78700\n",
      "Gradient: [ -7.1445   1.2008  41.4966 -19.0076 128.9602]\n",
      "Weights: [-4.8444  0.8752 -1.3001  0.1221  0.1458]\n",
      "MSE loss: 85.8358\n",
      "Iteration: 78800\n",
      "Gradient: [  -8.1434   19.7875  -46.2707 -211.8187 -379.7034]\n",
      "Weights: [-4.8354  0.8652 -1.3008  0.1223  0.1459]\n",
      "MSE loss: 85.8565\n",
      "Iteration: 78900\n",
      "Gradient: [ -13.9236    0.19    -93.8784    8.1162 -179.9883]\n",
      "Weights: [-4.8497  0.868  -1.3003  0.1223  0.1459]\n",
      "MSE loss: 85.8647\n",
      "Iteration: 79000\n",
      "Gradient: [ -2.4476  26.3164  90.8945 121.9468 147.2933]\n",
      "Weights: [-4.8459  0.8784 -1.2972  0.1221  0.1458]\n",
      "MSE loss: 86.657\n",
      "Iteration: 79100\n",
      "Gradient: [ 16.8667  -2.056  106.9727  13.1815 475.1328]\n",
      "Weights: [-4.8393  0.8779 -1.3023  0.1217  0.1462]\n",
      "MSE loss: 86.0767\n",
      "Iteration: 79200\n",
      "Gradient: [   3.3063   -7.9998   -5.0617 -150.1218 -293.1231]\n",
      "Weights: [-4.8585  0.8809 -1.3022  0.1218  0.1458]\n",
      "MSE loss: 85.8972\n",
      "Iteration: 79300\n",
      "Gradient: [ 11.3687  -7.139    8.5315 136.0099 327.7406]\n",
      "Weights: [-4.851   0.8653 -1.3003  0.1225  0.1464]\n",
      "MSE loss: 85.8957\n",
      "Iteration: 79400\n",
      "Gradient: [ -13.2155  -40.3426  -97.9664 -290.6624 -318.8889]\n",
      "Weights: [-4.848   0.8654 -1.3036  0.1227  0.1461]\n",
      "MSE loss: 86.1753\n",
      "Iteration: 79500\n",
      "Gradient: [  0.9357 -12.9126  -5.332  239.9202  96.2977]\n",
      "Weights: [-4.8439  0.8719 -1.3011  0.1228  0.146 ]\n",
      "MSE loss: 85.8868\n",
      "Iteration: 79600\n",
      "Gradient: [ -1.5241  24.4868 -19.5748 -21.7689  24.5119]\n",
      "Weights: [-4.8483  0.8852 -1.3036  0.1222  0.1459]\n",
      "MSE loss: 85.9589\n",
      "Iteration: 79700\n",
      "Gradient: [   6.5457  -31.7138  -59.8837   -7.7994 -229.2381]\n",
      "Weights: [-4.8499  0.8831 -1.3041  0.1222  0.1459]\n",
      "MSE loss: 85.8087\n",
      "Iteration: 79800\n",
      "Gradient: [  8.3146 -33.3213 -35.2775 -40.1449  10.0554]\n",
      "Weights: [-4.8629  0.8874 -1.3038  0.1218  0.1458]\n",
      "MSE loss: 85.9699\n",
      "Iteration: 79900\n",
      "Gradient: [  3.4999 -10.0776 -18.4653  28.2529 200.8151]\n",
      "Weights: [-4.8387  0.8768 -1.3044  0.1225  0.1462]\n",
      "MSE loss: 85.9515\n",
      "Iteration: 80000\n",
      "Gradient: [-12.3893 -17.6473  25.0589  -5.8562 111.1346]\n",
      "Weights: [-4.8563  0.8788 -1.3024  0.122   0.1461]\n",
      "MSE loss: 85.8128\n",
      "Iteration: 80100\n",
      "Gradient: [-2.6168 28.1516 23.246  -9.5898 -5.0327]\n",
      "Weights: [-4.8531  0.8793 -1.3025  0.1213  0.1463]\n",
      "MSE loss: 85.7906\n",
      "Iteration: 80200\n",
      "Gradient: [  1.0636 -13.6513  34.147   30.4952 336.1319]\n",
      "Weights: [-4.8453  0.8693 -1.3008  0.122   0.1461]\n",
      "MSE loss: 85.7836\n",
      "Iteration: 80300\n",
      "Gradient: [-7.290000e-02  8.782500e+00  1.459760e+01  6.023900e+01  2.678116e+02]\n",
      "Weights: [-4.8367  0.8763 -1.301   0.1222  0.1461]\n",
      "MSE loss: 86.444\n",
      "Iteration: 80400\n",
      "Gradient: [ -17.2717  -27.5811  -59.4634 -117.0764 -248.6843]\n",
      "Weights: [-4.8775  0.8914 -1.3044  0.1213  0.1461]\n",
      "MSE loss: 86.3549\n",
      "Iteration: 80500\n",
      "Gradient: [ 15.4738   4.5136  43.8282 -13.6517  76.9644]\n",
      "Weights: [-4.8428  0.8772 -1.3034  0.1212  0.1464]\n",
      "MSE loss: 85.8327\n",
      "Iteration: 80600\n",
      "Gradient: [ -8.7249  18.724    9.9761  23.5529 103.0383]\n",
      "Weights: [-4.8408  0.8745 -1.3051  0.1219  0.1466]\n",
      "MSE loss: 85.8405\n",
      "Iteration: 80700\n",
      "Gradient: [  5.5459   7.4706 -17.9272  31.3195  41.5649]\n",
      "Weights: [-4.841   0.8772 -1.3051  0.1214  0.1466]\n",
      "MSE loss: 85.8701\n",
      "Iteration: 80800\n",
      "Gradient: [   4.6626    8.582    -7.5032  -18.4886 -372.8217]\n",
      "Weights: [-4.8529  0.8931 -1.3077  0.121   0.1464]\n",
      "MSE loss: 85.8635\n",
      "Iteration: 80900\n",
      "Gradient: [ -23.5141  -21.3731  -43.0883  -94.8568 -255.4988]\n",
      "Weights: [-4.8571  0.8913 -1.3123  0.1229  0.1462]\n",
      "MSE loss: 85.8981\n",
      "Iteration: 81000\n",
      "Gradient: [  5.0234  -8.6922  98.4554 -25.9793 -44.4562]\n",
      "Weights: [-4.8503  0.8952 -1.3148  0.1233  0.1465]\n",
      "MSE loss: 85.8224\n",
      "Iteration: 81100\n",
      "Gradient: [  7.5229 -24.3    -34.8535 119.3349 130.1555]\n",
      "Weights: [-4.8462  0.893  -1.3157  0.1229  0.1466]\n",
      "MSE loss: 85.837\n",
      "Iteration: 81200\n",
      "Gradient: [  -1.8825  -21.5265   51.5259  -18.9006 -430.2883]\n",
      "Weights: [-4.8506  0.8828 -1.3152  0.1237  0.1466]\n",
      "MSE loss: 86.0369\n",
      "Iteration: 81300\n",
      "Gradient: [ -9.2381  28.3446  30.5322  84.4745 -38.8322]\n",
      "Weights: [-4.8357  0.8782 -1.3137  0.1249  0.1465]\n",
      "MSE loss: 85.9192\n",
      "Iteration: 81400\n",
      "Gradient: [  -5.0625  -13.6764   12.5304  112.0024 -278.634 ]\n",
      "Weights: [-4.8646  0.8917 -1.3132  0.1254  0.1459]\n",
      "MSE loss: 85.8574\n",
      "Iteration: 81500\n",
      "Gradient: [   7.8995   30.7438   56.1681   86.013  -225.1643]\n",
      "Weights: [-4.8342  0.8853 -1.316   0.1256  0.1461]\n",
      "MSE loss: 86.0785\n",
      "Iteration: 81600\n",
      "Gradient: [   3.3748   10.9976  -54.4396 -132.5636   59.9091]\n",
      "Weights: [-4.8548  0.8852 -1.3172  0.1261  0.1462]\n",
      "MSE loss: 85.8602\n",
      "Iteration: 81700\n",
      "Gradient: [ -14.8389  -26.2688  -47.7481 -176.5849 -117.3792]\n",
      "Weights: [-4.8624  0.8902 -1.3178  0.1258  0.146 ]\n",
      "MSE loss: 86.1499\n",
      "Iteration: 81800\n",
      "Gradient: [  -8.1917  -14.6871   -8.5705   86.1638 -109.4471]\n",
      "Weights: [-4.8604  0.9024 -1.3169  0.1253  0.146 ]\n",
      "MSE loss: 85.8577\n",
      "Iteration: 81900\n",
      "Gradient: [ -5.6671   8.8553  17.0264 -39.2656 408.171 ]\n",
      "Weights: [-4.869   0.9135 -1.3198  0.125   0.1462]\n",
      "MSE loss: 86.0567\n",
      "Iteration: 82000\n",
      "Gradient: [  10.6506   14.0133  -68.448   -85.3814 -378.5799]\n",
      "Weights: [-4.8562  0.9003 -1.323   0.1247  0.1465]\n",
      "MSE loss: 85.9668\n",
      "Iteration: 82100\n",
      "Gradient: [   8.6447   -7.8817   -4.7766 -164.3399  -92.243 ]\n",
      "Weights: [-4.8624  0.907  -1.3217  0.1245  0.1464]\n",
      "MSE loss: 85.8193\n",
      "Iteration: 82200\n",
      "Gradient: [ -5.6608 -35.8454   3.1493  -3.4381 189.9871]\n",
      "Weights: [-4.8665  0.9008 -1.3212  0.1253  0.1465]\n",
      "MSE loss: 85.9391\n",
      "Iteration: 82300\n",
      "Gradient: [ 13.5141 -12.2884 -16.9149  -5.3711  16.9221]\n",
      "Weights: [-4.8451  0.9003 -1.3225  0.1257  0.1462]\n",
      "MSE loss: 85.8609\n",
      "Iteration: 82400\n",
      "Gradient: [   2.9202   -7.1388  -15.0353  -29.4004 -318.878 ]\n",
      "Weights: [-4.8341  0.8858 -1.3222  0.1279  0.1458]\n",
      "MSE loss: 85.8658\n",
      "Iteration: 82500\n",
      "Gradient: [ -1.7529  12.8882  26.6323 155.7223  65.9762]\n",
      "Weights: [-4.8511  0.8978 -1.3193  0.127   0.1458]\n",
      "MSE loss: 85.8942\n",
      "Iteration: 82600\n",
      "Gradient: [-18.5581 -25.1241  11.715   89.5339 -25.4121]\n",
      "Weights: [-4.8628  0.8987 -1.3207  0.1268  0.1457]\n",
      "MSE loss: 85.9297\n",
      "Iteration: 82700\n",
      "Gradient: [  -4.8772    8.8831   33.0744   -5.9181 -288.0714]\n",
      "Weights: [-4.8524  0.9011 -1.3179  0.1255  0.146 ]\n",
      "MSE loss: 85.924\n",
      "Iteration: 82800\n",
      "Gradient: [-6.438000e-01  6.417600e+00  9.545200e+00 -1.358861e+02 -6.738803e+02]\n",
      "Weights: [-4.8478  0.8909 -1.3158  0.1244  0.1459]\n",
      "MSE loss: 85.9486\n",
      "Iteration: 82900\n",
      "Gradient: [ -6.7017  -4.4984 -43.7579 -51.4471 -69.3917]\n",
      "Weights: [-4.8594  0.8932 -1.3152  0.1243  0.146 ]\n",
      "MSE loss: 86.0274\n",
      "Iteration: 83000\n",
      "Gradient: [  17.66    -21.7713   11.3687  145.5552 -221.1964]\n",
      "Weights: [-4.8421  0.8921 -1.3154  0.1257  0.1458]\n",
      "MSE loss: 86.0664\n",
      "Iteration: 83100\n",
      "Gradient: [   3.6765  -16.3182   20.2696 -108.5911 -318.4727]\n",
      "Weights: [-4.8452  0.8847 -1.3161  0.1256  0.146 ]\n",
      "MSE loss: 85.7803\n",
      "Iteration: 83200\n",
      "Gradient: [  0.7477 -30.3282 -47.4844  -0.6771  -4.428 ]\n",
      "Weights: [-4.857   0.8952 -1.3182  0.1258  0.1459]\n",
      "MSE loss: 85.7802\n",
      "Iteration: 83300\n",
      "Gradient: [  0.4108 -11.3659   0.5437  38.5967  16.0212]\n",
      "Weights: [-4.8454  0.8856 -1.3175  0.1264  0.1461]\n",
      "MSE loss: 85.7501\n",
      "Iteration: 83400\n",
      "Gradient: [ -10.8291    3.2347   59.0542   11.6022 -114.2176]\n",
      "Weights: [-4.8376  0.8865 -1.3216  0.1272  0.1462]\n",
      "MSE loss: 85.8317\n",
      "Iteration: 83500\n",
      "Gradient: [-13.0318   6.2594  36.1717  62.4983 -79.8852]\n",
      "Weights: [-4.8467  0.8877 -1.3226  0.1277  0.1462]\n",
      "MSE loss: 85.7679\n",
      "Iteration: 83600\n",
      "Gradient: [  -9.1497  -12.479    -5.9811  -76.2543 -306.9228]\n",
      "Weights: [-4.8708  0.8973 -1.3229  0.1278  0.1459]\n",
      "MSE loss: 86.1878\n",
      "Iteration: 83700\n",
      "Gradient: [  5.1706  10.5896  29.4471  24.3108 182.9472]\n",
      "Weights: [-4.8443  0.8955 -1.3232  0.1277  0.1458]\n",
      "MSE loss: 85.7891\n",
      "Iteration: 83800\n",
      "Gradient: [ -24.9615  -12.2603 -122.7674   -4.3132  -12.6025]\n",
      "Weights: [-4.8617  0.8911 -1.3236  0.1286  0.1458]\n",
      "MSE loss: 86.1525\n",
      "Iteration: 83900\n",
      "Gradient: [ 24.7099  17.1863  43.6376 -14.4939 697.0056]\n",
      "Weights: [-4.8388  0.8882 -1.3194  0.1282  0.1456]\n",
      "MSE loss: 85.9965\n",
      "Iteration: 84000\n",
      "Gradient: [ -5.7158  -0.0727 -34.2278 -40.9058 -14.836 ]\n",
      "Weights: [-4.8586  0.8868 -1.3147  0.1287  0.1452]\n",
      "MSE loss: 85.8088\n",
      "Iteration: 84100\n",
      "Gradient: [  8.1757  36.406  -33.9762 117.7395 233.1917]\n",
      "Weights: [-4.8399  0.8883 -1.3174  0.1289  0.1449]\n",
      "MSE loss: 85.866\n",
      "Iteration: 84200\n",
      "Gradient: [  2.8193  26.178   16.5498 115.7717 215.4027]\n",
      "Weights: [-4.8437  0.895  -1.3183  0.1289  0.145 ]\n",
      "MSE loss: 86.1617\n",
      "Iteration: 84300\n",
      "Gradient: [  -8.3603    1.3736  -35.2769 -189.8989 -100.6666]\n",
      "Weights: [-4.863   0.8994 -1.3169  0.1279  0.145 ]\n",
      "MSE loss: 85.7562\n",
      "Iteration: 84400\n",
      "Gradient: [-12.132  -18.9974 -42.4181 -89.668   75.4438]\n",
      "Weights: [-4.8565  0.8935 -1.3208  0.1287  0.1453]\n",
      "MSE loss: 85.7415\n",
      "Iteration: 84500\n",
      "Gradient: [ -9.402    5.2195 -37.8813  62.6425 294.7893]\n",
      "Weights: [-4.8461  0.8811 -1.3243  0.1306  0.1454]\n",
      "MSE loss: 85.8786\n",
      "Iteration: 84600\n",
      "Gradient: [  1.0372 -23.1138  90.6038 -93.5448 204.2175]\n",
      "Weights: [-4.8357  0.8788 -1.3201  0.1293  0.1455]\n",
      "MSE loss: 85.7716\n",
      "Iteration: 84700\n",
      "Gradient: [ -13.1492  -23.8589  -29.9787   12.5781 -658.4128]\n",
      "Weights: [-4.8452  0.8763 -1.3215  0.1293  0.1454]\n",
      "MSE loss: 86.5047\n",
      "Iteration: 84800\n",
      "Gradient: [  -6.0736  -12.9218  -99.4567   55.9928 -367.0809]\n",
      "Weights: [-4.841   0.8757 -1.3198  0.1295  0.1454]\n",
      "MSE loss: 85.8225\n",
      "Iteration: 84900\n",
      "Gradient: [   7.659   -14.3282  -35.484     1.9179 -299.8389]\n",
      "Weights: [-4.8526  0.889  -1.3165  0.1283  0.1451]\n",
      "MSE loss: 85.6936\n",
      "Iteration: 85000\n",
      "Gradient: [  -2.1631   -3.9338  -54.6389  -99.595  -104.674 ]\n",
      "Weights: [-4.846   0.8689 -1.3123  0.1292  0.145 ]\n",
      "MSE loss: 85.7618\n",
      "Iteration: 85100\n",
      "Gradient: [ 10.1472  10.2197  45.2466 219.854  416.4374]\n",
      "Weights: [-4.8521  0.881  -1.3099  0.1286  0.1449]\n",
      "MSE loss: 85.9256\n",
      "Iteration: 85200\n",
      "Gradient: [ -5.4477  -7.851  -20.545   -4.2009 301.4505]\n",
      "Weights: [-4.8688  0.8952 -1.3099  0.1273  0.1447]\n",
      "MSE loss: 85.8678\n",
      "Iteration: 85300\n",
      "Gradient: [  -9.3608  -38.5108   -5.8433 -109.3115 -622.5   ]\n",
      "Weights: [-4.8537  0.8738 -1.3079  0.1281  0.1444]\n",
      "MSE loss: 85.9995\n",
      "Iteration: 85400\n",
      "Gradient: [ 13.4627   0.9787  41.3161 150.8691 364.5666]\n",
      "Weights: [-4.8303  0.8676 -1.3077  0.1288  0.1447]\n",
      "MSE loss: 86.0057\n",
      "Iteration: 85500\n",
      "Gradient: [ -8.0482  16.2683  38.8227 193.6017 282.8042]\n",
      "Weights: [-4.8279  0.8634 -1.3071  0.1287  0.1447]\n",
      "MSE loss: 85.9366\n",
      "Iteration: 85600\n",
      "Gradient: [ -19.0302   -8.8596    6.3357 -100.8669  -56.9326]\n",
      "Weights: [-4.8519  0.8601 -1.3069  0.128   0.145 ]\n",
      "MSE loss: 86.3498\n",
      "Iteration: 85700\n",
      "Gradient: [ 8.00780e+00 -1.33000e-02 -2.43360e+00  4.94367e+01 -1.54332e+01]\n",
      "Weights: [-4.8292  0.8618 -1.3076  0.1275  0.1452]\n",
      "MSE loss: 85.806\n",
      "Iteration: 85800\n",
      "Gradient: [ -11.7608  -23.6782  -43.5196 -153.01   -523.1292]\n",
      "Weights: [-4.861   0.8695 -1.3054  0.127   0.145 ]\n",
      "MSE loss: 86.165\n",
      "Iteration: 85900\n",
      "Gradient: [-17.4049  -0.5107  14.124   69.6821  79.9105]\n",
      "Weights: [-4.8578  0.8771 -1.3041  0.1266  0.1448]\n",
      "MSE loss: 85.7592\n",
      "Iteration: 86000\n",
      "Gradient: [ -19.1685  -17.3964  -38.9509   53.263  -196.0662]\n",
      "Weights: [-4.8552  0.8712 -1.3055  0.127   0.1451]\n",
      "MSE loss: 85.7932\n",
      "Iteration: 86100\n",
      "Gradient: [  4.7306   0.5981 -48.762   54.1541 203.8271]\n",
      "Weights: [-4.8365  0.8619 -1.3024  0.1266  0.1449]\n",
      "MSE loss: 85.7241\n",
      "Iteration: 86200\n",
      "Gradient: [ 11.5077  35.3777  54.536  203.256  421.7164]\n",
      "Weights: [-4.8186  0.8591 -1.2975  0.1252  0.1452]\n",
      "MSE loss: 87.092\n",
      "Iteration: 86300\n",
      "Gradient: [  -2.5588  -12.0059  -71.6856  -59.8655 -238.2318]\n",
      "Weights: [-4.8319  0.8453 -1.3004  0.1266  0.1452]\n",
      "MSE loss: 85.9439\n",
      "Iteration: 86400\n",
      "Gradient: [ 14.0678  22.1007  22.0905  28.6921 244.2588]\n",
      "Weights: [-4.8229  0.841  -1.295   0.1266  0.145 ]\n",
      "MSE loss: 85.9286\n",
      "Iteration: 86500\n",
      "Gradient: [  -1.7089  -23.516   -15.0867 -108.3881 -100.7977]\n",
      "Weights: [-4.8368  0.8412 -1.2941  0.1269  0.1447]\n",
      "MSE loss: 85.7733\n",
      "Iteration: 86600\n",
      "Gradient: [  9.3858  25.2181   9.1696 -68.2308 356.4009]\n",
      "Weights: [-4.8294  0.8353 -1.2935  0.1275  0.1447]\n",
      "MSE loss: 85.7883\n",
      "Iteration: 86700\n",
      "Gradient: [-1.3387 -3.2755 65.5911 44.0518 78.5765]\n",
      "Weights: [-4.832   0.8419 -1.2925  0.1273  0.1445]\n",
      "MSE loss: 85.8372\n",
      "Iteration: 86800\n",
      "Gradient: [  6.7018  33.812   43.5517  96.8716 107.336 ]\n",
      "Weights: [-4.8257  0.8444 -1.2936  0.1271  0.1445]\n",
      "MSE loss: 85.9509\n",
      "Iteration: 86900\n",
      "Gradient: [ -5.6249  24.5831  32.7115 181.1986 -94.2683]\n",
      "Weights: [-4.8342  0.8439 -1.2913  0.1267  0.1443]\n",
      "MSE loss: 85.7236\n",
      "Iteration: 87000\n",
      "Gradient: [ -13.9818   -4.1134  -21.642  -117.6607  -70.5233]\n",
      "Weights: [-4.8431  0.8461 -1.2886  0.1263  0.1441]\n",
      "MSE loss: 85.713\n",
      "Iteration: 87100\n",
      "Gradient: [  -8.9174    1.743   -37.5301 -111.3402  365.3652]\n",
      "Weights: [-4.85    0.8518 -1.2935  0.1269  0.1442]\n",
      "MSE loss: 85.8345\n",
      "Iteration: 87200\n",
      "Gradient: [11.6813 21.6567 -4.84   56.3767 80.9444]\n",
      "Weights: [-4.8336  0.8545 -1.2977  0.127   0.1444]\n",
      "MSE loss: 85.7281\n",
      "Iteration: 87300\n",
      "Gradient: [  -5.5559   -5.8537  -17.1961  -91.6062 -237.1763]\n",
      "Weights: [-4.8429  0.8441 -1.2949  0.1274  0.1443]\n",
      "MSE loss: 85.9716\n",
      "Iteration: 87400\n",
      "Gradient: [ 10.2267  17.7196  37.6359  25.1266 249.1904]\n",
      "Weights: [-4.8351  0.8502 -1.2936  0.1276  0.1441]\n",
      "MSE loss: 85.8281\n",
      "Iteration: 87500\n",
      "Gradient: [ -19.7116    5.2568  -44.2232  -55.0421 -452.5835]\n",
      "Weights: [-4.8632  0.8579 -1.2933  0.1269  0.1436]\n",
      "MSE loss: 86.7129\n",
      "Iteration: 87600\n",
      "Gradient: [-18.346    8.8416 -17.2103 204.1746 -36.6862]\n",
      "Weights: [-4.8453  0.8476 -1.2906  0.1271  0.1438]\n",
      "MSE loss: 85.7853\n",
      "Iteration: 87700\n",
      "Gradient: [  -8.1713   10.2836  -35.8025  -65.8811 -211.8802]\n",
      "Weights: [-4.8319  0.8353 -1.287   0.1267  0.1436]\n",
      "MSE loss: 85.9995\n",
      "Iteration: 87800\n",
      "Gradient: [  2.4729  24.6843 -17.8908 -56.2465 321.2048]\n",
      "Weights: [-4.8435  0.832  -1.2872  0.1281  0.1437]\n",
      "MSE loss: 85.9801\n",
      "Iteration: 87900\n",
      "Gradient: [ -15.259   -19.9936  -12.6576   44.6144 -788.1209]\n",
      "Weights: [-4.8364  0.8238 -1.2858  0.1285  0.1437]\n",
      "MSE loss: 85.9888\n",
      "Iteration: 88000\n",
      "Gradient: [   2.001     9.799    40.4228   22.8773 -186.3149]\n",
      "Weights: [-4.8299  0.8314 -1.2858  0.1291  0.1434]\n",
      "MSE loss: 85.7593\n",
      "Iteration: 88100\n",
      "Gradient: [  8.7325  -7.5401  -9.3267  31.1437 168.4558]\n",
      "Weights: [-4.8333  0.8362 -1.2858  0.128   0.1435]\n",
      "MSE loss: 85.7346\n",
      "Iteration: 88200\n",
      "Gradient: [ 26.9877  15.0461  57.2267 109.7766 216.9394]\n",
      "Weights: [-4.8207  0.8362 -1.2906  0.129   0.1436]\n",
      "MSE loss: 85.9208\n",
      "Iteration: 88300\n",
      "Gradient: [ 7.4862 -2.2366 -5.9445 75.38   97.6849]\n",
      "Weights: [-4.835   0.8378 -1.29    0.1288  0.1435]\n",
      "MSE loss: 85.6779\n",
      "Iteration: 88400\n",
      "Gradient: [  -4.7335  -27.7133   19.4585 -105.4553  208.759 ]\n",
      "Weights: [-4.8411  0.8395 -1.2899  0.1278  0.1438]\n",
      "MSE loss: 85.773\n",
      "Iteration: 88500\n",
      "Gradient: [ -9.3776  10.2243  64.934   -0.4337 377.7858]\n",
      "Weights: [-4.834   0.8323 -1.2913  0.1293  0.144 ]\n",
      "MSE loss: 85.7491\n",
      "Iteration: 88600\n",
      "Gradient: [  11.1709   -6.856    17.4909   48.9123 -377.3077]\n",
      "Weights: [-4.8311  0.8309 -1.2919  0.1297  0.1437]\n",
      "MSE loss: 85.7533\n",
      "Iteration: 88700\n",
      "Gradient: [-2.3258 25.6118 52.5609 60.3855 11.0576]\n",
      "Weights: [-4.8174  0.8315 -1.2947  0.1299  0.144 ]\n",
      "MSE loss: 85.8278\n",
      "Iteration: 88800\n",
      "Gradient: [ 16.7845  15.6231  89.9332  90.8828 235.4055]\n",
      "Weights: [-4.8181  0.8462 -1.2969  0.1302  0.1438]\n",
      "MSE loss: 86.5753\n",
      "Iteration: 88900\n",
      "Gradient: [  1.8593   2.5502   6.3648 -68.924  281.1052]\n",
      "Weights: [-4.8382  0.849  -1.2958  0.1288  0.1439]\n",
      "MSE loss: 85.6638\n",
      "Iteration: 89000\n",
      "Gradient: [   2.4272  -24.9974  -16.1527   25.2273 -198.0411]\n",
      "Weights: [-4.8418  0.8471 -1.2946  0.1294  0.1437]\n",
      "MSE loss: 85.6814\n",
      "Iteration: 89100\n",
      "Gradient: [ -12.5038  -38.4512  -97.0021 -179.9814 -430.5726]\n",
      "Weights: [-4.8456  0.845  -1.294   0.1289  0.1435]\n",
      "MSE loss: 86.046\n",
      "Iteration: 89200\n",
      "Gradient: [ -0.9536  16.8049 -14.561   36.6885 365.8326]\n",
      "Weights: [-4.8365  0.8438 -1.2917  0.1296  0.1434]\n",
      "MSE loss: 85.7073\n",
      "Iteration: 89300\n",
      "Gradient: [ 12.9659 -15.537  -61.6132  81.63    38.8817]\n",
      "Weights: [-4.834   0.8478 -1.2942  0.1298  0.1434]\n",
      "MSE loss: 85.7312\n",
      "Iteration: 89400\n",
      "Gradient: [ -10.3666   21.2652  -14.4963 -179.8448   85.4501]\n",
      "Weights: [-4.8431  0.841  -1.2916  0.1302  0.1431]\n",
      "MSE loss: 85.7598\n",
      "Iteration: 89500\n",
      "Gradient: [  -8.595    33.1139  -47.0622   -4.562  -130.7815]\n",
      "Weights: [-4.8396  0.8309 -1.2892  0.1311  0.1431]\n",
      "MSE loss: 85.7544\n",
      "Iteration: 89600\n",
      "Gradient: [ -9.1868 -13.6187 -95.1017 -75.4441 -28.2564]\n",
      "Weights: [-4.8341  0.8304 -1.2904  0.1315  0.1428]\n",
      "MSE loss: 85.7257\n",
      "Iteration: 89700\n",
      "Gradient: [ 24.9672  34.5833  48.8353  47.3224 414.2191]\n",
      "Weights: [-4.8323  0.8452 -1.2896  0.1307  0.1428]\n",
      "MSE loss: 86.1627\n",
      "Iteration: 89800\n",
      "Gradient: [ 15.2552  -0.8504  -3.9957  54.8044 488.164 ]\n",
      "Weights: [-4.8302  0.8468 -1.2942  0.1312  0.1429]\n",
      "MSE loss: 85.8728\n",
      "Iteration: 89900\n",
      "Gradient: [  -1.9595  -16.4121  -44.7311  -57.8335 -277.398 ]\n",
      "Weights: [-4.8491  0.8547 -1.2954  0.1308  0.143 ]\n",
      "MSE loss: 85.6875\n",
      "Iteration: 90000\n",
      "Gradient: [  -8.9065  -12.6329   43.8174  -63.5859 -228.1001]\n",
      "Weights: [-4.8407  0.8499 -1.2941  0.1302  0.1431]\n",
      "MSE loss: 85.6645\n",
      "Iteration: 90100\n",
      "Gradient: [  2.0046  27.7924  37.717  -30.0035 327.9355]\n",
      "Weights: [-4.8515  0.8595 -1.294   0.1293  0.1432]\n",
      "MSE loss: 85.7339\n",
      "Iteration: 90200\n",
      "Gradient: [ 10.2316  15.2512 -33.5923  17.1424  13.3633]\n",
      "Weights: [-4.8495  0.8689 -1.2949  0.1281  0.143 ]\n",
      "MSE loss: 85.9053\n",
      "Iteration: 90300\n",
      "Gradient: [ -17.0016   -8.9629   -5.1582 -135.6557  104.8252]\n",
      "Weights: [-4.8525  0.8574 -1.2944  0.1282  0.1434]\n",
      "MSE loss: 85.9115\n",
      "Iteration: 90400\n",
      "Gradient: [  -6.7555   11.7208  -34.4275   33.3173 -130.2605]\n",
      "Weights: [-4.8427  0.8555 -1.2955  0.1288  0.1435]\n",
      "MSE loss: 85.682\n",
      "Iteration: 90500\n",
      "Gradient: [  -2.3698   -6.3354   14.5674 -110.7428 -141.2926]\n",
      "Weights: [-4.8345  0.8458 -1.2964  0.1287  0.1439]\n",
      "MSE loss: 85.7466\n",
      "Iteration: 90600\n",
      "Gradient: [  6.3438  30.8779  36.195   65.6267 476.0269]\n",
      "Weights: [-4.8168  0.8377 -1.2913  0.1289  0.1439]\n",
      "MSE loss: 86.5105\n",
      "Iteration: 90700\n",
      "Gradient: [ -0.7135   2.5651  24.5901  87.77   119.1286]\n",
      "Weights: [-4.838   0.8416 -1.2946  0.1299  0.1438]\n",
      "MSE loss: 85.7123\n",
      "Iteration: 90800\n",
      "Gradient: [ -4.2106  15.074   27.8704 108.2623  -9.2976]\n",
      "Weights: [-4.8318  0.8479 -1.2948  0.1296  0.1435]\n",
      "MSE loss: 85.7538\n",
      "Iteration: 90900\n",
      "Gradient: [  -6.8902  -18.057   -22.2752 -109.0631 -114.1108]\n",
      "Weights: [-4.8553  0.8635 -1.2993  0.1302  0.1433]\n",
      "MSE loss: 85.7634\n",
      "Iteration: 91000\n",
      "Gradient: [  -1.4302   -9.9441  -20.9753  -35.0437 -106.8977]\n",
      "Weights: [-4.8447  0.8502 -1.2979  0.1299  0.1435]\n",
      "MSE loss: 85.8631\n",
      "Iteration: 91100\n",
      "Gradient: [  17.6223    6.4215   52.5643  147.5466 -128.4782]\n",
      "Weights: [-4.8169  0.843  -1.295   0.1304  0.1435]\n",
      "MSE loss: 86.3751\n",
      "Iteration: 91200\n",
      "Gradient: [ -5.136   -3.9647  -4.281  122.4146 -86.22  ]\n",
      "Weights: [-4.8375  0.8382 -1.2949  0.1304  0.1434]\n",
      "MSE loss: 85.8279\n",
      "Iteration: 91300\n",
      "Gradient: [  0.3896  20.9355   6.8036 139.3218 117.3905]\n",
      "Weights: [-4.8477  0.8534 -1.2937  0.1291  0.1434]\n",
      "MSE loss: 85.6976\n",
      "Iteration: 91400\n",
      "Gradient: [  9.6057   2.1813  66.508   90.7088 226.5539]\n",
      "Weights: [-4.8283  0.8418 -1.2916  0.1293  0.1437]\n",
      "MSE loss: 85.9885\n",
      "Iteration: 91500\n",
      "Gradient: [-16.5516  -4.711  -39.4263   5.279  182.9608]\n",
      "Weights: [-4.8646  0.8451 -1.2912  0.1293  0.1435]\n",
      "MSE loss: 86.6604\n",
      "Iteration: 91600\n",
      "Gradient: [ -4.5759 -14.6291  49.8306 -92.0786  35.2233]\n",
      "Weights: [-4.8388  0.8512 -1.2924  0.129   0.1434]\n",
      "MSE loss: 85.7669\n",
      "Iteration: 91700\n",
      "Gradient: [ 5.0644 22.3648 14.9353  3.7667 55.1253]\n",
      "Weights: [-4.8316  0.8533 -1.2948  0.1292  0.1432]\n",
      "MSE loss: 85.8541\n",
      "Iteration: 91800\n",
      "Gradient: [ -14.1938  -14.1532  -46.0569   37.7534 -235.7465]\n",
      "Weights: [-4.8395  0.8466 -1.2954  0.1295  0.1436]\n",
      "MSE loss: 85.674\n",
      "Iteration: 91900\n",
      "Gradient: [ -13.8837  -15.2527  -59.3656 -144.9865  196.9007]\n",
      "Weights: [-4.8447  0.8477 -1.2985  0.1295  0.144 ]\n",
      "MSE loss: 85.8807\n",
      "Iteration: 92000\n",
      "Gradient: [   9.1861    6.4415   15.7699   33.8314 -238.4338]\n",
      "Weights: [-4.833   0.8574 -1.3015  0.1292  0.1441]\n",
      "MSE loss: 85.7694\n",
      "Iteration: 92100\n",
      "Gradient: [-9.387800e+00  1.245000e-01 -3.064310e+01  7.444350e+01 -2.848827e+02]\n",
      "Weights: [-4.8472  0.8627 -1.304   0.1294  0.1441]\n",
      "MSE loss: 85.7012\n",
      "Iteration: 92200\n",
      "Gradient: [   1.2835   24.0528    3.9296  -40.4448 -116.4352]\n",
      "Weights: [-4.8331  0.8598 -1.3035  0.1299  0.1439]\n",
      "MSE loss: 85.7308\n",
      "Iteration: 92300\n",
      "Gradient: [ -15.5899   -8.2325  -35.1553  -97.4789 -241.7051]\n",
      "Weights: [-4.8482  0.854  -1.3026  0.1307  0.1438]\n",
      "MSE loss: 85.9078\n",
      "Iteration: 92400\n",
      "Gradient: [ -13.2913  -13.4988  -35.6633 -121.2063  -68.0457]\n",
      "Weights: [-4.8576  0.8608 -1.3032  0.1312  0.1434]\n",
      "MSE loss: 86.0817\n",
      "Iteration: 92500\n",
      "Gradient: [  -4.775    14.2912  -18.2934  -38.2831 -121.8492]\n",
      "Weights: [-4.84    0.8643 -1.3038  0.1313  0.1433]\n",
      "MSE loss: 85.6906\n",
      "Iteration: 92600\n",
      "Gradient: [   4.9208   -6.7197    5.99    -55.1502 -373.8496]\n",
      "Weights: [-4.8482  0.8589 -1.305   0.1327  0.1431]\n",
      "MSE loss: 85.7734\n",
      "Iteration: 92700\n",
      "Gradient: [ -3.2241   9.9118 -12.8792  18.4858  41.8852]\n",
      "Weights: [-4.841   0.852  -1.3029  0.1335  0.1429]\n",
      "MSE loss: 85.624\n",
      "Iteration: 92800\n",
      "Gradient: [ 20.3467  25.856    5.4767 131.5491 617.7452]\n",
      "Weights: [-4.8256  0.8575 -1.304   0.1331  0.1429]\n",
      "MSE loss: 86.0166\n",
      "Iteration: 92900\n",
      "Gradient: [ -12.8423   -5.6799  -50.0676 -225.1107  -25.2828]\n",
      "Weights: [-4.8473  0.8583 -1.305   0.1327  0.143 ]\n",
      "MSE loss: 85.8828\n",
      "Iteration: 93000\n",
      "Gradient: [  4.0469  16.1433  -6.3947 142.561   47.3366]\n",
      "Weights: [-4.8227  0.8483 -1.3089  0.1346  0.1433]\n",
      "MSE loss: 85.7108\n",
      "Iteration: 93100\n",
      "Gradient: [  11.6748  -24.7192   35.4352  150.3126 -246.3618]\n",
      "Weights: [-4.8283  0.8622 -1.3119  0.1347  0.1431]\n",
      "MSE loss: 85.7544\n",
      "Iteration: 93200\n",
      "Gradient: [ -2.1961   1.904   48.8974  14.473  215.3303]\n",
      "Weights: [-4.8427  0.8644 -1.3142  0.136   0.1433]\n",
      "MSE loss: 85.6422\n",
      "Iteration: 93300\n",
      "Gradient: [-12.3669   5.5192   4.7546 206.0379 618.6825]\n",
      "Weights: [-4.8414  0.8653 -1.3122  0.1354  0.1431]\n",
      "MSE loss: 85.6092\n",
      "Iteration: 93400\n",
      "Gradient: [ 15.3414  17.1965   8.3019   8.9388 142.8911]\n",
      "Weights: [-4.8392  0.8644 -1.3143  0.1356  0.143 ]\n",
      "MSE loss: 85.5874\n",
      "Iteration: 93500\n",
      "Gradient: [  6.8612  14.8445  92.1294 113.1304 410.4957]\n",
      "Weights: [-4.8394  0.8585 -1.3114  0.1366  0.1429]\n",
      "MSE loss: 85.6238\n",
      "Iteration: 93600\n",
      "Gradient: [ -17.7463   36.1684   43.8061   52.4082 -252.3591]\n",
      "Weights: [-4.8317  0.8508 -1.3101  0.1378  0.1425]\n",
      "MSE loss: 85.6529\n",
      "Iteration: 93700\n",
      "Gradient: [  4.5311  19.4805  16.0061  59.4236 -23.3843]\n",
      "Weights: [-4.8457  0.8728 -1.3136  0.1377  0.1424]\n",
      "MSE loss: 85.9414\n",
      "Iteration: 93800\n",
      "Gradient: [ 18.1312  13.3589  18.7156 110.1093 490.4206]\n",
      "Weights: [-4.8506  0.8806 -1.3188  0.1378  0.1427]\n",
      "MSE loss: 85.8027\n",
      "Iteration: 93900\n",
      "Gradient: [  -4.0002   11.8705   43.9033   68.2714 -200.4907]\n",
      "Weights: [-4.8583  0.8741 -1.3172  0.1385  0.1424]\n",
      "MSE loss: 85.6659\n",
      "Iteration: 94000\n",
      "Gradient: [  7.1641   9.9825 -31.5329 -48.3519  77.3233]\n",
      "Weights: [-4.8419  0.8819 -1.3193  0.1372  0.1422]\n",
      "MSE loss: 85.7261\n",
      "Iteration: 94100\n",
      "Gradient: [  7.8248  10.1875 -52.2771 240.7515 272.3873]\n",
      "Weights: [-4.8466  0.8908 -1.3238  0.138   0.1424]\n",
      "MSE loss: 85.7318\n",
      "Iteration: 94200\n",
      "Gradient: [ -12.0739  -46.4394 -127.0397 -102.4727 -249.0423]\n",
      "Weights: [-4.854   0.8802 -1.3222  0.1381  0.1425]\n",
      "MSE loss: 85.7147\n",
      "Iteration: 94300\n",
      "Gradient: [ -13.1815    1.5552  -40.2877 -106.8369 -123.1913]\n",
      "Weights: [-4.844   0.8644 -1.3235  0.1396  0.1424]\n",
      "MSE loss: 86.245\n",
      "Iteration: 94400\n",
      "Gradient: [ -1.0889  12.8575  34.1039  74.1383 -21.8636]\n",
      "Weights: [-4.8439  0.8741 -1.3223  0.1404  0.1423]\n",
      "MSE loss: 85.628\n",
      "Iteration: 94500\n",
      "Gradient: [  2.4741  21.7157  37.211  198.8798 389.778 ]\n",
      "Weights: [-4.8498  0.87   -1.3192  0.1397  0.1423]\n",
      "MSE loss: 85.5757\n",
      "Iteration: 94600\n",
      "Gradient: [  -4.1479   -7.5388  -23.8737 -122.9307  201.896 ]\n",
      "Weights: [-4.8462  0.8712 -1.3214  0.1399  0.1422]\n",
      "MSE loss: 85.5421\n",
      "Iteration: 94700\n",
      "Gradient: [-1.8786  4.7881 25.903  40.6284 89.3686]\n",
      "Weights: [-4.8341  0.8579 -1.317   0.1401  0.1423]\n",
      "MSE loss: 85.6041\n",
      "Iteration: 94800\n",
      "Gradient: [ 6.455600e+00 -1.945600e+00 -2.421000e-01 -7.835060e+01 -2.786415e+02]\n",
      "Weights: [-4.8277  0.852  -1.3211  0.1407  0.1421]\n",
      "MSE loss: 85.9772\n",
      "Iteration: 94900\n",
      "Gradient: [  -9.0423  -12.5803  -10.619  -104.1244 -286.4482]\n",
      "Weights: [-4.8492  0.871  -1.3226  0.14    0.1422]\n",
      "MSE loss: 85.7013\n",
      "Iteration: 95000\n",
      "Gradient: [ -2.9105 -17.7537  36.2749  66.9286 -19.8956]\n",
      "Weights: [-4.8467  0.8739 -1.3216  0.1401  0.1421]\n",
      "MSE loss: 85.5131\n",
      "Iteration: 95100\n",
      "Gradient: [  -5.0967    3.5091  -55.0699  128.3905 -134.3967]\n",
      "Weights: [-4.8402  0.8706 -1.3199  0.1397  0.1421]\n",
      "MSE loss: 85.5329\n",
      "Iteration: 95200\n",
      "Gradient: [ -3.3004   7.6856  36.8794  73.9543 -44.7219]\n",
      "Weights: [-4.8362  0.8599 -1.3188  0.1409  0.142 ]\n",
      "MSE loss: 85.5172\n",
      "Iteration: 95300\n",
      "Gradient: [  7.5959  36.1594 -12.3046 139.6817 111.3627]\n",
      "Weights: [-4.8367  0.8623 -1.3217  0.1423  0.1421]\n",
      "MSE loss: 85.7142\n",
      "Iteration: 95400\n",
      "Gradient: [  12.2788   15.9387   16.454  -111.6363 -167.9383]\n",
      "Weights: [-4.8416  0.8697 -1.3214  0.1414  0.1419]\n",
      "MSE loss: 85.5415\n",
      "Iteration: 95500\n",
      "Gradient: [ 8.4795  5.6019  4.7073  5.2476 88.1647]\n",
      "Weights: [-4.8321  0.8567 -1.3196  0.1417  0.142 ]\n",
      "MSE loss: 85.5296\n",
      "Iteration: 95600\n",
      "Gradient: [-26.2718 -24.5796 -55.1992 -67.6426 -70.8575]\n",
      "Weights: [-4.8434  0.8513 -1.3195  0.1424  0.1418]\n",
      "MSE loss: 85.9388\n",
      "Iteration: 95700\n",
      "Gradient: [  27.3385   17.1873   17.2316   38.9754 -154.0355]\n",
      "Weights: [-4.8428  0.8514 -1.3129  0.142   0.1414]\n",
      "MSE loss: 85.5622\n",
      "Iteration: 95800\n",
      "Gradient: [ 11.7967  31.8187 -27.7768  69.9613 545.8854]\n",
      "Weights: [-4.8452  0.8579 -1.3108  0.1425  0.1409]\n",
      "MSE loss: 85.7258\n",
      "Iteration: 95900\n",
      "Gradient: [ -13.7091  -27.4775  -64.879    26.1351 -226.1344]\n",
      "Weights: [-4.8538  0.8566 -1.3121  0.1424  0.1408]\n",
      "MSE loss: 85.7476\n",
      "Iteration: 96000\n",
      "Gradient: [  7.9439  -3.901   -9.1293 122.0646 155.8029]\n",
      "Weights: [-4.8443  0.8618 -1.3157  0.1432  0.1408]\n",
      "MSE loss: 85.5126\n",
      "Iteration: 96100\n",
      "Gradient: [   8.9564  -20.6581  -50.7486 -105.1251 -415.6977]\n",
      "Weights: [-4.8359  0.8579 -1.3196  0.1442  0.1407]\n",
      "MSE loss: 85.5591\n",
      "Iteration: 96200\n",
      "Gradient: [ -12.987    -0.3293  -30.9976   62.2559 -240.2018]\n",
      "Weights: [-4.8414  0.8585 -1.318   0.1442  0.1407]\n",
      "MSE loss: 85.481\n",
      "Iteration: 96300\n",
      "Gradient: [-10.3667  -4.3058 -42.6794  70.8547  64.7396]\n",
      "Weights: [-4.8435  0.8501 -1.3161  0.1455  0.1405]\n",
      "MSE loss: 85.5554\n",
      "Iteration: 96400\n",
      "Gradient: [  4.9589 -24.2507  25.0494  17.6326 -44.1698]\n",
      "Weights: [-4.8422  0.8511 -1.3139  0.1447  0.1402]\n",
      "MSE loss: 85.573\n",
      "Iteration: 96500\n",
      "Gradient: [-20.5937 -35.4301  -1.1864 -25.4361 213.7599]\n",
      "Weights: [-4.8562  0.8472 -1.3118  0.1454  0.14  ]\n",
      "MSE loss: 86.1691\n",
      "Iteration: 96600\n",
      "Gradient: [  2.6083  11.2684  34.6444 -18.7199  33.0419]\n",
      "Weights: [-4.849   0.8593 -1.3146  0.1447  0.1402]\n",
      "MSE loss: 85.533\n",
      "Iteration: 96700\n",
      "Gradient: [   7.1563  -23.2356  -32.5025  -74.0724 -337.0321]\n",
      "Weights: [-4.8587  0.8666 -1.3156  0.1448  0.1401]\n",
      "MSE loss: 85.6514\n",
      "Iteration: 96800\n",
      "Gradient: [ -2.5019  38.1652   3.9618 -23.267  218.3772]\n",
      "Weights: [-4.8528  0.875  -1.3207  0.1451  0.1403]\n",
      "MSE loss: 85.5709\n",
      "Iteration: 96900\n",
      "Gradient: [ 16.4537   9.6649  85.2559  86.8292 367.2396]\n",
      "Weights: [-4.8555  0.8746 -1.3226  0.1455  0.1405]\n",
      "MSE loss: 85.5547\n",
      "Iteration: 97000\n",
      "Gradient: [  4.9136   8.941   38.1922  58.6399 116.5617]\n",
      "Weights: [-4.8465  0.8692 -1.3214  0.1452  0.1405]\n",
      "MSE loss: 85.4847\n",
      "Iteration: 97100\n",
      "Gradient: [ 16.636   22.9085  44.4321 197.5322  88.5518]\n",
      "Weights: [-4.8296  0.8628 -1.324   0.1459  0.1411]\n",
      "MSE loss: 86.0614\n",
      "Iteration: 97200\n",
      "Gradient: [ -14.7578   -5.702   -38.3872 -120.2223 -444.0888]\n",
      "Weights: [-4.8505  0.8685 -1.3283  0.1457  0.141 ]\n",
      "MSE loss: 85.8111\n",
      "Iteration: 97300\n",
      "Gradient: [  11.8686  -12.9424  -42.0273   73.0782 -385.9219]\n",
      "Weights: [-4.8434  0.8697 -1.3254  0.1449  0.1409]\n",
      "MSE loss: 85.4668\n",
      "Iteration: 97400\n",
      "Gradient: [   8.3     -21.03    -66.0334 -105.9823  201.5562]\n",
      "Weights: [-4.8581  0.8694 -1.3208  0.1441  0.1406]\n",
      "MSE loss: 85.8299\n",
      "Iteration: 97500\n",
      "Gradient: [ 15.5441 -17.1811  34.2469 -96.3113  66.3495]\n",
      "Weights: [-4.8429  0.8562 -1.3168  0.1444  0.1406]\n",
      "MSE loss: 85.4996\n",
      "Iteration: 97600\n",
      "Gradient: [ 21.4397  31.5143  76.6929  35.1416 358.0427]\n",
      "Weights: [-4.8354  0.87   -1.3177  0.1447  0.1403]\n",
      "MSE loss: 86.1694\n",
      "Iteration: 97700\n",
      "Gradient: [   9.0305  -25.7915   22.6007 -131.2319 -423.9507]\n",
      "Weights: [-4.8406  0.8567 -1.3203  0.1457  0.1405]\n",
      "MSE loss: 85.5101\n",
      "Iteration: 97800\n",
      "Gradient: [ 12.6735   8.1875  29.1561 -36.4031 172.5012]\n",
      "Weights: [-4.8288  0.8631 -1.3201  0.1453  0.1404]\n",
      "MSE loss: 85.7631\n",
      "Iteration: 97900\n",
      "Gradient: [  -6.9012   -9.7989  -31.7712  -72.9574 -554.0199]\n",
      "Weights: [-4.8599  0.8635 -1.3203  0.1459  0.1404]\n",
      "MSE loss: 85.9276\n",
      "Iteration: 98000\n",
      "Gradient: [ 12.6325  30.7084  57.0606 169.1704 129.2724]\n",
      "Weights: [-4.8309  0.8637 -1.3212  0.146   0.1404]\n",
      "MSE loss: 85.7917\n",
      "Iteration: 98100\n",
      "Gradient: [ -19.3479  -23.7144  -31.9433 -181.4893  -82.1455]\n",
      "Weights: [-4.8441  0.8691 -1.323   0.1458  0.1405]\n",
      "MSE loss: 85.4743\n",
      "Iteration: 98200\n",
      "Gradient: [   2.3333  -11.7326   24.7274 -161.6382 -122.6269]\n",
      "Weights: [-4.8337  0.8622 -1.3236  0.1458  0.1404]\n",
      "MSE loss: 85.5205\n",
      "Iteration: 98300\n",
      "Gradient: [  6.6868  21.6372  40.4942 106.3765 503.2554]\n",
      "Weights: [-4.8393  0.8617 -1.3248  0.1469  0.1407]\n",
      "MSE loss: 85.4605\n",
      "Iteration: 98400\n",
      "Gradient: [ 11.4595  29.9676   7.2028 157.2382 429.6467]\n",
      "Weights: [-4.8274  0.8597 -1.3258  0.148   0.1404]\n",
      "MSE loss: 85.63\n",
      "Iteration: 98500\n",
      "Gradient: [  1.169  -13.7397   6.7214  52.4515 152.1091]\n",
      "Weights: [-4.8347  0.8573 -1.3238  0.1476  0.1405]\n",
      "MSE loss: 85.4711\n",
      "Iteration: 98600\n",
      "Gradient: [-0.79   11.5998  5.0452 28.2273 26.5305]\n",
      "Weights: [-4.8164  0.833  -1.3223  0.1485  0.1408]\n",
      "MSE loss: 85.645\n",
      "Iteration: 98700\n",
      "Gradient: [  23.7024   36.2329   -8.2567 -123.4798  271.8476]\n",
      "Weights: [-4.8221  0.8479 -1.3206  0.1478  0.1403]\n",
      "MSE loss: 85.6599\n",
      "Iteration: 98800\n",
      "Gradient: [   5.9442  -11.1731   -2.5511 -114.2822 -288.9982]\n",
      "Weights: [-4.8351  0.8517 -1.3193  0.1472  0.14  ]\n",
      "MSE loss: 85.4468\n",
      "Iteration: 98900\n",
      "Gradient: [  -3.287    -7.3605  -15.3739   27.9869 -334.9848]\n",
      "Weights: [-4.8394  0.8545 -1.3171  0.1465  0.1398]\n",
      "MSE loss: 85.5034\n",
      "Iteration: 99000\n",
      "Gradient: [   3.6875   -1.2925  -17.4568  -68.1579 -129.3139]\n",
      "Weights: [-4.8465  0.8548 -1.3172  0.1471  0.14  ]\n",
      "MSE loss: 85.5246\n",
      "Iteration: 99100\n",
      "Gradient: [ -12.0444  -17.1038  -11.9305  -27.8337 -464.2743]\n",
      "Weights: [-4.8496  0.851  -1.3174  0.1465  0.14  ]\n",
      "MSE loss: 85.9381\n",
      "Iteration: 99200\n",
      "Gradient: [ -2.2911   4.3245  69.668  -32.9903 581.197 ]\n",
      "Weights: [-4.8334  0.837  -1.3144  0.148   0.1399]\n",
      "MSE loss: 85.5122\n",
      "Iteration: 99300\n",
      "Gradient: [ -5.9567  -0.462  -49.1282  55.0774 270.0325]\n",
      "Weights: [-4.8271  0.8365 -1.3132  0.1474  0.1396]\n",
      "MSE loss: 85.5459\n",
      "Iteration: 99400\n",
      "Gradient: [-4.500000e-02 -1.322130e+01 -5.539550e+01 -3.793230e+01 -1.599336e+02]\n",
      "Weights: [-4.8231  0.826  -1.3092  0.1478  0.1395]\n",
      "MSE loss: 85.5199\n",
      "Iteration: 99500\n",
      "Gradient: [   6.9228   20.4077   37.6817  -63.903  -423.381 ]\n",
      "Weights: [-4.8114  0.8395 -1.3152  0.1488  0.1393]\n",
      "MSE loss: 85.9803\n",
      "Iteration: 99600\n",
      "Gradient: [-13.1483 -28.4853 -10.5851  95.3507 148.4353]\n",
      "Weights: [-4.835   0.8492 -1.3189  0.1488  0.1395]\n",
      "MSE loss: 85.4316\n",
      "Iteration: 99700\n",
      "Gradient: [   1.3367  -18.4582  -21.1593 -144.0031 -273.5567]\n",
      "Weights: [-4.8312  0.8491 -1.3185  0.1495  0.1395]\n",
      "MSE loss: 85.5934\n",
      "Iteration: 99800\n",
      "Gradient: [  6.8133  33.2053 -19.2676  65.1163 103.3408]\n",
      "Weights: [-4.8387  0.8628 -1.3248  0.1498  0.1395]\n",
      "MSE loss: 85.4722\n",
      "Iteration: 99900\n",
      "Gradient: [   8.9987   -4.5609   -0.7203  -22.6577 -260.2461]\n",
      "Weights: [-4.8349  0.8524 -1.3234  0.1497  0.1395]\n",
      "MSE loss: 85.4943\n"
     ]
    }
   ],
   "source": [
    "# Отключен Momentum.\n",
    "# Обучение на полном датасете.\n",
    "'''\n",
    "weights_2, losses_2, iter_final_2, fit_time_2 = model_fit(X_train, y_train,\n",
    "                                                          learning_rate=[1e-5, 1e-6, 1e-7, 1e-8, 1e-9],\n",
    "                                                          tolerance=(0.1**2 * N_points),\n",
    "                                                          beta=0,\n",
    "                                                          batch_ratio=1.0,\n",
    "                                                          lr_scaling=False,\n",
    "                                                          liveplot=False)\n",
    "'''\n",
    "weights_2, losses_2, iter_final_2, fit_time_2 = model_fit(X_train, y_train,\n",
    "                                                          learning_rate=[1e-3, 1e-4, 1e-5, 1e-6, 1e-7],\n",
    "                                                          tolerance=(0.1**2 * N_points),\n",
    "                                                          beta=0.8,\n",
    "                                                          batch_ratio=0.2,\n",
    "                                                          lr_scaling=False,\n",
    "                                                          liveplot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51dbe209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nweights_3, losses_3, iter_final_3, fit_time_3 = model_fit(X_train, y_train,\\n                                                          learning_rate=[1e-3, 1e-4, 1e-5, 1e-6, 1e-7],\\n                                                          tolerance=(0.1**2 * N_points),\\n                                                          beta=0,\\n                                                          batch_ratio=0.1,\\n                                                          lr_scaling=False,\\n                                                          liveplot=False)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "weights_3, losses_3, iter_final_3, fit_time_3 = model_fit(X_train, y_train,\n",
    "                                                          learning_rate=[1e-3, 1e-4, 1e-5, 1e-6, 1e-7],\n",
    "                                                          tolerance=(0.1**2 * N_points),\n",
    "                                                          beta=0,\n",
    "                                                          batch_ratio=0.1,\n",
    "                                                          lr_scaling=False,\n",
    "                                                          liveplot=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63ccc912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nweights_4, losses_4, iter_final_4, fit_time_4 = model_fit(X_train, y_train,\\n                                                          learning_rate=[1e-3, 1e-4, 1e-5, 1e-6, 1e-7],\\n                                                          tolerance=(0.1**2 * N_points),\\n                                                          beta=0.8,\\n                                                          batch_ratio=0.1,\\n                                                          lr_scaling=False,\\n                                                          liveplot=False)\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "weights_4, losses_4, iter_final_4, fit_time_4 = model_fit(X_train, y_train,\n",
    "                                                          learning_rate=[1e-3, 1e-4, 1e-5, 1e-6, 1e-7],\n",
    "                                                          tolerance=(0.1**2 * N_points),\n",
    "                                                          beta=0.8,\n",
    "                                                          batch_ratio=0.1,\n",
    "                                                          lr_scaling=False,\n",
    "                                                          liveplot=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8be7e7b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses_1, '.')\n",
    "plt.plot(losses_2, '.')\n",
    "#plt.plot(losses_3, '--')\n",
    "#plt.plot(losses_4, '--')\n",
    "plt.ylim(0,5000)\n",
    "plt.legend(range(2))\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a399437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_train, y_train)\n",
    "plt.plot(np.sort(X_train), f_trend(np.sort(X_train)), color='red')\n",
    "plt.plot(np.sort(X_train), f_poly(np.sort(X_train), weights_1), '.', color='black')\n",
    "plt.plot(np.sort(X_train), f_poly(np.sort(X_train), weights_2), '.', color='green')\n",
    "#plt.plot(np.sort(X_train), f_poly(np.sort(X_train), weights_3), '--', color='blue')\n",
    "#plt.plot(np.sort(X_train), f_poly(np.sort(X_train), weights_4), '--', color='yellow')\n",
    "plt.legend(range(4))\n",
    "plt.show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

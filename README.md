# Задача поиска оптимальных коэффициентов полиномиальной модели методом градиентного спуска

## Notebooks
- 'grad_method.ipynb' - разбираемся как работать с __[numpy.gradient](https://numpy.org/doc/2.0/reference/generated/numpy.gradient.html#numpy-gradient)__.
- 'ML_2.ipynb' - основной файл.

## Набор данных
Сгенерирован датасет синтетических данных размера ${N=2000}$.

В основу генерации положена полиномиальная функция с добавлением случайного шума ${\epsilon}$

${f(x) = \sum_{k=0}^{K-1}{w_k \cdot x^k} + \epsilon}$,

где ${w=[w_0, w_1, ..., w_{K-1}]}$ - массив весов размера ${K}$.

Итоговый датасет [x, y] сгенерирован для ${x \in [0; 3]}$ при следующих весах:
${w=[-5, 2, -3, 1]}$ ${(K=4)}$.

![dataset](plots/dataset_plot.png)

## Постановка задачи
Необходимо найти веса модели, представленной на рисунке ниже.
Количество весов принято большим на одно ${(K=5)}$, в ожидании получить нулевое значение этого искусственно добавленного коэффициента.
![architecture](plots/architecture.png)

## Метод решения
Поиск коэффициентов осуществлялся путем минимизации функции потерь.

${loss(w) = \sum \left(y_i - \bar{y}_i(w) \right)^2}$

Минимизация методом градиентного спуска.

Ожидаемые значения весов: ${w=[-5, 2, -3, 1, 0]}$.

В целях ускорения сходимости метода исследованы следующие гиперпараметры:

- Шаг обучения (learning rate).
- Размер тренировочной выборки (batch size).
- Постоянная $\beta$ фильтра градиентов (Momentum):

${G_n = \beta G_{n-1} + (1 - \beta)grad_n}$

Градиент вычислялся численно, с помощью __[numpy.gradient](https://numpy.org/doc/2.0/reference/generated/numpy.gradient.html#numpy-gradient)__.

## Темп сходимости
На графиках ниже представлены 4 наиболее репрезентативных варианта набора гиперпараметров и соответствующая им столбчатая диаграмма продолжительности вычислений. Время измерялось с помощью __[time.time()](https://docs.python.org/3/library/time.html)__

![MSE](plots/MSE_plot.png)

![time](plots/time_plot.png)

## Результаты
Среди определенных значений весов ближайшие к эталонным достигнуты при параметрах
learning rate=[1e-3, 1e-4, 1e-5, 1e-6, 1e-7],
batch=0.2,
beta=0.0.

По результатам 200 тыс. итераций значения весов следующие:
w=[-4.8173  1.0053 -1.6231  0.3159  0.1114]

В результате наиболее быстрой сходимости достаточно всего 10 тыс. итераций. Остальные 190 тыс. итераций практически не обеспечивают прирост точности.

![Trend lines](plots/trend_line_plot.png)


## Выводы
1. 'Learning rate' - ключевой параметр, влияющий на скорость обучения (темп сходимости).
1. В задачах, когда у минимизируемой функции чувствительность к параметрам не одинакова (как в рассмотренном тут случае) имеет смысл задавать 'learning rate' индивидуальный для каждого искомого параметра: чем больше чувствительность функции к параметру, тем меньше 'learning rate'.
1. Выполняя оптимизацию не на полном датасете (разбиением на батчи) удается сохранять устойчивость метода при бОльших значениях 'learning rate'. Таким образом, не только улучшается темп сходимости, но и снижается продолжительность обучения за счет меньшего числа вычислительных операций на каждом шаге.
1. В рассмотренном тут частном случае фильтрация градиента (Momentum) оказалась бесполезной.
1. Предположительно, асимптотический предел ошибки в данной примере определяется среднеквадратическим отклонением шумов, наложенных при генерации синтетических данных. Например, установленному значению ${\sigma = 0.2}$ соответствует сумма квадратом отклонений ${0.2^2\cdot2000=80}$ (достигнуто 83.0865). Дополнительный эксперимент при ${\sigma = 0.1}$ подтверждает эту гипотезу (сумма квадратов стремится к 20).
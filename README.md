# Задача поиска оптимальных коэффициентов полиномиальной модели методом градиентного спуска

## Notebooks
- 'grad_method.ipynb' - разбираемся как работать с __[numpy.gradient](https://numpy.org/doc/2.0/reference/generated/numpy.gradient.html#numpy-gradient)__.
- 'ML_2.ipynb' - основной файл.

## Набор данных
Сгенерирован датасет синтетических данных размера ${N=2000}$.

В основу генерации положена полиномиальная функция с добавлением случайного шума ${\epsilon}$

${f(x) = \sum_{k=0}^{K-1}{w_k \cdot x^k} + \epsilon}$,

где ${w=[w_0, w_1, ..., w_{K-1}]}$ - массив весов размера ${K}$.

Итоговый датасет [x, y] сгенерирован для ${x \in [0; 3]}$ при следующих весах:
${w=[-5, 2, -3, 1]}$ ${(K=4)}$.
![dataset](plots/dataset_plot.png)

## Постановка задачи
Необходимо найти веса модели, представленной на рисунке ниже.
Количество весов принято большим на одно ${(K=5)}$, в ожидание получить нулевое значение этого коэффициента.
![architecture](plots/architecture.png)

## Метод решения
Поиск коэффициентов осуществлятся путем минимизации функции потерь методом градиентного спуска.

${loss(w) = \sum \left(y_i - \bar{y}_i(w) \right)^2}$

Ожидаемые значения весов: ${w=[-5, 2, -3, 1, 0]}$.

В целях ускорения сходимости метода исследованы следующие гиперпараметры:

- Шаг обучения (learning rate).
- Размер тренировочной выборки (batch size).
- Постоянная $\beta$ фильтра градиентов (Momentum):

${G_n = \beta G_{n-1} + (1 - \beta)grad_n}$

Градиент вычислялся численно, с помощью __[numpy.gradient](https://numpy.org/doc/2.0/reference/generated/numpy.gradient.html#numpy-gradient)__.

## Темп сходимости

![MSE](plots/MSE_plot.png)

Время вычисления измерялось __[time.time()](https://docs.python.org/3/library/time.html)__

![time](plots/time_plot.png)

## Результаты
Среди определенных значений весов ближайшие к искомым достигнуты за 200 тыс. итераций при параметрах
learning rate=[1e-3, 1e-4, 1e-5, 1e-6, 1e-7],
batch=0.2,
beta=0.0.

Значения весов следующие:
w=[-4.8463  1.0719 -1.7345  0.3845  0.0983]

![Trend lines](plots/trend_line_plot.png)


## Выводы
1. Фильтрация градиента (Momentum) оказалась бесполезной в этом частном случае функции потерь.
1. Learning rate влияет на скорость сходимости существенно. Лучше делать его разным. Тренируясь на полной выборе приходится делать этот параметр малым, иначе ошибка "улетает".
1. Тренировка на батчах ускоряет процесс. Как за счет того, что удается увеличить гиперпараметры learning rate, так и ввиду меньших вычислительных затрат в каждой итерации.
